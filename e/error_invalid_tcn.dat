0|41|Public
5000|$|... {{function}} first(x: array|string) = x0 writeln(first(2, 3)) // prints 1 writeln(first("hello")) // prints h writeln(first(45)) // <b>error,</b> <b>invalid</b> parameter type 'int' ...|$|R
25|$|Skepticism: Scientific facts {{must not}} be based on faith. One should always {{question}} every case and argument and constantly check for <b>errors</b> or <b>invalid</b> claims.|$|R
50|$|Most exchanges confirm setup with {{a verbal}} announcement, however some exchanges may use a ringing tone to {{indicate}} successful service setup and a busy tone to indicate an <b>error</b> / <b>invalid</b> code.|$|R
40|$|In {{this work}} we show how to detect ZigBee inter- ference on {{commodity}} WiFi cards by monitoring the reception errors, such as synchronization <b>errors,</b> <b>invalid</b> header formats, too long frames, etc., caused by ZigBee transmissions. Indeed, in presence of non-WiFi modulated signals, {{the occurrence of}} these types of errors follows statistics that can be easily recognized. Moreover, the duration of the error bursts depends on the transmission interval of the interference source, while the error spacing depends on the receiver implementation. On the basis of these considerations, we propose the adoption of hidden Markov chains for characterizing the behavior of WiFi receivers in presence of controlled interference sources (training phase) and then run-time recognizing the most likely cause of error patterns. Experimental results prove the effectiveness of our approach for detecting ZigBee interference...|$|R
30|$|The {{sample is}} then checked for data <b>errors</b> and <b>invalid</b> observations. No {{negative}} quotes are detected. Four ask quotes {{are found to}} be lower than the corresponding bid quotes. These observations are dropped from the sample. The remaining order imbalances, bid-ask spreads, and returns are tested for validity as described in the following.|$|R
3000|$|... {{values are}} {{much smaller than}} that assumed. Therefore, WLSQ {{underestimates}} the standard deviation of random <b>errors,</b> suggesting an <b>invalid</b> application of WLSQ to the present issue.|$|R
50|$|Carvaka school {{accepted}} {{only one}} valid source of knowledge - perception. It held all remaining methods as outright invalid or prone to <b>error</b> and therefore <b>invalid.</b>|$|R
40|$|This {{descriptive}} study determined {{which of the}} sources of errors would predict the errors committed by novice Java programmers. Descriptive statistics revealed that the respondents perceived that they committed the identified eighteen errors infrequently. Thought error was perceived to be the main source of error during the laboratory programming exercises. Factor analysis showed that there were five categories for the types of errors committed. Four of them were symbol- or keyword-related <b>errors</b> (<b>Invalid</b> symbols or keywords, Mismatched symbols, Missing symbols, and Excessive symbols) and the fifth one was Naming-related error (Inappropriate naming error). Regression analysis showed that Sensorimotor and Habit errors, together with Knowledge error, were found to predict Mismatched symbols and Missing symbols errors, respectively. Knowledge error was found to be the consistent source of the five types of errors. Thus, the null hypothesis stating that sources of errors do not predict errors committed by novice Java programmers is partially rejected. The implications of the findings were also discussed...|$|R
5000|$|Only {{a single}} species, Dasornis emuinus, is {{accepted}} today. However, {{it has a}} very convoluted synonymy, with its fossil remains assigned to no less than six genera (of which two were invalid junior homonyms) and divided between at least four species [...] - [...] excluding spelling <b>errors</b> and <b>invalid</b> [...] "corrections" [...] - [...] that were variously moved between these genera for almost 150 years: ...|$|R
25|$|The MMU {{may also}} {{generate}} illegal access <b>error</b> conditions or <b>invalid</b> page faults upon illegal or non-existing memory accesses, respectively, leading to segmentation fault or bus error conditions when {{handled by the}} operating system.|$|R
40|$|It {{is often}} {{reported}} in forecast combination literature that a simple average of candidate forecasts is more robust than sophisticated combining methods. This phenomenon is usually {{referred to as}} the "forecast combination puzzle". Motivated by this puzzle, we explore its possible explanations including estimation <b>error,</b> <b>invalid</b> weighting formulas and model screening. We show that existing understanding of the puzzle should be complemented by the distinction of different forecast combination scenarios known as combining for adaptation and combining for improvement. Applying combining methods without consideration of the underlying scenario can itself cause the puzzle. Based on our new understandings, both simulations and real data evaluations are conducted to illustrate the causes of the puzzle. We further propose a multi-level AFTER strategy that can integrate the strengths of different combining methods and adapt intelligently to the underlying scenario. In particular, by treating the simple average as a candidate forecast, the proposed strategy is shown to avoid the heavy cost of estimation error and, to a large extent, solve the forecast combination puzzle...|$|R
40|$|Abstract. As same {{as other}} products, {{computer}} software also has quality problem. This article, based on software’s features, which are function ability, reliability, usability, efficiency, maintainability and portability, has illustrated {{the relation between}} software defect, <b>error</b> and <b>invalid</b> and discussed the reliability and maintainability of software to ensure the designed function within life cycle. This article mainly discusses the relations among the defects, troubles and invalidities and the reliability {{as well as the}} maintainability of computer software, and the six features of describing and appraising computer software are put on according to the characteristics of our country’s computer software...|$|R
50|$|In {{fault-tolerant}} computer systems, {{programs that}} are considered robust are designed to continue operation despite an <b>error,</b> exception, or <b>invalid</b> input, instead of crashing completely. Software brittleness {{is the opposite of}} robustness. Resilient networks continue to transmit data despite the failure of some links or nodes; resilient buildings and infrastructure are likewise expected to prevent complete failure in situations like earthquakes, floods, or collisions.|$|R
40|$|Changes e 7 fb 645 chore: remove uses of "Object. assign" for typescript object spread de 16 e 1 a chore: bump "tested to" tag + update styles 491 d 323 fix: correct crossref {{abbreviation}} resolving issue fd 89685 chore: update dependencies b 15573 e fix: improve <b>error</b> {{message for}} <b>invalid</b> file extension on imports dd 2 be 97 fix: remove the words "from RIS file" from import tooltip 5 d 21 e 69 chore: update dependencie...|$|R
40|$|A {{prominent}} {{function of}} the anterior cinglI 1 ate cortex (ACC) is to process conflict between competing response options, In this study, we investigated the role of conflict processing in a response-priming task in which manual responses were either validly or invalidly cued. Examining electrophysiological measure-ments of oscillatory brain activity on the source level, we found response priming {{to be related to}} a beta power decrease in the premotor cortex and conflict processing to be linked to a theta power increase in the ACe. In particular, correlation of oscillatory brain activities in the ACC and the premotor cortex showed that conflict processing reduces response priming by slowing response time in valid trials and lowering response <b>errors</b> in <b>invalid</b> trials. This relationship emerged on a between subjects level as well as within subjects, on a single trial leveL These findings suggest that conflict processing in the ACC constrains the automatic priming process...|$|R
40|$|Static-semantics {{determines the}} {{validity}} of a program, while a typechecker provides more specific type error information. Type-checkers are specified based on the static semantics specification, {{for the purpose of}} identifying and presenting type <b>errors</b> in <b>invalid</b> programs. We discuss a style of algebraically specifying the static semantics of a language which facilitates automatic generation of a type-checker and a language specific error reporter. Such a specification can also be extended in a modular manner to yield human-readable error messages. 1 An Introduction Static-semantics of a language determines {{the validity of}} a program written in that language. Type-checking of a program, to be useful in practice, should not only indicate whether a given program is valid or not, but also summarize the type errors and show the location of the erroneous constructs which caused the errors. Thus, specifying a type-checker that is useful in practice results in (textually) modifying th [...] ...|$|R
30|$|The {{sample is}} then checked for data <b>errors</b> and <b>invalid</b> {{observations}}. Four observations are dropped because of negative bid-ask spreads, one {{because of a}} negative number of shares traded. The remainder of the sample is checked for validity. First, one observation is excluded as order imbalance differs from the cross-sectional daily average by more than 1.0, and other market variables around that date {{do not support the}} extreme value. Second, six observations from two stocks are excluded due to bid-ask spreads larger than 20 % of the bid quote. Another stock is dropped from the 2008 subperiod, since a share price slump leads to extraordinarily high bid-ask spreads for several weeks. Third, several stocks exhibit extreme returns on May  25 and May  26, 2005 although there are no unusual economic news, index returns or trading volumes on either of these days. To ensure data validity, we exclude May  25, 2005 for all stocks.|$|R
50|$|Two {{separate}} {{kinds of}} NaNs are provided, termed quiet NaNs and signaling NaNs. Quiet NaNs {{are used to}} propagate <b>errors</b> resulting from <b>invalid</b> operations or values, whereas signaling NaNs can support advanced features such as mixing numerical and symbolic computation or other extensions to basic floating-point arithmetic. For example, 0/0 is undefined as a real number, and so represented by NaN; the square root of a negative number is imaginary, and thus not representable as a real floating-point number, and so is represented by NaN; and NaNs {{may be used to}} represent missing values in computations.|$|R
40|$|With {{traditional}} testing, {{the test}} case has {{no control over}} non-deterministic scheduling decisions, and thus errors dependent on scheduling are only found by pure chance. Java Path Finder (JPF) is a specialized Java virtual machine that can systematically explore execution paths for all possible schedulings, and thus catch these errors. Unfortunately, execution-based model checkers, including JPF, cannot be easily adapted to support real-time programs. We propose a scheduling algorithm for JPF which allows testing of Safety Critical Java (SCJ) applications with periodic event handlers at SCJ levels 0 and 1 (without aperiodic event handlers). The algorithm requires that deadlines are not missed {{and that there is}} an execution time model that can give best- and worst-case execution time estimates for a given program path and specific program inputs. Our implementation, named R SJ, allows to search for scheduling dependent memory access <b>errors,</b> certain <b>invalid</b> argument <b>errors,</b> priority ceiling emulation protocol violations, and failed assertions in application code in SCJ programs for levels 0 and 1. It uses the execution time model of the Java Optimized Processor (JOP). We test our tool wit...|$|R
40|$|Counting or {{estimating}} {{the number of}} tags is crucial for RFID system. Researchers have proposed several fast cardinality estimation schemes to estimate the quantity of a batch of tags {{within a short time}} frame. Existing estimation schemes scarcely consider the privacy issue. Without effective protection, the adversary can utilize the responding signals to estimate the number of tags as accurate as the valid reader. To address this issue, we propose a novel privacy-preserving estimation scheme, termed as MEAS, which provides an active RF countermeasure against the estimation from invalid readers. MEAS comprises of two components, an Estimation Interference Device (EID) and two well-designed Interference Blanking Estimators (IBE). EID is deployed with the tags to actively generate interfering signals, which introduce sufficiently large estimation <b>errors</b> to <b>invalid</b> or malicious readers. Using a secret interference factor shared with EID, a valid reader can perform accurate estimation via two IBEs. Our theoretical analysis and simulation results show the effectiveness of MEAS. Meanwhile, MEAS can also maintain a high estimation accuracy using IBEs. © 2010 IEEE...|$|R
40|$|Through the years, several {{translated}} {{versions of}} Wechsler's intelligence test {{have been used}} in Indonesia, in clinical, educational or industrial settings. However, instruments such as Wechsler-Bellevue Intelligence Scale are outdated, have not been validated and lack proper normative data, resulting in measurement <b>errors</b> and <b>invalid</b> decisions made on the intellectual potential of individuals. The primary aim {{of this study was to}} adapt and validate the Wechsler Adult Intelligence Scale - fourth edition (WAIS-IV) for use in Indonesia. We described the first phase in the adaptation of the WAIS-IV in the Indonesian language, including translation, item analysis, and reliability of the subtests. The sample of this research consisted of 148 healthy participants who are representative for the Indonesian population with respect to gender, age groups (ages 16 to 83), educational levels, and ethnic background. Results showed that the sequence of the US WAIS-IV cannot be applied in Indonesia due to differences in index difficulties. Cronbach’s coefficient alphas for the WAIS-IV subtests ranged from. 74 -. 92. For the subtests from the Verbal Comprehension Index, the inter-rater agreement ranged between. 91 -. 97. In all, the adaptation of the WAIS-IV for Indonesia is psychometrically promising...|$|R
40|$|In {{a recent}} paper, Shim (2012) {{presented}} {{a very interesting}} authentication scheme for vehicular sensor networks. Shim claimed that the scheme is secure against the highest adopted level of attack, namely the chosen-message attack (CID-CMA). Nevertheless, {{we find that the}} proof in Shim 2 ̆ 7 s paper does not actually prove that the scheme is secure in this level. Instead, it can only ensure that the scheme is secure in a strictly weaker level of attack, the adaptive chosen-identity and no-message attack (CID-NMA). In this paper, first we show that there exist some security risks in vehicular networks if a scheme, which is only secure against CID-NMA but not CID-CMA, is deployed. Hence, having the proof that the scheme is only CID-NMA is insufficient for the aforementioned application. That is, Shim did not prove that the proposed scheme can resist these kinds of attack. Here, we use a different approach to prove the scheme for security against CID-CMA. We note that this proof is essential to ensure that the scheme can indeed be used for the aforementioned scenario. In addition, we also show that the batch verification of the scheme, proposed in the same paper, may have non-negligible <b>error.</b> Two <b>invalid</b> signatures may give a positive result. We further improve the batch verification part so that the error rate can be reduced to negligible level...|$|R
40|$|This paper {{presents}} structural {{estimates of}} the probability of validity, and the probability of Type I and Type II errors by courts in patent litigation. Patents are modeled as uncertain property rights, and implications of the model are tested using stock market reactions to patent litigation decisions. The estimation quantifies beliefs about patent validity and court errors in a Bayesian context. I estimate that the underlying beliefs about validity range from 0. 6 to 0. 7 for litigated patents. Market beliefs about courts show that Type I errors (finding a valid patent invalid) occur very frequently–an estimated probability of 0. 45. However, Type II <b>errors</b> (finding an <b>invalid</b> patent valid) occur with near zero probability. Additional implications of the model address patent value. My results are the first structural estimates of court errors. Additionally, {{this study is the}} first to perform event studies on patent litigation. ...|$|R
40|$|Abstract. A basic {{feature of}} many field {{experiments}} is that investigators are {{only able to}} randomize clusters of individuals—such as households, communities, firms, medical practices, schools or classrooms—even when the individual is the unit of interest. To recoup the resulting efficiency loss, some studies pair similar clusters and randomize treatment within pairs. However, many other studies avoid pairing, {{in part because of}} claims in the literature, echoed by clinical trials standards organizations, that this matched-pair, cluster-randomization design has serious problems. We argue that all such claims are unfounded. We also prove that the estimator recommended for this design in the literature is unbiased only in situations when matching is unnecessary; its standard <b>error</b> is also <b>invalid.</b> To overcome this problem without modeling assumptions, we develop a simple design-based estimator with much improved statistical properties. We also propose a model-based approach that includes some of the benefits of our design-based estimator as well as the estimator in the literature. Our methods also address individualleve...|$|R
40|$|Journal {{editors and}} academy presidents are {{increasingly}} calling on researchers {{to evaluate the}} substantive, {{as opposed to the}} statistical, significance of their results. To measure {{the extent to which these}} calls have been heeded, I aggregated the meta-analytically derived effect size estimates obtained from 965 individual samples. I then surveyed 204 studies published in the Journal of International Business Studies. I found that the average effect size in international business research is small, and that most published studies lack the statistical power to detect such effects reliably. I also found that many authors confuse statistical with substantive significance when interpreting their research results. These practices have likely led to unacceptably high Type II <b>error</b> rates and <b>invalid</b> inferences regarding real-world effects. By emphasizing p values over their effect size estimates, researchers are under-selling their results and settling for contributions that are less than what they really have to offer. In view of this, I offer four recommendations for improving research and reporting practices. ...|$|R
40|$|Abstract. Exception {{handling}} is {{a powerful}} abstraction {{that can be used}} to help manage errors and support the construction of reliable operating systems. Using exceptions to notify system components about exceptional conditions also reduces coupling of error handling code and increases the modularity of the system. We explore the benefits of incorporating exception handling into the Choices operating system in order to improve reliability. We extend the set of exceptional error conditions in the kernel to include critical kernel <b>errors</b> such as <b>invalid</b> memory access and undefined instructions by wrapping them with language-based software exceptions. This allows developers to handle both hardware and software exceptions in a simple and unified manner through the use of an exception hierarchy. We also describe a catch-rethrow approach for exception propagation across protection domains. When an exception is caught by the system, generic recovery techniques like policy-driven micro-reboots and restartable processes are applied, thus increasing the reliability of the system. ...|$|R
40|$|Analysis of the {{absorption}} spectra in the visible range {{has made it}} possible to monitor the oxygen supply and metabolism of organic tissues continuously and non-invasively. The nonlinear multicomponent analysis (NLMCA) is an evaluation method for the reflection spectra of the tissues. In this report, the derivation of the relation between the reflection and absorption spectra based upon Kubelka-Monk theory is reviewed to introduce the foundation of the NLMCA technique. Then, the procedures of the NLMCA algorithm are described and simulation studies are performed. The results indicate that the NLCMA algorithm generally works well after a minor modification but is sensitive to modelling <b>errors</b> due to <b>invalid</b> assumptions on the scattering properties of samples. A direct algorithm based on linear least squares is the suggested to deal with the same problem and simulation studies are performed to make a comparison with the NLMCA algorithm, which demonstrate a better robust properties of the alternative strategy...|$|R
30|$|Different {{from the}} second order {{displacement}} obstacle problems (solutions of that have the H^ 2 regularity, which allows the corresponding strong form of the variational inequality {{to be used in}} the convergence analysis of finite element methods (FEMs) [3 – 9]), the fourth order problem (1) has a unique solution u belonging to H^ 3 (Ω)∩ C^ 2 (Ω) (in general u ∉ H^ 4 _loc(Ω) even for smooth data) [10 – 12]. This lack of the H^ 4 regularity means that the corresponding strong form of the variational inequality (2) is not available for the convergence analysis of FEMs, which leads to a cardinal difficulty in optimal order error estimates. Although FEMs for the fourth order variational inequality with one side displacement obstacle were investigated and optimal order error estimates were obtained in [13 – 16], the techniques, which depend greatly on the one side displacement obstacle condition, can not be applied to the two-sided case directly. The main reason is that the two-sided condition will lead to an important inequality in the <b>error</b> estimate <b>invalid,</b> which makes the convergence analysis complicated and difficult to be handled. Recently, a new unified convergence analysis for C^ 1 -conforming FEs, classical nonconforming FEs, and discontinuous FEMs for problem (1) was developed in [17, 18]. In which the optimal error estimate in the energy norm with order O(h) was established by using an auxiliary obstacle problem and an enriching operator (here and later h denotes the mesh parameter). Subsequently the idea was extended to a generalized FEM in [19] and a quadratic C^ 0 interior penalty method in [20].|$|R
40|$|Software product-lines (SPLs) are {{software}} architectures {{that can}} be readily reconfigured for different project requirements. A key part of an SPL is a model that captures the rules for reconfiguring the software. SPLs commonly use feature models to capture SPL configuration rules. Each SPL configuration is represented as a selection of features from the feature model. Invalid SPL configurations can be created due to feature conflicts introduced via staged or parallel configuration or changes to the constraints in a feature model. When invalid configurations are created, a method is needed to automate the diagnosis of the errors and repair the feature selections. This paper provides two contributions to research on automated configuration of SPLs. First, it shows how configurations and feature models can be transformed into constraint satisfaction problems to automatically diagnose <b>errors</b> and repair <b>invalid</b> feature selections. Second, it presents empirical results from diagnosing configuration errors in feature models {{ranging in size from}} 100 to 5, 000 features. The results of our experiments show that our CSP-based diagnostic technique can scale up to models with thousands of features...|$|R
40|$|In this paper, {{we present}} MAS, a {{practical}} memory analysis system for identifying a kernel rootkit’s memory footprint in an infected system. We also present two large-scale studies of applying MAS to 848 real-world Windows kernel crash dumps and 154, 768 potential malware samples. <b>Error</b> propagation and <b>invalid</b> pointers are two key challenges that stop previous pointer-based memory traversal solutions from {{effectively and efficiently}} analyzing real-world systems. MAS uses a new memory traversal algorithm to support error correction and stop error propagation. Our enhanced static analysis allows the MAS memory traversal to avoid error-prone operations and provides it with a reliable partial type assignment. Our experiments show that MAS was able to analyze all memory snapshots quickly with typical running times between 30 and 160 seconds per snapshot and with near perfect accuracy. Our kernel malware study observes that the malware samples we tested hooked 191 different function pointers in 31 different data structures. With MAS, {{we were able to}} determine quickly that 95 out of the 848 crash dumps contained kernel rootkits...|$|R
40|$|The reputed {{deterrent}} effect exerted by capital punishment, that executing convicted murderers reduces {{the number of}} homicides, is frequently cited as a justification for this penalty's employment. Considerable attention {{has been devoted to}} the deterrence hypothesis, with economists producing an impressive amount of research although with inconclusive results. However, the majority of studies focus on the USA with a major void in research relating to Britain. This thesis addresses the issue of deterrence utilising data from England and Wales. We test the robustness of an earlier study's results, reproducing Wolpin's (1987) data set and following a comparable regression analysis. As Wolpin's investigation only includes three years post-abolition data, it may have failed to capture the true {{deterrent effect}} should policy changes only modify behaviour in the long term. Consequently, this thesis extends the analysis to include the full impact of data from the post-abolition period. Additionally, we examine the series for stationarity as it has recently been demonstrated that regression analysis using non-stationary series produces inconsistent coefficient estimates, biased estimates of coefficient standard <b>errors</b> and <b>invalid</b> standard statistical inference procedures. Ultimately, we consider a more appropriate modelling technique.;Contrary to previous studies the general conclusion reached in this thesis is that there is limited evidence that the death sentence exerted a significant deterrent effect on murders and manslaughters in England and Wales. Although a robust deterrent effect did not emerge using many of the models, it must be stressed that this hardly qualifies as unequivocal confirmation that capital punishment does not exert a deterrent effect, for absence of proof is not at all the same thing as proof of absence. Indeed, while a statistically significant deterrent effect could only be detected using a 'stripped down' model, the fact that such a result emanated from an extended data set and the preferred model specification does convey this finding extra weight...|$|R
40|$|Neurophenomenology {{requires}} {{the marriage of}} first- and third-person data. Yet, for decades, cognitive scientists mostly considered first-person descriptions {{to be prone to}} <b>error</b> and thus <b>invalid.</b> Only recently, studies have shown {{that it is possible to}} gain trustworthy subjective data under ideal reporting conditions with certain in-depth introspective techniques, such as Elicitation-Interviews, focusing on singular experiences. In the context of the ReSource Project, a multimethod, longitudinal mental-training study, 107 Elicitation-Interviews were conducted on (1) breathing- (2) observing-thoughts- and (3) loving-kindness meditation in order to investigate the respective differential first-person experiences. Comprehensive analyses of 78 resulting transcripts – ranging from linguistic computer-based quantitative analyses, over refined assessment of the rich first-person data by 4 independent raters on the basis of a newly developed coding system, to clustering the interviewees’ original reports – clearly indicate a distinct experiential nature for each of these meditation techniques, e. g. regarding bodily, temperature and color sensations, and affective states. Apart from quantitatively analyzing these qualitative reports, the raw data allow for many more ways of exploring the underlying experiences which common questionnaires cannot capture. These can inform third-person data collected with an exhaustive battery of tasks in the ReSource Project...|$|R
40|$|The ‘belief bias’ {{effect is}} one of the most {{pervasive}} findings in the study of syllogistic reasoning. Here, participants respond “valid” to more believable than unbelievable conclusions, regardless of the actual validity of the conclusion. There is also an interaction characteristic of the belief bias effect, in that conclusion believability plays a greater role when conclusions are invalid than when they are valid. The experiments reported in this thesis had two goals: first, to determine how individual differences in working memory (WM) capacity influence belief bias in reasoning; and second, to indentify which WM systems are involved in syllogistic deductive reasoning. To this end, both experiments employed a dual task paradigm. In Experiment 1, participants remembered spatial arrays whilst reasoning through syllogisms in order to load the visuospatial sketchpad. Results demonstrated that performance on the secondary spatial memory task suffered when participants reasoned through syllogisms of which the validity and believability of conclusions were incongruent (i. e., “conflict” problems), indicating that reasoning through conflict problems utilized limited visuospatial WM resources. Also, only participants with high WM capacities showed the typical belief-bias effect, with greater effects of conclusion believability on invalid than on valid conclusions. This interaction was not present for low WM span participants, because they made greater errors on problems with invalid, unbelievable conclusions. In Experiment 2, participants remembered digit sequences whilst reasoning in order to load the phonological loop. Both of the major results from Experiment 1 were replicated. Accuracy on the secondary digit recall task was impaired when participants reasoned through conflict problems, demonstrating that limited verbal WM resources were directed toward reasoning. Again, only high WM span participant showed the interaction between conclusion validity and believability characteristic of the belief bias effect. Effects were additive for low WM span participants because they made more <b>errors</b> on <b>invalid,</b> unbelievable syllogisms. Results from both experiments demonstrate first, that both visuospatial and verbal WM resources are involved in syllogistic reasoning, and second, that individuals with different amounts of available WM resources demonstrate differential belief bias. These results are discussed in terms of the mental models and mental logic theories of reasoning and in terms of dual process accounts of reasoning...|$|R
40|$|This thesis {{presents}} the first automatic date processing system developed on a Canadian real-life standard cheque database. This system can process unconstrained handwritten dates written in English or in French, {{and it can}} also be applied to the recognition of any handwritten dates with similar format on many other kinds of documents. A knowledge-based module has been proposed for the date segmentation and a new cursive month word recognition system has also been implemented based on a combination of classifiers. The interaction between the segmentation and recognition stages has been properly established by using a multi-hypotheses generation and evaluation module. In addition, a verification module with two levels is designed in the postprocessing stage to correct some <b>errors</b> and reject <b>invalid</b> results, which further improves the reliability of the system. The segmentation of the date zone can be implemented in the knowledge-based segmentation module, the multi-hypotheses generation and evaluation module, or the verification module. An effective neural network ensemble system is proposed in this knowledge extraction stage to differentiate handwritten alphabetic words from numeric strings (A/N). We investigate the use of effective features extensively, and propose several new methods in the design of neural networks, creation of neural network ensembles, and combination methods for the ensembles created. For date recognition, the new cursive month word recognizer is implemented by combining a Hidden Markov Model classifier (HMM) with two Multi-Layer Perceptron (MLP) classifier...|$|R
40|$|A basic {{feature of}} many field {{experiments}} is that investigators are {{only able to}} randomize clusters of individuals [...] such as households, communities, firms, medical practices, schools or classrooms [...] even when the individual is the unit of interest. To recoup the resulting efficiency loss, some studies pair similar clusters and randomize treatment within pairs. However, many other studies avoid pairing, {{in part because of}} claims in the literature, echoed by clinical trials standards organizations, that this matched-pair, cluster-randomization design has serious problems. We argue that all such claims are unfounded. We also prove that the estimator recommended for this design in the literature is unbiased only in situations when matching is unnecessary; its standard <b>error</b> is also <b>invalid.</b> To overcome this problem without modeling assumptions, we develop a simple design-based estimator with much improved statistical properties. We also propose a model-based approach that includes some of the benefits of our design-based estimator as well as the estimator in the literature. Our methods also address individual-level noncompliance, which is common in applications but not allowed for in most existing methods. We show that from the perspective of bias, efficiency, power, robustness or research costs, and in large or small samples, pairing should be used in cluster-randomized experiments whenever feasible; failing to do so is equivalent to discarding a considerable fraction of one's data. We develop these techniques {{in the context of a}} randomized evaluation we are conducting of the Mexican Universal Health Insurance Program. Comment: This paper commented in: [arXiv: 0910. 3754], [arXiv: 0910. 3756]. Rejoinder in [arXiv: 0910. 3758]. Published in at [URL] the Statistical Science ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
40|$|In {{distributed}} systems, if {{a hardware}} fault corrupts {{the state of}} a process, this error might propagate as a corrupt message and contaminate other processes in the system, causing severe outages. Recently, state corruptions of this nature have been observed surprisingly often in large computer populations, e. g., in large-scale data centers. Moreover, since the resilience of processors is expected to decline in the near future, the likelihood of state corruptions will increase even further. In this work, we argue that preventing the propagation of state corruption should be a first-class requirement for large-scale fault-tolerant distributed systems. In particular, we propose developers to target error isolation, the property in which each correct process ignores any corrupt message it receives. Typically, a process cannot decide whether a received message is corrupt or not. Therefore, we introduce hardening as a class of principled approaches to implement error isolation in distributed systems. Hardening techniques are (semi-) automatic transformations that enforce that each process appends an evidence of good behavior {{in the form of}} error codes to all messages it sends. The techniques “virtualize” state corruptions into more benign failures such as crashes and message omissions: if a faulty process fails to detect its state corruption and abort, then hardening guarantees that any corrupt message the process sends has <b>invalid</b> <b>error</b> codes. Correct processes can then inspect received messages and drop them in case they are corrupt. With this dissertation, we contribute theoretically and practically to {{the state of the art}} in fault-tolerant distributed systems. To show that hardening is possible, we design, formalize, and prove correct different hardening techniques that enable existing crash-tolerant designs to handle state corruption with minimal developer intervention. To show that hardening is practical, we implement and evaluate these techniques, analyzing their effect on the system performance and their ability to detect state corruptions in practice...|$|R
