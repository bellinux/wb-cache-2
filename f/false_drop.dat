12|56|Public
40|$|Abstract — In this paper, we {{investigate}} the tradeoff between performance and confidentiality in signature-based air indexing schemes for wireless data broadcast. Two metrics, namely <b>false</b> <b>drop</b> probability and false guess probability, are defined {{to quantify the}} filtering efficiency and confidentiality loss of a signature scheme. Our analysis reveals that <b>false</b> <b>drop</b> probability and false guess probability share a similar trend as the tuning parameters of a signature scheme change and {{it is impossible to}} achieve a low <b>false</b> <b>drop</b> probability and a high false guess probability simultaneously. In order to balance the performance and confidentiality, we perform an analysis to provide a guidance for parameter settings of the signature schemes to meet different system requirements. In addition, we propose the jump pointer technique and the XOR signature scheme to further improve the performance and confidentiality. A comprehensive simulation has been conducted to validate our findings. I...|$|E
40|$|A {{signature}} file organization, called the weight-partitioned {{signature file}}, for supporting document ranking is proposed. It employs multiple signature files, {{each of which}} corresponds to one term frequency, to represent terms with different term frequencies. Words with the same term frequency in a document are grouped together and hashed into the signature file corresponding to that term frequency. This eliminates the need to explicitly record the term frequency for each word. We investigate the effect of false drops on retrieval effectiveness {{if they are not}} eliminated in the search process. We have shown that false drops introduce insignificant degradation on precision and recall when the <b>false</b> <b>drop</b> probability is below a certain threshold. This is an important result since <b>false</b> <b>drop</b> elimination could become the bottleneck in systems using fast signature file search techniques. We perform an analytical study on the performance of the weight-partitioned signature file un [...] ...|$|E
40|$|A {{variation}} of the superimposed coding version of the signature file method is introduced. The processing of a user query {{is similar to that}} of the conventional method with one difference: the output is provided in a ordered (ranked) way. The latter is based on a credibility value reflecting the probability of each candidate block to pass the validation test rather than being a <b>false</b> <b>drop.</b> The principles, an analytical model, and simulation results of the new technique are presented. The model’s usefulness is the reduced I/O activity for cases where the user is only sampling the text base and is satisfied by retrieving a limited number of the many documents that qualify. 1...|$|E
40|$|Advanced {{database}} {{systems have}} to support complex data structures as treated in object-oriented data models and nested relational data models. In particular, efficient processing of set-valued object retrieval (simply, set retrieval) is indispensable for such systems. In the previous paper [6], we proposed the use of signature files as efficient set retrieval facilities and showed their potential capabilities based on a disk page access cost model. Retrieval with signature files is always accompanied by mismatches called <b>false</b> <b>drops,</b> {{and it is very}} important in designing signature files to properly control the <b>false</b> <b>drops.</b> In this paper, we present an in-depth study of <b>false</b> <b>drops</b> in set retrieval with signature files. We derive formulas estimating <b>false</b> <b>drops</b> in four types of set retrieval based on the "has-subset," "is-subset," "has-intersection," and "is-equal" relationships. Then we evaluate their validity by computer simulations. Simulation study is also done to investigate fal [...] ...|$|R
30|$|It was observed, {{that the}} only problem of the method is {{shivering}} of the measured limb, what causes <b>false</b> <b>dropping</b> and consequently increase of measured volume. It is clear from SD, which is about 2 times lower in case of reference cylinders. This phenomenon has been observed more at the last measurements, when examined subject started to be tired. Measure procedure is also time consuming (15  min.) comparing to circumferential methods, because the dropping is very slow.|$|R
40|$|The article {{describes}} ways {{of increasing the}} cost- effectiveness of online searching. Information from previous searches is stored and up-dated as searches are repeated. Recall is improved by broadening the search, e. g. by adding broader indexing terms, multiple file searching and citation searching, and then precision is improved, particularly in infringement searching, by imposing limitations such as 'and' and 'and not' operators. For example, <b>false</b> <b>drops</b> found in previous searches, expired patents, and 'not in force patents' obtained from legal status databases are eliminated and the search limited to specific countries. ...|$|R
40|$|Studies on the {{performance}} issues (i. e., access latency and energy conservation) of wireless data broadcast {{have appeared in the}} literature. However, the important security issues have not been well addressed. This paper investigates the tradeoff between performance and security of signature-based air index schemes in wireless data broadcast. From {{the performance}} perspective, keeping low <b>false</b> <b>drop</b> probability helps clients retrieve the information from a broadcast channel efficiently. Meanwhile, from the security perspective, achieving high false guess probability prevents the hacker from guessing the information easily. There is a tradeoff between these two aspects. An administrator of the wireless broadcast system may balance this tradeoff by carefully configuring the signatures used in broadcast. This study provides a guidance for parameter settings of the signature schemes {{in order to meet the}} performance and security requirements. Experiments are performed to validate the analytical results and to obtain optimal signature configuration corresponding to different application criteria. Copyright 2005 ACM...|$|E
40|$|Bloom lters are {{a scheme}} to improve the run time of access methods that ordinarily would search through a page of user records to {{determine}} whether the desired record is present. The scheme is useful in situations typi ed by linear hashing, sequence nodes in B+trees, and interpolation-based index maintenance in which there are large pages to be searched. A lter consists of a sequence of m bits. A suitable hash function h(x) =f 0; 1; 2;:::; m, 1 g is assumed to be available. During an insertion, when key k is to be placed on page p, the bit in position h(k) of the lter of page p is set. During an access operation for key k, as page q is considered the bit position h(k) of the signature of page q is examined. If it is set, we must examine the contents of the page; otherwise we can immediately proceed to next page. Bloom Filters Analysis Assume we are looking for key k within the data structure. We have just read from disk page p. Suppose that lter bit set by key k is set within page p. Now we can ask what is the probability that key k is really on page p? Or put in a negative way, what is the probability that the proper signature bit is set while the desired key k is not on the page. We refer to the situation, where the proper signature bit is indeed set while the desired key in not on the page, as a <b>false</b> <b>drop.</b> The purpose of this analysis is to determine an upper bound on the probability of a <b>false</b> <b>drop</b> for a given lter size and page size. We assume that a signature contains m bits. We also assume a page always constains a signature as well as n keys. The probability that a particular bit is set when akey is inserted into a page is 1. Then the probability that a particular bit is not set after n keys are inserted is m, 1 m a particular bit is set after n keys are inserted i...|$|E
40|$|Previous work on {{superimposed}} coding {{are characterized by}} two aspects. First, it is in general assumed that signatures are generated from logical text blocks of the same size. That is, each block contains {{the same number of}} unique terms after stopword and duplicate removal. We call this approach the fixed size block (FSB) method, since each text block has the same size as measured by the number of unique terms contained in it. Second, with only a few exceptions [7, 8, 9, 16], most previous work has assumed that each term in the text contributes the same number of 1 's to the signature (i. e., the weight of the term signatures is fixed). The main objective {{of this paper is to}} derive an optimal weight assignment which assigns weights to document terms according to their occurrence and query frequencies to minimize the <b>false</b> <b>drop</b> probability. The optimal scheme can account for both uniform and nonuniform occurrence and query frequencies, and the signature generation method is still based on ha [...] ...|$|E
50|$|The {{simplification}} rule allows values whose {{condition is}} <b>false</b> to be <b>dropped.</b>|$|R
5000|$|As this {{statement}} is asserted true, all the <b>false</b> values are <b>dropped</b> giving, ...|$|R
2500|$|... including, {{shortest}} path, traveling salesman, knapsack, <b>false</b> coin, egg <b>dropping,</b> {{bridge and}} torch, replacement, chained matrix products, and critical path problem.|$|R
40|$|For {{signature}} files {{we propose}} a new <b>false</b> <b>drop</b> estimation method for databases with varying record lengths. Our approach provides more accurate {{estimation of the}} number of false drops by considering the lengths of individual records instead of using the average number of terms per record. In signature file processing, accurate estimation {{of the number of}} false drops is essential to obtain a more accurate signature file and therefore to obtain a better (query) response time. With a formal proof we show that under certain conditions the number of false drops estimated by considering the average record length is {{less than or equal to}} the precise 'expected' estimation which is based on the individual record lengths. The experiments with real data show that the proposed method accurately estimates the number of false drops and the actual response time. Depending on the space overhead, our approach obtains up to 33 % and 20 % response time improvements for the conventional sequential and new efficient multiframe signature file methods, respectively...|$|E
40|$|Signature files {{seem to be}} a {{promising}} access method for text and attributes. According to this method, the documents (or records) are stored sequentially in one file (“text file”), while abstractions of the documents (“signatures”) are stored sequentially in another file (“signature file”). In order to resolve a query, the signature file is scanned first, and many nonqualifying documents are immediately rejected. We develop a framework that includes primary key hashing, multiattribute hashing, and signature files. Our effort is to find the optimal signature extraction method. The main contribution of this paper is that we present optimal and efficient suboptimal algorithms for assigning words to signatures in several environments. Another contribution is that we use information theory, and study the relationship of the <b>false</b> <b>drop</b> probability Fd and the information that is lost during signature extraction. We give tight lower bounds on the achievable Fd and show that a simple relationship holds between the two quantities in the case of optimal signature extraction with uniform occurrence and query frequencies. We examine hashing as a method to map words to signatures (instead of the optimal way), and show that the same relationship holds between Fd and loss, indicating that an invariant may exist between these two quantities for every signature extraction method...|$|E
40|$|Signature {{files are}} {{extremely}} compressed versions of text files {{which can be}} used as access or index files to facilitate searching documents for text strings. These access files, or signatures, are generated by storing 2 ̆ 2 hashed 2 ̆ 2 codes for individual words. Given the possible generation of similar codes in the hashing or storing process, the primary concern in researching signature files is to determine the accuracy of retrieving information. Inaccuracy is always represented by the false signaling of the presence of a text string. Two suggested ways to alter <b>false</b> <b>drop</b> rates are: 1) to determine if either of the two methologies for storing hashed codes, by superimposing them or by concatenating them, is more efficient; and 2) to determine if a particular hashing algorithm has any impact. To assess these issues, the history of suprimposed coding is traced from its development as a tool for compressing information onto punched cards in the 1950 s to its incorporation into proposed signature file methodologies in the mid- 19802 ̆ 7 s. Likewise, the concept of compressing individual words by various algorithms, or by hashing them is traced through the research literature. Following this literature review, benchmark trials are performed using both superimposed and concatenated methodologies while varying hashing algorithms. It is determined that while one combination of hashing algorithm and storage methodology is better, all signature file mehods can be considered viable...|$|E
40|$|Todays {{abundance}} of storage coupled with digital technologies in virtually any scientific or commercial application such as medical and biological imaging or music archives deal with tremendous quantities of images, videos or audio files stored in large multimedia databases. For content-based data mining and retrieval purposes suitable similarity models are crucial. The Earth Mover’s Distance {{was introduced in}} Computer Vision to better approach human perceptual similarities. Its computation, however, is too complex for usage in interactive multimedia database scenarios. In order to enable efficient query processing in large databases, we propose an index-supported multistep algorithm. We therefore develop new lower bounding approximation techniques for the Earth Mover’s Distance which satisfy high quality criteria including completeness (no <b>false</b> <b>drops),</b> index-suitability and fast computation. We demonstrate the efficiency of our approach in extensive experiments on large image databases. ...|$|R
50|$|The {{mountain}} rises gradually {{from the}} north, with many <b>false</b> summits. The <b>drop</b> to the south, to the col with Giant Ledge, is much sharper.|$|R
40|$|Many {{database}} applications require effective {{representation of}} regional objects in high-dimensional spaces. By applying an original query transformation, a recently proposed access method for regional data, called the simple QSF-tree (sQSF-tree), effectively attacks {{the limitations of}} traditional spatial access methods in spaces with many dimensions. Nevertheless, sQSF-trees are not immune to all problems associated with high data dimensionality. Based on the analysis of sQSF-trees, this paper presents a new variant of sQSF-trees, called the scalable QSF-tree (cQSF-tree), which relies on a heuristic optimization {{to reduce the number}} of <b>false</b> <b>drops</b> into pages that contain no object satisfying the query. By increasing the selectivity of search predicates, cQSF-trees improve the performance of multidimensional selections. Experimental evidence shows that cQSF-trees are more scalable than sQSF-trees to the growing data dimensionality. The performance improvements also increase with more skewed data distribution. Keywords: database management, spatial access methods, data dimensionality...|$|R
40|$|The {{purpose of}} this study was to develop and {{evaluate}} a measurement model for Internet information retrieval strategy performance evaluation whose theoretical basis is a modification of the classical measurement model embodied in the Cranfield studies and their progeny. Though not the first, the Cranfield studies were the most influential of the early evaluation experiments. The general problem with this model was and continues to be the subjectivity of the concept of relevance. In cyberspace, information scientists are using quantitative measurement models for evaluating information retrieval performance that are based on the Cranfield model. This research modified this model by incorporating enduser relevance judgment rather than using objective relevance judgments, and by adopting a fundamental unit of measure developed for the cyberspace of Internet information retrieval rather than using recall and precision-type measures. The proposed measure, the Content-bearing Click (CBC) Ratio, was developed as a quantitative measure reflecting the performance of an Internet IR strategy. Since the hypertext "click" is common to many Internet IR strategies, it was chosen as the fundamental unit of measure rather than the "document. " The CBC Ratio is a ratio of hypertext click counts that can be viewed as a <b>false</b> <b>drop</b> measure that determines the average number of irrelevant content-bearing clicks that an enduser check before retrieving relevant information. After measurement data were collected, they were used to evaluate the reliability of several methods for aggregating relevance judgments. After reliability coefficients were calculated, measurement model was used to compare web catalog and web database performance in an experimental setting. Conclusions were the reached concerning the reliability of the proposed measurement model and its ability to measure Internet IR performance, as well as implications for clinical use of the Internet and for future research in Information Science...|$|E
40|$|Wong Chi Yin. Thesis (M. Phil.) [...] Chinese University of Hong Kong, 1998. Includes bibliographical {{references}} (leaves 107 - 114). Abstract also in Chinese. Abstract [...] - p. iiAcknowledgements [...] - p. viChapter 1 [...] - Introduction [...] - p. 1 Chapter 1. 1 [...] - Introduction to Chinese IR [...] - p. 1 Chapter 1. 2 [...] - Contributions [...] - p. 3 Chapter 1. 3 [...] - Organization of this Thesis [...] - p. 5 Chapter 2 [...] - Background [...] - p. 6 Chapter 2. 1 [...] - Indexing methods [...] - p. 6 Chapter 2. 1. 1 [...] - Full-text scanning [...] - p. 7 Chapter 2. 1. 2 [...] - Inverted files [...] - p. 7 Chapter 2. 1. 3 [...] - Signature files [...] - p. 9 Chapter 2. 1. 4 [...] - Clustering [...] - p. 10 Chapter 2. 2 [...] - Information Retrieval Models [...] - p. 10 Chapter 2. 2. 1 [...] - Boolean model [...] - p. 11 Chapter 2. 2. 2 [...] - Vector {{space model}} [...] - p. 11 Chapter 2. 2. 3 [...] - Probabilistic model [...] - p. 13 Chapter 2. 2. 4 [...] - Logical model [...] - p. 14 Chapter 3 [...] - Investigation of Segmentation on the Vector Space Retrieval Model [...] - p. 15 Chapter 3. 1 [...] - Segmentation of Chinese Texts [...] - p. 16 Chapter 3. 1. 1 [...] - Character-based segmentation [...] - p. 16 Chapter 3. 1. 2 [...] - Word-based segmentation [...] - p. 18 Chapter 3. 1. 3 [...] - N-Gram segmentation [...] - p. 21 Chapter 3. 2 [...] - Performance Evaluation of Three Segmentation Approaches [...] - p. 23 Chapter 3. 2. 1 [...] - Experimental Setup [...] - p. 23 Chapter 3. 2. 2 [...] - Experimental Results [...] - p. 24 Chapter 3. 2. 3 [...] - Discussion [...] - p. 29 Chapter 4 [...] - Signature File Background [...] - p. 32 Chapter 4. 1 [...] - Superimposed coding [...] - p. 34 Chapter 4. 2 [...] - <b>False</b> <b>drop</b> probability [...] - p. 36 Chapter 5 [...] - Partitioned Signature File Based On Chinese Word Length [...] - p. 39 Chapter 5. 1 [...] - Fixed Weight Block (FWB) Signature File [...] - p. 41 Chapter 5. 2 [...] - Overview of PSFC [...] - p. 45 Chapter 5. 3 [...] - Design Considerations [...] - p. 50 Chapter 6 [...] - New Hashing Techniques for Partitioned Signature Files [...] - p. 59 Chapter 6. 1 [...] - Direct Division Method [...] - p. 61 Chapter 6. 2 [...] - Random Number Assisted Division Method [...] - p. 62 Chapter 6. 3 [...] - Frequency-based hashing method [...] - p. 64 Chapter 6. 4 [...] - Chinese character-based hashing method [...] - p. 68 Chapter 7 [...] - Experiments and Results [...] - p. 72 Chapter 7. 1 [...] - Performance evaluation of partitioned signature file based on Chi- nese word length [...] - p. 74 Chapter 7. 1. 1 [...] - Retrieval Performance [...] - p. 75 Chapter 7. 1. 2 [...] - Signature Reduction Ratio [...] - p. 77 Chapter 7. 1. 3 [...] - Storage Requirement [...] - p. 79 Chapter 7. 1. 4 [...] - Discussion [...] - p. 81 Chapter 7. 2 [...] - Performance evaluation of different dynamic signature generation methods [...] - p. 82 Chapter 7. 2. 1 [...] - Collision [...] - p. 84 Chapter 7. 2. 2 [...] - Retrieval Performance [...] - p. 86 Chapter 7. 2. 3 [...] - Discussion [...] - p. 89 Chapter 8 [...] - Conclusions and Future Work [...] - p. 91 Chapter 8. 1 [...] - Conclusions [...] - p. 91 Chapter 8. 2 [...] - Future work [...] - p. 95 Chapter A [...] - Notations of Signature Files [...] - p. 96 Chapter B [...] - <b>False</b> <b>Drop</b> Probability [...] - p. 98 Chapter C [...] - Experimental Results [...] - p. 103 Bibliography [...] - p. 10...|$|E
40|$|In this paper, {{we propose}} an {{original}} {{solution for the}} general reverse k-nearest neighbor (RkNN) search problem. Compared to the limitations of existing methods for the RkNN search, our approach works on top of any hierarchically organized tree-like index structure and, thus, is applicable to any type of data {{as long as a}} metric distance function is defined on the data objects. We will exemplarily show how our approach works on top of the most prevalent index structures for Euclidean and metric data, the R-Tree and the M-Tree, respectively. Our solution is applicable for arbitrary values of k and can also be applied in dynamic environments where updates of the database frequently occur. Although being the most general solution for the RkNN problem, our solution outperforms existing methods in terms of query execution times because it exploits different strategies for pruning <b>false</b> <b>drops</b> and identifying true hits as soon as possible...|$|R
40|$|The {{items in}} a spatial {{database}} have location, extent, and shape {{with respect to}} a spatial coordinate system. Simple approximations to these attributes, say by bounding rectangles, are storage efficient and easy to manipulate. But effective spatial retrieval (on either location, extent, or shape) require a more precise representation of these attributes. In this report, we describe a highly compressed quadtree representation, called a Q 0 -tree, which supports spatial queries without <b>false</b> <b>drops</b> or unnecessary storage accesses. This access structure is dynamic. Moreover, because it is an exact representation of the spatial configuration, the spatial operators union, intersection, and difference can be coded with respect to the Q 0 -tree itself without needing a separate representation of the configuration, and, in worst case, exhibit linear performance. We discuss quadtrees, octtrees, grid files, R-trees, cell trees, and zkd B-trees; and provide a more detailed qualitative comparis [...] ...|$|R
40|$|Signature {{files are}} one {{technique}} for indexing documents for full-text retrieval systems. This paper discusses two methods for generating text signatures [...] the word fragmentation and the pseudo-random generation techniques. The paper evaluates the effectiveness {{and efficiency of}} generating text signatures using these techniques. It also determines the optimal set of characteristics that define a text signature {{that is to be}} used for superimposed signature file indexes. The optimal set of characteristics can be used to create text signatures that minimise the number of <b>false</b> <b>drops</b> retrieved from the information system. Keywords Full-text retrieval; Searching; Signature Files; Superimposed coding; Text retrieval systems; Text signatures. Page 1 1. Introduction A text retrieval system is characterised by two components. The text database consists of a collection of text documents. The documents can either be unstructured (that is, devoid of any of the traditional database field str [...] ...|$|R
40|$|Abstract. This paper {{presents}} a new method for similarity retrieval of 3 D surface segments in spatial database systems {{as used in}} molecular biology, medical imaging, or CAD. We propose a similarity criterion and algorithm for 3 D surface segments {{which is based on}} the approximation of segments by using multi-parametric functions. The method can be adjusted to individual requirements of specific applications by choosing appropriate surface functions as approximation models. For an efficient evaluation of similarity queries, we developed a filter function which supports fast searching based on spatial index structures and guarantees no <b>false</b> <b>drops.</b> The evaluation of the filter function requires a new query type with multidimensional ellipsoids as query regions. We present an algorithm to efficiently perform ellipsoid queries on the class of spatial index structures that manage their directory by rectilinear hyperrectangles, such as R-trees or X-trees. Our experiments show both, effectiveness as well as efficiency of our method using a sample application from molecular biology. ...|$|R
40|$|Abstract. Similarity {{search in}} large {{multimedia}} databases requires efficient query processing based on suitable similarity models. Similarity models {{consist of a}} feature extraction step {{as well as a}} distance defined for these features, and they demand an efficient algorithm for retrieving similar objects under this model. In this work, we focus on the Earth Movers Distance (EMD), a recently introduced similarity model which has been successfully employed in numerous applications and has been reported as well reflecting human perceptual similarity. As its computation is complex, the direct application of the EMD to large, high-dimensional databases is not feasible. To remedy this and allow users to benefit from the high quality of the model even in larger settings, we developed various lower bounds for the EMD to be used in index-supported multistep query processing algorithms. We prove that our algorithms are complete, thus producing no <b>false</b> <b>drops.</b> We also show that it is highly efficient as experiments on large image databases with high-dimensional features demonstrate...|$|R
40|$|Encryption {{is a well}} {{established}} technology for protecting sensitive data. However, once encrypted, data {{can no longer be}} easily queried aside from exact matches. We present an order-preserving encryption scheme for numeric data that allows any comparison operation to be directly applied on encrypted data. Query results produced are sound (no false hits) and complete (no <b>false</b> <b>drops).</b> Our scheme handles updates gracefully and new values can be added without requiring changes in the encryption of other values. It allows standard database indexes to be built over encrypted tables and can easily be integrated with existing database systems. The proposed scheme has been designed to be deployed in application environments in which the intruder can get access to the encrypted database, but does not have prior domain information such as the distribution of values and cannot encrypt or decrypt arbitrary values of his choice. The encryption is robust against estimation of the true value in such environments. 1...|$|R
40|$|In this paper, {{we propose}} {{a variant of}} the {{signature}} file, called Bit-Sliced Bloom-Filtered Signature File (BBS), as the basis for implementing filter-and-refine strategies for mining frequent patterns. In the filtering step, the candidate patterns are obtained by scanning BBS instead of the database. The resultant candidate set contains a superset of the frequent patterns. In the refinement phase, each algorithm refines the candidate set to prune away the <b>false</b> <b>drops.</b> Based on this indexing structure, we study two filtering (single and dual filter) and two refinement (sequential scan and probe) mechanisms, thus giving rise to four different strategies. We conducted an extensive performance study to study the effectiveness of BBS, and compared the four proposed processing schemes with the traditional Apriori algorithm and the recently proposed FP-tree scheme. Our results show that BBS, as a whole, outperforms the Apriori strategy. Moreover, one of the schemes that is based on dual filter and probe refinement performs the best in all cases. 1...|$|R
40|$|This paper {{presents}} a new method for similarity retrieval of 3 D surface segments in spatial database systems {{as used in}} molecular biology, medical imaging, or CAD. We propose a similarity criterion and algorithm for 3 D surface segments {{which is based on}} the approximation of segments by using multi-parametric functions. The method can be adjusted to individual requirements of specific applications by choosing appropriate surface functions as approximation models. For an efficient evaluation of similarity queries, we developed a filter function which supports fast searching based on spatial index structures and guarantees no <b>false</b> <b>drops.</b> The evaluation of the filter function requires a new query type with multidimensional ellipsoids as query regions. We present an algorithm to efficiently perform ellipsoid queries on the class of spatial index structures that manage their directory by rectilinear hyperrectangles, such as R-trees or X-trees. Our experiments show both, effectiveness as well as efficiency of our method using a sample application from molecular biology...|$|R
40|$|Similarity {{search in}} large {{multimedia}} databases requires ef- ficient query processing based on suitable similarity models. Similarity models {{consist of a}} feature extraction step {{as well as a}} distance defined for these features, and they demand an efficient algorithm for retrieving similar objects under this model. In this work, we focus on the Earth Movers Distance (EMD), a recently introduced similarity model which has been successfully employed in numerous applications and has been reported as well reflecting human perceptual similarity. As its computation is complex, the direct application of the EMD to large, high-dimensional databases is not feasible. To remedy this and allow users to benefit from the high quality of the model even in larger settings, we developed various lower bounds for the EMD to be used in index-supported multistep query processing algorithms. We prove that our algorithms are complete, thus producing no <b>false</b> <b>drops.</b> We also show that it is highly efficient as experiments on large image databases with high-dimensional features demonstrate...|$|R
40|$|Topological spaces - such as {{classifying}} spaces, configuration {{spaces and}} spacetimes - often admit extra temporal structure. Qualitative invariants on such directed spaces often are more informative yet {{more difficult to}} calculate than classical homotopy invariants on underlying spaces because directed spaces rarely decompose as homotopy colimits of simpler directed spaces. Directed spaces often arise as geometric realizations of simplicial sets and cubical sets equipped with temporal structure encoding the orientations of simplices and 1 -cubes. In an attempt to develop calculational tools for directed homotopy theory, we prove appropriate simplicial and cubical approximation theorems. We consequently show that geometric realization induces an equivalence between weak homotopy diagram categories of cubical sets and directed spaces and that its right adjoint satisfies an excision theorem. Along the way, we give criteria for two different homotopy relations on directed maps in the literature to coincide. Comment: extensive changes. "Colimit" changed to "coproduct" in defn of compact C-streams, etc. "Cubical composition", cubical "weak equivalence", "stream embedding", category P in Thm 4. 1 redefined. Lem 8. 5 <b>false,</b> <b>dropped,</b> Prop 8. 17 <b>dropped</b> (pf <b>false),</b> Cor. 8. 18, 8. 19, 8. 34, Eg. 2. 7 <b>dropped</b> (<b>false</b> in general). Lemma 2. 8 weakened. Pfs of Lemmas 2. 8, 5. 7, Prop 6. 28, Cor 8. 32 corrected. 4 Figs added. 40 p...|$|R
40|$|Abstract. While sensor {{networks}} have recently {{emerged as a}} promising computing model, they are vulnerable to various node compromising attacks. In this paper, we propose COOL, a COmpromised nOde Locating protocol for detecting and locating compromised nodes once they misbehave in the sensor network. We exploit a proven collision-resilient incremental hashing algorithm and design secure steps to confidently locate compromised nodes. The scheme can also be combined with existing en-route false report filtering schemes to achieve both early <b>false</b> report <b>dropping</b> and accurate compromised nodes isolation. ...|$|R
40|$|Many {{strategies}} for similarity search in image databases assume a metric and quadratic form-based similarity model where an optimal lower bounding distance function exists for filtering. These strategies are mainly two-step, {{with the initial}} "filter " step based on a spatial or metric access method followed by a "refine " step employing expensive computation. Recent research on robust matching methods for computer vision has discovered that similarity models behind human visual judgment are inherently non-metric. When applying such models to similarity search in image databases, one has {{to address the problem}} of non-metric distance functions that might not have an optimal lower bound for filtering. Here, we propose a novel three-step "prune-filter-refine " strategy for approximate similarity search on these models. First, the "prune " step adopts a spatial access method to roughly eliminate improbable matches via an adjustable distance threshold. Second, the "filter " step uses a quasi lower-bounding distance derived from the non-metric distance function of the similarity model. Third, the "refine " stage compares the query with the remaining candidates by a robust matching method for final ranking. Experimental results confirmed that the proposed strategy achieves more filtering than a two-step approach with close to no <b>false</b> <b>drops</b> in the final result...|$|R
50|$|Haunted House {{has three}} playfields (a mini {{underground}} playfield, a main playfield, and an upper playfield.), eight flippers, at unique angles, four pop bumpers, two kick-out holes, a secret passage (a <b>false</b> target that <b>drops</b> down after impact to allow entrance to cellar.), a trap door that opens for ball, a lightning animation in the backglass and kicking bat targets.|$|R
5000|$|George Dupre - Canadian {{who claimed}} that {{he worked for the}} SOE and the French Resistance during World War II. Dupre served in World War II, but he was never in France or with the SOE. Was the subject of a best-selling book about his {{fabricated}} experiences. Confessed after being interviewed by a reporter who tricked him by <b>dropping</b> <b>false</b> names.|$|R
40|$|Similarity {{retrieval}} mechanisms should utilize generalized {{quadratic form}} distance functions {{as well as}} the Euclidean distance function since ellipsoid queries parameters may vary with the user and situation. In this paper, we present a spatial transformation technique that yields a new search method for adaptive ellipsoid queries. The technique is based on the notion of spatial transformation and e#ciently supports adaptive ellipsoid queries with quadratic form distance functions. Although conventional search methods can support ellipsoid queries by using multi-dimensional index structures, these methods incur high CPUcost for measuring distances between a query point and bounding rectangles with respect to quadratic form distance functions, which exceeds disk access cost on search processing. The basic idea is to transform the bounding rectangles in the original space, wherein distance from a query point is measured by quadratic form distance functions, into spatial objects in a new space wherein distance is measured by Euclidean distance functions. Since bounding rectangles in the original space are transformed into multi-dimensional polygons in the new space, and thus incurring high CPU-cost for distance calculations, our proposed method approximates a polygon by the rectangle that totally encloses the polygon, and measures the distance from the query point to the rectangle instead to the polygon in the Euclidean space. It follows that the spatial transformation technique guarantees no <b>false</b> <b>drops.</b> In contrast to the conventional methods, our proposed method significantly reduces CPU-cost due to the distance approximation by the spatial transformation; exact distance evaluations are avoided for most of the accessed bounding rectangles in the index structures. We als [...] ...|$|R
40|$|Data {{aggregation}} {{is important}} in wireless sensor networks. However, it also introduces many security problems, {{one of which is}} that a compromised node may inject <b>false</b> data or <b>drop</b> a message during data aggregation. Most existing solutions rely on encryption, which however requires high computation and communication cost. But they can only detect the occurrence of an attack without finding the attacking node. This makes sensor nodes waste their energy in sending false data if attacks occur repeatedly. Even an existing work can identify the location of a false data injection attack but it has a limitation that at most 50 % of total sensor nodes can participate in data transmission. Therefore, a novel approach is required such that it can identify an attacker and also increase the number of nodes which participate in data transmission. In this paper, we propose a monitoring-based secure data aggregation protocol to prevent against a compromised aggregator which injects <b>false</b> data or <b>drops</b> a message. The proposed protocol consists of aggregation tree construction an...|$|R
