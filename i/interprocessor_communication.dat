551|14|Public
25|$|As {{the number}} of {{processors}} increases, efficient <b>interprocessor</b> <b>communication</b> and synchronization on a supercomputer becomes a challenge. A number of approaches {{may be used to}} achieve this goal. For instance, in the early 1980s, in the Cray X-MP system, shared registers were used. In this approach, all processors had access to shared registers that did not move data back and forth but were only used for <b>interprocessor</b> <b>communication</b> and synchronization. However, inherent challenges in managing a large amount of shared memory among many processors resulted in a move to more distributed architectures.|$|E
5000|$|... {{processor}} communication {{object is}} used for <b>interprocessor</b> <b>communication</b> ...|$|E
5000|$|Lazy queue saves both on {{allocation}} {{and potentially}} on <b>interprocessor</b> <b>communication</b> by avoiding allocate or enqueue with an optimistic fast-path; ...|$|E
40|$|Abstract. In {{this paper}} we propose and analyze a new {{parallel}} SOR method, the PSOR method, formulated by using domain partitioning and <b>interprocessor</b> data <b>communication</b> techniques. We prove that the PSOR method has the same asymptotic rate of convergence as the Red/Black (R/B) SOR method for the five-point stencil on both strip and block partitions, and as the four-color (R/B/G/O) SOR method for the nine-point stencil on strip partitions. We also demonstrate the parallel performance of the PSOR method on four different MIMD multiprocessors (a KSR 1, an Intel Delta, a Paragon, and an IBM SP 2). Finally, we compare the parallel performance of PSOR, R/B SOR, and R/B/G/O SOR. Numerical results on the Paragon indicate that PSOR is more efficient than R/B SOR and R/B/G/O SOR in both computation and <b>interprocessor</b> data <b>communication...</b>|$|R
40|$|In this paper, {{we propose}} and analyze a new {{parallel}} SOR method, the PSOR method, formulated by using domain partitioning and <b>interprocessor</b> data <b>communication</b> techniques. We {{prove that the}} PSOR method has the same asymptotic rate of convergence as the Red/Black (R/B) SOR method for the 5 -point stencil on both strip and block partitions, and as the four-color (R/B/G/O) SOR method for the 9 -point stencil on strip partitions. We also demonstrate the parallel performance of the PSOR method on four different MIMD multiprocessors (a KSR 1, the Intel Delta, a Paragon and an IBM SP 2). Finally, we compare the parallel performance of PSOR, R/B SOR and R/B/G/O SOR. Numerical results on the Paragon indicate that PSOR is more efficient than R/B SOR and R/B/G/O SOR in both computation and <b>interprocessor</b> data <b>communication.</b> Key words. parallel computing, SOR, multicolor SOR, JSOR, PSOR, convergence analysis, nonmigratory permutation AMS subject classifications. Primary 65 Y 05; Secondary 65 F 10. 1. [...] ...|$|R
40|$|Abstract [...] Testing digital {{controllers}} {{for power}} electronic systems in real-time using a real-time digital simulator presents a problem, {{due to the}} discrete nature of the controller outputs which are not necessarily in synchronism with the time-step of the simulator. Reducing the simulation time-step resolves the issues resulting {{from the lack of}} synchronism between the digital simulator and the hardware controller under test. The simulation time can be reduced by distributing the computational burden among a number of parallel processors. However, the reduction in time is not directly proportional to the number of processors due to the <b>interprocessors</b> <b>communication</b> overhead. However, the simulation time can be greatly reduced if the simulator's hardware is custom configured. This paper is an effort to introduce and develop a reconfigurable hardware simulator using FPGAs to simulate power systems that contains power electronic switching elements. The simulation time-step for this simulator is less than 100 ns. A new time domain model for power electronic switching elements is also presented in this paper. The new model is very convenient for real-time simulation purposes due to its simplicity and the elimination of numerical oscillations associated with other models...|$|R
50|$|As {{the number}} of {{processors}} increases, efficient <b>interprocessor</b> <b>communication</b> and synchronization on a supercomputer becomes a challenge. A number of approaches {{may be used to}} achieve this goal. For instance, in the early 1980s, in the Cray X-MP system, shared registers were used. In this approach, all processors had access to shared registers that did not move data back and forth but were only used for <b>interprocessor</b> <b>communication</b> and synchronization. However, inherent challenges in managing a large amount of shared memory among many processors resulted in a move to more distributed architectures.|$|E
50|$|Experiments {{performed}} in the early 1990s on Connection Machine supercomputers showed samplesort to be particularly good at sorting large datasets on these machines, because its incurs little <b>interprocessor</b> <b>communication</b> overhead. On latter-day GPUs, the algorithm may be less effective than its alternatives.|$|E
50|$|The ScaLAPACK (or Scalable LAPACK) library {{includes}} {{a subset of}} LAPACK routines redesigned for distributed memory MIMD parallel computers. It is currently written in a Single-Program-Multiple-Data style using explicit message passing for <b>interprocessor</b> <b>communication.</b> It assumes matrices are {{laid out in a}} two-dimensional block cyclic decomposition.|$|E
40|$|HEDRA, an ESPRIT project (nr. 6768), aims {{to develop}} a {{heterogeneous}} distributed real-time architecture for robot and machine control. This paper describes an open and flexible programming system, {{as a part of}} this architecture, that is based on an existing system called Virtuoso. The programming system is an evolution of Virtuoso towards the architecture at which the project is aiming. The work concentrates on the achievement of guaranteed real-time behaviour, minimum interrupt latency and transparency of <b>interprocessor</b> data <b>communication.</b> status: publishe...|$|R
40|$|In this paper, we outline {{a unified}} {{approach}} {{for building a}} library of collective communication operations that performs well on a cross-section of problems encountered in real applications. The target architecture is a two-dimensional mesh with worm-hole routing, but the techniques also apply to higher dimensional meshes and hypercubes. We stress a general approach, addressing the need for implementations that perform well for various sized vectors and grid dimensions, including non-power-of-two grids. This requires the development of general techniques for building hybrid algorithms. Finally, our approach also supports collective communication within a group of nodes, which is required by many scalable algorithms. Results from the Intel Paragon system are included. 1 Introduction The <b>Interprocessor</b> Collective <b>Communication</b> (InterCom) Project is a comprehensive study of tech- Copyright c fl 1994 by the Institute of Electrical and Electronics Engineers, Inc. Reprinted with the permiss [...] ...|$|R
40|$|We {{study the}} {{parallel}} implementations of a block iterative method in heterogeneous computing environments for solving linear systems of equations. The method is a generalization of the row-projection Cimmino method where blocks are obtained by partitioning the original linear system of equations. The method {{is referred to}} as the Block Cimmino method. In addition, we accelerate the Block Cimmino convergence rate by the Block Conjugate Gradient method (Block-CG). Firstly, we present three different implementations of the Block-CG to compare different parallelization strategies of the method. Secondly, we present a scheduler to balance the computational load of the parallel distributed implementation of the Block Cimmino iteration. The computational resources used in these experiments are a BBN TC 2000, and a network of Sun Sparc 10 and IBM RS- 6000 workstations. We use PVM 3. 3 to handle the <b>interprocessor</b> heterogeneous <b>communication.</b> KEYWORDS. Block iterative methods, Cimmino method, co [...] ...|$|R
5000|$|The {{architecture}} of the network interface has been developed to offload the entire task of <b>interprocessor</b> <b>communication</b> from the main processor, and to avoid the overhead of system calls for user process to user process messaging. QsNetII is designed for use within SMP systems [...] - [...] multiple, concurrent processes can utilise the network interface without any task switching overhead. A I/O processor offloads protocol handling from the main CPU. Local memory on the PCI card provides storage for buffers, translation tables and I/O adapter code. All the PCI bandwidth is available to data communication.|$|E
5000|$|Regarding <b>interprocessor</b> <b>communication</b> in a {{multicore}} setup, {{there are}} {{similarities between the}} Cell's inter-localstore DMA and a shared L2 cache setup as in the Intel Core 2 Duo or the Xbox 360's custom powerPC: the L2 cache allows processors to share results without those results having {{to be committed to}} main memory. This can be an advantage where the working set for an algorithm encompasses the entirety of the L2 cache. However, when a program is written to take advantage of inter-localstore DMA, the Cell has the benefit of each-other-Local-Store serving the purpose of BOTH the private workspace for a single processor AND the point of sharing between processors; i.e., the other Local Stores are on a similar footing viewed from one processor as the shared L2 cache in a conventional chip. The tradeoff is that of memory wasted in buffering and programming complexity for synchronization, though this would be similar to precached pages in a conventional chip. Domains where using this capability is effective include: ...|$|E
40|$|This paper {{addresses}} {{the problem of}} communication-free partitioning of iteration spaces and data spaces along hyperplanes. We consider statement-level partitioning for the iteration spaces. The technique explicitly formulates array references as transformations from statementiteration spaces to data spaces. Based on these transformations, the necessary and sufficient conditions for the feasibility of communication-free hyperplane partitions are presented. 1 Introduction It has been widely accepted that local memory access is much faster than memory access involving <b>interprocessor</b> <b>communication</b> on distributed-memory multicomputers. If data and computation are not properly distributed across processors, it may cause heavy <b>interprocessor</b> <b>communication.</b> Excessive <b>interprocessor</b> <b>communication</b> will offset the benefit of parallelization even if the program has {{a large amount of}} parallelism. Consequently, parallelizing compilers must pay more attention on the distribution of computation and [...] ...|$|E
40|$|Instances where {{asynchronous}} parallel computations can be {{used for}} the solution of fluid flow problems are discussed. The use of asynchronous portions of solvers can effectively counteract the consequences of the inefficiencies associated with high latency in the <b>interprocessor</b> (or intercomputer) <b>communication.</b> This is especially so in the case of irregular regions, irregular partitions, or in clusters of inhomogeneous computers. Another situation where asynchronous methods can yield important gains is in the cases of moving boundaries, or adaptive grids. Asynchronous methods can be seen as an alternative to load balancing...|$|R
40|$|By {{combining}} {{a number}} of embedded digital signal processors (DSPs) within a desktop PC, we have developed a system that offers not only the mass storage, network utilities, user interface and presentation graphics of a regular PC, but also the real-time response rates normally associated only with embedded systems. In this paper we outline the design of such {{a system that has}} been built to demonstrate a real-time 3 D laser range sensor. The prototype range sensor consists of a custom-built auto-synchronized 3 D laser scanner head that is directly interfaced to off-the-shelf computing hardware. The hardware includes {{a number of}} PCI bus DSP cards that communicate using dedicated high-speed <b>interprocessor</b> links. <b>Communication</b> between the desktop PC and the embedded DSPs uses the PCI bus. This processing power will be required to achieve real-time data acquisition and 3 D geometrical tracking capabilities. This paper outlines the prototype 3 D laser range sensor and describes its computing architecture. The embedded DSPs run under a commercial multiprocessor real-time operating system. This combination leads to a highly modular system in which processors may be added or removed with minimal side effects...|$|R
40|$|Instances where {{asynchronous}} parallel computations can be {{used for}} the solution of uid ow problems are discussed. The use of asynchronous portions of solvers can eectively counteract the consequences of the ineciencies associated with high latency in the <b>interprocessor</b> (or intercomputer) <b>communication.</b> This is especially so in the case of irregular regions, irregular partitions, or in clusters of inhomogeneous computers. Another situation where asynchronous methods can yield important gains is in the cases of moving boundaries, or adaptive grids. Asynchronous methods can be seen as an alternative to load balancing. Keywords: Asynchronous communication, parallel computing, numerical solutions, uid dynamics. 1 Introduction In a perfect parallelizable world all computational problems would be divided in tasks of approximately equal diculty, achieving perfect load balancing. At the same time, communication times between processors of a MIMD machine would be negligible. The same would [...] ...|$|R
40|$|<b>Interprocessor</b> <b>communication</b> {{is one of}} the {{ultimate}} factors affecting the performance of parallel DBMSs for massively parallel computer systems. This paper presents a hierarchical architecture of parallel DBMS Omega which allows to manage the problem of <b>interprocessor</b> <b>communication.</b> The paper describes a message system designed for DBMS Omega. This message system consists of two self-contained subsystems: Omega-Conductor and Omega-Router. The Omega system is a prototype of the parallel database machine elaborated for MVS- 100 massively parallel computer system...|$|E
40|$|This paper {{addresses}} {{the problem of}} partitioning data for distributed memory machines (multicomputers). In current day multicomputers, <b>interprocessor</b> <b>communication</b> is more time-consuming than instruction execution. If insufficient {{attention is paid to}} the data allocation problem, then {{the amount of time spent}} in <b>interprocessor</b> <b>communication</b> might be so high as to seriously undermine the benefits of parallelism. It is therefore worthwhile for a compiler to analyze patterns of data usage to determine allocation, in order to minimize <b>interprocessor</b> <b>communication.</b> We present a machineindependent analysis of communication-free partitions. We present a matrix notation to describe array accesses in fully parallel loops which lets us derive sufficient conditions for communication-free partitioning (decomposition) of arrays. In the case of a commonly occurring class of accesses, we present a problem formulation to minimize communication costs, when communication-free partitioning of arrays is n [...] ...|$|E
40|$|Efficient <b>interprocessor</b> <b>communication</b> {{is a key}} {{parameter}} {{to achieve}} high performance in distributed memory multicomputer networks. Thus. modelling and analysing message communication latency is critical for optimizing the performance of interprocessor communications. To demonstrate how to achieve efficient (low latency) <b>interprocessor</b> <b>communication.</b> we have developed generalized stochastic Petri net (GSPN) routing models for congestion-prone communication patterns occurring in multicomputer networks. Under congestion-prone adjacent and non-adjacent node communication patterns, we use our developed GSPN routing models to {{study the effects of}} congestion (arising due to problems such as buffer overflow and traffic contention) on the message communication latency of multicomputer networks. This modelling has been carried out for mesh multicomputer networks employing the packet switching technique...|$|E
40|$|In {{this paper}} we generalize the {{framework}} of linear loop transformations: we consider loop alignment as a new component in the transformation process. The aim is to exploit the additional inherent statement-level parallelism and {{reduce the amount of}} <b>interprocessor</b> synchronization and <b>communication</b> when a coarse-grain MIMD execution model is considered. The transformation process is modelled with non-singular matrices and we use the ideas recently proposed in this field to generate an efficient transformed code. However, additional aspects have to be studied when statements are considered in the process. We try to reduce the overhead due to conditionals that appear in the loop body of the transformed loops. KEYWORDS Loop transformations, Parallelizing compilers, Statement alignment, Unimodular and non-singular matrices. 1. Introduction Loop transformations have been recognized {{to be one of the}} most important components of the parallelizing and vectorizing technology for current super [...] ...|$|R
40|$|Abstract: The {{successive}} over-relaxation (SOR) how {{to quickly}} verify and generate a iterative method {{is an important}} solver for linear multicoloring ordering according to the given systems. In this paper, a parallel algorithm for {{the structure of a}} matrix or a grid. However, the red-black SOR method with domain decomposition is multi-color SOR method is parallel only within investigated. The parallel SOR algorithm is designed the same color. For some problems such as two by combining the traditional red-black SOR and row block domain decomposition technique, which reduces the communication cost and simplifies the parallel Poisson equations, the Red-Black two-color SOR implementation. Two other iterative methods, Jacobi method is preferred. Yanheh [4] showed that the and Gauss-Seidel(G-S), are also implemented in Red-Black SOR method is more efficient and parallel for comparison. The three parallel iterative smoother than the sequential SOR method. Xie algorithm are implemented in C and MPI (Message proposed an efficient parallel SOR method Passing Interface) for solving the Dirichlet problem (PSOR) using domain decomposition and on a Linux cluster with eight dual processor 2. 6 ghz 32 <b>interprocessor</b> data <b>communication</b> techniques bit Intel Xeons, totaling 16 processors. The [5]. It is shown that PSOR is just the SOR performances of the three algorithms are evaluated in method applied to a reordered linear system, soterms of speedup and efficiency. that the theory of SOR can also be applied to th...|$|R
40|$|In this article, a {{parallel}} algorithm which applies Givens rotations to selectively annihilate k(k + 1) / 2 nonzero elements from two k x n(k {{not more than}} n) upper trapezoidal submatrices is described. The new algorithm is suitable for implementation on either a pair of directly connected local-memory processors or two clusters of multiple tightly-coupled processors. Analyses show that {{in both cases the}} proposed algorithms achieve optimal speed-up by balancing the work load distribution and masking <b>interprocessor</b> or intercluster <b>communication</b> by computation if k is much small than n. In the context of solving large scale least squares problems, this submatrix merging step is repetitively needed during the entire computation and, furthermore, there are usually many pairs of such submatrices to be merged with each submatrix stored in the memory of a processor or a cluster of processors. The proposed algorithm can be applied to each pair of submatrices concurrently, and thus parallelizes an important step in solving the least squares problems...|$|R
30|$|Basically, {{in order}} to reduce the interprocessor communications, our {{algorithm}} exploits the fact that the butterfly selections follow a pattern when computing the multipliers in the FFT. Limiting the <b>interprocessor</b> <b>communication</b> is possible by grouping 4 butterfly calculations into a single thread. If the correct butterflies are chosen, within each thread 3 stages of calculation can be implemented without <b>interprocessor</b> <b>communication.</b> Given that for a radix- 2, 512 -point FFT there are 9 stages, using this strategy, shared memory need only be accessed at two points in the entire calculation (see Figure 10). All other accesses within the calculation are to the internal thread registers, which have inherent speed enhancements as these are the fastest accesses. The details of the algorithms are given in the following subsection.|$|E
40|$|We {{present a}} method for {{compressing}} binary images via monochromatic pattern substitution. Such method has no relevant loss of compression effectiveness if the image is partitioned into up to a thousand blocks, approximately, and each block is compressed independently. Therefore, it can be implemented on a distributed system with no <b>interprocessor</b> <b>communication.</b> In the theoretical context of unbounded parallelism, <b>interprocessor</b> <b>communication</b> is needed. Compression effectiveness has a bell-shaped behaviour which is again competitive with the sequential performance when the highest degree of parallelism is reached. Finally, the method has a speed-up if applied sequentially to an image partitioned into up to 256 blocks. It follows that such speed-up {{can be applied to}} a parallel implementation on a small scale system. © 2013 Springer Basel...|$|E
40|$|Abstract: In this paper, {{we present}} an interprocessor communication-aware task {{scheduling}} algorithm applicable to a multiprocessor system executing an application with dependent tasks. Our algorithm takes the application task graph and the architecture graph as inputs, assigns the tasks to processors and then schedules them. As main theoretical contribution, the algorithm we propose reduces the overall systems energy by (i) reducing the total <b>interprocessor</b> <b>communication</b> and (ii) executing certain cycles {{at a lower}} voltage level. Experimental results show that by tuning the parameter for communication awareness, a schedule using our algorithm can reduce upto 80 % <b>interprocessor</b> <b>communication</b> in a complex video/audio application (compared to a schedule which is only voltage-selection aware) without losing much {{in the number of}} cycles executed at lower voltage...|$|E
40|$|Abstract. As the {{development}} of the PSOR method (a point parallel SOR method by mesh domain partitioning proposed in [SIAM J. Sci. Comput., 20 (1999), pp. 2261 – 2281], this paper introduces a new mesh domain partition and ordering (the multitype partition and ordering), and proposes a new block parallel SOR (BPSOR) method for numerically solving 2 -dimensional (2 D) or three-dimensional (3 D) elliptic boundary problems. A general mathematical analysis shows that the BPSOR method can have the same asymptotic convergence rate as the corresponding sequential block SOR method if the coefficient matrix of the block linear system is “consistently ordered. ” It also shows that the original sequential ordering can be maintained in the parallel implementation of the BPSOR method so that the BPSOR method can be effectively applied to solve complex elliptic boundary problems. Furthermore, three particular multitype orderings are proposed based on strip and block mesh partitions, which lead to three effective BPSOR methods for solving the five-point like linear systems (in two dimensions) and the seven-point like linear systems (in three dimensions). In addition, it is shown that the PSOR method can be generated from BPSOR if each block equation is solved approximately by only one point SOR iteration. Thus, the PSOR method is well defined without involving any <b>interprocessor</b> data <b>communication</b> operations, and extended to solving three-dimensional (3 D) problems. Finally, numerical results are presented which confirm the theoretical results and show that BPSOR has a good parallel performance on a parallel MIMD computer...|$|R
40|$|For {{neural network}} {{simulations}} on parallel machines, <b>interprocessor</b> spike <b>communication</b> {{can be a}} significant portion of the total simulation time. The performance of several spike exchange methods using a Blue Gene/P (BG/P) supercomputer has been tested with 8 – 128 K cores using randomly connected networks of up to 32 M cells with 1 k connections per cell and 4 M cells with 10 k connections per cell, i. e., on the order of 4 · 1010 connections (K is 1024, M is 10242, and k is 1000). The spike exchange methods used are the standard Message Passing Interface (MPI) collective, MPI_Allgather, and several variants of the non-blocking Multisend method either implemented via non-blocking MPI_Isend, or exploiting the possibility of very low overhead direct memory access (DMA) communication available on the BG/P. In all cases, the worst performing method was that using MPI_Isend due to the high overhead of initiating a spike communication. The two best performing methods—the persistent Multisend method using the Record-Replay feature of the Deep Computing Messaging Framework DCMF_Multicast; and a two-phase multisend in which a DCMF_Multicast is used to first send to a subset of phase one destination cores, which then pass it on to their subset of phase two destination cores—had similar performance with very low overhead for the initiation of spike communication. Departure from ideal scaling for the Multisend methods is almost completely due to load imbalance caused by the large variation in number of cells that fire on each processor in the interval between synchronization. Spike exchange time itself is negligible since transmission overlaps with computation and is handled by a DMA controller. We conclude that ideal performance scaling will be ultimately limited by imbalance between incoming processor spikes between synchronization intervals. Thus, counterintuitively, maximization of load balance requires that the distribution of cells on processors should not reflect neural net architecture but be randomly distributed so that sets of cells which are burst firing together should be on different processors with their targets on as large a set of processors as possible...|$|R
40|$|Pipelines {{that operate}} on buffers often work well to {{mitigate}} the high latency inherent in <b>interprocessor</b> <b>communication</b> and in accessing data on disk. Running a single pipeline on each node works well when each pipeline stage consumes and produces data at the same rate. If a stage might consume data faster or slower than it produces data, a single pipeline becomes unwieldy. We describe how we have extended the FG programming environment to support multiple pipelines in two forms. When a node might send and receive data at different rates during <b>interprocessor</b> <b>communication,</b> we use disjoint pipelines that send and receive on each node. When a node consumes and produces data from different streams on the node, we use multiple pipelines that intersect at a particular stage. Experimental results for two out-of-core sorting algorithms—one based on columnsort {{and the other a}} distribution-based sort—demonstrate the value of multiple pipelines. The FG programming environment [3, 4, 5, 6, 7, 8] uses pipelines {{to mitigate the}} high latency inherent in <b>interprocessor</b> <b>communication</b> and in accessing the outer levels of the memory hierarchy. For example, in out-of-core programs, the dataset is so large that it exceeds the size of the main memory, and so it resides on one or more disks. Severa...|$|E
40|$|<b>Interprocessor</b> <b>communication</b> is a {{vital part}} of any {{multiprocessor}} system. This work focuses on integration of an asynchronous message passing mechanism and a message notification support in the form of limited hardware queues. To fulfill requirements the <b>interprocessor</b> <b>communication</b> must be predictable, efficient, maintain memory integrity, and use the semantics of the available message passing mechanism. Various solution possibilities are identified, evaluated and compared, resulting in a design recommendation. The design uses memory restriction to build a firewall between the processors, using pointers to avoid message copying. The message queue {{is in the form of}} an array ring that can piggyback acknowledgement information. The design is general and applicable by a real-time operating systems using asynchronous message passing with explicit buffering, and has hardware support in the form of limited interrupt generating queues...|$|E
40|$|It is {{well-known}} that most scheduling problems arising from parallel systems are NP-hard, even under very special assumptions. Thus various suboptimal algorithms, in particular heuristics, were {{proposed in the}} literature. Worst-case error bounds are established in this note for heuristics of makespan minimization of parallel computations. Different parallel computation models are investigated, including <b>interprocessor</b> <b>communication,</b> task duplication, multiprocessor tasks and parallel tasks. Due to the heterogeneity of these systems, scheduling heuristics can be {{far away from the}} optimal solutions. The bounds presented here provide insights to the design of scheduling heuristics in order to obtain good performance guarantee. Key-words: heuristics for multiprocessor scheduling, makespan, schedule length, worst-case error bound, parallel computation, <b>interprocessor</b> <b>communication,</b> task duplication, multiprocessor task, parallel task. (R'esum'e : tsvp) Correspondence: Zhen LIU, INRIA C [...] ...|$|E
40|$|Advances in {{programming}} {{languages and}} parallelizing compilers are making parallel computers {{easier to use}} by providing a high-level portable programming model that protects software investment. However, experience has shown that simply finding parallelism is not always sufficient for obtaining good performance from today's multiprocessors, largely because the cost of <b>interprocessor</b> <b>communication</b> {{is much greater than}} computation or local memory accesses. To overcome this problem, I believe compilers need to perform communication analysis to locate and optimize <b>interprocessor</b> <b>communication.</b> I show how communication analysis has been used to improve performance for both shared and distributed memory machines, and describe a new project to apply these techniques to compilers for software distributed-shared-memory (DSM) systems. 1 Introduction High-performance computing is rapidly becoming an important component of scientific research and development. Today's scientists and engineers depe [...] ...|$|E
40|$|Applications with {{irregular}} data {{structures such as}} sparse matrices or {{finite element}} meshes account for a large fraction of engineering and scientific applications. Domain decomposition techniques are commonly used to partition these applications to reduce <b>interprocessor</b> <b>communication</b> on message passing parallel systems. Our work investigates the use of domain decomposition techniques on cache-coherent parallel systems. Many good domain decomposition algorithms are now available. We show that further application improvements are attainable using data and program restructuring in conjunction with domain decomposition. We give techniques for data layout to reduce communication, blocking with subdomains to improve uniprocessor cache behavior, and insertion of prefetches to hide the latency of <b>interprocessor</b> <b>communication.</b> This paper details our restructuring techniques and provides experimental results on the KSR 1 multiprocessor for a sparse matrix application. The experimental results [...] ...|$|E
40|$|ABSTRACT We {{show the}} NP-Completeness of two {{processor}} scheduling with tasks of execution time 1 or 2 units and unit <b>interprocessor</b> <b>communication</b> latency. We develop {{a model of}} scheduling {{in the presence of}} communication contention, and show the NP-Completeness of two processor scheduling with unit execution time tasks in our model...|$|E
40|$|An {{analysis}} is presented of several factors influencing {{the performance of}} a parallel implementation of the UCLA atmospheric general circulation model (AGCM) on massively parallel computer systems. Several modificaitons to the original parallel AGCM code aimed at improving its numerical efficiency, <b>interprocessor</b> <b>communication</b> cost, load-balance and issues affecting single-node code performance are discussed...|$|E
