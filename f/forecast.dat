10000|10000|Public
5|$|On May 19, 2003, NOAA {{published}} its <b>forecast</b> for the Central Pacific hurricane season. The <b>forecast</b> {{called for}} a slightly below-average level of activity in 2003, due to the same <b>forecast</b> of the onset of La Niña which would later prompt the low <b>forecast</b> for the Eastern Pacific.|$|E
5|$|After {{the season}} began, Dr. Gray {{announced}} he had revised his predictions slightly downwards on August6, citing mild El Niño conditions. His new <b>forecast</b> was thirteen named storms, seven hurricanes, and three reaching major hurricane intensity. On August10, NOAA released an updated prediction as well, with a 90% probability of above-to-near normal activity, {{but the same}} number of storms <b>forecast.</b> CSU issued another <b>forecast</b> on September3, indicating sixteen tropical storms, eight hurricanes, and five major hurricanes. The season ended up with sixteen tropical depressions, fifteen named storms, nine hurricanes, and six major hurricanes, which matched CSU's final prediction on October1.|$|E
5|$|CSU began issuing outlooks in December1997 and {{initially}} predicted 9named storms, 5hurricanes, and 2major hurricanes would {{occur in the}} upcoming season. Later, in April1998, CSU released a <b>forecast</b> calling for 10named storms, 6hurricanes, and 2major hurricane. The predictions by CSU in June and August1998 remained {{the same as the}} <b>forecast</b> in April. Additionally, forecasters at CSU predicted that the El Niño event that began in 1997 would dissipate either before or shortly after the 1998 season began. The WRC predicted 8named storms and 5hurricanes in early 1998, but did not include a <b>forecast</b> for the number of major hurricanes.|$|E
40|$|The grey system <b>forecasting</b> model, {{neural network}} <b>forecasting</b> model and {{support vector machine}} <b>forecasting</b> model are {{proposed}} in this paper. Taking the road goods traffic volume from year of 1996 to 2003 {{in the whole country}} as a study case, the <b>forecasting</b> results are got by three methods. From the <b>forecasting</b> results, we can conclude that the accuracy of the support vector machine <b>forecasting</b> method is higher. Analyzing the characteristic of combining <b>forecasting</b> method, based on grey system <b>forecasting</b> model, neural network <b>forecasting</b> model and support vector machine <b>forecasting</b> model, the linear combining <b>forecasting</b> model, neural network combining <b>forecasting</b> model and support vector machine combining <b>forecasting</b> model are set up. Compared with single prediction methods, linear combining <b>forecasting</b> method and neural network combining <b>forecasting</b> model, the accuracy of the support vector machine combining <b>forecasting</b> method is higher. </p...|$|R
50|$|Foresight publishes peer-reviewed {{articles}} {{written by}} academics and business forecasters on topics including design {{and management of}} <b>forecasting</b> processes, <b>forecasting</b> models, integrating <b>forecasting</b> into business planning, sales <b>forecasting,</b> prediction markets, reviews of <b>forecasting</b> tools and books, sales and operations planning (S&OP), improving <b>forecasting</b> accuracy, and demand <b>forecasting.</b>|$|R
40|$|Abstract—In {{view of the}} {{characteristic}} that the traffic system is a dynamic and time-varying parameter system, the multi-level recursive <b>forecasting</b> method is proposed, and the multi-level recursive <b>forecasting</b> model of road accidents is established in this thesis. In this method, the <b>forecasting</b> of road accidents {{is divided into two}} parts: the <b>forecasting</b> of time-varying parameters and the future <b>forecasting</b> of road accidents based on the <b>forecasting</b> of time-varying parameters. By the precise <b>forecasting</b> of time-varying parameter, <b>forecasting</b> of road accidents is carried out. <b>Forecasting</b> results indicate the multi-level recursive <b>forecasting</b> model is suitable for the <b>forecasting</b> of road accidents and can improve the <b>forecasting</b> precision. Keywords—road traffic, accident, multi-level recursive, time-varying parameter, forecastin...|$|R
5|$|CSU {{issued a}} more {{detailed}} update on April7, upping their <b>forecast</b> to 15 named storms, 8 hurricanes, 4 major hurricanes, and an ACE index of 150. More confidence in the weakening of El Niño (and thus lower vertical wind shear) and the continuation of anomalously warm sea surface temperatures across the Atlantic Ocean were cited. Two days later, TSR revised their <b>forecast</b> upward to 16.3 named storms, 8.5 hurricanes, 4 major hurricanes, and an ACE index of 159 units. In their April21 update, WSI predicted the most active year since the record-breaking 2005 season, with 16 named storms, 9 hurricanes, and 5 major hurricanes. Five days later, North Carolina State University (NCSU) released their only <b>forecast</b> for the season, with 15 to 18 named storms and 8 to 11 hurricanes. TSR largely maintained their <b>forecast</b> for their May25 release. The next day, TWC/WSI again raised their <b>forecast</b> to 18 named storms, 10 hurricanes, and 5 major hurricanes. The National Oceanic and Atmospheric Administration (NOAA), meanwhile, released their <b>forecast</b> prediction for the 2010 season on May27. The organization called for an active to extremely active year including 14 to 23 named storms, 8 to 14 hurricanes, and 3 to 7 major hurricanes, referencing {{a continuation of the}} active era that began in 1995.|$|E
5|$|CSU updated their <b>forecast</b> on June 1 {{to include}} 14 named storms, 6 hurricanes and 2 major hurricanes to include Tropical Storm Bonnie. It was again updated on July 1 to include 15 named storms, 6 hurricanes and 2 major hurricanes, to {{accommodate}} for tropical storms Colin and Danielle. On July 5, TSR released their fourth <b>forecast</b> for the season, slightly lowering the predicted numbers to 16 tropical storms, 8 hurricanes and 3 major hurricanes. On August 5, TSR released their final <b>forecast</b> for the season, lowering {{the numbers to}} 15 named storms and 7 hurricanes due to the influence that La Niña being less than anticipated previously. NOAA updated their <b>forecast</b> on August11, increasing the their predictions to 12–17 named storms, 5–8 hurricanes, and 2–4 major hurricanes.|$|E
5|$|Day 4–8 outlooks are the longest-term {{official}} SPC <b>Forecast</b> Product, {{and often}} change significantly {{from day to}} day. This extended <b>forecast</b> for severe weather was an experimental product until March 22, 2007, when the Storm Prediction Center incorporated it as an official product. Areas are delineated in this <b>forecast</b> that have least a 15% or 30% chance of severe weather in the Day 4–8 period (equivalent to a slight risk and an enhanced risk, respectively); as forecaster confidence is not fully resolute on how severe weather will evolve more than three days out, the Day 4–8 outlook only outlines {{the areas in which}} severe thunderstorms are <b>forecast</b> to occur during the period at the 15% and 30% likelihood, and does not utilize other categorical risk areas or outline where general (non-severe) thunderstorm activity will occur.|$|E
40|$|This paper {{investigates the}} <b>forecasting</b> {{performance}} of the diffusion index approach for the Australian economy, and considers the <b>forecasting</b> {{performance of the}} diffusion index approach relative to composite <b>forecasts.</b> Weighted and unweighted factor <b>forecasts</b> are benchmarked against composite <b>forecasts,</b> and <b>forecasts</b> derived from individual <b>forecasting</b> models. The results suggest that diffusion index <b>forecasts</b> tend to improve on the benchmark AR <b>forecasts.</b> We also observe that weighted factors tend to produce better <b>forecasts</b> than their unweighted counterparts. We find, however, {{that the size of}} the <b>forecasting</b> improvement is less marked than previous research, with the diffusion index <b>forecasts</b> typically producing mean square errors of a similar magnitude to the VAR and BVAR approaches. Copyright 2010 The Authors. Journal compilation 2010 Blackwell Publishing Ltd/University of Adelaide and Flinders University. ...|$|R
40|$|This chapter {{provides}} an overview of the recent developments in tourism demand modelling and <b>forecasting</b> since the 1990 s. While a wide range of <b>forecasting</b> models is available for tourism demand <b>forecasting,</b> tourism managers should use models that are based on solid economic theories and provide reliable <b>forecasts.</b> In addition, this chapter suggests that in addition to <b>forecasting</b> error magnitude, directional change errors and turning point <b>forecasting</b> should be studied. Moreover, since no single model consistently generates superior <b>forecasts</b> across all situations, combining the <b>forecasts</b> generated from different <b>forecasting</b> methods improves tourism demand <b>forecasting</b> accuracy. School of Hotel and Tourism Managemen...|$|R
50|$|Tropical cyclone <b>forecasting</b> is {{the science}} of <b>forecasting</b> where a {{tropical}} cyclone's center, and its effects, {{are expected to be}} {{at some point in the}} future. There are several elements to tropical cyclone forecasting: track <b>forecasting,</b> intensity <b>forecasting,</b> rainfall <b>forecasting,</b> storm surge, tornado, and seasonal <b>forecasting.</b> While skill is increasing in regard to track <b>forecasting,</b> intensity <b>forecasting</b> skill remains nearly unchanged over the past several years. Seasonal <b>forecasting</b> began in the 1980s in the Atlantic basin and has spread into other basins in the years since.|$|R
5|$|In the past, {{the human}} {{forecaster}} {{was responsible for}} generating the entire weather <b>forecast</b> based upon available observations. Today, human input is generally confined to choosing a model based on various parameters, such as model biases and performance. Using a consensus of <b>forecast</b> models, as well as ensemble members of the various models, can help reduce <b>forecast</b> error. However, regardless how small the average error becomes with any individual system, large errors within any particular piece of guidance are still possible on any given model run. Humans are required to interpret the model data into weather forecasts that are understandable to the end user. Humans can use knowledge of local effects that may be too small in size to be resolved by the model to add information to the <b>forecast.</b> While increasing accuracy of <b>forecast</b> models implies that humans {{may no longer be}} needed in the <b>forecast</b> process {{at some point in the}} future, there is currently still a need for human intervention.|$|E
5|$|Meanwhile, on April8, Weather Services International (WSI) {{issued its}} first <b>forecast</b> for the {{hurricane}} season. In its report, the organization <b>forecast</b> 16named storms, nine hurricanes, and five major hurricanes.|$|E
5|$|On December 3, 2004, Gray's team {{issued its}} first extended-range <b>forecast</b> for the 2005 season, {{predicting}} a slightly above-average season. Additionally, the team predicted a greatly increased {{chance of a}} major hurricane striking the East Coast of the United States and the Florida peninsula. Though the <b>forecast</b> predicted above-average activity, the level predicted was significantly less than the 2004 season. On April 1, 2005, after confirming that El Niño conditions would not develop, Gray and his team revised the December <b>forecast</b> upward, expecting 13 tropical storms instead of 11 and seven hurricanes instead of six. In addition, {{the chance of a}} storm impacting the United States was raised slightly.|$|E
40|$|A general <b>forecasting</b> {{correcting}} formula, as {{a framework}} for long-use and standardized <b>forecasts,</b> is created. The formula provides new <b>forecasting</b> resources and new possibilities for expansion of <b>forecasting</b> including economic <b>forecasting</b> into the areas of municipal needs, middle-size and small-size business and, even, to individual <b>forecasting.</b> ...|$|R
40|$|AbstractWe {{consider}} <b>forecasting</b> systems which, {{when given}} an initial segment of a binary string, guess the next bit (deterministic <b>forecasting)</b> or estimate {{the probability of}} the next bit being 1 (probabilistic <b>forecasting).</b> The quality of <b>forecasting</b> {{is measured by the}} number of errors (in the deterministic case) or by the sum of distances between the <b>forecasts</b> and the “true” probabilities (in the probabilistic case). A <b>forecasting</b> system is said to be simple if it has a short description (e. g., a simple formula) which admits fast computation of the <b>forecasts.</b> There is a “universal <b>forecasting</b> algorithm” which for any given bound Γ on time of <b>forecasting</b> computes <b>forecasts</b> within time Γ, and the quality of these <b>forecasts</b> is not much lower than that of any simple <b>forecasting</b> system (the complexity of the “rival” <b>forecasting</b> system may increase as Γ increases). The aim of the paper is to study the possibilities and limitations of universal <b>forecasting...</b>|$|R
500|$|Tropical cyclone <b>forecasting</b> is {{the science}} of <b>forecasting</b> where a {{tropical}} cyclone's center, and its effects, {{are expected to be}} {{at some point in the}} future. [...] There are several elements to tropical cyclone forecasting: [...] track <b>forecasting,</b> intensity <b>forecasting,</b> rainfall <b>forecasting,</b> storm surge, tornado, and seasonal <b>forecasting.</b> [...] While skill is increasing in regard to track <b>forecasting,</b> intensity <b>forecasting</b> skill remains nearly unchanged over the past several years. [...] Seasonal <b>forecasting</b> began in the 1980s in the Atlantic basin and has spread into other basins in the years since.|$|R
5|$|On December10, 2008, Klotzbach's team {{issued its}} first extended-range <b>forecast</b> for the 2009season, {{predicting}} above-average activity (14named storms, 7hurricanes, 3of Category3 or higher and ACE Index of 125). On April7, 2009, Klotzbach's team issued an updated <b>forecast</b> for the 2009season, predicting near-average activity (12named storms, 6hurricanes, 2of Category3 or higher and ACE Index of 100), citing the possible cause {{as the high}} probability of a weak El Niño forming during the season. On May21, 2009, NOAA issued their <b>forecast</b> for the season, predicting near or slightly above average activity, (9 to 14named storms, 4 to 7hurricanes, and 1 to 3of Category3 or higher).|$|E
5|$|On June1, CSU {{released}} an updated <b>forecast,</b> {{increasing the number}} of predicted named storms to 8, due to the early formation of Tropical Storm Ana, while keeping the predictions for hurricanes and major hurricanes at 3 and 1, respectively; the ACE index <b>forecast</b> was also kept at 40 units. Probabilities of a major hurricane making landfall on various coastal areas remained below average. On August5, TSR updated their <b>forecast</b> and lowered the number of hurricanes developing within the basin to 4, with only 1 forecasted to be a major hurricane. The ACE index was also reduced to 44 units.|$|E
5|$|On June2, 2009, Klotzbach's team issued another updated <b>forecast</b> for the 2009 season, {{predicting}} {{slightly below}} average activity (11 named storms, 5hurricanes, 2 of Category3 or higher and ACE Index of 85). Also on June2, 2009, the Florida State University Center for Ocean-Atmospheric Prediction Studies (FSU COAPS) issued its first ever Atlantic hurricane season <b>forecast.</b> The FSU COAPS <b>forecast</b> predicted 8 named storms, including 4 hurricanes, and an ACE Index of 65. On June18, 2009, the UK Met Office (UKMO) issued a <b>forecast</b> of 6tropical storms in the July to November period with a 70% {{chance that the}} number {{would be in the}} range3 to 9. They also predicted an ACE Index of 60 with a 70%chance that the index would be in the range40 to 80. On August4, 2009, Klotzbach's team updated their <b>forecast</b> for the 2009 season, again predicting slightly below average activity (10 named storms, 4hurricanes, and 2 major hurricanes). On August6, 2009, the NOAA also updated their <b>forecast</b> for the 2009 season, predicting below average activity (7–11 named storms, 3–6 hurricanes, and 1–2 major hurricanes).|$|E
40|$|This paper {{evaluates the}} {{professional}} <b>forecasts,</b> {{those made by}} financial and non-financial forecasters and the aggregate between them, by comparing their results to academic <b>forecasts.</b> The US quarterly inflation rate and the professional <b>forecasts</b> are considered {{for the period of}} 1981 third quarter to 2012 final quarter. This paper examines whether academic <b>forecasts</b> outperforms the professional <b>forecasts.</b> For short term inflation <b>forecasting</b> the professional forecasters (non-financial, financial and the aggregate) proved to be the most accurate, however for long term inflation <b>forecasting</b> academic <b>forecasts</b> showed to be most accurate. The results also indicate that the long term aggregate <b>forecasts</b> related to information from the aggregate short term <b>forecasts</b> and current inflation rate. Furthermore, financial forecasters use the short term non-financial <b>forecasts</b> in their expectations and the non-financial forecasters use the short term financial <b>forecasts</b> in their long term expectations. In addition, the results confirm causality between the short and long term <b>forecasts</b> of the non-financial forecasters. For the financial inflation <b>forecasts,</b> there is no causality between the short and long term financial <b>forecasts...</b>|$|R
40|$|This paper {{examines}} {{the feasibility of}} rule -based <b>forecasting,</b> a procedure that applies <b>forecasting</b> expertise and domain knowledge to produce <b>forecasts</b> according to features of the data. We developed a rule base to make annual extrapolation <b>forecasts</b> for economic and demographic time series. The development of the rule base drew upon protocol analyses of five experts on <b>forecasting</b> methods. This rule base, consisting of 99 rules, combined <b>forecasts</b> from four extrapolation methods (the random walk, regression, Brown's linear exponential smoothing, and Holt's exponential smoothing) according to rules using 18 features of time series. For one-year ahead ex ante <b>forecasts</b> of 90 annual series, the median absolute percentage error (MdAPE) for rule- based <b>forecasting</b> was 13 % less than that from equally-weighted combined <b>forecasts.</b> For six-year ahead ex ante <b>forecasts,</b> rule-based <b>forecasting</b> had a MdAPE that was 42 % less. The improvement in accuracy of the rule - based <b>forecasts</b> over equally-weighted combined <b>forecasts</b> was statistically significant. Rule-based <b>forecasting</b> was more accurate than equal-weights combining in situations involving significant trends, low uncertainty, stability, and good domain expertise. Rule-based <b>forecasting,</b> time series...|$|R
40|$|This paper {{investigates the}} {{accuracy}} of <b>forecasts</b> from four DSGE models for inflation, output growth and the federal funds rate using a real-time dataset synchronized with the Fed’s Greenbook projections. Conditioning the model <b>forecasts</b> on the Greenbook nowcasts leads to <b>forecasts</b> that are as accurate as the Greenbook projections for output growth and the federal funds rate. Only for inflation the model <b>forecasts</b> are dominated by the Greenbook projections. A comparison with <b>forecasts</b> from Bayesian VARs shows that the economic structure of the DSGE models which is useful for the interpretation of <b>forecasts</b> does not lower {{the accuracy of}} <b>forecasts.</b> Combining <b>forecasts</b> of several DSGE models increases precision in comparison to individual model <b>forecasts.</b> Comparing density <b>forecasts</b> with the actual distribution of observations shows that DSGE models overestimate uncertainty around point <b>forecasts.</b> Keywords: JEL-Codes: DSGE models, Bayesian VAR, <b>forecasting,</b> model uncertainty...|$|R
5|$|On March5, 2003, meteorologists {{from the}} University College London at the Tropical Storm Risk (TSR) Consortium issued an {{extended}} range <b>forecast</b> for the typhoon season, noting {{the likelihood of}} near average tropical cyclone activity {{as a result of}} projected neutral sea surface temperatures. The <b>forecast</b> indicated the potential for 26.2tropical storms, compared to the 10– and 30-year average of 27.8 and 26.3storms, respectively. The following month, the group raised their <b>forecast</b> for tropical storms to 26.7, indicating a slightly above average season. Over next two months, however, fluctuations in sea surface temperatures, particularly those in the Central Pacific, caused the group to revise their predictions downward and indicated the probability for a slightly below average typhoon season in their June <b>forecast.</b> A rise in sea surface temperatures in the following months prompted the forecasting group to once again raise their forecasts to indicate a near-average season in their final August <b>forecast</b> update, which predicted 27tropical storms. The group was very accurate in their forecasts, with their April and August forecasts being the most accurate.|$|E
5|$|Once a human-only {{endeavor}} based mainly upon {{changes in}} barometric pressure, current weather conditions, and sky condition, weather forecasting now relies on computer-based models that take many atmospheric factors into account. Human input is still required {{to pick the}} best possible <b>forecast</b> model to base the <b>forecast</b> upon, which involves pattern recognition skills, teleconnections, knowledge of model performance, and knowledge of model biases. The inaccuracy of forecasting {{is due to the}} chaotic nature of the atmosphere, the massive computational power required to solve the equations that describe the atmosphere, the error involved in measuring the initial conditions, and an incomplete understanding of atmospheric processes. Hence, forecasts become less accurate as the difference between current time and the time for which the <b>forecast</b> is being made (the range of the <b>forecast)</b> increases. The use of ensembles and model consensus help narrow the error and pick the most likely outcome.|$|E
5|$|Once an all-human {{endeavor}} based mainly upon {{changes in}} barometric pressure, current weather conditions, and sky condition, <b>forecast</b> models are now {{used to determine}} future conditions. On the other hand, human input is still required to pick the best possible <b>forecast</b> model to base the <b>forecast</b> upon, which involve many disciplines such as pattern recognition skills, teleconnections, knowledge of model performance, and knowledge of model biases.|$|E
40|$|This study proposes the New Product Sales <b>Forecasting</b> Procedure (NPSFP) and the New Product <b>Forecasting</b> System (NPFS) {{for solving}} the new product sales <b>forecasting</b> problem. The NPSFP standardizes the steps {{involved}} in sales <b>forecasting,</b> guiding data acquisition and analysis, {{choice of the}} <b>forecasting</b> model, calculation of the actual <b>forecasts,</b> and subjective manual adjustment of the <b>forecasting</b> results. The NPFS decision-support system includes four modules: one that guides data acquisition and analysis, one that contains <b>forecasting</b> model templates, one that helps the users choose the best model for the available data, and one that calculates and adjusts the actual <b>forecasts...</b>|$|R
40|$|Iterated {{multi-step}} <b>forecasts</b> {{are usually}} constructed assuming {{the same model}} in each <b>forecasting</b> iteration. In this paper, the model coefficients are allowed to change across <b>forecasting</b> iterations according to the in-sample prediction performance at a particular <b>forecasting</b> horizon. The technique can thus {{be viewed as a}} combination of iterated and direct <b>forecasting.</b> The superior point and density <b>forecasting</b> performance of this approach is demonstrated on a standard medium-scale vector autoregression employing variables used in the Smets and Wouters (2007) model of the US economy. The estimation of the model and <b>forecasting</b> are carried out in a Bayesian way on data covering the period 1959 Q 1 - 2016 Q 1. Bayesian estimation, direct <b>forecasting,</b> iterated <b>forecasting,</b> multi-step <b>forecasts,</b> VA...|$|R
40|$|International audienceWe derive {{internal}} consistency restrictions on short-term, medium-term,and long-term oil price <b>forecasts.</b> We then analyze whether oil price <b>forecasts</b> {{extracted from the}} Survey of Professional Forecasters conducted by the European Central Bank satisfy these {{internal consistency}} restrictions. We find that neither short-term <b>forecasts</b> are consistent with medium-term <b>forecasts</b> nor that medium-term <b>forecasts</b> are consistent with long-term <b>forecasts.</b> Using a more complex expectation formation structure featuring a distributed lag structure, however, we find stronger evidence of internal consistency of medium-term <b>forecasts</b> with long-term <b>forecasts...</b>|$|R
5|$|There is a three-stage {{process in}} which the area, time period, and details of a severe weather <b>forecast</b> are refined from a broad-scale <b>forecast</b> of {{potential}} hazards to a more specific and detailed <b>forecast</b> of what hazards are expected, and where and in what time frame they are expected to occur. If warranted, forecasts will also increase in severity through this three-stage process.|$|E
5|$|When Linda was <b>forecast</b> to make landfall, {{statements}} about its possible impact were {{issued by the}} Oxnard, California office of the National Weather Service. They stressed the uncertainty of a <b>forecast</b> that far in the future.|$|E
5|$|With {{tropical}} cyclones which {{impact the}} United States, the GFS global <b>forecast</b> model performed best {{in regards to}} its rainfall forecasts {{over the last few}} years, outperforming the NAM and ECMWF <b>forecast</b> models.|$|E
40|$|This paper {{examines}} {{the effects of}} disclosures on information asymmetry by studying bid-ask spreads around independent management <b>forecasts</b> and earnings announcements released with <b>forecasts.</b> The findings suggest the disclosure of independent management <b>forecasts</b> increase information asymmetry in the market rather than resolving it. Regulation FD has reduced the overall level of information asymmetry in the market with respect to both earnings announcements and management <b>forecasts</b> although it has a greater effect on management <b>forecasts,</b> post-forecast spreads. Closer analysis reveals that when “good news” <b>forecasts</b> and separated from “bad news” independent management <b>forecasts,</b> good news management <b>forecasts</b> decrease information asymmetry. Since initial tests demonstrated that management <b>forecasts</b> increase information asymmetry, these findings suggests that {{the magnitude of the}} effect of bad news management <b>forecasts</b> is greater than that of good news <b>forecasts...</b>|$|R
40|$|An {{alternative}} to using a single <b>forecasting</b> {{method is to}} average the <b>forecasts</b> obtained from several methods. In this paper we investigate empirically {{the impact of the}} number and choice of <b>forecasting</b> methods on the accuracy of simple averages. It is concluded that the <b>forecasting</b> accuracy improves, and that the variability of accuracy among different combinations decreases, as the number of methods in the average increases. Thus, combining <b>forecasts</b> seems to be a reasonable practical alternative when, as is often the case, a "true" model of the data-generating process or a single "best" <b>forecasting</b> method cannot be or is not, for whatever reasons, identified. <b>forecasting,</b> combined <b>forecasts,</b> averages of <b>forecasts...</b>|$|R
40|$|This paper {{presents}} {{a new approach}} to the evaluation of FOMC macroeconomic <b>forecasts.</b> Its distinctive feature is the interpretation, under reasonable conditions, of the minimum and maximum <b>forecasts</b> reported in FOMC meetings as indicative of probability density <b>forecasts</b> for these variables. This leads to some straightforward binomial tests of the performance of the FOMC <b>forecasts</b> as <b>forecasts</b> of macroeconomic risks. Empirical results suggest that there are serious problems with the FOMC <b>forecasts.</b> Most particularly, there are problems with the FOMC <b>forecasts</b> of the tails of the macroeconomic density functions, including a tendency to under-estimate the tails of macroeconomic risks. Macroeconomic risks, FOMC <b>forecasts,</b> density <b>forecasting...</b>|$|R
