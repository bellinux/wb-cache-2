6637|10000|Public
25|$|The <b>hidden</b> <b>markov</b> <b>model</b> can be {{represented}} as the simplest dynamic Bayesian network. The mathematics behind the HMM were developed by L. E. Baum and coworkers.|$|E
25|$|In {{recursive}} Bayesian estimation, {{the true}} state {{is assumed to}} be an unobserved Markov process, and the measurements are the observed states of a <b>hidden</b> <b>Markov</b> <b>model</b> (HMM).|$|E
25|$|<b>Hidden</b> <b>Markov</b> <b>Model</b> (HMM) is a {{statistical}} Markov {{model in which}} the system being modeled {{is assumed to be}} a Markov process with unobserved (i.e. hidden) states.|$|E
40|$|We {{describe}} pair <b>hidden</b> <b>Markov</b> <b>models,</b> with {{an emphasis}} on their relationship to evolutionary <b>models</b> and <b>hidden</b> <b>Markov</b> <b>models.</b> We then explain the statistical interpretation of alignment with pair <b>hidden</b> <b>Markov</b> <b>models,</b> and highlight connections to the Needleman–Wunsch algorithm and other dynamic programming–based alignment algorithms...|$|R
40|$|Abstract: In this paper, the second-order <b>hidden</b> <b>Markov</b> <b>models</b> (HMM 2 s) {{have been}} used to enhance the {{recognition}} performance of isolated-word text-dependent speaker authentication systems under the neutral talking condition. Our results show that HMM 2 s enhance the speaker authentication performance under such a condition compared to the firstorder <b>hidden</b> <b>Markov</b> <b>models</b> (HMM 1 s). The speaker authentication performance has been improved by 6 %. Key words: First-order <b>hidden</b> <b>Markov</b> <b>models,</b> neutral talking condition, second-order <b>hidden</b> <b>Markov</b> <b>models,</b> speaker authentication performance...|$|R
40|$|An on-line {{unsupervised}} algorithm for {{estimating the}} <b>hidden</b> <b>Markov</b> <b>models</b> (HMM) parame-ters is presented. The problem of <b>hidden</b> <b>Markov</b> <b>models</b> adaptation to emotional speech is solved. To increase {{the reliability of}} estimated HMM parameters, a mechanism of forgetting and updating is proposed. A functional block diagram of the <b>hidden</b> <b>Markov</b> <b>models</b> adaptation algorithm is also provided with obtained results, which improve the efficiency of emotional speech recognition...|$|R
25|$|Note {{that this}} process has {{identical}} structure to the <b>hidden</b> <b>Markov</b> <b>model,</b> except that the discrete state and observations are replaced with continuous variables sampled from Gaussian distributions.|$|E
25|$|The Kalman {{filters are}} based on linear {{dynamical}} systems discretized in the time domain. They are modelled on a Markov chain built on linear operators perturbed by errors that may include Gaussian noise. The state {{of the system is}} represented as a vector of real numbers. At each discrete time increment, a linear operator is applied to the state to generate the new state, with some noise mixed in, and optionally some information from the controls on the system if they are known. Then, another linear operator mixed with more noise generates the observed outputs from the true ("hidden") state. The Kalman filter may be regarded as analogous to the <b>hidden</b> <b>Markov</b> <b>model,</b> with the key difference that the hidden state variables take values in a continuous space (as opposed to a discrete state space as in the <b>hidden</b> <b>Markov</b> <b>model).</b> There is a strong duality between the equations of the Kalman Filter and those of the <b>hidden</b> <b>Markov</b> <b>model.</b> A review of this and other models is given in Roweis and Ghahramani (1999), and Hamilton (1994), Chapter 13.|$|E
25|$|The use of {{temporal}} probabilistic models {{has been shown}} to perform well in activity recognition and generally outperform non-temporal models. Generative models such as the <b>Hidden</b> <b>Markov</b> <b>Model</b> (HMM) and the more generally formulated Dynamic Bayesian Networks (DBN) are popular choices in modelling activities from sensor data.|$|E
40|$|AbstractThe basic {{theory of}} <b>hidden</b> <b>Markov</b> <b>models</b> was {{developed}} and applied to problems in speech recognition in the late 1960 s, and has since then been applied to numerous problems, e. g. biological sequence analysis. Most applications of <b>hidden</b> <b>Markov</b> <b>models</b> are based on efficient algorithms for computing the probability of generating a given string, or computing the most likely path generating a given string. In this paper we consider the problem of computing the most likely string, or consensus string, generated by a given model, and its implications on the complexity of comparing <b>hidden</b> <b>Markov</b> <b>models.</b> We show that computing the consensus string, and approximating its probability within any constant factor, is NP-hard, and that the same holds for the closely related labeling problem for class <b>hidden</b> <b>Markov</b> <b>models.</b> Furthermore, we establish the NP-hardness of comparing two <b>hidden</b> <b>Markov</b> <b>models</b> under the L∞- and L 1 -norms. We discuss {{the applicability of the}} technique used for proving the hardness of comparing two <b>hidden</b> <b>Markov</b> <b>models</b> under the L 1 -norm to other measures of distance between probability distributions. In particular, we show that it cannot be used for proving NP-hardness of determining the Kullback–Leibler distance between two <b>hidden</b> <b>Markov</b> <b>models,</b> or of comparing them under the Lk-norm for any fixed even integer k...|$|R
40|$|This paper reviews recent {{advances}} in Bayesian nonparametric techniques for constructing and performing inference in infinite <b>hidden</b> <b>Markov</b> <b>models.</b> We focus on variants of Bayesian nonparametric <b>hidden</b> <b>Markov</b> <b>models</b> that enhance a posteriori state-persistence in particular. This paper also introduces a new Bayesian nonparametric framework for generating left-to-right and other structured, explicit-duration infinite <b>hidden</b> <b>Markov</b> <b>models</b> that we call the infinite structured hidden semi-Markov model. Comment: 23 pages, 10 figure...|$|R
40|$|In this paper, {{we study}} {{the problem of}} {{learning}} phylogenies and <b>hidden</b> <b>Markov</b> <b>models.</b> We call a <b>Markov</b> <b>model</b> nonsingular if all transition matrices have determinants bounded away from 0 (and 1). We highlight {{the role of the}} nonsingularity condition for the learning problem. Learning <b>hidden</b> <b>Markov</b> <b>models</b> without the nonsingularity condition is at least as hard as learning parity with noise. On the other hand, we give a polynomial-time algorithm for learning nonsingular phylogenies and <b>hidden</b> <b>Markov</b> <b>models...</b>|$|R
25|$|Extensions and generalizations to {{the method}} {{have also been}} developed, such as the {{extended}} Kalman filter and the unscented Kalman filter which work on nonlinear systems. The underlying model is a Bayesian model similar to a <b>hidden</b> <b>Markov</b> <b>model</b> except that the state space of the latent variables is continuous and all latent and observed variables have Gaussian distributions.|$|E
25|$|Conventional {{temporal}} probabilistic models {{such as the}} <b>hidden</b> <b>Markov</b> <b>model</b> (HMM) and conditional random fields (CRF) model directly {{model the}} correlations between the activities and the observed sensor data. In recent years, increasing evidence has supported the use of hierarchical models which take into account the rich hierarchical structure that exists in human behavioral data. The core idea here is that the model does not directly correlate the activities with the sensor data, but instead breaks the activity into sub-activities (sometimes referred to as actions) and models the underlying correlations accordingly. An example could be the activity of preparing spaghetti, which can be broken down into the subactivities or actions of cutting vegetables, frying the vegetables in a pan and serving it on a plate. Examples of such a hierarchical model are Layered Hidden Markov Models (LHMMs) and the hierarchical <b>hidden</b> <b>Markov</b> <b>model</b> (HHMM), which have been shown to significantly outperform its non-hierarchical counterpart in activity recognition.|$|E
25|$|Oxford Nanopore Technologies’ MinION {{has been}} used to detect DNAm. As each DNA strand passes through a pore, it {{produces}} electrical signals which {{have been found to be}} sensitive to epigenetic changes in the nucleotides, and a <b>hidden</b> <b>Markov</b> <b>model</b> (HMM) was used to analyze MinION data to detect 5-methylcytosine (5mC) DNA modification. The model was trained using synthetically methylated E. coli DNA and the resulting signals measured by the nanopore technology. Then the trained model was used to detect 5mC in MinION genomic reads from a human cell line which already had a reference methylome. The classifier has 82% accuracy in randomly sampled singleton sites, which increases to 95% when more stringent thresholds are applied.|$|E
40|$|In {{the current}} thesis several {{selected}} {{aspects of the}} two related latent class models; finite mixtures and <b>hidden</b> <b>Markov</b> <b>models,</b> were considered. The problem of calculating the MLE of a Gaussian mixture with Newton's method and the consistency of penalized MLE were investigated. A penalized MLE procedure for univariate Gaussian <b>hidden</b> <b>Markov</b> <b>models</b> was introduced and shown to be consistent. Furthermore, a result on identifiability of nonparametric <b>hidden</b> <b>Markov</b> <b>models</b> is derived...|$|R
40|$|We {{propose a}} dynamic {{graphical}} model which generalizes nonhomogeneous <b>hidden</b> <b>Markov</b> <b>models.</b> Inference and forecast procedures are developed. A comparison with an exact propagation algorithm is established and equivalence is stated. Graphical <b>models</b> Nonhomogeneous <b>hidden</b> <b>Markov</b> <b>models</b> Inference and forecast procedures...|$|R
40|$|In this thesis, maximum {{likelihood}} estimation of <b>hidden</b> <b>Markov</b> <b>models</b> in several settings is investigated. Nonparametric estimation of state-dependent general mixtures and log-concave densities is discussed theoretically and algorithmically. Penalized estimation for parametric <b>hidden</b> <b>Markov</b> <b>models</b> comparing several penalty functions is studied. In addition, various models based on mixture <b>models</b> and <b>hidden</b> <b>Markov</b> <b>models</b> differing in dependency {{structure and the}} inclusion of covariables are applied {{to a set of}} panel data containing the GDP of several countries...|$|R
25|$|Processing of the {{raw data}} – such as {{normalization}} to the median signal – was needed on MinION raw data, reducing real-time capability of the technology. Consistency of the electrical signals is still an issue, {{making it difficult to}} accurately call a nucleotide. MinION has low throughput; since multiple overlapping reads are hard to obtain, this further leads to accuracy problems of downstream DNA modification detection. Both the <b>hidden</b> <b>Markov</b> <b>model</b> and statistical methods used with MinION raw data require repeated observations of DNA modifications for detection, meaning that individual modified nucleotides need to be consistently present in multiple copies of the genome, e.g. in multiple cells or plasmids in the sample.|$|E
25|$|Generally {{heuristic}} algorithm {{relies on}} the iterative strategy, scilicet based on a comparison method, optimizing the results of multiple sequence alignment by the iterative process. Davie M proposed using particle swarm optimization algorithm to solve the multiple sequence alignment problem; Ikeda T proposed a heuristic algorithm {{which is based on}} A* search algorithm; Bimey E first proposed using <b>hidden</b> <b>Markov</b> <b>model</b> to solve the multiple sequence alignment problem; and many other biologists use genetic algorithm to solve it. All these algorithms generally are robust and insensitive to the number of sequences, but they also have shortcoming, for example, the result got from particle swarm optimization algorithm is unstable and its merits depend on the selection of random numbers, the runtime of A * search algorithm is too long and the genetic algorithm is easy to fall into local excellent.|$|E
500|$|Typical HMM-based methods work by {{representing}} an MSA {{as a form}} of directed acyclic graph {{known as}} a partial-order graph, which consists of a series of nodes representing possible entries in the columns of an MSA. In this representation a column that is absolutely conserved (that is, that all the sequences in the MSA share a particular character at a particular position) is coded as a single node with as many outgoing connections as there are possible characters in the next column of the alignment. In the terms of a typical <b>hidden</b> <b>Markov</b> <b>model,</b> the observed states are the individual alignment columns and the [...] "hidden" [...] states represent the presumed ancestral sequence from which the sequences in the query set are hypothesized to have descended. An efficient search variant of the dynamic programming method, known as the Viterbi algorithm, is generally used to successively align the growing MSA to the next sequence in the query set to produce a new MSA. This is distinct from progressive alignment methods because the alignment of prior sequences is updated at each new sequence addition. However, like progressive methods, this technique can be influenced by the order in which the sequences in the query set are integrated into the alignment, especially when the sequences are distantly related.|$|E
30|$|Section 2 {{introduces}} notation {{and provides}} some background to speech recognition using <b>hidden</b> <b>Markov</b> <b>models.</b> In addition, it discusses multistream methods for combining multiple <b>hidden</b> <b>Markov</b> <b>models</b> to perform speech recognition. Finally, it introduces the ensemble methods {{used in the}} paper, bagging and boosting, in their basic form.|$|R
40|$|In the paper, the {{approximate}} sequence for entropy of some binary <b>hidden</b> <b>Markov</b> <b>models</b> {{has been found}} to have two bound sequences, the low bound sequence and the upper bound sequence. The error bias of {{the approximate}} sequence is bound by a geometric sequence with a scale factor less than 1 which decreases quickly to zero. It helps to understand the convergence of entropy rate of generic <b>hidden</b> <b>Markov</b> <b>models,</b> and it provides a theoretical base for estimating the entropy rate of some <b>hidden</b> <b>Markov</b> <b>models</b> at any accuracy. Comment: 6 pages, in Chines...|$|R
5000|$|... #Subtitle level 3: Nonparametric {{filtering}} in <b>hidden</b> <b>Markov</b> <b>models</b> ...|$|R
2500|$|Using these {{assumptions}} the probability distribution over all {{states of the}} <b>hidden</b> <b>Markov</b> <b>model</b> can be written simply as: ...|$|E
2500|$|Researchers {{demonstrated}} (2010) {{that deep}} neural networks interfaced with a <b>hidden</b> <b>Markov</b> <b>model</b> with context-dependent states {{that define the}} neural network output layer can drastically reduce errors in large-vocabulary speech recognition tasks such as voice search [...]|$|E
2500|$|Instead of {{predicting}} the 3D {{structure of a}} protein sequence, often users have a solved structure and {{they are interested in}} determining if there is a related structure in a genome of interest. In Phyre2 an uploaded protein structure can be converted into a <b>hidden</b> <b>Markov</b> <b>model</b> and then scanned against a set of genomes (more than 20 genomes as of March 2011). This functionality is called [...] "BackPhyre" [...] to indicate how Phyre2 is being used in reverse.|$|E
50|$|The SUPERFAMILY {{annotation}} {{is based}} on a collection of <b>hidden</b> <b>Markov</b> <b>models</b> (HMM), which represent structural protein domains at the SCOP superfamily level. A superfamily groups together domains which have an evolutionary relationship. The annotation is produced by scanning protein sequences from completely sequenced genomes against the <b>hidden</b> <b>Markov</b> <b>models.</b>|$|R
5000|$|Poisson <b>hidden</b> <b>Markov</b> <b>models</b> (PHMM) {{are special}} cases of <b>hidden</b> <b>Markov</b> <b>models</b> where a Poisson process has a rate which varies in {{association}} with changes between the different states of a <b>Markov</b> <b>model.</b> [...] PHMMs are not necessarily Markovian processes themselves because the underlying Markov chain or Markov process cannot be observed and only the Poisson signal is observed.|$|R
40|$|This unit {{introduces}} {{the concept of}} <b>hidden</b> <b>Markov</b> <b>models</b> in computational biology. It describes them using simple biological examples, requiring as little mathematical knowledge as possible. The unit also presents {{a brief history of}} <b>hidden</b> <b>Markov</b> <b>models</b> and an overview of their current applications before concluding with a discussion of their limitations...|$|R
2500|$|In {{a section}} {{entitled}} A Strategy for Creating a Mind Kurzweil summarizes {{how he would}} put together a digital mind. He would start with a pattern recognizer and arrange for a hierarchy to self-organize using a hierarchical <b>hidden</b> <b>Markov</b> <b>model.</b> All parameters of the system would be optimized using genetic algorithms. He would add in a [...] "critical thinking module" [...] to scan existing patterns in the background for incompatibilities, to avoid holding inconsistent ideas. Kurzweil says the brain {{should have access to}} [...] "open questions in every discipline" [...] and have the ability to [...] "master vast databases", something traditional computers are good at. He feels the final digital brain would be [...] "as capable as biological ones of effecting changes in the world".|$|E
2500|$|An illustrative {{example is}} a {{bistable}} system that can be characterized by a <b>hidden</b> <b>Markov</b> <b>model</b> (HMM) subject to measurement noise (Figure 2). Such models are employed for many biological systems: they have for example been used in development, cell signaling, activation/deactivation, logical processing and non-equilibrium thermodynamics. For instance, {{the behavior of the}} Sonic Hedgehog (Shh) transcription factor in Drosophila melanogaster can be modeled with a HMM. The (biological) dynamical model consists of two states: A and B. If the probability of a transition from one state to the other is defined as [...] in both directions, the probability to remain in the same state at each time step is 1-. The probability to measure the state correctly is [...] (conversely, the probability of an incorrect measurement is 1-).|$|E
2500|$|Jelinek {{regarded}} {{speech recognition}} as an information theory problema noisy channel, {{in this case}} the acoustic signalwhich some observers considered a daring approach. The concept of perplexity was introduced in their first model, New Raleigh Grammar, which was published in 1976 as the paper [...] "Continuous Speech Recognition by Statistical Methods" [...] in the journal Proceedings of the IEEE. According to Young, the basic noisy channel approach [...] "reduced the speech recognition problem to one of producing two statistical models". Whereas New Raleigh Grammar was a <b>hidden</b> <b>Markov</b> <b>model,</b> their next model, called Tangora, was broader and involved n-grams, specifically trigrams. Even though [...] "it was obvious to everyone that this model was hopelessly impoverished", it was not improved upon until Jelinek presented another paper in 1999. The same trigram approach was applied to phones in single words. Although the identification of parts of speech {{turned out not to be}} very useful for speech recognition, tagging methods developed during these projects are now used in various NLP applications.|$|E
40|$|When {{is keeping}} {{a memory of}} {{observations}} worthwhile? We use <b>hidden</b> <b>Markov</b> <b>models</b> to look at phase transitions that emerge when comparing state estimates in systems with discrete states and noisy observations. We infer the underlying state of the <b>hidden</b> <b>Markov</b> <b>models</b> from the observations in two ways: through naive observations, which take into account only the current observation, and through Bayesian filtering, which takes the history of observations into account. Defining a discord order parameter {{to distinguish between the}} different state estimates, we explore <b>hidden</b> <b>Markov</b> <b>models</b> with various numbers of states and symbols and varying transition-matrix symmetry. All behave similarly. We calculate analytically the critical point where keeping a memory of observations starts to pay off. A mapping between <b>hidden</b> <b>Markov</b> <b>models</b> and Ising models gives added insight into the associated phase transitions. Comment: 11 pages, 8 figure...|$|R
40|$|Abstract. — The aim of {{the present}} paper is to {{document}} the need for adapting the definition of <b>hidden</b> <b>Markov</b> <b>models</b> (HMM) to population studies, which rigorous interpretation typically {{requires the use of}} mixed-effects models, as well as for corresponding learning methodologies. In this article, mixed <b>hidden</b> <b>Markov</b> <b>models</b> (MHMM) are introduced through a brief state of the art on <b>hidden</b> <b>Markov</b> <b>models</b> and related applications, especially focusing on disease related problems. Making the main assumption that a given pathology can be considered at different stages, <b>hidden</b> <b>Markov</b> <b>models</b> have for example already been used to study epileptic activity or migraine. Mixed-effects <b>hidden</b> <b>Markov</b> <b>models</b> have been newly introduced in the statistical literature. The notion of mixed <b>hidden</b> <b>Markov</b> <b>models</b> is particularly relevant for modeling medical symptoms, but the data complexity generally requires specific care and the available methodology for MHMM is relatively poor. Our new approach can be briefly described as follows. First, we suggest to estimate the population parameters with the SAEM (Stochastic Approximation EM) algorithm, which has the property to converge quickly. The well-known forward recursions developed for HMM allow to compute easily the complete likelihood at each step of the MCMC procedure used within SAEM. Then, for dealing with the individuals, we suggest to estimate each set of individual parameters with the MAP (Maximum A Posteriori) of the parameter distributions. Finally, the hidden state sequences are decoded using the Viterbi algorithm. Some Monte-Carlo experiments are presented to illustrate the accuracy of our algorithms. 2000 Mathematics Subject Classification. — 62 - 02. Key words and phrases. — <b>hidden</b> <b>Markov</b> <b>models,</b> mixed-effects, longitudinal data, stochasti...|$|R
40|$|In {{this paper}} we present an {{alternative}} to <b>hidden</b> <b>Markov</b> <b>models</b> for the recognition of image sequences. The approach {{is based on a}} stochastic version of recurrent neural networks, which we call diffusion networks. Contrary to <b>hidden</b> <b>Markov</b> <b>models,</b> diffusion networks operate with continuous state dynamics, and generate continuous paths. This aspect that may be beneficial in computer vision tasks in which continuity is a useful constraint. In this paper we review results required for the implementation of diffusion networks, and then apply them to a visual speech recognition task. Diffusion networks outperformed the results obtained with the best <b>hidden</b> <b>Markov</b> <b>models...</b>|$|R
