4|3897|Public
40|$|There {{has been}} {{considerable}} recent {{interest in the}} use of cortical auditory evoked potentials (CAEPs) as an electrophysiological measure of human speech encoding in individuals with normal as well as <b>impaired</b> <b>auditory</b> <b>systems.</b> The development of such electrophysiological measures such as CAEPs is important because they can be used to evaluate the benefits of hearing aids and cochlear implants in infants, young children, and adults that cannot cooperate for behavioral speech discrimination testing. The current study determined whether CAEPs produced by seven different speech sounds, which together cover a broad range of frequencies across the speech spectrum, could be differentiated from each other based on response latency and amplitude measures. CAEPs were recorded from ten adults with normal hearing in response to speech stimuli presented at a conversational level (65 dB SPL) via a loudspeaker. Cortical responses were reliably elicited by each of the speech sounds in all participants. CAEPs produced by speech sounds dominated by high-frequency energy were significantly different in amplitude from CAEPs produced by sounds dominated by lower-frequency energy. Significant effects of stimulus duration were also observed, with shorter duration stimuli producing larger amplitudes and earlier latencies than longer duration stimuli. This research demonstrates that CAEPs can be reliably evoked by sounds that encompass the entire speech frequency range. Further, CAEP latencies and amplitudes may provide an objective indication that spectrally different speech sounds are encoded differently at the cortical level. 14 page(s...|$|E
40|$|Objective : Cortical auditory-evoked {{potentials}} (CAEPs), {{an objective}} measure of human speech encoding in individuals with normal or <b>impaired</b> <b>auditory</b> <b>systems,</b> {{can be used}} to assess the outcomes of hearing aids and cochlear implants in infants, or in young children who cannot co-operate for behavioural speech discrimination testing. The current study aimed to determine whether naturally produced speech stimuli /m/, /g/ and /t/ evoke distinct CAEP response patterns that can be reliably recorded and differentiated, based on their spectral information and whether the CAEP could be an electrophysiological measure to differentiate between these speech sounds. Method : CAEPs were recorded from 18 school-aged children with normal hearing, tested in two groups: younger (5 - 7 years) and older children (8 - 12 years). Cortical responses differed in their P 1 and N 2 latencies and amplitudes in response to /m/, /g/ and /t/ sounds (from low-, mid- and high-frequency regions, respectively). The largest amplitude of the P 1 and N 2 component was for /g/ and the smallest was for /t/. The P 1 latency in both age groups did not show any significant difference between these speech sounds. The N 2 latency showed a significant change in the younger group but not in the older group. The N 2 latency of the speech sound /g/ was always noted earlier in both groups. Conclusion : This study demonstrates that spectrally different speech sounds are encoded differentially at the cortical level, and evoke distinct CAEP response patterns. CAEP latencies and amplitudes may provide an objective indication that spectrally different speech sounds are encoded differently at the cortical level. </p...|$|E
40|$|Declines in {{auditory}} temporal processing are {{a common}} consequence of natural aging. Interactions between aging and spectro-temporal pitch processing {{have yet to be}} thoroughly investigated in humans, though recent neurophysiologic and electrophysiologic data lend support to the notion that periodicity coding using only unresolved harmonics (i. e., those available via the temporal envelope) is negatively affected as a consequence of age. Individuals with cochlear implants (CIs) must rely on the temporal envelope of speech to glean information about voice pitch [coded through the fundamental frequency (f 0) ], as spectral f 0 cues are not available. While cochlear implants {{have been shown to be}} efficacious in older adults, it is hypothesized that they would experience difficulty perceiving spectrally-degraded voice-pitch information. The current experiments were aimed at quantifying the ability of younger and older listeners to utilize spectro-temporal cues to obtain voice pitch information when performing simple and complex auditory tasks. Experiment 1 measured the ability of younger and older NH listeners to perceive a difference in the frequency of amplitude modulated broad-band noise, thereby exploiting only temporal envelope cues to perform the task. Experiment 2 measured age-related differences in f 0 difference limens as the degree of spectral degradation was manipulated to approximate CI processing. Results from Experiments 1 and 2 demonstrated that spectro-temporal processing of f 0 information in non-speech stimuli is affected in older adults. Experiment 3 showed that age-related performances observed in Experiments 1 and 2 translated to voice gender identification using a natural speech stimulus. Experiment 4 attempted to estimate how younger and older NH listeners are able to utilize differences in voice pitch information in everyday listening environments (i. e., speech in noise) and how such abilities are affected by spectral degradation. Comprehensive results provide further insight on pitch coding in both normal and <b>impaired</b> <b>auditory</b> <b>systems,</b> and demonstrate that spectro-temporal pitch processing is dependent upon the age of the listener. Results could have important implications for elderly cochlear implant recipients...|$|E
40|$|Spontaneous neural {{activity}} in the auditory nerve fibers and in auditory cortex in healthy animals is discussed {{with respect to the}} question: Is spontaneous activity noise or information carrier? The studies reviewed suggest strongly that spontaneous activity is a carrier of information. Subsequently, I review the numerous findings in the <b>impaired</b> <b>auditory</b> <b>system,</b> particularly with reference to noise trauma and tinnitus. Here the common assumption is that tinnitus reflects increased noise in the <b>auditory</b> <b>system</b> that among others affects temporal processing and interferes with the gap-startle reflex, which is frequently used as a behavioral assay for tinnitus. It is, however, more likely that the increased spontaneous {{activity in}} tinnitus, firing rate as well as neural synchrony, carries information that shapes the activity of downstream structures, including non-auditory ones, and leading to the tinnitus percept. The main drivers of that process are bursting and synchronous firing, which facilitates transfer of activity across synapses, and allows formation of auditory objects, such as tinnitu...|$|R
40|$|International audienceThe {{rules of}} western music were {{developed}} across centuries and driven by two main constraints: {{the physics of}} the instruments and {{the mechanics of the}} <b>auditory</b> <b>system.</b> For example, the importance of intervals such as the octave and the fifth can be linked to the physics of instruments that impose energy at multiples of the fundamental frequency. Likewise, the semitone (the smallest common interval in music), can be linked to the frequency selectivity of the <b>auditory</b> <b>system.</b> Created by musicians with normal hearing, these rules collapse for listeners with hearing loss. Hearing impairment can result in changes such as elevated sensitivity, widening of critical bands, and recruitment. Furthermore, hearing devices such as the cochlear implant generally discard parts of the signal that carry pitch information. To address this problem a new framework was developed based on audience response data and psychoacoustics experiments. The enjoyment of six new works, developed in collaboration with electroacoustic composers specifically to address the <b>impaired</b> <b>auditory</b> <b>system,</b> was assessed. A psychoacoustics study involved enhancing auditory streaming cues in existing music: enhancing the perceptual difference between different instruments or lines of melody in order to make their separation easier for hearing impaired listeners...|$|R
40|$|The <b>auditory</b> <b>system</b> {{processes}} temporal {{information at}} multiple scales, and disruptions to this temporal processing {{may lead to}} deficits in auditory tasks such as detecting and discriminating sounds in a noisy environment. Here, a modelling approach is used to study the temporal regularity of firing by chopper cells in the ventral cochlear nucleus, in both the normal and <b>impaired</b> <b>auditory</b> <b>system.</b> Chopper cells, which have a strikingly regular firing response, divide into two classes, sustained and transient, based on {{the time course of}} this regularity. Several hypotheses have been proposed to explain the behaviour of chopper cells, and the difference between sustained and transient cells in particular. However, there is no conclusive evidence so far. Here, a reduced mathematical model is developed and used to compare and test a wide range of hypotheses with a limited number of parameters. Simulation results show a continuum of cell types and behaviours: chopper-like behaviour arises {{for a wide range of}} parameters, suggesting that multiple mechanisms may underlie this behaviour. The model accounts for systematic trends in regularity as a function of stimulus level that have previously only been reported anecdotally. Finally, the model is used to predict the effects of a reduction in the number of auditory nerve fibres (deafferentation due to, for example, cochlear synaptopathy). An interactive version of this paper in which all the model parameters can be changed is available online...|$|R
40|$|It is {{estimated}} that 26 {{million people in the}} United States alone aged 20 to 70 suffer from hearing loss of a sensorineural nature due to exposure to loud sounds at work or leisurely activities. People suffering from sensorineural hearing loss (SNHL) face difficulty in perceiving speech when competing sounds are present (e. g. in a restaurant or a bar). SNHL is often treated with prosthetic devices such as hearing aids and/or cochlear implants. Despite recent advances in these prosthetic devices, listeners continue to face difficulty perceiving speech in noise. ^ Speech and other complex sounds can be mathematically separated into rapidly varying temporal fine structure (TFS) and slowly varying envelope components. Recent perceptual studies have shown that poor speech intelligibility experienced by hearing-impaired listeners in degraded listening conditions is associated with their reduced ability to use TFS cues. These results have fueled an active debate about the role of TFS and envelope coding in normal and impaired hearing and have important implications for improving the ability of hearing aids and cochlear implants to restore speech perception in noise. However, these implications depend critically on the underlying physiological (neural) bases for these perceptual deficits. ^ The present study thoroughly characterized neural coding of envelope and TFS in normal and <b>impaired</b> <b>auditory</b> <b>systems.</b> Spike trains were recorded from auditory-nerve (AN) fibers in chinchillas with either normal-hearing or a noise-induced SNHL. Within- and across-fiber temporal coding (i. e., phase locking) to a broad range of stimuli was quantified. In contrast to common assumptions, our data suggest that SNHL does not degrade the fundamental ability of AN fibers to phase lock to either TFS or envelope. ^ Rather, several other effects of SNHL were observed that may contribute to perceptual deficits in the temporal processing of complex stimuli like speech. For example, (1) envelope coding was enhanced following SNHL, which may over-emphasize fluctuating background sounds, e. g., competing talkers, (2) across-fiber estimates of traveling-wave delays were reduced significantly, and (3) larger than expected shifts in the best frequency of excitation were observed, resulting in a disruption of the normal tonotopic map. Overall, this work demonstrates that perceptual TFS deficits do not result from a simple reduction in the temporal-coding ability of AN fibers, but rather are more likely due to neural response properties that are relevant for complex stimuli and that are not currently accounted for in hearing-aid and cochlear-implant signal processing strategies. ...|$|E
40|$|Head {{movements}} are {{intimately involved in}} sound localization and may provide information that could aid an <b>impaired</b> <b>auditory</b> <b>system.</b> Using an infrared camera system, head position and orientation was measured for 17 normal-hearing and 14 hearing-impaired listeners seated {{at the center of}} a ring of loudspeakers. Listeners were asked to orient their heads as quickly as was comfortable toward a sequence of visual targets, or were blindfolded and asked to orient toward a sequence of loudspeakers playing a short sentence. To attempt to elicit natural orienting responses, listeners were not asked to reorient their heads to the 0 degrees loudspeaker between trials. The results demonstrate that hearing-impairment is associated with several changes in orienting responses. Hearing-impaired listeners showed a larger difference in auditory versus visual fixation position and a substantial increase in initial and fixation latency for auditory targets. Peak velocity reached roughly 140 degrees/s in both groups, corresponding to a rate of change of approximately 1 mu s of interaural time difference per millisecond of time. Most notably, hearing-impairment was associated with a large change in the complexity of the movement, changing from smooth sigmoidal trajectories to ones characterized by abruptly changing velocities, directional reversals, and frequent fixation angle corrections...|$|R
40|$|Speech {{recognition}} by cochlear implant users {{can be improved}} by adding an audible low frequency acoustic signal to electrical hearing; the resulting improvement is deemed “electro-acoustic stimulation (EAS) benefit. ” However, a crucial low frequency cue, fundamental frequency (F 0), can be distorted via the <b>impaired</b> <b>auditory</b> <b>system.</b> In order to understand how F 0 distortions may affect EAS benefit, normal-hearing listeners were presented monaurally with vocoded speech (frequencies > 250 [*]Hz) and an acoustical signal (frequencies < 250 [*]Hz) with differing manipulations of the F 0 signal, specifically: a pure tone with the correct mean F 0 but with smaller variations around this mean, or a narrowband of white noise centered around F 0, at varying bandwidths; a pure tone down-shifted in frequency by 50 [*]Hz but keeping overall frequency modulations. Speech-recognition thresholds improved when tones with reduced frequency modulation were presented, and improved significantly for noise bands maintaining F 0 information. A down-shifted tone, or only a tone to indicate voicing, showed no EAS benefit. These results confirm {{that the presence of}} the target's F 0 is beneficial for EAS hearing in a noisy environment, and they indicate that the benefit is robust to F 0 distortion, as long as the mean F 0 and frequency modulations of F 0 are preserved...|$|R
40|$|Background noise, room reflections, or interfering sound sources {{represent}} {{a challenge for}} daily one-to-one communication, particularly for hearing-impaired listeners, even when wearing hearing aid devices. Through a modeling approach, this project investigated how peripheral hearing loss impairs the processing of spatial cues in adverse listening conditions. A binaural model in which the peripheral processor can be tuned to account for individual hearing loss was developed to predict localization in anechoic and reverberant rooms. Hearing impairment was accounted for by a loss of sensitivity, a loss of cochlear compression and reduced frequency selectivity. A spatial cue-selection mechanism processed {{the output of the}} binaural equalization-&-cancellation processor to evaluate the localization information’s reliability based on interaural coherence. The simulations in anechoic environment suggested that the sound-source-location estimates become less reliable and blurred in the case of reduced audibility. Simulations in rooms suggested that the broadening of the auditory filters reduces the fidelity of spectral cues and affects the internal representation of interaural level differences. The model-based analysis of hearing-aid processing showed that amplification and compression used to recover audibility also partially recovered the internal representation of the spatial cues in the <b>impaired</b> <b>auditory</b> <b>system.</b> Future work is needed to extend and experimentally validate the model. Overall, the current model represents a first step towards the development of a dedicated research tool for investigating and understanding the processing of spatial cues in adverse listening conditions, with a long-term goal of contributing to solving the cocktail-party problem for normal hearing and hearing-impaired listeners. 24 page(s...|$|R
40|$|Speech {{perception}} in hearing-impaired listeners can be {{adversely affected by}} various factors including the type of hearing aid signal processing, environmental limitations, and individual differences in the peripheral auditory physiology and higher-level cognitive processes. These factors together influence which aspects of the speech signal are potentially informative and beneficial for the hearing-impaired listener. Particular aspects were discussed {{in the context of}} two hearing aid signal processing techniques: amplitude compression and nonlinear frequency compression. ^ Speech recognition in thirty-six hearing impaired and thirty-seven normal-hearing listeners were evaluated across generic and proprietary methods of amplitude compression in background noise and reverberation. Amplitude compression method had an overall effect on speech recognition, but did not vary across environmental conditions or listener-related differences. However, individual differences across hearing-impaired listeners influenced the use of audibility and temporal envelope information in noisy and reverberant environments. ^ Improvements to existing acoustic models of speech perception were proposed with two neural models. The Neural-scaled entropy (NSE) model has potential to predict speech perception outcomes with nonlinear frequency compression technology in hearing aids, while the neural-signal-to-noise ratio envelope (neural-SNRENV) model has the potential to predict speech perception outcomes in noise due to individual differences in underlying physiology with hearing impairment. ^ In general, results highlight the need to individualize the hearing aid fitting process in order to maximize useful speech information for a hearing-impaired listener. In order to achieve this goal, models based on auditory nerve output can be beneficial in designing measures to predict the nature of speech information being modified by hearing aid signal processing and the environment. These models can also provide insights into the encoding of processed and unprocessed signals in the <b>impaired</b> <b>auditory</b> <b>system.</b> ...|$|R
40|$|Abstract—An ideal {{hearing aid}} for a {{peripheral}} hearing loss would process the incoming signal {{in order to}} give a perfect match between the cochlear outputs of the impaired ear and a reference normal ear. As a first step toward this objective, a model of the normal and <b>impaired</b> peripheral <b>auditory</b> <b>system</b> was used to derive the optimal hearing-aid processing filter based on a minimum mean-squared error criterion. The auditory model includes the compression and suppression effects of the cochlear mechanics and the sensitivity of the neural transduction process. Simplifying assumptions were then incorporated into the processing to yield a practical frequency-dependent adaptive gain system. Processing examples of several individual speech sounds are pre-sented for a flat hearing loss, and the results indicate that a three-channel compression system with adjustable gains and band edges will be close to the optimal solution for this case. Key words: frequency dependent adaptive gain system, hearing aid, hearing aid processing filter, three channel compression system...|$|R
40|$|In this paper, we have {{investigated}} the differences between normal and <b>impaired</b> <b>auditory</b> processing for a frequency discrimination task by analyzing the responses of a computational auditory model using signal detection theory. Two detectors, one using {{all of the information}} in the signal, the other using only the number of neural responses, were implemented. An evaluation of the performance differences between the two theoretical detectors and experimental data may provide insight into quantifying the type of information present in the <b>auditory</b> <b>system</b> as well as whether the human <b>auditory</b> <b>system</b> uses this information efficiently. Results support previous hypotheses that, for low- and mid-range frequencies, the <b>auditory</b> <b>system</b> is able to use temporal information to perform frequency discrimination [8]. The results also suggest that some temporal information is represented in the neural spike train, even at high frequencies. However, the ability of the <b>auditory</b> <b>system</b> to use this information deteriorates at higher frequencies...|$|R
40|$|Hearing {{aids are}} tasked with the {{undesirable}} job of compensating an <b>impaired,</b> highly-nonlinear <b>auditory</b> <b>system.</b> Historically, these devices have either employed linear processing or relatively unsophisticated, nonlinear processing techniques. With increasingly more accurate {{models of the}} <b>auditory</b> <b>system,</b> expanding computational power, and many more objective measures which utilize these models, {{we are at a}} turning point in hearing aid design. Although subjective listener tests are often the most accepted methods for evaluating the quality and intelligibility of speech, they inherently treat the <b>auditory</b> <b>system</b> as a "black box. " Conversely, model-based objective measures typically treat the <b>auditory</b> <b>system</b> as a cascade of physical processes. As a result, objective measures have the potential to provide more detailed information about how sound is processed and about where and why quality or intelligibility breaks down. Provided that we can generalize model-based objective measures, we can use the measures as tools for understanding how to best process degraded signals, and therefore, how to best design hearing aids. However, generalizability is a key requirement. Since many of the well-known objective measures have been developed for normal-hearing listeners in the context of audio codecs, we are unsure about the generalizability of these measures to predicting quality and intelligibility for hearing-impaired listeners with "unknown" datasets (i. e. a set on which it was not trained) and distortions which are specific to hearing aids. Relatively recently, however, Kates and Arehart (Journal of the Audio Engineering Society, 2010) proposed the Hearing Aid Speech Quality Index (HASQI), which is a model-based objective measure that predicts quality for normal-hearing and hearing-impaired listeners by taking into account many of the distortions which hearing aids introduce. HASQI solves many of our concerns of generalizability for predicting quality, but it still remains to test HASQI's ability to predict quality with datasets on which it was not trained. Thus, we explore the robustness of HASQI by testing its ability to predict quality for "unknown" de-noised speech, and we directly compare its performance to some other metrics in the literature. M. S. Committee Chair: Rozell, Christopher; Committee Member: Anderson, David; Committee Member: Clements, Mar...|$|R
40|$|To avoid {{mortality}} {{caused by}} passage through dam turbines and spillways, juvenile Chinook salmon Oncorhynchus tshawytscha are annually transported downstream by barge through the federal hydropower {{system on the}} Snake and Columbia rivers. Survival of transported fish is {{higher than that of}} in-river migrants; however, transported fish experience higher rates of postrelease mortality. Increased mortality could result from a decrease in the ability to detect or avoid predators due to stressors associated with the barge environment. This study examined the effects of barging on juvenile Chinook salmon olfaction and auditory function, two sensory systems involved in predator detection. We focused on dissolved metals known to be toxic to the salmon olfactory system and on the level of noise from the barge, which could <b>impair</b> the <b>auditory</b> <b>system.</b> Experimental groups included animals collected (1) before barge loading (control group), (2) at the Bonneville Dam bypass system (migrant fish), (3) immediately after barge transport, and (4) within 7 d postbarging and at or after 7 d postbarging. Measured concentrations of dissolved metals from the water within the barge were below established water quality criteria for the protection of aquatic life. Moreover, ultrastructural examination of the olfactory epithelium surface showed no evidence of injury to olfactory sensory neurons. Noise in the barge holding tanks had levels up to 136 dB referenced to 1 mu Pa (root mean square) with primary energy below 400 Hz. Auditory sensitivity was measured using the auditory-evoked potentials (AEP) technique. We found a small but statistically significant threshold shift for fish collected within 7 d postbarging, while in the 7 -d-and-later postbarging group the AEP thresholds were similar to the control. Our findings indicate that the olfactory systems of transported Chinook salmon are intact and probably functional, while the auditory sensitivities are compromised with probable recovery...|$|R
40|$|ABSTRACT—Auditory {{processing}} {{forms the}} basis of humans’ ability to engage in complex behaviors such as under-standing spoken language or playing a musical instru-ment. Auditory processing is not a rigid, encapsulated process; rather, it interacts intimately with other neural systems and is affected by experience, environmental in-fluences, and active training. Auditory processing is re-lated to language and cognitive function, and <b>impaired</b> <b>auditory</b> processing negatively affects {{the quality of life}} of many people. Recent studies suggest that the malleability of the <b>auditory</b> <b>system</b> may be used to study the interaction between sensory and cognitive processes and to enhance human well-being. KEYWORDS—perceptual learning; plasticity; training Auditory processing refers to the broad range of sensory and perceptual skills used to extract meaningful information from sound. Traditionally, the initial stages of auditory processing were attributed to a passive system automatically encoding the physical properties of sound in a bottom-up hierarchical manner (that is, from peripheral to more central structures). Here, we discuss evidence to the contrary: Not only are most stages of auditory processing susceptible to change resulting from either long- or short-term experiences, many of these changes are mediated in a top-down fashion (that is, in a manner consistent with the influence of higher-level cognitive factors such as at-tention, memory and context), allowing even low levels of the <b>auditory</b> <b>system</b> to encode sound in a context-specific manner. This dynamic processing is achieved through the intricate an-atomical and functional connections between auditory and other brain areas and between cortical and subcortical areas within the <b>auditory</b> <b>system...</b>|$|R
40|$|No sponsorships or {{competing}} interests have been disclosed for this article. Age-related dysfunction {{of the central}} <b>auditory</b> <b>system</b> (central presbycusis) is common but rarely looked for by those who provide aural rehabilitation. Patients who complain of difficulty hearing in noise—the key symptom of central presbycusis—are generally disadvantaged with conventional rehabilitation. This symptom should be documented with commercially available speech-in-noise tests, which use materials that are uncompli-cated to administer. Those patients who perform poorly on such tests should have a customized rehabilitation program aimed at optimizing their remaining communication abilities. Otolaryngologists who provide auditory rehabilitation may wish to consider expanding their practices to meet the communica-tion needs of older patients with central presbycusis. Central presbycusis is an emerging area for basic and clinical research in auditory neurotology, particularly in the relation of cognitive dysfunction to <b>impaired</b> <b>auditory</b> processing. Keywords age-related hearing loss, presbycusis, central presbycusis, executive function, dementi...|$|R
25|$|Hearing {{begins in}} utero, but the central <b>auditory</b> <b>system</b> {{continues}} to develop {{for at least}} the first decade. There is considerable interest in the idea that disruption to hearing during a sensitive period may have long-term consequences for auditory development. One study showed thalamocortical connectivity in vitro was associated with a time sensitive developmental window and required a specific cell adhesion molecule (lcam5) for proper brain plasticity to occur. This points to connectivity between the thalamus and cortex shortly after being able to hear (in vitro) as at least one critical period for auditory processing. Another study showed that rats reared in a single tone environment during critical periods of development had permanently <b>impaired</b> <b>auditory</b> processing. ‘Bad’ auditory experiences, such as temporary deafness by cochlear removal in rats leads to neuron shrinkage. In a study looking at attention in APD patients, children with one ear blocked developed a strong right-ear advantage but were not able to modulate that advantage during directed-attention tasks.|$|R
2500|$|Genetic {{disruption}} of the Kcne3 gene in mice impairs intestinal cyclic AMP-stimulated chloride secretion via {{disruption of}} intestinal KCNQ1-KCNE3 channels that are important for regulating the chloride current. [...] KCNE3 also performs a similar function in mouse tracheal epithelium. Kcne3 deletion in mice also predisposes to ventricular arrhythmogenesis, but KCNE3 expression is not detectable in mouse heart. The mechanism for ventricular arrhythmogenesis was demonstrated to be indirect, and associated with autoimmune attack of the adrenal gland and secondary hyperaldosteronism (KCNE3 is not detectable in the adrenal gland). [...] The elevated serum aldosterone predisposes to arrhythmias triggered in a coronary artery ligation ischemia/reperfusion injury model. [...] Blockade of the aldosterone receptor with spironolactone removed the ventricular arrhythmia predisposition in Kcne3-/- mice. [...] Kcne3 deletion also <b>impairs</b> <b>auditory</b> function because {{of the loss of}} regulation of Kv4.2 channels by KCNE3 in spiral ganglion neurons (SGNs) of the <b>auditory</b> <b>system.</b> KCNE3 is thought to regulate SGN firing properties and membrane potential via its modulation of Kv4.2.|$|R
50|$|Hearing {{begins in}} utero, but the central <b>auditory</b> <b>system</b> {{continues}} to develop {{for at least}} the first decade. There is considerable interest in the idea that disruption to hearing during a sensitive period may have long-term consequences for auditory development. One study showed thalamocortical connectivity in vitro was associated with a time sensitive developmental window and required a specific cell adhesion molecule (lcam5) for proper brain plasticity to occur. This points to connectivity between the thalamus and cortex shortly after being able to hear (in vitro) as at least one critical period for auditory processing. Another study showed that rats reared in a single tone environment during critical periods of development had permanently <b>impaired</b> <b>auditory</b> processing. ‘Bad’ auditory experiences, such as temporary deafness by cochlear removal in rats leads to neuron shrinkage. In a study looking at attention in APD patients, children with one ear blocked developed a strong right-ear advantage but were not able to modulate that advantage during directed-attention tasks.|$|R
40|$|Introduction: The transient-evoked otoacoustic {{emissions}} (TEOAE) {{have been}} the most widespread technique to perform neonatal hearing screening. Scrutinizing their measures by way of an association with other alterations that may <b>impair</b> the infant's <b>auditory</b> <b>system</b> is important. Objective: Analyze the incidence and the response levels of the transient-evoked otoacoustic emissions on infants having a physiological gastroesophageal reflux disease (GERD). Method: A prospective study was performed at Santa Juliana Hospital's Otorhinolaryngology Department. 118 prematurely-born and timely-born babies, from newly-born to 6 months old, who were sent by pediatricians and gastropediatricians, participated in the study and they were divided into two groups: Study Group: 63 infants clinically diagnosed of a physiological gastroesophageal reflux disease, and Control Group: 55 infants without a physiological gastroesophageal reflux. The peripheral hearing function was evaluated by both transient-evoked otoacoustic emissions and otoscopy examinations performed by an otorhinolaryngologist. Results: The average response levels of the transient-evoked otoacoustic emissions were higher in the non-reflux group for frequency bands of 2 kHz, 2. 5 kHz, 3 kHz, 3. 5 kHz and 4. 5 kHz bilaterally, with a statistically significant difference, achieving the average values of 7. 71 dB and 7 dB in the right ear found in the frequency bands of 2 and 4 kHz, respectively. Conclusion: There was a lower incidence and a lower response level of the transient-evoked otoacoustic emissions in physiological gastroesophageal reflux children in comparison with children having no reflux...|$|R
50|$|In his work, Goldstein studied transcortical {{sensory aphasia}} (TSA), characterizing it as <b>impaired</b> <b>auditory</b> comprehension, with intact {{repetition}} and fluent speech. Goldstein studied word comprehension {{in patients with}} aphasia, theorizing that naming shows relatively little specificity {{to the site of}} lesion within the left hemisphere.|$|R
40|$|Abstract — The human <b>auditory</b> <b>system</b> {{is divided}} into three main parts: outer, middle and inner <b>auditory</b> <b>system.</b> The <b>auditory</b> <b>system</b> is mainly {{responsible}} for hearing and balance. The Cochlea, which is present in the inner <b>auditory</b> <b>system,</b> consists of Basilar Membrane, Organ of Corti, OHC and IHC together execute the hearing mechanism. Otoacoustic Emissions are inaudible sounds which come out from the inner <b>auditory</b> <b>system.</b> These are produced due to Cochlear amplification wherein OHCs play an important role by providing electro-mechanical positive feedback. OAEs are mainly used for diagnosis of hearing sensitivity in new-born babies and adults. They are not produced for persons having hearing disabilities more than 25 - 30 dB. The OAEs are classified into Spontaneous and Evoked types. The main objective of this project is to generate Transient-Evoked and Sustained-Frequency OAEs by simulating a human inner <b>auditory</b> <b>system</b> model. The basic underlying principle of modeling the human inner <b>auditory</b> <b>system</b> used in this project is Hydrodyamics. Hydrodynamics is the study of motion of incompressible and compressible fluids. The incompressible fluids present inside the cochlea, Endolymph and Perilymph, are responsible for the movement of the BM [...] Keyword-OHC,IHC,Baislar Membrane,OAE,Hydrodynamic model I...|$|R
40|$|The human <b>auditory</b> <b>system</b> {{is divided}} into three main parts: outer, middle and inner <b>auditory</b> <b>system.</b> The <b>auditory</b> <b>system</b> is mainly {{responsible}} for hearing and balance. The Cochlea, which is present in the inner <b>auditory</b> <b>system,</b> consists of Basilar Membrane, Organ of Corti, OHC and IHC together execute the hearing mechanism. Otoacoustic Emissions are inaudible sounds which come out from the inner <b>auditory</b> <b>system.</b> These are produced due to Cochlear amplification wherein OHCs play an important role by providing electro-mechanical positive feedback. OAEs are mainly used for diagnosis of hearing sensitivity in new-born babies and adults. They are not produced for persons having hearing disabilities more than 25 - 30 dB. The OAEs are classified into Spontaneous and Evoked types. The main objective of this project is to generate Transient-Evoked and Sustained- Frequency OAEs by simulating a human inner <b>auditory</b> <b>system</b> model. The basic underlying principle of modeling the human inner <b>auditory</b> <b>system</b> used in this project is Hydrodyamics. Hydrodynamics is the study of motion of incompressible and compressible fluids. The incompressible fluids present inside the cochlea, Endolymph and Perilymph, are responsible for the movement of the BM. ...|$|R
50|$|The sound {{localization}} {{mechanisms of}} the mammalian <b>auditory</b> <b>system</b> have been extensively studied. The <b>auditory</b> <b>system</b> uses several cues for sound source localization, including time- and level-differences (or intensity-difference) between both ears, spectral information, timing analysis, correlation analysis, and pattern matching.|$|R
40|$|Elderly humans {{often not}} only {{experience}} peripheral hearing loss but also suffer from more central deficits in temporal auditory processing affecting speech perception. <b>Impaired</b> <b>auditory</b> temporal resolution {{has also been}} observed in old rodents. Other studies have demonstrated a reduction of GABAergic function in the auditory pathway of old animals. Here we test the hypothesis that deficits in the GABAergic <b>system</b> affect central <b>auditory</b> processing. Our data suggests that pharmacological augmentation of the GABAergic <b>system</b> ameliorates <b>impaired</b> temporal <b>auditory</b> processing in the gerbil and might be a strategy {{for the treatment of}} at least some forms of central hearing loss in humans...|$|R
40|$|Hypothesis: The {{role of the}} corticofugal efferent <b>auditory</b> <b>system</b> in {{the origin}} or {{maintenance}} of tinnitus is currently mostly overlooked. Changes in the balance between excitation and inhibition after an auditory trauma are likely {{to play a role}} in the origin of tinnitus. The efferent <b>auditory</b> <b>system</b> can be expected to be involved in such changes. Background: The goal of this article was to investigate the current knowledge of the functional efferent <b>auditory</b> <b>system</b> in humans, mostly based on animal research, and to look for new possibilities to try to answer the question of the specific role(s) of the corticofugal efferent <b>auditory</b> <b>system</b> in tinnitus. Methods: Literature review. Results: Several suggestions for future research are made, for studies in humans as well as in animals. Conclusion: We think that it will be worthwhile to investigate the efferent <b>auditory</b> <b>system</b> and its relations to tinnitus in the near future. With this article, we hope to inspire such work...|$|R
40|$|The <b>auditory</b> <b>system</b> {{recognized}} {{sound waves}}. The sound waves are longitudinal {{waves in the}} air, where the pressure varies in time. It is distinguished as the rapid pressure changes, referred to as ‘fine structure’, and slower overall changes of amplitude fluctuations, as ‘envelope’. The <b>auditory</b> <b>system</b> has a limited ability to follow the time-varying envelope, and this ability is known as ‘temporal resolution’. Our <b>auditory</b> <b>system</b> analyzes sound waves in frequency, in-tensity, and time domain. The understanding about frequency and intensity domain is relatively easy compare to time domain. Hearing threshold is measured by sound intensity in frequency domain. However the speech discrimination {{and understanding of the}} sentence in quiet and noise are associated with temporal resolution. So for the comprehensive understanding about the <b>auditory</b> <b>system</b> and hearing ability, we must extend our knowledge to the temporal ability of the <b>auditory</b> <b>system.</b> Korean J Otorhinolaryngol-Head Neck Surg 2011; 54 : 585 - 9...|$|R
5000|$|The <b>auditory</b> <b>system</b> can {{extract the}} sound of a desired sound source out of interfering noise. So the <b>auditory</b> <b>system</b> can {{concentrate}} on only one speaker if other speakers are also talking (the cocktail party effect). With the help of the cocktail party effect sound from interfering directions is perceived attenuated compared to the sound from the desired direction. The <b>auditory</b> <b>system</b> can increase the signal-to-noise ratio by up to 15 dB, which means that interfering sound is perceived to be attenuated to half (or less) of its actual loudness.|$|R
5000|$|AIDS and ARC {{patients}} frequently experience <b>auditory</b> <b>system</b> anomalies.|$|R
5000|$|... #Caption: Diagram {{of signal}} {{processing}} in the <b>auditory</b> <b>system</b> ...|$|R
50|$|The {{specificity}} of the <b>auditory</b> <b>system</b> {{is proportional to}} the range of frequencies found in the resting call frequency (RF) of the bat. Therefore, the narrow range of frequencies found within the RF of a CF bat is reflected in the sharp sensitivity of the <b>auditory</b> <b>system</b> (the <b>auditory</b> fovea).|$|R
5000|$|Sound {{goes from}} speaker's mouth to hearer's ear <b>auditory</b> <b>system</b> ...|$|R
5000|$|... #Subtitle level 2: Sound {{localization}} by {{the human}} <b>auditory</b> <b>system</b> ...|$|R
30|$|The oversubtracted {{spectral}} subtraction {{reduces the}} noise {{to some extent}} but the musical noise is not completely eliminated, effecting {{the quality of the}} speech signal. There is a trade-off between the amount of noise reduction and speech distortion. The perceptual based techniques help in reducing the noise by taking advantage of the masking properties of the <b>auditory</b> <b>system.</b> In order to further enhance the quality, the noise and tones are masked and critical band variance normalization is performed by incorporating the masking properties of human <b>auditory</b> <b>system.</b> The human <b>auditory</b> <b>system</b> does not perceive all the frequencies in the similar way, and is limited to mask certain sounds in the presence of competitive sounds. The two main properties of the human <b>auditory</b> <b>system</b> that make up the psychoacoustic model are: absolute threshold of hearing and auditory masking.|$|R
5000|$|... #Subtitle level 4: Limitation of Weber's {{law in the}} <b>auditory</b> <b>system</b> ...|$|R
