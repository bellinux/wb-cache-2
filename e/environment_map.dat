297|1417|Public
500|$|Production of Crash Bandicoot: Warped {{began on}} January 1998, with Naughty Dog given only ten {{and a half}} months and a budget of 2.2 million to {{complete}} the game. Crash Bandicoot - Time Line ] |publisher=Naughty Dog |accessdate=April 4, 2010 |deadurl=yes |df= }} Programmers Andy Gavin, Stephen White and Greg Omi created three new gameplay engines for the game. Two of the three new engines were three-dimensional in nature and were created for the airplane and jet-ski levels; the third new engine was created for the motorcycle levels in the style of a driving simulator. The new engines combined make up a third of the game, while the other two-thirds of the game consist of the same engine used in the previous games. Jason Rubin explained that the [...] "classic" [...] engine and game style was preserved due {{to the success of the}} previous two games and went on to say that [...] "were we to abandon that style of gameplay, that would mean that we would be abandoning a significant proportion of gamers out there." [...] An arbitrary plane z-buffer was created for the jet-ski and flooded Egyptian hallway levels of the game. To create a completely fluid feel for the water on these levels, an <b>environment</b> <b>map</b> that reflects the sky was fitted onto the surface of the water. A real shadow was given to the Crash character at the request of the Sony Computer Entertainment America producers, who were [...] "sick of that little discus that's following him around." [...] To create an [...] "arcade" [...] experience in the airplane levels and to differentiate them from flight simulators, the enemy planes were programmed to come out in front of the player and give the player ample time to shoot them before they turn around and shoot the player rather than come up behind the player and hit them from behind. The Relic system was introduced to give players a reason to return to the game after it has been completed.|$|E
50|$|It can be {{implemented}} as an <b>environment</b> <b>map</b> using radiating pie wedges as the source texture.|$|E
50|$|Unfortunately, this {{technique}} does not scale well when multiple reflective objects are present. A unique dynamic <b>environment</b> <b>map</b> is usually required for each reflective object. Also, further complications are added if reflective objects can reflect each other - dynamic cube maps can be recursively generated approximating the effects normally generated using raytracing.|$|E
40|$|Different {{methods for}} prefiltered <b>environment</b> <b>maps</b> have been proposed, {{each of which}} has {{different}} advantages and disadvantages. We present a general notation for prefiltered <b>environment</b> <b>maps,</b> which will be used to classify and compare the existing methods. Based on that knowledge we develop three new algorithms: 1. A fast hierarchical prefiltering method that can be utilized for all previously proposed prefiltered <b>environment</b> <b>maps.</b> 2. A technique for hardware-accelerated prefiltering of <b>environment</b> <b>maps</b> that achieves interactive rates even on low-end workstations. 3. Anisotropic <b>environment</b> <b>maps</b> using the Banks model. 1 Introduction <b>Environment</b> <b>maps</b> [3] are a widely used technique to approximate reflections in interactive rendering. Although <b>environment</b> <b>maps</b> make the assumption that the reflected environment is far away [...] - thus being an approximation [...] - they often nevertheless achieve convincing reflections. Recently <b>environment</b> <b>maps</b> have been introduced as a means to r [...] ...|$|R
40|$|Static <b>environment</b> <b>maps</b> fail {{to capture}} local {{reflections}} including effects like selfreflections and parallax in the reflected imagery. We instead propose parameterized <b>environment</b> <b>maps</b> (PEMs), {{a set of}} per-view <b>environment</b> <b>maps</b> which accurately reproduce local reflections at each viewpoint as computed by an offline ray tracer. Even with a small set of viewpoint samples, PEMs support plausible movement away from and between the pre-rendered viewpoint samples while maintaining local reflections. They also make use of <b>environment</b> <b>maps</b> supported in graphics hardware to provide real-time exploration of the pre-rendered space. In addition to parameterization by viewpoint, our notion of PEMextends to general, multidimensional parameterizations of the scene, including relative motions of objects and lighting changes. Our contributions include a technique for inferring <b>environment</b> <b>maps</b> providing a close match to ray-traced imagery. We also explicitly infer and encode all MIPMAP levels of the PEMs to achieve higher accuracy. We propose layered <b>environment</b> <b>maps</b> that separate local and distant reflected geometry. We explore several types of <b>environment</b> <b>maps</b> including finite spheres, ellipsoids, and boxes that better approximate the environmental geometry. We demonstrate results showing faithful local reflections in an interactive viewer...|$|R
50|$|Real-time CG {{shadowing}} and <b>environment</b> <b>mapping.</b>|$|R
50|$|Environment mapping {{is a form}} of {{texture mapping}} in which the texture {{coordinates}} are view-dependent. One common application, for example, is to simulate reflection on a shiny object. One can <b>environment</b> <b>map</b> the interior of a room to a metal cup in a room. As the viewer moves about the cup, the texture coordinates of the cup’s vertices move accordingly, providing the illusion of reflective metal.|$|E
5000|$|In {{addition}} to the ambient occlusion value, a [...] "bent normal" [...] vector [...] is often generated, which points in the average direction of unoccluded samples. The bent normal {{can be used to}} look up incident radiance from an <b>environment</b> <b>map</b> to approximate image-based lighting. However, there are some situations in which the direction of the bent normal is a misrepresentation of the dominant direction of illumination, e.g., ...|$|E
50|$|If normal mapping is used, each polygon {{has many}} face normals (the {{direction}} a given point on a polygon is facing), {{which can be}} used in tandem with an <b>environment</b> <b>map</b> to produce a more realistic reflection. In this case, the angle of reflection at a given point on a polygon will take the normal map into consideration. This technique is used to make an otherwise flat surface appear textured, for example corrugated metal, or brushed aluminium.|$|E
5000|$|<b>Environment</b> <b>maps,</b> bump maps, transparency, specularity amongst others ...|$|R
5000|$|In Computer Graphics, {{circular}} fisheye {{images can}} be used to create <b>environment</b> <b>maps</b> from the physical world. One complete 180-degree wide angle fisheye image will fit to half of cubic mapping space using the proper algorithm. <b>Environment</b> <b>maps</b> {{can be used to}} render 3D objects and virtual panoramic scenes.|$|R
40|$|International audienceIn {{the context}} of virtual reality, the {{simulation}} of complex environments with many animated objects {{is becoming more and}} more common. Virtual reality applications have always promoted the development of new efficient algorithms and image-based rendering techniques for real-time interaction. In this paper, we propose a technique which allows the real-time simulation in a city of the reflections of static geometry (eg. building) on specular dynamic objects (vehicles). For this, we introduce the idea of multiple <b>environment</b> <b>maps.</b> We pre-compute a set of reference <b>environment</b> <b>maps</b> at strategic positions in the scene, that are used at run time and for each visible dynamic object, to compute local <b>environment</b> <b>maps</b> by resampling images. To efficiently manage a small number of reference <b>environment</b> <b>maps,</b> compared to the scene dimension, for each vertex of the reconstructed environment we perform a ray tracing in a heightfield representation of the scene. We control the frame rate by adaptative reconstruction of <b>environment</b> <b>maps.</b> We have implemented this approach, and the results show that it is efficient and scalable to many dynamic objects while maintaining interactive frame rates...|$|R
5000|$|Engineers use {{reflection}} {{lines to}} judge a surface's quality. Reflection lines reveal surface flaws, particularly discontinuities in normals indicating that the surface is not [...] Reflection lines may be created and examined on physical surfaces or virtual surfaces {{with the help of}} computer graphics. For example, the shiny surface of an automobile body is illuminated with reflection lines by surrounding the car with parallel light sources. Virtually, a surface can be rendered with reflection lines by modulating the surfaces point-wise color according to a simple calculation involving the surface normal, viewing direction and a square wave <b>environment</b> <b>map.</b>|$|E
50|$|The {{reflection}} mapping approach {{is more efficient}} than the classical ray tracing approach of computing the exact reflection by tracing a ray and following its optical path. The reflection color used in the shading computation at a pixel is determined by calculating the reflection vector at the point on the object and mapping it to the texel in the <b>environment</b> <b>map.</b> This technique often produces results that are superficially similar to those generated by raytracing, but is less computationally expensive since the radiance value of the reflection comes from calculating the angles of incidence and reflection, followed by a texture lookup, rather than followed by tracing a ray against the scene geometry and computing the radiance of the ray, simplifying the GPU workload.|$|E
5000|$|Like the first, {{the second}} game was a {{commercial}} success, green-lighting a third game. Production of Crash Bandicoot: Warped began in January 1998, with Naughty Dog given only 10½ months to complete the game. Programmers Andy Gavin, Stephen White and Greg Omi created three new gameplay engines for the game. Two of the three new engines were three-dimensional in nature and were created for the airplane and jet-ski levels; the third new engine was created for the motorcycle levels {{in the style of}} a driving simulator. The new engines combined make up a third of the game, while the other two-thirds of the game consist of the tweaked engine used in the previous games. Jason Rubin explained that the [...] "classic" [...] engine and game style was preserved due {{to the success of the}} previous two games and went on to say that [...] "were we to abandon that style of gameplay, that would mean that we would be abandoning a significant proportion of gamers out there". An arbitrary plane z-buffer was created for the jet-ski and flooded Egyptian hallway levels of the game. To create a completely fluid feel for the water on these levels, an <b>environment</b> <b>map</b> that reflects the sky was fitted onto the surface of the water. A real shadow was given to the Crash character at the request of the Sony Computer Entertainment America producers, who were [...] "sick of that little discus that's following him around." [...] To create an [...] "arcade" [...] experience in the airplane levels and to differentiate them from flight simulators, the enemy planes were programmed to come out in front of the player and give the player ample time to shoot them before they turn around and shoot the player rather than come up behind the player and hit them from behind. The Relic system was introduced to give players a reason to return to the game after it has been completed.|$|E
40|$|Existing <b>environment</b> <b>mapping</b> {{techniques}} include spherical mapping and cube mapping. These {{techniques have}} inherent flaws that cause sampling issues and aliasing. Continuous cube mapping is offered {{as an alternative}} <b>environment</b> <b>mapping</b> approach that effectively folds the cube onto the sphere, providing a better parameterization of cube mapping. We provide a hardware implementation. 1...|$|R
40|$|<b>Environment</b> <b>maps,</b> like texture maps or {{any other}} maps {{consisting}} of discretely stored data have to be properly filtered, if they are being resampled {{in the process of}} rendering an image. For <b>environment</b> <b>maps,</b> this is especially important, as the sampling rate is subject to extreme changes due to the curvature of the reflecting surfaces...|$|R
50|$|Gene Miller {{experimented with}} spherical <b>environment</b> <b>mapping</b> in 1982 at MAGI Synthavision.|$|R
5000|$|Production of Crash Bandicoot: Warped {{began on}} January 1998, with Naughty Dog given only 10½ months and {{a budget of}} $2.2 million to {{complete}} the game. Programmers Andy Gavin, Stephen White and Greg Omi created three new gameplay engines for the game. Two of the three new engines were three-dimensional in nature and were created for the airplane and jet-ski levels; the third new engine was created for the motorcycle levels {{in the style of}} a driving simulator. The new engines combined make up a third of the game, while the other two-thirds of the game consist of the same engine used in the previous games. Jason Rubin explained that the [...] "classic" [...] engine and game style was preserved due {{to the success of the}} previous two games and went on to say that [...] "were we to abandon that style of gameplay, that would mean that we would be abandoning a significant proportion of gamers out there." [...] An arbitrary plane z-buffer was created for the jet-ski and flooded Egyptian hallway levels of the game. To create a completely fluid feel for the water on these levels, an <b>environment</b> <b>map</b> that reflects the sky was fitted onto the surface of the water. A real shadow was given to the Crash character at the request of the Sony Computer Entertainment America producers, who were [...] "sick of that little discus that's following him around." [...] To create an [...] "arcade" [...] experience in the airplane levels and to differentiate them from flight simulators, the enemy planes were programmed to come out in front of the player and give the player ample time to shoot them before they turn around and shoot the player rather than come up behind the player and hit them from behind. The Relic system was introduced to give players a reason to return to the game after it has been completed.|$|E
40|$|From left to right, the {{original}} <b>environment</b> <b>map,</b> the approximate area lights, our results, and a reference image. Environment maps are a popular method of reproducing complex natural lighting. However, current methods for hardware <b>environment</b> <b>map</b> shadows depend on significant pre-computation and cannot support dynamic ob-jects. This work presents a pre-process that decomposes an <b>environment</b> <b>map</b> into two components: {{a set of}} area lights and an ambient map. Once the map is split into these components, each is rendered with an appropriate mechanism. The area lights are rendered using an existing hardware-accelerated soft-shadow algorithm; for our implementation we use penumbra wedges [AMA 02]. The ambient region is rendered using pre-integrated irra-diance mapping. Using an NVidia 6800 on a standard desktop, we demonstrate high-quality <b>environment</b> <b>map</b> shadows for dynamic scenes at interactive rates...|$|E
40|$|A {{radiance}} <b>environment</b> <b>map</b> pre-integrates {{a constant}} surface reflectance with the lighting environment. It {{has been used}} to generate photo-realistic rendering at interactive speed. However, one of its limitations is that each radiance <b>environment</b> <b>map</b> can only render the object which has the same surface reflectance as what it integrates. In this paper, we present a ratio-image based technique to use a radiance <b>environment</b> <b>map</b> to render diffuse objects with different surface reflectance properties. This method has the advantage that it does not require the separation of illumination from reflectance, and it is simple to implement and runs at interactive speed...|$|E
5000|$|The {{ability to}} index texture <b>maps,</b> <b>environment</b> <b>maps,</b> and shadow depth maps ...|$|R
50|$|In some cases, a hemicube {{may be used}} in <b>environment</b> <b>mapping</b> or {{reflection}} mapping.|$|R
5000|$|... {{supports}} per-pixel lighting techniques: normal mapping, virtual displacement <b>mapping,</b> <b>environment</b> <b>mapping</b> or parametrized Phong lighting ...|$|R
40|$|We {{present a}} system which allows {{wearable}} computer users to share their views of their current environments with each other. Our system uses an EyeTap: a device which allows {{the eye of the}} wearer to function both as a camera and a display. A wearer, by looking around his/her environment, "paints" or "builds" an <b>environment</b> <b>map</b> composed of images from the EyeTap device, along with head [...] tracking information recording the orientation of each image. The head [...] tracking algorithm uses a featureless image motion estimation algorithm coupled with a head mounted gyroscope. The <b>environment</b> <b>map</b> is then transmitted to another user, who, through their own head-tracking EyeTap system, browses the first user's environment solely by head motion, seeing the environment as though it were their own. As a result of browsing the transmitted <b>environment</b> <b>map,</b> the viewer builds and extends his/her own <b>environment</b> <b>map,</b> and thus this is a data [...] producing head [...] tracking system. These environment maps can then be shared reciprocally between wearers...|$|E
40|$|A {{radiance}} <b>environment</b> <b>map</b> pre-integrates {{a constant}} surface reflectance with the lighting environment. It {{has been used}} to generate photo-realistic rendering at interactive speed. However, one of its limitations is that each radiance <b>environment</b> <b>map</b> can only render the object which has the same surface reflectance as what it integrates. In this paper, we present a ratio-image based technique to use a radiance <b>environment</b> <b>map</b> to render diffuse objects with different surface reflectance properties. This method has the advantage that it does not require the separation of illumination from reflectance, and it is simple to implement and runs at interactive speed. In order to use this technique for human face relighting, we have developed a technique that uses spherical harmonics to approximate the radiance <b>environment</b> <b>map</b> for any given image of a face. Thus we are able to relight face images when the lighting environment rotates. Another benefit of the radiance <b>environment</b> <b>map</b> is that we can interactively modify lighting by changing the coefficients of the spherical harmonics basis. Finally we can modify the lighting condition of one person’s face so that it matches the new lighting condition of a different person’s face image assuming the two faces have similar skin albedos. ...|$|E
40|$|Environment {{maps are}} a popular method of {{reproducing}} complex natural lighting. However, current methods for hardware <b>environment</b> <b>map</b> shadows depend on significant pre-computation and cannot support dynamic objects. This work presents a pre-process that decomposes an <b>environment</b> <b>map</b> into two components: {{a set of}} area lights and an ambient map. Once the map is split into these components, each is rendered with an appropriate mechanism. The area lights are rendered using an existing hardware-accelerated soft-shadow algorithm; for our implementation we use penumbra wedges. The ambient region is rendered using pre-integrated irradiance mapping. Using an NVidia 6800 on a standard desktop, we demonstrate high-quality <b>environment</b> <b>map</b> shadows for dynamic scenes at interactive rates...|$|E
40|$|In {{the context}} of virtual reality, the {{simulation}} of complex environments with many animated objects {{is becoming more and}} more common. Virtual reality applications have always promoted the development of new ecient algorithms and image-based rendering techniques for real-time interaction. In this paper, we propose a technique which allows the real-time simulation in a city of the re ections of static geom-etry (eg. building) on specular dynamic objects (vehicles). For this, we introduce the idea of multiple <b>environment</b> <b>maps.</b> We pre-compute a set of reference <b>environment</b> <b>maps</b> at strategic positions in the scene, that are used at run time and for each visible dynamic object, to compute local envi-ronment maps by resampling images. To eciently manage a small number of reference <b>environment</b> <b>maps,</b> compared to the scene dimension, for each vertex of the reconstructed en-vironment we perform a ray tracing in a heighteld represen-tation of the scene. We control the frame rate by adaptative reconstruction of <b>environment</b> <b>maps.</b> We have implemented this approach, and the results show that it is ecient and scalable to many dynamic objects while maintaining inter-active frame rates...|$|R
5000|$|Additional features: bump mapping, fog, alpha-blending (transparency), {{mip mapping}} (polygon-texture auto switch), tri-*linear filtering, anti-aliasing, <b>environment</b> <b>mapping,</b> and {{specular}} effect ...|$|R
5000|$|Cube mapping {{was first}} {{proposed}} in 1986 by Ned Greene in his paper “Environment Mapping and Other Applications of World Projections”, {{ten years after}} <b>environment</b> <b>mapping</b> was first put forward by Jim Blinn and Martin Newell. However, hardware limitations {{on the ability to}} access six texture images simultaneously made it infeasible to implement cube mapping without further technological developments. This problem was remedied in 1999 with the release of the Nvidia GeForce 256. Nvidia touted cube mapping in hardware as “a breakthrough image quality feature of GeForce 256 that ... will allow developers to create accurate, real-time reflections. Accelerated in hardware, cube <b>environment</b> <b>mapping</b> will free up the creativity of developers to use reflections and specular lighting effects to create interesting, immersive environments.” Today, cube mapping is still used in a variety of graphical applications as a favored method of <b>environment</b> <b>mapping.</b>|$|R
40|$|In this appendix, we {{describe}} {{the details of the}} illuminant calibration step for the sky and the sun. First, because our model separates sun light from sky light, we need to remove sun pixels from the <b>environment</b> <b>map.</b> We define the sun position as the barycenter of the saturated sun pixels, and use inpainting to fill-in these saturated pixels from their neighbors. Since our model also separates sky light from indirect light, we use a standard color selection tool to label sky pixels that will contribute to the sky illumination, while other pixels (building, trees) will contribute to indirect lighting. This is illustrated in Fig. 5 c. Second, we align the <b>environment</b> <b>map</b> and sun with the reconstructed scene. To do so we manually mark a vertical edge of the reconstructed geometry and rotate the <b>environment</b> <b>map</b> and sun until the cast shadow of th...|$|E
40|$|We {{introduce}} structured importance sampling, a {{new technique}} for efficiently rendering scenes illuminated by distant natural illumination given in an <b>environment</b> <b>map.</b> Our method handles occlusion, high-frequency lighting, and is significantly faster than alternative methods based on Monte Carlo sampling. We achieve this speedup {{as a result of}} several ideas. First, we present a new metric for stratifying and sampling an <b>environment</b> <b>map</b> taking into account both the illumination intensity as well as the expected variance due to occlusion within the scene. We then present a novel hierarchical stratification algorithm that uses our metric to automatically stratify the <b>environment</b> <b>map</b> into regular strata. This approach enables a number of rendering optimizations, such as pre-integrating the illumination within each stratum to eliminate noise at the cost of adding bias, and sorting the strata {{to reduce the number of}} sample rays. We have rendered several scenes illuminated by natural lighting, and our results indicate that structured importance sampling is better than the best previous Monte Carlo techniques, requiring one to two orders of magnitude fewer samples for the same image quality...|$|E
40|$|The {{acquisition}} of surround-view panoramas using a single handheld or head-worn camera relies on robust real-time camera orientation tracking. In absence of robust tracking recovery methods, the complete acquisition process {{has to be}} re-started when tracking fails. This paper presents methodology for camera orientation relocalization, using virtual keyframes for online <b>environment</b> <b>map</b> construction. Instead of relying on real keyframes from incoming video, the proposed approach enables camera orientation relocalization by employing virtual keyframes which are distributed strategically within an <b>environment</b> <b>map.</b> We discuss our insights about a suitable number and distribution of virtual keyframes, as suggested by our experiments on virtual keyframe generation and orientation relocalization. After a shading correction step, we relocalize camera orientation in real-time by comparing the current camera frame to virtual keyframes. While expanding the captured <b>environment</b> <b>map,</b> we continue to simultaneously generate virtual keyframes within the completed portion of the map, as descriptors to estimate camera orientation. We implemented our camera orientation relocalizer {{with the help of}} a GPU fragment shader for real-time application, and evaluated the speed and accuracy of the proposed approach...|$|E
5000|$|However, in most {{circumstances}} a mapped reflection {{is only an}} {{approximation of}} the real reflection. <b>Environment</b> <b>mapping</b> relies on two assumptions that are seldom satisfied: ...|$|R
40|$|Abstract — The paper {{investigates the}} use of Radio <b>Environment</b> <b>Maps</b> (REMs) {{as a tool for}} Interference Management (IM) in two-tier {{cellular}} networks comprising macro- and femto-cells. The REMs are databases that provide, through different instances distributed over network elements, a variety of network- and user-related context information for improving IM and Radio Resource Management (RRM) procedures. In this context, the focus in this paper is to present the benefit of using REM’s information on practical power control schemes for the Femtocell DownLink transmission in co-channel two-tier deployment. Keywords-femtocell; power control; Radio <b>Environment</b> <b>Maps</b> I...|$|R
40|$|Abstract. Inthispaper,wedescriberepresentationsand inferencetechniques {{that are}} used in the RoboEarth system for the web-based {{exchange}} of information between robots. We present novel representations for <b>environment</b> <b>maps</b> that combine expressive semantic environment models with techniques for selecting suitable maps from the web-based RoboEarth knowledge base. We further propose techniques for improving class-level object models with additional information as needed for distributed learning of object properties. In an integrated experiment, we show that the system enables robots to perform mobile manipulation tasks including the retrieval of suitable <b>environment</b> <b>maps</b> and the estimation and exchange of object property information. ...|$|R
