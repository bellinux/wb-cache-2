746|241|Public
25|$|In 1991, {{there were}} two {{available}} proposals that were assessed for an MPEG audio standard: MUSICAM (Masking pattern adapted Universal Subband Integrated Coding And Multiplexing) and ASPEC (Adaptive Spectral Perceptual <b>Entropy</b> <b>Coding).</b> As proposed by the Dutch corporation Philips, the French research institute CCETT, and the German standards organization Institute for Broadcast Technology, the MUSICAM technique was chosen due to its simplicity and error robustness, {{as well as for}} its high level of computational efficiency. The MUSICAM format, based on sub-band coding, became the basis for the MPEG Audio compression format, incorporating, for example, its frame structure, header format, sample rates, etc.|$|E
2500|$|<b>Entropy</b> <b>coding</b> is {{a special}} form of {{lossless}} data compression. It involves arranging the image components in a [...] "zigzag" [...] order employing run-length encoding (RLE) algorithm that groups similar frequencies together, inserting length coding zeros, and then using Huffman coding on what is left.|$|E
2500|$|In {{information}} theory, {{a symbol}} (event, signal) of probability [...] contains [...] bits of information. Hence, Zipf law for natural numbers: [...] is equivalent with number [...] containing [...] bits of information. To add {{information from a}} symbol of probability [...] into information already stored in a natural number , {{we should go to}} [...] such that , or equivalently [...] For instance, in standard binary system we would have , what is optimal for [...] probability distribution. Using [...] rule for a general probability distribution is the base of Asymmetric Numeral Systems family of [...] <b>entropy</b> <b>coding</b> methods used in data compression, which state distribution is also governed by Zipf law.|$|E
40|$|We {{propose a}} general class of {{concatenated}} errorcorrecting <b>entropy</b> <b>codes</b> and channel codes. In this way we extend and generalize the existing {{body of work}} on iterative decoding of <b>entropy</b> and channel <b>codes.</b> Using the structure and properties of serial concatenated codes, we employ error-correcting <b>entropy</b> <b>codes</b> as the outer code, and a convolutional code as the inner code. The generalization from <b>entropy</b> <b>codes</b> to redundant <b>entropy</b> <b>codes</b> allows powerful error correction similar to turbo codes. We provide upper bounds for the concatenated <b>entropy</b> <b>code</b> and channel code. We also show that iterative decoding of the proposed concatenated code outperforms iterative decoding of previously reported <b>entropy</b> and channel <b>codes</b> that operate at the same overall rate...|$|R
30|$|Decoding of {{received}} packets involves decoding the FEC {{code and}} decoding the <b>entropy</b> <b>code.</b> Decoding the FEC code {{can be done}} by, e.g., Gaussian elimination, which has complexity O(N^ 3) per layer, and therefore at most O(N^ 4) for decoding the entire control vector. Decoding of the <b>entropy</b> <b>code</b> is done by a look-up table and has, thus, complexity O(N), since the control vector contains N elements.|$|R
50|$|H. B. Barlow, T. P. Kaushal, and G. J. Mitchison. Finding minimum <b>entropy</b> <b>codes.</b> Neural Computation, 1:412-423, 1989.|$|R
2500|$|Windows Media Audio Professional (WMA Pro) is an {{improved}} lossy codec {{closely related to}} WMA standard. [...] It retains most of the same general coding features, but also features improved <b>entropy</b> <b>coding</b> and quantization strategies {{as well as more}} efficient stereo coding. [...] Notably, many of the WMA standard's low bitrate features have been removed, as the core codec is designed for efficient coding at most bitrates. [...] Its main competitors include AAC, HE-AAC, Vorbis, Dolby Digital, and DTS. It supports 16-bit and 24-bit sample bit depth, sampling rates up to 96kHz and up to eight discrete channels (7.1 channel surround). WMA Pro also supports dynamic range compression, which reduces the volume difference between the loudest and quietest sounds in the audio track. According to Microsoft's Amir Majidimehr, WMA Pro can technically go beyond 7.1 surround sound and support [...] "an unlimited number of channels." ...|$|E
5000|$|... {{conversion}} between Huffman and {{arithmetic coding}} in the <b>entropy</b> <b>coding</b> layer.|$|E
5000|$|The {{use of this}} {{approximation}} {{can allow}} the <b>entropy</b> <b>coding</b> design problem {{to be separated from}} the design of the quantizer itself. Modern <b>entropy</b> <b>coding</b> techniques such as arithmetic coding can achieve bit rates that are very close to the true entropy of a source, given a set of known (or adaptively estimated) probabilities [...]|$|E
40|$|<b>Entropy</b> <b>coded</b> vector {{quantization}} is studied using high resolution multidimensional companding over {{a class of}} non-difference distortion measures. For distortion measures which are "locally quadratic" a rigorous derivation of the asymptotic distortion and <b>entropy</b> <b>coded</b> rate of multidimensional companders is given along with conditions for the optimal choice of the compressor function. Examples are shown {{for the existence of}} optimal compressors. The rate distortion performance of the companding scheme is studied using a recently obtained asymptotic expression for the rate distortion function which parallels the Shannon lower bound for difference distortion measures. It is proved that the high resolution performance of the scheme is arbitrarily close to the rate distortion limit for large quantizer dimensions if the compressor function and the lattice quantizer used in the companding scheme are optimal, extending an analogous statement for <b>entropy</b> <b>coded</b> lattice quantization and MSE disto [...] ...|$|R
5000|$|In lossy {{transform}} codecs, {{samples of}} picture or sound are taken, chopped into small segments, {{transformed into a}} new basis space, and quantized. The resulting quantized values are then <b>entropy</b> <b>coded.</b>|$|R
3000|$|This {{result is}} {{fundamental}} especially for postcompression algorithms that perform encryption on <b>entropy</b> <b>coded</b> data. Since <b>entropy</b> coders can be considered, {{to a certain}} extent, as perfect compressors, it is required to encrypt at least [...]...|$|R
50|$|Zstandard {{combines}} {{use of a}} dictionary-type algorithm (LZ77) and Finite State Entropy (tANS) {{stage of}} <b>entropy</b> <b>coding.</b>|$|E
5000|$|... (5) Adaptive {{multilevel}} {{arithmetic coding}} {{which is a}} fast and efficient method for <b>entropy</b> <b>coding</b> strings of symbols.|$|E
50|$|In {{computer}} science and information theory, Tunstall coding {{is a form}} of <b>entropy</b> <b>coding</b> used for lossless data compression.|$|E
5000|$|Option 1: {{take the}} values of two {{consecutive}} samples; if they are analog samples, quantize them; calculate {{the difference between the}} first one and the next; the output is the difference, and it can be further <b>entropy</b> <b>coded.</b>|$|R
3000|$|... subband to the first-level wavelet subbands {{and symbols}} {{computed}} {{in the first}} stage are <b>entropy</b> <b>coded</b> by means of an arithmetic encoder. Recall that no LOWER_COMPONENT is encoded. The value of significant coefficients and their sign are raw encoded.|$|R
50|$|The {{transformation}} is usually done by projection or {{by using a}} codebook. In some cases, a codebook can be also used to <b>entropy</b> <b>code</b> the discrete value in the same step, by generating a prefix coded variable-length encoded value as its output.|$|R
5000|$|CAVLC 4:4:4 Intra Profile: The High 4:4:4 Profile {{constrained}} to all-Intra use and to CAVLC <b>entropy</b> <b>coding</b> (i.e., {{not supporting}} CABAC).|$|E
50|$|Transform skip context enabling, using a {{separate}} context for <b>entropy</b> <b>coding</b> the indication of which blocks are coded using transform skipping.|$|E
50|$|CAVLC 444 Intra Profile (44): The High 4:4:4 Profile {{constrained}} to all-Intra use and to CAVLC <b>entropy</b> <b>coding</b> (i.e., {{not supporting}} CABAC).|$|E
50|$|WebP’s {{lossless}} compression uses advanced {{techniques such as}} dedicated <b>entropy</b> <b>codes</b> for different color channels, exploiting 2D locality of backward reference distances and a color cache of recently used colors. This complements basic techniques such as dictionary coding, Huffman coding and color indexing transform.|$|R
40|$|This {{correspondence}} {{considers the}} use of punctured quasi-arithmetic (QA) codes for the Slepian–Wolf problem. These <b>entropy</b> <b>codes</b> are defined by finite state machines for memoryless and first-order memory sources. Puncturing an <b>entropy</b> <b>coded</b> bit-stream leads to an ambiguity at the decoder side. The decoder makes use of a correlated version of the original message in order to remove this ambiguity. A complete distributed source coding (DSC) scheme based on QA encoding with side information at the decoder is presented, together with iterative structures based on QA codes. The proposed schemes are adapted to memoryless and first-order memory sources. Simulation results reveal that the proposed schemes are efficient in terms of decoding performance for short sequences compared to well-known DSC solutions using channel codes. Peer ReviewedPostprint (published version...|$|R
30|$|The SBR {{envelope}} data, {{tonal component}} data, and noise-floor data are quantized and differentially coded {{in either the}} time or frequency direction {{in order to minimize}} the bit rate. All data is <b>entropy</b> <b>coded</b> using Huffman tables. Details about SBR data coding are given in the next section.|$|R
50|$|Persistent Rice adaptation, using a Rice coding {{parameter}} derivation for <b>entropy</b> <b>coding</b> that has memory that persists across transform coefficient sub-block boundaries.|$|E
50|$|The {{compression}} {{is based}} on lossless entropy reduction, by means of variousdifferentiation operations, followed by lossless <b>entropy</b> <b>coding</b> using theLZMA compression library.|$|E
5000|$|The use of {{sufficiently}} well-designed <b>entropy</b> <b>coding</b> {{techniques can}} result {{in the use of}} a bit rate that is close to the true information content of the indices , such that effectivelyand therefore ...|$|E
3000|$|... [...]. Thus, u⃗_t is a {{continuous}} variable whereas ũ_t = Ψ ^- 1 ξ _t' is the corresponding discrete valued variable, which is <b>entropy</b> <b>coded</b> and thereby {{converted into a}} bit-stream (to be transmitted over the network), see Fig. 1. Throughout this work, we will use u [...]...|$|R
30|$|Whenever variable-length <b>entropy</b> <b>codes</b> {{are used}} in the {{presence}} of a noisy channel, any channel errors will propagate and cause significant harm. Despite using channel codes, some residual errors always remain, whose effect will get magnified by error propagation. Mitigating this undesirable effect is of great practical interest. One approach is to use the residual redundancy of variable length codes for joint source-channel decoding. In this paper, we improve the performance of residual redundancy source-channel decoding via an iterative list decoder made possible by a nonbinary outer CRC code. We show that the list decoding of VLC's is beneficial for <b>entropy</b> <b>codes</b> that contain redundancy. Such codes {{are used in}} state-of-the-art video coders, for example. The proposed list decoder improves the overall performance significantly in AWGN and fully interleaved Rayleigh fading channels.|$|R
40|$|Abstract—In this paper, a new {{algorithm}} for lossless compres-sion of hyperspectral {{images is}} proposed. The spectral redundancy in hyperspectral images is exploited using a context-match method {{driven by the}} correlation between adjacent bands. This method is suitable for hyperspectral images in the band-sequential format. Moreover, this method compares favorably with the recent pro-posed lossless compression algorithms in terms of compression, with a lower complexity. Index Terms—Conditional average, context <b>coding,</b> correlation, <b>entropy</b> <b>code,</b> Golomb–Rice code, hyperspectral image, image coding. I...|$|R
50|$|Since 2014, data {{compressors}} {{have started}} using the Asymmetric Numeral Systems family of <b>entropy</b> <b>coding</b> techniques, which allows {{combination of the}} compression ratio of arithmetic coding with a processing cost similar to Huffman coding.|$|E
5000|$|This {{quantization}} process usually reduces {{a significant}} number of the AC coefficients to zero, (known as [...] data) which can then be more efficiently compressed by <b>entropy</b> <b>coding</b> (lossless compression) in the next step.|$|E
50|$|The rzip {{program is}} huge-scale data {{compression}} software designed around initial LZ77-style string matching on a 900 MB dictionary window, followed by bzip2-based Burrows-Wheeler transform (BWT) and <b>entropy</b> <b>coding</b> (Huffman) on 900 kB output chunks.|$|E
5000|$|JPEG XR's {{design is}} conceptually {{very similar to}} JPEG: the source image is {{optionally}} converted to a luma-chroma colorspace, the chroma planes are optionally subsampled, each plane is divided into fixed-size blocks, the blocks are transformed into the frequency domain, and the frequency coefficients are quantized and <b>entropy</b> <b>coded.</b> Major differences include the following: ...|$|R
50|$|Applying one {{of these}} two processes, {{short-term}} redundancy (positive correlation of nearby values) of the signal is eliminated; compression ratios on the order of 2 to 4 can be achieved if differences are subsequently <b>entropy</b> <b>coded,</b> because the <b>entropy</b> of the difference signal is much smaller than that of the original discrete signal treated as independent samples.|$|R
40|$|We {{study the}} average {{distortion}} introduced by scalar, vector, and <b>entropy</b> <b>coded</b> quantization of compressive sensing (CS) measurements. The asymptotic {{behavior of the}} underlying quantization schemes is either quantified exactly or characterized via bounds. We adapt two benchmark CS reconstruction algorithms to accommodate quantization errors, and empirically demonstrate that these methods significantly reduce the reconstruction distortion when compared to standard CS techniques. I...|$|R
