8|12|Public
5000|$|Sometimes a [...] "dummy task" [...] is added, to {{represent}} a dependency between tasks, which does not represent any actual activity. The <b>dummy</b> <b>task</b> is added to indicate precedence that can't be expressed using only the actual activities. Such a <b>dummy</b> <b>task</b> often has a completion time of 0.|$|E
5000|$|However, the {{assignment}} {{problem can be}} made rather more flexible than it first appears. In the above example, suppose that there are four taxis available, but still only three customers. Then a fourth <b>dummy</b> <b>task</b> can be invented, perhaps called [...] "sitting still doing nothing", with a cost of 0 for the taxi assigned to it. The assignment problem can then be solved in the usual way and still give the best solution to the problem.|$|E
3000|$|After {{calculating the}} {{expected}} start time and completion time of task t, the method checks whether t has an execution time equal to 0 (i.e., checks if task t is a <b>dummy</b> <b>task,</b> defined in Section [...] "Problem Description and Resource Management Model") (line 10). If task t is a <b>dummy</b> <b>task,</b> {{it does not}} need to be scheduled on a resource because it has an execution time equal to 0 and only the task’s scheduled start time and completion time need to be set (line 11). The task t is also added to the RM-DCWF’s mappedTasks list (line 12), which stores all the tasks that have been successfully mapped for job j. On the other hand, if task t has an execution time greater than 0 (line 13), the method attempts to find a resource r in R that can execute t at its expected start time. If t cannot be scheduled to execute at its expected start time, the task is scheduled at the next best time depending on the value of the taskSchedulingPolicy field (line 14). If taskSchedulingPolicy is set to TSP 1, the method schedules task t at its next earliest possible time. On the other hand, if taskSchedulingPolicy is set to TSP 2, the method schedules the task at its next latest possible time, while ensuring the task’s sub-deadline is satisfied.|$|E
30|$|For {{taking the}} user to a {{particular}} emotional state, we defined some intentional emotion changing <b>dummy</b> <b>tasks.</b> A participant took around thirty to forty minutes {{to carry out the}} tasks. We informed the participants beforehand that the tasks were likely to trigger a change in emotional state and obtained their informed consent.|$|R
30|$|We trained each {{participant}} {{to make him}} familiar with the device. During the training session, participants’ were familiarized with the app also. The app training included introducing the participants to the active task icons, the steps required to locate those icons in the four screens and the steps for executing the seven tasks. They were given some <b>dummy</b> <b>tasks</b> to perform for the purpose. Training sessions lasted for about 10 – 15  min. Each participant was provided with a volunteer ready to help at any stage.|$|R
40|$|This paper {{presents}} {{a new approach}} for modeling hardware and software tasks in codesign system. The model has the advantage that the hardware tasks are structured {{in a way that}} is compatible with the software tasks. As a result, both hardware and software tasks can be managed in a uniform manner using a single task manager. A hardware/software partitioning and schedule algorithm is developed to automatically map the tasks to the codesign resources to minimize the processing time (makespan). The practicality of our approach is demonstrated with an implementation of <b>dummy</b> <b>tasks</b> for an existing reconfigurable computer, the UltraSONIC. The results show that our approach is promising for a real application. 1...|$|R
40|$|This paper {{discusses}} {{some aspects}} of a game for research into decision making. From our experience with commercial board wargames, we identify six requirements for a research game, and three requirements for its control. The Organisational Control Game, a wargame which we have developed, is shown to satisfy the requirements. The Superior Commander system is used to control the game, {{and the importance of}} the <b>dummy</b> <b>task</b> is discussed. We conclude that commercial board wargames can be adapted to examine decision making scientifically, we offer some guidelines for game development, and we consider extensions to other contexts, particularly those related to business games...|$|E
40|$|Our {{previous}} research showed that vertical vection could modulate human mood. We further examined this possibility by using memory recognition task of positive, negative and neutral emotional images with {{high and low}} arousal levels. Those images were remembered accidentally while the participants did visual <b>dummy</b> <b>task,</b> and later presented together with novel images during vertical vection-inducing or neutral visual stimuli. The results showed that downward vection facilitated the recognition of negative images and inhibited the recognition of positive ones. These modulations of incidental memory task provide an additional evidence for vection influence on cognitive and emotional processing, and also provide a new paradigm {{that can be used}} in future vection and embodied cognition research...|$|E
40|$|Following {{a review}} of {{previous}} work in this area, a presentation is made which illustrates {{the impact of a}} naive application of DVS in a system incorporating a time-triggered co-operative (TTC) scheduler. Novel algorithms (TTC-jDVS, TTC-jDVS 2) and then introduced which more successfully integrate TTC and DVS techniques. These algorithms involve: (i) changes to system timer settings when the frequency is altered; (ii) use of a form of 'sandwich delay' to reduce the impact of changes to the scheduler overhead which arise as a result of frequency changes, and (iii) execution of jitter-sensitive tasks at a fixed operating frequency. The impact of these algorithms on both jitter and energy consumption is illustrated empirically on a representative hardware platform, using both 'dummy' task sets and a more realistic case study. In designs for which low jitter is an important consideration, at least a limited degree of task pre-emption may be required. A simple time-triggered hybrid (TTH) scheduler can be used to achieve such behaviour. A novel TTH secluding algorithm (TTH-jDVS) is presented and evaluated, again through use of <b>dummy</b> <b>task</b> sets and a case study. The third piece of experimental work presented in this thesis illustrates that [...] - in situations where minimal jitter is required [...] - hardware support is required. To illustrate the potential of such an approach a final case study is employed...|$|E
40|$|STSS(International Symposium on Socially and Technically Symbiotic Systems) /ISSNP(International Symposium on Symbiotic Nuclear Power Systems) 2015 : August 25 - 28, Main Campus of Kyoto University. Importance of {{organizational}} learning {{has been widely}} recognized {{and it has been}} introduced in various companies in recent years. Informal communication such as a chat is expected to share unexpected but useful information without heavy workload. In this study, therefore, a method to induce a chat has been proposed which can promote organizational learning. In order to implement the proposed method, a resting room has been prepared and a system has been developed in it where work related information on the Internet is displayed on a large display. A case study was conducted to confirm whether the proposed method induced a chat which led to organizational learning or not. Two groups of two female specialists participated in the case study, where they conducted <b>dummy</b> <b>tasks</b> and took a break in the prepared resting room in which the system had been installed. As the result, a few chats leading to organizational learning were induced. In addition, {{it was found that the}} proposed method had a possibility to promote their work related chat for the workers who usually talk about their works...|$|R
40|$|In this project, a complex, serial {{application}} that models networks of oil wells is analyzed for today's parallel architectures. By heavy {{use of the}} profiling tool Valgrind, several serial optimizations are achieved, causing up to a 30 - 50 x speedup on previously dominant sections of the code, on different architectures. Our initial main goal is to parallelize our application for GPGPUs (General Purpose Graphics Processing Units) such as the NVIDIA GeForce 8800 GTX. However, our optimized application is shown {{not to have a}} high enough computational intensity to be suitable for the GPU platforms, with the data transfer over the PCI-express port showing to be a serious bottleneck. We then target our applications for another, more common, parallel architecture [...] the multi-core CPU. Instead of focusing on the low-level hotspots found by the profiler, a new approach is taken. By analyzing the functionality of the application and the problem it is to solve, the high-level structure of the application is identified. A thread pool in combination with a task queue is implemented using PThreads in Linux, which fit the structure of the application. It also supports nested parallel queues, while maintaining all serial dependencies. However, the sheer size and complexity of the serial application, introduces a lot of problems when trying to go multithreaded. A tight coupling of all parts of the code, introduces several race conditions, creating erroneous results for complex cases. Our focus is hence shifted to developing models to help analyze how suitable applications with traversal of dependence-tree structures, such as our oil well network application is, given benchmarks of the node times. First, we benchmark the serial execution of each child in the network and predict the overall parallel performance by computing <b>dummy</b> <b>tasks</b> reflecting these times on the same tree structure on two given well networks, a large and a small case. Based on these benchmarks, we then predict the speedup of these two cases, with the assumption of balanced loads on each level in the network. Finally, the minimum amount of time needed to calculate a given network is predicted. Our predictions of low scalability, {{due to the nature of}} the oil networks in the test cases, are then shown. This project thus concludes that the amount of work needed to successfully introduce multithreading in this application might not be worth it, due to all the serial dependencies in the problem the application tries to solve. However, if there are multiple individual networks to be calculated, we suggest using Grid technology to manage multiple individual instances of the application simultaneously. This can be done either by using script files or by adding DRMAA API calls in the application. This, in combination with further serial optimizations, is the way to go for good speedup for these types of applications. </p...|$|R
40|$|Neuroimaging {{studies have}} {{reported}} greater activation of the human amygdala in response to faces than to nonfacial stimuli, yet {{little is known about}} the temporal profile of this activation. We investigated this issue by recording the intracranial field poten-tials of the amygdala in participants undergoing preneurosurgical assessment (n = 6). Participants observed faces, mosaics, and houses in upright and inverted orientations using a <b>dummy</b> target detection <b>task.</b> Time–frequency statistical parametric mapping analyses revealed that the amygdala showed greater gamma-band activity in response to faces than to mosaics at 200 – 300 msec, with a peak at 255 msec. Gamma-band activation with a similar tem-poral profile was also found in response to faces versus houses. Activation patterns did not differ between upright and inverted presentations of stimuli. These results suggest that the human amygdala is involved in the early stages of face processing, includ-ing the modulation of subjective perception of faces. ...|$|R
3000|$|... {{execution}} phase. An execution {{phase in}} a multi-stage {{job is a}} collection of tasks that perform a specific function in the job. Note that the execution phase that a constituent task belongs to is specified by the user when the job is submitted to the system. In the sample job shown in Fig. 2, the first phase of execution comprises three tasks: t 1, t 2, and t 3, and the function of these three tasks is to read and parse the input data. These three tasks do not have any direct preceding tasks (referred to as parent tasks) that need to be completed before they start executing. This implies that these tasks can start executing at the job’s earliest start time specified by the user. The tasks t 4 and t 5, which analyze and perform computation on the parsed data, are part of the second phase of execution. Each of these tasks has a parent task t 0, as well as indirect preceding tasks t 1, t 2, and t 3. The tasks t 4 and t 5 cannot start executing until task t 0 finishes, which in turn cannot start executing until tasks t 1, t 2, and t 3 finish executing. Note that some workflows are modelled using a DAG with special tasks, referred to as dummy tasks, whose only purpose is to enforce precedence relationships between tasks in the DAG, and thus, dummy tasks have an execution time equal to 0. For example, in Fig. 2, task t 0 is a <b>dummy</b> <b>task</b> that ensures tasks in the second phase of execution start to execute only after all the tasks in the first phase have completed.|$|E
40|$|By {{studying}} the Orbital Express mission, modeling the spacecraft and scenarios, and testing the system, a technique {{has been developed}} that uses recursive decomposition to represent procedural actions declaratively, schema-level uncertainty reasoning to make uncertainty reasoning tractable, and lightweight, natural language processing to automatically parse procedures to produce declarative models. Schema-level uncertainty reasoning has, at its core, the basic assumption that certain variables are uncertain, but not independent. Once any are known, then the others become known. This is important where a variable is uncertain for an action and many actions of the same type exist in the plan. For example, {{if the number of}} retries to purge pump lines was unknown (but bounded), and each attempt required a sub-plan, then, once the correct number of attempts required for a purge was known, it would likely be the same for all subsequent purges. This greatly reduces the space of plans that needs to be searched to ensure that all executions are feasible. To accommodate changing scenario procedures, each is ingested into a tabular format in temporal order, and a simple natural-language parser is used to read each step and to derive the impact of that step on memory, power, and communications. Then an ASPEN (Activity Scheduling and Planning Environment) model is produced based on this analysis. The model is tested and further changed by hand, if necessary, to reflect the actual procedure. This results in a great savings of time used for modeling procedures. Many processes that need to be modeled in ASPEN (a declarative system) are, in fact, procedural. ASPEN includes the ability to model activities in a hierarchical fashion, but this representation breaks down if there is a practically unbounded number of sub-activities and decomposition topologies. However, if recursive decomposition is allowed, HTN-like encodings are enabled to represent most procedural phenomena. For example, if a switch requires a variable (but known {{at the time of the}} attempt) number of attempts to switch on, one can recurse on the number of remaining switch attempts and decompose into either the same switching activity with one less required attempt, or not decompose at all (or decompose into a <b>dummy</b> <b>task),</b> resulting in the end of the decomposition. In fact, any bounded procedural behavior can be modeled using recursive decompositions assuming that the variables impinging the disjunctive decomposition decision are computable at the time that the decision is made. This enables one to represent tasks that are controlled outside of the scheduler, but that the scheduler must accommodate, without requiring one to give a declarative model of the procedural behavior...|$|E
40|$|Specialist police {{officers}} carry external loads during operations {{to assist with}} task outcomes and reduceoccupational risk. To investigate the effects of load carriage on the operational mobility of specialist {{police officers}}, 6 male police officers (mean age 33. 3 ± 4. 13 years) from a tactical operations unit (mean years as specialist 5. 3 ± 2. 5 years) completed a tactical move and dummy rescue. Scenarios were conducted in a randomised unloaded or tactically loading condition (22. 8 ± 1. 8 kg). Time to complete the tactical movement task increased with load (unloaded= 18. 59 ± 2. 44 sec: loaded= 19. 89 ± 1. 61 sec) and officers were significantly slower (p 3 ̆c 0. 01) during the <b>dummy</b> drag mobility <b>task</b> when loaded (unloaded= 9. 29 ± 0. 53 sec: loaded= 10. 25 ± 0. 77 sec). Officers carrying loads greater than 25...|$|R
30|$|Having {{discussed}} {{aspects of}} education, jobs and wages, {{we will consider}} the development of returns to education (wage premia) over time as the final step in analyzing the chances of job market entrants. Declining opportunities of young high-skilled workers getting a high-paying job can also result in lower wage premia for higher degrees. However, since wage premia are the returns relative to other education groups, the evolution of (entry) wages of those groups is of particular importance. If (entry) wages of the other groups have decreased even more strongly, than wage premia for high-skilled workers could increase independently of the overall development of wages of that group. We estimate a variant of the Mincer earnings equation that considers the effect of potential experience, education level (categorical) and gender (dummy). We augment the model in a second step by <b>dummy</b> variables for <b>tasks</b> (reference category: non-routine manual jobs). 10 To analyze wage premia over time, we estimate the models for each year separately.|$|R
40|$|Entry-level {{crowd work}} is often {{reported}} to pay less than minimum wage. While {{this may be}} appropriate or even necessary, due to various legal, economic, and pragmatic factors, some Requesters and workers continue to question this status quo. To promote further discussion on the issue, we survey Requesters and workers whether they would support restricting tasks to require minimum wage pay. As a form of design activism, we confronted workers with this dilemma directly by posting a <b>dummy</b> Mechanical Turk <b>task</b> which {{told them that they}} could not work on it because it paid less than their local minimum wage, and we invited their feedback. Strikingly, for those workers expressing an opinion, two-thirds of Indians favored the policy while two-thirds of Americans opposed it. Though a majority of Requesters supported minimum wage pay, only 20 % would enforce it. To further empower Requesters, and to ensure that effort or ignorance are not barriers to change, we provide a simple public API to make it easy to find a worker's local minimum wage by his/her IP address. Comment: This is an extended online version of the paper accepted to the 5 th AAAI Conference on Human Computation and Crowdsourcing (HCOMP 2017...|$|R
40|$|The {{analysis}} covers 27 {{international organizations}} in the years 1950 - 2001. From the first to the last year, staff increased at a compound average rate of 3. 2 percent per annum. Since the number of member states rose by only 2. 5 percent, the elasticity of staff to member states is larger than one (1. 28). As {{this may be due}} to an expansion of tasks, we estimate time-series regressions and panel-data regressions which contain output proxies or <b>task</b> <b>dummies</b> wherever possible. The pooled analysis of 817 observations reveals that (i) the elasticity of staff to membership is much larger than unity (1. 36) if, and only if, the non- stationary component of staff size is not removed, (ii) United Nations organizations have significantly more staff, (iii) international {{organizations in the}} United States and Switzerland have significantly less staff, (iv) heterogeneity in terms of per capita income limits the size of an international organization and that (v) its staff is larger if its membership comprises many industrial or communist countries. In a reduced sample, the financing share of the largest contributor in combination with the party or programmatic orientation of its government has a significantly negative effect on staff because the size of the largest financing share determines the incentive to monitor. U. S. exit from an international organization reduces its staff significantly. Most of these results depend on the condition that the non-stationary component of staff size is not taken account of by time dummies or trends. political economy, principal-agent problem, bureaucratic inefficiency, international organizations...|$|R
30|$|We {{now turn}} to {{exploring}} how the variables discussed above are related to earnings, and whether there are important differences between self-employment and paid employment. This is investigated by estimating various earnings regressions that either include a sex dummy or are run separately for men and women. As the dependent variable we use the logarithm of monthly earnings rather than hourly earnings as is usually done in wage regressions of paid employees, {{because we want to}} see to what extent gender earnings differences {{can be traced back to}} women working fewer hours, and whether this effect differs between paid employment and self-employment. Our explanatory variables, which were already discussed in section 2, are the following: Human capital is captured by 6 dummies for educational degrees, the years of working experience, working intermissions and tenure (all in linear and quadratic form), and the number of changes of profession. The amount and the flexibility of working time are captured by (the logarithm of) weekly working hours and by the frequency at which individuals succeeded in balancing working time scheduling with family and private interests (3 dummies). We use a dummy for living together with a partner and a dummy for the existence of young children (aged 0 – 5) in the household to reflect family background. We also include the interaction of these two dummies, thus differentiating between individuals with partner and young children, individuals with partner and no young children, singles with young children and singles without young children (who form the reference group). Furthermore a dummy variable indicating high career aspirations is included. 54 dummies for different professional fields, 17 <b>dummies</b> for the <b>tasks</b> occurring at work and 8 firm size dummies serve as segregation variables. Finally, we include some control variables such as migration background, disability status and place of residence.|$|R
30|$|Before {{turning to}} a {{decomposition}} analysis, consider some simple OLS earnings regressions as displayed in Table  3. The table displays the results of regressions of logarithmic gross hourly earnings on a self-employment dummy and several control variables. The BIBB/BAuA Employment Survey 2012 contains exceptionally rich information on characteristics of individuals {{and in particular the}} jobs they perform, which enables one to account for a large set of control variables. To begin with, interviewees were asked about the specific skill requirements at their jobs. For eight different areas they were to state whether their work required basic or expert knowledge in this area. Examples are technical skills, economic skills, math skills, and legal knowledge. I include 16 dummies for basic and expert skills in these eight areas as control variables in the regressions. Further controls for human capital are the highest professional qualification (four dummies) and actual general and specific working experience. Actual general working experience is known because interviewees were asked when they were employed {{for the first time and}} also what the total amount of time of working intermissions was. Both variables are measured in years and included in the regressions in linear and squared form. Specific working experience is measured as tenure, i.e., years running the current business (years working at the current workplace for paid employees, respectively), and is also included in the regressions in linear and squared form. Regarding the job characteristics of individuals, interviewees were also asked in what tasks they were engaged. Examples are producing goods, quality control, purchasing or selling, advertising or marketing, etc. There were 17 tasks altogether, so I include 17 <b>dummies</b> capturing the <b>tasks</b> occurring at work. Additionally, eleven dummies capture the physical working environment of individuals, for instance, whether they were exposed to noise, dirt, or coldness. 12 Finally, several socio-demographic variables (migration background, family status, place of residence) are included as control variables.|$|R

