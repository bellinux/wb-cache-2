0|56|Public
5000|$|Partitioning <b>divides</b> a <b>disk</b> {{into one}} or more regions, writing data {{structures}} to the disk to indicate {{the beginning and end}} of the regions. This level of formatting often includes checking for defective tracks or defective sectors.|$|R
40|$|Although {{there are}} many {{efficient}} techniques to minimize the speed gap between processor and the memory, it remains a bottleneck for various commercial implementations. Since secondary memory technologies are much slower than main memory, {{it is challenging to}} match memory speed to the processor. Usually, hard disk drives include semiconductor caches to improve their performance. A hit in the disk cache eliminates the mechanical seek time and rotational latency. To further improve performance a <b>divided</b> <b>disk</b> cache, subdivided between metadata and data, has been proposed previously. We propose a new algorithm to apply the SSD that is flash memory-based solid state drive by applying FTL. First, this paper evaluates the performance of such a disk cache via simulations using DiskSim. Then, we perform an experiment to evaluate the performance of the proposed algorithm. clos...|$|R
5000|$|Description: On a gold <b>disk</b> <b>divided</b> per pairle {{reversed}} Gules, Argent and Azure, {{the crest}} from the National Guard of the State of Pennsylvania.|$|R
40|$|A thin disk is suspended, {{with very}} small annular clearance, in a {{cylindrical}} {{opening in the}} base plate of a calibration chamber. A continuous flow of calibration gas passes through the chamber and annular opening to a downstream high vacuum pump. The ratio of pressures on the two faces of the disk is very large, so that the upstream pressure is substantially equal to net force on the <b>disk</b> <b>divided</b> by <b>disk</b> area. This force is measured with a dynamometer that is calibrated in place with dead weights. A probable error of + or - (0. 2 millitorr plus 0. 2 percent) is attainable when downstream pressure is known to 10 percent...|$|R
40|$|The {{retrieval}} process of a continuous media is {{considered as a}} periodic request which each subrequest retrieves a block of data. The next data block has to be ready before the display of the current data block is completed. The size of data block, called buffer size, along with the access time of the data block then {{determine the number of}} such requests that can be supported. This implies that to reduce access time, it can either decrease the required buffer size and/or increase the number of {{retrieval process}}es. This paper addresses the issue of reducing access time from data placement perspective. We propose a data placement scheme called Cluster Placement for a mass storage system of Video-On-Demand (VOD) servers. The cluster placement <b>divides</b> <b>disk</b> cylinders into equal-size clusters and stores video data across clusters. Retrieval processes are synchronized such that a batch of subrequests are within a cluster range. However, the cluster accessed by the current batch of s [...] ...|$|R
40|$|Using disk form multilayers, a semi-analytical {{solution}} {{has been}} derived for determination of displacements and stresses in a rotating cylindrical shell with variable thickness under uniform pressure. The thick cylinder is <b>divided</b> into <b>disk</b> form layers form with their thickness {{corresponding to the}} thickness of the cylinder. Due to the existence of shear stress in the thick cylindrical shell with variable thickness, the equations governing disk layers are obtained based on first-order shear deformation theory (FSDT). These equations are {{in the form of a}} set of general differential equations. Given that the cylinder is <b>divided</b> into n <b>disks,</b> n sets of differential equations are obtained. The solution of this set of equations, applying the boundary conditions and continuity conditions between the layers, yields displacements and stresses. A numerical solution using finite element method (FEM) is also presented and good agreement was found...|$|R
5000|$|The second {{property}} requires <b>dividing</b> the <b>disk</b> {{into several}} sectors, usually 512 bytes ( [...] bits) long, which are encrypted and decrypted independently of each other. In turn, if {{the data is}} to stay confidential, the encryption method must be tweakable; no two sectors should be processed {{in exactly the same}} way. Otherwise, the adversary could decrypt any sector of the disk by copying it to an unused sector of the disk and requesting its decryption.|$|R
50|$|Desktop {{layering}} is {{a method}} of desktop virtualization that <b>divides</b> a <b>disk</b> image into logical parts to be managed individually. For example, if all members of a user group use the same OS, then the core OS only needs to be backed up once for the entire environment who share this layer. Layering {{can be applied to}} local physical disk images, client-based virtual machines, or host-based desktops. Windows operating systems are not designed for layering, therefore each vendor must engineer their own proprietary solution.|$|R
50|$|LFS <b>divides</b> the <b>disk</b> into segments, {{only one}} of which is active at any one time. Each segment has a header called a summary block. Each summary block {{contains}} a pointer to the next summary block, linking segments into one long chain that LFS treats as a linear log. The segments do not necessarily have to be adjacent to each other on disk; for this reason, larger segment sizes (between 384KB and 1MB) are recommended because they amortize the cost of seeking between segments.|$|R
5000|$|Basic storage {{involves}} <b>dividing</b> a <b>disk</b> into {{primary and}} extended partitions. [...] This is the route that all versions of Windows that were reliant on DOS-handled storage took, and disks formatted {{in this manner}} are known as basic disks. Dynamic storage {{involves the use of}} a single partition that covers the entire disk, and the <b>disk</b> itself is <b>divided</b> into volumes or combined with other disks to form volumes that are greater in size than one disk itself. Volumes can use any supported file system.|$|R
5000|$|A {{partition}} is a fixed-size {{subset of}} a disk drive which {{is treated as}} a unit by the operating system. A partition table is a table maintained on disk by the operating system describing the partitions on that disk. The terms partition table and partition map are most commonly associated with the MBR partition table of a Master Boot Record (MBR) in IBM PC compatibles, {{but it may be}} used generically to refer to other [...] "formats" [...] that <b>divide</b> a <b>disk</b> drive into partitions, such as: GUID Partition Table (GPT), Apple partition map (APM), or BSD disklabel.|$|R
40|$|Abstract. A semi-analytical stress {{solution}} is obtained for a rotating anisotropic disk of constant thickness and density. The solution proceeds along the classical line by <b>dividing</b> the <b>disk</b> into elastic and plastic zones, and then solving for the axially-symmetric stress distributions in each zone, matching subsequently stresses at the elastic-plastic border. The {{edges of the}} disk {{are supposed to be}} stress free and no kinematics boundary conditions are involved in the analysis. The principal axes of anisotropy coincide with the in-plane radial and circumferential directions. Comparison with an isotropic material modeling suggests an improvement in a preliminary engineering design when plastic orthotropy is accounted for...|$|R
30|$|For {{measurements}} {{using the}} nonlinear modulation method and compressive strength test, 16 cylindrical samples, including four reference specimens, {{were used for}} each mix. On the other hand, for the nonlinear resonance vibration method and splitting tensile strength test, 13 cylindrical samples, including one reference specimen, were used for each mix. Moreover, each cylindrical sample was <b>divided</b> into five <b>disk</b> samples that were 25  mm in height.|$|R
5000|$|The biggest {{changes were}} to the disk format and file storage methods, as Apple Pascal was {{designed}} {{to take advantage of}} 140K 5.25" [...] floppy <b>disks.</b> Instead of <b>dividing</b> the <b>disk</b> into 256-byte sectors as with DOS 3.2, Apple Pascal divided it into [...] "blocks" [...] of 512 bytes each, each block thus contained two sectors. This made for a different method for saving and retrieving files. Under Apple DOS, files were saved to any available sector that the OS could find, regardless of location. This caused larger files to become fragmented and slowed down access to the disk when loading and saving. Apple Pascal attempted to rectify this by saving only to consecutive blocks on the disk.|$|R
40|$|Abstract. The {{eigenvalue}} {{problem for}} the p-Laplace operator with p> 1 on planar domains with the zero Dirichlet boundary condition is considered. The Constrained Descent Method and the Constrained Mountain Pass Algorithm {{are used in the}} Sobolev space setting to numerically investigate the dependence of the two smallest eigenvalues on p. Computations are conducted for values of p between 1. 1 and 10. Symmetry properties of the second eigenfunction are also examined numerically. While for the disk an odd symmetry about the nodal line <b>dividing</b> the <b>disk</b> in halves is maintained for all the considered values of p, for rectangles and triangles symmetry changes as p varies. Based on the numerical evidence the change of symmetry in this case occurs at a certain value p 0 which depends on the domain. 1...|$|R
40|$|Abstract. Five-axis {{ball-end}} milling {{technology is}} widely used in many industries such as aerospace, automotive and die-mold for complex surface machining. Despite recent advances in machining technology, productivity in five-axis ball-end milling is still limited due to the high cutting forces and stability. Moreover, cutting forces in machining is determined by extracting the cutter workpiece engagement (CWE) from the in-process workpiece. A discrete boundary representation method is developed. Cutter is firstly <b>divided</b> into <b>disk</b> elements along the tool axis. And in each disk element, boundary representation based exact Boolean method is introduced for extracting complex cutter-workpiece engagements at every cutter location due to its efficiency and speed over other discrete methods. Developed engagement model is proved to calculate complex engagement regions between tool and workpiece efficiently and accurately...|$|R
40|$|The {{eigenvalue}} {{problem for}} the p-Laplace operator with p> 1 on planar domains with the zero Dirichlet boundary condition is considered. The Constrained Descent Method and the Constrained Mountain Pass Algorithm {{are used in the}} Sobolev space setting to numerically investigate the dependence of the two smallest eigenvalues on p. Computations are conducted for values of p between 1. 1 and 10. Symmetry properties of the second eigenfunction are also examined numerically. While for the disk an odd symmetry about the nodal line <b>dividing</b> the <b>disk</b> in halves is maintained for all the considered values of p, for rectangles and triangles symmetry changes as p varies. Based on the numerical evidence the change of symmetry in this case occurs at a certain value p_ 0 which depends on the domain...|$|R
50|$|Ten {{returned}} {{in the forefront}} again {{with the release of}} Return to Evermore in 2004 initially through Gary Hughes' own record label Intensity Records, while in 2005 the band announced that they would release a double disk Best Of collection, aptly named The Essential Collection 1995-2005, in order to celebrate the band's tenth anniversary. The Essential Collection 1995-2005 featured newly recorded versions of the songs, which were <b>divided</b> into two <b>disks</b> - one containing the ballads and one the heavier material.|$|R
5000|$|The fileserver, {{sometimes}} referred to as the LexICON, was a simple box with an internal 10MB hard drive and a 5.25" [...] floppy drive opening to the front, and parallel port for a shared printer. Later Lexicons included a 64MB hard <b>disk,</b> <b>divided</b> into two partitions. Unlike the PET's floppy system, however, users of the ICON used Unix commands to copy data to their personal floppy disks from its [...] "natural" [...] location in the user's home directory on the hard drive.|$|R
5000|$|The is 20-23 mm. Antennae bipectinate in male, {{branches}} {{short and}} dilated distally. Antennae serrate in female. Body dull black in color {{with a large}} yellow spot on prothorax and streak on metathorax. Abdomen with yellow spots on vertex and side of each segment. Female has an ochreous anal tuft. Fore wings are with a hyaline spot in cell. There is one in interno-median interspace and one in each marginal interspace. Hind wings with a hyaline patch on <b>disk,</b> <b>divided</b> into four by veins.|$|R
40|$|We {{develop a}} {{reliable}} disk array based storage architecture for digital video retrieval. Our goals are twofold: maximizing {{the number of}} concurrent real-time sessions while minimizing the buffering requirements, and ensuring {{a high degree of}} reliability. The first goal is achieved by adopting a pipelined approach and by reducing latencies through specialized caching and constrained data placement schemes which are made possible by the orderly consumption pattern of digital video and audio. The second goal is achieved by <b>dividing</b> the <b>disks</b> into RAID 3 reliability groups which serve as pipeline stages. The video retrieval environment introduces tradeoffs between the size of a reliability group, the number of supportable streams, and the buffering requirements. These tradeoffs are explored in this paper. In particular, we note that the buffering requirement decreases as the number of groups increases. The storage architecture is further tailored {{to meet the demands of}} variable bit rate ( [...] ...|$|R
40|$|We {{develop a}} {{reliable}} disk array based storage architecture for digital video retrieval. Our goals are twofold: maximizing {{the number of}} concurrent realtime sessions while minimizing the buffering requirements, and ensuring {{a high degree of}} reliability. The first goal is achieved by adopting a pipelined approach and by reducing latencies through specialized disk caching and constrained data placement schemes. The second goal is achieved by <b>dividing</b> the <b>disks</b> into RAID 3 reliability groups which serve as pipeline stages. We note that the buffering requirement decreases as the number of groups increases. To improve the performance further, we introduce two techniques for more efficient movie retrieval: on arrival caching, and interleaved annular layout. We present a case study of the performance of these techniques which shows a significant improvement when they are incorporated. 1 Introduction Storage servers for video retrieval typically consist of an array of high-performance disks [...] ...|$|R
5000|$|File {{systems have}} {{traditionally}} <b>divided</b> the <b>disk</b> into equally sized blocks to simplify their design {{and limit the}} worst-case fragmentation. Block sizes are typically multiples of 512 due {{to the size of}} hard disk sectors. When files are allocated by some traditional file systems, only whole blocks can be allocated to individual files. But as file sizes are often not multiples of the file system block size, this design inherently results in the last blocks of files (called tails) occupying only a part of the block, resulting in what is called internal fragmentation (not to be confused with external fragmentation). This waste of space can be significant if the file system stores many small files and can become critical when attempting to use higher block sizes to improve performance. FFS and other derived UNIX file systems support fragments [...] which greatly mitigate this effect.|$|R
40|$|We {{present the}} MIT {{data from the}} OSO- 7 {{satellite}} for observations of the galactic plane between 1971 and 1974. A number of sources discovered in the MIT all-sky survey are described in detail: MX 0049 + 59, MX 0836 - 42, MX 1353 - 64, MX 1406 - 61, MX 1418 - 61, MX 1709 - 40, and MX 1608 - 52 (the persistent source suggested {{to be associated with}} the X-ray burst source XB 1608 - 52). Upper limits to the X-ray emission from a number of interesting objects are also derived. General results describing all of our observations of galactic sources are presented. Specifically, we display the number-intensity diagrams, luminosity functions, and color-color diagrams for all of the sources we detected. The data are <b>divided</b> between <b>disk</b> and bulge populations, and the characteristics of the two groups are contrasted. Finally, the concept of X-ray source populations and the relationship of globular cluster sources and burst sources to the disk and bulge populations are discussed...|$|R
40|$|Caching {{is one of}} {{the most}} {{important}} schemes for improving the performance of continuous media servers. Continuous object caching enables a server to support more clients simultaneously since it reduces the disk load imposed at each round. But, without a quantative analysis of the disk load reduction induced by caching, the caching effect can not be reflected in the admission control scheme, which limits the number of simultaneous clients serviced. In this paper, we propose a probabilistic model of the caching effect in a continuous media server. The proposed model <b>divides</b> the <b>disk</b> load reduction induced by caching into disk overhead reduction and block transfer reduction, and models each reduction stochastically to generate the overall reduction model. The proposed model enables the development of a lot of statistical admission control algorithms that can increase the number of clients serviced simultaneously. In this paper, we present a simple example of a statistical admission contr [...] ...|$|R
40|$|HighLight is a {{file system}} {{combining}} secondary disk storage and tertiary robotic storage {{that is being}} developed {{as part of the}} Sequoia 2000 Project. [1]. HighLight is an extension of the 4. 4 BSD log-structured file system (LFS) [2], that provides hierarchical storage management without requiring any special support from applications. This paper presents HighLight's design and various policies for automatic migration of file data between the hierarchy levels. INTRODUCTION 4. 4 BSD LFS derives directly from the Sprite log-structured file system (LFS) [3], developed by Mendel Rosenblum and John Ousterhout as part of the Sprite operating system. Primarily, LFS is optimized for writing data, whereas most file systems are optimized for reading data. LFS <b>divides</b> the <b>disk</b> into 512 KB or 1 MB segments, and writes data sequentially within each segment. The segments are threaded together to form a log, so recovery is simple and quick, entailing a roll-forward of the log from the last checkpoint. Disk [...] ...|$|R
50|$|The album {{features}} the collaboration from Marvin Bell (Iowa Poet Laureate), Mighty Mike McGee (2003 National Poetry Slam Champion), Patricia Smith (author of Close to Death, nominee for the Pulitzer in journalism), Luis J. Rodriquez (author of Always Running, activist), Viggo Mortensen, Georganne Deen (author of Western Witch, Season of the, painter), Mark Eleveld (author of The Spoken Word Revolution), Saul Williams, and Regie Gibson (author of Storms Beneath the Skin) with musical accompaniment by John Condron. The album contains {{a total of}} 33 tracks, <b>divided</b> on 2 <b>disks.</b> The CDs credits only show the performers but not the tracks.|$|R
40|$|Immunocytochemical {{techniques}} have localized a large protein {{which is an}} intrinsic membrane component of isolated frog rod outer segments (ROS). This large protein whose apparent mol wt is 290, 000 daltons comprises about 1 - 3 % of the ROS membrane mass. Its molar ratio to opsin is between 1 : 300 and 1 : 900. Adequate immune responses were obtained with < 30 /. ~g (100 pmol) of antigen per rabbit. Antibodies to the large protein were used for its localization on thin sections of frog retina embedded in glutaraldehyde cross-linked bovine serum albumin (BSA). Specifically bound antibodies were detected by an indirect sequence with ferritin-conjugated antibodies. This technique detected the protein which is represented by 1, 000 - 3, 000 molecules per disk. This indicates that the procedure is sufficiently sensitive for analysis of membrane components in low molar proportions, The large protein was specifically localized to the incisures of ROS <b>disks</b> which <b>divide</b> the <b>disks</b> into lobes and to the disk margin. Thus, opsin is mobile within the membrane of the disk while the large protein is apparentl...|$|R
40|$|In {{this thesis}} we study the {{stochastic}} model of fragmentation phenomena. We focus on two themes: applications to random laminations of the disk, and growth-fragmentation processes. In {{the first part}} we use fragmentatio n theory as the principal tool to study Aldous' Brownian triangulation of the disk, that is a random set of non-crossing chords that <b>divide</b> the <b>disk</b> into triangles. We investigate the number of large triangles and {{the law of the}} length of the longest chord, and generalize these results to stable laminations. As part of the proof apparatus, we obtain new results on the number of large splitting events of self-similar fragmentations. The second part concerns growth-fragmentation processes, which describe particle systems in which each particle grows and splits randomly and independently of the others. We prove that the law of a self-similar growth-fragmentation is determined by a cumulant function and its index of self-similarity. We also introduce a new class of growth-fragmentations that are related to Lévy driven Ornstein-Uhlenbeck type processes and prove a law of large numbers for these growth-fragmentations...|$|R
40|$|The present paper {{reports the}} results of an {{experiment}} designed to determine the constraints of the factorial pattern of data that a valid model for the perceived extent of achromatic transparency, rated, must predict. Consider a transparent achromatic disk in the middle of two adjoining achromatic rectangles with the common border of the rectangles <b>dividing</b> the <b>disk</b> in half. Let a and b be the l-minances of the left and right rectangles, respectively, and let p and q be the lu-minances of the left and right halves of the disk, respectively. By varying p and q factorially for different pairs of a and b and plotting mean rated as a function of p, we have found that a valid model of transparency must meet the constraints that (i) the rated extent of transparency of the disk varies essentially linearly with p, (ii) that factorial curves converge upward as p increases, and (iii) that the mean slope of factorial curves increases as the difference between a and b decreases. A new model of transparency is proposed which satisfies these constraints...|$|R
5000|$|The {{requirement}} {{that the number of}} sectors be a multiple of four is necessary: as Don Coppersmith showed, <b>dividing</b> a <b>disk</b> into four sectors, or a number of sectors that is not divisible by four, does not in general produce equal areas. [...] answered a problem of [...] by providing a more precise version of the theorem that determines which of the two sets of sectors has greater area in the cases that the areas are unequal. Specifically, if the number of sectors is 2 (mod 8) and no slice passes {{through the center of the}} disk, then the subset of slices containing the center has smaller area than the other subset, while if the number of sectors is 6 (mod 8) and no slice passes through the center, then the subset of slices containing the center has larger area. An odd number of sectors is not possible with straight-line cuts, and a slice through the center causes the two subsets to be equal regardless of the number of sectors.|$|R
40|$|A new mode! of {{achromatic}} transparency {{has been}} recendy proposed by Singh and Anderson {{as an alternative}} to the model proposed long ago by Metelli. The study reported here compared these models using achromatic stimuli consisting of a transparent disk on a background formed by two adjoining rectangJes, with the common border of the rectangles <b>dividing</b> the <b>disk</b> in hai E. Let a and b denote the luminances of the left and right parts of the background, respective!y, and let p and q denote the luminances of the left and right parts of the disk, respective!y. The value of b was varied for fixed values of a, p, and q. For these values the Singh-Anderson mode! predicts that the perceived extent of transparency T of the disk is constant with b, while Metelli's mode! predicts that T decreases as b increases. Participants rated T. The results confirm the prediction of Metelli's mode!. It is also shown that the Singh-Anderson mode! is invalid in principle in that, unlike Metelli's mode!, it fails co capture the principle of independence of the effects of a, b, p, and q on T...|$|R
40|$|Information {{integration}} methodology {{was used}} to test Metelli’s and Morinaga’s theories of achromatic transparency. Stimuli were transparent achromatic disks on a background formed by two adjacent horizontal rectangles. The common border of these rectangles <b>divided</b> each <b>disk</b> in two halves. Let P and Q be the luminances of the left and right halves of a disk and let A and B be those of the left and right rectangles, respectively. Transparency is given by the ratio (P – Q) / (A – B) in Metelli’s theory and is given by a weighted average of the ratios (P – Q) / (A – Q) and (P – Q) / (P – B) in Morinaga’s theory. Participants rated the transparency of disks with A and B fixed and P and Q combined factorially. Morinaga’s theory closely predicted the obtained patterns of factorial curves and Metelli’s theory predicted them incorrectly. Morinaga’s theory could also account well for individual differences in the ratings of transparency. The results support the general idea that transparency depends on the integration of luminance information rather than on the integration of lightness information...|$|R
40|$|Abstract—To {{maintain}} the continuing growth of bit density in magnetic recording media, the disk industry {{will have to}} change technologies. Shingled write disks {{are expected to be}} the next generation of high capacity magnetic disks and already in prototype. Shingled write technology is not disruptive at the level of disk design and manufacturing, but as shingled writes prevent updates in place, the technology is disruptive at the level of usage. It is possible to design a disk device driver or disk firmware that allows a shingled write disk {{to be used as a}} drop in replacement for traditional disks. Database implementations however have traditionally bypassed the file system and accessed the disk directly in order to achieve better performance. We discuss here adaptation of B+-trees and linear hash tables to shingled write disk to support indexed database tables and secondary indices. Our proposal is based on <b>dividing</b> the <b>disk</b> in low-capacity Random Access Zones (RAZ) and high capacity Log Access Zones (LAZ). The LAZ use the shingled disk effectively while RAZ places guard bands around each track in the zone in order to regain the capacity of in-place updates at the costs of loosing capacity. I...|$|R
40|$|It {{is usually}} {{proposed}} that hyperaccretion disks surrounding stellar-mass black holes at an accretion rate of {{a fraction of}} one solar mass per second, which are produced during the mergers of double compact stars or the collapses of massive stars, are central engines of gamma-ray bursts (GRBs). In some origin/afterglow models, however, newborn compact objects are invoked to be neutron stars rather than black holes. Thus, hyperaccretion disks around neutron stars seem to exist in some GRBs. Such disks may also occur in type-II supernovae. In this paper we study {{the structure of a}} hyperaccretion disk around a neutron star. We consider a steady-state <b>disk</b> model and <b>divide</b> the <b>disk</b> into two regions, called inner and outer disks. The inner disk satisfies an adiabatic self-similar structure and the outer disk is similar to the outer region of a hyperaccretion disk around a black hole. By using analytical and numerical methods, we explore the size of the inner disk, the radial distributions of the density, temperature and pressure of the whole disk, the mechanisms of energy heating and cooling, and the efficiency of neutrino cooling. We find that, compared with a black-hole disk, the hyperaccretion disk around a neutron star can cool more efficiently and produce a much higher neutrino luminosity...|$|R
40|$|Abridged) We here {{study the}} {{structure}} of a hyperaccretion disk around a neutron star. We consider a steady-state hyperaccretion disk around a neutron star, and as a reasonable approximation, <b>divide</b> the <b>disk</b> into two regions, which are called inner and outer disks. The outer disk {{is similar to that of}} a black hole and the inner disk has a self-similar structure. In order to study physical properties of the entire disk clearly, we first adopt a simple model, in which some microphysical processes in the disk are simplified, following Popham et al. and Narayan et al. Based on these simplifications, we analytically and numerically investigate the size of the inner disk, the efficiency of neutrino cooling, and the radial distributions of the disk density, temperature and pressure. We see that, compared with the black-hole disk, the neutron star disk can cool more efficiently and produce a much higher neutrino luminosity. Finally, we consider an elaborate model with more physical considerations about the thermodynamics and microphysics in the neutron star disk (as recently developed in studying the neutrino-cooled disk of a black hole), and compare this elaborate model with our simple model. We find that most of the results from these two models are basically consistent with each other. Comment: 44 pages, 10 figures, improved version following the referees' comments, main conclusions unchanged, accepted for publication in Ap...|$|R
