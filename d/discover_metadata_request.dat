0|49|Public
5000|$|<b>Discover</b> <b>metadata</b> of {{the source}} database, {{including}} value patterns and distributions, key candidates, foreign-key candidates, and functional dependencies ...|$|R
40|$|This paper {{presents}} a novel metadata management mechanism on the metadata server (MDS) for parallel and distributed file systems. In this technique, the client file system backs up the sent <b>metadata</b> <b>requests,</b> {{which have been}} handled by the metadata server, so that the MDS {{does not need to}} log metadata changes to nonvolatile storage for achieving highly available metadata service, as well as better performance improvement in metadata processing. As the client file system backs up certain sent <b>metadata</b> <b>requests</b> in its memory, the overhead for handling these backup requests is much smaller than that brought by the metadata server, while it adopts logging or journaling to yield highly available metadata service. The experimental results show that this newly proposed mechanism can significantly improve the speed of metadata processing and render a better I/O data throughput, in contrast to conventional metadata management schemes, that is, logging or journaling on MDS. Besides, a complete metadata recovery can be achieved by replaying the backup logs cached by all involved clients, when the metadata server has crashed or gone into nonoperational state exceptionally...|$|R
50|$|To {{arbitrate}} accesses to {{the shared}} storage device(s), one computer was elected {{master of the}} SAN. This computer became the metadata controller (MDC). Other computers were clients of the SAN. They communicated and sent <b>metadata</b> <b>requests</b> to the MDC over LAN. Once request had been acknowledged, clients could access the storage device directly. In a metaSAN setup, {{any member of the}} SAN could act as MDC. By default, the first computer to boot became the MDC but it was possible to define priorities or assign a dedicated computer for this task.|$|R
40|$|This paper {{presents}} a scalable and adaptive decentralized metadata lookup scheme for ultra large-scale file systems (≥ Petabytes or even Exabytes). Our scheme logically organizes metadata servers (MDS) into a multi-layered query hierarchy and exploits grouped Bloom filters to efficiently route <b>metadata</b> <b>requests</b> to desired MDS through the hierarchy. This metadata lookup scheme can be executed {{at the network}} or memory speed, without being bounded by the performance of slow disks. Our scheme is evaluated through extensive trace-driven simulations and prototype implementation in Linux. Experimental results show that this scheme can significantly improve metadata management scalability and query efficiency in ultra large-scale storage systems. ...|$|R
40|$|Abstract—This paper {{presents}} a scalable and adaptive decentralized metadata lookup scheme for ultralarge-scale file systems (more than Petabytes or even Exabytes). Our scheme logically organizes metadata servers (MDSs) into a multilayered query hierarchy and exploits grouped Bloom filters to efficiently route <b>metadata</b> <b>requests</b> to desired MDSs through the hierarchy. This metadata lookup scheme can be executed {{at the network}} or memory speed, without being bounded by the performance of slow disks. An effective workload balance method is also developed in this paper for server reconfigurations. This scheme is evaluated through extensive trace-driven simulations and a prototype implementation in Linux. Experimental results show that this scheme can significantly improve metadata management scalability and query efficiency in ultralarge-scale storage systems...|$|R
40|$|In this paper, {{we look at}} how Lustre {{performs}} using Myricom's 10 Gb/s NICs. Lustre {{clients and}} servers can use these programmable NICs in either native Ethernet mode or in Myrinet mode. Lustre provides metrics for measuring metadata retrieval (small message) performance {{as well as for}} bulk read and write operations. We show that with multiple clients Lustre can achieve 95 % of line-rate performance using Ethernet (TCP) when reading but only 50 % when writing. We show that Lustre achieves a 95 % of line-rate with a single client when using the NIC in Myrinet mode. In both cases, Lustre can serve up to 45, 000 <b>metadata</b> <b>requests</b> per second using multiple clients...|$|R
40|$|The sos 4 R package {{provides}} simple yet powerful {{access to}} OGC Sensor Observation Service instances. The package supports both encapsulation and abstraction {{from the service}} interface for novice users as well as powerful request building for specialists. sos 4 R is motivated by the idea {{to close the gap}} between the Sensor Web and tools for (geo-) statistical analyses. It implements the core profile of the SOS specification and supports temporal, spatial, and thematical filtering of observations. This document briefly introduces the SOS specification. The package’s features are explained extensively: exploration of service <b>metadata,</b> <b>request</b> building with filters, function exchangeability, result data transformation. The package is published under GPL 2 license within the geostatistic...|$|R
40|$|Abstract Prefetching is an {{effective}} technique for improving file access performance, which can reduce access latency for I/O systems. In distributed storage system, prefetching for metadata files is critical for the overall system performance. In this paper, an Affinitybased Metadata Prefetching (APM) scheme is proposed for metadata servers in large-scale distributed storage systems to provide aggressive metadata prefetching. Through mining useful information about metadata assesses from past history, AMP can <b>discover</b> <b>metadata</b> file affinities accurately and intelligently for prefetching. Compared with LRU {{and some of the}} latest file prefetching algorithms such as NEXUS and C-miner, trace-driven simulations show that AMP can improve the hit rates by up to 12 %, 4. 5 % and 4 %, respectively, whil...|$|R
5000|$|It uses a SOAP {{message to}} <b>request</b> <b>metadata,</b> and so {{goes beyond the}} basic {{technique}} of appending [...] "?wsdl" [...] to a service name's URL ...|$|R
40|$|Prefetching is an {{effective}} technique for improving file access performance, which can reduce access latency for I/O systems. In distributed storage system, prefetching for metadata files is critical for the overall system performance. In this paper, an Affinity-based Metadata Prefetching (APM) scheme is proposed for metadata servers in large-scale distributed storage systems to provide aggressive metadata prefetching. Through mining useful information about metadata assesses from past history, AMP can <b>discover</b> <b>metadata</b> file affinities accurately and intelligently for prefetching. Compared with LRU {{and some of the}} latest file prefetching algorithms such as NEXUS and C-miner, trace-driven simulations show that AMP can improve the hit rates by up to 12 %, 4. 5 % and 4 %, respectively, while reduce the average response time by up to 60 %, 12 % and 8 %, respectively...|$|R
40|$|The {{exponential}} growth of high bandwidth global networks has rapidly exposed the design {{limitations of the}} TCP protocol. The network research community has risen {{to the challenge of}} developing new protocols and methods that greatly improve the efficiency of bulk transfers over high bandwidth-delay product networks, but these solutions fail {{to address the needs of}} a wide variety of applications that do not involve long, sustained data transfers. These applications include metacomputing and task-farming codes that exhibit bursty high-throughput communication patterns during state synchronization; advanced remotely-accessible formats and distributed filesystems that exhibit small MTU-sized <b>metadata</b> <b>requests</b> mixed in with bulk data streams; highly interactive visualization applications with large instantaneous data-handling requirements; and of course high-definition, fixed data-rate network video streams. Current bulk-data-oriented solutions either fail to address these application requirements (as is the case for network sampling solutions), or they create the potential for rampant uncontrolled resource conflicts (as is the case for fixed rate solutions). In short, there is a clear need to redirect attention from bulk transfer requirements in order to determine how the current crop o...|$|R
40|$|Magnetic RAM (MRAM) {{is a new}} memory {{technology}} with access and cost characteristics {{comparable to those of}} conventional dynamic RAM (DRAM) and the non-volatil-ity of magnetic media such as disk. Simply replacing DRAM with MRAM will make main memory non-volatile, but it will not improve file system performance. However, effective use of MRAM in a file system has the potential to significantly improve performance over existing file sys-tems. The HeRMES file system will use MRAM to dramati-cally improve file system performance by using it as a permanent store for both file system data and metadata. In particular, metadata operations, which make up over 50 % of all file system requests [14], are nearly free in HeRMES because they do not require any disk accesses. Data requests will also be faster, both because of increased <b>metadata</b> <b>request</b> speed and because using MRAM as a non-volatile cache will allow HeRMES to better optimize data placement on disk. Though MRAM capacity is too small to replace disk entirely, HeRMES will use MRAM to provide high-speed access to relatively small units of data and metadata, leaving most file data stored on disk. 1...|$|R
40|$|In {{large-scale}} distributed file systems, efficient meta- {{data operations}} are critical since most file operations have {{to interact with}} metadata servers first. In existing distributed hash table (DHT) based metadata management systems, the lookup service could be a performance bottleneck due to its significant CPU overhead. Our investigations showed that the lookup service could reduce system throughput by up to 70 %, and increase system latency {{by a factor of}} up to 8 compared to ideal scenarios. In this paper, we present MetaFlow, a scalable metadata lookup service utilizing software-defined networking (SDN) techniques to distribute lookup workload over network components. MetaFlow tackles the lookup bottleneck problem by leveraging B-tree, which is constructed over the physical topology, to manage flow tables for SDN-enabled switches. Therefore, <b>metadata</b> <b>requests</b> can be forwarded to appropriate servers using only switches. Extensive performance evaluations in both simulations and testbed showed that MetaFlow increases system throughput by a factor of up to 3. 2, and reduce system latency by a factor of up to 5 compared to DHT-based systems. We also deployed MetaFlow in a distributed file system, and demonstrated significant performance improvement. Comment: in IEEE Transactions on Big Data 201...|$|R
40|$|Traditional network file systems, like NFS, do {{not extend}} to wide-area due to low bandwidth, high network latency, and {{dynamism}} introduced in the WAN environment. Metadata access latency is a significant performance problem for Wide Area File Systems, since <b>metadata</b> <b>requests</b> constitute {{a large portion of}} all file system requests, are synchronous, and cannot be cached at clients. We present WireFS, a Wide Area File System, which enables delegation of metadata management to nodes at client sites (homes). The home of a file stores the most recent copy of the file, serializes all updates, and streams updates to the central file server. WireFS uses access history to migrate the home of a file to the client site which accesses the file most frequently. We formulate the home migration problem as an integer programming problem, and present two algorithms: a dynamic programming approach to find the optimal solution, and a greedy algorithm which is non-optimal but is faster than the optimal algorithm. We show through extensive simulations that even in the WAN setting, access latency over WireFS is comparable to NFS’s performance in the LAN setting; the migration overhead is also marginal after the initial delegation...|$|R
40|$|Abstract — This paper {{describes}} iTrust, a novel distributed {{search and}} retrieval system that provides trustworthy {{access to information}} over the Internet. Nodes with information to distribute transmit their metadata to nodes that are selected at random from a set of participating nodes. Similarly, nodes seeking information distribute their requests to nodes that are selected at random from the set of participating nodes. When a node receives a request, the node tries to match the <b>metadata</b> in the <b>request</b> with the <b>metadata</b> that it holds. If the node has a match, it supplies a URL for {{the information to the}} requesting node, which then retrieves the information from the source node. The paper describes our implementation of iTrust, and provides a performance evaluation of iTrust, based on both analysis and simulation using our implementation. Distribution of <b>metadata</b> and <b>requests</b> to relatively few nodes suffices to achieve a high probability of a match...|$|R
40|$|A {{wealth of}} data and {{services}} {{are available on the}} Web, and often have geographical context as well. But the vast quan-tity of offered geospatial information is rather difficult to explore, and its quality hard to assess, due to lack of suf-ficient metadata. Hence, the Open Geospatial Consortium has specified application profiles for publishing, accessing, and searching over collections of spatial metadata with stan-dardized Catalogue Services for the Web (CSW). Unfortu-nately, existing spatial metadata remain largely unexploited by Semantic Web technologies. In this paper, we introduce TripleGeo-CSW, a middleware {{that can be used to}} <b>discover</b> <b>metadata</b> from existing CSWs through a virtual SPARQL endpoint. Acting as broker between a request (in SPARQL) and catalogue services (in XML/GML), this platform can provide on-the-fly information (as RDF triples) on available geodata according to multiple, user-specified criteria (e. g., area of interest, date of last update, keywords). As a proof of concept, we have set up an instance of this middleware against CSWs from public authorities across Europe, which involve datasets complying with the EU INSPIRE Directive. Our experience testifies that TripleGeo-CSW can assist stake-holders to repurpose existing CSWs with minimal overhead and readily expose spatial metadata on the Semantic Web...|$|R
40|$|We {{propose a}} new {{supervised}} topic model {{that uses a}} nonparametric density estimator to model the distribution of real-valued metadata given a topic. The model is similar to Topics Over Time, but replaces the beta distributions used in that model with a Dirichlet process mixture of normals. The use of a nonparametric density estimator allows for the fitting of a greater class of metadata densities. We compare our model with existing supervised topic models in terms of prediction and show that {{it is capable of}} <b>discovering</b> complex <b>metadata</b> distributions in both synthetic and real data. ...|$|R
40|$|As {{the world}} moves to digital storage for archival purposes, {{there is an}} {{increasing}} demand for reliable, lowpower, cost-effective, easy-to-maintain storage that can still provide adequate performance for information retrieval and auditing purposes. Unfortunately, no current archival system adequately fulfills all of these requirements. Tape-based archival systems suffer from poor random access performance, which prevents the use of inter-media redundancy techniques and auditing, and requires the preservation of legacy hardware. Many diskbased systems are ill-suited for long-term storage because their high energy demands and management requirements make them cost-ineffective for archival purposes. Our solution, Pergamum, is a distributed network of intelligent, disk-based, storage appliances that stores data reliably and energy-efficiently. While existing MAID systems keep disks idle to save energy, Pergamum adds NVRAM at each node to store data signatures, metadata, and other small items, allowing deferred writes, <b>metadata</b> <b>requests</b> and inter-disk data verification to be performed while the disk is powered off. Pergamum uses both intra-disk and inter-disk redundancy to guard against data loss, relying on hash tree-like structures of algebraic signatures to efficiently verify the correctness of stored data. If failures occur, Pergamum uses staggered rebuild to reduce peak energy usage while rebuilding large redundancy stripes. We show that our approach is comparable in both startup and ongoing costs to other archival technologies and provides very high reliability. An evaluation of our implementation of Pergamum shows that it provides adequate performance. ...|$|R
40|$|Abstract: Over the past several years the large scale digital library service has undergone enormous popularity. Arco project is a digital library storage project in Portuguese National library. To a digital library storage system like ARCO system,    there are several challenges,  such as the availability of petascale storage,  {{seamless}} spanning of storage cluster,  administration and utilization of distributed storage and computing resources, safety and stability of data transfer,  scalability of the whole system,  automatic discovery and monitoring of metadata,   etc. Grid computing   appears as an effective technology   coupling  geographically  distributed resources for solving large scale problems in the wide area or local area network. The ARCO system has been developed on the Grid computational infrastructure,  and on the basis of various other toolkits,  such as PostgreSQL,  LDAP,  Apache http server. Main developing languages are C,  PHP,  and Perl. In this paper, we discuss the logical structure sketch of the digital library ARCO system,  resources organization,  <b>metadata</b> <b>discovering</b> and utilising,  system operation details and some operations examples,  and the solution of large file transfer problem in Globus grid toolkit...|$|R
5000|$|A {{principal}} {{purpose of}} metadata {{is to help}} users find relevant information and <b>discover</b> resources. <b>Metadata</b> also helps to organize electronic resources, provide digital identification, and support the archiving and preservation of resources. Metadata assists users in resource discovery by [...] "allowing resources to be found by relevant criteria, identifying resources, bringing similar resources together, distinguishing dissimilar resources, and giving location information." [...] Metadata of telecommunication activities including Internet traffic is very widely collected by various national governmental organizations. This data {{is used for the}} purposes of traffic analysis and can be used for mass surveillance.|$|R
40|$|The CORES {{metadata}} schemas registry {{is designed}} to enable users to <b>discover</b> and navigate <b>metadata</b> element sets. The paper reflects {{on some of the}} experiences of implementing the registry, and examines some of the issues of promoting such services {{in the context of a}} "partially Semantic Web" where metadata applications are evolving and many have not yet adopted the RDF model...|$|R
40|$|Over {{the past}} few years the University of North Texas Libraries' Digital Projects Unit (DPU) has {{developed}} a set of metadata analysis tools, processes, and methodologies aimed at helping to focus limited quality control resources on the areas of the collection where they might have the most benefit. The key to this work lies in its simplicity: records harvested from OAI-PMH-enabled digital repositories are transformed into a format that makes them easily parsable using traditional Unix/Linux-based command-line tools. This article describes the overall methodology, introduces two simple open-source tools developed to help with the aforementioned harvesting and breaking, and provides example commands to demonstrate some common <b>metadata</b> analysis <b>requests.</b> All software tools described in the article are available with an open-source license via the author's GitHub account...|$|R
40|$|An {{increasing}} number of datasets from government, public organizations and institutions are published as open data. Metadata that describes them, are cataloged at central places to enable a better access to these datasets. Quantifying the metadata quality can help to measure the efficiency of a catalog and <b>discover</b> low-quality <b>metadata</b> records which prevent the user from finding what she is looking for. We researched and implemented a range of metrics {{from the field of}} metadata quality assessment as part of an open data platform. This paper describes the platform that automatically assesses the quality of different open government data portals using the CKAN catalog software. The results are aggregated and visualized through a web application in order to establish a continuous and sustainable monitoring service...|$|R
30|$|We {{have two}} {{programs}} which are names as vlanonymity.java and sensitivity.java. One {{program is to}} apply (v, l)-Anonymization {{to the table and}} other is to calculate the sensitivity level respectively. It is not required to update the table for vlanonymity.java because operations taken place at runtime but not saved again in the table. But it is different with sensitivity.java where it is required to update the table. A normal table creation is not sufficient to support update operations. So we have to create a table which supports bucketization that supports updations. The execution starts with calculation of sensitive levels. The execution takes place with the help of CLI. Then the query sends to the driver such as JDBC or ODBC to execute. With the help of query compiler the driver parses the query. It checks the syntax and query plan or the requirement of query. The compiler sends <b>metadata</b> <b>request</b> to Metastore. Metastore sends metadata {{as a response to the}} compiler. The compiler checks the requirement and resends the plan to the driver. Up to here, the parsing and compiling of a query is complete. The driver sends the execute plan to the execution engine. Internally, the process of execution job is a MapReduce job in case of sensitivity level calculation but not to the anonymization process. The execution engine sends the sensitivity calculation job to JobTracker, which is in Name node and it assigns this job to TaskTracker, which is in Data node. Here, the query executes MapReduce job. Meanwhile in execution, the execution engine can execute metadata operations with Metastore. The execution engine receives the results from Data nodes. The execution engine sends those resultant values to the driver. The driver sends the results to Hive Interfaces. With the help of the interface we can see the result which contains all the anonymized records in case of vlanonymity.java execution and the records with sensitive values in case of sensitivity.java execution.|$|R
40|$|Grey {{resources}} are {{spread over a}} large set of matters. In each field of interest, people have their own concepts; each group has in use its own proper vocabulary. Most authors of grey resources {{were not able to}} determine themselves the Decimal or Dewey notation, neither are they able to fix themselves the appropriate terms to embed in the DC elements. It is then worth providing the web user with an interface that let him feel at home, by adapting automatically the terms designing the fields of the search form to which are un use in his domain of activity. The whole classification proposed is about 110 different topics, each one having a set of terms chosen to entitle the fields of the interface of thematic search as well the submission forms. Moreover, seven-entry points are proposed in the first step of the connection, driving the user towards the appropriate context whether the purpose of the document to find: teaching and scholarship, technical data, image and graphics, popular work, literature, society, other [...] . Indeed, the system must allow the flexibility that allows the replacement of any term by another if necessary. So the entire set of terms is put in a database. This system is also able to adapt the language chosen by the user. The submission forms filled by authors are parsed regarding the topic selected and the Dublin Core metadata file - as an XML file - is generated and stored in the indexing server. It is delivered on simple <b>metadata</b> <b>request,</b> and is reachable by a hyperlink included in any page of search results. The experimental system has been built and after a test period of two years, is drawn to take place in the site of the University of Poitiers by June 2005. The software will be soon released under free software licence (CECILL). Includes: Conference preprint, Powerpoint presentation, Abstract and Biographical notes, Pratt student commentaryXAInternationa...|$|R
40|$|The Astrophysics Source Code Library (ASCL) {{is a free}} online {{registry}} {{of research}} codes; it is indexed by ADS and Web of Science and has over 1300 code entries. Its entries are increasingly used to cite software; citations have been doubling each year since 2012 and every major astronomy journal accepts citations to the ASCL. Codes in the resource cover all aspects of astrophysics research and many programming languages are represented. In the past year, the ASCL added dashboards for users and administrators, started minting Digital Objective Identifiers (DOIs) for software it houses, and added <b>metadata</b> fields <b>requested</b> by users. This presentation covers the ASCL's growth {{in the past year}} and the opportunities afforded it {{as one of the few}} domain libraries for science research codes. Comment: 4 pages, 2 figures; to be published in ADASS XXVI (held October 16 - 20, 2016) proceeding...|$|R
5000|$|In August 2013, CNet {{reported}} that DITU helped developing custom [...] "port reader" [...] software {{that enables the}} FBI to collect metadata from internet traffic in real time. This software copies the internet communications as they flow through a network and then extracts only the <b>requested</b> <b>metadata.</b> The CNet report says that the FBI is quietly pressing telecom carriers and Internet service providers to install this software onto their networks, {{so it can be}} used in cases where the carriers' own lawful interception equipment cannot fully provide the data the Bureau is looking for.|$|R
50|$|Singingfish {{employed}} its own web crawler, Asterias, {{designed specifically}} to ferret out audio and video links across the web. In 2003 and 2004, Asterias discovered {{an average of about}} 50,000 new pieces of multimedia content a day. A proprietary system was used to process each of the <b>discovered</b> links, extracting <b>metadata</b> and then enhancing it prior to indexing as much multimedia content on the web has little or poor metadata. Many of the multimedia URLs used as seeds for Singingfish's crawlers and annotation engines came from cache logs from the NSF-funded National Laboratory for Applied Network Research (NLANR) IRCache Web Caching project.|$|R
40|$|Researchers have {{traditionally}} used bibliographic data-bases {{to search out}} information. Today, the full-text of resources is increasingly available for searching, and more researchers are performing full-text searches. This study compares differences {{in the number of}} articles <b>discovered</b> between <b>metadata</b> and full-text searches of the same literature cohort when searching for gene names in two biomedical literature domains. Three re-viewers additionally ranked 100 articles in each domain. Significantly more articles were discovered via full-text searching; however, the precision of full-text searching also is significantly lower than that of metadata search-ing. Certain features of articles correlated with higher relevance ratings. A significant feature measured was the number of matches of the search term in the full-text of the article, with a larger number of matches having a statistically significant higher usefulness (i. e., rele-vance) rating. By using the number of hits of the search term in the full-text to rank the importance of the article, performance of full-text searching was improved so that both recall and precision were as good as or better than that for metadata searching. This suggests that full-text searching alone may be sufficient, and that metadata searching as a surrogate is not necessary...|$|R
5000|$|Problems {{involving}} metadata {{in litigation}} in the United States are becoming widespread. Courts {{have looked at}} various questions involving metadata, including the discoverability of metadata by parties. Although the Federal Rules of Civil Procedure have only specified rules about electronic documents, subsequent case law has elaborated on the requirement of parties to reveal metadata. In October 2009, the Arizona Supreme Court has ruled that metadata records are public record. Document metadata have proven particularly important in legal environments in which litigation has <b>requested</b> <b>metadata,</b> which can include sensitive information detrimental to a certain party in court. Using metadata removal tools to [...] "clean" [...] or redact documents can mitigate the risks of unwittingly sending sensitive data. This process partially (see data remanence) protects law firms from potentially damaging leaking of sensitive data through electronic discovery.|$|R
40|$|With {{the growth}} of digital {{libraries}} and the digital library federation in addition to partially unstructured collections of documents such as web sites, a large set of vendors are offering engines for retrieving content and <b>metadata</b> via search <b>requests</b> by the end user (queries). In most cases these queries are short unstructured fragments of text in different languages {{that are difficult to}} make sense of {{because of the lack of}} context. When attempting to perform automatic translation of these queries, using machine learning approaches, the problem becomes worse as aligned corpora are almost inexistent for such types of linguistic data. The GALATEAS European project concentrates on analyzing language-based information from transaction logs and facilitates the development of improved navigation and search technologies for multilingual content access, in order to offer digital content providers an innovative approach to understanding users' behaviour...|$|R
40|$|Project Andvari is {{designed}} to provide integrated access to dispersed collections of northern European art and artifacts of the early medieval period (4 th- 12 th centuries). Our goal is to create a digital portal offering aggregated search options and enhanced <b>metadata.</b> Funding is <b>requested</b> to convene an international workshop for humanities scholars, museum professionals, and technology experts to refine the conceptual design of the proposed research tool and identify its technological requirements in preparation for a pilot project. Ultimately, Project Andvari will facilitate interdisciplinary research in art, archaeology, history, and literary and religious studies of the northern periphery of medieval Europe. It will allow users to study visual culture across media and beyond traditional geographical and disciplinary boundaries. Its innovative application of search methods will promote analyses of relationships of artifacts and cultures, and help us discover the hitherto unnoticed...|$|R
40|$|Most recent {{distributed}} file {{systems have}} adopted architecture with an independent metadata server cluster. However, potential multiple hotspots and flash crowds access patterns often cause a metadata service that violates performance Service Level Objectives. To maximize the throughput of the metadata service, an adaptive request load balancing framework is critical. We present a distributed cache framework above the distributed metadata management schemes to manage hotspots rather than managing all <b>metadata</b> to achieve <b>request</b> load balancing. This benefits the metadata hierarchical locality {{and the system}} scalability. Compared with data, metadata has its own distinct characteristics, such as small size and large quantity. The cost of useless metadata prefetching is much less than data prefetching. In light of this, we devise a time period-based prefetching strategy and a perfecting-based adaptive replacement cache algorithm to improve {{the performance of the}} distributed caching layer to adapt constantly changing workloads. Finally, we evaluate our approach with a hadoop distributed file system cluster. © 2013 IEEE. Most recent distributed file systems have adopted architecture with an independent metadata server cluster. However, potential multiple hotspots and flash crowds access patterns often cause a metadata service that violates performance Service Level Objectives. To maximize the throughput of the metadata service, an adaptive request load balancing framework is critical. We present a distributed cache framework above the distributed metadata management schemes to manage hotspots rather than managing all <b>metadata</b> to achieve <b>request</b> load balancing. This benefits the metadata hierarchical locality and the system scalability. Compared with data, metadata has its own distinct characteristics, such as small size and large quantity. The cost of useless metadata prefetching is much less than data prefetching. In light of this, we devise a time period-based prefetching strategy and a perfecting-based adaptive replacement cache algorithm to improve the performance of the distributed caching layer to adapt constantly changing workloads. Finally, we evaluate our approach with a hadoop distributed file system cluster. © 2013 IEEE...|$|R
40|$|Introduction The {{functions}} {{to be provided}} by ECS are organized in the proposed architecture in four different layers as described in this chapter. The architectural principles {{that led to the}} architecture are also described here. 1. 2 Architectural Principles The ECS Federated Client-Server architecture was designed to meet the requirements and research drivers presented in chapter 1 of Part I. Several architectures could have resulted from these requirements and drivers. In order to guide and focus the design process, the following architectural principles were used. 2 ffl P 1. Location transparency: users should be able to access data products, <b>request</b> <b>metadata,</b> and browse data, without having to know in which DAAC, DADC, or ADC, the data resides. This implies that data can migrate to achieve load balance, cope with failures, and improve performance without impacting the users of EOSDIS. ffl P 2. Modularity: th...|$|R
40|$|Abstract. This paper {{describes}} the operational characteristics of “CompTorrent”, a general purpose distributed computing platform {{that provides a}} low entry cost to creating new distributed computing projects. An algorithm is embedded into a metadata file along with data set details which are then published on the Internet. Potential nodes <b>discover</b> and download <b>metadata</b> files for projects they wish to participate in, extract the algorithm and data set descriptors, and join other participants in maintaining a swarm. This swarm then cooperatively shares the raw data set in pieces between nodes and applies the algorithm to produce a computed data set. This computed data set is also shared and distributed amongst participating nodes. CompTorrent allows a simple, “home-brewed ” solution for small or individual distributed computing projects. Testing and experimentation have shown CompTorrent {{to be an effective}} system that provides similar benefits for distributed computing to those BitTorrent provides for large file distribution...|$|R
40|$|Abstract. Current dataintegration systems allow avariety of {{heterogeneous}} structured or semi-structured {{data sources}} {{to be combined}} and queried by providing an integrated view over them. The Semantic Web also requires {{us to be able}} to integrate information from a variety of heterogeneous information sources. However, these information sources will also include natural language (e. g. web pages) and ontologies. In this paper we describeanarchitecture which combines the data integration approach with techniques from Information Extraction in order to allow information from ontologies and natural language sources to be integrated with other, semantically related, structured or semi-structured data. Our architecture is based on the AutoMed data integration system, and in this paper we describe several extensions which have been made to AutoMed in order to support this work, including adding RDF to the data sources supported by AutoMed and providing a repository for the data and <b>metadata</b> <b>discovered</b> by the Information Extraction process. ...|$|R
