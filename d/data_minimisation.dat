26|11|Public
5000|$|The Data Protection Act 1998 (s33) gives {{research}} exemptions for {{the purposes}} of statistical and historic research purposes, most significantly on the principles of indefinite retention and <b>data</b> <b>minimisation,</b> as well as Subject Access rights, for as long as data are processed for the legitimate interests of the Data Controller. To qualify for the research exemption, the research must be able to comply with the following ‘relevant conditions’: ...|$|E
40|$|<b>Data</b> <b>minimisation</b> is a privacy-enhancing {{principle}} {{considered as}} one of the pillars of personal data regulations. This principle dictates that personal data collected should be no more than necessary for the specific purpose consented by the user. In this paper we study <b>data</b> <b>minimisation</b> from a programming language perspective. We assume that a given program embodies the purpose of data collection, and define a data minimiser as a pre-processor for the input which reduces the amount of information available to the program without compromising its functionality. In this context we study formal definitions of <b>data</b> <b>minimisation,</b> present different mechanisms and architectures to ensure <b>data</b> <b>minimisation,</b> and provide a procedure to synthesise a correct data minimiser for a given program...|$|E
40|$|Systems {{dealing with}} {{personal}} information are legally required {{to satisfy the}} principle of <b>data</b> <b>minimisation.</b> Privacy-enhancing protocols use cryptographic primitives to minimise the amount of personal information exposed by communication. However, the complexity of these primitives and their interplay {{makes it hard for}} non-cryptography experts to understand the privacy implications of their use. In this paper, we present TRIPLEX, a framework for the analysis of <b>data</b> <b>minimisation</b> in privacy-enhancing protocols. Keywords: Data minimisation; Coalition Graphs; Detectability; Linkabilit...|$|E
3000|$|... [*]To {{define and}} {{implement}} algorithms {{to manage the}} storage, replication and migration of content across multiple repository storage providers. This takes into account both policy-based <b>data</b> management and <b>minimisation</b> of storage costs.|$|R
3000|$|This approach, however, {{would not}} {{explicitly}} {{take into account}} the structure of multiview <b>data</b> in the <b>minimisation.</b> In the following we show how (19) is solved by imposing the camera setup and the occlusion constraints.|$|R
40|$|A {{review of}} {{progress}} {{in the treatment of}} wastewater from slaughterouses is presented. Significant progress in issues such as nutrient removal and high-rate anaerobic treatment are highlighted. Nevertheless, few <b>data</b> concerning waste <b>minimisation</b> and source reduction in slaughterhouses, which offers the most cost-effective route to waste management in the industry, exist. The information presented enables {{a better understanding of the}} problems encountered with the effluent from the industry and common pitfalls with its treatment...|$|R
40|$|In recent years, {{a number}} of {{infrastructures}} have been proposed for the collection and distribution of medical data for research purposes. The design of such infrastructures is challenging: on the one hand, they should link patient data collected from different hospitals; on the other hand, they can only use anonymised data because of privacy regulations. In addition, they should allow data depseudonymisation in case research results provide information relevant for patients’ health. The privacy analysis of such infrastructures {{can be seen as}} a problem of <b>data</b> <b>minimisation.</b> In this work, we introduce coalition graphs, a graphical representation of knowledge of personal information to study <b>data</b> <b>minimisation.</b> We show how this representation allows identification of privacy issues in existing infrastructures. To validate our approach, we use coalition graphs to formally analyse <b>data</b> <b>minimisation</b> in two (de) -pseudonymisation infrastructures proposed by the Parelsnoer initiative...|$|E
40|$|<b>Data</b> <b>minimisation</b> is a privacy {{enhancing}} principle, {{stating that}} personal data collected {{should be no}} more than necessary for the specific purpose consented by the user. Checking that a program satisfies the <b>data</b> <b>minimisation</b> principle is not easy, even for the simple case when considering deterministic programs-as-functions. In this paper we prove (im) possibility results concerning runtime monitoring of (non-) minimality for deterministic programs both when the program has one input source (monolithic) and for the more general case when inputs come from independent sources (distributed case). We propose monitoring mechanisms where a monitor observes the inputs and the outputs of a program, to detect violation of <b>data</b> <b>minimisation</b> policies. We show that monitorability of (non) minimality is decidable only for specific cases, and detection of satisfaction of different notions of minimality in undecidable in general. That said, we show that under certain conditions monitorability is decidable and we provide an algorithm and a bound to check such properties in a pre-deployment controlled environment, also being able to compute a minimiser for the given program. Finally, we provide a proof-of-concept implementation for both offline and online monitoring and apply that to some case studies. Comment: 24 page...|$|E
40|$|With {{the growing}} amount of {{personal}} information exchanged over the Internet, privacy {{is becoming more}} and more a concern for users. One of the key principles in protecting privacy is <b>data</b> <b>minimisation.</b> This principle requires that only the minimum amount of information necessary to accomplish a certain goal is collected and processed. "Privacy-enhancing" communication protocols have been proposed to guarantee <b>data</b> <b>minimisation</b> in a wide range of applications. However, currently there is no satisfactory way to assess and compare the privacy they offer in a precise way: existing analyses are either too informal and high-level, or specific for one particular system. In this work, we propose a general formal framework to analyse and compare communication protocols with respect to privacy by <b>data</b> <b>minimisation.</b> Privacy requirements are formalised independent of a particular protocol in terms of the knowledge of (coalitions of) actors in a three-layer model of personal information. These requirements are then verified automatically for particular protocols by computing this knowledge from a description of their communication. We validate our framework in an identity management (IdM) case study. As IdM systems are used more and more to satisfy the increasing need for reliable on-line identification and authentication, privacy is becoming an increasingly critical issue. We use our framework to analyse and compare four identity management systems. Finally, we discuss the completeness and (re) usability of the proposed framework...|$|E
40|$|Abstract—This paper {{examines}} orthonormal regression and wavelet denoising {{within the}} Minimum Message Length (MML) framework. A criterion for hard thresholding that naturally incorporates parameter shrinkage {{is derived from}} a hierarchical Bayes approach. Both parameters and hyperparameters are jointly estimated from the <b>data</b> directly by <b>minimisation</b> of a two-part message length, and the threshold implied by the new criterion is shown to have good asymptotic optimality properties with respect to zero-one loss under certain conditions. Empirical comparisons made against similar criteria derived from the Minimum Description Length principle demonstrate that the MML procedure is competitive in terms of squared-error loss. I...|$|R
40|$|This study {{presents}} a minimum jitter-based adaptive decision feedback equaliser (DFE) for giga-bit-per-second (Gbps) serial links. The adaptation {{in search for}} the optimal tap coefficients of DFE is carried out with the objective to minimise data jitter {{at the edge of}} <b>data</b> eyes. Jitter <b>minimisation</b> is achieved by adjusting the slope of the DFE that counteracts that of the channel. The effectiveness of the proposed adaptive DFE is evaluated by embedding the DFE in a 2 Gbps serial link. The data link is analysed using Spectre from Cadence Design Systems with BSIM 4 device models. Simulation results demonstrate that the proposed adaptive DFE is capable of opening closed data eyes with 83 % vertical opening, 68 % horizontal opening and 16 % data jitter over 1 m FR 4 channel while consuming 15. 45 mW...|$|R
40|$|The {{ability to}} generalize beyond the {{training}} set is paramount for any {{machine learning algorithm}} and Genetic Programming (GP) is no exception. This paper investigates a recently proposed technique to improve generalisation in GP, termed Interleaved Sampling where GP alternates between using the entire data set and only a single data point in alternate generations. This paper proposes two alternatives to using a single data point : the use of random search instead of a single data point, and simply minimising the tree size. Both the approaches are more efficient than the original Interleaved Sampling because they simply do not evaluate the fitness in half the number of generations. The results show {{that in terms of}} generalisation, random search and size minimisation are as effective as the original Interleaved Sampling; however, they are computationally more efficient in terms of <b>data</b> processing. Size <b>minimisation</b> is particularly interesting because it completely prevents bloat while still being competitive in terms of training results as well as generalisation. The tree sizes with size minimisation are substantwlly smaller reducing the computational expense substantially...|$|R
40|$|International audiencePrivacy {{by design}} {{will become a}} legal {{obligation}} in the European Community if the Data Protection Regulation eventually gets adopted. However, taking into account privacy requirements {{in the design of}} a system is a challenging task. We propose an approach based on the specification of privacy architectures and focus on a key aspect of privacy, <b>data</b> <b>minimisation,</b> and its tension with integrity requirements. We illustrate our formal framework through a smart metering case study...|$|E
40|$|The ongoing {{development}} of computing, communications and storage technologies presents {{a challenge to}} privacy protection, given the increasing ease with which personal data can be collected, analysed, stored and shared. Computer scientists have developed “privacy by design” techniques such as <b>data</b> <b>minimisation,</b> which help to enforce the data protection and privacy safeguards contained in national legal frameworks and international human rights instruments. Such techniques provide a template for societies that wish to ensure the continued protection of core social values in an increasingly technology-mediated world. </p...|$|E
40|$|Privacy {{by design}} {{will become a}} legal {{obligation}} in the European Community if the Data Protection Regulation eventually gets adopted. However, taking into account privacy requirements {{in the design of}} a system is a challenging task. We propose an approach based on the specification of privacy architectures and focus on a key aspect of privacy, <b>data</b> <b>minimisation,</b> and its tension with integrity requirements. We illustrate our formal framework through a smart metering case study. Comment: appears in STM - 10 th International Workshop on Security and Trust Management 8743 (2014...|$|E
40|$|Abstract Background Elementary flux modes (EFM) {{are unique}} and non-decomposable sets of {{metabolic}} reactions {{able to operate}} coherently in steady-state. A metabolic network has in general a very high number of EFM reflecting the typical functional redundancy of biological systems. However, most of these EFM are either thermodynamically unfeasible or inactive at pre-set environmental conditions. Results Here we present a new algorithm that discriminates the "active" set of EFM {{on the basis of}} dynamic envirome data. The algorithm merges together two well-known methods: projection to latent structures (PLS) and EFM analysis, and is therefore termed projection to latent pathways (PLP). PLP has two concomitant goals: (1) maximisation of correlation between EFM weighting factors and measured envirome <b>data</b> and (2) <b>minimisation</b> of redundancy by eliminating EFM with low correlation with the envirome. Conclusions Overall, our results demonstrate that PLP slightly outperforms PLS in terms of predictive power. But more importantly, PLP is able to discriminate the subset of EFM with highest correlation with the envirome, thus providing in-depth knowledge of how the environment controls core cellular functions. This offers a significant advantage over PLS since its abstract structure cannot be associated with the underlying biological structure. </p...|$|R
40|$|ABSTRACT Much of our {{knowledge}} of drugs originates from clinical trials of drug efficacy performed on stringently selected patient groups, often without multiple concurrent diseases. However, the effectiveness of treatment under conditions of use in ordinary clinical practice may be very different to conditions in the randomised clinical trial. Use of large computerised data bases and record linkage has thus become increasingly common in pharmacoepidemiologic research. The greatest advantages of using routinely collected <b>data</b> are the <b>minimisation</b> of study costs and time required to complete a study, considerations that are particularly relevant for longitudinal studies. The advantages of using data bases also include the possibility of obtaining large sample sizes and to retrospectively study long-term outcomes. The risk for recall bias, a significant problem in interviews and questionnaires, is also reduced. However, computerised data bases also have some potentially serious disadvantages, primarily {{in the areas of}} data validity and data availability. The Tierp study, including individually based data bases of prescription drug use, will be used here as an example of research. In this paper an example of a comprehensive data base study concerning health care and drug utilisation in depressed patients is presented. Methodological considerations in data base research are discussed in relation to experiences from the antidepressant study. A well planned and research oriented computerised data base on prescription drugs represents an important tool {{in the study of the}} outcome of drug treatment in real world clinical practice. </span...|$|R
40|$|In {{the context}} of climate change and {{increasing}} energy conversion efficiency solid oxide fuel cells (SOFCs) are likely {{to play an important}} role in the production of electricity. The tubular SOFC configuration is considered to be the most advanced and is approaching commercialisation. A major advantage of SOFCs over other types of fuel cells is that they can utilise a wide spectrum of fuels (natural gas, coal and biomass syn-gas). This is due to its high operating temperature, which also makes them suitable for integration with gas turbines and for cogeneration. A R&D project is underway to develop a computer simulation model of a tubular SOFC that can accurately predict performance under various conditions and using a range of fuels. A model is developed using the process simulator aspen plus. The software uses unit operation blocks, which are models of process operations. The user places these blocks on a flowsheet, specifying material and energy streams. There is no built in model that can represent a SOFC, however it is possible to construct one using the built in unit operation blocks. This method is an alternative to developing a fuel cell model using programming languages. The model is based on Gibbs free energy <b>minimisation.</b> <b>Data</b> available in the literature on the Siemens Power Generation tubular SOFC was used to validate the model. The model predicts thermodynamic condition and chemical composition of the stack exhaust gases, heat generated, voltage, current, and electrical efficiency. Fuel composition and operating parameters were varied over a wide range. Operating parameters such as fuel utilisation factor, current density, and steam to carbon ratio were found to have significant influence. In a future study this SOFC stack model will be integrated with a biomass gasifier model and balance of plant models all developed in aspen plus. From examination of the sensitivity analyses’ results optimum conditions are established...|$|R
40|$|Part 7 : Software Security and PrivacyInternational audienceData minimisation is a privacy-enhancing {{principle}} {{considered as}} one of the pillars of personal data regulations. This principle dictates that personal data collected should be no more than necessary for the specific purpose consented by the user. In this paper we study <b>data</b> <b>minimisation</b> from a programming language perspective. We define a data minimiser as a pre-processor for the input which reduces the amount of information available to the program without compromising its functionality. We give its formal definition and provide a procedure to synthesise a correct data minimiser for a given program...|$|E
40|$|Secondary use {{of health}} data has {{a vital role}} in {{improving}} and advancing medical knowledge. While digital health records offer scope for facilitating the flow of data to secondary uses, it remains essential that steps are taken to respect wishes of the patient regarding secondary usage, and to ensure the privacy of the patient during secondary use scenarios. Consent, together with depersonalisation and its related concepts of anonymisation, pseudonymisation, and <b>data</b> <b>minimisation</b> are key methods used to provide this protection. This paper gives an overview of technical, practical, legal, and ethical aspects of secondary data use and discusses their implementation in the multi-institutional @neurIST research project...|$|E
40|$|Abstract. More {{and more}} {{personal}} information is exchanged on-line using com-munication protocols. This makes it increasingly important that such protocols satisfy <b>data</b> <b>minimisation.</b> Formal {{methods have been}} used to verify privacy prop-erties of protocols; but so far, mostly for ad-hoc applications. In previous work, we provided general definitions for the fundamental privacy concepts of linkabil-ity and detectability. However, this approach is only able to verify privacy prop-erties for given protocol instances. In this work, by generalising the approach, we formally analyse privacy of communication protocols independently from any instance. We implement the model; identify its assumptions by relating it to the instantiated model; and show how to visualise results. To demonstrate our ap-proach, we analyse privacy in Identity Mixer. ...|$|E
40|$|Saarbrücken, den 16. Mai 2003 iii iv This thesis {{introduces}} data graphs as {{a formal}} {{model for the}} objects in a programming system’s memory, and describes three services on such <b>data</b> graphs: linearisation, <b>minimisation,</b> and transformation. The SEAM system offers an abstract store that provides a programming language’s implementor with a platform- and language-independent abstraction layer, hiding the complex issues of memory management. This thesis aims at giving a formal description of this store and of the services mentioned above. Data graphs are presented here as a formal model for the objects that reside in such a store. Starting from this model, an abstract store can be described by an abstract data type (ADT) that implements data graphs as imperative objects. The linearisation service (also known as “pickling”) translates a data graph into a linear, external, platform-independent representation (a pickle) from which {{a copy of the}} original graph can be reconstructed. Pickles can be written to files for persistence, or distributed over a network, implementing inter-process communication. Linearisation and delinearisation are described formally in terms of the data graph, and the SEAM implementation of pickling and unpickling is discussed. Minimisation applies graph <b>minimisation</b> techniques to <b>data</b> graphs, yielding a store service that eliminates redundancy in the graph. The formal background as well as implementation issues of this service are presented, and its applicability to data graphs is evaluated. Finally, the store is extended by transients, a mechanism essential for an efficient implementation of futures, logic variables and lazy evaluation. Transients are applied to both minimisation and linearisation; for the latter, they allow for the implementation of a powerful transformation mechanism that translates between an internal and an external representation of data graphs during pickling and unpickling. v v...|$|R
40|$| {{clinical}} trials {{is not necessarily}} the same as health outcome of drug use in everyday practice. The {{clinical trials}} often have limited samples of carefully se-lected patients. Moreover, drug utilization research also provides insight into the efficiency of drug use, i. e. whether a certain drug therapy provides relevant health gains. Drug utilization research can thus help to set priorities for the rational allocation of health care budgets. Therefore for the second aspect of drug use, we focused on health outcome (benefits and side-effects) and economic consequences of drug use (chapter 4, 5 and 7). In these chapters, we assessed the potential effects of a specific drug (as an exposure factor) on cardio-renal outcomes such as changes in blood pressure, UAE and GFR. Economic consequences including cost/benefit of treatment were estimated using the data from a clinical trial (chapter- 7)), using state-of-the-art pharmacoeconomic methods. Thirdly, we provided detailed information on patterns of drug use, particularly for those drug related to cardio-renal outcomes, as we were interested in the differences between the effects of current versus former users. Such descriptions are most meaningful when {{they are part of a}} continuous evaluation system, i. e. when the patterns are followed over time and trends in drug use can be described, enabling us to specify the differences between those who have stopped, started or continuously used a medication (chapter 4 and 5). Fourthly, we could assess the quality of drug use. Quality indicators of drug use that are included in our analysis involve type of regimens, drug dosage and period of drug exposure. In this thesis, these indicators are used to determine the difference between type of regimen, drug dosage and period of expousre on renal outcomes such as UAE and GFR. 	 Methodological consideration: experiences and challenges Study design, confounding and propensity score in observational study An important aspect of our studies is that we were able to provide a data from a prospective observational cohort study with long term follow-up (4. 2 years for the clinical data of PREVEND and 6. 5 years of pharmacy data of IADB). Observational studies always raise concerns about biases in some that may account for or contribute to the findings. In observational studies, patients and their physicians select treatment on the basis of clinical need or preference, which can result in differences in clinical outcomes solely because of differences between those who do and do not receive treatment (indication bias). In contrast, random assignments in an RCT garantees that patient characteristics, both known and unknown, will be the same in the treatment groups. However, knowledge derived from RCT’s cannot always be translated directly to daily clinical practice. Patients that are included in an RCT frequently are not the ones that clinicians encounter in their office. This is especially true for nephrology, as in many of the important cardiovascular trials studying the effect of antihypertensives, statins, or anticoagulants patients with chronic kidney disease are excluded. In other words, the observational study provides information about treatment in the population in daily practice, and differs from the situation in clinical trials [23 - 24]. An alternative way of dealing with confounding by indication caused by non-randomized assignment of treatments in cohort studies, is the use of propensity scores, a method developed by Rosenbaum and Rubin [25 - 26]. The propensity score of an individual is defined as the conditional probability of being treated given the individual’s co-variates. However, this technique can not adjust for residual un-measured covariates, but can aid in understanding determinants of drug use and lead to improved estimates of drug effects [27]. Internal and external validity-All studies in the pharmacoepidemiology part of this thesis are based on electronic-drug-dispensing data from community pharmacies. We used the computerised pharmacy database (from IADB) that provides valid and reliable information on drug use. Previous studies have demonstrated that dispensing data from Dutch pharmacies offer an accurate picture of the use of prescription drugs outside the hospital [28 - 29]. However, one limitation is that pharmacy data do not have information about indication of the drug. Use of large computerised databases and record linkage has become increasingly important in pharmacoepidemiology research. The greatest advantages of using routinely collected <b>data</b> are <b>minimisation</b> of study costs and time required to complete a study, considerations that are particularly relevant for longitudinal studies such as in this thesis. The advantage of using databases also includes the possibility of obtaining large sample sizes and reduces the risk for recall bias, which is a significant problem in interviews and questionnaire methods. Another advantage of using pharmacy data is the detailed information regarding drug use (duration, dose, and drug classes) during the whole study period. We also were able to compare the drug use between subjects in the PREVEND cohort with drug use in the general population (from IADB). Clinical impact and future research A screening programme on microalbuminuria could also help to detect subjects with undiagnosed diabetes, hypertension and hyperlipidemia. Our study showed that one third of the hypertensive subjects were not yet known to have hypertension and one third of those with hypertension were not adequately treated. Comparable data were found for hyperlipidemia. Furthermore, we found that the likelihood to use blood pressure or lipid lowering drugs increased when higher values of blood pressure or cholesterol level were found. Unfortunately, the presence of concomitant risk factors did not influence the prescribing behaviour. This aspect was further studied in chapter- 3. After a long period of follow-up, we could not find any medicalization effect in our screened population compared to a non-screened population. We could thus not confirm that screening for cardio-renal risk factors leads to more drug prescribing and might therefore be harmful for the population. Interestingly, we also showed that a screening is in fact only effective in improving drug use, when it is limited to those with a higher risk, such as in a cohort enriched for albuminuria. 	Another point addressed in this thesis refers to the relation between use of a specific drug and the change in albuminuria and renal function over time. Firstly, we should pay attention to our finding that statins are associated with a rise in albuminuria in our observational study. Although this finding was not confirmed in our PREVEND IT clinical trial, this finding requires further attention. Fortunately, statins were not associated with a fall in GFR. Secondly, we found that hormonal contraceptives independently were associated with a worsening of blood pressure, albuminuria and renal function, but our data also showed that stopping may result in correction of these effects. Even though our data are limited to subjects with only modest renal damage, we do think these data are of interest for clinicians as also early renal damage is associated with an impaired renal and vascular prognosis. Because statins and hormonal contraceptives are widely used in the general population our results may be of public health importance and need confirmation in other studies. Can we implement a screening program for albuminuria in daily practice? The evidence showed that a screening program with subsequent treatment for those with an elevated albuminuria improved CV morbidity and mortality. Our data suggest that screening of albuminuria and subsequent treatment with an ACE inhibitor appeared to be cost effective in our population. The costs needed to gain one life year were € 16, 700. The costs were even lower when we limited the analysis to subjects with an UAE > 50 mg/d and an age over 50 or 60 year. This study is based on a single screening for microalbuminuria of the general population. Further studies are needed to evaluate whether re-screening at some later time offers additional benefit. A Markov Model should be developed to simulate a periodic screening procedure in the general population inclusive long-term beneficial effects on renal damage. The PREVEND cohort and IADB form a unique set of data. Presently, third screening of the PREVEND subjects has been completed and data on cardiovas-cular and renal morbidity and mortality are available and pharmacy data havebeen collected until the end of 2005. These long-term follow-up (approximately 10 years) makes it possible to provide longitudinal analyses on cardiovascular and renal disease progression in relation to drug use. ...|$|R
40|$|Part 1 : Full PapersInternational audienceMore {{and more}} {{personal}} information is exchanged on-line using communication protocols. This makes it increasingly important that such protocols satisfy privacy by <b>data</b> <b>minimisation.</b> Formal {{methods have been}} used to verify privacy properties of protocols; but so far, mostly in an ad-hoc way. In previous work, we provided general definitions for the fundamental privacy concepts of linkability and detectability. However, this approach is only able to verify privacy properties for given protocol instances. In this work, by generalising the approach, we formally analyse privacy of communication protocols independently from any instance. We implement the model; identify its assumptions by relating it to the instantiated model; and show how to visualise results. To demonstrate our approach, we analyse privacy in Identity Mixer...|$|E
40|$|Transparency {{is a key}} {{principle}} in democratic societies. For example, the public sector is in part kept honest and fair {{with the help of}} transparency through different freedom of information (FOI) legislations. In the last decades, while FOI legislations have been adopted by more and more countries worldwide, we have entered the information age enabled by the rapid development of information technology. This has led to the need for technological solutions that enhance transparency, for example to ensure that FOI legislation can be adhered to in the digital world. These solutions are called transparency-enhancing tools (TETs), and consist of both technological and legal tools. TETs, and transparency in general, can be in conflict with the privacy principle of <b>data</b> <b>minimisation.</b> The goal of transparency is to make information available, while the goal of <b>data</b> <b>minimisation</b> is to minimise the amount of available information. This thesis presents two privacy-preserving TETs: one cryptographic system forenabling transparency logging, and one cryptographic scheme for storing the data for the so called Data Track tool at a cloud provider. The goal of the transparency logging TET is to make data processing by data controllers transparent to the user whose data is being processed. Our work ensures that the process in which the data processing is logged does not leak sensitive information about the user, and thatthe user can anonymously read the information logged on their behalf. The goal of the Data Track is to make it transparent to users which data controllers they have disclosed data to under which conditions. Furthermore, the Data Track intends to empower users to exercise their rights, online and potentially anonymously, with regard to their disclosed data at the recipient data controllers. Our work ensures that the data kept by the Data Track can be stored at acloud storage provider, enabling easy synchronisation across multiple devices, while preserving the privacy of users by making their storage anonymous toward the provider and by enabling users to hold the provider accountable for the data it stores...|$|E
40|$|Bringing {{pedagogy}} and learner {{agency to}} the forefront of the design of learning analytics systems is the central concern of this paper. With the new European data protection regulations now i n place we consider their potential to influence systems design from this perspective. In particular, the principles of Data Protection by Design and Date Protection by Default are explored to see if conforming to these principles will bring the focus of l earning analyti cs systems back to the learner. The emerging understanding leads to a model of the relationship between <b>data</b> <b>minimisation</b> and utility within contexts of data sharing, which allows constructing scenarios that highlight how pedagogy and data p rotection are related and could inform the design of LA solutions. Our analysis suggests that the new data protection regulations will influence development and implementation of learning analytics systems and that the pedagogical grounding of these system s will be strengthene...|$|E
40|$|Abstract In [13] Dutch {{government}} proposes {{an identity}} scheme sup-porting personal data exchange of pupils with private e-textbook pub-lishers. This design propagates sharing personal numbers of pupils among private parties violating the <b>data</b> <b>minimisation</b> principle in privacy laws. We describe a privacy friendly alternative, giving pupils (and parents) control on exchange {{of their personal}} data. Three generic forms based on homomorphic encryption are used as building blocks. These forms do not yield personal numbers, or even personal data from a legal per-spective, and have strong, unlinkability properties. Only if required a school provides a party with a party-specific pseudonym identifying a pupil. For this the school is provided an encrypted pseudonym by a cent-ral party based on a polymorphic pseudonym formed by the school. Only intended parties, not even schools, have access to pseudonyms. Different publishers can send pupil test results to a school {{without being able to}} assess whether pupils are identical. We also describe support for privacy friendly attributes and user inspection as required by privacy laws...|$|E
40|$|Criminal records check {{not known}} £ 500 Locating a named person not known £ 60 Ex-directory search £ 40 £ 65 – £ 75 Mobile phone account not known £ 750 Licence check not known £ 250 “What price privacy?”, Information Commissioner’s Office (2006) Designing for privacy <b>Data</b> <b>minimisation</b> key: is your {{personal}} data really necessary? Limit personal data collection, storage, access and usage – enforced using cryptography Protects against hackers, corrupt insiders, data loss, {{as well as}} function creep Users must also be notified and consent to the processing of data – easy-to-use interfaces are critical. What are defaults? Jedrzejczyk et al. (2010) Transport pricing Monitor all traffic centrally (London), at kerbside (W London) or deduct payment from pay-as-you-go toll cards (Singapore) ? On-board unit (Balasch et al. 2010) ? Or tax parking spaces? Link all payment card usage (Oyster) or use unlinkable RFID tokens (Shenzen) ? MIT Technology Review (2006) Mobile data Is communication uni- or bi-directional or broadcast? Oblivious transfer Does sensor, user agent or network carry out triangulation and processing? What resolution data can network access? How long-lived and linkable are identifiers? IMSIs, TMSIs and location patterns Location-Based Services Can we use features of mobile phone networks to supply anonymous, targeted adverts...|$|E
40|$|This {{deliverable}} {{presents an}} assessment of the privacy and data protection compliance framework of the eVACUATE project, as evaluated in the previous tasks and during the four validation demonstrations. It contains analysis of the different elements of the eVACUATE solution from an ethical, privacy and data protection perspective and a summary of the insights on the impact of the technology on individuals’ rights as observer during the demonstrations. The deliverable lays specific emphasis on the regulatory impact of the entry into force of the General Data Protection Regulation (GDPR) on the different elements of eVACUATE and pays attention to the data protection by design and by default measures considered in the context of eVACUATE. It formulates future-proof recommendations for end users when considering the implementation of eVACUATE in a production environment and also provides guidance on the new requirement of the GDPR of conducting a data protection impact assessment (DPIA). The outcomes of the privacy and data protection analysis could be summarised as follows: RFID technology provides for detailed information for the data subjects, its use is entirely voluntary and sufficient safeguards respecting the data subjects’ rights have been suggested; <b>data</b> <b>minimisation</b> techniques have been implemented such as non-individualisation of the chipless RFID tags and not using unique identification numbers; clear retention periods or criteria for determining such periods must be established in a production environment. MobiMESH technology has clearly defined purposes and processes proportionate amount of data to the defined purposes of processing; data security measures have been implemented; in a production environment, data controllers should elaborate a clear privacy policy and terms of use, taking into account the conditions for valid consent of Article 7 GDPR (if consent is relied upon); clear retention periods or criteria for determining such periods must be established in a production environment. eVAMAPP applications are used on a purely voluntary basis; compliance with the GDPR’s intensified information and transparency obligations is essential; data protection by design and by default measures have been implemented: device’s location is not tracked, GPS satellites and iBeacons are not used to identify or store the IDs of the terminal devices, applications do not store unnecessary identity attributes; the key moment for activation of identification functions is proportionately related to the moment of declaring an evacuation. SoNeMa technology provides for targeted content analysis but it should be performed in a limited timespan, eg, when an emergency is clearly present; the legal ground for processing must be carefully assessed by the controller in a production environment. Crowd behaviour detection technology has narrowly defined purposes and does not perform identification of natural persons; it has improved accuracy; facial images from optical cameras should not be processed to perform identification; data from behaviour detection algorithms should not be linked with data about identifiable persons; transparency obligations are key, especially when combining this technology with relatively known legacy technologies, such as CCTV; the technology should not be active at all times. Counting technology has clearly defined purposes and improved data accuracy; <b>data</b> <b>minimisation</b> includes video of low quality which does not allow for performing facial recognition operations and automatic deletion of frames shortly after their technical processing by the system. nrpages: 190 status: publishe...|$|E
40|$|International audienceHealthcare Information Systems {{typically}} {{fall into}} the group of systems in which the need of data sharing conflicts with the privacy. A myriad of these systems have to, however, constantly communicate among each other. One {{of the ways to}} address the dilemma between data sharing and privacy is to use data obfuscation by lowering data accuracy to guarantee patient’s privacy while retaining its usefulness. Even though many obfuscation methods are able to handle numerical values, the obfuscation of non-numerical values (e. g., textual information) is not as trivial, yet extremely important to preserve data utility along the process. In this paper, we preliminary investigate how to exploit ontologies to create obfuscation mechanism for releasing personal and electronic health records (PHR and EHR) to selected audiences with different degrees of obfuscation. <b>Data</b> <b>minimisation</b> and access control should be supported to enforce different actors, e. g., doctors, nurses and managers, will get access to no more information than needed for their tasks. Besides that, ontology-based obfuscation can also be used for the particular case of data anonymisation. In such case, the obfuscation has to comply with a specific criteria to provide anonymity, so that the data set could be safely released. This research contributes to: state the problems in the area; review related privacy and data protection legal requirements; discuss ontology-based obfuscation and anonymisation methods; and define relevant healthcare use cases. As a result, we present the early concept of our Ontology-based Data Sharing Service (O-DSS) that enforces patient’s privacy by means of obfuscation and anonymisation functions...|$|E
40|$|The data {{protection}} laws in Europe require that data controllers provide privacy policies to inform individuals about the prospective processing {{of their personal}} data. The ever growing expressiveness of privacy policy languages allows to specify policies in {{a growing number of}} details. This and new options for policy negotiations transformed rather general privacy policies into specific privacy contracts between the data controller and the individual. In this report, we specify a privacy contract language and call it the Privacy Option Language. It is modelled after the analogy between financial option contracts and data disclosures which has been presented in previous work and led to the Privacy Option notion. The language specification provides privacy by design through its <b>data</b> <b>minimisation</b> provisions, i. e., all contracts are automatically reduced to their canonical form so that individual differences in the contract formulation are inherently normalised. The language specification is extensible in two ways. First, hooks are specified in the core language and can be used to connect sublanguages. The freedom to choose any suitable sublanguage allows to specify language details independent of the core language. Second, the Privacy Option Language itself {{can be used as a}} sublanguage within a more general-domain language. We give examples for both types of extensions. Additionally, we provide tools for evaluating semantics such as human-readable presentations of Privacy Options and contract management. The definitions of the semantics are kept simple and serve as templates for more practical ones. All functionality can be checked by interactive tests in a standard multi-purpose programming language interpreter, since the Privacy Option Language is specified as an embedded domain-specific language within Haskell. Hands-on examples are provided along with the language specification. PETweb I...|$|E
40|$|The Internet of Things {{is nothing}} new. First {{introduced}} as Ubiquitous Computing by Mark Weiser [49] around 1990, the basic {{concept of the}} “disappearing computer” has been studied as Ambient Intelligence or Pervasive Computing in the decades that followed. Today we witness the first large scale applications of these ideas. We see RFID technology being used in logistics, shopping, public transport and the like. The use of smart phones is soaring. Many of them are able to determine their location using GPS (Global Positioning System). Some phones already have NFC (Near Field Communication) capabilities, allowing them to communicate with objects tagged with RFID directly. Combined with social networking (like Facebook or Twitter), this gives rise to advanced location based services, and augmented reality applications. In fact social networks interconnecting things as well as humans have already emerged. Example are Patchube, a web-based service built to manage the world’s real-time data 1 and Flukso, a web-based community metering application 2. As the full ramifications of the Internet of Things start to unfold, this confluence of cyberspace and physical space is posing interesting new and fundamental research challenges. In particular, as we will argue in this essay, it has a huge impact {{in the area of}} security, privacy and trustability. As Bruce Schneier puts it {{in a recent issue of}} CryptoGram [38] (while discussing IT ingeneral) : “[ [...] . ] it’s not under your control, it’s doing things without your knowledge and consent, and it’s not necessarily acting in your best interests. ” The question then is how to ensure that, despite these adverse conditions, the Internet of Things is a safe, open, supportive and in general pleasant environment for people to engage with, or in fact for people to live in. This essay is structured as follows. We define the Internet of Things in section 2, and describe the main privacy, security and trustability issues associated with it in section 3. Solutions to these problems will have to deal with certain constraints, as explained in section 4. Section 5 discusses classical solutions based on <b>data</b> <b>minimisation</b> techniques, while section 6 discusses more recent alternative approaches. We conclude with an extensive overview of research challenges in section 7...|$|E
40|$|This study aims to {{generate}} from a three-dimensional data set {{of carbon dioxide}} ux in the Southern Ocean, a sample set for use with Kriging in order {{to generate}} estimated carbon dioxide ux values across the complete three-dimensional data set. In order to determine which sampling strategies are {{to be used with}} the three-dimensional data set, a number of a-priori and a-posteriori sampling methods are tested on a two-dimensional subset. These various sampling methods are used {{to determine whether or not}} the estimated error variance generated by Kriging is a good substitute for the true error as a measure of error. Carbon dioxide is a well known "greenhouse gas" and is partially responsible for climate change. However, some anthropogenic carbon dioxide is absorbed by the oceans and as such, the oceans currently play a mitigating role in climate change by acting as a sink for carbon dioxide. It has been suggested that if the production of carbon dioxide continues unabated that the oceans may become a source rather than a sink for carbon dioxide. This would mean that the oceanic carbon dioxide ux (exchange of carbon dioxide between the atmosphere and the surface of the ocean) would invert. As such, modelling of the carbon dioxide ux is of clear importance. Additionally as the Southern Ocean is highly undersampled, a sampling strategy for this ocean which would allow for high levels of accuracy with small sample sizes would be ideal. Kriging is a geostatistical weighted interpolation technique. The weights are based on the covariance structure of the data and the distances between points. In addition to an estimate at a point, Kriging also produces an estimated error variance which can be used as an indication of uncertainty. This study made use of model data for carbon dioxide ux in the Southern i Ocean. This data covers a year by making use of averaged data for 5 day intervals. This results in a three-dimensional data set covering latitude, longitude and time. This study used this data to generate a covariance structure for the data after the removal of trend and using this covariance structure, tested various sampling strategies in two dimensions, sampling approximately 10 % of the two-dimensional data subset. These sampling strategies made use of either the estimated error variance or the true error and included two simple heuristics, genetic algorithms, the Updated Kriging Variance Algorithm and Random Sampling. Two of the genetic algorithms tested were selected to maximise the error measure of interest, in order to determine the full range of errors that could be generated. The percentage absolute errors obtained across these methods ranged from 2 : 1 % to 64 : 4 %. Based on these strategies, the estimated error variance was determined to not be an accurate surrogate for true error and that in cases where absolute error is available, such as <b>data</b> <b>minimisation,</b> absolute error should be used as the measure of error. However, if no data is available then it does provide an easy to calculate measure of error. This study also concluded that Addition of a Point at Point of Maximum Absolute Error does provide a good validation sampling method to which other methods may be compared. Additionally, based on true errors and computational requirements, three methods were selected to be implemented on a three-dimensional subset of the data. These methods were Random Sampling, Addition of a Point at Point of Maximum Absolute Error and Addition of a Point at Point of Maximum Estimated Error Variance. Each of these methods for sampling were performed twice on the data, sampling up to approximately 5 % of the data. Random Sampling produced percentage absolute errors of 21 : 02 % and 20 : 98 %, Addition of a Point at Point of Maximum Estimated Error Variance produced errors of 18 : 54 % and 18 : 55 % while Addition of a Point at Point of Maximum Absolute Error was able to produce percentage absolute errors of 14 : 33 % and 14 : 32 %. Dissertation (MSc) [...] University of Pretoria, 2014. lk 2014 Mechanical and Aeronautical EngineeringMScUnrestricte...|$|E
40|$|Chapter 1 : Introduction •	The {{increasing}} {{demands for}} {{the benefits of}} payback from publicly funded R&D to be assessed are based partly {{on the need to}} justify or account for expenditure on R&D, and partly on the desire for information to assist resource allocation and the better management of R&D funds. The former consideration is particularly strong in relation to the R&D expenditure that comes out of the wider NHS budget. •	In this report a range of categories of payback will be identified along with a variety of methods for assessing them. •	The aim of the report is to make recommendations as to how the outcomes from health research might best be monitored on a regular basis. The specific context of the report is the NHS R&D Programme but many of the issues will be relevant {{for a wide range of}} funders of health R&D. •	The introduction sets out not only a plan of the report but also suggests that readers familiar with the general arguments and existing literature may choose to jump to Chapter 6. Chapter 2 : Review of Existing Approaches to Assessing the Payback from Research •	Existing work describes various approaches to valuing research. Some are ex ante and attempt to predict the outcomes of research being considered, others are ex post or retrospective. •	The five categories of benefit or payback from health R&D that have been identified involve contributions: to knowledge; to research capacity and future research; to improved information for decision making; to the efficiency, efficacy and equity of health care services; and to the nation’s economic performance. These are shown in Table 1 of the report •	The process by which R&D generates final outcomes can be modelled as a sequence. This includes primary outputs such as publications; secondary outputs in the form of policy or administrative decisions; and final outcomes which comprise the health and economic benefits. Feedback loops are also introduced and mitigate the limitations of a linear approach. •	Qualitative and quantitative approaches can be used but there are immense problems with time lags and attributing outcomes, and sometimes even outputs, to specific items of research funding. •	Four common methods of measuring payback can be used. Expert review, by peers or, sometimes, users is the traditional way of assessing the quality of research. Bibliometric techniques can involve not only counting publications but also using datasets such as the Science Citation Index and Wellcome’s Research Outputs Database (ROD). The various methods of economic analysis of payback are difficult to undertake given the costs and problems of acquiring relevant information and estimating benefits. Social science methods include case studies, which can provide useful information but are resource intensive, and questionnaires to researchers and potential research users. Chapter 3 : Characteristics of a Routine Monitoring System •	In moving from ad hoc or research studies of payback towards a more regular monitoring it is noted that whereas there has always been a tradition of evaluation of research, in the public services in general there is now a greater emphasis on audit and performance measurement and indicators. A review of these various systems suggests we should be looking to develop a system of outcomes monitoring that incorporates performance indicators (PIs) and measurement rather than an audit system that is trying to monitor activities against predetermined targets. •	Standard characteristics of performance measurement systems do not necessarily apply to research where, for example, there are non-standard outputs. Difficulties have arisen in the USA in attempting to apply the Government Performance and Results Act to research funding agencies. It is shown that because the findings of basic research, in particular, enter a knowledge pool in which people and ideas interact, it is difficult to use a PIs’ approach to track eventual outcomes. However, for some types of health research it has proved more feasible to trace the flow between research outputs and outcomes. •	An outcomes monitoring system could be useful if it met the following criteria: relevant to, with as comprehensive coverage as possible of, the funders objectives; relevant to the funder’s decision making processes; encourages accurate compliance; minimises unintended consequences; and has acceptable costs. Chapter 4 : Differences Between Research Types •	The range of differences between types of research can be relevant for the design of a routine monitoring system. The OECD distinguishes between basic research, applied research and experimental development. Most DH/NHS research is applied. There might be more of a tradition of publication of findings in applied research in health than in other fields. Nevertheless, the publication and incentives patterns operating in basic research mean that it would be inappropriate to use bibliometric indicators in a simple way across all fields even in health research. •	Despite having some differences from health research in publication patterns and in the detailed categories of payback, the broad approach proposed in Chapter 6 could be applied to social care research. •	Research that is commissioned, especially by the government, has some of the minimum conditions built into it that are associated with outcomes being generated, in particular because the funder has identified that a contribution in this area will be valuable. Chapter 5 : What Units of Research? •	The term programme has various meanings including being used to describe a collection of projects on a common theme and to describe a block of funding for a research unit. •	Three main streams or modes of funding can be identified: projects, which are administratively grouped into programmes including a responsive programme; institutions/centres/units; individual researchers. These 3 streams are displayed in Figure 1. It is probable that the regular data-gathering for a monitoring system would operate at the basic level of each stream or mode. •	Previous work demonstrates that the full range of benefits can sometimes be applied at the level of projects, either in the responsive mode or in programmes, through the use of questionnaires to researchers. Expert and user review and user surveys have also been applied. •	Institutions and centres increasingly have experience not only of traditional periodic expert review but also of producing annual reports, although there are debates about what dimensions to include in such reviews and reports. •	Individuals in receipt of research development awards have completed questionnaires during and after the awards. These concentrate on the development of research capacity but can go wider. Chapter 6 : A Possible Comprehensive Outcomes Monitoring System •	The proposed system is intended for DH/NHS to monitor the outcomes from its R&D in order to justify the R&D expenditure and assist with managing the portfolio. More detailed information is required for the latter purpose. •	We propose a multidimensional approach be adopted to cover all the dimensions of payback and that information be gathered from three sets of sources and Table 3 shows which methods would cover which output/outcome categories. •	Firstly, possibly annually, a questionnaire (possibly electronic) covering most payback categories should gather data from the basic level of each funding stream ie. from lead researchers of projects, from research institutions/centres, and from individual award holders. •	Secondly, supplementary information should be gathered from external databases (including the citation indices and Wellcome’s ROD). •	Thirdly, a range of approaches ie. user surveys, reviews by experts and peers, case studies including economic evaluations, and analysis of sources used in policy documents such as NICE guidelines, would be undertaken on a sample basis. They would provide not only supplementary information but, as with the external databases, would also verify the data collected directly from researchers. •	These proposals can be evaluated against the criteria set out in Chapter 3 : •	The system is relevant to DH’s objectives of generating payback in a range of categories. •	Various problems have to be overcome before the system could be fully decision relevant. Firstly it might be necessary to ask researchers to apportion the contribution made to specific outputs from various funding streams. Second, to be decision relevant the information would have to be analysed and presented in a manner consistent with funders’ decision making processes. This would involve a) showing how for each outcome and output, for example publications, data from one project or stream could be compared with those from another and b) demonstrating how different outputs and outcomes could be aggregated. •	The questions of accuracy of <b>data,</b> <b>minimisation</b> of unintended consequences and the acceptability of the net costs are also addressed. Chapter 7 : Research and Monitoring •	Whilst this report is primarily concerned with moving from ad hoc studies towards a routine monitoring system there are issues that need further research. •	Before embarking on full implementation the feasibility needs to be tested of items such as on-line recording of data and asking researchers to attribute proportions of research outputs to separate funding agencies. •	Once the system is implemented the value of some items can be better assessed, for example the additional value provided by self reporting of publications beyond that gained from relying on external databases. •	The data provided by the system would provide opportunities for further payback research on, for example, links between publications and other categories of payback. •	Some items such as network analysis could potentially be added to the monitoring system after further examination of them. •	Finally the benefit from the monitoring system itself should be assessed. Department of Health; Wellcome Trus...|$|E

