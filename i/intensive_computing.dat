191|607|Public
25|$|Roboticists {{launch an}} online {{database}} and cloud computing platform {{which can be}} accessed by robots worldwide, allowing them to more easily recognise unfamiliar objects and perform <b>intensive</b> <b>computing</b> tasks.|$|E
2500|$|On 10 August 2017, Microsoft {{announced}} a [...] edition {{to be made}} available in September, along with the Fall Creators Update for Windows 10. This edition is designed for high-end hardware for <b>intensive</b> <b>computing</b> tasks and supports Intel Xeon or AMD Opteron processors, up to 4 CPUs, up to 6TB RAM, the ReFS file system, Non-Volatile Dual In-line Memory Module (NVDIMM) and remote direct memory access (RDMA). The announcement included no licensing details.|$|E
5000|$|Equip@meso (Excellence {{facility}} for <b>intensive</b> <b>computing</b> of coordinated Mesocentres - Springboard towards petaflopic computing and exascale).|$|E
30|$|The {{authors in}} [31] {{argued that the}} {{dependability}} of cloud services may be attained through the quantification of security for <b>intensive</b> <b>compute</b> workload clouds to facilitate provision of assurance for quality of service. They subsequently defined seven security requirements which include: Workload state integrity, Guest OS Integrity, zombie protection, Denial of Service attacks, malicious resource exhaustion, platform attacks and backdoor protection. Unfortunately the paper does not provide any evidence of effort towards quantification of security as it claimed. Moreover it remains unclear as to how information relating those security requirements may be conveyed to the provider and consumer alike.|$|R
50|$|Rock {{aimed at}} higher per-thread performance, higher floating-point performance, and greater SMP {{scalability}} than the Niagara family. The Rock processor targeted traditional high-end data-facing workloads, such as back-end database servers, {{as well as}} floating-point <b>intensive</b> high-performance <b>computing</b> workloads, whereas the Niagara family targets network-facing workloads such as web servers.|$|R
40|$|Abstract Design of {{efficient}} System-on-Chips (SoCs) require thor-ough {{application analysis}} to identify various <b>compute</b> <b>intensive</b> parts. These <b>compute</b> <b>intensive</b> parts can be mapped tohardware {{in order to}} meet the cost as well as the performance constraints. However, faster time to market requiresautomation of synthesis of these code segments of the application from high level specification such as C alongwithits interfaces. Such synthesis system should be able to generate hardware which is easily plug-gable in various typesof architectures, as well as augment the application code to automatically take advantage of this new hardware compo-nent. In this paper, we address this problem and present an ap-proach for complete SoC synthesis. We automatically generate synthesizable VHDL for the <b>compute</b> <b>intensive</b> partof the application alongwith necessary interfaces. Our approach is generic in the sense that it supports various pro-cessors and buses by keeping a generic hardware interface on one end and a dedicated one on the other. The generatedhardware can be used in a tightly or loosely coupled manner in terms of memory and register communication. We presentthe effectiveness of this approach for some commonly used image processing spatial filter applications. 1...|$|R
5000|$|From 1990-2010, he {{primarily}} {{worked in}} computer science, specifically, data mining and data <b>intensive</b> <b>computing.</b> With Stuart Bailey and Yunhong Gu, he developed {{open source software}} to move large datasets over wide area high performance networks (PTool and the UDP-based Data Transfer Protocol or UDT). [...] With Yunhong Gu, he also developed Sector/Sphere, a distributed platform for data <b>intensive</b> <b>computing.</b> [...] During this period, he also founded the Data Mining Group, which develops data mining standards, and led the technical working group that developed the Predictive Model Markup Language (PMML), which is now the dominant standard in analytics.|$|E
5000|$|Avetec's DICE (Data <b>Intensive</b> <b>Computing</b> Environment) group helps vendor {{and data}} center {{customers}} optimize technology investments and transform innovations into proven and verified products and services, thereby reducing risk and time-to-solution for High Performance Computing (HPC) and Information Technology (IT) challenges.|$|E
50|$|In {{computer}} science, computation offloading {{refers to}} the transfer of certain computing tasks to an external platform, such as a cluster, grid, or a cloud. Offloading may be necessary due to hardware limitations of a computer system handling a particular task on its own. These <b>intensive</b> <b>computing</b> tasks {{may be used in}} artificial intelligence, artificial vision and object tracking, or computational decision making. Computation offloading may also be used to save energy.|$|E
40|$|Design of {{efficient}} System-on-Chips (SoCs) require thorough {{application analysis}} to identify various <b>compute</b> <b>intensive</b> parts. These <b>compute</b> <b>intensive</b> parts can be mapped to hardware {{in order to}} meet the cost as well as the performance constraints. However, faster time to market requires automation of synthesis of these code segments of the application from high level specification such as C alongwith its interfaces. Such synthesis system should be able to generate hardware which is easily plug-gable in various types of architectures, as well as augment the application code to automatically take advantage of this new hardware component. In this paper, we address this problem and present an approach for complete SoC synthesis. We automatically generate synthesizable VHDL for the <b>compute</b> <b>intensive</b> part of the application alongwith necessary interfaces. Our approach is generic in the sense that it supports various processors and buses by keeping a generic hardware interface on one end and a dedicated one on the other. The generated hardware can be used in a tightly or loosely coupled manner in terms of memory and register communication. We present the effectiveness of this approach for some commonly used image processing spatial filter applications. 1...|$|R
40|$|To achieve {{accessible}} computational {{power for}} their research goals, the authors developed {{the tools to}} build easy-to-use, numerically <b>intensive</b> parallel <b>computing</b> clusters using the Macintosh platform. Their approach enables the user, without expertise in the operating system, to develop and run parallel code efficiently, maximizing the advancement of scientific research...|$|R
5000|$|This {{facility}} has been funded by Government of Khyber Pakhtunkhwa, Pakistan in 2012. It is a <b>compute</b> <b>intensive</b> platform and comprises following hardware components: ...|$|R
50|$|The CDC Cyber {{range of}} mainframe-class {{supercomputers}} (except for the Cyber 18 and Cyber 1000 minicomputers) {{were the primary}} products of Control Data Corporation (CDC) during the 1970s and 1980s. In their day, they were the computer architecture of choice for scientific and mathematically <b>intensive</b> <b>computing.</b> They were used for modeling fluid flow, material science stress analysis, electrochemical machining analysis, probabilistic analysis, energy and academic computing, radiation shielding modeling, and other applications. Like their predecessor, the CDC 6600, they were unusual in using the Ones' complement binary representation.|$|E
5000|$|Cost {{reductions}} are {{claimed by}} cloud providers. A public-cloud delivery model converts capital expenditures (e.g., buying servers) to operational expenditure. This purportedly lowers barriers to entry, as infrastructure is typically {{provided by a}} third party and need not be purchased for one-time or infrequent <b>intensive</b> <b>computing</b> tasks. Pricing on a utility computing basis is [...] "fine-grained", with usage-based billing options. As well, less in-house IT skills are required for implementation of projects that use cloud computing. The e-FISCAL project's state-of-the-art repository contains several articles looking into cost aspects in more detail, most of them concluding that costs savings depend {{on the type of}} activities supported and the type of infrastructure available in-house.|$|E
50|$|In 2003, General Atomics {{turned over}} {{operation}} of the SDSC to the University of California San Diego (UCSD) and Dr. Moore became a full-time professor there establishing the Data <b>Intensive</b> <b>Computing</b> Environments (DICE) Center, continuing development of SRB. In that same year, General Atomics acquired the exclusive license to develop a commercial version of SRB, calling it Nirvana.The DICE team ended development of SRB in 2006 and started a rules oriented data management project called iRODS for open source distribution. Dr. Moore and his DICE team relocated to the University of North Carolina at Chapel Hill where iRODS is now maintained by the iRODS Consortium. General Atomics continued development of Nirvana at their San Diego headquarters, focusing on capabilities to serve government and commercial users, including high scalability, fail-over, performance, implementation, maintenance and support.|$|E
40|$|Abstract Security and {{fault-tolerance}} is a {{big issue}} for <b>intensive</b> parallel <b>computing</b> in pervasive environments with hardware errors or malicious acts that may alter the result. In [10, 15]is presented a novel, robust and secure architecture able to offer <b>intensive</b> parallel <b>computing</b> in environments where resources may be corrupted. Some efficient result-checking mechanisms are used to certify the results of an execution. The architecture is based on a limited number of safe resources that host the checkpoint server (used to store the graph) and the verifiers able to securely re-execute piece of tasks in a trusted way. We extend this approach in the case we have also some secure processors to build a trusted check-pointing infrastructure and to replace some untrusted nodes, avoiding some re-execution. Our approach is illustrated on a medical application and some experimental results are presented...|$|R
40|$|Fast {{and robust}} {{tracking}} of multiple faces is receiving increased attention from computer vision researchers as it finds potential applications in many fields like video surveillance and computer mediated video conferencing. Real-time tracking of multiple faces in high resolution videos involve three basic tasks namely initialization, tracking and display. Among these, tracking is quite <b>compute</b> <b>intensive</b> as it involves particle filtering that won’t yield a real time performance {{if we use}} a conventional CPU based system alone. While looking forward a design that optimizes the system for an appreciable real-time performance, calls {{for the use of}} compute efficient platforms like GPU for <b>compute</b> <b>intensive</b> tasks along with conventional CPU. This paper discusses our heterogeneous design model which combines conventional programming for CPU efficient tasks and the nVIDIA CUDA for GP-GPU that implements the <b>compute</b> <b>intensive</b> tasks...|$|R
50|$|Some {{motherboards}} {{have two}} or more PCI-E 16x slots, to allow more than 2 monitors without special hardware, or use a special graphics technology called SLI (for Nvidia) and Crossfire (for AMD). These allow 2 to 4 graphics cards to be linked together, to allow better performance in <b>intensive</b> graphical <b>computing</b> tasks, such as gaming, video editing, etc.|$|R
40|$|Abstract—Data <b>intensive</b> <b>computing</b> is {{a common}} {{research}} problem in science, industry and computer academia. In recent twenty years, the explosive growth of science data has appeared all over the world. Typical data <b>intensive</b> <b>computing</b> applications include Internet text data processing, scientific research data processing, large scale graph computing, inverse and perspective problems. Data <b>intensive</b> <b>computing</b> research faces challenges of scalability of massive data management and processing, integrated data processing technology, system management, new transaction demand, unstructured data processing, service mode, good fault tolerance and high availability. Appropriate data <b>intensive</b> <b>computing</b> programming model needs to be suitable for large scale data sets parallel computing, multiple virtual machine task scheduling and constructing new data <b>intensive</b> <b>computing</b> applications. As a typical data intensive application, aerosol inverse computing I/O data volume is analyzed. Although industry and academia has brought some methods for data intensive applications, scientific discovery problems with both data intensive and computation intensive features are still no appropriate solution at present. Keywords- data <b>intensive</b> <b>computing,</b> Big data, parallel processin...|$|E
40|$|Data <b>Intensive</b> <b>Computing</b> is {{characterized}} by problems where data is the primary challenger, {{whether it is the}} complexity, size, or rate of the data acquisition. The hardware platform required for a data <b>intensive</b> <b>computing</b> environment consists of tens, sometimes even hundreds, of thousands of compute nodes wit...|$|E
30|$|We thank Fujian Provincial Key Laboratory of Data <b>Intensive</b> <b>Computing</b> and Key Laboratory of Intelligent Computing and Information Processing of Fujian Province University.|$|E
5000|$|HPC {{platform}} {{has been}} donated to GIK Institute by Directorate of Science and Technology (DoST) KPK Pakistan. It is a <b>compute</b> <b>intensive</b> platform {{and has the}} following hardware components: ...|$|R
30|$|Methods {{for data}} {{reduction}} or compression must be included. We anticipate that some analysis stages, especially data reduction stages {{that are not}} <b>compute</b> <b>intensive,</b> may be best placed close to the detectors.|$|R
50|$|HetMark - {{focusing}} on <b>compute</b> <b>intensive</b> application flows which {{are common to}} embedded heterogeneous computing architectures; the first phase includes real-world workloads from automotive surround view and image recognition, and mobile augmented reality.|$|R
40|$|Abstract—Data <b>intensive</b> <b>computing</b> {{is having}} an {{increasing}} awareness among computer science researchers. As the data size increases even faster than Moore's Law, many traditional systems are failing {{to cope with the}} extreme large volumetric datasets. In this paper we use a real world graph processing application to demonstrate the challenges from the emerging data <b>intensive</b> <b>computing</b> and present a solution with a system called Sector/Sphere that we developed in the last several years. Sector provides scalable, fault-tolerant storage using commodity computers, while Sphere supports in-storage parallel data processing with a simplified programming interface. This paper describes the rationale behind Sector/Sphere and how to use it to effectively process massive sized graph...|$|E
40|$|In this paper, {{we discuss}} {{some of the}} lessons that we have learned working with the Hadoop and Sector/Sphere systems. Both of these systems are {{cloud-based}} systems designed to support data <b>intensive</b> <b>computing.</b> Both include distributed file systems and closely coupled systems for processing data in parallel. Hadoop uses MapReduce, while Sphere supports the ability to execute an arbitrary user defined function over the data managed by Sector. We compare and contrast these systems and discuss some of the design trade-offs necessary in data <b>intensive</b> <b>computing.</b> In our experimental studies over the past year, Sector/Sphere has consistently performed about 2 – 4 times faster than Hadoop. We discuss {{some of the reasons}} that might be responsible for this difference in performance...|$|E
40|$|Abstract—Data <b>intensive</b> <b>computing</b> can {{be defined}} as {{computation}} involving large datasets and complicated I/O patterns. Data <b>intensive</b> <b>computing</b> is challenging because there is a five-orders-of-magnitude latency gap between main memory DRAM and spinning hard disks; the result is that an inordinate amount of time in data <b>intensive</b> <b>computing</b> is spent accessing data on disk. To address this problem we designed and built a prototype data intensive supercomputer named DASH that exploits flash-based Solid State Drive (SSD) technology and also virtually aggregated DRAM to fill the “latency gap”. DASH uses commodity parts including Intel ® X 25 -E flash drives and distributed shared memory (DSM) software from ScaleMP®. The system is highly competitive with several commercial offerings by several metrics including achieved IOPS (input output operations per second), IOPS per dollar of system acquisition cost, IOPS per watt during operation, and IOPS per gigabyte (GB) of available storage. We present here an overview of the design of DASH, an analysis of its cost efficiency, then a detailed recipe for how we designed and tuned it for high data-performance, lastly show that running data-intensive scientific applications from graph theory, biology, and astronomy, we achieved as much as two orders-ofmagnitude speedup compared to the same applications run on traditional architectures. I...|$|E
40|$|Abstract—In {{this paper}} we present some {{research}} results on <b>computing</b> <b>intensive</b> applications using modern high performance architectures {{and from the}} perspective of high computational needs. <b>Computing</b> <b>intensive</b> applications are an important family of applications in distributed computing domain. They have been object of study using different distributed computing paradigms and infrastructures. Such applications distinguish for their demanding needs for CPU computing, independently of the amount of data associated with the problem instance. Among <b>computing</b> <b>intensive</b> applications, there are applications based on simulations, aiming to maximize system resources for processing large computations for simulation. In this research work, we consider an application that simulates scheduling and resource allocation in a Grid computing system using Genetic Algorithms. In such application, a rather large number of simulations is needed to extract meaningful statistical results about the behavior of the simulation results. We study the performance of Oracle Grid Engine for such application running in a Cluster of high computing capacities. Several scenarios were generated to measure the response time and queuing time under different workloads and number of nodes in the cluster...|$|R
40|$|Abstract. <b>Intensive</b> {{parallel}} <b>computing</b> in grid environments {{are subject}} to various concerns, in particular in terms of security and fault-tolerance. In [1, 2], the authors present a robust and secure architecture {{able to deal with}} <b>intensive</b> parallel <b>computing</b> in such environment where resources could be corrupted. The corruption either comes from hardware issues (such as net-work disconnection) or from malicious act in order to alter the computation and consequently its result. The proposed approach uses dataflow graph to represent a parallel execution and to provide efficient result-checking mech-anisms to certify the results of an execution. The architecture is based on a limited number of safe resources that host the checkpoint server (used to store the graph) and the verifiers which could securely re-execute piece of tasks in a trusted way. It remains to detail the effective construction of these resources and this is the purpose of this article. While this issue is generally treated with software solutions, we combine both software and hardware approaches to build strongly secure resources. ...|$|R
40|$|MapReduce has {{gradually}} become {{the framework of}} choice for ”big data”. The MapReduce model allows for efficient and swift processing of large scale data with a cluster of compute nodes. However, the efficiency here comes at a price. The performance of widely used MapReduce implementations such as Hadoop suffers in heterogeneous and load-imbalanced clusters. We show the disparity in performance between homogeneous and heteroge-neous clusters in this paper to be high. Subsequently, we present MARLA, a MapReduce framework capable of performing well not only in homogeneous settings, but also when the cluster exhibits heterogeneous properties. We address {{the problems associated with}} existing MapReduce implementations affecting cluster heterogeneity, and subsequently present through MARLA the components and trade-offs necessary for better MapReduce performance in heterogeneous cluster and cloud environments. We quantify the performance gains exhibited by our approach against Apache Hadoop and MARIANE in data <b>intensive</b> and <b>compute</b> <b>intensive</b> applications. I...|$|R
40|$|Data volumes {{have been}} {{increasing}} substantially {{over the past}} several years. Such data is often processed concurrently on a distributed collection of machines to ensure reasonable completion times. Load balancing {{is one of the most}} important issues in data <b>intensive</b> <b>computing.</b> Often, the choice of the load balancing strateg...|$|E
40|$|This paper {{indicates}} two {{errors in}} the formulation of the main optimization model in the article "Dynamic power management in energy-aware computer networks and data <b>intensive</b> <b>computing</b> systems" by Niewiadomska-Szynkiewicz et al. [FGCS, vol. 37 (2014), pp. 284 - 296] and shows how to fix them. Comment: 7 page...|$|E
40|$|AbstractThe Map Reduce {{framework}} {{provides a}} scalable model for large scale data <b>intensive</b> <b>computing</b> and fault tolerance. In this paper, we propose an algorithm {{to improve the}} I/O performance of the Hadoop distributed file system. The results prove that the proposed algorithm show better I/O performance with comparatively less synchronizatio...|$|E
40|$|Synthetic Aperture Radar (SAR) {{raw data}} {{simulation}} {{is a fundamental}} problem in radar system design and imaging algorithm research. The growth of surveying swath and resolution results in {{a significant increase in}} data volume and simulation period, which can be considered to be a comprehensive data <b>intensive</b> and <b>computing</b> <b>intensive</b> issue. Although several high performance computing (HPC) methods have demonstrated their potential for accelerating simulation, the input/output (I/O) bottleneck of huge raw data has not been eased. In this paper, we propose a cloud computing based SAR raw data simulation algorithm, which employs the MapReduce model to accelerate the raw data computing and the Hadoop distributed file system (HDFS) for fast I/O access. The MapReduce model is designed for the irregular parallel accumulation of raw data simulation, which greatly reduces the parallel efficiency of graphics processing unit (GPU) based simulation methods. In addition, three kinds of optimization strategies are put forward from the aspects of programming model, HDFS configuration and scheduling. The experimental results show that the cloud computing based algorithm achieves 4 _ speedup over the baseline serial approach in an 8 -node cloud environment, and each optimization strategy can improve about 20 %. This work proves that the proposed cloud algorithm is capable of solving the <b>computing</b> <b>intensive</b> and data intensive issues in SAR raw data simulation, and is easily extended to large scale computing to achieve higher acceleration...|$|R
40|$|In {{this paper}} we present some {{research}} results on <b>computing</b> <b>intensive</b> applications using modern high performance architectures {{and from the}} perspective of high computational needs. <b>Computing</b> <b>intensive</b> applications are an important family of applications in distributed computing domain. They have been object of study using different distributed computing paradigms and infrastructures. Such applications distinguish for their demanding needs for CPU computing, independently of the amount of data associated with the problem instance. Among <b>computing</b> <b>intensive</b> applications, there are applications based on simulations, aiming to maximize system resources for processing large computations for simulation. In this research work, we consider an application that simulates scheduling and resource allocation in a Grid computing system using Genetic Algorithms. In such application, a rather large number of simulations is needed to extract meaningful statistical results about the behavior of the simulation results. We study the performance of Oracle Grid Engine for such application running in a Cluster of high computing capacities. Several scenarios were generated to measure the response time and queuing time under different workloads and number of nodes in the cluster. Peer ReviewedPostprint (author's final draft...|$|R
40|$|The aims of this {{investigation}} were to review the clinical behavior of deep neck infections (DNIs) treated in our institution {{in order to identify}} the predisposing factors of life-threatening complications and propose valuable recommendations for management and treatment. A total of 365 adult patients with DNIs were retrospectively identified. One-hundred and thirty-nine patients (38. 1 %) underwent surgical drainage. Overall, 226 patients (61. 9 %) responded effectively to intravenous antimicrobial therapy only. There were 67 patients (18. 4 %) developing life-threatening complications. Diabetes mellitus (odd ratio 5. 43; P < 0. 001) and multiple deep neck spaces involvement (odd ratio 4. 92; P < 0. 001) were the strongest independent predictors of complications. The mortality rate was 0. 3 %. Airway obstruction and descending mediastinitis are the most troublesome complications of DNIs. In selected patients, a trial of intravenous antibiotic therapy associated with an <b>intensive</b> <b>computed</b> tomography-based wait-and-watch policy may avoid an unnecessary surgical procedure. However, about one-fourth of patients present significant comorbidities, which may negatively affect the course of the infection. In these cases and in patients with large or multiple spaces infections, a more aggressive surgical strategy is mandatory...|$|R
