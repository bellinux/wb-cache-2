39|10000|Public
5000|$|HPC-Europa is an EC-funded {{programme}} in {{the field}} of <b>HPC</b> (<b>High</b> <b>Performance</b> <b>Computing).</b> Its main focus is its Transnational Access programme, which offers computational scientists based in Europe the opportunity to travel to another country to work in collaboration with researchers in a similar field, while having access {{to some of the most}} powerful computers in Europe.|$|E
50|$|MPFS {{technology}} {{is intended for}} <b>HPC</b> (<b>High</b> <b>Performance</b> <b>Computing)</b> environments in which multiple compute nodes require concurrent access to data sets. This technology {{can be used to}} store and access data for grid computing, where the individual computing power of many systems is combined to perform a single process. Example uses include processing geological data, voice recognition datasets, and modal processing. Virtualized computing environments will also benefit from high performance shared storage.|$|E
50|$|Companies like Hauppauge and Microway {{that were}} {{impacted}} by their new competitor that made their living accelerating floating point applications being run on PCs followed suit by venturing into the Intel i-860 vector coprocessor business: Hauppauge {{came out with}} an Intel 80486 motherboard that included an Intel i-860 vector processor while Microway came out with add-in cards that had between one or more i-860s. These products along with Transputer-based add-in cards would eventually lead into {{what became known as}} <b>HPC</b> (<b>high</b> <b>performance</b> <b>computing).</b> HPC was actually initiated in 1986 by an English company, Inmos, that designed a CPU competitive with an Intel 80386/387 that also included four twisted pair high speed interconnects that could communicate with other Transputers and be linked to a PC motherboard making it possible to create distributed memory processing computers that could employ 32 processors with the same throughput as 32 Intel 386/387s operating in a single PC. The add in card parallel processing business morphed from the Transputer to the Intel i-860 around 1989 when Inmos was purchased by STmicroelectronics that cut R&D funding eventually forcing companies that had entered the parallel processing business to shift to the Intel i860. The i-860 was a vector processor with graphics extensions that could initially provide 50 Megaflops of throughput in an era when an 80486 with an Intel 80487 peaked at half a Megaflop and would eventually top out at 100 Megaflops making it as fast as 100 Inmos T414 Transputers. i-860 Add in cards made it possible for as many as 20 Intel i-860s to run in parallel and could be programmed using a software library similar to today's MPI libraries which today support distributed memory parallel processing in which servers sitting in 1U rack mount chassis that are essentially PCs provide the horsepower behind the majority of the world's Supercomputers. This same approach could be employed using Hauppauge's motherboards connected by Gigabit Ethernet, something that was however first demonstrated using a wall of IBM RS/6000 PCs at the 1991 Supercomputing Conference. IBM's lead was quickly followed by academic users who realized they could {{do the same thing with}} much less expensive hardware by adapting their x86 PCs to run in parallel at first using a software library adapted from similar Transputer libraries called PVM (parallel virtual machines) that would eventually morph into today's MPI. Products like the Intel i860 vector processor that could be employed both as a vector and graphics processor were end of life'd around 1993 at the same time that Intel introduced the Intel Pentium P5: a CISC processor that used CISC instructions that were pipelined into hard coded lower level RISC like primitives that provided the Pentium with a Superscalar architecture that also could execute the x87 FPU instruction set using a built in FPU that was essentially implemented using the scalar instructions of the i-860 as well as a memory bus that provided a 400 MB/sec interface to memory that was borrowed from the i-860 as well. This high speed bus played a crucial role in speeding up the most common floating point intensive applications that at this point in time used Gauss Elimination to solve simultaneous linear equations buy which today are solved using blocking and LU decomposition. The Intel Pentium while good, did not provide enough floating point performance to compete with a 300 MHz 21164 DEC Alpha that provided 600 Megaflops in 1995. At the same point in time Intel Supercomputing had moved from the 50 MHz Intel i-860XP that was six times slower than the DEC 21164 to the special version of their Pentium that at 200 Megaflops was only three times slower than the 21164. However, the impending speed upgrade of the Alpha to 600 MHz ultimately doomed the future of Intel Supercomputing.|$|E
50|$|Windows HPC Server 2008, {{released}} by Microsoft on 22 September 2008, is the successor product to Windows Compute Cluster Server 2003. Like WCCS, Windows HPC Server 2008 {{is designed for}} high-end applications that require <b>high</b> <b>performance</b> <b>computing</b> clusters (<b>HPC</b> stands for <b>High</b> <b>Performance</b> <b>Computing).</b> This version of the server software is claimed to efficiently scale to thousands of cores. It includes features unique to HPC workloads: a new high-speed NetworkDirect RDMA, highly efficient and scalable cluster management tools, a service-oriented architecture (SOA) job scheduler, an MPI library based on open-source MPICH2, and cluster interoperability through standards such as the <b>High</b> <b>Performance</b> <b>Computing</b> Basic Profile (HPCBP) specification produced by the Open Grid Forum (OGF).|$|R
50|$|Starting in 1983 {{the company}} {{followed}} Microway, {{the company that}} a year earlier provided the software needed by scientists and engineers to modify the IBM-PC Fortran compiler {{so that it could}} transparently employ Intel 8087s. The 80-bit Intel 8087 math coprocessor ran a factor of 50 faster than the 8/16-bit 8088 CPU that the IBM-PC software came with. However, in 1982 the speed up in floating point intensive applications was only a factor of 10 as the initial software developed by Microway and Hauppauge continued to call floating point libraries to do computations instead of placing inline x87 instructions inline with the 8088's instructions that allowed the 8088 to drive the 8087 directly. By 1984 inline compilers made their way into the market providing increased speed ups. Hauppauge provided similar software products in competition with Microway that they bundled with math coprocessors and remained in the Intel math coprocessor business until 1993 when the Intel Pentium came out with a built in math coprocessor. However like other companies that entered the math coprocessor business, Hauppauge produced other products that contributed to a field that is today called <b>HPC</b> - <b>High</b> <b>Performance</b> <b>Computing.</b>|$|R
40|$|This project {{addresses}} the {{issues associated with}} providing Decentralized Data Offloading service to HPC Centers. <b>HPC</b> centers are <b>High</b> <b>Performance</b> <b>Computing</b> centers that use Parallel Processing for running advanced applications more reliably and efficiently. The main concept of this project is the offloading of data from a HPC Center to the destination site in decentralized mode. This project uses the decentralization concept where {{it is possible for}} the end user to retrieve the data even when the center logs out. This is possible by moving the data from the center to the Scratch Space. From Scratch space the data is moved to the intermediate storage nodes 1 [...] n and from the nth node the data is transferred to the destination site within a deadline. These techniques are implemented within a Production Job Scheduler which schedules the jobs and Bit Torrent tool is used for data transfer in a decentralized environment. Thus the total offloading times are minimized; data loss and offload delays are also prevented...|$|R
40|$|L'objectiu {{principal}} d'aquest treball és modelar un sistema <b>HPC</b> (<b>High</b> <b>Performance</b> <b>Computing),</b> fent ús de diferents tecnologies, tant a nivell topològic com d'interconnexió de xarxa, per tal d'avaluar com afecten aquestes al rendiment final del sistema. El objetivo principal de este trabajo es modelar un sistema <b>HPC</b> (<b>High</b> <b>Performance</b> <b>Computing),</b> haciendo uso de diferentes tecnologías, tanto a nivel topológico como de interconexión de red, para evaluar cómo afectan éstas al rendimiento final del sistema. The {{main objective}} of this work is to model an HPC system (High Performance Computing) using different kind of technologies, both in a topological way and network interconnection, with the goal to evaluate how they affect {{the performance of the}} system...|$|E
40|$|This paper gives {{a general}} {{overview}} {{of the type of}} <b>HPC</b> (<b>high</b> <b>performance</b> <b>computing)</b> cluster installed by the Boise State Automated Cluster Installer, or BSACI. The installation tool and its various components are then described in detail. A description of how to upgrade the installation tool is given, illustrated with specific examples from the Fedora Core 5 to Fedora 7 upgrade...|$|E
40|$|By {{the late}} 1990 s, the Internet was {{adequately}} equipped to move {{vast amounts of}} data between <b>HPC</b> (<b>High</b> <b>Performance</b> <b>Computing)</b> systems, and efforts were initiated to link together the national infrastructure of high performance computational and data storage resources together into a general computational utility 'grid', analogous to the national electrical power grid infrastructure. The purpose of the Computational grid is to provide dependable, consistent, pervasive, and inexpensive access to computational resources for the computing community {{in the form of}} a computing utility. This paper presents a fully distributed view of Grid usage accounting and a methodology for allocating Grid computational resources for use on a Grid computing system...|$|E
40|$|We develop {{asymptotic}} {{methods to}} solve problems arising {{in the context of}} acoustic, elastic or electromagnetic wave propagation phenomena and which involve small parameters (boundary layer problems, thin-layer problems, problems with small defects). Such phenomena are eventually described with multi-scale expansions and asymptotic models. Reduced models have been implemented in Finite Element codes in order to analyse numerically their properties. Several works are presented in a first part which is divided into two chapters. The first chapter is devoted to thin-layer transmission problems in acoustic and elastic media. This work enters into the scope of the <b>HPC</b> GA project (<b>High</b> <b>Performance</b> <b>Computing</b> for Geophysics Applications). The second chapter concerns transmission problems of electromagnetic waves across thin layers with high conductivities. This part is complemented with an asymptotic study on the accuracy of different approximations of the electromagnetic field for thin-layer transmission problems, Appendix A. In the context of fluid flow modeling in complex media combining porous and fluid regions with free flow and a high contrast of viscosities, we develop a WKB expansion to solve a singular perturbation problem, Appendix B. Corner and edge singularities on interfaces may increase the level of difficulty in the analysis of elliptic problems in comparison with smooth interfaces. We develop asymptotic technics to determine corner and edge asymptotics in elliptic systems for several applications in electromagnetism and in elasticity. Eddy current problems have been addressed {{in the presence of a}} corner singularity on an interface (dielectric/conductor) and crack problems have been addressed in the context of elasticity. The asymptotic analysis of such a problem relies on the determination of a (Kondrat’ev type) singular expansion which is a series of asymptotics defined in a vicinity of a corner or edge singularity. This expansion generalizes the Taylor one which holds in smooth domains. The notion of asymptotics involve two ingredients which are the singular coefficients and the singular functions (also called the singularities). The singularities are associated with singular exponents and may be determined with the Mellin analysis. The singular coefficients may be extracted with numerical methods such as the quasi dual function method (QDFM) which requires the knowledge of dual singularities. Several works related to these problems are presented in a second part which is divided into two chapters. Chapter 3 is mainly devoted to the determination of corner asymptotics of the magnetic potential in the eddy-current model. The first terms of a multi-scale expansion of the magnetic potential are also introduced to tackle the magneto-harmonic problem as the skin depth goes to zero. As an application, a modified (Leontovich) impedance boundary condition close to a corner singularity has been proposed. Chapter 4 is concerned with the notion of edge asymptotics for the Laplace operator. We make the focus on the extraction of edge flux intensity factors (EFIFs) associated with the integer eigenvalues for the Laplace operator over a 3 -D domain with a straight crack. The dual singularities are determined and the QDFM has been extended for the extraction of EFIFs. Finally, spectral methods and polynomial solutions have been developed to solve hypersingular integral equations over a disc and the airfoil equation (with a Cauchy type of singularity) over close disjoint intervals, Appendix...|$|R
5000|$|SUSE Linux Enterprise <b>High</b> <b>Performance</b> <b>Computing</b> - an {{infrastructure}} solution for <b>high</b> <b>performance</b> <b>computing</b> ...|$|R
5000|$|The International Conference on <b>High</b> <b>Performance</b> <b>Computing</b> (or HiPC) is an {{international}} meeting on <b>high</b> <b>performance</b> <b>computing.</b> It serves as a forum to present current work by researchers {{from around the world}} as well as highlight activities in Asia in the <b>high</b> <b>performance</b> <b>computing</b> area. The meeting focuses on all aspects of <b>high</b> <b>performance</b> <b>computing</b> systems and their scientific, engineering, and commercial applications.|$|R
40|$|<b>HPC</b> (<b>High</b> <b>Performance</b> <b>Computing)</b> {{has become}} {{essential}} for the acceleration of innovation and the companies’ assistance in creating new inventions, better models and more reliable products as well as obtaining processes and services at low costs. The information in this paper focuses particularly on: description the field of high performance scientific computing, parallel computing, scientific computing, parallel computers, and trends in the HPC field, presented here reveal important new directions toward the realization of a high performance computational society. The practical {{part of the work}} is an example of use of the HPC tool to accelerate solving an electrostatic optimization problem using the Parallel Computing Toolbox that allows solving computational and data-intensive problems using MATLAB and Simulink on multicore and multiprocessor computers...|$|E
40|$|Nowadays, the {{scientific}} applications are developed with more complexity and accuracy and these precisions need high computational resources to be executed. Also, {{the current trend}} in high performance computing (HPC) is to find clusters composed of Multicore nodes, and these nodes include heterogeneity levels which have to be handled carefully {{if we want to}} improve the performance metrics. The integration of these Multicore nodes in <b>HPC</b> (<b>High</b> <b>Performance</b> <b>Computing)</b> has allowed the inclusion of more parallelism within nodes, but this parallelism generates challenges that have to be managed considering some troubles present in these environments that affect the application efficiency and speedup. Aspects associated to number of cores, data locality, shared cache, communications link inside the node, etc are considered relevant when our goal is to improve the performance...|$|E
40|$|International audienceAmong the {{techniques}} {{widely used in}} CEM (Computational Electromagnetics), the FDTD method (Finite-Difference Time Domain) {{is one of the}} most demanding in computational resources. Despite the computing power available today on a personal computer, industry demands the analysis of large and complex models to be used in EMC (ElectroMagnetic Compatibility) applications among others. In the last decade, the main strategies used to improve the performance of FDTD have been (1) parallelizing FDTD in clusters of computers, (2) designing digital systems which act as accelerators using reconfigurable hardware, and (3) incorporating Graphics Processing Units (GPU) to accelerate computations. This paper presents a brief review of the state of the art in <b>HPC</b> (<b>High</b> <b>Performance</b> <b>Computing)</b> in FDTD, foresees the trend in HPC for CEM and explains the behaviour of algorithm in one of these architectures...|$|E
40|$|<b>High</b> <b>performance</b> <b>computing</b> {{has become}} {{accepted}} as a tool {{that can be used}} to solve many large scale computational problems. Because of the complexity of the problems associated with <b>high</b> <b>performance</b> <b>computing,</b> visualization of the output of <b>high</b> <b>performance</b> <b>computing</b> applications has always been an important factor in providing a complete problem solving environment for the <b>high</b> <b>performance</b> <b>computing</b> user. As visualization technology advances, it is important to consider what impacts those advances will have on the integration of <b>high</b> <b>performance</b> <b>computing</b> and visualization. Virtual environments are the most recent, and arguably the most powerful, visualization environments in use today. In this paper we analyze the current state of the research of integrating visualization, and in particular virtual environments, with <b>high</b> <b>performance</b> <b>computing.</b> We also present a framework for implementing such an environment and report on the status of its implementation at the Australian National Un [...] ...|$|R
40|$|<b>High</b> <b>performance</b> {{interconnects}} such as InfiniBand (IB) {{have enabled}} large scale deployments of <b>High</b> <b>Performance</b> <b>Computing</b> (<b>HPC)</b> systems. <b>High</b> <b>performance</b> communication and IO middleware such as MPI and NFS over RDMA {{have also been}} redesigned to leverage the performance of these modern interconnects. With the advent of long haul InfiniBand (IB WAN), IB applications now have inter-cluster reaches. While this technology is intended to enable <b>high</b> <b>performance</b> network connectivity across WAN links, {{it is important to}} study and characterize the actual performance that the existing IB middleware achieve in these emerging IB WAN scenarios. In this paper, we study and analyze the performance characteristics of the following three HPC middleware: (i) IPoIB (IP traffic over IB), (ii) MPI and (iii) NFS over RDMA. W...|$|R
40|$|A {{review of}} the <b>High</b> <b>Performance</b> <b>Computing</b> and Communications (HPCC) program is {{provided}} in vugraph format. The goals and objectives of this federal program are as follows: extend U. S. leadership in <b>high</b> <b>performance</b> <b>computing</b> and computer communications; disseminate the technologies to speed innovation and to serve national goals; and spur gains in industrial competitiveness by making <b>high</b> <b>performance</b> <b>computing</b> integral to design and production...|$|R
40|$|The aim of {{this paper}} is to present a mobile agents model for {{distributed}} classification of Big Data. The great challenge is to optimize the communication costs between the processing elements (PEs) in the parallel and distributed computational models by the way to ensure the scalability and the efficiency of this method. Additionally, the proposed distributed method integrates a new communication mechanism to ensure <b>HPC</b> (<b>High</b> <b>Performance</b> <b>Computing)</b> of parallel programs as distributed one, by means of cooperative mobile agents team that uses its asynchronous communication ability to achieve that. This mobile agents team implements the distributed method of the Fuzzy C-Means Algorithm (DFCM) and performs the Big Data classification in the distributed system. The paper shows the proposed scheme and its assigned DFCM algorithm and presents some experimental results that illustrate the scalability and the efficiency of this distributed method...|$|E
40|$|X 10 is a <b>HPC</b> (<b>High</b> <b>Performance</b> <b>Computing)</b> {{programming}} language proposed by IBMfor supporting a PGAS (Partitioned Global Address Space) programming model offering a shared address space. The address space {{can be further}} partitioned into several logical locations where objects and activities (or threads) will be dynamically created. An analysis of locations can help to check the safety of object accesses through exploring which objects and activities may reside in which locations, while in practice the objects and activities are usually designated at runtime and their locations may also vary under different environments. In this paper, we propose a constraint-based locality analysis method called Leopard for X 10. Leopard calculates the points-to relations for analyzing the objects and activities in a program and uses a place constraint graph to analyze their locations. We have developed a tool to support Leopard, and conducted an experiment to evaluate its effectiveness and efficiency. The experimental results show that Leopard can calculate the locations of objects and activities precisely. Copyright © 2013 ACM. ACM SIGPLANX 10 is a <b>HPC</b> (<b>High</b> <b>Performance</b> <b>Computing)</b> {{programming language}} proposed by IBMfor supporting a PGAS (Partitioned Global Address Space) programming model offering a shared address space. The address space can be further partitioned into several logical locations where objects and activities (or threads) will be dynamically created. An analysis of locations can help to check the safety of object accesses through exploring which objects and activities may reside in which locations, while in practice the objects and activities are usually designated at runtime and their locations may also vary under different environments. In this paper, we propose a constraint-based locality analysis method called Leopard for X 10. Leopard calculates the points-to relations for analyzing the objects and activities in a program and uses a place constraint graph to analyze their locations. We have developed a tool to support Leopard, and conducted an experiment to evaluate its effectiveness and efficiency. The experimental results show that Leopard can calculate the locations of objects and activities precisely. Copyright © 2013 ACM...|$|E
40|$|Abstract—Supercomputer I/O loads {{are often}} {{dominated}} by writes. <b>HPC</b> (<b>High</b> <b>Performance</b> <b>Computing)</b> file systems {{are designed to}} absorb these bursty outputs at high bandwidth through massive parallelism. However, the delivered write band-width often falls well below the peak. This paper characterizes the data absorption behavior of a center-wide shared Lustre parallel file system on the Jaguar supercomputer. We use a statistical methodology to address the challenges of accurately measuring a shared machine under production load and to obtain the distribution of bandwidth across samples of compute nodes, storage targets, and time intervals. We observe and quantify limitations from competing traffic, contention on storage servers and I/O routers, concurrency limitations in the client compute node operating systems, {{and the impact of}} variance (stragglers) on coupled output such as striping. We then examine the implications of our results for application performance and the design of I/O middleware systems on shared supercomputers. I...|$|E
40|$|Abstract:- Research of {{computing}} {{security is a}} hotspot recently. In this paper, data fusion technology is {{used to analyze the}} security situation under the <b>high</b> <b>performance</b> <b>computing</b> environment. A unique efficient method is given to analyze the whole systems security situation. With the result, complex security data fusion under <b>high</b> <b>performance</b> <b>computing</b> can be solved in an easy way. Key-Words:- <b>high</b> <b>performance</b> <b>computing,</b> data fusion, security...|$|R
40|$|Atrial {{fibrillation}} (AF) is {{a common}} cardiac disease with high rates of morbidity, leading to major personal and NHS costs. Computer modeling of AF using a detailed cellular model with realistic 3 D anatomical geometry allows investigation of the underlying ionic mechanisms in far more detail than in a physiology laboratory. We have developed a 3 D virtual human atrium that combines detailed cellular electrophysiology, including ion channel kinetics and homeostasis of ionic concentrations, with anatomical detail. The segmented anatomical structure and multi-variable nature of the system makes the 3 D simulations of AF large and computationally intensive. The computational demands are such that a full problem solving environment requires access to resources of <b>High</b> <b>Performance</b> <b>Computing</b> (<b>HPC),</b> <b>High</b> <b>Performance</b> Visualization (HPV), remote data repositories and a backend infrastructure. This {{is a classic example}} of eScience and Gridenabled computing. Initial work has been carried out using multiple processor machines with shared memory architectures. As spatial resolution of anatomical models increases, requirement of HPC resources is predicted to increase many-fold (~ 1 – 10 teraflops). Distributed computing is essential, both through massively parallel systems (a single supercomputer) and multiple parallel systems made accessible through the Grid. 1...|$|R
40|$|In this lecture {{we present}} <b>high</b> <b>performance</b> <b>computing</b> from a programmers perspective. Several {{approaches}} to programming high-performance computers are reviewed. Examples are given of extensions to FORTRAN for programming shared-memory systems {{as well as}} distributed memory systems. Keywords: <b>High</b> <b>Performance</b> <b>Computing,</b> Parallel Language...|$|R
40|$|Abstract—Different {{departments}} {{of a large}} organization often run dedicated cluster systems for different computing loads, like <b>HPC</b> (<b>high</b> <b>performance</b> <b>computing)</b> jobs or Web service applications. In this paper, we have designed and implemented a cloud management system software Phoenix Cloud to consolidate heterogeneous workloads from different departments affiliated to the same organization on the shared cluster system. We have also proposed cooperative resource provisioning and management policies for a large organization and its affiliated departments, running HPC jobs and Web service applications, to share the consolidated cluster system. The experiments show that {{in comparison with the}} case that each department operates its dedicated cluster system, Phoenix Cloud significantly decreases the scale of the required cluster system for a large organization, improves the benefit of the scientific computing department, {{and at the same time}} provisions enough resources to the other department running Web services with varying loads. 1 I...|$|E
40|$|Abstract—Understanding {{workload}} characteristics {{is critical}} for optimizing and improving the performance of current systems and software, and architecting new storage systems based on observed workload patterns. In this paper, we characterize the scientific workloads of the world’s fastest <b>HPC</b> (<b>High</b> <b>Performance</b> <b>Computing)</b> storage cluster, Spider, at the Oak Ridge Leadership Computing Facility (OLCF). Spider provides an aggregate bandwidth of over 240 GB/s with over 10 petabytes of RAID 6 formatted capacity. OLCFs flagship petascale simulation platform, Jaguar, and other large HPC clusters, in total over 250 thousands compute cores, depend on Spider for their I/O needs. We characterize the system utilization, the demands of reads and writes, idle time, {{and the distribution of}} read requests to write requests for the storage system observed over a period of 6 months. From this study we develop synthesized workloads and we show that the read and write I/O bandwidth usage as well as the inter-arrival time of requests can be modeled as a Pareto distribution. I...|$|E
40|$|Conventional {{methods for}} {{computing}} 3 D projects are nowadays usually implemented on standard or graphics processors. The performance {{of these devices}} is limited especially by the used architecture, which to some extent works in a sequential manner. In this article we describe a project which utilizes parallel computation for simple projection of a wireframe 3 D model. The algorithm is optimized for a FPGA-based implementation. The design of the numerical logic is described in VHDL {{with the use of}} several basic IP cores used especially for computing trigonometric functions. The implemented algorithms allow smooth rotation of the model in two axes (azimuth and elevation) and a change of the viewing angle. Tests carried out on a FPGA Xilinx Spartan- 6 development board have resulted in real-time rendering at over 5000 fps. In the conclusion of the article, we discuss additional possibilities for increasing the computational output in graphics applications via the use of <b>HPC</b> (<b>High</b> <b>Performance</b> <b>Computing)</b> ...|$|E
40|$|Abstract. MPI {{is one of}} the {{important}} standards in <b>high</b> <b>performance</b> <b>computing.</b> MPI <b>performance</b> is generally focused on collective communications. And FCA (Fabric Collective Accelerator) is a new method accelerating collective communications. Through <b>high</b> <b>performance</b> <b>computing</b> environment testing, this paper mainly analyses the result of FCA with shared memory and without share memory accelerating IBM Platform MPI, FCA's principle and integration between IBM Platform MPI and FCA. At the same time, this paper may be a good reference for <b>high</b> <b>performance</b> <b>computing</b> using FCA...|$|R
40|$|This paper {{discusses}} {{basics of}} <b>high</b> <b>performance</b> <b>computing</b> on {{the example of}} the Latvian SuperCluster (LASC), installed at the Institute of Solid State Physics of the University of Latvia. The application of LASC to abinitio simulations of x-ray absorption spectra from nanoparticles is considered as a particular case. [Keywords: <b>high</b> <b>performance</b> <b>computing,</b> cluster computing, x-ray absorption spectroscopy, nanoparticles...|$|R
40|$|Abstract — Last {{few decades}} {{have seen the}} {{emergence}} of computational neuroscience as a mature field where researchers are interested in modeling complex and large neuronal systems and require access to <b>high</b> <b>performance</b> <b>computing</b> machines and associated cyberinfrastructure to manage computational workflow and data. The neuronal simulation tools, used in this research field, are also implemented for parallel computers and suitable for <b>high</b> <b>performance</b> <b>computing</b> machines. But using these tools on complex <b>high</b> <b>performance</b> <b>computing</b> machines remain a challenge due to issues with acquiring computer time on these machines located at national supercomputer centers, dealing with complex user interface of these machines, dealing with data management and retrieval. The Neuroscience Gateway is being developed to alleviate all of these barriers to entry for computational neuroscientist. It hides or eliminates, {{from the point of}} view of the users, all the administrative and technical barriers and makes parallel neuronal simulation tools easily available and accessible on complex <b>high</b> <b>performance</b> <b>computing</b> machines and handles the running of jobs and data management and retrieval. This paper describes the architecture it is based on, how it is implemented, and how users can use this for computational neuroscience research using <b>high</b> <b>performance</b> <b>computing</b> at the back end. Keywords—computational neuroscience, science gateway, <b>high</b> <b>performance</b> <b>computing</b> I...|$|R
40|$|Reliability {{engineering}} {{is relatively new}} scientific discipline which develops in close connection with computers. Rapid development of computer technology recently requires adequate novelties of source codes and appropriate software. New parallel computing technology based on <b>HPC</b> (<b>high</b> <b>performance</b> <b>computing)</b> for availability calculation will be demonstrated in this paper. The technology is particularly effective in context with simulation methods; nevertheless, analytical methods {{are taken into account}} as well. In general, basic algorithms for reliability calculations must be appropriately modified and improved to achieve better computation efficiency. Parallel processing is executed by two ways, firstly by the use of the MATLAB function parfor and secondly by the use of the CUDA technology. The computation efficiency was significantly improved which is clearly demonstrated in numerical experiments performed on selected testing examples as well as on industrial example. Scalability graphs are used to demonstrate reduction of computation time caused by parallel computing. Web of Scienceart. no. 18736...|$|E
40|$|As {{increasing}} {{awareness of}} climate changes and surging power costs for big data centers today energy efficiency becomes increasingly important. In {{addition to that}} we carry mobile devices that depend on battery technology that is falling behind the rapid evolution of transistor technology and ever increasing power demands. At {{the same time there}} is an understanding that computer resources are not efficiently used. One solution to this is the proposed Space-Bounded scheduling, a scheduler that schedules tasks with the goal of achieving better cache locality. At the other side there is also a rise in <b>HPC</b> (<b>High</b> <b>Performance</b> <b>Computing)</b> popularity and a rising demand for powerful and easy-to-implement systems that are portable yet still customizable. For this demand the PGAS (Portable Global Address Space) model fits well and UPC++ is one of the newest editions to that category. Implemented as a C++ library it is both portable, powerful and easy to use. We combine the advantages of Space-Bounded scheduling with the performance and simplicity of UPC++ to create Space-Bounded Async Tasks: A UPC++ extension that schedules async tasks with consideration of cache locality...|$|E
40|$|Abstract- We have {{proposed}} a unified approach to the modeling and study of developments {{in the field of}} Nanotechnology and its application in futuristic Nano-enabled Cells. The necessity of a nonporous membrane has been eminent in recent fuel cells. The need for MEMS based models for porous silicon based membranes based on nano imprints technology has been met by modeling it in SUGAR in MATLAB environment. We have identified and categorized the domains of Nano enabled solar cells and have put forth a proposition for multi scale modeling of Solar cells. Multi scale modeling on <b>HPC</b> (<b>High</b> <b>performance</b> <b>computing)</b> of a Nano enabled solar cell is shown under MCCS (Microsoft compute cluster environment) environment using extreme optimization numerical library is implemented. Distribution and performance analysis of four levels of computation in a multi scale model is implemented with the distribution being carried out from classical semi-conductor to quantum levels as to accurately predict the behavior and properties of the solar cell as per the needs of the engineering of devices. I...|$|E
5000|$|... #Article: International Conference on <b>High</b> <b>Performance</b> <b>Computing</b> ...|$|R
5000|$|... #Article: Massachusetts Green <b>High</b> <b>Performance</b> <b>Computing</b> Center ...|$|R
5000|$|Northern Illinois University, <b>High</b> <b>Performance</b> <b>Computing</b> Laboratory ...|$|R
