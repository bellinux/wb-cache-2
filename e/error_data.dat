356|5486|Public
25|$|Subsequent to {{this the}} ICC {{received}} data from laboratory based analyses, {{on the basis that}} these measurement environments are more controlled, involving more sophisticated measurement technologies such as the Vicon Motion Analysis system. These were subject to less measurement <b>error.</b> <b>Data</b> was provided by the Australian Institute of Sport, the University of Western Australia and the Motion Analysis Corporation system from the University of Auckland. The ICC also carried out further video based three-dimensional analyses on all bowlers during the 2004 Champions Trophy in England. Regardless of the biomechanical measurement protocol used, a strikingly similar pattern emerged: the normal biomechanics of cricket bowling, whether it be spin or pace, features an element of elbow extension. The average extension of a normal, seemingly legal delivery was 8-10 degrees for all bowler types. There were virtually zero instances of no elbow extension at all in accordance with the original laws.|$|E
50|$|Until {{the late}} 1960s {{research}} on speech {{was focused on}} comprehension. As researchers collected greater volumes of speech <b>error</b> <b>data,</b> they began to investigate the psychological processes responsible {{for the production of}} speech sounds and to contemplate possible processes for fluent speech. Findings from speech error research were soon incorporated into speech production models. Evidence from speech <b>error</b> <b>data</b> supports the following conclusions about speech production.|$|E
5000|$|... #Caption: A vector signal {{analyzer}} display {{featuring a}} constellation diagram, demodulation <b>error</b> <b>data,</b> signal spectrum, and the real-time measured signal ...|$|E
5000|$|... {{drift in}} the <b>errors</b> (<b>data</b> {{collected}} over time): run charts {{of the response}} and errors versus time ...|$|R
30|$|However, during <b>data</b> collection, <b>data</b> <b>errors</b> and <b>data</b> missing {{would occur}} {{occasionally}} because the wireless transmission network {{is affected by}} the external environment during data transmission. Therefore, the original data need to be processed before being used in experiments. Deletion and interpolation processing are adopted to deal with <b>data</b> <b>errors</b> and <b>data</b> missing, respectively.|$|R
40|$|Experiments {{show that}} giving users {{incentives}} helps them detect <b>errors</b> in <b>data</b> and that {{expectations about the}} base rate of <b>errors</b> in <b>data</b> improve <b>error</b> detection.   However, experimental findings are often criticized as not being generalizable to organizational settings. The study reported here examines the generalizability of experimental findings on user detection of <b>data</b> <b>errors.</b> A field interview {{study was conducted to}} examine this question. Twenty interviews were conducted with users of information systems in a variety of organizational settings. The findings of the field interview study show that strong informal incentives, perceptions about the materiality of <b>data</b> <b>errors,</b> and perceptions about the base rate of <b>errors</b> in <b>data</b> affect the detection of <b>data</b> <b>errors</b> in organizational settings.  ...|$|R
5000|$|Errors {{in speech}} are non-random. Linguists can elicit from the speech <b>error</b> <b>data</b> how speech errors are {{produced}} and which linguistic rules they adhere to. As a result, {{they are able}} to predict speech errors.|$|E
50|$|A {{more recent}} (than Fromkin's) {{attempt to explain}} speech {{production}} was published by Garrett in 1975. Garrett also created this model by compiling speech <b>error</b> <b>data</b> {{and there are many}} overlaps between this model and the Fromkin model off which it was based, but he did add a few things to the Fromkin model that filled some of the gaps being pointed out by other researchers. The Garrett Model and the Fromkin model both distinguish between three levels—a conceptual level, and sentence level, and a motor level. These three levels are common to contemporary understanding of Speech Production.|$|E
5000|$|At {{each moment}} of using TOC search, one can {{immediately}} {{start a new}} search or switch to advanced search tool. While exploring the data base, a researcher usually performs multiple search. In order to help the user of the TOC search, we have implemented the “select tool”. This tool enables the user to put all the important results obtained from different search. In this way, all the data that are essential in the research are available during the further TOC search exploration. The data in the “selection” tool are easily added or removed. By using option “Feedback” the user may send a message on TOC search administrator on different topics (site bugs, <b>error</b> <b>data,</b> comments, suggestions, etc. [...] ) ...|$|E
3000|$|It can be {{seen that}} the MSE can be {{decomposed}} into two components: the bias (also known as the <b>data</b> <b>error)</b> and the variance (also known as the noise <b>error).</b> The <b>data</b> <b>error</b> is caused by using a modified inverse of the ICI matrix [...]...|$|R
40|$|Data {{warehouses}} consolidate {{various activities}} {{of a business}} and often form the backbone for generating reports that support important business decisions. <b>Errors</b> in <b>data</b> tend to creep in {{for a variety of}} reasons. Some of these reasons include <b>errors</b> during input <b>data</b> collection and <b>errors</b> while merging <b>data</b> collected independently across different databases. These <b>errors</b> in <b>data</b> warehouses often result in erroneous upstream reports, and could impact business decisions negatively. Therefore, one of the critical challenges while maintaining large data warehouses is that of ensuring the quality...|$|R
5000|$|The {{fundamental}} {{advantage of}} ERP {{is that the}} integration of myriad business processes saves time and expense. Management can make decisions faster and with fewer <b>errors.</b> <b>Data</b> becomes visible across the organization. Tasks that benefit from this integration include: ...|$|R
50|$|In survey data, {{careless}} {{responses are}} those that are defined to have not been entirely authentic or to be lacking in relevance to the topic being examined in the study. Also referred to as random response, this is an area of concern in research studies and data collection due to the possible impacts that <b>error</b> <b>data</b> could have on the significance conclusion to be drawn later. Attention and interest are both factors that have a possible influence on the validity of an individual's responses. Careless data can lead to lower reliability which will ultimately decrease the intensity of correlation, if one exists. A method known as data screening is recommended as a means of discerning between response data that is valid and that which is careless.|$|E
50|$|Subsequent to {{this the}} ICC {{received}} data from laboratory based analyses, {{on the basis that}} these measurement environments are more controlled, involving more sophisticated measurement technologies such as the Vicon Motion Analysis system. These were subject to less measurement <b>error.</b> <b>Data</b> was provided by the Australian Institute of Sport, the University of Western Australia and the Motion Analysis Corporation system from the University of Auckland. The ICC also carried out further video based three-dimensional analyses on all bowlers during the 2004 Champions Trophy in England. Regardless of the biomechanical measurement protocol used, a strikingly similar pattern emerged: the normal biomechanics of cricket bowling, whether it be spin or pace, features an element of elbow extension. The average extension of a normal, seemingly legal delivery was 8-10 degrees for all bowler types. There were virtually zero instances of no elbow extension at all in accordance with the original laws.|$|E
5000|$|Levelt further refined the {{lexical network}} {{proposed}} by Dell. Through {{the use of}} speech <b>error</b> <b>data,</b> Levelt recreated the three levels in Dell's model. The conceptual stratum, the top and most abstract level, contains information a person has about ideas of particular concepts. The conceptual stratum also contains ideas about how concepts relate to each other. This is where word selection would occur, a person would choose which words they wish to express. The next, or middle level, the lemma-stratum, contains information about the syntactic functions of individual words including tense and function. [...] This level functions to maintain syntax and place words correctly into sentence structure that makes sense to the speaker. The lowest and final level is the form stratum which, similarly to the Dell Model, contains syllabic information. From here, the information stored at the form stratum level {{is sent to the}} motor cortex where the vocal apparatus are coordinated to physically produce speech sounds.|$|E
30|$|A graphic user {{interface}} is designed and implemented to make data entry easier {{for people who are}} not expert in Structured Query Language (SQL). Control mechanisms are also implemented to constrain and regulate data entry by checking <b>errors,</b> <b>data</b> completeness, and consistency.|$|R
5000|$|The McDonnell Genome Institute {{makes all}} {{sequence}} data {{available to the}} research community, pending appropriate quality analysis. Some of this data is preliminary and is subject to omissions and <b>errors.</b> <b>Data</b> also changes based {{on the availability of}} new data and assembly versions.|$|R
3000|$|It can be {{seen that}} the error between yp+ 1 (which is an {{estimate}} of the vector of the transmitted symbols b) and the vector of the transmitted symbols b consists of two components: the <b>data</b> <b>error</b> and the noise <b>error.</b> The <b>data</b> <b>error</b> is caused by using a modified inverse of the system cross-correlation matrix [...]...|$|R
40|$|Research on {{practical}} {{design verification}} techniques {{has long been}} impeded {{by the lack of}} published and yet detailed <b>error</b> <b>data.</b> Over the last few years we have systematically collected design <b>error</b> <b>data</b> from a number of academic microprocessor design projects. We present an analysis of this data and report on the lessons learned in the collection effort...|$|E
40|$|With {{transistor}} budgets ever expanding, microprocessor architects are steadily integrat-ing new {{and more}} sophisticated mechanisms into their designs to boost performance. To cope with this increase in complexity, successful processor verification efforts must employ a vari-ety of complementary verification technologies to achieve an acceptable level of functional cor-rectness in the final product. Research on prac-tical verification techniques for microprocessors has long been impeded {{by the lack of}} published <b>error</b> <b>data,</b> despite the abundance of design errors in large-scale projects. It is common indus-try practice to record design errors, but this infor-mation is considered proprietary and, perhaps, embarrassing, so it rarely appears in public. Detailed <b>error</b> <b>data</b> is especially valuable to veri-fication approaches that use error models to direct test generation. 1, 2 Furthermore, sets of designs and corresponding errors can serve as benchmarks to compare different verification methods. Finally, statistical reliability analysis methods rely heavily on this type of data. 3 These considerations led us to turn to acad-emia as a source of <b>error</b> <b>data</b> from micro-processor design. We first report on the modest amount of <b>error</b> <b>data</b> published by industry. We then describe our method to systematical-ly collect design <b>error</b> <b>data</b> from university design projects. After presenting and analyzing the data we collected, we offer some advice on the collection process based on lessons painfully learned. Industrial <b>Error</b> <b>Data</b> Although design errors that make their way into final products are common, microproces-sor manufacturers have not always been forth-coming about them. This has changed sinc...|$|E
40|$|Rather than {{automatically}} {{proceeding to}} forecast with data {{at the same}} level of aggregation as that required for an organizationÕs operations, the authors explain that the best level of aggregation for forecasting should be chosen by the forecasters in consideration of the trade- off between sampling <b>error</b> (<b>data</b> inadequate to generate reliable forecasts) and specification <b>error</b> (<b>data</b> too aggregated to represent diverse demands...|$|E
40|$|A new {{extension}} of the Boolean association rules, ordinal association rules, that incorporates ordinal relationships among data items, is introduced. One use for ordinal rules is to identify possible <b>errors</b> in <b>data.</b> A method that finds these rules and identifies potential <b>errors</b> in <b>data</b> is proposed...|$|R
40|$|Experimental {{data are}} {{correlated}} with cubic {{equation of state}} with vdW and GE mixing rules. Model parameters are adjusted by minimization of objective function defined by maximum likelihood method for data with normal distribution function <b>errors.</b> <b>Data</b> are compared to existing data sets at temperatures 30, 50, 60 °C...|$|R
50|$|Check for {{transcription}} <b>errors</b> in <b>data</b> {{input and}} reference.|$|R
40|$|A design {{verification}} methodology for microprocessor hardware based on modeling design errors and generating simulation vectors for the modeled errors via physical fault testing techniques is presented. We have systematically collected design <b>error</b> <b>data</b> {{from a number}} of microprocessor design projects. The <b>error</b> <b>data</b> is used to derive error models suitable for {{design verification}} testing. A class of basic error models is identified [...] ...|$|E
40|$|Abstract. Research on {{practical}} {{design verification}} techniques {{has long been}} impeded {{by the lack of}} published and yet detailed <b>error</b> <b>data.</b> Over the last few years we have systematically collected design <b>error</b> <b>data</b> from a number of academic microprocessor design projects. We present an analysis of this data and report on the lessons learned in the collection effort. Index terms: functional verification, data collection, design verification, design errors, microprocessor testing Contact information...|$|E
40|$|A {{project is}} under way at the University of Michigan to develop a design {{verification}} methodology for microprocessor hardware based on modeling design errors and generating simulation vectors for the modeled errors via physical fault testing techniques. We have developed a method to systematically collect design <b>error</b> <b>data,</b> and gathered concrete <b>error</b> <b>data</b> {{from a number of}} microprocessor design projects. The <b>error</b> <b>data</b> are being used to derive error models suitable for design verification testing. Design verification is done by simulating tests targeted at instances of the modeled errors. We are conducting experiments in which targeted tests are generated for modeled errors in circuits ranging from RTL combinational circuits to pipelined microprocessors. The experiments gauge the quality of the error models and explore test generation for these models. This paper describes our approach and presents some initial experimental results. ...|$|E
50|$|In {{reference}} to databases, this is <b>data</b> that contain <b>errors.</b> Unclean <b>data</b> can contain such mistakes as spelling or punctuation <b>errors,</b> incorrect <b>data</b> {{associated with a}} field, incomplete or outdated data, or even data that has been duplicated in the database. It can be cleaned through {{a process known as}} data cleansing.|$|R
40|$|Two {{separate}} algorithms {{are derived}} for testing filter sensitivity to systematic <b>data</b> <b>errors.</b> One algorithm provides the absolute minimum Euclidean norm <b>data</b> <b>error</b> {{for a given}} estimate component error. The second algorithm {{can be used to}} find the minimum norm <b>data</b> <b>error</b> which can be generated by restricted degree Legendre polynomials. A specific very long baseline interferometry (VLBI) baseline estimation is analyzed with the algorithm. It is found that the local vertical is the most sensitive component to <b>error</b> in the <b>data</b> space. The efficiency of a <b>data</b> <b>error</b> sequence linear in elevation angle is within 7 % that of the absolute worst case sequence. Elevation angle dependent errors are explored and the special case of a mismodeled troposphere is treated...|$|R
40|$|This paper {{examines}} {{the consequences of}} <b>data</b> <b>error</b> in <b>data</b> series used to construct aggregate indicators. Using the most popular indicator of country level economic development, the Human Development Index (HDI), we identify three separate sources of <b>data</b> <b>error.</b> We propose a simple statistical framework to investigate how <b>data</b> <b>error</b> may bias rank assignments and identify two striking consequences for the HDI. First, using the cutoff values used by the United Nations to assign a country as ‘low’, ‘medium’, or ‘high’ developed, we find that currently up to 45 % of developing countries are misclassified. Moreover, by replicating prior development/macroeconomic studies, we find that key estimated parameters such as Gini coefficients and speed of convergence measures vary by up to 100 % due to <b>data</b> <b>error.</b> Measurement Error, International Comparative Statistics, International Development, O 10, C 82,...|$|R
40|$|Abstract. Coordinate {{measuring}} machine (CMM) plays a {{more and more}} {{important role in the}} flatness evaluation. The pre-processing for eliminating <b>error</b> <b>data</b> is an important process {{to improve the quality of}} flatness evaluation. So, flatness uncertainty estimation becomes a key problem. To solve the problem, a flatness uncertainty estimation method based on data elimination is presented. Firstly, a method for excluding the <b>error</b> <b>data</b> based on statistical theory is expatiated with the consideration of probability theory and mathematical statistics. Secondly, details of calculating flatness uncertainty based on least-square method are analyzed. Then the measured 3 -dimensional data are processed so as to put the excluding method into use. So the <b>error</b> <b>data</b> can be deleted and the flatness uncertainty can be decreased and can achieve a good evaluation quality. Finally, an example is given and the result shows that the method proposed in this paper is feasible...|$|E
3000|$|By {{reducing}} the number of training samples, the performance of the KDA-based ordinal regression tends to become worse. Note that in the proposed method, we regard the <b>error</b> <b>data</b> e [...]...|$|E
40|$|A {{categorized}} {{data base}} of software errors which were discovered during the {{various stages of}} development and operational use of the Deep Space Network DSN/Mark 3 System was developed. A study team identified several existing error classification schemes (taxonomies), prepared a detailed annotated bibliography of the error taxonomy literature, and produced a new classification scheme which was tuned to the DSN anomaly reporting system and encapsulated the work of others. Based upon the DSN/RCI error taxonomy, <b>error</b> <b>data</b> on approximately 1000 reported DSN/Mark 3 anomalies were analyzed, interpreted and classified. Next, <b>error</b> <b>data</b> are summarized and histograms were produced highlighting key tendencies...|$|E
50|$|Computer Communication Systems, Volume 1, <b>Data</b> Circuits, <b>Error</b> Detection, <b>Data</b> Links, John Wiley & Sons, 1990.|$|R
40|$|We {{measure and}} examine <b>data</b> <b>error</b> in health, {{education}} and income statistics {{used to construct}} the Human Development Index. We identify three sources of <b>data</b> <b>error</b> which are due to (i) data updating, (ii) formula revisions and (iii) thresholds to classify a country’s development status. We propose a simple statistical framework to calculate country specific measures of data uncertainty and investigate how <b>data</b> <b>error</b> biases rank assignments. We find that up to 34 % of countries are misclassified and, by replicating prior studies, we show that key estimated parameters vary by up to 100 % due to <b>data</b> <b>error.</b> ...|$|R
30|$|A {{spreadsheet}} of {{the data}} will be provided as an Additional file 1 : <b>Error</b> analysis <b>data.</b>|$|R
