61|153|Public
5000|$|Speculative {{execution}} and out-of-order completion (called [...] "dynamic execution" [...] by Intel), which required new retire {{units in the}} <b>execution</b> <b>core.</b> This lessened pipeline stalls, and in part enabled greater speed-scaling of the Pentium Pro and successive generations of CPUs.|$|E
5000|$|In {{the case}} of C7, the design team have focused on further {{streamlining}} the (front-end) of the chip, i.e. cache size, associativity and throughput {{as well as the}} prefetch system. [...] At the same time no significant changes to the <b>execution</b> <b>core</b> (back-end) of the chip.|$|E
50|$|The Pineview {{platform}} {{has proven}} to be only slightly faster than the previous Diamondville platform. This is because the Pineview platform uses the same Bonnell <b>execution</b> <b>core</b> as Diamondville and is connected to the memory controller via the FSB, hence memory latency and performance in CPU-intensive applications are minimally improved.|$|E
5000|$|<b>Execution</b> <b>cores.</b> (single {{precision}} floating-point units, {{double precision}} floating-point units, special function units (SFUs)).|$|R
40|$|Nowadays, {{there is}} a clear trend in {{industry}} towards employing the growing amount of transistors on chip in replicating <b>execution</b> <b>cores,</b> where each core is Simultaneous Multithreading (SMT). In order to appropriately connect such a growing number of on-chip <b>execution</b> <b>cores</b> to a shared cache subsystem, some traditional considerations regarding SMT should be revisited. In this paper we study the effects of long latency instructions and their interaction with the processor to cache interconnection network in new scenarios comprising multiple on-chip SMT cores. We show results that clearly indicate the important role of the on-chip interconnection networks, used to communicate the <b>execution</b> <b>cores</b> with the shared L 2 Cache, and its interaction with the specific architecture of each core. ...|$|R
50|$|Smart Cache is a level 2 or level 3 caching {{method for}} {{multiple}} <b>execution</b> <b>cores,</b> developed by Intel.|$|R
5000|$|... #Caption: In this {{high-level}} {{depiction of}} HTT, instructions are fetched from RAM (differently colored boxes represent the instructions of four different programs), decoded and reordered {{by the front}} end (white boxes represent pipeline bubbles), and passed to the <b>execution</b> <b>core</b> capable of executing instructions from two different programs during the same clock cycle.|$|E
50|$|SuperPrime is a {{computer}} program used for calculating the primality of a large set of positive natural numbers. Because of its multi-threaded nature and dynamic load scheduling, it scales excellently when using more than one thread (<b>execution</b> <b>core).</b> It is commonly used as an overclocking benchmark to test the speed and stability of a system.|$|E
5000|$|In {{the case}} of VIA C7, the design team have focused on further {{streamlining}} the [...] "front-end" [...] of the chip, i.e. cache size, associativity and throughput {{as well as the}} prefetch system. [...] At the same time no significant changes to the <b>execution</b> <b>core</b> ("back-end") of the chip seem to have been made.|$|E
40|$|Semiconductor {{technological}} {{advances in the}} recent years have led to the inclusion of multiple CPU <b>execution</b> <b>cores</b> in a single processor package. This processor architecture is known as Multi-core (MC) or Chip Multi Processing (CMP). Any application which is well optimized and scales with SMP will take immediate benefit of the multiple <b>execution</b> <b>cores,</b> provided by the Multi-core architecture. Even if the application is single-threaded, multi-tasking environment will take advantage of these multiple <b>execution</b> <b>cores.</b> 2. 6 Linux kernels (which have better SMP scalability compared to 2. 4 kernels) take instant advantage of the MC architecture. MC also brings in performance optimization opportunities which will further enhance the performance. This paper captures the recent enhancements to the 2. 6 Linux Kernel which better support and enhance the performance of Multi-core capable platforms. 1...|$|R
40|$|Abstract. The {{execution}} of the PLC application is accomplished through hardware, while the PAC designed a generic form of software <b>execution</b> <b>cores</b> for executing user application <b>execution</b> <b>cores</b> located between the real-time operating systems and applications, {{the implementation of the}} kernel and device hardware platform independent, this paper presents under the PLC and PC continuous development and integration of the case, its organizational structure with high computing and large storage space advantages of a PC with high stability...|$|R
5000|$|On-die memory controller: {{the memory}} is {{directly}} connected to the processor. It is called the uncore part and runs at a different clock (uncore clock) than the <b>execution</b> <b>cores.</b>|$|R
50|$|With a superscalar processor, the {{instruction}} {{window of the}} processor fills up {{with a number of}} instructions (known as the issue rate). Depending on the scheme that the superscalar processor uses to dispatch these instruction from the window to the <b>execution</b> <b>core</b> of the CPU, we may encounter problems if there is a dependency not unlike the one shown above.|$|E
50|$|CPUs {{that fully}} support out-of-order {{execution}} of loads and stores {{must be able}} to detect RAW dependence violations when they occur. However, many CPUs avoid this problem by forcing all loads and stores to execute in-order, or by supporting only a limited form of out-of-order load/store execution. This approach offers lower performance compared to supporting full out-of-order load/store execution, but it can significantly reduce the complexity of the <b>execution</b> <b>core</b> and caches.|$|E
50|$|An xCORE200 node {{comprises}} two physical cores and a switch. The <b>execution</b> <b>core</b> has a data path, a memory, and register {{banks for}} eight threads.The switches {{of two or}} more xCORE200 nodes can be connected using links, whereupon threads on all of the cores can communicate with each other by exchanging messages through the switches. The switching mechanism is abstracted by means of a channel, a virtual connection between two threads.The switch has eight external links, permitting a maximum throughput of 3.2 GBits/s to other cores.|$|E
40|$|We propose and {{evaluate}} a data filtering method {{to reduce the}} power consumption of high-end processors with multiple <b>execution</b> <b>cores.</b> Although the proposed method {{can be applied to}} a wide variety of multi-processor systems including MPPs, SMPs and any type of single-chip multiprocessor, we concentrate on Network Processors. The proposed method uses an execution unit called Data Filtering Engine that processes data with low temporal locality before it is placed on the system bus. The <b>execution</b> <b>cores</b> use locality to decide which load instructions have low temporal locality and which portion of the surrounding code should be off-loaded to the data filtering engine...|$|R
40|$|We {{propose a}} data {{filtering}} method {{to reduce the}} power consumption of high-end processors with multiple <b>execution</b> <b>cores.</b> Although the proposed method {{can be applied to}} a wide variety of multi-processor systems including MPPs, SMPs and any type of single-chip multiprocessor, we concentrate on Network Processors, where most of the existing architectures use the multiple core design methodology. The proposed method uses an execution unit called Data Filtering Engine that processes data with low temporal locality before it is placed on the system bus. The <b>execution</b> <b>cores</b> use locality to decide which load instructions have low temporal locality and which portion of the surrounding code should be off-loaded to the data filtering engine. Our techniqu...|$|R
40|$|In {{this era}} of computing, each {{processor}} package has multiple <b>execution</b> <b>cores.</b> Each of these <b>execution</b> <b>cores</b> {{is perceived as a}} discrete logical processor by the software. Any operating system that is optimized for Symmetric Multi Processing (SMP) and that scales well with the increase in processor count can instantaneously benefit from these multiple <b>execution</b> <b>cores.</b> Design innovations in multi-core processor architectures bring new optimization opportunities and challenges for the system software. Addressing these challenges will further enhance system performance. The process (task) scheduler, in particular, one of the critical components of system software, is garnering great interest. In this paper, we look at how the different multi-core topologies and the associated processor power management technologies bring new optimization opportunities to the process scheduler. We look into different scheduling mechanisms and the associated tradeoffs. Using the Linux * Operating System as an example, we also look into how some of these scheduling mechanisms are currently implemented. As the multi-core platform is evolving, some portions of the hardware and software are being reshaped to take maximum advantage of the platform resources. We close this paper with a look at where future efforts in this technology are heading...|$|R
50|$|The XS1-SU1 {{comprises}} {{a single}} core processor, a USB PHY, a switch, digital I/O ports, and analogue I/O ports. The <b>execution</b> <b>core</b> has a data path, a memory, and register banks for eight threads. The switches {{of two or}} more XS1-SU and Xcore XS1-L processors can be connected using one or more links, whereupon threads on all of the cores can communicate with each other by exchanging messages through the switches. The XCore XS1 instruction set architecture supports 12 general purpose registers per thread. A standard 3-operand instruction set is used for programming the thread.|$|E
50|$|The Pentium M coupled the <b>execution</b> <b>core</b> of the Pentium III with a Pentium 4 {{compatible}} bus interface, {{an improved}} instruction decoding/issuing front end, improved branch prediction, SSE2 support, {{and a much}} larger cache. The usually power-hungry secondary cache uses an access method which only switches on the portion being accessed. The main intention behind the large cache was to keep a decent-sized portion of it still available to the processor even {{when most of the}} L2 cache was switched off, but its size led to a welcome improvement in performance.|$|E
50|$|An XS1-L node {{comprises}} {{a single}} core processor and a switch. The <b>execution</b> <b>core</b> has a data path, a memory, and register banks for eight threads. The switches {{of two or}} more XS1-L nodes can be connected using a link, whereupon threads on all of the cores can communicate with each other by exchanging messages through the switches. The switching mechanism is abstracted by means of a channel, a virtual connection between two threads.The switch has eight external links, permitting a maximum throughput of 3.2 GBits/s to other cores.|$|E
25|$|While the Core {{microarchitecture}} {{is a major}} architectural revision it {{is based}} in part on the Pentium M processor family designed by Intel Israel. The Penryn pipeline is 12–14 stages long — less than half of Prescott's, a signature feature of wide order <b>execution</b> <b>cores.</b> Penryn's successor, Nehalem borrowed more heavily from the Pentium 4 and has 20-24 pipeline stages. <b>Core's</b> <b>execution</b> unit is 4 issues wide, compared to the 3-issue cores of P6, Pentium M, and 2-issue cores of NetBurst microarchitectures. The new architecture is a dual core design with linked L1 cache and shared L2 cache engineered for maximum performance per watt and improved scalability.|$|R
50|$|Smart Cache {{shares the}} actual cache memory between the cores of a {{multi-core}} processor. In comparison to a dedicated per-core cache, the overall cache miss rate decreases when not all cores need equal {{parts of the}} cache space. Consequently, a single core can use the full level 2 or level 3 cache, if the other cores are inactive. Furthermore, the shared cache makes it faster to share memory among different <b>execution</b> <b>cores.</b>|$|R
5000|$|... 2005 October: Shift to dual-core processors: DP 2.0 GHz → DC 2.0 GHz, DP 2.3 GHz → DC 2.3 GHz, DP 2.7 GHz → DP DC 2.5 GHz (termed a Quad Power Mac G5, {{with four}} CPU <b>execution</b> <b>cores</b> and more {{reliable}} liquid cooling), all with DDR2 memory, and PCI Express expansion {{in place of}} PCI-X. The older PCI-X, DP 2.7 GHz model remained available for a while, but the slower speed single-core models were discontinued immediately.|$|R
5000|$|The Power Mac G5 line in 2006 {{consisted}} of three, dual-core PowerPC G5 configurations, which can communicate through its FSB at half its internal clock speed. Each processor in the Power Mac G5 has two unidirectional 32-bit pathways: one {{leading to the}} processor and the other from the processor. These result in a total bandwidth of up to 20 GB/s. The processor {{at the heart of}} the Power Mac G5 has a [...] "superscalar, superpipelined" [...] <b>execution</b> <b>core</b> that can handle up to 216 in-flight instructions, and uses a 128-bit, 162-instruction SIMD unit (AltiVec).|$|E
50|$|The XS1-AnA {{comprises}} {{a single}} or dual tile processor, {{each with a}} switch, digital I/O ports, and set of analogue input channels. The <b>execution</b> <b>core</b> has a data path, a memory, and register banks for eight threads. The switches {{of two or more}} XS1-AnA, xCORE-UnA and Xcore XS1-L processors can be connected using one or more links, whereupon threads on all of the tiles can communicate with each other by exchanging messages through the switches. The XCore XS1 instruction set architecture supports 12 general purpose registers per thread. A standard 3-operand instruction set is used for programming the thread.|$|E
50|$|Core Duo {{contains}} 151 million transistors, {{including the}} shared 2 MB L2 cache. Yonah's <b>execution</b> <b>core</b> contains a 12-stage pipeline, forecast to eventually {{be able to}} run at a maximum frequency of 2.33 - 2.50 GHz. The communication between the L2 cache and both execution cores is handled by a bus unit controller through arbitration, which reduces cache coherency traffic over the FSB, at the expense of raising the core-to-L2 latency from 10 clock cycles (in the Dothan Pentium M) to 14 clock cycles. The increase in clock frequency offsets the impact of the increased clock cycle latency. The power management components of the core features improved grained thermal control, as well as independent scaling of power between the two cores, resulting in very efficient management of power.|$|E
40|$|As process {{technology}} scales, both fabrication induced and in-operation hard faults {{will become more}} prevalent, limiting yield and effective product lifetime. The simultaneous emergence of chip multiprocessors (CMPs) and revitalization of machine virtualization offers several opportunities for hard failure tolerance. In this paper, we provide preliminary analysis of methods for lifetime recoverability on CMPs which contain partially functional <b>execution</b> <b>cores.</b> Specifically, we examine how processor virtualization can help a CMP architecture architecture overcome faults by migrating computation or virtualizing functionality which cannot be supported by the hardware {{as a result of}} failure. ...|$|R
30|$|A {{multicore}} architecture has {{a single}} processor package that contains two or more processors. All cores can execute instructions independently and simultaneously. The operating system will treat each of the <b>execution</b> <b>cores</b> as a discrete processor. The design and integration of such processors with transistor counts in the millions poses a challenge to designers given {{the complexity of the}} task and the time to market constraints. Hence, early virtual system prototyping and performance analysis provides designers with critical information {{that can be used to}} evaluate various architectural approaches, functionality, and processing requirements.|$|R
40|$|The load {{instructions}} {{of some of}} the bioinformatics applications in the BioPerf suite possess interesting characteristics: only a few static loads cover almost the entire dynamic load execution and they almost always hit in the data cache. Nevertheless, these {{load instructions}} represent a major performance bottleneck. They often precede or follow branches that are hard to predict, which makes their L 1 hit latency difficult to hide even in dynamically scheduled <b>execution</b> <b>cores.</b> This paper investigates this behavior and suggests simple source-code transformations to improve the performance of these benchmark programs by up to 92 %. 1...|$|R
5000|$|CMT is {{a simpler}} but similar design {{philosophy}} to SMT; both designs try to utilize execution units efficiently; in either method, when two threads compete for some execution pipelines, {{there is a}} loss in performance {{in one or more}} of the threads. Due to dedicated integer cores, the Bulldozer family modules performed roughly like a dual core dual thread processor during sections of code that were either wholly integer or a mix of integer and floating point; yet, due to the SMT use of the shared floating point pipelines, the module would perform similarly to a single core dual thread SMT processor (SMT2) for a pair of threads saturated with floating point instructions. (Both of these last two comparisons make the assumption that the comparison processor possesses and equally wide and capable <b>execution</b> <b>core,</b> integer-wise and floating-point wise, respectively.) ...|$|E
40|$|Abstract: The study {{presents}} an <b>execution</b> <b>core</b> {{which can be}} reconfigured either for calculation of digital convolution or for computation of discrete orthogonal transform by appropriate local buffer initialization of processing cells. It is shown that the data flow pattern can be changed by a single bit control signal. The proposed core can be connected to port 1 of Intel 8051 to derive the necessary control signals for reconfiguration. The core {{can be used as}} a pluggable module with existing microcontroller when DSP algorithms are required to be implemented. Using such <b>execution</b> <b>core</b> the computational load of the processor can be significantly reduced as the math-intensive components of the DSP algorithm is relegated to the <b>execution</b> <b>core.</b> The use of such pipelined core will not only caters to the need of real-time performance, but also it will facilitate scalability, reusability and flexibility for wide varieties of DSP functionalities...|$|E
30|$|We use the Alpha EV 6 as {{our base}} {{processor}} (Kessler 1999) with a 3  GHz clock frequency. The Alpha EV 6 is an out-of-order speculative <b>execution</b> <b>core</b> that {{is commonly used}} as a test-bench core in thermal management research.|$|E
5000|$|Superscalar Out-of-order <b>execution</b> Power Architecture <b>core,</b> {{specially}} modified for the Wii platform ...|$|R
40|$|High {{performance}} on multicore processors requires that schedulers be reinvented. Traditional schedulers focus on keeping execution units busy by assigning each core a thread to run. Schedulers ought to focus, however, on high utilization of on-chip memory, {{rather than of}} <b>execution</b> <b>cores,</b> to reduce the impact of expensive DRAM and remote cache accesses. A challenge in achieving good use of on-chip memory is that the memory is split up among the cores {{in the form of}} many small caches. This paper argues for a form of scheduling that assigns each object and its operations to a specific core, moving a thread among the cores as it uses different objects. ...|$|R
5000|$|For many reasons, it {{is highly}} desired that the {{interrupt}} handler execute as briefly as possible, and {{it is highly}} discouraged (or forbidden) for a hardware interrupt to invoke potentially blocking system calls. In a system with multiple <b>execution</b> <b>cores,</b> considerations of reentrancy are also paramount. If the system provides for hardware DMA, concurrency issues can arise even with only a single CPU core. (It {{is not uncommon for}} a mid-tier microcontroller to lack protection levels and an MMU, but still provide a DMA engine with many channels; in this scenario, many interrupts are typically triggered by the DMA engine itself, and the associated interrupt handler is expected to tread carefully.) ...|$|R
