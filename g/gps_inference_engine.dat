0|1860|Public
40|$|Abstract—This paper extends a {{previous}} market microstruc-ture model, where we used Genetic Programming (<b>GP)</b> as an <b>inference</b> <b>engine</b> for trading rules, and Self Organizing Maps as a clustering machine for those rules. Experiments in that work took place {{under a single}} financial market and investigated whether its behavior is non-stationary or cyclic. Results showed that the market’s behavior was constantly changing and strategies that would not adapt to these changes, would become obsolete, and their performance would thus decrease over time. However, because experiments in that work {{were based on a}} specific GP algorithm, we are interested in this paper to prove that those results are independent of the choice of such algorithms. We thus repeat our previous tests under two more GP frameworks. In addition, while our previous work surveyed only a single market, in this paper we run tests under 10 markets, for generalization purposes. Finally, we deepen our analysis and investigate whether the performance of strategies, which have not co-evolved with the market, follows a continuous decrease, as it has been previously suggested in the agent-based artificial stock market literature. Results show that our previous results are not sensitive to the choice of GP. Strategies that do not co-evolve with the market, become ineffective. However, we do not find evidence for a continuous performance decrease of these strategies. I...|$|R
40|$|Abstract. Isaac is a {{rule-based}} {{visual language}} for mobile robots using evidential reasoning and a fuzzy <b>inference</b> <b>engine.</b> A prototype <b>inference</b> <b>engine</b> for Isaac has been implemented, permitting experiments with the Isaac language. This paper discusses this <b>inference</b> <b>engine,</b> describes some preliminary experiences with programming Isaac rulesets, and proposes future optimizations and enhancements to the <b>inference</b> <b>engine.</b> ...|$|R
5000|$|OntoBroker is an <b>inference</b> <b>engine</b> {{with native}} {{reasoning}} over F-Logic, ObjectLogic, RIF, and OWL. (https://www.w3.org/2001/sw/wiki/OntoBroker, W3C-listed <b>inference</b> <b>engine)</b> ...|$|R
50|$|An {{expert system}} {{is divided into}} two subsystems: the <b>inference</b> <b>engine</b> and the {{knowledge}} base. The knowledge base represents facts and rules. The <b>inference</b> <b>engine</b> applies the rules to the known facts to deduce new facts. <b>Inference</b> <b>engines</b> can also include explanation and debugging abilities.|$|R
40|$|Abstract —The {{semantic}} web has brought many challenges for knowledge representation and inference system [2]. <b>Inference</b> <b>Engine</b> play {{important role to}} extract additional information implicitly by using fact and ontologies [3]. Standard benchmark practices are {{used to analyze the}} performance of different <b>inference</b> <b>engine</b> for different version of ontology. This paper aims to execute different parameters for variety of <b>inference</b> <b>engines</b> and generate statistics based on suitability of <b>inference</b> <b>engine</b> with respect to domain under consideration. The results may be useful in choosing the <b>inference</b> <b>engine</b> for different version of ontology and domain. Keywords—Inference Engine, OWL-DL, Performance. I...|$|R
40|$|Constraint <b>inference</b> <b>engine</b> as {{the core}} part of {{constraint}} logic programming system comprises variable set, temporary container and constraint filter and <b>inference</b> <b>engine</b> and adopts branching strategy, exploration strategy and node backtracking strategy to complete the inference task. This study introduces variable event to reduce the triggering times of constraint filter and analyzes the trigger probability of <b>inference</b> <b>engine.</b> The experiments show that variable event setting can enhance the efficiency of constraint <b>inference</b> <b>engine...</b>|$|R
5000|$|Cyc <b>inference</b> <b>engine,</b> {{a forward}} and {{backward}} chaining <b>inference</b> <b>engine</b> with numerous specialized modules for high-order logic. (http://research.cyc.com/ ResearchCyc) (http://opencyc.org/ OpenCyc) ...|$|R
50|$|In {{the field}} of Artificial Intelligence, <b>inference</b> <b>engine</b> is a {{component}} of the system that applies logical rules to the knowledge base to deduce new information. The first <b>inference</b> <b>engines</b> were components of expert systems. The typical expert system consisted of a knowledge base and an <b>inference</b> <b>engine.</b> The knowledge base stored facts about the world. The <b>inference</b> <b>engine</b> applies logical rules to the knowledge base and deduced new knowledge. This process would iterate as each new fact in the knowledge base could trigger additional rules in the <b>inference</b> <b>engine.</b> <b>Inference</b> <b>engines</b> work primarily in one of two modes either special rule or facts: forward chaining and backward chaining. Forward chaining starts with the known facts and asserts new facts. Backward chaining starts with goals, and works backward to determine what facts must be asserted so that the goals can be achieved.|$|R
50|$|An <b>inference</b> <b>engine</b> is a {{computer}} program that tries to derive answers from a knowledge base.The Cyc <b>inference</b> <b>engine</b> performs general logical deduction (including modus ponens, modus tollens, universal quantification and existential quantification).|$|R
40|$|The <b>inference</b> <b>engine</b> {{is one of}} main {{components}} of expert system that influences the performance of expert system. The task of <b>inference</b> <b>engine</b> is to give answers and reasons to users by inference the knowledge of expert system. Since the idea of ternary grid issued in 2004, there is only several developed method, technique or engine working on ternary grid knowledge model. The in 2010 developed <b>inference</b> <b>engine</b> is less efficient because it works based on iterative process. The in 2011 developed <b>inference</b> <b>engine</b> works statically and quite expensive to compute. In order to improve the previous inference methods, a new <b>inference</b> <b>engine</b> has been developed. It works based on backward chaining process in ternary grid expert system. This paper describes the development of <b>inference</b> <b>engine</b> of expert system that can work in ternary grid knowledge model. The strategy to inference knowledge uses backward chaining with recursive process. The design result is implemented {{in the form of}} software. The result of experiment shows that the inference process works properly, dynamically and more efficient to compute in comparison to the previous developed methods...|$|R
40|$|A {{software}} architecture that interfaces a concurrent logic programming {{system to a}} Prolog database machine is described. The concurrent logic programming system connects a guarded clause <b>inference</b> <b>engine</b> with a definite clause <b>inference</b> <b>engine</b> to support concurrent execution of mutually invoking guarded and definite clause programs. An interface to a Prolog database machine allows the concurrent logic programming system to invoke its clause retrieval facilities via the definite clause <b>inference</b> <b>engine.</b> The result is a concurrent system able to handle both systems programming tasks on the guarded clause <b>inference</b> <b>engine</b> and exhaustive search on the definite clause <b>inference</b> <b>engine</b> that can support sizable concurrent knowledge based applications in logic. Keywords: stream and-parallelism, don't know non-determinism, knowledge base machine, Parlog, Prolog 1. Introduction Alvey Project IKBS 90 Parlog on Parallel Architectures has been concerned {{with the development of}} the concurrent logic [...] ...|$|R
40|$|Abstract — {{an ability}} of {{reasoning}} capability of <b>inference</b> <b>engine</b> {{is useful to}} derive new & useful information from existing knowledgebase. Classifying algorithms is use to classifying an ontology which improves quality of search on web. Currently <b>Inference</b> <b>engines</b> are able to classify the small ontology completely. For Large and complex version based ontology size is been increase practically. The main aim of paper is necessary to evaluating the performance of <b>inference</b> <b>engine</b> which focus on classification parameter for large and complex version based ontology for different domain. Result might be useful to select <b>inference</b> <b>engine</b> for practically on version based ontology for different domain...|$|R
40|$|An {{expert system}} <b>inference</b> <b>engine</b> is {{described}} {{which is based}} on the utilization of certainty factors, and has a structure similar to Naylor's probabilistic <b>inference</b> <b>engine.</b> After {{a brief description of the}} latter, a modified probabilistic criterion is developed which naturally leads to the certainty factors question selection criterion employed in the <b>inference</b> <b>engine.</b> Also, the case in which there is user reporting bias is considered, and a method for dealing with it is presented. The paper discusses the required aspects of the model of certainty factors, and presents the major implementation issues relating to the new <b>inference</b> <b>engine,</b> which is written in pascal. © 1994...|$|R
40|$|Semantic web is {{a web of}} data, where data {{should be}} related {{to one another and}} also Knowledge will be {{organized}} in conceptual spaces according to its meaning. To understand and use the data and knowledge encoded in semantic web documents requires <b>inference</b> <b>engine.</b> There are number of <b>inference</b> <b>engines</b> used for consistency checking and classification like Pellet, Fact, Fact++, Hermit, Racer Pro, KaON 2, and Base Visor. Some of them are reviewed and tested for few prebuilt ontologies. This paper presents the analysis of different <b>inference</b> <b>engines</b> with set of ontologies. It requires assessment and evaluation before selecting an appropriate <b>inference</b> <b>engine</b> for a given application...|$|R
40|$|This is a {{new feature}} and cleanup release. Features: 	Added GEMPLP for {{approximate}} inference to the structured output framework [Jiaolong Xu]. 	Effeciency improvements of the FITC framework for <b>GP</b> <b>inference</b> (FITC_Laplce, FITC, VarDTC) [Wu Lin]. 	Added optimisation of inducing variables in sparse <b>GP</b> <b>inference</b> [Wu Lin]. 	Added optimisation methods for <b>GP</b> <b>inference</b> (Newton, Cholesky, LBFGS, [...] .) [Wu Lin]. 	Added Automatic Relevance Determination (ARD) kernel functionality for variational <b>GP</b> <b>inference</b> [Wu Lin]. 	Updated Notebook for variational <b>GP</b> <b>inference</b> [Wu Lin]. 	New framework for stochastic optimisation (L 1 / 2 loss, mirror descent, proximal gradients, adagrad, SVRG, RMSProp, adadelta, [...] .) [Wu Lin]. 	New Shogun meta-language for automatically generating code listings in all target languages [Esben Sörig]. 	Added periodic kernel [Esben Sörig]. 	Add gradient output functionality in Neural Nets [Sanuj Sharma]. Bugfixes: 	Fixes for java_modular build using OpenJDK [Björn Esser]. 	Catch uncaught exceptions in Neural Net code [Khaled Nasr]. 	Fix build of modular interfaces with SWIG 3. 0. 5 on MacOSX [Björn Esser]. 	Fix segfaults when calling delete[] twice on SGMatrix-instances [Björn Esser]. 	Fix for building with full-hardening-(CXX|LD) FLAGS [Björn Esser]. 	Patch SWIG to fix a problem with SWIG and Python >= 3. 5 [Björn Esser]. 	Add modshogun. rb: make sure narray is loaded before modshogun. so [Björn Esser]. 	set working-dir properly when running R (# 2654) [Björn Esser]. Cleanup, efficiency updates, and API Changes: 	Added GPU based dot-products to linalg [Rahul De]. 	Added scale methods to linalg [Rahul De]. 	Added element wise products to linalg [Rahul De]. 	Added element-wise unary operators in linalg [Rahul De]. 	Dropped parameter migration framework [Heiko Strathmann]. 	Disabled Python integration tests by default [Sergey Lisitsyn, Heiko Strathmann]...|$|R
40|$|The main {{elements}} of a fuzzy rule based system are a knowledge base and an <b>inference</b> <b>engine.</b> In this paper we present two alternatives for implementing knowledge base in a fuzzy rule based system. The first one implements knowledge base {{as a part of}} an <b>inference</b> <b>engine</b> program written in Java programming languange. The other implements knowledge base as a separate part from an <b>inference</b> <b>engine</b> program. Parser as a separate tool is needed in the later alternative to translate knowledge base in some structured semi-natural language into an intermediate format in a text file to be read efficiently by the <b>inference</b> <b>engine.</b> Comparisons between the two alternatives are discussed and some suggestions for development are given...|$|R
40|$|Abstract — The <b>inference</b> <b>engine</b> {{is one of}} main {{components}} of expert system that influences the performance of expert system. The task of <b>inference</b> <b>engine</b> is to give answers and reasons to users by inference the knowledge of expert system. Since the idea of ternary grid issued in 2004, there is only several developed method, technique or engine working on ternary grid knowledge model. The in 2010 developed <b>inference</b> <b>engine</b> is less efficient because it works based on iterative process. The in 2011 developed <b>inference</b> <b>engine</b> works statically and quite expensive to compute. In order to improve the previous inference methods, a new <b>inference</b> <b>engine</b> has been developed. It works based on backward chaining process in ternary grid expert system. This paper describes the development of <b>inference</b> <b>engine</b> of expert system that can work in ternary grid knowledge model. The strategy to inference knowledge uses backward chaining with recursive process. The design result is implemented {{in the form of}} software. The result of experiment shows that the inference process works properly, dynamically and more efficient to compute in comparison to the previous developed methods. Keywords- expert systems; ternary grid; inference engine; backward chaining. I...|$|R
40|$|We {{present a}} {{preliminary}} design of an experimentation system {{that consists of}} a declarative language and an <b>inference</b> <b>engine.</b> The language allows to formulate a hypothesis about a data population, whereafter the <b>inference</b> <b>engine</b> automatically provides an answer, based on a limited sample. status: publishe...|$|R
40|$|An <b>inference</b> <b>engine</b> for {{a hybrid}} {{representation}} {{scheme based on}} neumles is presented. Neumles {{are a kind of}} hybrid rules that combine a symbolic (production rules) and a connectionist representation (adaline unit). The <b>inference</b> <b>engine</b> uses a connectionist technique, which is based on the 'firing potential', a measurement of the firing tendency of a neumle, and symbolic pattem matching. It is proved to be more efficient and natural than pure connectionist <b>inference</b> <b>engines.</b> Explanation of 'how' type can be provided in the form of if-then symbolic rules...|$|R
5000|$|Perform inferences as {{a forward}} {{chaining}} FOPL <b>inference</b> <b>engine.</b>|$|R
40|$|Ubiquitous {{computing}} services {{started taking}} advantage of the reasoning capabilities of <b>inference</b> <b>engines</b> to acquire hidden and potentially useful contextual information. However, performance evaluations of the <b>inference</b> <b>engines</b> have been limited to the domain of static information reasoning; evaluations of requirements pertaining to ubiquitous computing environment have been largely neglected. This paper aims to examine how different types of <b>inference</b> <b>engines</b> perform by applying them to realistic ubiquitous computing scenarios. Based on the scenarios, three measurement criteria are proposed and measured including scalability as data set gets large, responsiveness for user’s requests, and adaptability to frequent inference requests...|$|R
5000|$|Non-classical logic: Dienes-Rescher <b>inference</b> <b>engine</b> (also Rescher-Dienes implication); Rescher-Manor {{consequence}} relation ...|$|R
40|$|Expert System {{is one of}} many {{application}} {{computer on}} Artificial Intelligence, Expert System has 2 part, there are Knowledge Base and <b>Inference</b> <b>Engine.</b> Knowledge Base is a basic of knowledge in database and <b>Inference</b> <b>Engine</b> is engine to make decision support for find the problem and solution. This system can use for diagnose about health of tooth, so computer as expert systems can help the patients to get the solution...|$|R
40|$|A {{system for}} {{on-board}} anomaly resolution for a vehicle has a data repository. The data repository stores data related to different systems, subsystems, and {{components of the}} vehicle. The data stored is encoded in a tree-based structure. A query engine is coupled to the data repository. The query engine provides a user and automated interface and provides contextual query to the data repository. An <b>inference</b> <b>engine</b> is coupled to the query <b>engine.</b> The <b>inference</b> <b>engine</b> compares current anomaly data to contextual data stored in the data repository using inference rules. The <b>inference</b> <b>engine</b> generates a potential solution to the current anomaly by referencing the data stored in the data repository...|$|R
40|$|Mobile and {{wireless}} devices suffer from technological limitations such as limited battery life and limited memory size. Hence, use of technologies for mobile applications {{is confined to}} those technologies that are faster and take small footprint in memory. Firstly, this paper presents a survey of technologies {{that can be used}} for realization of <b>inference</b> <b>engine,</b> satisfying the qualities mentioned above. Secondly, this paper introduces a Scandinavian invention called Array-Based Logic that enables realization of <b>inference</b> <b>engines</b> for decision making that are compact and fast. Finally, a case study is presented to show how easy it is to use array-based logic for realizing <b>inference</b> <b>engine</b> in mobile applications...|$|R
40|$|Abstract. Existing {{constraint}} programming systems offer a fixed set of <b>inference</b> <b>engines</b> implementing search {{strategies such as}} single, all, {{and best}} solution search. This is unfortunate, since new engines cannot be integrated by the user. The paper presents first-class computation spaces as abstractions with which the user can program <b>inference</b> <b>engines</b> at a high level. Using computation spaces, the paper covers several <b>inference</b> <b>engines</b> ranging from standard search strategies to techniques new to constraint programming, including limited discrepancy search, visual search, and saturation. Saturation is an inference method for tautologychecking used in industrial practice. Computation spaces have shown their practicability in the constraint programming system Oz. ...|$|R
40|$|The {{improving}} {{performance of}} <b>inference</b> <b>engine</b> in expert system {{has become an}} important research in recent years. As it is not realistic to search through all production rule during each cycle using an exhaustive search. Expert systems with a large set of rules can be slow, {{and can not be}} suitable for real-time application. In this paper, new algorithm for forward chaining and backward chaining in <b>inference</b> <b>engine</b> was proposed. This algorithm accommodates balanced binary searched tree and binary tree sort that have good performance in large database. Moreover, this new <b>inference</b> <b>engine</b> is supported by certainty factor as well. Displaying image and other supporting materials as the answer is facilitated...|$|R
30|$|Defuzzification {{converts}} {{the fuzzy}} outputs {{obtained by the}} <b>inference</b> <b>engine</b> into a crisp value.|$|R
50|$|Programming {{languages}} such as Prolog, Knowledge Machine and ECLiPSe support {{backward chaining}} within their <b>inference</b> <b>engines.</b>|$|R
50|$|Reasoning {{can be done}} by {{translating}} graphs into logical formulas, then {{applying a}} logical <b>inference</b> <b>engine.</b>|$|R
50|$|As Expert Systems {{prompted}} significant {{interest from}} the business world various companies, many of them started or guided by prominent AI researchers created productized versions of <b>inference</b> <b>engines.</b> For example, Intellicorp was initially guided by Edward Feigenbaum. These <b>inference</b> <b>engine</b> products were also often developed in Lisp at first. However, demands for more affordable and commercially viable platforms eventually made Personal Computer platforms very popular.|$|R
40|$|Abstract. Programmers {{employing}} inference in Bayesian networks typically rely on {{the inclusion}} of the model as well as an <b>inference</b> <b>engine</b> into their application. Sophisticated <b>inference</b> <b>engines</b> require non-trivial amounts of space and are also difficult to implement. This limits their use in some applications that would otherwise benefit from probabilistic inference. This paper presents a system that minimizes the space requirement of the model. The <b>inference</b> <b>engine</b> is sufficiently simple as to avoid space-limitation and be easily implemented in almost any environment. We show a fast, compact indexing structure that is linear {{in the size of the}} network. The additional space required to compute over the model is linear in the number of variables in the network. ...|$|R
30|$|<b>Inference</b> <b>engine</b> simulates {{the human}} {{reasoning}} process by making fuzzy inference on the inputs and IF-THEN rules.|$|R
5000|$|Robert Kowalski {{developed}} the connection graph theorem-prover and SLD resolution, the <b>inference</b> <b>engine</b> that executes logic programs.|$|R
5000|$|With forward reasoning, the <b>inference</b> <b>engine</b> can derive that Fritz {{is green}} {{in a series}} of steps: ...|$|R
40|$|Abstract: This paper {{concerns}} consideration on {{methods of}} <b>inference</b> <b>engine</b> constructing in diagnostic expert systems. The problem of correct processing of incomplete and unreliable information was under special consideration. For example of solution {{of this problem}} we describe the <b>inference</b> <b>engine</b> of expert system VibroExpert, that is built {{on the base of}} five-layer neural network. This network is compiled basing on rules in the beginning of system working. Note: Publication language:russia...|$|R
