0|16|Public
30|$|In {{order to}} control the finger {{grasping}} accurately, there are two cable driving force sensors, three joint <b>sensors</b> and two <b>grasping</b> force <b>sensors</b> on each knuckle. Besides, two <b>grasping</b> force <b>sensors</b> are installed on base plate. As the damping elements set on the joint-shafts, the finger owns a certain movement sequence of segments called grasping strategy. The spring damping and friction damping are independent.|$|R
40|$|A <b>grasp</b> force <b>sensor</b> for {{robotic hands}} is disclosed. A {{flexible}} block {{is located in}} the base of each claw through which the grasp force is exerted. The block yields minute parallelogram deflection when the claws are subjected to grasping forces. A parallelogram deflection closely resembles pure translational deflection, whereby the claws remain in substantial alignment with each other during grasping. Strain gauge transducers supply signals which provide precise knowledge of and control over grasp forces...|$|R
40|$|We {{present the}} design, {{analysis}} {{and construction of}} a biologicallyinspired tactile sensor. The sensor can measure normal and lateralforces, conform to the surfaces with which it comes in contact andincrease the friction of the surface for a good <b>grasp.</b> The <b>sensor</b> is built using a simple process and the applied forcesare read using standard electronics. These features make thesensors ideal for mass production. We are motivated to build tactile sensors that are useful forrobotic manipulation given that the current ones do not have thefeatures that we consider necessary. The sensors presented in thispaper {{have been designed to}} deal with these issues. They have beendesigned and implemented in the fingers of the humanoid robotObrero...|$|R
3000|$|These pre-experimented {{data are}} {{collected}} in advance: that is, {{picking up a}} folded cloth product is performed with an instructed <b>grasping</b> position, and <b>sensor</b> data therebetween is recorded. In the remaining of this paper, we call one data unit (a pair of [...] P and [...] R) “task experience data,” and call a dataset consisting {{of all of the}} data “task experience dataset,” and a dataset collecting only successful case “successfully experience dataset.” [...]...|$|R
40|$|Presented at the 2012 WSU Academic Research SymposiumWith the {{continued}} success of research into using smart homes for elder care applications, {{there is a}} commiserate drive to build tools {{to assist in the}} deployment of these smart technologies in the real world. This deployment process has numerous open issues, including knowing where to place the sensors for effective monitoring in the home. Until the smart home engineering community has a strong <b>grasp</b> on installing <b>sensors</b> in any home, it will have trouble providing successful commercial offerings based upon smart home technologies...|$|R
40|$|Accurate {{and high}} {{bandwidth}} robot control for grasping and manipulation is greatly facilitated by continuous and high bandwidth sensing {{of the states}} of the object to be grasped. To this end we describe a new sensory system and the associated signal processing, using miniature intensity-based electro-optical proximity sensors, small enough to be directly embedded in a robot's end effector. In situations where the geometry of the object to be <b>grasped</b> is known, <b>sensor</b> fusion via an extended Kalman filter provides an estimate of the object's planar position, velocity and surface reflectance properties. Experimental results show robust estimation of the object's states as well as its surface properties despite noisy sensor data and unmodelled object dynamics...|$|R
40|$|This paper {{describes}} the electro-mechanical and control features of an 8 -DOF manipulator manufactured by AAI Corporation and {{installed at the}} Jet Propulsion Lab. (JPL) in a dual-arm setting. The 8 -DOF arm incorporates a variety of features not found in other lab or industrial manipulators. Some of the unique features are: 8 -DOF revolute configuration with no lateral offsets at joint axes; 1 to 5 payload to weight ratio with 20 kg (44 lb) payload at a 1. 75 m (68. 5 in.) reach; joint position measurement with dual relative encoders and potentiometer; infinite roll of joint 8 with electrical and fiber optic slip rings; internal fiber optic link of 'smart' end effectors; four-axis wrist; graphite epoxy links; high link and joint stiffness; use of an upgraded JPL Universal Motor Controller (UMC) capable of driving up to 16 joints. The 8 -DOF arm {{is equipped with a}} 'smart' end effector which incorporates a 6 -DOF forcemoment sensor at the end effector base and <b>grasp</b> force <b>sensors</b> {{at the base of the}} parallel jaws. The 8 -DOF arm is interfaced to a 6 DOF force reflecting hand controller. The same system is duplicated for and installed at NASA-Langley...|$|R
40|$|The Programming by Demonstration (PbD) {{technique}} aims at {{teaching a}} robot {{to accomplish a}} task by learning from a human demonstration. In a manipulation context, recognizing the demonstrator's hand gestures, specifically when and how objects are grasped, plays a significant role. Here, a system is presented that uses both hand shape and contact point information obtained from a data glove and tactile sensors to recognize continuous human <b>grasp</b> sequences. The <b>sensor</b> fusion, <b>grasp</b> classification and task segmentation are made by a Hidden Markov Model recognizer that distinguishes 14 grasp types, as presented in Kamakura's taxonomy. An accuracy of up to 92. 2 % for a single user system, and 90. 9 % for a multiple user system could be achieved...|$|R
40|$|This book {{provides}} a comprehensive overview of state-of-the-art sensors technology in specific leading areas. Industrial researchers, engineers and professionals can find {{information on the}} most advanced technologies and developments, together with data processing. Further research covers specific devices and technologies that capture and distribute data to be processed by applying dedicated techniques or procedures, which is where sensors play the most important role. The book provides insights and solutions for different problems covering {{a broad spectrum of}} possibilities, thanks to a set of applications and solutions based on sensory technologies. Topics include:• Signal analysis for spectral power• 3 D precise measurements• Electromagnetic propagation• Drugs detection• e-health environments based on social sensor networks• Robots in wireless environments, navigation, teleoperation, object <b>grasping,</b> demining• Wireless <b>sensor</b> networks• Industrial IoT• Insights in smart cities• Voice recognition• FPGA interfaces• Flight mill device for measurements on insects• Optical systems: UV, LEDs, lasers, fiber optics• Machine vision• Power dissipation• Liquid level in fuel tanks• Parabolic solar tracker• Force sensors• Control for a twin roto...|$|R
40|$|A stress-component-selective tactile array, {{based on}} {{piezoelectric}} polymers, {{and its associated}} data acquisition system are described. The seven-taxel sensor array {{is made up of}} elements sensitive to one or more components of the stress field acting upon them. The six independent components of the stress tensor can be calculated from a linear combination of the responses of the six miniaturized elements composing each taxel. The array is developed with the aim of obtaining a skin-like tactile sensor able to perform fine-form discrimination of objects in contact with it. It is also thought to be instrumental in revealing phenomena related to incipient slippage during object manipulation and <b>grasping.</b> The multi-component <b>sensor,</b> based on 42 miniaturized piezoelectric polymer discs, is supported by a polyimide sheet with microlithographically defined electrodes. The sensor elements are embedded into a rubber layer with appropriate thickness in order to sample properly the stress field and to obtain good signal-to-noise ratio at the input of the electronic signal processing unit. Preliminary experiments to test the ability of individual sensor elements to resolve normal and shear stress components have shown good agreement with the responses predicted by analytical solutions of elastic contact problems...|$|R
40|$|Robots can move, see, and {{navigate}} in {{the real}} world outside carefully structured factories, but they cannot yet grasp and manipulate objects without human intervention. Two key barriers are the complexity of current approaches, which require complicated hardware or precise perception to function effectively, and the challenge of understanding system performance in a tractable manner given the wide range of factors that impact successful grasping. This thesis presents sensors and simple control algorithms that relax the requirements on robot hardware, and a framework to understand the capabilities and limitations of <b>grasping</b> systems. The <b>sensors</b> and algorithms build on the recent success of underactuated hands, which use passive mechanics to adapt to object shape and position rather than trying to perceive a precise model of the object and control the grasp to match it. They include piezoelectric contact sensors that expand the range of positioning offsets the hand can tolerate, joint-angle sensors for compliant flexure joints that enable full-finger contact detection and determine object shape, and tactile sensors based on MEMS barometers that enable the hand to more gently adapt to object shape. The framework poses the grasping problem as "overcoming variation. " It is not tractable t...|$|R
40|$|Abstract — In {{this work}} we present {{grasping}} movements recognition in 3 D space. We also present {{the idea of}} a database of different sensors data for different scenarios of grasping and manipulation tasks for our future works. Multi-sensor information for <b>grasp</b> tasks require <b>sensors</b> calibration and synchronized data with timestamp that we start to develop to share with the researches of this area. In the scenario presented in this work we are performing the grasp recognition integrating 2 different types of features from the reach-to-grasp movement. Observing the reach-to-grasp movements of different subjects we perform a learning phase based on histogram using the segmentation data. Based on a learning phase is possible to recognize the grasping movements applying Bayes rule by continuous classification based on multiplicative updates of beliefs. We developed an automated system to estimate and recognize two possible types of grasping (e. g. Side-Grasp and Top-Grasp) by the hand movements performed by humans that are tracked by a magnetic tracking device [9]. These reported steps are important to understand some human behaviors before the object manipulation and can be used to endow a robot with autonomous capabilities, like showing how to reach some object for manipulation or object displacement. G I...|$|R
40|$|As {{mobile and}} {{tangible}} devices are getting {{smaller and smaller}} it is desirable to extend the interaction area to their whole surface area. The HandSense prototype employs capacitive sensors for detecting when it is touched or held against a body part. HandSense is also able to detect in which hand the device is held, and how. The general properties of our approach were confirmed by a user study. HandSense was able to correctly classify over 80 percent of all touches, discriminating six different ways of touching the device (hold left/right, pick up left/right, pick up at top/bottom). This information {{can be used to}} implement or enhance implicit and explicit interaction with mobile phones and other tangible user interfaces. For example, graphical user interfaces can be adjusted to the user’s handedness. obvious parameter is the pressure with which the object is held. The contact areas at which an object is touched and the amount of fingers used for grasping an object can also provide information. Another hint is whether the object is held between fingers or in the palm. Finally, the information which hand is holding the object- left or right- can be useful for explicit or implicit interaction. All of these parameters can be utilized in the design of touch-based user interfaces. Author Keywords touch, <b>grasp,</b> capacitive sensing, <b>sensors,</b> input devices, handednes...|$|R
40|$|Proceedings of: 14 th International Conference on Advanced Robotics (ICAR 2009), 22 - 26 June 2009, Munich (Germany) In {{this work}} we present {{grasping}} movements recognition in 3 D space. We also present {{the idea of}} a database of different sensors data for different scenarios of grasping and handling tasks for our future works. Multi-sensor information for <b>grasp</b> tasks require <b>sensors</b> calibration and synchronized data with timestamp that we start to develop to share with the researches of this area. In the scenario presented in this work we are performing the grasp recognition combining 2 different types of features from the reach-to-grasp movement. Observing the reach-to-grasp movements of different subjects we perform a learning phase based on histogram using the segmentation data. Based on a learning phase is possible to recognize the grasping movements applying Bayes rule by continuous classification based on multiplicative updates of beliefs. We developed an automated system to estimate and recognize two possible types of grasping by the hand movements performed by humans that are tracked by a magnetic tracking device [9]. These reported steps are important to understand some human behaviors before the object manipulation and can be used to endow a robot with autonomous capabilities, like showing how to reach some object for manipulation or object displacement. European Community's Seventh Framework ProgramThis work is partially supported by the European project Handle ICT- 23 - 16 - 40...|$|R
40|$|There is an {{emerging}} {{need to apply}} adaptive robotic hands to substitute humans in dangerous, laborious, or monotonous work. The state-of-the-art robotic hands cannot fulfill this need, because they are expensive, hard to control and they consist of many vulnerable motors and sensors. It is aimed to develop simple, adaptive hands {{that are capable of}} grasping and holding a large variety of objects. To achieve these properties, the concept of underactuation (i. e. having fewer actuators than independently moving fingers) is applied. First new metrics are defined which quantify the range of object sizes that underactuated hands can grasp and hold. Furthermore, a new method is developed to dimension the main design parameters of underactuated hands, such that the fingers can envelope and stably grasp the required range of objects. The new performance metrics and design method are applied to the design and evaluation of a new robotic hand that consists of a minimum number of motors (i. e. one) and sensors (i. e. zero). The innovation of this hand is that it mechanically decides whether to hold an object in a precision grasp or a power <b>grasp</b> configuration. No <b>sensors,</b> auxiliary actuation mechanisms, motors or control are needed to convert between these two distinct grasp configurations. It is concluded that the principle of underactuation and the proposed design method are effective to achieve self-adaptive, robust and cheap hands that are capable to grasp and hold a large range of different objects. BioMechanical EngineeringMechanical, Maritime and Materials Engineerin...|$|R
40|$|Thesis (Ph. D.) [...] University of Washington, 2014 Robotic {{grasping}} {{of unknown}} objects is recognized to be a challenging problem; {{this is due}} to the uncertainty of object shape, caused by the imperfect perception capability of the robot. Vision and depth sensors are commonly used to sense objects before <b>grasping.</b> These <b>sensors</b> suffer from shortcomings such as occlusion, inaccurate sensor readings, and failures due to reflection and transparency. Grasping solely relying on an incomplete object shape can fail regardless of the grasp planning. On the other hand, tactile exploration is widely used to acquire local geometric information of the object and does not subject to occlusion. However, since touch sensing relies on physical contact between the manipulator and the object, it tends to unintentionally displace objects, particularly light objects. This dissertation considers "pretouch", a sensing modality that is intermediate in range between long-range depth and tactile sensing. This is potentially beneficial for robotic grasping as it provides reliable geometric information in the last centimeter before contact. In this dissertation, a novel pretouch technique, "seashell effect pretouch", is first presented. It is effective for a set of materials that other pretouch techniques fail to sense. This pretouch modality is inspired by the phenomenon of "hearing the sea" when a seashell is held to the ear; in particular, the observation that the "sound of the sea" changes as the distance from the seashell to the ear varies. It is because environmental noise is amplified the most (attenuated the least) at the cavity's resonant frequency, which changes as the cavity approaches an object. In order to turn the familiar seashell effect into a pretouch sensor, I study the underlying acoustic principle, i. e., the acoustic radiation impedence changes caused by the object being close to the opening of the cavity. The sensor design, including the acoustic properties, hardware/software design, and signal processing, are discussed in detail. The resulting implementation is fully integrated into the finger of a Willow Garage PR 2 robot. The sensor detects resonance frequency shifts in the spectrum of ambient sound, which occur when the finger approaches an object. The performance of the proposed sensor is characterized and evaluated, in terms of the sensing range, accuracy, and its material selectivity. This results in the ability to reliably detect the presence of the object within 5 mm. In addition, a new infrared optical pretouch sensor can be developed, with minimal modification on the proposed sensor system design. The first explored application is detecting extremely compliant objects during grasp execution. In a pre-grasp execution experiment, the ability to detect compliant objects of the seashell effect pretouch sensor is compared with that of a pressure sensor. The results suggest advantages of seashell pretouch over tactile sensing. The second application is pretouch-assisted grasp planning. When the pretouch sensor senses the object during a series of probing motions, it provides points collected by recording the position of end effector on the robot; these additional points augment the point cloud from the depth sensor. This method compensates for object shape, that is otherwise incomplete due to depth sensor failure or occlusion. Furthermore, a unified probabilistic framework is proposed to (1) identify shape uncertainty for the target object; (2) automatically explore the uncertain areas to reduce the uncertainty, resulting in a grasp with higher confidence. In the beginning, the robot is provided with only the incomplete object shape data acquired from a Kinect depth sensor [...] -it does not have a model of the object. Next, combining the Kinect point cloud with prior probability distributions for occlusion and transparency, it makes inferences about unobserved portions of the object. Operating on the inferred shape of the object, an iterative grasp replanning algorithm decides whether further exploration is required, and where to explore in the scene. The information gathered by the exploration action is added directly to the environment representation in real-time and hence considered in the next grasp planning iteration. Experimental results showed that, the robot is able to grasp partially transparent objects with a high success rate of 96 %. Finally, I propose to augment streaming point cloud data with the seashell effect pretouch information. This is inspired by the use case of haptic rendering in a telerobotic grasping scenario. The non-contact seashell-effect pretouch sensor fixed to the robot end effector is used to sense physical geometries within the vicinity of the sensor. Thus, the point cloud representation of an unknown environment, which may be sparse or poorly visible, is enhanced through telerobotic exploration/sensing in real-time. Furthurmore, real-time haptic rendering algorithms are applied on the augmented point clouds to create haptic virtual fixtures, and also provide haptic force feedback to the operator. This method provides the teleoperator with critical geometrical information about the grasp target, while preventing the robot end effector from collision. The augmented virual environment after the pretouch exploration represents more complete object shapes, which helps the operator align the gripper on the slave robot with the target object for grasping...|$|R

