69|1113|Public
2500|$|A {{fuzzy set}} is defined for the input <b>error</b> <b>variable</b> [...] "e", and the derived change in error, [...] "delta", {{as well as}} the [...] "output", as follows: ...|$|E
2500|$|Given a {{data set}} [...] of n {{statistical}} units, a linear regression model {{assumes that the}} relationship between the dependent variable yi and the p-vector of regressors xi is linear. This relationship is modeled through a disturbance term or <b>error</b> <b>variable</b> εi — an unobserved random variable that adds noise to the linear relationship between the dependent variable and regressors. Thus the model takes the form ...|$|E
50|$|Error {{handling}} in {{many other}} languages is done {{through the use of}} exceptions. TScript uses a similar process of error handling, although slightly different. TScript has a global <b>error</b> <b>variable</b> similar to the traditional errno in C, although the <b>error</b> <b>variable</b> in TScript is capable to holding both an error code and a detailed error message.|$|E
40|$|The general {{linear model}} with {{correlated}} <b>error</b> <b>variables</b> {{can be transformed}} {{by means of the}} generalized singular value decomposition to a very simple model (canonical form) where the least squares solution is obvious. The method works also if X and the covariance matrix of the <b>error</b> <b>variables</b> do not have full rank or are nearly rank deficient (rank-k approximation). By backtransformation one obtains the solution for the original model. ...|$|R
40|$|The strong {{convergence}} rates in nonparametric regression estimation have been mostly discussed when the <b>error</b> <b>variables</b> {{in the regression}} models have finite variances. A few recent studies concern heavy-tailed error distributions for two comparable methods using the kernel and the k-nearest neighbor estimators. The obtained convergence rates are however noncomparable. Assuming the <b>error</b> <b>variables</b> have finite pth moments for the same p, 1 Strong convergence rates Kernel regression Nearest neighbor regression Random and constant regressors...|$|R
50|$|Note {{that this}} model allows for {{arbitrary}} correlation between the <b>error</b> <b>variables,</b> so that it doesn't necessarily respect independence of irrelevant alternatives.|$|R
5000|$|A {{fuzzy set}} is defined for the input <b>error</b> <b>variable</b> [...] "e", and the derived change in error, [...] "delta", {{as well as}} the [...] "output", as follows: ...|$|E
5000|$|... where [...] is a {{disturbance}} term or <b>error</b> <b>variable</b> — an unobserved random variable that adds noise to the linear {{relationship between the}} dependent variable and predictor function.|$|E
5000|$|... i.e. {{the latent}} {{variable}} {{can be written}} directly {{in terms of the}} linear predictor function and an additive random <b>error</b> <b>variable</b> that is distributed according to a standard logistic distribution.|$|E
3000|$|... where, {{recalling that}} the MUSIC {{estimator}} is asymptotically unbiased and Gaussian distributed (see section 3.2), <b>error</b> <b>variables</b> wi,j are modeled as zero-mean Gaussian random variables: [...]...|$|R
40|$|AbstractIn the literature, {{there are}} {{basically}} {{two kinds of}} resampling methods for least squares estimation in linear models; the E-type (the efficient ones like the classical bootstrap), which is more efficient when <b>error</b> <b>variables</b> are homogeneous, and the R-type (the robust ones like the jackknife), which is more robust for heterogeneous errors. However, for M-estimation of a linear model, we find a counterexample showing that a usually E-type method is less efficient than an R-type method when <b>error</b> <b>variables</b> are homogeneous. In this paper, we give sufficient conditions under which the classification of {{the two types of}} the resampling methods is still true...|$|R
5000|$|It {{turns out}} that this model is {{equivalent}} to the previous model, although this seems non-obvious, since there are now two sets of regression coefficients and <b>error</b> <b>variables,</b> and the <b>error</b> <b>variables</b> have a different distribution. In fact, this model reduces directly to the previous one with the following substitutions:An intuition for this comes from the fact that, since we choose based on the maximum of two values, only their difference matters, not the exact values — and this effectively removes one degree of freedom. Another critical fact is that the difference of two type-1 extreme-value-distributed variables is a logistic distribution, i.e. [...] We can demonstrate the equivalent as follows: ...|$|R
5000|$|Binomial {{regression}} models are {{essentially the same}} as binary choice models, one type of discrete choice model. The primary difference is in the theoretical motivation: Discrete choice models are motivated using utility theory so as to handle various types of correlated and uncorrelated choices, while binomial {{regression models}} are generally described in terms of the generalized linear model, an attempt to generalize various types of linear regression models. As a result, discrete choice models are usually described primarily with a latent variable indicating the [...] "utility" [...] of making a choice, and with randomness introduced through an <b>error</b> <b>variable</b> distributed according to a specific probability distribution. Note that the latent variable itself is not observed, only the actual choice, which is assumed to have been made if the net utility was greater than 0. Binary regression models, however, dispense with both the latent and <b>error</b> <b>variable</b> and assume that the choice itself is a random variable, with a link function that transforms the expected value of the choice variable into a value that is then predicted by the linear predictor. It can be shown that the two are equivalent, at least in the case of binary choice models: the link function corresponds to the quantile function of the distribution of the <b>error</b> <b>variable,</b> and the inverse link function to the cumulative distribution function (CDF) of the <b>error</b> <b>variable.</b> The latent variable has an equivalent if one imagines generating a uniformly distributed number between 0 and 1, subtracting from it the mean (in the form of the linear predictor transformed by the inverse link function), and inverting the sign. One then has a number whose probability of being greater than 0 {{is the same as the}} probability of success in the choice variable, and can be thought of as a latent variable indicating whether a 0 or 1 was chosen.|$|E
50|$|GLM's {{can easily}} handle {{arbitrarily}} distributed response variables (dependent variables), not just categorical variables or ordinal variables, which discrete choice models {{are limited to}} by their nature. GLM's are also not limited to link functions that are quantile functions of some distribution, unlike {{the use of an}} <b>error</b> <b>variable,</b> which must by assumption have a probability distribution.|$|E
5000|$|Given a {{data set}} [...] of n {{statistical}} units, a linear regression model {{assumes that the}} relationship between the dependent variable yi and the p-vector of regressors xi is linear. This relationship is modeled through a disturbance term or <b>error</b> <b>variable</b> εi — an unobserved random variable that adds noise to the linear relationship between the dependent variable and regressors. Thus the model takes the form ...|$|E
40|$|In the literature, {{there are}} {{basically}} {{two kinds of}} resampling methods for least squares estimation in linear models; the E-type (the efficient ones like the classical bootstrap), which is more efficient when <b>error</b> <b>variables</b> are homogeneous, and the R-type (the robust ones like the jackknife), which is more robust for heterogeneous errors. However, for M-estimation of a linear model, we find a counterexample showing that a usually E-type method is less efficient than an R-type method when <b>error</b> <b>variables</b> are homogeneous. In this paper, we give sufficient conditions under which the classification of {{the two types of}} the resampling methods is still true. bootstrap jackknife M-estimator resampling method variance estimations E-type R-type...|$|R
40|$|<b>Errors</b> in <b>variables</b> {{models in}} linear {{regression}} are {{an old and}} important theoretical topic, but rather neglected in applied statistics. This {{is especially true for}} Bayesian statistics where the numerical difficulties for the estimation of the models are even greater. Using a Bayesian approach and modern numerical integration techniques (the Gibbs sampler) we derive the necessary full conditional distributions for the simulation procedure to obtain the complete posterior distribution. The multivariate <b>errors</b> in <b>variable</b> model is considered with a correlation structure between dependent and independent variables. Furthermore, we show how this GEIV approach can be extended to a censored regression (the Tobit GEIV) model. Keywords: Bayesian (censored) regression, <b>errors</b> in <b>variables,</b> Gibbs sampling, Tobit models 1. Introduction <b>Errors</b> in <b>variables</b> (abbreviated: eiv) models are an important class of regression models which have been neglected in practical applications because of large co [...] ...|$|R
40|$|We {{provide an}} example for an <b>errors</b> in <b>variables</b> problem which might be often {{neglected}} but which is quite common in lab experimental practice: In one task, attitude towards risk is measured, in another task participants behave {{in a way that}} can possibly be explained by their risk attitude. How should we deal with inconsistent behaviour in the risk task? Ignoring these observations entails two biases: An <b>errors</b> in <b>variables</b> bias and a selection bias. We argue that inconsistent observations should be exploited to address the <b>errors</b> in <b>variables</b> problem, which can easily be done within a Bayesian framework...|$|R
5000|$|The {{previous}} paragraph can {{be generalized}} to any variance: given a variable (such as an unbiased <b>error</b> <b>variable)</b> , evaluating the error function at [...] describes the probability of &epsilon; falling in the range x. This is used in statistics to predict behavior of any sample {{with respect to the}} population mean. This usage is similar to the Q-function, which in fact can be written in terms of the error function.|$|E
5000|$|This formulation—which is {{standard}} in discrete choice models—makes clear {{the relationship between}} logistic regression (the [...] "logit model") and the probit model, which uses an <b>error</b> <b>variable</b> distributed according to a standard normal distribution instead of a standard logistic distribution. Both the logistic and normal distributions are symmetric with a basic unimodal, [...] "bell curve" [...] shape. The {{only difference is that}} the logistic distribution has somewhat heavier tails, which means that it is less sensitive to outlying data (and hence somewhat more robust to model mis-specifications or erroneous data).|$|E
5000|$|In statistics, it {{is common}} to have a {{variable}} [...] and its unbiased estimator [...] The error is then defined as [...] This makes the error a normally distributed random variable with mean 0 (because the estimator is unbiased) and some variance this is written as [...] For the case where , i.e. an unbiased <b>error</b> <b>variable</b> , erf(x) describes the probability of the error &epsilon; falling in the range x; in other words, the probability that the absolute error is no greater than x. This is true for any random variable with distribution but the application to error variables is how the error function got its name.|$|E
5000|$|The Gauss-Markov {{assumptions}} {{concern the}} set of <b>error</b> random <b>variables,</b> : ...|$|R
40|$|AbstractIn {{this paper}} a {{procedure}} of construction of β-expectation tolerance {{regions in the}} framework of the structural method of inference has been developed. The procedure has been applied to the generalized multivariate model and the β-expectation tolerance region for this case has been constructed assuming the normal distribution for the <b>error</b> <b>variables</b> of the model...|$|R
30|$|When {{applying}} a size of bin equal to 0.5 on OSEMbreast images {{to meet the}} quantification scale of PSFbreast, the OOB estimates of classification error decreased, equal to 21.4 %, but were still higher than PSFbreast OOB estimates of classification <b>error.</b> <b>Variables</b> of importance and their correlations are displayed on Additional file 1 : Figure S 1 b.|$|R
5000|$|The second {{parameter}} in an extreme-value or logistic {{distribution is}} a scale parameter, such that if [...] then [...] This {{means that the}} effect of using an <b>error</b> <b>variable</b> with an arbitrary scale parameter in place of scale 1 can be compensated simply by multiplying all regression vectors by the same scale. Together with the previous point, this shows that the use of a standard extreme-value distribution (location 0, scale 1) for the error variables entails no loss of generality over using an arbitrary extreme-value distribution. In fact, the model is nonidentifiable (no single set of optimal coefficients) if the more general distribution is used.|$|E
50|$|In real systems, {{there are}} {{practical}} {{limits to the}} range of the manipulated variable (MV). For example, a heater can be off or fully on, or a valve can be closed or fully open. Adjustments to the gain simultaneously alter the range of error values over which the MV is between these limits. The width of this range, in units of the <b>error</b> <b>variable</b> and therefore of the PV, is called the proportional band (PB). While the gain is useful in mathematical treatments, the proportional band is often used in practical situations. They both refer to the same thing, but the PB has an inverse relationship to gain - higher gains result in narrower PBs, and vice versa.|$|E
50|$|The {{choice of}} {{modeling}} the <b>error</b> <b>variable</b> specifically {{with a standard}} logistic distribution, rather than a general logistic distribution with the location and scale set to arbitrary values, seems restrictive, {{but in fact it}} is not. It must be kept in mind that we can choose the regression coefficients ourselves, and very often can use them to offset changes in the parameters of the error variable's distribution. For example, a logistic error-variable distribution with a non-zero location parameter μ (which sets the mean) is equivalent to a distribution with a zero location parameter, where μ has been added to the intercept coefficient. Both situations produce the same value for Yi* regardless of settings of explanatory variables. Similarly, an arbitrary scale parameter s is equivalent to setting the scale parameter to 1 and then dividing all regression coefficients by s. In the latter case, the resulting value of Yi* will be smaller by a factor of s than in the former case, for all sets of explanatory variables — but critically, it will always remain {{on the same side of}} 0, and hence lead to the same Yi choice.|$|E
40|$|We {{consider}} {{a class of}} generalized M(GM) -estimators for the autoregressive parameter in a linear Markov model with <b>errors</b> in <b>variables.</b> We show, under some minimal regularity assumptions, that these estimators have almost sure representations of the Bahadur-Kiefer type and consequently they are consistent and asymptotically normal. <b>Errors</b> in <b>variables</b> Linear Markov scheme Almost sure convergence Bahadur-Kiefer type...|$|R
40|$|In {{this paper}} an {{arbitrarily}} high speed adaptive lattice algorithm for multichannel Least Squares FIR filtering and multivariable system identification, is presented. The design procedure {{consists of two}} steps. First, a channel decomposition technique is applied and the multichannel algorithm is decomposed into multiple single channel stages. Then, look-ahead techniques are applied to expand the feedback loops inherent in the adaptive lattice recursions. Look-ahead with pipeline interleaving as well as look-ahead with vectorization are used to increase the system's overall throughput rate. 1 Introduction Adaptive lattice algorithms update the so called error parameters, that is, the difference between system's output and a desired response signal, for all intermediate filter orders [1]-[2]. The number of <b>error</b> <b>variables</b> used {{as well as the}} operations needed for their time update, depends linearly on the dimension of system's parameters. The <b>error</b> <b>variables</b> are utilized for the computa [...] ...|$|R
40|$|The fitting of a {{straight}} line to bivariate data (x,y) is a common procedure. Standard linear regression theory deals with the situation when there is only <b>error</b> in one <b>variable,</b> either x, or y. A procedure known as y on x regression fits a line where the error {{is assumed to be}} associated with the y variable, alternatively, x on y regression fits a line when the error is associated with the x variable. The model to describe the scenario when there are <b>errors</b> in both <b>variables</b> is known as an <b>errors</b> in <b>variables</b> model. <b>Errors</b> in <b>variables</b> modelling is fundamentally different from standard regression techniques. The problems of model fitting and parameter estimation of {{a straight}} line <b>errors</b> in <b>variables</b> model cannot be solved by generalising a simple linear regression model. Briefly, this thesis provides a unified framework to the fitting of {{a straight line}} <b>errors</b> in <b>variables</b> model using the method of moments. Estimators of the line using a higher moments approach have been detailed, and asymptotic variance covariance matrices of a plethora of slope estimators are provided. Simulations demonstrate that these variance covariance matrices are accurate for even small data sets. The topic of prediction is considered, with an estimator for the latent variable presented, as well as advice on the mean value of y given x via both a parametric and non-parametric approach. The problem of residuals in an <b>errors</b> in <b>variables</b> model is described, and some quick solutions given. Some examples are presented towards the end of this thesis to demonstrate how the ideas provided may be applied to real-life data sets, as well as some areas which may demand further research...|$|R
30|$|The border {{variable}} was left off as zero in every case, as the nearest international border to Cincinnati is about 5  h drive by automobile. The <b>error</b> <b>variable</b> {{only makes sense}} when trying to calibrate existing systems to each other.|$|E
3000|$|... where ñ^j {{is called}} the soft <b>error</b> <b>variable</b> and the {{constant}} η {{is called the}} soft scalar. We compute the value of η which minimizes the mean-square value of the soft error, i.e., η =E(x_Rx̃_R) (c.f. [5]). In general, the soft scalar may be computed offline as η = 1 /N∑ _i= 1 ^N[x_R,ix̃_R,i] for any desired source-relay SNR. We identified some properties of the parameter η as follows: [...]...|$|E
40|$|Proposes a {{possible}} strategy for disturbance attenuation and set-point regulation for a rigid robot using {{the notion of}} L 2 -gain of a system. The authors discuss the use of H∞ control for trajectory tracking and show, for the particular case of rigid robots, that a PD type controller is sufficient to render the closed loop system from the exogenous input (disturbances and references) to the <b>error</b> <b>variable</b> dissipative {{with respect to the}} supply rate associated with the notion of L 2 -gai...|$|E
30|$|The {{model fit}} in lavaan yields {{estimates}} for B, A, and the covariance matrices of the <b>error</b> <b>variables</b> ε and ζ. Each of these parameter estimates {{is provided with}} robust p values (for the hypothesis of being equal to zero), when using the MLM estimation procedure [23]. Using the estimated model parameters, one can then calculate unbiased Bartlett scores for the latent variables [24].|$|R
40|$|This paper {{addresses}} {{the problem of}} formation control of groups of unicycle robots with possibly time-varying formation shapes. To solve the problem, we propose two simple distributed formation control algorithms based on the virtual structure approach. We prove exponential convergence of <b>error</b> <b>variables</b> to the origin and illustrate {{the behavior of a}} group of robots under the formation control algorithm in simulations and experiment...|$|R
40|$|We {{consider}} {{the problem of}} estimating quantile regression coefficients in errorsin -variables models. When the <b>error</b> <b>variables</b> for both the response and the manifest variables have a joint distribution that is spherically symmetric but otherwise unknown, the regression quantile estimates based on orthogonal residuals are shown to be consistent and asymptotically normal. We also extend the work to partially linear models when the response is related to some additional covariate...|$|R
