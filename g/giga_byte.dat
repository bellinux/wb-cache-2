5|19|Public
50|$|<b>Giga</b> <b>Byte</b> (1994): 14 {{winds and}} piano obbligato. Rio de Janeiro, 1994.|$|E
40|$|Blu-ray Disc {{format is}} {{required}} by the forthcoming of High Definition TV era which calls for a brand new generation of optical storage after DVD. It is such a technology with wavelength short as 405 nm and numerical aperture high as 0. 85. The format is designed to have an even wider disc tolerances than those of DVD. Thanks for those more innovative concepts Blu-ray Disc is the most economical storage solution in terms of cost per <b>giga</b> <b>byte.</b> And {{it is expected to}} be a long-term optical storage standard rather than an interim solution...|$|E
40|$|In {{this paper}} {{we present a}} novel {{approach}} to reduce information proliferation and aid better information structure by automatically generating extraction of abbreviation for emergency management websites. 5. 7 <b>Giga</b> <b>Byte</b> web data from 624 emergency management related web sites is collected {{and a list of}} acronyms is automatically generated by proposed system (AbbrevExtractor). Being the first attempt of applying abbreviation extraction to the field, this work is expected to provide comprehensive and timely information for emergency management communities in emergency preparedness, training and education. Future work is likely to involve more data collection and intelligent text analysis for dynamically maintaining and updating the list of acronyms and abbreviations...|$|E
50|$|Notes: Her henchwomen are <b>Giga</b> and <b>Byte.</b>|$|R
5000|$|Telecommunications is an {{environment}} where there are many heterogeneous systems creating a multitude of data. Typically a single day’s information runs into many <b>Giga</b> <b>Bytes</b> {{or in some cases}} Tera Bytes also. This creates a need to have very proficient and rigorous extraction, transformation and loading tool for a Revenue Assurance application. Some of the common features of this tool are: ...|$|R
40|$|When backup {{operators}} have {{to handle}} <b>giga</b> <b>bytes</b> of information daily, they usually find the task {{hard enough to}} leave security aside. This paper consequently intends to focus on proving documents' authenticity regarding both content and date. In the context of WORM technology, it proposes a media independent tamper evident system - using original cryptographic hash function techniques - and secure time stamping within a given accuracy. Finally, time stamping upgrade mechanisms are proposed to extend security guarantees over years...|$|R
40|$|Business Data always {{growth from}} kilo byte, mega byte, <b>giga</b> <b>byte,</b> tera byte, peta byte, and so far. There {{is no way}} to avoid this {{increasing}} rate of data till business still running. Because of this issue, database tuning be critical part of a information system. Tuning a database in a cost-effective manner is a growing challenge. The total cost of ownership (TCO) of information technology needs to be significantly reduced by minimizing people costs. In fact, mistakes in operations and administration of information systems are the single most reasons for system outage and unacceptable performance [3]. One way of addressing the challenge of total cost of ownership is by making information systems more self-managing. A particularly difficult piece of the ambitious vision of making database systems self-managing is the automation of database performance tuning. In this paper, we will explain the progress made thus far on this important problem. Specifically, we will propose the architecture and Algorithm for this problem...|$|E
40|$|High Energy Physics {{experiments}} {{are known for}} their production of large amounts of data. Even small projects may have to manage several <b>Giga</b> <b>Byte</b> of event information. One possible solution for the management of this data is to use today`s technology to archive the raw data files in tertiary storage and build on-line catalogs which reference interesting data. This approach has been taken by the Gammas, Electrons and Muons (GEM) Collaboration for their evaluation of muon chamber technologies at the Superconducting Super Collider Laboratory (SSCL). Several technologies were installed and tested during a 6 month period. Events produced were first recorded in the UNIX filesystem of the data acquisition system and then migrated to the Physics Detector Simulation Facility (PDSF) for long term storage. The software system makes use of a commercial relational database management system (SYBASE) and the Data Management System (DMS), a tape archival system developed at the SSCL. The components are distributed among several machines inside and outside PDSF. A Motif-based graphical user interface (GUI) enables physicists to retrieve interesting runs from the archive using the on-line database catalog...|$|E
30|$|Amazon use {{an example}} to show that if You upload one 500 MB file each day {{during the month of}} March and You {{download}} one 500 MB file each day during the month of March your bill for March (imagine 2011) will be calculated as follows. The DataTransfer–In would be 500 M B[*]×[*](1 / 1024)[*]×[*] 31 [*]=[*] 15.14 G B. At a price of 10 cents per <b>Giga</b> <b>Bytes,</b> the total charge would be 15.14 [*]×[*] 10 [*]=[*] 151.4 cents. In a second example they show that if You download one 500 MB file each day during the month of March the total amount of DataTransfer–Out would be 15.14 GB which charged at 15 cents per GB would amount to 227 cents.|$|R
40|$|To {{meet the}} needs of {{research}} in the robustness of the continuous Chinese speech recognition systems, we have established a Cantonese accent Chinese speech corpus (CACSC) as {{the first of a series}} of Chinese speech corpora with different accents. CACSC contains 25 <b>Giga</b> <b>Bytes</b> utterances uttered by 104 males and 100 females. The sampling was undertaken at 16 KHz rate with 16 bit-width data precision through a standard SoundBlaster of a personal computer under ordinary office environment. CACSC is mainly based on the standard Chinese, known as Mandarin, with light Cantonese accents. The establishment of CACSC offers a testing bed for robust speech recognition of a certain regional accent. This paper is to describe how CACSC was established and its features in details...|$|R
30|$|In {{contrast}} to above aggregated data publishing, some cities tend to share more detailed data with frequent updates (i.e. velocity) on daily basis. Such detailed data is large in size {{as well as}} in dimensions (i.e. variety). It can be used to get more detailed insights to derive predictive analysis. For instance, San Francisco’s open data portalh has such open data sets with records in millions and data size reaching from few hundred Mega bytes to 10 s of <b>Giga</b> <b>bytes.</b> For example, the crime incidents dataset alone consists of 800, 000 + records starting from early 2003 and is updated daily. The size of the dataset is already 300 + MB as of the revising of this paper. The approaches discussed in this paper can be applied to such larger data by grouping yearly datasets.|$|R
30|$|Amazon {{explains}} that the GB of storage billed in a month is the average storage used throughout the month. This includes all object data and metadata stored in buckets that you created under your account. We measure your usage in TimedStorage–ByteHrs, which are added {{up at the end}} of the month to generate your monthly charges. Next, an example that illustrates how to calculate your bill if you keep 2, 684, 354, 560 bytes (or 2.5 GB) of data in your bucket for the entire month of March is provided. In accordance with Amazon the total number of bytes consumed for each day of March is 2684354560; thus the total number of ByteHrs is calculated as 2684354560 [*]×[*] 31 [*]×[*] 24 [*]=[*] 1997159792640, which is equivalent to 2.5 GBMonths. At a price of 15 cents per <b>Giga</b> <b>Bytes</b> per month, the total charge amounts to 2.5 [*]×[*] 15 [*]=[*] 37.5 cents.|$|R
40|$|The {{proposed}} scheme of pyramid broadcasting {{is a new}} way of rendering Video On Demand service at metropolitan scale. In pyramid broadcasting, the most frequently requested movies are multiplexed on the broadcast network, resulting in radical improvement of access time and efficient bandwidth utilization. We provide analytical and experimental evaluations of pyramid broadcasting based on its implementation on ethernet LAN, illustrating its advantages. 1 Introduction Consider a future Video On Demand (VOD) service where movies are provided to subscribers over a high speed fiber-optic network. Advances in networking technologies will contribute to the realization of the VOD service over the Metropolitan Area Network [11] and [13]. Video objects are very large even in compressed form. One motion picture with NTSC quality video which is 100 minutes long, in uncompressed form occupies 40 GB (<b>Giga</b> <b>Bytes)</b> of storage and 1 GB when compressed according to MPEG standard [5]. These video objects [...] ...|$|R
40|$|Test {{equipments}} have {{range from}} manual test equipments to fully automatic test equipments (ATE). These test equipments generally test different IC’s in a circuit. ATE is interfaced {{with a personal}} computer (PC) through a port. Test patterns of different IC’s are usually generated & stored on workstations. The increased variety of Application Specific IC’s (ASICs) require more frequent download of test data sets from work station to ATE. The size of test sets for ASICs are often a large as several <b>giga</b> <b>bytes,</b> the time spent to down load test data therefore is significant. An approach to reduce the down load time is to compress data before the download. Since ATE should have accurate data and also, ATEs are slow, the compression algorithm should be non lossy and decompression should be time efficient. To achieve this, a conventional method, Burrows and Wheeler (BW) Transformation along with Run Length Coding (RLC) has been in use. Our proposal is a modification in conventional method, which offers reduced decompression time & higher compression ratio. Thus improving the performance and reducing the download time...|$|R
30|$|The figure {{shows the}} {{execution}} time of four PUT and one DEL operations executed by an S 3 consumer {{during the last}} two days of March. The first day of April is also shown for completeness. For simplicity, the figure assumes that the earliest PUT operation is the very first executed by the consumer after opening his S 3 account. The figure also shows the specific points in time when checkpoints are conducted independently by two parties, namely, Amazon and a consumer. Thus, CP and cp represent, respectively, Amazon’s and the consumer’s checkpoints; the <b>Giga</b> <b>Bytes</b> shown next to CP and cp indicate the storage consumption detected by the checkpoint. For example, on the 30 th, Amazon conducted its checkpoint about five in the morning and detected that, at that time, the customer had 6 GB stored (C P 30 [*]:[*] 6 G B). On the same day, the consumer conducted his checkpoint just after midday and detected that, at that time, he had 6 GB stored (C P 30 [*]:[*] 6 G B). SC and sc represent, respectively, the storage consumption for the month of March, calculated by Amazon and consumer, based on their checkpoints.|$|R
40|$|Motivated by the {{pervasive}} discrepancy among the pricing schemes of data services, this paper investigates {{the selection of}} pricing metrics (variables) and the corresponding pricing plans. We construct a stylized model in which a monopoly data services seller faces heterogeneous consumers whose utilities depend on the usage and the connection speed. We examine three options for the seller to conduct the second-degree (indirect) price discrimination: by minutes, by <b>giga</b> <b>bytes</b> (Gigs), and by mega bytes per second (Mbps). We show that the after-sales self-selection behaviors {{have a significant impact}} on the seller's profitability, and it leads to a first-order influence on the pricing metric selection. We prove that either pricing by Gigs or pricing by Mbps can be optimal. Pricing by Gigs can dominate the pricing by Mbps even if the consumer's utility is more sensitive in changes in the connection speed. We also find that when incorporating the bandwidth costs or congestion costs, pricing by Mbps becomes more attractive as it allows the seller to directly control the congestion effect. These findings may help practitioners to develop their own pricing plans and pricing metrics selection...|$|R
40|$|The {{extraction}} of Chunk candidates from real corpora {{is one of}} the fundamental tasks of building example-based machine translation model. This paper presents a statistical approach to extract Chinese chunk candidates from large monolingual corpora. The first step is to extract large N-grams (up to 20 -gram) from raw corpus. Then two newly proposed Fast Statistical Substring Reduction (FSSR) algorithms {{can be applied to the}} initial N-gram set to remove some unnecessary N-grams using their frequency information. The two algorithms are efficient (both have a time complexity of O(n)) and can effectively reduce the size of N-gram set up to 50 %. Finally, mutual information is used to obtain chunk candidates from reduced N-gram set. Perhaps the biggest contribution of this paper is that it is the first time to apply Fast Statistical Substring Reduction algorithm to large corpora and demonstrate the effectiveness and efficiency of this algorithm which, in our hope, will shed new light on large scale corpus oriented research. Experiments on three corpora with different sizes show that this method can extract chunk candidates from corpora of <b>giga</b> <b>bytes</b> efficiently under current computational power. We get an extraction accuracy of 86. 3 % from People Daily 2000 news corpus...|$|R
40|$|Includes bibliographical {{references}} (pages 55 - 56) In {{a modern}} era, {{most of the}} consumer and industrial electronic devices use High Speed data transfer. High speed data transfer can be achieved with different protocols like I 2 C, SPI, AGP, PCI, and PCIE. A 32 x 32 photo detector data acquisition system is a group project which needs high speed data transfer at approximate rate of 2. 5 <b>Giga</b> <b>Bytes</b> per Second (GBPS). This project {{is part of this}} group project, which includes the research work of selecting right protocol, hardware and software requirement for such high speed data transfer. Followed by, detailed configuration and implementation of Xilinx???s Virtex 6 FPGA integrated block for PCI Express v 1. 3 with x 8 Gen 1. Some testing and verification on the PCIE core using [...] Xilinx???s chip scope pro, ISE simulation, DMA performance demo application xapp 1052 and a third party tool PCITREE are also performed. The core is generated and simulated in Verilog. The PCIE core performance achieved is 2. 3 GTPS for read and 2. 8 GTPS for write using DMA performance demo xapp 1052. This module and the instantiated core can be used at five different levels for data transfer at rate of 2. 5 GTPS for the main project after making EZDMA module and main memory mapping...|$|R
40|$|Deep neural {{networks}} are state-of-the-art models {{for understanding the}} content of images, video and raw input data. However, implementing a deep neural network in embedded systems is a challenging task, because a typical deep neural network, such as a Deep Belief Network using 128 x 128 images as input, could exhaust <b>Giga</b> <b>bytes</b> of memory and result in bandwidth and computing bottleneck. To address this challenge, this paper presents a hardware-oriented deep learning algorithm, named as the Deep Adaptive Network, which attempts to exploit the sparsity in the neural connections. The proposed method adaptively reduces the weights associated with negligible features to zero, leading to sparse feedforward network architecture. Furthermore, since the small proportion of important weights are significantly larger than zero, they can be robustly thresholded and represented using single-bit integers (- 1 and + 1), leading to implementations of deep {{neural networks}} with sparse and binary connections. Our experiments showed that, {{for the application of}} recognizing MNIST handwritten digits, the features extracted by a two-layer Deep Adaptive Network with about 25 % reserved important connections achieved 97. 2 % classification accuracy, which was almost the same with the standard Deep Belief Network (97. 3 %). Furthermore, for efficient hardware implementations, the sparse-and-binary-weighted deep neural network could save about 99. 3 % memory and 99. 9 % computation units without significant loss of classification accuracy for pattern recognition applications. Comment: 10 pages, extended and submitted to IEEE Transactions of Systems, Man, and Cybernetic...|$|R
40|$|Figure 1 : Three {{images of}} the Lawrence-Livermore National Laboratory (LLNL) {{simulation}} of a Richtmyer-Meshkov instability (time step 270; iso-value 16) at different level-of-detail (LOD) levels. While data is loaded asynchronously in the background {{it is possible to}} fully interact with the scene. A kD-tree based LOD structure is used to bridge loading times while allowing interactive ray tracing of up to four frames per second on a custom PC. The visualization of iso-surfaces from gridded volume data is an important tool in many scientific applications. Today, it is possible to ray trace high-quality iso-surfaces at interactive frame rates even on commodity PCs. However, current algorithms fail if the data set exceeds a certain size either because they are not designed for out-of-core data sets or the loading times are too high because there is too much overhead involved in the out-of-core (OOC) techniques. We propose a kD-tree based OOC data structure that allows to ray trace iso-surfaces of large volumetric data sets of many <b>giga</b> <b>bytes</b> at interactive frame rates on a single PC. A LOD technique is used to bridge loading times of data that is fetched asynchronously in the background. Using this framework we are able to ray trace iso-surfaces between 2 and 4 fps on a single dual-core Opteron PC at 640 × 480 resolution and an in-core memory footprint that is {{only a fraction of the}} entire data size...|$|R
40|$|In many Multimedia content {{analytics}} frameworks feature likelihood maps {{represented as}} histograms {{play a critical}} role in the overall algorithm. Integral histograms provide an efficient computational framework for extracting multi-scale histogram-based regional descriptors in constant time which are considered as the principle building blocks of many video content analytics frameworks. We evaluate four different mappings of the integral histogram computation onto Graphics Processing Units (GPUs) using different kernel optimization strategies. Our kernels perform cumulative sums on row and column histograms in a cross-weave or wavefront scan order, use different data organization and scheduling methods that is shown to critically affect utilization of GPU resources (cores and shared memory). Tiling the 3 -D array into smaller regular data blocks significantly speeds up the efficiency of the computation compared to a strip-based organization. The tiled integral histogram using a diagonal wavefront scan has the best performance of about 300. 4 frames/sec for 640 x 480 images and 32 bins with a speedup factor of about 120 using GTX Titan X graphics card compared to a single threaded sequential CPU implementation. Double-buffering has been exploited to overlap computation and communication across sequence of images. Mapping integral histogram bins computations onto multiple GPUs enables us to process 32 <b>giga</b> <b>bytes</b> integral histogram data (of 64 MB Image and 128 bins) with a frame rate of 0. 73 Hz and speedup factor of 153 X over single-threaded CPU implementation and the speedup of 45 X over 16 -threaded CPU implementation. Comment: GPU Implementation of Integral Histogra...|$|R
30|$|In {{order to}} {{validate}} the approach, two tests have been performed. The objective of the first test is to evaluate {{the performance of the}} proposed tracking approach in terms of quality of solutions. against the participants of the ETISEO project [47] for video analysis performance evaluation benchmarking. The obtained results have been compared with algorithms developed by 15 anonymous participants in the ETISEO project, considering four benchmark videos publicly available, which are part of the evaluation framework. The objective of the second test is to evaluate the performance of the proposed tracking approach in terms of quality of solutions and time performance, suppressing different features of the proposed approach, against a known tracker implementation. For this purpose, the performance of the proposed approach has been compared with the OpenCV frame-to-frame tracker [42] implementation, presented in Section 2. The same videos of the ETISEO project have been used for this test, considering versions with only 2 D features and suppressing the reliability effect, {{in order to understand the}} contribution of each feature of the approach. Also a single-object video has been tested to give a closer insight of the effects of multi-target to object associations (poorly segmented objects). The tests were performed with a computer with processor Intel Xeon CPU 3.00 GHz, with 2 <b>Giga</b> <b>Bytes</b> of memory. For obtaining the 3 D model information, two parallelepiped models have been pre-defined for person and vehicle classes. The precision on 3 D parallelepiped height values to search the classification solutions has been fixed in 0.08 (m), while the precision on orientation angle has been fixed in π/ 40 (rad).|$|R
40|$|Abstract To {{clarify the}} issues {{associated}} with the applications of virtual microscopy to the daily cytology slide screening, we conducted a survey at a slide conference of cytology. The survey was conducted specifically to the Japanese cytology technologists who use microscopes on a routine basis. Virtual slides (VS) were prepared from cytology slides using NanoZoomer (Hamamatsu Photonics, Japan), which is capable of adjusting focus on {{any part of the}} slide. A total of ten layers were scanned from the same slides, with 2 micrometer intervals. To simulate the cytology slide screening, no marker points were created. The total data volume of six slides was approximately 25 <b>Giga</b> <b>Bytes.</b> The slides were stored on the Windows 2003 Server, and were made accessible on the web to the cytology technologists. Most cytotechnologists answered "Satisfied" or "Acceptable" to the VS resolution and drawing speed, and "Dissatisfied" to the operation speed. To the ten layered focus, an answer "insufficient" was slightly more frequent than the answer "sufficient", while no one answered "fewer is acceptable" or "no need for depth". As for the use of cytology slide screening, answers "usable, but requires effort" and "not usable" were about equal in number. In a Japanese cytology meeting, a unique VS system has been used in slide conferences with markings to the discussion point for years. Therefore, Japanese cytotechnologists are relatively well accustomed to the use of VS, and the survey results showed that they regarded VS more positively than we expected. Currently, VS has the acceptable resolution and drawing speed even on the web. Most cytotechnologists regard the focusing capability crucial for cytology slide screening, but the consequential enlargement of data size, longer scanning time, and slower drawing speed are the issues that are yet to be resolved. </p...|$|R

