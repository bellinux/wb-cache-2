1105|298|Public
25|$|The usage-based {{model is}} based on <b>inductive</b> <b>learning,</b> meaning that {{linguistic}} knowledge is acquired in a bottom-up manner through use. It allows for redundancy and generalizations, because the language user generalizes over recurring experiences of use.|$|E
25|$|<b>Inductive</b> <b>learning</b> {{problem can}} occur as words are oftentimes used in {{ambiguous}} {{situations in which}} there are more than one possible referents available. This can lead to confusion for the infants as {{they may not be able}} to distinguish which words should be extended to label objects being referenced to. Smith and Yu proposed that a way to make a distinction in such ambiguous situations is to track the word-referent pairings over multiple scenes. For instance, an infant who hears a word in the presence of object A and object B will be unsure of whether the word is the referent of object A or object B. However, if the infant then hears the label again in the presence of object B and object C, the infant can conclude that object B is the referent of the label because object B consistently pairs with the label across different situations.|$|E
2500|$|... <b>inductive</b> <b>learning</b> called {{supervised}} learning. [...] In supervised ...|$|E
40|$|<b>Inductive</b> concept <b>{{learning}}</b> is {{the task}} of learning to assign cases to a discrete set of classes. In real-world applications of concept learning, {{there are many different}} types of cost involved. The majority of the machine learning literature ignores all types of cost (unless accuracy is interpreted as a type of cost measure). A few papers have investigated the cost of misclassification errors. Very few papers have examined the many other types of cost. In this paper, we attempt to create a taxonomy of the different types of cost that are involved in <b>inductive</b> concept <b>learning.</b> This taxonomy may help to organize the literature on cost-sensitive learning. We hope that it will inspire researchers to investigate all types of cost in <b>inductive</b> concept <b>learning</b> in more depth. 1. Introduction This paper is an attempt to list the different costs that may be involved in <b>inductive</b> concept <b>learning.</b> The paper assumes the standard <b>inductive</b> concept <b>learning</b> scenario. We have a se [...] ...|$|R
40|$|The paper {{reviews the}} recent {{developments}} of incorporating prior domain knowledge into <b>inductive</b> machine <b>learning,</b> and proposes a guideline that incorporates prior domain knowledge in three key issues of <b>inductive</b> machine <b>learning</b> algorithms: consistency, generalization and convergence. With respect to each issue, this paper gives some approaches to improve {{the performance of the}} <b>inductive</b> machine <b>learning</b> algorithms and discusses the risks of incorporating domain knowledge. As a case study, a hierarchical modelling method, VQSVM, is proposed and tested over some imbalanced data sets with various imbalance ratios and various numbers of subclasses. 1...|$|R
40|$|International audienceAn {{investigation}} of rule learning processes {{that allow the}} inclusion of negated features is described. The objective is to establish whether the use of negation in <b>inductive</b> rule <b>learning</b> systems is effective with respect to classification. This paper seeks {{to answer this question}} by considering two issues relevant to such systems; feature identification and rule refinement. Both synthetic and real datasets are used to illustrate solutions to the identified issues and to demonstrate that the use of negative features in <b>inductive</b> rule <b>learning</b> systems is indeed beneficial...|$|R
2500|$|It is {{a common}} myth to credit Plan-Do-Check-Act (PDCA) to Deming. Deming {{referred}} to the PDCA cycle as a [...] "corruption." [...] Deming worked from the Shewhart cycle and over time eventually developed the Plan-Do-Study-Act (PDSA) cycle, which has the idea of deductive and <b>inductive</b> <b>learning</b> built into the learning and improvement cycle. [...] Deming finally published the PDSA cycle in 1993, in The New Economics on p.132. Deming has added to the myth that he taught the Japanese the PDSA cycle with this quote on p.247, [...] "The PDSA Cycle originated in my teaching in Japan in 1950. It appeared in the booklet Elementary Principles of the Statistical Control of Quality (JUSE, 1950: out of print).|$|E
5000|$|Semi-supervised {{learning}} may {{refer to}} either transductive learning or <b>inductive</b> <b>learning.</b> The goal of transductive learning is to infer the correct labels for the given unlabeled data [...] only. The goal of <b>inductive</b> <b>learning</b> is to infer the correct mapping from [...] to [...]|$|E
50|$|For formulating a {{groundbreaking}} new Bayesian model of human <b>inductive</b> <b>learning</b> and for using this model to generate innovative empirical studies of human perception, language, and reasoning.|$|E
40|$|Pre-Pruning and Post-Pruning are two {{standard}} {{methods of}} dealing with noise in decision tree learning. Pre-Pruning methods deal with noise during learning, while post-pruning methods try {{to address this problem}} after an overfitting theory has been learned. This paper shows how pre- and post-pruning algorithms can be used for separate-and-conquer rule learning algorithms. We discuss some fundamental problems and show how to solve them with two new algorithms that combine and integrate pre- and post-pruning. Keywords: Pruning, Noise Handling, <b>Inductive</b> Rule <b>Learning,</b> <b>Inductive</b> Logic Programmin...|$|R
40|$|Knowledge Acquisition is an {{important}} task when developing image interpretation systems. Whereas in the past this task {{has been done by}} interviewing an expert, the current trend is to collect large data bases of images associated with expert description (known as picture archiving systems). This makes it possible to use <b>inductive</b> machine <b>learning</b> techniques for knowledge acquisition of image interpretation systems. We use decision tree induction in order to learn the symbolic knowledge for image interpretation. We applied the method to interpretation of x-ray images for lung cancer diagnosis. In the paper, we present our methodology for applying <b>inductive</b> machine <b>learning.</b> We discuss our results and compare it to other knowledge acquisition methods. ...|$|R
40|$|This paper {{reports a}} study to {{identify}} static software reuse potential metrics {{that can be used}} to classify C source code into reusable and non-reusable classes. The techniques used exploit a decision tree <b>inductive</b> machine <b>learning</b> and rough sets theory. The results we obtained show that the former technique, as implemented by C 4. 5, produces a much more accurate set of classification rules than the latter technique, as implemented by DataLogic/R. The C 4. 5 rules are also plausible as they support current understanding of how software metrics can be used to measure software reuse potental. keywords: <b>inductive</b> concept <b>learning,</b> machine learning, rough sets, software reuse track: intelligent system technologies (machine learning) 1...|$|R
50|$|An {{admissible}} heuristic can {{be derived}} from a relaxedversion of the problem, or by information from pattern databases that store exact solutions to subproblems of the problem, or by using <b>inductive</b> <b>learning</b> methods.|$|E
50|$|The usage-based {{model is}} based on <b>inductive</b> <b>learning,</b> meaning that {{linguistic}} knowledge is acquired in a bottom-up manner through use. It allows for redundancy and generalizations, because the language user generalizes over recurring experiences of use.|$|E
5000|$|Inductive {{reasoning}} aptitude (also called differentiation or <b>inductive</b> <b>learning</b> ability) measures {{how well}} a person can identify a pattern within {{a large amount of}} data. It involves applying the rules of logic when inferring general principles from a constellation of particulars.|$|E
40|$|The Internet {{presents}} numerous {{sources of}} useful information [...] -telephone directories, product catalogs, stock quotes, weather forecasts, etc. Recently, many {{systems have been}} built that automatically gather and manipulate such information on a user's behalf. However, these resources are usually formatted for use by people (e. g., the relevant content is embedded in HTML pages), so extracting their content is difficult. Wrappers are often used for this purpose. A wrapper is a procedure for extracting a particular resource's content. Unfortunately, hand-coding wrappers is tedious. We introduce wrapper induction, a technique for automatically constructing wrappers. Our techniques {{can be described in}} terms of three main contributions. First, we pose the problem of wrapper construction as one of <b>inductive</b> <b>learn</b> [...] ...|$|R
40|$|<b>Inductive</b> {{transfer}} <b>learning</b> {{has attracted}} increasing attention {{for the training}} of effective model in the target domain by leveraging {{the information in the}} source domain. However, most transfer learning methods are developed for a specific model, such as the commonly used support vector machine, which makes the methods applicable only to the adopted models. In this regard, the generalized hidden-mapping ridge regression (GHRR) method is introduced in order to train various types of classical intelligence models, including neural networks, fuzzy logical systems and kernel methods. Furthermore, the knowledge-leverage based transfer learning mechanism is integrated with GHRR to realize the <b>inductive</b> transfer <b>learning</b> method called transfer GHRR (TGHRR). Since the information from the induced knowledge is much clearer and more concise than that from the data in the source domain, it is more convenient to control and balance the similarity and difference of data distributions between the source and target domains. The proposed GHRR and TGHRR algorithms have been evaluated experimentally by performing regression and classification on synthetic and real world datasets. The results demonstrate that the performance of TGHRR is competitive with or even superior to existing state-of-the-art <b>inductive</b> transfer <b>learning</b> algorithms. Department of Applied Social Science...|$|R
40|$|Person re-identification {{concerns}} {{about the problem of}} recognizing people across space (captured by different cam-eras) and/or over time gaps. Though recently the literature on it grows rapidly, all the proposed solutions have treated it as a normal classification or ranking problem. In this paper, however, we argue that it is in fact a natural transfer learning problem, thus it’s valuable and also necessary to investigate how the progress on transfer learning could benefit the re-search on it. We present so far the first study on justifying the effectiveness of a representative transfer <b>learning</b> method-ology: feature-based <b>inductive</b> transfer <b>learning,</b> for person re-identification. Extensive experiments on standard datasets with typical methods result in several important findings. Index Terms — Person re-identification, transfer learn-ing, <b>inductive</b> transfer <b>learning,</b> feature mappin...|$|R
50|$|While {{the spacing}} effect refers to {{improved}} recall for spaced versus successive (mass) repetition, the term 'lag' {{can be interpreted}} as the time interval between repetitions of learning. The lag effect is simply an idea branching off the spacing effect that states recall after long lags between learning is better versus short lags. Michael Kahana's study showed strong evidence that the lag effect is present when recalling word lists. In 2008, Kornell and Bjork published a study that suggested <b>inductive</b> <b>learning</b> is more effective when spaced than massed. <b>Inductive</b> <b>learning</b> is learning through observation of exemplars, so the participants did not actively take notes or solve problems. These results were replicated and backed up by a second independent study.|$|E
50|$|Attributional {{calculus}} is a {{logic and}} representation system defined by Ryszard S. Michalski. It combines elements of predicate logic, propositional calculus, and multi-valued logic. Attributional calculus provides a formal language for natural induction, an <b>inductive</b> <b>learning</b> process whose {{results are in}} forms natural to people.|$|E
5000|$|The transductive {{learning}} framework was formally introduced by Vladimir Vapnik in the 1970s. Interest in <b>inductive</b> <b>learning</b> using generative models also {{began in the}} 1970s. A probably approximately correct learning bound for semi-supervised learning of a Gaussian mixture was demonstrated by Ratsaby and Venkatesh in 1995.|$|E
40|$|Abstract—Inductive {{transfer}} {{learning and}} semi-supervised learning {{are two different}} branches of machine learning. The former tries to reuse knowledge in labeled out-of-domain instances while the later attempts to exploit the usefulness of unlabeled in-domain instances. In this paper, we bridge the two branches {{by pointing out that}} many semi-supervised learning methods can be extended for <b>inductive</b> transfer <b>learning,</b> if the step of labeling an unlabeled instance is replaced by re-weighting a diff-distribution instance. Based on this recognition, we develop a new transfer learning method, namely COITL, by extending the co-training method in semi-supervised learning. Experimental results reveal that COITL can achieve significantly higher generalization and robustness, compared with two state-of-the-art methods in <b>inductive</b> transfer <b>learning.</b> Keywords-Inductive transfer learning; semi-supervised learning; co-training I...|$|R
40|$|Summary. This chapter {{provides}} a short overview of a GA-based system for <b>inductive</b> concept <b>learning</b> (in {{a fragment of}} first-order logic). The described system exploits problem–specific knowledge by means of ad-hoc selection, mutation operators and optimization applied to the single individuals. We focus on the experimental analysis of selection operators incorporating problem knowledge. ...|$|R
5000|$|Alexey Grigoryevich (Olexiy Hryhorovych) Ivakhnenko (Олексíй Григо́рович Іва́хненко; 30 March 1913 - 16 October 2007) was a Soviet and Ukrainian {{mathematician}} {{most famous}} {{for developing the}} Group Method of Data Handling (GMDH), a method of <b>inductive</b> statistical <b>learning,</b> {{for which he is}} sometimes referred to as the [...] "father of Deep Learning".|$|R
50|$|Rules Extraction System (RULES) is {{one family}} of <b>inductive</b> <b>learning</b> that include several {{covering}} algorithms. This family {{is used to}} build a predictive model based on given observation. It works based {{on the concept of}} separate-and-conquer to directly induce rules from a given training set and build its knowledge repository.|$|E
50|$|<b>Inductive</b> <b>learning</b> {{had been}} {{divided into two}} types: Decision Tree (DT) and Covering Algorithms (CA). DTs {{discover}} rules using decision tree based {{on the concept of}} divide-and-conquer, while CA directly induces rules from the training set based on the concept of separate and conquers. Although DT algorithms was well recognized in the past few decades, CA started to attract the attention due to its direct rule induction property, as emphasized by Kurgan et al. 1. Under this type of <b>inductive</b> <b>learning</b> approach, several families have been developed and improved. RULES family 2, known as RULe Extraction System, is one family of covering algorithms that separate each instance or example when inducing the best rules. In this family, the resulting rules are stored in an ‘IF condition THEN conclusion’ structure. It has its own induction procedure that is used to induce the best rules and build the knowledge repository.|$|E
5000|$|John B. Carroll, an {{influential}} psychologist {{in the field}} of educational linguistics, developed a theory about a cluster of four abilities that factored into language learning aptitude, separate from verbal intelligence and motivation. Using these four distinct abilities (phonetic coding ability, grammatical sensitivity, rote learning ability, and <b>inductive</b> <b>learning</b> ability), Carroll developed the MLAT, a language aptitude assessment for adults.|$|E
40|$|We {{review the}} notion of {{polynomial}} learnability (Valiant, 1984) for unrestricted DNF and present an alternative notion of the problem-specific incremental learnability (Oblow, 1992) of unrestricted DNF. We then present an incremental, polynomial-time algorithm for inducing disjunctive normal form representations, for propositional concepts, from positive and negative examples described in an attribute-based formalism, and show that this satisfies learnability criteria for distribution-specific problems as examples are sampled. Having established a theoretical basis for learnability we then describe a learning system that uses this algorithm to learn DNF from noisy, and inconsistent data, which may be {{described in terms of}} both Boolean and symbolic attributes. We empirically compare our system with a number of well known inductive classifier systems on some benchmark problems. 1 Contents 1 Introduction 3 2 Research Goals 3 3 The Problem 3 3. 1 Attribute-based <b>Inductive</b> <b>Learn</b> [...] ...|$|R
40|$|This paper {{proposes a}} unifying {{framework}} for <b>inductive</b> rule <b>learning</b> algorithms. We {{suggest that the}} problem of constructing an appropriate inductive hypothesis (set of rules) can be broken down in the following subtasks: rule construction, body construction, and feature construction. Each of these subtasks may have its own declarative bias, search strategies, and heuristics. In particular, we argue that feature construction is a crucial notion in explaining the relations between attribute-value rule <b>learning</b> and <b>inductive</b> logic programming (ILP). We demonstrate this by a general method for transforming ILP problems to attributevalue form, which overcomes some of the traditional limitations of propositionalisation approaches. 1 Introduction Rule learning tasks are typically approached as search problems, and hence the construction of candidate hypotheses is a crucial task in <b>inductive</b> rule <b>learning</b> algorithms. Traditionally, hypotheses are constructed by constructing rules [...] ...|$|R
40|$|Abstract. This paper {{describes}} {{experiments on}} using <b>inductive</b> machine <b>learning</b> to guide a deterinistic dependency parser for unrestricted natural language text. Using {{data from a}} small treebank of Swedish, an eager probabilistic learning algorithm is used to induce context-sensitive parse tables. Evaluation shows a significant improvement over the baseline, which uses a table without contextual information. ...|$|R
5000|$|Anomaly {{detection}} {{was proposed}} for intrusion detection systems (IDS) by Dorothy Denning in 1986. Anomaly detection for IDS is normally accomplished with thresholds and statistics, {{but can also}} be done with soft computing, and <b>inductive</b> <b>learning.</b> Types of statistics proposed by 1999 included profiles of users, workstations, networks, remote hosts, groups of users, and programs based on frequencies, means, variances, covariances, and standard deviations. [...] The counterpart of anomaly detection in intrusion detection is misuse detection.|$|E
5000|$|In 1990, the Time-based Inductive Machine (TIM) did anomaly {{detection}} using <b>inductive</b> <b>learning</b> of sequential user patterns in Common Lisp on a VAX 3500 computer. [...] The Network Security Monitor (NSM) performed masking on access matrices for {{anomaly detection}} on a Sun-3/50 workstation. [...] The Information Security Officer's Assistant (ISOA) was a 1990 prototype that considered {{a variety of}} strategies including statistics, a profile checker, and an expert system. [...] ComputerWatch at AT&T Bell Labs used statistics and rules for audit data reduction and intrusion detection.|$|E
50|$|RULES family {{algorithms}} {{are mainly}} used in data mining {{to create a}} model that predicts {{the actions of a}} given input features. It goes under the umbrella of <b>inductive</b> <b>learning,</b> which is a machine learning approach. In this type of learning, the agent is usually provided with previous information to gain descriptive knowledge based on the given historical data. Thus, it is a supervised learning paradigm that works as a data analysis tool, which uses the knowledge gained through training to reach a general conclusion and identify new objects using the produced classifier.|$|E
40|$|The aim of <b>inductive</b> machine <b>learning</b> (ML) is to {{generate}} models {{that can make}} predictions from analysis of data sets. These data sets consist {{of a number of}} instances or examples, each example described by a set of attributes. It is known that the quality or relevance of the attributes of a data set is a key issu...|$|R
40|$|In {{this paper}} {{we argue that}} the use of a {{language}} with a type system, together with higher-order facilities and functions, provides a suitable basis for knowledge representation in <b>inductive</b> concept <b>learning</b> and, in particular, illuminates the relationship between attribute-value <b>learning</b> and <b>inductive</b> logic programming (ILP). Individuals are represented by closed terms: tuples of constants in the case of attribute-value learning; arbitrarily complex terms in the case of ILP. To illustrate the point, we take some learning tasks from the machine learning and ILP literature and represent them in Escher, a typed, higher-order, functional logic programming language being developed at the University of Bristol. We argue {{that the use of}} a type system provides better ways to discard meaningless hypotheses on syntactic grounds and encompasses many ad hoc approaches to declarative bias. 1. Motivation and scope <b>Inductive</b> concept <b>learning</b> consists of finding mappings of individuals (or objects [...] ...|$|R
40|$|This study proposes on how <b>inductive</b> {{discovery}} <b>learning</b> supported {{with technology}} used in learning environment can facilitate English grammar teaching to undergraduate education. It is hypothesized {{that students can}} learn grammar concepts better when they are engaged in the induction process with instructional technology {{in the form of}} e learning and the internet, including observing some instances of a concept, searching and testing the pattern behind those instances, and generalizing their findings with proper written words. With supports of the technology, students can devote their thinking efforts in such an individual learning task and discover on their own. To investigate the effect of <b>inductive</b> discovery <b>learning</b> with the use of technology, three third- grade classes were involved in the experiment. The result suggests that students have better concept retention, especially for the high and medium performance students through the <b>learning</b> material of <b>inductive</b> discovery...|$|R
