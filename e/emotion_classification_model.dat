2|6965|Public
30|$|We also {{extracted}} PSD data {{to characterize}} EEG signals {{in the frequency}} domain, {{which has become a}} common practice in the estimation of emotional states [3]. We used the same PSD ranges as those used in the previous section as features for <b>emotion</b> <b>classification</b> <b>model</b> training.|$|E
40|$|There {{have been}} some studies about spoken natural {{language}} dialog, {{and most of them}} have successfully been developed within the specified domains. However, current human-computer interfaces only get the data to process their programs. Aiming at developing an affective dialog system, we have been exploring how to incorporate emotional aspects of dialog into existing dialog processing techniques. As a preliminary step toward this goal, we work on making a Chinese <b>emotion</b> <b>classification</b> <b>model</b> which is used to recognize the main affective attribute from a sentence or a text. Finally we have done experiments to evaluate our model. Key words: Emotion thesaurus, Emotion Classification, Image Value...|$|E
40|$|We present EmoTxt, a toolkit for emotion {{recognition}} from text, trained and tested on a gold standard of about 9 K question, answers, and comments from online interactions. We provide empirical {{evidence of the}} performance of EmoTxt. To the best of our knowledge, EmoTxt is the first open-source toolkit supporting both emotion {{recognition from}} text and training of custom <b>emotion</b> <b>classification</b> <b>models...</b>|$|R
40|$|We present EmoTxt, a toolkit for emotion {{recognition}} from text, trained and tested on a gold standard of about 9 K question, answers, and comments from online interactions. We provide empirical {{evidence of the}} performance of EmoTxt. To the best of our knowledge, EmoTxt is the first open-source toolkit supporting both emotion {{recognition from}} text and training of custom <b>emotion</b> <b>classification</b> <b>models.</b> Comment: In Proc. 7 th Affective Computing and Intelligent Interaction (ACII' 17), San Antonio, TX, USA, Oct. 23 - 26, 2017, p. 79 - 80, ISBN: 978 - 1 - 5386 - 0563 -...|$|R
40|$|We present MoodSwipe, a soft {{keyboard}} {{that suggests}} text messages given the user-specified emotions utilizing the real dialog data. The aim of MoodSwipe {{is to create}} a convenient user interface to enjoy the technology of <b>emotion</b> <b>classification</b> and text suggestion, {{and at the same time}} to collect labeled data automatically for developing more advanced technologies. While users select the MoodSwipe keyboard, they can type as usual but sense the emotion conveyed by their text and receive suggestions for their message as a benefit. In MoodSwipe, the detected emotions serve as the medium for suggested texts, where viewing the latter is the incentive to correcting the former. We conduct several experiments to show the superiority of the <b>emotion</b> <b>classification</b> <b>models</b> trained on the dialog data, and further to verify good emotion cues are important context for text suggestion. Comment: 6 pages (including references), EMNLP 2017 Demo pape...|$|R
30|$|This project aims at {{the current}} Tweets {{sentiment}} analysis technology such as traditional attitude classification for positive-negative and median-type, multivariate <b>emotion</b> <b>model</b> <b>classification.</b> At the same time, we alter the machine learning method in the <b>emotion</b> <b>classification</b> and deep learning method in entity recognition.|$|R
40|$|We {{introduce}} a novel emotion recognition approach which integrates ranking models. The approach is speaker independent, {{yet it is}} designed to exploit information from utterances from the same speaker in the test set before making predictions. It achieves much higher precision in identifying emotional utterances than a conventional SVM classifier. Furthermore we test several possibilities for combining conventional classification and predictions based on ranking. All combinations improve overall prediction accuracy. All experiments are performed on the FAU AIBO database which contains realistic spontaneous emotional speech. Our best combination system achieves 6. 6 % absolute improvement over the Interspeech 2009 emotion challenge baseline system on the 5 -class classification tasks. Index Terms: <b>emotion</b> <b>classification,</b> ranking <b>models,</b> spontaneous speec...|$|R
40|$|The {{amygdala}} is {{the neural}} structure {{that acts as}} an evaluator of potentially threatening stimuli. We present a biologically plausible model of the visual fear conditioning pathways leading to the amygdala, using a topographic conditioning map (TCM). To evaluate the model, we first use abstract stimuli to understand its ability to form topographic representations, and subsequently to condition on arbitrary stimuli. We then present results on facial emotion recognition using the sub-cortical pathway of the model. Compared to other <b>emotion</b> <b>classification</b> approaches, our <b>model</b> performs well, but {{does not have the}} need to pre-specify features. This generic ability to organise visual stimuli is enhanced through conditioning, which also improves classification performance. Our approach demonstrates that a biologically motivated model can be applied to real-world tasks, while allowing us to explore biological hypotheses...|$|R
30|$|Furthermore, {{extending}} a previously developed AER system, which {{made use of}} linear approximations to pitch contours as features, {{to take into account}} linear approximations to contours of other parameters of the source-filter model as well, led to a relative increase in classification accuracy of 9.6 % on the Emotional Prosody Speech and Transcripts corpus. Comparisons based on experiments using the LDC corpus revealed that an automatic <b>emotion</b> <b>classification</b> system that <b>modelled</b> contours outperformed one that modelled statistical distributions by 6.1 % (relative) with regards to classification accuracy, and the classification accuracy of this contour-based system was comparable with human classification accuracy. The paper also includes the classification accuracies obtained when tested on the German FAU Aibo Emotion Corpus on the five-class <b>emotion</b> <b>classification</b> problem originally outlined in the INTERSPEECH 2009 Emotion Challenge. Work is currently underway on collecting an Australian speech corpus with emotional speech, {{and the use of the}} proposed contour-based features will be validated on those data in the future. Further, the choice of using linear approximations to model temporal information within voiced speech segments, while the simplest, may not be optimal and is an avenue for future work.|$|R
40|$|We {{study the}} online {{micro-blog}} sentiment detection problem, {{which aims to}} determine whether a micro-blog post expresses emotions. This problem is challenging because a micro-blog post is very short and individuals have distinct ways of expressing <b>emotions.</b> A single <b>classification</b> <b>model</b> trained on the entire corpus may fail to capture characteristics unique to each user. On the other hand, a personalized model for each user may be inaccurate due to the scarcity of training data, especially at the very beginning where users have just posted a few entries. To overcome these challenges, we propose learning a global model over all micro-bloggers, which is then leveraged to continuously refine the individual models through a collaborative online learning way. We evaluate our algorithm on a real-life micro-blog dataset collected from the popular micro-blog site – Twitter. Results show that our algorithm is effective and efficient for timely sentiment detection in real micro-blogging application...|$|R
40|$|<b>Emotion</b> <b>classification</b> can be {{generally}} done {{from both the}} writer’s and reader’s perspectives. In this study, we find that two foundational tasks in <b>emotion</b> <b>classification,</b> i. e., reader’s <b>emotion</b> <b>classification</b> on the news and writer’s <b>emotion</b> <b>classification</b> on the comments, are strongly {{related to each other}} in terms of coarse-grained emotion categories, i. e., negative and positive. On the basis, we propose a respective way to jointly model these two tasks. In particular, a co-training algorithm is proposed to improve semi-supervised learning of the two tasks. Experimental evaluation shows the effectiveness of our joint modeling approach...|$|R
40|$|The goal of <b>emotion</b> <b>classification</b> is to {{estimate}} an emotion label, given representative data and discriminative features. Humans {{are very good}} at deriving high-level representations of emotion state and integrating this information over time to arrive at a final judg-ment. However, currently, most <b>emotion</b> <b>classification</b> algorithms do not use this technique. This paper presents a hierarchical static-dynamic <b>emotion</b> <b>classification</b> framework that estimates high-level emotional judgments and locally integrates this information over time to arrive at a final estimate of the affective label. The results suggest that this framework for <b>emotion</b> <b>classification</b> leads to more accurate results than either purely static or purely dynamic strategies...|$|R
30|$|Human {{emotions}} {{are thought to}} be discrete in nature with distinguishable EEG signals. The process of <b>emotion</b> <b>classification</b> based on EEG signals may require some sort of channel selection to save computation time. In addition, there is a certain area in the brain that is concerned with emotions, which makes channels from other areas unrelated to <b>emotion</b> <b>classification.</b> Channel selection approaches adopted for <b>emotion</b> <b>classification</b> can be categorized to filtering and wrapper techniques.|$|R
40|$|Abstract. The {{performance}} of categorical music <b>emotion</b> <b>classification</b> that divides <b>emotion</b> into classes and uses audio features alone for <b>emotion</b> <b>classification</b> {{has reached a}} limit due {{to the presence of}} a semantic gap between the object feature level and the human cognitive level of emotion perception. Motivated by the fact that lyrics carry rich semantic information of a song, we propose a multi-modal approach to help improve categorical music <b>emotion</b> <b>classification.</b> By exploiting both the audio features and the lyrics of a song, the proposed approach improves the 4 -class <b>emotion</b> <b>classification</b> accuracy from 46. 6 % to 57. 1 %. The results also show that the incorporation of lyrics significantly enhances the classification accuracy of valence...|$|R
40|$|The typical <b>emotion</b> <b>classification</b> {{approach}} adopts one-step single-label classification using intra-sentence {{features such}} as unigrams, bigrams and emotion words. However, single-label classifier with intra-sentence features cannot ensure good performance for short microblogs text which has flexible expressions. Target to this problem, this paper proposes an iterative multi-label <b>emotion</b> <b>classification</b> approach for microblogs by incorporating intra-sentence features, as well as sentence and document contextual information. Based on the prediction of the base classifier with intra-sentence features, the iterative approach updates the prediction by further incorporating both sentence and document contextual information until the classification results converge. Experimental results obtained by three different multi-label classifiers on NLP & CC 2013 Chinese microblog <b>emotion</b> <b>classification</b> bakeoff dataset demonstrates the effectiveness of our iterative <b>emotion</b> <b>classification</b> approach. Department of ComputingRefereed conference pape...|$|R
40|$|This paper {{proposes a}} novel {{approach}} using a coarse-to-fine analysis strategy for sentence-level <b>emotion</b> <b>classification</b> which takes into consideration of similarities to sentences in training set as well as adjacent sentences in the context. First, we use intra-sentence based features to determine the emotion label set of a target sentence coarsely through the statistical information gained from the label sets of the k most similar sentences in the training data. Then, we use the emotion transfer probabilities between neighboring sentences to refine the emotion labels of the target sentences. Such iterative refinements terminate when the <b>emotion</b> <b>classification</b> converges. The proposed algorithm is evaluated on Ren-CECps, a Chinese blog emotion corpus. Experimental {{results show that the}} coarse-to-fine <b>emotion</b> <b>classification</b> algorithm improves the sentence-level <b>emotion</b> <b>classification</b> by 19. 11 % on the average precision metric, which outperforms the baseline methods. Department of ComputingRefereed conference pape...|$|R
40|$|As an {{essential}} approach to understanding human interactions, <b>emotion</b> <b>classification</b> {{is a vital}} component of behavioral studies {{as well as being}} important in the design of context-aware systems. Recent studies have shown that speech contains rich information about emotion, and numerous speech-based <b>emotion</b> <b>classification</b> methods have been proposed. However, the classification performance is still short of what is desired for the algorithms to be used in real systems. We present an <b>emotion</b> <b>classification</b> system using several one-against-all support vector machines with a thresholding fusion mechanism to combine the individual outputs, which provides the functionality to effectively increase the <b>emotion</b> <b>classification</b> accuracy at the expense of rejecting some samples as unclassified. Results show that the proposed system outperforms three state-of-the-art methods and that the thresholding fusion mechanism can effectively improve the <b>emotion</b> <b>classification,</b> which is important for applications that require very high accuracy but do not require that all samples be classified. We evaluate the system performance for several challenging scenarios including speaker-independent tests, tests on noisy speech signals, and tests using non-professional acted recordings, in order to demonstrate the performance of the system and the effectiveness of the thresholding fusion mechanism in real scenarios. Peer ReviewedPreprin...|$|R
40|$|Genre {{and emotion}} {{have been applied}} to {{content-based}} music retrieval and organization; however, the intrinsic correlation between them has not been explored. In this paper we present a statistical association analysis to examine such intrinsic correlation and propose a two-layer scheme that exploits the correlation for <b>emotion</b> <b>classification.</b> Significant improvement of classification accuracy over the traditional single-layer scheme is obtained. Index Terms—Music genre classification, association analysis, music <b>emotion</b> <b>classification.</b> 1...|$|R
40|$|Abstract—The <b>emotion</b> <b>classification</b> of text is an {{important}} research direction of text mining. Application on <b>emotion</b> text <b>classification,</b> latent semantic analysis algorithm has advantage of small occupied space, applicable to a large scale of text classifications. Compared with the traditional vector space model, latent semantic analysis algorithms reduce the search space for text classification by means of singular value decomposition for term and document matrix. Moreover, latent semantic analysis algorithms {{solve the problem of}} words with multiple meanings by analyzing the term at the semantic level. Using an improved latent semantic analysis algorithm to classify the test set by their emotion. The new cluster centroid is the average vector for each emotion category, and access to <b>emotions</b> <b>classification</b> for training dataset by calculating similarity of the average vector and test textual. The experimental results show that the improved latent semantic analysis algorithm have high precision and recall rate as same as the original algorithm, the efficiency of text <b>emotion</b> <b>classification</b> improved 4 percentage points...|$|R
30|$|Lee et al. [22] {{classified}} {{positive and}} negative <b>emotions.</b> <b>Classification</b> accuracy was obtained as 78.45 % by using adaptive neuro-fuzzy inference system (ANFIS).|$|R
50|$|The {{interpersonal}} scales measure {{two factors}} that affect interpersonal functioning for the respondent. They {{are based on the}} circumplex <b>model</b> of <b>emotion</b> <b>classification.</b>|$|R
40|$|In this paper, a {{comparison}} of <b>emotion</b> <b>classification</b> undertaken by the Support Vector Machine (SVM) and the Multi-Layer Perceptron (MLP) Neural Network, using prosodic and voice quality features extracted from the Berlin Emotional Database, is reported. The features were extracted using PRAAT tools, while the WEKA tool was used for classification. Different parameters were set up for both SVM and MLP, which are used to obtain an optimized <b>emotion</b> <b>classification.</b> The results show that MLP overcomes SVM in overall <b>emotion</b> <b>classification</b> performance. Nevertheless, the training for SVM was much faster when compared to MLP. The overall accuracy was 76. 82 % for SVM and 78. 69 % for MLP. Sadness was the emotion most recognized by MLP, with accuracy of 89. 0 %, while anger was the emotion most recognized by SVM, with accuracy of 87. 4 %. The most confusing <b>emotions</b> using MLP <b>classification</b> were happiness and fear, while for SVM, the most confusing emotions were disgust and fear...|$|R
40|$|In {{this paper}} {{comparisons}} <b>emotion</b> <b>classification</b> between Support Vector Machine (SVM) and Multi Layer Perception (MLP) Neural Network using prosodic and voice quality features extracted from Berlin Emotional Database are reported. The features were extracted using PRAAT tools while WEKA tool {{was used for}} classification. Different parameters set up for both SVM and MLP were implemented in getting the optimized <b>emotion</b> <b>classification.</b> The results show that MLP overcomes SYM in overall <b>emotion</b> <b>classification.</b> Nevertheless, the training for SYM was much faster compared to MLP. The overall recognition rate was (76. 82 %) for SYM and (78. 69 %) for MLP. Sadness was the highest emotion recognized by MLP with recognition rate of (89. 0 %) while anger was the highest emotion recognized by SYM with recognition rate of (87. 4 %). The most confusing <b>emotion</b> using MLP <b>classification</b> were happiness and fear while for SYM, the most confusing emotions were disgust and fear...|$|R
40|$|Abstract. In <b>emotion</b> <b>classification</b> {{of speech}} signals, the popular {{features}} employed are statistics of fundamental frequency, energy contour, duration {{of silence and}} voice quality. However, the performance of systems employing these features degrades substantially when more than two categories of emotion are to be classified. In this paper, a text independent method of <b>emotion</b> <b>classification</b> of speech is proposed. The proposed method makes use of short time log frequency power coefficients(LFPC) to represent the speech signals and a discrete Hidden Markov Model (HMM) as the classifier. The category labels used are, the archetypal emotions of anger, joy, sadness and neutral. Results show that the proposed system yields an average accuracy of 82. 55 %and the best accuracy of 94. 4 % in the <b>classification</b> of 4 <b>emotions.</b> Results also reveal that LFPC is a better choice as feature parameters for <b>emotion</b> <b>classification</b> than the traditional feature parameters...|$|R
40|$|Abstract. In this paper, {{we address}} the — interrelated — {{problems}} of speaker characteristics (personalization) and suboptimal performance of <b>emotion</b> <b>classification</b> in state-of-the-art modules {{from two different}} points of view: first, we focus on a specific phenomenon (irregular phonation or laryngealization) and argue that its inherent multi-functionality and speaker-dependency makes its use as feature in <b>emotion</b> <b>classification</b> less promising than one might expect. Second, we focus on a specific application of emotion recognition in a voice portal and argue that constraints on time and budget often prevent the implementation of an optimal emotion recognition module. ...|$|R
40|$|In this paper, we {{investigate}} the <b>emotion</b> <b>classification</b> of web blog corpora using {{support vector machine}} (SVM) and conditional random field (CRF) machine learning techniques. The emotion classifiers are trained at the sentence level and applied to the document level. Our methods also determine an emotion category by taking {{the context of a}} sentence into account. Experiments show that CRF classifiers outperform SVM classifiers. When applying <b>emotion</b> <b>classification</b> to a blog at the document level, the emotion of the last sentence in a document {{plays an important role in}} determining the overall emotion. 1...|$|R
40|$|This paper {{studies the}} problem of <b>emotion</b> <b>classification</b> in {{microblog}} texts. Given a microblog text which consists of several sentences, we classify its emotion as anger, disgust, fear, happiness, like, sadness or surprise if available. Existing methods can be categorized as lexicon based methods or machine learning based methods. However, due to some intrinsic characteristics of the microblog texts, previous studies using these methods always get unsatisfactory results. This paper introduces a novel approach based on class sequential rules for <b>emotion</b> <b>classification</b> of microblog texts. The approach first obtains two potential emotion labels for each sentence in a microblog text by using an emotion lexicon and a machine learning approach respectively, and regards each microblog text as a data sequence. It then mines class sequential rules from the dataset and finally derives new features from the mined rules for <b>emotion</b> <b>classification</b> of microblog texts. Experimental results on a Chinese benchmark dataset show the superior performance of the proposed approach...|$|R
40|$|Working memory (WM) and <b>emotion</b> <b>classification</b> are {{amongst the}} {{cognitive}} domains where specific deficits {{have been reported}} for patients with schizophrenia. In healthy individuals, the capacity of visual working memory is enhanced when the material to be retained is emotionally salient, particularly for angry faces. We investigated whether patients with schizophrenia also have an enhanced WM capacity for angry faces. We compared 34 inpatients with schizophrenia and 34 age-, handedness- and gender-matched control participants in three separate tasks. In the WM task, participants saw two faces with angry, happy or neutral emotional expressions for 2 s and had to decide whether a probe face presented after a 1 s delay was identical to one of them. In the <b>emotion</b> <b>classification</b> task, they had to assign these faces to the appropriate categorical emotion. They also rated faces for valence and arousal. Although patients performed generally worse on the working memory task, they showed the same benefit for angry faces as control participants. However, patients were specifically impaired for angry faces on the <b>emotion</b> <b>classification</b> task. These results indicate preserved implicit emotion processing in schizophrenia patients, which contrasts with their impairment in explicit <b>emotion</b> <b>classification.</b> With regard to clinical practice, our findings underline the importance of assessing responsiveness to emotions in patients with schizophrenia, with a view possibly to utilize preserved implicit emotion processing in cognitive remediation programs...|$|R
40|$|Support vector machine Manifold {{learning}} a b s t r a c t Recently, <b>emotion</b> <b>classification</b> from EEG data has {{attracted much attention}} with the rapid development of dry electrode techniques, machine {{learning a}}lgorithms, and various real-world applications of brain– computer interface for normal people. Until now, however, researchers had little understanding {{of the details of}} relationship between different emotional states and various EEG features. To improve the accuracy of EEG-based <b>emotion</b> <b>classification</b> and visualize the changes of emotional states with time, this paper systematically compares three kinds of existing EEG features for <b>emotion</b> <b>classification,</b> introduces an efficient feature smoothing method for removing the noise unrelated to emotion task, and proposes a simple approach to tracking the trajectory of emotion changes with manifold learning. To examine the effectiveness of these methods introduced in this paper, we design a movie induction experiment that spontaneously leads subjects to real emotional states and collect an EEG data set of six subjects. From experimental results on our EEG data set, we found that (a) power spectrum feature is superior to other two kinds of features; (b) a linear dynamic system based feature smoothing method can significantly improve <b>emotion</b> <b>classification</b> accuracy; and (c) the trajectory of emotion changes can be visualized by reducing subject-independent features with manifold learning. & 2013 Elsevier B. V. All rights reserved. 1...|$|R
40|$|Emotion adds an {{important}} element to the discussion of how information is conveyed and processed by humans; indeed, it plays {{an important}} role in the contextual understanding of messages. This research is centered on investigating relevant features for affect <b>classification,</b> along with <b>modeling</b> the multimodal and multitemporal nature of emotion. The use of formant-based features for affect classification is explored. Since linear predictive coding (LPC) based formant estimators often encounter problems with modeling speech elements, such as nasalized phonemes and give inconsistent results for bandwidth estimation, a robust formant-tracking algorithm was introduced to better model the formant and spectral properties of speech. The algorithm utilizes Gaussian mixtures to estimate spectral parameters and refines the estimates using maximum a posteriori (MAP) adaptation. When the method was used for features extraction applied to <b>emotion</b> <b>classification,</b> the results indicate that an improved formant-tracking method will also provide improved <b>emotion</b> <b>classification</b> accuracy. Spectral features contain rich information about expressivity and emotion. However, most of the recent work in affective computing has not progressed beyond analyzing the mel-frequency cepstral coefficients (MFCC’s) and their derivatives. A novel method for characterizing spectral peaks was introduced. The method uses a multi-resolution sinusoidal transform coding (MRSTC). Because of MRSTC’s high precision in representing spectral features, including preservation of high frequency content not present in the MFCC’s, additional resolving power was demonstrated. Facial expressions were analyzed using 53 motion capture (MoCap) markers. Statistical and regression measures of these markers were used for <b>emotion</b> <b>classification</b> along the voice features. Since different modalities use different sampling frequencies and analysis window lengths, a novel classifier fusion algorithm was introduced. This algorithm is intended to integrate classifiers trained at various analysis lengths, as well as those obtained from other modalities. Classification accuracy was statistically significantly improved using a multimodal-multitemporal approach with the introduced classifier fusion method. A practical application of the techniques for <b>emotion</b> <b>classification</b> was explored using social dyadic plays between a child and an adult. The Multimodal Dyadic Behavior (MMDB) dataset was used to automatically predict young children’s levels of engagement using linguistic and non-linguistic vocal cues along with visual cues, such as direction of a child’s gaze or a child’s gestures. Although this and similar research is limited by inconsistent subjective boundaries, and differing theoretical definitions of emotion, a significant step toward successful <b>emotion</b> <b>classification</b> has been demonstrated; key to the progress has been via novel voice and visual features and a newly developed multimodal-multitemporal approach. Ph. D...|$|R
40|$|Despite the {{enormous}} interest in <b>emotion</b> <b>classification</b> from speech, {{the impact of}} noise on <b>emotion</b> <b>classification</b> is not well understood. This is important because, due to the tremendous advancement of the smartphone technology, {{it can be a}} powerful medium for speech emotion recognition in the outside laboratory natural environment, which is likely to incorporate background noise in the speech. We capitalize on the current breakthrough of Recurrent Neural Network (RNN) and seek to investigate its performance for <b>emotion</b> <b>classification</b> from noisy speech. We particularly focus on the recently proposed Gated Recurrent Unit (GRU), which is yet to be explored for emotion recognition from speech. Experiments conducted with speech compounded with eight different types of noises reveal that GRU incurs an 18. 16 % smaller run-time while performing quite comparably to the Long Short-Term Memory (LSTM), which is the most popular Recurrent Neural Network proposed to date. This result is promising for any embedded platform in general and will initiate further studies to utilize GRU to its full potential for emotion recognition on smartphones...|$|R
40|$|This {{thesis is}} focused on EEG {{processing}} and <b>emotion</b> <b>classification</b> within two-dimensional <b>emotion</b> space. First part consists of theoretical research about emotional responses of human subjects on sound, image and video stimuli. Emotions are examined from aspect of physiology and psychology. Furthermore technical overview of measurement, analysis and <b>emotion</b> <b>classification</b> within two-dimensional emotional space is discussed. Based on gathered knowledge measurement setup with audiovisual stimuli was designed and measured with two independent instruments – EGI GES 400 MR in laboratory conditions and Emotiv EPOC device in non-laboratory conditions. Signals were processed and emotions were classified based on chosen features. Performance of classifiers in multiple feature selection setups was evaluated...|$|R
5000|$|<b>Emotion</b> <b>classification,</b> {{the means}} by which one may {{distinguish}} one emotion from another, is a contested issue in emotion research and in affective science. Researchers have approached the <b>classification</b> of <b>emotions</b> from one of two fundamental viewpoints: ...|$|R
30|$|For {{the feature}} {{extraction}} process, the Marsyas tool [13] was used, {{which is a}} free software framework. It is modular, fast, and flexible for rapid development and evaluation of computer audition applications and has been commonly used for music <b>emotion</b> <b>classification</b> and MIR tasks.|$|R
40|$|We {{introduce}} a new <b>emotion</b> <b>classification</b> task based on Leary’s Rose, a framework for interpersonal communication. We present a small dataset of 740 Dutch sentences, outline the annotation process and evaluate annotator agreement. We then evaluate the performance of several automatic classification systems when classifying individual sentences according to the four quadrants and the eight octants of Leary’s Rose. SVM-based classifiers achieve average F-scores of up to 51 % for 4 -way classification and 31 % for 8 -way classification, which is well above chance level. We conclude that <b>emotion</b> <b>classification</b> according to the Interpersonal Circumplex is a challenging task for both humans and machine learners. We expect classification performance to increase as context information becomes available in future versions of our dataset. ...|$|R
40|$|AbstractOne of the {{greatest}} challenges in speech technology is estimating the speaker's emotion. Most of the existing approaches concentrate either on audio or text features. In this work, we propose a novel approach for <b>emotion</b> <b>classification</b> of audio conversation based on both speech and text. The novelty in this approach is in the choice of features and the generation of a single feature vector for classification. Our main intention is to increase the accuracy of <b>emotion</b> <b>classification</b> of speech by considering both audio and text features. In this work we use standard methods such as Natural Language Processing, Support Vector Machines, WordNet Affect and SentiWordNet. The dataset for this work have been taken from Semval - 2007 and eNTERFACE’ 05 EMOTION Database...|$|R
