10|55|Public
5000|$|Edwards and Foley, as {{editors of}} {{a special edition of}} the American Behavioural Scientist on [...] "Social Capital, Civil Society and Contemporary Democracy", raised two key issues in the study of social capital. First, social capital is not equally {{available}} to all, {{in much the same way}} that other forms of capital are differently available. Geographic and social isolation limit access to this resource. Second, not all social capital is created equally. The value of a specific source of social capital depends in no small part on the socio-economic position of the source with society. On top of this, Portes has identified four negative consequences of social capital: exclusion of outsiders; <b>excess</b> <b>claims</b> on group members; restrictions on individual freedom; and downward levelling norms.|$|E
40|$|The {{purpose of}} the paper {{is to develop a}} method of calculating the {{aggregate}} loss distribution of <b>excess</b> <b>claims</b> based on a formula described in the book Risk Theory by Beard, Pentikainenen, and Pesonen. This formula requires that the claim frequency distribution satisfy a certain recursive relationship. The first part of the paper shows that a claim frequency distributions of <b>excess</b> <b>claims</b> derived from a claim frequency distribution satisfying the recursive relationship also has that recursive property. The second part describes a simple Pascal program that implements the calculation of aggregate loss distributions using these formulas. 19...|$|E
40|$|By {{conducting}} an extensive exploration on claim data, this paper attempts {{to investigate the}} fraud problem in Taiwan automobile physical damage insurance. Based on the different claim patterns between data in calendar year and policy year, <b>excess</b> <b>claims</b> are significantly identified {{in the last month}} of policy year. Censored regression provides robust estimation concerning the sources of the fraud payment...|$|E
40|$|This paper {{presents}} axiomatic {{characterizations of}} two bankruptcy rules disscused in Jewish legal literature: the Constrained Equal Awards rule and the Contested Garment principle (the latter is defined only for two-creditor problems.) A major property in these characterizations is independence of irrelevant claims, which requires {{that if an}} individual claim exceeds the total to be allocated the <b>excess</b> <b>claim</b> should be considered irrelevant. ...|$|R
50|$|On the Avenged Sevenfold DVD All <b>Excess,</b> Gates <b>claimed</b> {{that his}} name was created on a drunken drive through the park with the Rev.|$|R
5000|$|<b>Excess</b> Baggage and <b>Claim,</b> co-authored with Terry Jaensch (Transit Lounge, 2007) ...|$|R
40|$|<b>Excess</b> <b>claims</b> {{lead to an}} {{unsatisfactory}} {{behavior of}} standard linear credibility estimators. We suggest in this paper to use robust methods {{in order to obtain}} better estimators. Our first proposal is the linear credibility estimator with the claims replaced by a robust M-estimator of scale calculed from the claims. This corresponds to a truncation of the claims with a truncation point depending on the data and different for each contract. We discuss the properties of the robust M-estimator and present several examples. In order to improve the performance for {{a very small number of}} years, we propose a second estimator, which incorporates information from other claims into the M-estimato...|$|E
40|$|The {{purpose of}} this study is to {{determine}} how social capital (informal obligations and social norms) affects the ability to save within IDAs among immigrant and US Native individuals. Social capital is the capacity for someone or a group of persons to access resources and secure benefits through social networks (Portes, 1998; Flores, 2010). Coleman (1988) identifies three forms of social capital: obligations and expectations, informational channels, and social norms. Portes (1998) states that at least four negative consequences can be identified: exclusion of outsiders, <b>excess</b> <b>claims</b> on group members, restrictions on individual freedoms, and downward leveling norms. The author aims to examine the influence of social capital among IDA participants...|$|E
40|$|In actuarial practice, {{regression}} models {{serve as a}} popular statistical tool for analyzing insurance data and tariff ratemaking. In this paper, we consider classical credibility models that can be embedded {{within the framework of}} mixed linear models. For inference about fixed effects and variance components, likelihood-based methods such as (restricted) maximum likelihood estimators are commonly pursued. However, it is well-known that these standard and fully efficient estimators are extremely sensitive to small deviations from hypothesized normality of random components {{as well as to the}} occurrence of outliers. To obtain better estimators for premium calculation and prediction of future claims, various robust methods have been successfully adapted to credibility theory in the actuarial literature. The objective of this work is to develop robust and efficient methods for credibility when heavy-tailed claims are approximately log-location-scale distributed. To accomplish that, we first show how to express additive credibility models such as Bühlmann-Straub and Hachemeister ones as mixed linear models with symmetric or asymmetric errors. Then, we adjust adaptively truncated likelihood methods and compute highly robust credibility estimates for the ordinary but heavy-tailed claims part. Finally, we treat the identified <b>excess</b> <b>claims</b> separately and find robust-efficient credibility premiums. Practical performance of this approach is examined-via simulations-under several contaminating scenarios. A widely studied real-data set from workers' compensation insurance is used to illustrate functional capabilities of the new robust credibility estimators. IB 83 IM 10 IM 31 IM 41 IM 54 Adaptive robust-efficient estimation Asymmetric heavy-tailed residuals Credibility ratemaking Mixed linear model Treatment of <b>excess</b> <b>claims...</b>|$|E
40|$|We {{present a}} model of leptoquarks (LQs) with a {{significant}} partial branching ratio into an extra sector, taken {{to be a viable}} dark matter candidate, other than the canonical lepton and jets final state. For LQs with mass around 500 GeV, the model reproduces the recent <b>excess</b> <b>claimed</b> by the CMS Collaboration in the ℓ+ ℓ-jjET final state: the event rate, the distribution in the dilepton invariant mass and the rapidity range are compatible with the data. The model is compatible with other collider bounds including LQ searches, as well as bounds from meson mixing and decays. Prospects of discovery at run II of the LHC are discussed...|$|R
40|$|Motivated by excesses in ee jj and eν jj {{channels}} {{observed by}} the CMS collaboration, in 8 TeV LHC data, {{a model of}} lepto-quarks with mass around 500 GeV was proposed in the literature. In order to reproduce the claimed event rate, lepto-quarks were assumed {{to have a significant}} partial branching ratio into an extra sector, taken to be Dark Matter, other than the canonical ej. We here show that the decay channel of lepto-quark into Dark Matter can fit another <b>excess</b> <b>claimed</b> by CMS, in ℓ^+ℓ^- jj E/_ T: the event rate, the distribution in di-lepton invariant mass and the rapidity range are compatible with the data. We provide predictions for the forthcoming Run II of the 14 TeV LHC and discuss aspects of dark matter detection. Comment: 6 pages, 2 figures. Version accepted in PR...|$|R
40|$|Numerical {{evaluation}} of ruin probabilities {{in the classical}} risk model is an important problem. If claim sizes are heavy-tailed, then such evaluations are challenging. To overcome this, an attractive way is to approximate the claim sizes with a phase-type distribution. What is not clear though is how many phases are enough {{in order to achieve}} a specific accuracy in the approximation of the ruin probability. The goals of this paper are to investigate the number of phases required so that we can achieve a pre-specified accuracy for the ruin probability and to provide error bounds. Also, in the special case of a completely monotone claim size distribution we develop an algorithm to estimate the ruin probability by approximating the <b>excess</b> <b>claim</b> size distribution with a hyperexponential one. Finally, we compare our approximation with the heavy traffic and heavy tail approximations. © 2014 Copyright Taylor & Francis Group, LLC...|$|R
40|$|The {{purpose of}} this paper is to {{describe}} a possible application on the rating of excess of loss covers of Mr. Bühlmann's work on Experience Rating and Credibility). One of the most important problems in connection with the rating of such treaties consists in estimating the number of <b>excess</b> <b>claims</b> and the average excess claim amount. Especially with cases where claims data are scarce there is a temptation to estimate these two quantities by means of the credibility theory. This approach leads, on the one hand, to a relatively complicated formula when considering the average excess claim amount, and, on the other, to a rather simple one for the credibility factor of the number of excess claim...|$|E
40|$|This paper {{addresses}} the question: How valuable is {{a sample of}} <b>excess</b> <b>claims</b> in determining the expected claim se-verity in an excess layer of insurance? An established procedure to estimate this expected claim severity is to first fit a model distribution to claim size data and then, using the fitted distribution, estimate the expected claim severity in the given excess layer. One {{of the more popular}} models used is the single parameter Pareto. This pa-per provides a means of quanttfiing the uncertainty in these excess claim severity estimates when using the single pa-rameter Pareto. This approach requires one to incorporate prior opinions about the distribution of the Pareto parameter using Bayes ’ Theorem. 1...|$|E
40|$|For {{estimating}} the shape parameter of Paretian <b>excess</b> <b>claims,</b> certain Bayesian estimators, which {{are closely related}} to the Hill estimator, have been suggested in the insurance literature. It turns out that these estimators may have a poor performance- just as the Hill estimator- if a certain location parameter is unequal to zero in the Paretian modeling. In an alternative formulation this means that a scale parameter is unequal to 1. Thus, it suggests itself to add the scale parameter in the modeling and to deal with Bayesian estimators of the shape and scale parameters in a full Paretian model. These estimators will be applied to fire and motor reinsurance data. The performance of these estimators will be illustrated by means of Monte Carlo simulations...|$|E
40|$|International audienceWe {{study the}} Sunyaev-Zel'dovich effect {{potentially}} generated by relativistic electrons injected from dark matter annihilation or decay in the Galaxy, and check whether {{it could be}} observed by Planck or the Atacama Large Millimeter Array (ALMA), or even imprint the current CMB data as, e. g., the specific fluctuation <b>excess</b> <b>claimed</b> from an recent reanalysis of the WMAP- 5 data. We focus on high-latitude regions to avoid contamination of the Galactic astrophysical electron foreground, and consider the annihilation or decay coming from the smooth dark matter halo {{as well as from}} subhalos, further extending our analysis to a generic modeling of spikes arising around intermediate-mass black holes. We show that all these dark Galactic components are unlikely to produce any observable Sunyaev-Zel'dovich effect. For a self-annihilating dark matter particle of 10 GeV with canonical properties, the largest optical depth we find is τe≲ 10 - 7 for massive isolated subhalos hosting intermediate-mass black holes. We conclude that dark matter annihilation or decay on the Galactic scale cannot lead to significant Sunyaev-Zel'dovich distortions of the CMB spectrum...|$|R
40|$|We {{study the}} Sunyaev-Zel'dovich effect {{potentially}} generated by relativistic electrons injected from dark matter annihilation or decay in the Galaxy, and check whether {{it could be}} observed by Planck or the Atacama Large Millimeter Array (ALMA), or even imprint the current CMB data as, e. g., the specific fluctuation <b>excess</b> <b>claimed</b> from an recent reanalysis of the WMAP- 5 data. We focus on high-latitude regions to avoid contamination of the Galactic astrophysical electron foreground, and consider the annihilation or decay coming from the smooth dark matter halo {{as well as from}} subhalos, further extending our analysis to a generic modeling of spikes arising around intermediate-mass black holes. We show that all these dark Galactic components are unlikely to produce any observable Sunyaev-Zel'dovich effect. For a self-annihilating dark matter particle of 10 GeV with canonical properties, the largest optical depth we find is τ_e ≲ 10 ^- 7 for massive isolated subhalos hosting intermediate-mass black holes. We conclude that dark matter annihilation or decay on the Galactic scale cannot lead to significant Sunyaev-Zel'dovich distortions of the CMB spectrum. Comment: 14 pages, 6 figures. V 2 : Minor changes to match the published versio...|$|R
40|$|This is {{the author}} {{accepted}} manuscript. The final version is available from APS via [URL] present a model of leptoquarks (LQs) with a significant partial branching ratio into an extra sector, taken {{to be a viable}} dark matter candidate, other than the canonical lepton and jets final state. For LQs with mass around 500 GeV, the model reproduces the recent <b>excess</b> <b>claimed</b> by the CMS collaboration in ℓ^+ℓ^- jj final state: the event rate, the distribution in di-lepton invariant mass and the rapidity range are compatible with the data. The model is compatible with other collider bounds including LQ searches, as well as bounds from meson mixing and decays. Prospects of discovery at Run II of the LHC are discussed. This work was supported by Fundacão de Amparo à Pesquisa do Estado de São Paulo (FAPESP) grant 2013 / 22079 - 8, STFC grant ST/L 000385 / 1, US Department of Energy Award SC 0010107 and the Brazilian National Counsel for Technological and Scientific Development (CNPq) grant 307098 / 2014 - 1 (AA), and NASA Astrophysics Theory Grant NNH 12 ZDA 001 N, by ESF grant MTT 8...|$|R
40|$|Given the long-tailed {{nature of}} certain lines of business, such as workers ’ compensation, {{and the impact}} of {{inflation}} on claim costs, determination of development factors, particularly in the tail, can be challenging. Reliance on excess loss development triangles can present challenges from both a credibility and volatility perspective. Furthermore, the application of excess development factors selected directly from excess loss triangles does not fully account for the impact of claim cost inflation, which has a greater impact on <b>excess</b> <b>claims</b> than on claims limited to a retention. Therefore many actuaries fall back on industry development patterns that are not necessarily indicative of the individual company’s development and may be impacted by other distortions (e. g., non consistent interpretation of limits or retentions across companies in the compilation of data). We will discuss these distortions and the limitations of reliance on excess data and then present an alternative approach that relies on more stable ground-up data and can adjust for changing retention levels by year via calculation of excess development factors using excess loss factors (ELFs). We will discuss the theory behind the formula and its own benefits and limitations...|$|E
40|$|We {{show that}} {{compatibility}} between the DAMA modulation result (as well as less statistically significant excesses {{such as the}} CDMS Silicon effect and the <b>excess</b> <b>claimed</b> by CRESST) with constraints from other experiments {{can be achieved by}} extending the analysis of direct detection data beyond the standard elastic scattering of a WIMP off nuclei with a spin [...] dependent or a spin [...] independent cross section and with a velocity distribution as predicted by the Isothermal Sphere model. To do so we discuss several new approaches for the analysis of Dark Matter direct detection data, with the goal to remove or reduce its dependence on specific theoretical assumptions, and to extend its scope: the factorization approach of astrophysics uncertainties, the classification and study of WIMP-nucleon interactions within non [...] relativistic field theory, inelastic scattering and isovector-coupling cancellations including subdominant two-nucleon NLO effects. Typically, combining two or more of these ingredients can lead to conclusions which are very different to what usually claimed in the literature. This shows that we are only starting now to scratch the surface of the most general WIMP direct detection parameter space. Comment: 6 pages, 3 figures, to appear in the proceedings of the 14 th Marcel Grossmann meeting, Roma, July 201...|$|R
50|$|In May 2016 {{the company}} {{was found to have}} {{systematically}} evaded taxes and <b>claimed</b> <b>excess</b> tax refunds through stating the wrong tax codes for imported equipment - VND1.55 trillion (US$69.2 million) in tax refunds and VND5.5 billion ($245.54 million) in taxes were required to be paid to the government.|$|R
2500|$|During the 1920s, in {{the wake}} of Modigliani's career and spurred on by {{comments}} by André Salmon crediting hashish and absinthe with the genesis of Modigliani's style, many hopefuls tried to emulate his [...] "success" [...] by embarking on a path of substance abuse and bohemian <b>excess.</b> Salmon <b>claimed</b> that whereas Modigliani was a totally pedestrian artist when sober, [...] "...from the day that he abandoned himself to certain forms of debauchery, an unexpected light came upon him, transforming his art. From that day on, he became one who must be counted among the masters of living art." ...|$|R
40|$|XMM-Newton {{observations}} of {{the outskirts of the}} Coma cluster of galaxies confirm the existence of a soft X-ray <b>excess</b> <b>claimed</b> previously and show it comes from warm thermal emission. Our data provide a robust estimate of its temperature (~ 0. 2 keV) and oxygen abundance (~ 0. 1 solar). Using a combination of XMM-Newton and ROSAT All-Sky Survey data, we rule out a Galactic origin of the soft X-ray emission. Associating this emission with a 20 Mpc region in front of Coma, seen in the skewness of its galaxy velocity distribution, yields an estimate of the density of the warm gas of ~ 50 f_baryon rho_critical, where f_baryon is the baryon fraction of the gas and rho_critical is the critical density needed to halt the expansion of the universe. Our measurement of the gas mass associated with the warm emission strongly support its nonvirialized nature, suggesting that we are observing the warm-hot intergalactic medium (WHIM). Our measurements provide a direct estimate of the O, Ne and Fe abundance of the WHIM. Differences with the reported Ne/O ratio for some OVI absorbers hints at a different origin of the OVI absorbers and the Coma filament. We argue that the Coma filament has likely been preheated, but at a substantially lower level compared to what is seen in the outskirts of groups. The thermodynamic state of the gas in the Coma filament reduces the star-formation rate in the embedded spiral galaxies, providing an explanation for the presence of passive spirals observed in this and other clusters. Comment: 9 pages, 5 figures, accepted by A&...|$|R
40|$|Abstract. XMM-Newton {{observations}} of {{the outskirts of the}} Coma cluster of galaxies confirm the existence of a soft X-ray <b>excess</b> <b>claimed</b> previously and show it comes from warm thermal emission. Our data provide a robust estimate of its temperature (∼ 0. 2 keV) and oxygen abundance (∼ 0. 1 solar). Using a combination of XMM-Newton and ROSAT All-Sky Survey data, we rule out a Galactic origin of the soft X-ray emission. Associating this emission with a 20 Mpc region in front of Coma, seen in the skewness of its galaxy velocity distribution, yields an estimate of the density of the warm gas of ∼ 50 fbaryonρcritical, where fbaryon is the baryon fraction and ρcritical is the critical density needed to halt the expansion of the universe. Our measurement of the gas mass associated with the warm emission strongly support its nonvirialized nature, suggesting that we are observing the warm-hot intergalactic medium (WHIM). Our measurements provide a direct estimate of the O, Ne and Fe abundance of the WHIM. Differences with the reported Ne/O ratio for some OVI absorbers hints at a different origin of the OVI absorbers and the Coma filament. We argue that the Coma filament has likely been preheated, but at a substantially lower level compared to what is seen in the outskirts of groups. The thermodynamic state of the gas in the Coma filament reduces the star-formation rate in the embedded spiral galaxies, providing an explanation for the presence of passive spirals observed in this and other clusters...|$|R
40|$|Responding to an {{overpopulation}} of {{wild horses}} on the BLM {{lands in the}} state, Wyoming sued the Secretary of the Interior and the BLM for failure to manage the <b>excess</b> numbers. Wyoming’s <b>claim,</b> based on the Wild Horses and Burros Act and Administrative Procedure Act, jumped the gun by bringing it before the BLM made its determination that removal was necessary to manage the overpopulation...|$|R
50|$|In 1956, the Soviet premier, Nikita Khrushchev, denounced Stalin in {{a secret}} speech before the Twentieth Congress of the Communist Party of the Soviet Union (CPSU). Gheorghiu-Dej and the {{leadership}} of the Romanian Workers' Party (Partidul Muncitoresc Român, PMR) were fully braced to weather de-Stalinization. Gheorghiu-Dej made Pauker, Luca and Georgescu scapegoats for the Romanian communist past <b>excesses</b> and <b>claimed</b> that the Romanian party had purged its Stalinist elements even before Stalin died in 1953. In all likelihood, Gheorghiu-Dej himself ordered the violence and coercion in the collectivization movements, since he did not rebuke those who perpetuated abuses. In fact, Pauker reprimanded any cadre who forced peasants, and once she was purged, the violence reappeared.|$|R
50|$|VAT that {{is charged}} by a {{business}} and paid by its customers is known as output VAT (that is, VAT on its output supplies). VAT that is paid by a business to other businesses on the supplies that it receives is known as input VAT (that is, VAT on its input supplies). A business is generally able to recover input VAT {{to the extent that}} the input VAT is attributable to (that is, used to make) its taxable outputs. Input VAT is recovered by setting it against the output VAT for which the business is required to account to the government, or, if there is an <b>excess,</b> by <b>claiming</b> a repayment from the government.|$|R
2500|$|VAT that {{is charged}} by a {{business}} and paid by its customers is known as [...] "output VAT" [...] (that is, VAT on its output supplies). VAT that is paid by a business to other businesses on the supplies that it receives is known as [...] "input VAT" [...] (that is, VAT on its input supplies). A business is generally able to recover input VAT {{to the extent that}} the input VAT is attributable to (that is, used to make) its taxable outputs. Input VAT is recovered by setting it against the output VAT for which the business is required to account to the government, or, if there is an <b>excess,</b> by <b>claiming</b> a repayment from the government. Private people are generally allowed to buy goods in any member country and bring it home and pay only the VAT to the seller.|$|R
40|$|We reexamine the {{hypothesis}} that the optical/UV/soft X-ray continuum of active galactic nuclei (AGNs) is thermal emission from an accretion disk. Previous studies have shown that fitting the spectra with the standard optically thick and geometrically thin accretion disk models often led to luminosities that contradict the basic assumptions adopted in the standard model. There is no known reason why the accretion rates in AGNs should not be larger than the thin disk limit. In fact, more general, slim accretion disk models are self-consistent even for moderately super-Eddington luminosities. We calculate here spectra from a set of thin and slim, optically thick accretion disks, assuming for simplicity a modified blackbody local emission with no relativistic corrections. We discuss the differences between the thin and slim disk models, stressing the implications of these differences for the interpretation of the observed properties of AGNs. We find that the spectra can be fitted not only by models with a high mass and a low accretion rate (as in the case of thin disk fitting) but also by models with a low mass and a high accretion rate. In the first case, fitting the observed spectra in various redshift categories gives black hole masses of ∼ 10 9 M ⊙ {{for a wide range of}} redshifts and for accretion rates ranging from 0. 4 (low redshift) to 8 M⊙ yr - 1 (high redshift). In the second case, the accretion rate is ∼ 10 2 M ⊙ yr - 1 for all AGNs, and the mass ranges from 3 × 10 6 (low redshift) to 10 8 M ⊙ (high redshift). Unlike the disks with a low accretion rate, the spectra of the high accretion rate disks extend into the soft X-ray region. A comparison with observations shows that such disks could produce the soft X-ray <b>excesses</b> <b>claimed</b> for some AGNs. We show also that the sequence of our models with fixed mass and different accretion rates can explain the time evolution of the observed spectra in Fairall 9...|$|R
40|$|We reexamine the {{hypothesis}} that the optical/UV/soft X-ray continuum of Active Galactic Nuclei is thermal emission from an accretion disk. Previous studies have shown that fitting the spectra with the standard, optically thick and geometrically thin accretion disk models often led to luminosities which contradict the basic assumptions adopted in the standard model. There is no known reason why the accretion rates in AGN should not be larger than the thin disk limit. In fact, more general, slim accretion disk models are self-consistent even for moderately super-Eddington luminosities. We calculate here spectra from a set of thin and slim, optically thick accretion disks. We discuss the differences between the thin and slim disk models, stressing the implications of these differences for the interpretation of the observed properties of AGN. We found that the spectra can be fitted not only by models with a high mass and a low accretion rate (as in the case of thin disk fitting) but also by models with a low mass and a high accretion rate. In the first case fitting the observed spectra in various redshift categories gives black hole masses around 10 ^ 9 solar masses {{for a wide range of}} redshifts, and for accretion rates ranging from 0. 4 to 8 solar masses/year. In the second case the accretion rate is around 10 ^ 2 solar masses/year for all AGN and the mass ranges from 3 * 10 ^ 6 to 10 ^ 8 solar masses. Unlike the disks with a low accretion rate, the spectra of the high-accretion-rate disks extend into the soft X-rays. A comparison with observations shows that such disks could produce the soft X-ray <b>excesses</b> <b>claimed</b> in some AGNs. We show also that the sequence of our models with fixed mass and different accretion rates can explain the time evolution of the observed spectra in Fairall 9. Comment: LaTeX file, 21 pages, 21 figures (on request from ewa@sissa. it) accepted by Astrophysical Journa...|$|R
40|$|CASE AT A GLANCE In chapter 7 bankruptcy, a debtor keeps certain statutorily defined “exempt” assets, {{while all}} other assets are sold to pay creditors. In exchange, {{most of the}} debtor’s debts are discharged. In this case, the Court must decide whether a debtor may be sanctioned {{by the loss of}} exempt assets as an {{equitable}} remedy for trying to fraudulently <b>claim</b> <b>excess</b> exemptions or hide assets, with the forfeited assets awarded to the bankruptcy estate to recover litigation costs arising from the debtor’s misconduct...|$|R
40|$|International audienceAs far as Protection Insurance is concerned, the {{emergence}} of an influenza pandemic could lead to an <b>excess</b> of <b>claims</b> in the insured population. A pandemic would obviously {{have a significant impact on}} Death risk, but could also affect the risk of Temporary Disability and Hospitalisation. The aim of this article is to determine how to model the effects of a pandemic event on mortality risk, and to analyse how a pandemic may affect the risk of Temporary Disability and Hospitalisation. Knowing that a pandemic could cause bankruptcy in the worst scenarios, an insurance company should consider buying adequate coverage. The last part of this article is devoted to the search for a solution to hedge against a pandemic event. This overview is not limited to standardized products sold by reinsurers, but more elaborate and exotic offers are described, with their advantages and drawbacks...|$|R
5000|$|EU VAT (known as [...] "output VAT", that is, VAT on its output supplies) {{is charged}} by a {{business}} and paid by its customers. VAT that is paid by a business to other businesses on the supplies that it receives is known as [...] "input VAT" [...] (that is, VAT on its input supplies). A business is generally able to recover input VAT {{to the extent that}} the input VAT is attributable to (that is, used to make) its taxable outputs. Input VAT is recovered by offsetting it against the output VAT for which the business is required to account to the government, or, if there is an <b>excess,</b> by <b>claiming</b> a repayment from the government. The final consumer does not receive a credit for the VAT paid. The net effect of this is that each supplier in the chain remits tax on the value added, and ultimately the tax is paid by the end consumer.|$|R
40|$|Dr. E. Storms has {{published}} a Letter [1] in which he argues that in a sequence of recent papers [2 - 5], the apparent <b>excess</b> heat signal <b>claimed</b> by Dr. Shanahan to arise from a calibration constant shift is actually true excess heat. In particular he proposes that the mechanisms proposed that foster the proposed calibration constant shifts [3, 5] cannot occur as postulated for several reasons. As well, he proposes Shanahan has ignored the extant data proving this. Because this Letter may lend unwarranted support to acceptance of cold fusion claims, these erroneous arguments used by Storms need to be answered...|$|R
40|$|This paper studies {{excess of}} loss {{reinsurance}} with reinstatements in the {{case in which the}} aggregate claims are generated by a discrete distribution, in the framework of risk adjusted premium principle. By regarding to comonotonic exchangeability, a generalized definition of initial premium is proposed and some regularity properties characterizing it are presented, both with reference to conditions on underlying distortion functions both with respect to composing functions. The attention is then focused on conditions ensuring feasibility of generalized initial premiums with reference to the limit on the payment of each <b>claim.</b> <b>Excess</b> of loss reinsurance; reinstatements; initial premium; exchangeability; distortion risk measures; feasibility. ...|$|R
50|$|In May 2012, the Australian Competition and Consumer Commission (ACCC) {{reported}} it was investigating about 100 cases where customers had possibly been misled into paying excessive price rises falsely {{claimed to be}} {{as a result of}} the carbon tax. By the middle of June, the commission was investigating about 200 cases. The consumer watchdog also set up a phone hotline and online form for complaints regarding <b>excess</b> pricing <b>claimed</b> to be due to the carbon tax. The ACCC had forecast that home construction costs would be at the lower end of the 0.7% to 1.8% range predicted by building companies. The Housing Industry Association estimated an average new house would experience a price increase of between 0.8% and 1.7% due to the carbon price. Housing construction was expected to be significantly impacted by the carbon tax because new homes require cement, bricks, aluminium, and glass, which are all typically energy-intensive materials. A forecast by the Centre for International Economics predicted the housing construction industry could decline by 12.6% {{as a result of the}} carbon price.|$|R
