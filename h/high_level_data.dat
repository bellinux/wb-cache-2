100|10000|Public
50|$|Core Data {{describes}} {{data with}} a <b>high</b> <b>level</b> <b>data</b> model {{expressed in terms}} of entities and their relationships plus fetch requests that retrieve entities meeting specific criteria. Code can retrieve and manipulate this data on a purely object level without having to worry about the details of storage and retrieval. The controller objects available in Interface Builder can retrieve and manipulate these entities directly. When combined with Cocoa bindings the UI can display many components of the data model without needing background code.|$|E
5000|$|Navlab 2 {{was built}} in 1990 using a US Army HMMWV. Computer power was uprated for this new vehicle with three Sparc 10 {{computers}}, [...] "for <b>high</b> <b>level</b> <b>data</b> processing", and two 68000-based computers [...] "used for low level control". The Hummer was capable of driving both off- or on-road. When driving over rough terrain, its speed was limited with {{a top speed of}} 6 mph. When Navlab 2 was driven on-road it could achieve as high as 70 mph ...|$|E
50|$|The {{information}} storage and delivery system {{was based upon}} GPT4190 computers coupled to dial in modem for public to access and via microwave link to a UHF TV Transmitter fitted with full field teletext inserters. Transmission of lower level protocol data and user terminal management control was via telephone V22 modem. <b>High</b> <b>level</b> <b>data</b> such as Geometric and photographic displays sent exclusively by full field teletext with interchange control and handshaking via V22 modem. The service profile is based upon extensions to the existing CEPT Videotext Services Recommendations T/CD 6-1 as defined in Teleview Videotext Service Profile and Data Standard Ref; 7630/DS/1 1987.|$|E
50|$|Oxford Nanopore Technologies offers Pipeline Pilot NGS {{collection}} as {{the preferred}} and supported solution for secondary and <b>higher</b> <b>level</b> <b>data</b> analysis in their GridION system.|$|R
50|$|The raw payload data {{received}} {{through the}} data reception stations is further processed to generate Level-0 and Level-1 data {{products that are}} stored in the ISSDC archives for subsequent dissemination. Automation in the entire chain of data processing is planned. Raw payload data / Level-0 data/ Level-1 data for each science payload is transferred to the respective Payload Operations Centers (POC) for further processing, analysis and generation of <b>higher</b> <b>level</b> <b>data</b> products. The <b>higher</b> <b>level</b> <b>data</b> products generated by the POC’s are subsequently transferred to ISSDC archives for storage and dissemination. The data archives for Level-0 and higher products are organized following the Planetary Data System (PDS) standards.|$|R
5000|$|Five-way {{data and}} six-way {{data can be}} {{represented}} by similarly <b>higher</b> <b>levels</b> of <b>data</b> aggregation.|$|R
50|$|Data lineage {{provides}} the audit {{trail of the}} data points at the highest granular level, but presentation of the lineage may be done at various zoom levels to simplify the vast information, similar to analytic web maps. Data Lineage can be visualized at various levels based on the granularity of the view. At a very <b>high</b> <b>level</b> <b>data</b> lineage provides what systems the data interacts before it reaches destination. As the granularity increases it goes up to the data point level where it can provide {{the details of the}} data point and its historical behavior, attribute properties, and trends and data quality of the data passed through that specific data point in the data lineage.|$|E
40|$|<b>High</b> <b>level</b> <b>data</b> {{structures}} are {{a cornerstone of}} modern programming {{and at the same}} time stand in the way of compiler optimizations. In order to reason about user or library-defined data structures, compilers need to be extensible. Common mechanisms to extend compilers fall into two categories. Frontend macros, staging or partial evaluation systems can be used to programmatically remove abstraction and specialize programs before they enter the compiler. Alternatively, some compilers allow extending the internal workings by adding new transformation passes at different points in the compile chain or adding new intermediate representation (IR) types. None of these mechanisms alone is sufficient to handle the challenges posed by <b>high</b> <b>level</b> <b>data</b> structures. This paper shows a novel way to combine them to yield benefits that are greater than the sum of the parts...|$|E
40|$|Severe weather prediction, such as {{tropical}} cyclone (TC) forecast {{is a typical}} data mining and forecasting problem that involves <b>high</b> <b>level</b> <b>data</b> manipulation and interpretation of meteorological information such as satellite pictures and other meteorological observation data. In this paper, we present a fully automatic and integrated system known a...|$|E
40|$|When using {{parallel}} I/O, {{some limitations}} are currently present in MPI-I/O and <b>higher</b> <b>level</b> <b>data</b> access libraries like Parallel netCDF. One of these limitations is I/O across multiple files. Another limitation in netCDF {{is not being}} able to request disjoint regions of a file in one I/O request. The Block I/O Layer (BIL) is designed to specifically handle situations lik...|$|R
5000|$|The {{joints and}} access points should be secured and be {{controlled}} by personnel cleared to the <b>highest</b> <b>level</b> of <b>data</b> handled by the PDS ...|$|R
5000|$|... gammapy: A <b>high</b> <b>level</b> gamma-ray {{astronomy}} <b>data</b> analysis package ...|$|R
40|$|The {{underlying}} {{principles of}} concurrency and data flow are summarized {{along with a}} survey of the current data flow languages. A <b>high</b> <b>level</b> <b>data</b> flow language, DIVA, is developed that provides the basic data types and language constructs of traditional languages as well as some unique features of data flow. The organization and data structures of the compiler and assembler are also discussed...|$|E
40|$|Data {{warehousing}} {{has captured}} the attention of practitioners and researchers for a long time, whereas aspects of data quality is one of the crucial issues in data warehousing [ENG 99; HEL 00]. Still, ensuring <b>high</b> <b>level</b> <b>data</b> quality {{is one of the most}} expensive and time-consuming tasks to perform in data warehousing projects [HAE 98]. Many data warehouse projects are discontinued due to insufficient data quality [HEL 00]. The following article describes an approach for managing data quality in data warehouse systems through a metadata based data quality system. The results are integrated in a comprehensive management approach and are based on practical experiences within a Swiss bank. 1 Data Quality Framework 1. 1 Proactive Data Quality Management There are several textbook approaches for managing and defining data quality [e. g. JAR 00; HEL 00; HUA 99; ENG 99; TAY 98, EPP 00], but still the question remains how to ensure <b>high</b> <b>level</b> <b>data</b> quality in data warehouse systems. To provide a management concept for ensuring <b>high</b> <b>level</b> <b>data</b> quality, current research by the Competence Center ‘Data Warehousing 2 ’ 1 applies the concept of total quality management (TQM) to data warehouse systems. Focus on customer requirements, participation of all stakeholders as well as continuous improvement and a comprehensive management approach are important characteristics of TQM [JUR 79; SEG 96]. All enterprise wide activities are integrated into an enterprise wide structure which continuously improves products, services and process quality in order to satisfy customer requirements. Proactive data quality management (proDQM) is based on TQM and combines an organizational structure defining functions and responsibilities with processes ensuring continuous quality improvement. Techniques and tools support the processes, standards an...|$|E
40|$|Abstract. This paper {{describes}} an innovative sensor system which can detect and track {{people in a}} room {{by means of an}} array of capacitive sensors beneath the floor covering. By combining cutting-edge technology from the domains of capacitive sensing, wireless data transmission, interconnecting technology between textiles and microelectronics and <b>high</b> <b>level</b> <b>data</b> processing it is possible to support various groundbreaking applications in the domains of Ambient Assisted Living, energy saving, comfort, marketing, healthcare and security. ...|$|E
40|$|The U. S. ASTER Science Team is {{currently}} engaged in numerous ASTER related activities, {{many of them}} jointly with our Japanese colleagues. These include vicarious instrument calibration, algorithm development and validation for <b>higher</b> <b>level</b> <b>data</b> products, assistance to ERSDAC for scheduling activities (primarily for U. S. users), assistance to data users other than Science Team members, and science applications of ASTER data, notably {{in the areas of}} glacial monitoring, volcanic monitoring, heat balance determinations, geologic mapping, and cloud studies...|$|R
40|$|One of {{the major}} {{problems}} facing decision makers today is a proliferation of data from a multitude of sources. Many techniques have been proposed for de-cluttering displays. The key idea is to show only the relevant data at the proper time, avoiding the visual distraction of other non-pertinent data. Many of the techniques deal with clustering or organizing the information at <b>higher</b> <b>levels</b> of abstraction. Sometimes through, the details are important. The challenge is {{to provide access to}} these lower <b>level</b> <b>data</b> sources from within the visual context of simplified <b>higher</b> <b>level</b> <b>data.</b> In this paper we develop several techniques providing efficient access to detailed information, and timely triggers to help the user know when, where, and how to access this information...|$|R
30|$|<b>Higher</b> <b>levels</b> of {{transparency}} and openness: The need for better {{management and control}} of the different smart city aspects and applications, will drive the interoperability and openness to <b>higher</b> <b>levels.</b> <b>Data</b> and resource sharing will be the norm. In addition, this will increase information transparency for everyone involved. This will encourage collaboration and communication between entities and creating more services and applications that further enhance the smart city. One example is the US government that collected and released a wide range of data, publications, and content in the name {{of transparency}} and openness. These offered the citizens and the government entities the chance to exchange and use the data effectively.|$|R
40|$|Abstract—In this paper, we {{implement}} a modern serial backplane platform for telecommunication inter-rack systems. For combination high reliability and low cost protocol property, we applied <b>high</b> <b>level</b> <b>data</b> link control (HDLC) protocol {{with low voltage}} differential signaling (LVDS) bus for card to card communicated over backplane. HDLC protocol is a high performance with several operation modes and is famous in telecommunication systems. LVDS bus is a high reliability with high immunity against electromagnetic interference (EMI) and noise...|$|E
40|$|This paper {{shows that}} {{mathematical}} models of biological pattern formation are {{ideally suited to}} data parallelism. We present two new algorithms, one for simulating the dynamic structure of fibroblasts, {{and the other for}} studying the self-organization of motile bacteria. We describe implementations of these algorithms using a <b>high</b> <b>level</b> <b>data</b> parallel language called ZPL, and we give performance results for the Kendall Square Research KSR- 2 and the Intel Paragon that include comparisons against sequential Fortran. ...|$|E
40|$|This paper {{describes}} the throughput {{evaluation of the}} <b>high</b> <b>level</b> <b>data</b> link control protocol. The analytical models are outlined and the throughput analysis is carried out under fully loaded condition for the following three error recovery schemes: (1) Checkpoint retransmission scheme, (2) SREJ recovery scheme and (3) REJ recovery scheme. We derive the optimal packet text length which achieves the maximum throughput for each error recovery scheme. Also we denote the optimal window size for the checkpoint retransmission scheme...|$|E
30|$|Rolf Schwitter is a Senior Lecturer in the Department of Computing and {{associated}} with the Centre for Language Technology at Macquarie University. He was the President of the Australasian Language Technology Association between 2007 and 2008. Between July 2007 and June 2008, he was a NICTA Fellow and worked on the SAIL (Situation Awareness by Inference and Logic) project where he developed techniques for <b>higher</b> <b>level</b> <b>data</b> fusion. His main research interests include natural (and formal) language processing, in particular: controlled natural languages, answer extraction, knowledge representation, automated reasoning and the Semantic Web.|$|R
50|$|Heterogeneous {{clusters}} {{are fully}} supported, {{and there are}} several deployment options that are available, including some that provide very <b>high</b> <b>levels</b> of <b>data</b> redundancy and fault tolerance. This feature is marketed by IBM as Informix Flexible Grid.|$|R
40|$|CALIPSO aerosol {{backscatter}} enhancement in {{the transition}} zone between clouds and clear sky areas is revisited with particular attention to effects of data selection based on the confidence level of cloud-aerosol discrimination (CAD). The results show that backscatter behavior {{in the transition}} zone strongly depends on the CAD confidence <b>level.</b> <b>Higher</b> confidence <b>level</b> <b>data</b> has a flatter backscatter far away from clouds and a much sharper increase near clouds (within 4 km), thus a smaller transition zone. For <b>high</b> confidence <b>level</b> <b>data</b> it is shown that the overall backscatter enhancement is more pronounced for small clear-air segments and horizontally larger clouds. The results suggest that data selection based on CAD reduces the possible effects of cloud contamination when studying aerosol properties {{in the vicinity of}} clouds...|$|R
30|$|At the funded {{beginning}} of the project enera, the third phase for the Use Cases started. During this phase, the Use Cases were imported from the MS Word template to the webbased Use Case Management Repository (UCMR) from OFFIS. 1 Meanwhile, the actors and information exchanged within the Use Cases were subject to a consistency check, where the information exchanged was mapped manually to a <b>high</b> <b>level</b> <b>data</b> object catalogue and the consistency of actors {{was supported by the}} merge-function of the UCMR.|$|E
30|$|The major {{contribution}} {{of this paper}} is a design methodology consisting in a domain-specific language (DSL), which combines data frame information with dataflow computational model to synthesize an FPGA-SDR waveform. The aim is to hide the complexity of specifying an SDR waveform while automating all the control requirements from a <b>high</b> <b>level</b> <b>data</b> frame description. Furthermore, the HLS tools are employed to generate efficient signal processing blocks, at the RTL-level, while the compiling framework consistently builds the datapath and associates the control logic.|$|E
40|$|Involving 28 grade eight students, {{this action}} {{research}} study examined strategies {{that lead to}} effective and efficient band rehearsals at the junior <b>high</b> <b>level.</b> <b>Data</b> was gathered concerning the proportion of instructional time spent on teaching concepts and skills, on active music making, and on classroom management. From this, and {{a review of the}} literature on best practice, new ways were designed to improved rehearsal practice. Data was then gathered to track their effectiveness and students ’ perceptions regarding the changes...|$|E
50|$|The {{fundamental}} {{building blocks}} of Q are atoms, lists and functions. Atoms are scalars and include numeric, character, date and time data types. Lists are ordered collections of atoms (or other lists) upon which the <b>higher</b> <b>level</b> <b>data</b> structures dictionaries and tables are internally constructed. A dictionary is a map of a list of keys {{to a list of}} values. A table is a transposed dictionary of symbol keys and equal length lists (columns) as values. A keyed table, analogous to a table with a primary key placed on it, is a dictionary where the keys and values are arranged as two tables.|$|R
5|$|Data {{received}} from the satellite is free to the public. There are multiple <b>levels</b> of <b>data</b> available. Level-1 data takes 1–3 days to process, and the user will receive multiple files that they can then piece together to generate an RGB image. <b>Higher</b> <b>level</b> science <b>data</b> can also be requested, which contains data such as surface reflectance.|$|R
40|$|System R is a {{database}} management system which provides a <b>high</b> <b>level</b> relational <b>data</b> interface. The system provides a <b>high</b> <b>level</b> of <b>data</b> independence by isolating the end user {{as much as possible}} from underlying storage structures. The system permits definition of a variety of relational views on common underlying data. Data control features are provided, including authorization, integrity assertions, triggered transactions, a logging and recovery subsystem, and facilities for maintaining data consistency in a shared-update environment. This paper contains a description of the overall architecture and design of the system. At the present time the system is being implemented and the design evaluated. We emphasize that System R is a vehicle for research in database architecture, and is not planned as a product...|$|R
40|$|Communication with data-base {{processor}} {{flexible and}} efficient. <b>High</b> <b>Level</b> <b>Data</b> Abstraction (HILDA) system is three-layer system supporting data-abstraction features of Intel data-base processor (DBP). Purpose of HILDA establishment of flexible method of efficiently communicating with DBP. Power of HILDA {{lies in its}} extensibility with regard to syntax and semantic changes. HILDA's high-level query language readily modified. Offers powerful potential to computer sites where DBP attached to DEC VAX-series computer. HILDA system written in Pascal and FORTRAN 77 for interactive execution...|$|E
40|$|Abstract. Bioinformatics {{applications}} are often {{characterized by a}} combination of (pre) processing of raw data representing biological elements, (e. g. sequence alignment), and an <b>high</b> <b>level</b> <b>data</b> mining analysis. Developing such applications needs knowledge of both data mining and bioinformatics domains, that can be effectively achieved by combining ontology about the application domain (the problem) and ontology about the resolving approaches (the solution). We talk about using ontologies to model proteomics in silico experiments. In particular data mining of mass spectrometry proteomics data is considered. ...|$|E
40|$|A {{survey of}} OECD member {{countries}} {{was carried out}} to provide <b>high</b> <b>level</b> <b>data</b> {{on a consistent basis}} to identify and account for current patterns of child road safety. This paper reports the findings relating to children, aged 0 - 14 years, as pedestrians. Key survey elements included analyses of fatality data, relationships between socio-economic, demographic factors and fatality rates, and a questionnaire based survey. League tables based on average child pedestrian fatality rates were constructed for each OECD member country participating in our questionnaire enabling identification of the top five countries wit...|$|E
50|$|The <b>high</b> <b>level</b> of big <b>data</b> {{collection}} and analytics has raised questions regarding surveillance in smart cities, particularly {{as it relates}} to predictive policing.|$|R
5000|$|The 4th Part of the Austrian Health Telematics Act 2012 (HTA 2012) - {{these are}} the EHR {{provisions}} - {{are one of the}} most detailed data protection rules within Austrian legislation. Numerous safeguards according to Art 8(4) DPD guarantee a <b>high</b> <b>level</b> of <b>data</b> protection. For example: ...|$|R
40|$|Abstract. Appearance of GridFTP in Grid won't {{substitute}} the old {{data transfer}} protocols, however, existing data servers confuse the users and applications. There is no unified data accessing approach. This paper proposes an implementation of web based adaptive Grid data transfer solution over multiple protocols. The system {{is made up}} of three tiers: a transfer core named PAFTP, transfer service based on WSRF, and portlet application. The core hides diversity of various protocols and provides a universal interface to <b>higher</b> <b>level</b> <b>data</b> access. This solution not only offers multi-protocol reliable data transfer, simple file operation and monitoring on transfer status, but also emphasizes performance of mass file transfer. ...|$|R
