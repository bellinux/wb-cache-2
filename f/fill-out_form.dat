3|9|Public
40|$|The morbility for Metabolic-Syndrome in five doctor's {{office of}} the north health area in the {{municipality}} Accomplished a descriptive transverse cut investigation in 151 adult women's probabilistic sign to characterize itself Sancti - Spíritus, among 1 ro of Noviembre of the 2007 and the November 30 the 2008. Morbility, the antecedent factors personnels of risk and of chronic illnesses were variables gone into no transmissible, they gathered data in a <b>fill-out</b> <b>form,</b> statistical analysis included percentages calculation, parameters esteem and tests them of proportions difference (x 2). The main things aftermaths were Metabolic Syndrome prevalence of 33, 3 % In the patients with metabolic syndrome the risk factor of chronic illnesses no transmissible that predominate was {{in order not to}} accomplish physical activity (96, 1 %), her hiperlipidemia (27, 3 %), as well as the obesity (24, 4 %) in this entity's bearers. The antecedent pathological personals for chronic illnesses no transmissible registered hypertension went with 64, 7 %...|$|E
40|$|Purpose Determine {{the work}} {{environment}} factors related to hazards in voice production for elementary school teachers, {{as well as the}} impact of vocal rest and teaching methodology. Methods This research features a quantitative, cross-sectional, non-experimental, correlational study, which applies an instrument consisting of a <b>fill-out</b> <b>form</b> and a questionnaire performed by the evaluator to 90 elementary school teachers. The following variables were taken into account: classroom size, acoustics, noise, amount of students in the classroom, chemical substances, temperature, subject taught, teaching method and classroom vocal use time. The statistical analysis was performed using the PASW statistical software, version 20. Results Regarding the external acoustic insulation, the windows are made of glass or structures with metal bars, and no elements covering the surface of the window, zinc sheets or cement in ceilings and aluminum doors. The average noise measurement in the classroom is 77 dB, and the temperature and humidity measurements show a warm humid weather tendency. The most frequent teaching method is the lecture-type class. Elementary teachers must teach all of the subjects, and have an average voice rest period of 30 minutes. The inferential analysis using the chi-square test found no correlation between work environment factors of the teacher and the presence of dysphonia. Conclusion The elements of the teaching environment and the intrinsic factors of the teaching practice are not directly related to the presence of dysphonia, but they are associated elements that do not generate vocal disorders by themselves...|$|E
40|$|Projects {{consist of}} complex {{structures}} that require strategizing. As {{an important part}} of a project, strategic approaches of procurement may yield greater success, quality and value. This report focuses on the contracting strategy and public procurement practices in the Norwegian construction industry. In order to shed light on these practices and the reasons for them being as they are, this report aim to map out perceptions of these practices through the eyes of contractors and proprietors. Furthermore, due to the impressive claims of project success through Best Value Procurement (BVP), a comparison between that method and the contemporary practices is presented. The study utilizes a qualitative approach which consist of a literature study whose output functions as the theoretical background and a case study where data is collected through semi-structured interviews and mapping of a figure <b>fill-out</b> <b>form.</b> The sample population consisted of five interviewees representing some of the largest contractors and public proprietors in Norway. Analysis of data was conducted in several steps utilizing multiple loops to ensure extraction of relevant information. The results gave clear indications of a preference towards integrated contracting approaches, which there is little of in current practices The study indicated that most procurement strategies were habitual and separation based. Also, both contractors and proprietors recognized that there was an unbalanced relationship between the two, where contractors were pressured with small earnings. Furthermore, both contractors and proprietors indicated the belief that awarding contracts based on multiple criteria would produce the highest quality end product. Also, the study found that the current Norwegian procurement practices and BVP are mostly dissimilar. However, indications were found showing that especially contractors wished for practices with similar philosophies as those of BVP. Still, the knowledge of BVP was found to be limited amongst study participants...|$|E
5000|$|Berners-Lee {{considered}} HTML to be {{an application}} of SGML. It was formally defined as such by the Internet Engineering Task Force (IETF) with the mid-1993 publication of the first proposal for an HTML specification, the [...] "Hypertext Markup Language (HTML)" [...] Internet Draft by Berners-Lee and Dan Connolly, which included an SGML Document type definition to define the grammar. The draft expired after six months, but was notable for its acknowledgment of the NCSA Mosaic browser's custom tag for embedding in-line images, reflecting the IETF's philosophy of basing standards on successful prototypes. Similarly, Dave Raggett's competing Internet-Draft, [...] "HTML+ (Hypertext Markup Format)", from late 1993, suggested standardizing already-implemented features like tables and <b>fill-out</b> <b>forms.</b>|$|R
40|$|A {{hypertext}} {{version of}} this paper is also available [1]. This paper provides a brief description of a software tool developed within the tltp Interact Project [2] which allows application programs, concurrently executing within a Unix environment, to be controlled by scripts delivered to Mosaic from W 3 servers. This tool was developed to allow the coupling of interactive simulations of scientific and engineering phenomena with courseware provided by W 3. This software is known as the Interact Communication Facility (icf). The features of the icf include: ffl The ability to allow application programs to receive control messages and data from scripts embedded as links within html documents. ffl The use of html <b>fill-out</b> <b>forms</b> to enter data intended for application programs. ffl A simple interface to allow programs to control Mosaic, including the automatic execution of Mosaic if it is not currently being used. ffl A means of allowing secure execution of applications from scripts. Thi [...] ...|$|R
40|$|The {{quality of}} the results {{produced}} by personalized e-service applications like product recommenders, buying advisory applications, or product configurators is strongly determined by {{the accuracy of the}} system’s estimate of the individual customer’s real needs and preferences. In particular in domains where customers cannot be classified automatically, e. g., based on past buying behavior, these needs have to be interactively elicited by questioning the user. In many existing systems only a “one-style-fits-all ” approach based on static <b>fill-out</b> <b>forms</b> is chosen. However, this does not take the user’s background or capabilities into account, which consequently leads to a poor {{quality of the}} acquired user model. In this paper, we show how extensive personalization of the user preference elicitation process itself can significantly improve the accuracy of interactively acquired user models. A comprehensive view on adaptation and personalization opportunities in the elicitation process is developed and corresponding examples for the domain of interactive buying advisory are given. The presented personalization and adaptation techniques are implemented in a domain-independent software framework for building interactive advisory applications. We describe specific architectural requirements for such a system and discuss results from various real-world applications. 1...|$|R
40|$|Keywords: Self-service applications, e-services, {{knowledge-based}} systems. Abstract: The {{business plan}} {{is one of}} the essential tools for companies to attract investors and raise venture capital. Getting a business plan &quot;investor ready &quot; in that context means {{that it has to be}} professionally prepared such that it answers all questions of potential investors. This, however, requires in-depth knowledge of a typical investor's expectations and viewpoints, a type of knowledge which in particular first-time entrepreneurs or small companies do not dispose of. As a consequence, significant amounts of consulting hours are required to get the business plan ready to be presented to investors. The goal of the SAT project presented in this paper is to provide an in-depth business plan advisory service over the Web. While current approaches in that area rely on static <b>fill-out</b> <b>forms</b> or checklists, the SAT tool is based on personalized interactive dialogs, knowledge-based input analysis and feedback generation, as well as on detailed financial calculations. Within the paper, we thus show how knowledge-based approaches can serve as a technological foundation for such next-generation electronic services and how the corresponding development and maintenance efforts can be minimized. The paper gives an overview on the general knowledge-based architecture of the system, discusses the integrated graphical modelling environment, and finally reports on experiences gained from the practical use of the tool. ...|$|R
40|$|The World-Wide Web (WWW) {{changes the}} way people {{retrieve}} information from the Internet. The Web {{is regarded as a}} public information system which offers access to the network community with the client/server protocol Hypertext Transfer Protocol (HTTP). The Web uses client interface programs (browsers), hypertext and multimedia techniques to make it easy for anyone to roam, browse, and contribute to the information base. However, most information available on the Web is read-only, frozen messages. The introduction of HTML <b>Fill-out</b> <b>Forms</b> and Common Gateway Interfaces changes this situation. In this thesis we explore supplying dynamic information to users through the Web. We build an environment on the Web using CGI and HTML forms. This environment uses forms to let the user interact with the Web server, and generates text, pages and hyper links on the Web based on the users 2 ̆ 7 requirements. This environment is accessible to all authorized users and the information in it is changeable. An changes are visible immediately to all users. Applications of the environment are described. Paper copy at Leddy Library: Theses 2 ̆ 6 Major Papers - Basement, West Bldg. / Call Number: Thesis 1995. W 37. Source: Masters Abstracts International, Volume: 34 - 06, page: 2408. Adviser: R. Frost. Thesis (M. Sc.) [...] University of Windsor (Canada), 1995...|$|R
40|$|Abstract. Webbases are {{database}} {{systems that}} enable creation of Web applications that allow end users to shop around for {{products and services}} at various Web sites without having to manually browse and fill out forms at each of these sites. In this paper we describe XRover which is an implementation of the physical layer of the webbase architecture. This layer is primarily responsible for automatically locating and extracting dynamic data from Web sites, i. e data {{that can only be}} obtained by <b>form</b> <b>fill-outs.</b> We discuss our experience in building XRover using FLORA, a deductive object-oriented system. ...|$|R
40|$|The {{needs for}} {{three-dimensional}} (3 D) visualization and navigation within 3 D-GIS environment are growing and expanding rapidly {{in a variety}} of fields. In a steady shift from traditional two-dimensional (2 D) GIS toward 3 D-GIS, a great amount of accurate 3 D data sets (e. g. city models) have become necessary to be produced {{in a short period of}} time and provided widely on the market. This requires a number of specific issues to be investigated, e. g. 3 D routing accuracy, appropriate means to visualize 3 D spatial analysis, tools to effortlessly explore and navigate through large models in real time, with the correct texture and geometry. There had been a lot of study on 3 D landscapes, urban and city models. The rapid advancement in science and technology had opened wide options for a change and development of current methods and concepts. Virtual Reality (VR) is one of those developments, which gives the sense of feel in virtual environment. It enables users to visualize, make query and exploring 3 D data. Such system can, not only help laymen, who often have trouble in understanding or interpreting complex data, but they also can help experts in decision making. The objective of this paper is to discuss some initial requirements of the proposed solution towards 3 D-GIS. Eventually, this paper will serve as a starting point for a more challenging research idea. The focus of this research is to investigate and implementing 3 D navigation techniques and solutions for 3 D-GIS. Investigation on the support of navigation in real world environment will be carried out. This will include a research on the benefits of using 3 D network model (non-planar graph) compared to 2 D, how to use visual landmarks in route descriptions and using 3 D geometry to get more accurate routing (in buildings, or in narrow street, etc). And as for implementation, a GUI provides the users with means (e. g. <b>fill-out</b> <b>forms)</b> to specify SQL queries interact and visualize 3 D outcomes in virtual reality environment. This has opened up the ability to distribute and navigate accurately in 3 D virtual worlds. The initial study on Klang Valley will go through data conversion processes from different formats like Laser, VRML, CAD and Shape 3 D in a first person view environment using a developed system using VRML, JAVA and. Net compiler. The dataset structure will be in the form of various 2 D, 2. 5 D and 3 D array of height fields...|$|R
40|$|Web {{transactions}} (e. g. {{buying a}} CD {{player on the}} Web) typically involve a number of steps spanning several pages. This task gets strenuous when the Web is accessed non-visually (e. g. when the user is a visually handicapped individual). But usually one needs to browse only a small fragment of a Web page in a transactional step such as a <b>form</b> <b>fill-out,</b> selecting an item from search results, etc. Identifying and presenting such segments from a Web page can overcome the information overload problem when accessing the Web using non-visual interaction modalities like speech. Based on the aforementioned observation I have developed a transactional model that delivers only the “relevant” page fragments at each transactional step, thereby reducing the information overload. In my prior research, such transactional models were learned using a supervised learning approach. Supervised learning requires manually labeled training examples, consequences being that it is nether scalable nor flexible for creating personalized transaction models for arbitrary Web sites. In my PhD research, I am exploring automatic mining of transaction models from unlabeled transactional sequences. My approach is centered on leveraging contextual information of a hyper-link, geometric segmentation of a Web page, and clustering to mine the models. The approach will be scalable but more importantly end-users (with visual disabilities) {{will be able to}} create their own “personalized” transactional models for Web sites that they need to use on a regular basis for doing online transactions...|$|R
40|$|Automatic data {{extraction}} from semistructured {{sources such as}} HTML pages is rapidly growing into a problem of significant importance, spurred by {{the growing popularity of}} the so called "shopbots" that enable end users to compare prices of goods and other services at various web sites without having to manually browse and fill out forms at each one of these sites. The main problem one has to contend with when designing {{data extraction}} techniques is that the contents of a web page changes frequently, either because its data is generated dynamically, in response to filling out a form, or because of changes to its presentation format. This makes the problem of data extraction particularly challenging, since a desirable requirement of any data extraction technique is that it be "resilient", i. e., using it we should always be able to locate the object of interest in a page (such as a form or an element in a table generated by a <b>form</b> <b>fill-out)</b> in spite of changes to the page's content and layout. In this paper we propose a formal computation model for developing resilient data extraction techniques from semistructured sources. Specifically we formalize the problem of data extraction as one of generating unambiguous extraction expressions, which are regular expressions with some additional structure. The problem of resilience is then formalized as one of generating a maximal extraction expression of this kind. We present characterization theorems for maximal extraction expressions, complexity results for testing them, and algorithms for synthesizing them...|$|R

