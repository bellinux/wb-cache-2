11|35|Public
50|$|Graeme C. Simsion FACS is an Australian author, screenwriter, {{playwright and}} <b>data</b> <b>modeller.</b>|$|E
5000|$|Bill Kent, in his 1978 book Data and Reality, {{compared}} a {{data model}} to {{a map of}} a territory, emphasizing that in the real world, [...] "highways are not painted red, rivers don't have county lines running down the middle, and you can't see contour lines on a mountain". In contrast to other researchers who tried to create models that were mathematically clean and elegant, Kent emphasized the essential messiness of the real world, and {{the task of the}} <b>data</b> <b>modeller</b> to create order out of chaos without excessively distorting the truth.|$|E
40|$|The Dalradian terrane in the north-west of Northern Ireland is {{prospective}} for orogenic vein-hosted gold mineralisation {{with important}} deposits at Curraghinalt and Cavanacaw. New geochemical and geophysical {{data from the}} DETI-funded Tellus project have been used, {{in conjunction with other}} spatial geoscience datasets, to map the distribution of prospectivity for this style of mineralisation over this terrane. A knowledge-based fuzzy logic modelling methodology using Arc Spatial <b>Data</b> <b>Modeller</b> was utilised. Four main groups of targets were identified, many close to known occurrences in the Lack - Curraghinalt zone and others in prospective areas identified by previous investigations. Additional targets are located along west-north-west trending linear zones at the eastern end of the Newtownstewart Basin and {{to the north of the}} Omagh-Kesh Basin. These zones may be related to major structures linked to a westward extension of the Curraghinalt lateral ramp which is regarded as an important control on the location of the Curraghinalt deposit...|$|E
40|$|This paper {{describes}} {{teaching practices}} {{designed to help}} novice <b>data</b> <b>modellers</b> become expert <b>data</b> <b>modellers.</b> We base these practices on extant empirical research which highlights the strengths of expert <b>data</b> <b>modellers</b> and reveals the weaknesses of novices. After reviewing this research and analysing {{the causes of the}} novices' difficulties, we describe the strategy and specific techniques for helping novices to overcome their weaknesses and acquire the strengths and skills of expert <b>data</b> <b>modellers.</b> Techniques recommended include explicit comparison and teaching of novice and expert characteristics and behaviours, providing students with a realistic plan for how to acquire expert data modellers' capabilities, exposure to and comparison {{of a wide variety of}} data modelling approaches and topics, extensive amounts of practice on a wide variety of application domains, and critique of practical work in light of the understanding of novice errors and expert behaviours. Our intent is not just to make significant progress during a course, but to provide students with a means to continue to learn and improve in the long term...|$|R
50|$|RM/T {{contributed to}} the body of {{knowledge}} called semantic data modeling and semantic object modeling and continues to influence new <b>data</b> <b>modellers.</b> See the paper by Hammer and McLeod (1981), the book by Knoenke (2001) and implementation by Grabczewski et alia (2004).|$|R
40|$|This study {{explores the}} {{differences}} between conceptual data models designed by expert and novice data modelling practitioners. The data models are evaluated using a number of quality factors synthesised from previous empirical studies and frameworks for quality in conceptual modelling. This study extends previous studies by using practitioners as participants and using {{a number of different}} quality factors in the evaluation. The study found that data models produced by expert <b>data</b> <b>modellers</b> are more correct, complete, innovative and flexible than those produced by novices. The results suggest that further research into the aspects of expertise that lead to such differences and how training courses can narrow the gap between expert and novice performance is required...|$|R
40|$|This {{research}} is focused {{in the investigation}} of spatial data modeling by data-driven predictive methods, the weights of evidence (WofE) model in particular, aiming the identification of potential areas for gold mineralization in the proximities of Amapari region. The study area is situated in the Amapá State and has a special interest since it comprises one of the last exploration frontiers in Brazil, where much of the state is protected by national parks and indigenous reserves. The area hosts Archaean basements, greenstone belts (Province Vila Nova) and intrusive rocks that collectively show a complex geological evolution and with an ample metallogenic potential. The Amapari deposit is characterized as an orogenic-type gold mineralization, where the highest contents of gold are associated with the most deformed rocks in a system of sulfide veins, mainly composed of pyrrhotite. The inputs maps for the modelling process were geological and airbone geophysical data and gold mine and occurrences (training points). These data were integrated in a GIS environment using the Arc Spatial <b>Data</b> <b>Modeller</b> (ArcSDM) tool. The results using weights of evidence show that Amapari deposits were classified as a hight potential site as expected and, most importantly, new targets with similar signatures were identified throughout the area. Pages: 4011 - 401...|$|E
40|$|The Southern Uplands-Down-Longford Terrane in {{south-east}} Northern Ireland is prospective for Caledonian-age, turbidite-hosted orogenic gold mineralisation {{with important}} deposits at Clontibret in the Republic of Ireland and in Scotland. Geochemical and geophysical {{data from the}} DETI-funded Tellus project have been used, {{in conjunction with other}} spatial geoscience datasets, to map the distribution of prospectivity for this style of mineralisation over this terrane. A knowledge-based fuzzy logic modelling methodology using Arc Spatial <b>Data</b> <b>modeller</b> was utilised. The prospectivity analysis has identified several areas prospective for turbidite-hosted gold mineralisation, comparable to that at Clontibret and gold occurrences in the Southern Uplands of Scotland. A number of these either coincide with known bedrock gold occurrences or with areas considered prospective and targeted by previous exploration work, validating the predictive capability of the exploration model devised and its translation into a GIS-based prospectivity model. The results of the modelling suggest that as {{in other parts of the}} Southern Uplands the coincidence of regional strike-parallel structures and intersecting transverse faults are highly prospective, as these are likely to create zones of anomalous stress for fluid flow and deposit accumulation. Those areas in which there are no known gold occurrences are considered to be favourable targets for further exploration and should be followed up...|$|E
40|$|Abstract: The {{landslide}} susceptibility of hillsides {{has been}} assessed using the statistical methodology known as “weight of evidence”, {{in order to}} estimate its potentiality, its difficulty of application and its suitability to specific geomorphological settings. The Valley of the Rio Pardu River, eastern-central part of Sardinia, Italy, has been chosen for applying the method. On {{the basis of the}} IFFI Project (Inventory of Landslides Phenomena in Italy), the Rio Pardu Valley turns {{out to be one of}} more hazardous zone in Sardinia for the presence of ancient and current landslides, whose causes have to be searched in the structural and lithological characteristics, the extreme meteorological conditions and the anthropic factors. The weight of evidence method has been implemented by means of the ArcView 3. 2 software and the ArcSDM extension (Spatial <b>Data</b> <b>Modeller).</b> Like all the statistical methodologies, this method requires to identify and to locate on a map all the instability phenomena which affected the studied area. On the basis of the territorial distribution of the past and present landslides the method calculates the weights to be assigned to the single classes of every considered parameter. The landslide map of the Pardu Valley has been provided by the IFFI Project. The map has been digitized, georeferenced and supplemented by the observation of the 2006 colour orthophotos. For evaluating the landslide hazard of the studied area, three basic parameters for landslide susceptibilit...|$|E
40|$|This paper proposes {{advanced}} visualization {{and interaction}} techniques as a {{support for the}} analysis of system identification data. Non-linear or timedependent dynamics often leave a significant residual with linear, time-invariant models. The structure of this residual is decisive for the subsequent modelling and by visually analysing the <b>data</b> the <b>modeller</b> may gain a deeper insight into its structure than can be gained using only standard correlation analysis. Copyright c 2005 IFAC Keywords: Interactive visualization, system identification, model validation...|$|R
40|$|For {{some people}} on this planet {{cadastral}} modelling can be a life time job. I’m amongst them and I’m happy to know quite a few from my own and from other countries. It is the generation heavily involved in bringing the cadastral maps and registers from analogue to computerised environments. A unique event with unique knowledge built up for that purpose. Cadastral maps required extra attention in computerisation {{in order to keep}} the spatial data consistent and accessible. After conversion the data have been harmonised in many countries using extended or linked data models; the data quality has been improved; complete archives have been scanned and digital workflows have been introduced. Today products and services can be offered to users in society from complete digital cadastres. Data integration continues. Harmonisation of spatial data is a policy in the European Union in support to the implementation of environmental policies. The cadastral parcel is a core element here: a High Tea for cadastral <b>data</b> <b>modellers.</b> New, user dominated, applications appear with the introduction of all kind of mobile devices and social networks. The next generation can work and live now with all the created digital dat...|$|R
40|$|Most CAD {{or other}} spatial data models, in {{particular}} boundary representation models, are called "topological" and represent spatial data by a structured collection of "topological primitives" like edges, vertices, faces, and volumes. These then represent spatial objects in geo-information- (GIS) or CAD systems or in building information models (BIM). Volume objects may then either {{be represented by}} their 2 D boundary or by a dedicated 3 D-element, the "solid". The latter may share common boundary elements with other solids, just as 2 D-polygon topologies in GIS share common boundary edges. Despite the frequent reference to "topology" in publications on spatial modelling the formal link between mathematical topology and these "topological" models is hardly described in the literature. Such link, for example, cannot be established by the often cited nine-intersections model which is too elementary for that purpose. Mathematically, the link between spatial data and the modelled "real world" entities is established by a chain of "continuous functions" - a very important topological notion, yet often overlooked by spatial <b>data</b> <b>modellers.</b> This article investigates how spatial data can actually be considered topological spaces, how continuous functions between them are defined, and how CAD systems can make use of them. Having found examples of applications of continuity in CAD data models {{it turns out that}} of continuity has much practical relevance for CAD systems...|$|R
40|$|This study {{provides}} {{an assessment of}} the potential for stratiform massive sulphide mineralisation in two parts of south-west England, the Central Area between Bodmin Moor and Dartmoor, and North Devon. The Central Area was {{selected on the basis of}} its tectonic, stratigraphical and lithological similarities with the Iberian Pyrite Belt (IPB) where numerous volcanogenic massive sulphide (VMS) deposits occur. North Devon is considered prospective on account of its similarities to the Harz Massif in Germany that hosts the major polymetallic sedimentary exhalative (Sedex) deposit at Rammelsberg. More than 60 stratiform mineral occurrences are known in the two areas and previous exploration, including extensive drainage geochemical surveys, geophysical surveying and drilling, has revealed stratiform mineralisation at some localities, including at Egloskerry, near Bodmin Moor, where up to 10 % Pb over 4. 5 metres was recorded. In North Devon, stratiform mineralisation has been recorded from a borehole at Honeymead Farm, and further evidence of potential is provided by a distinct aeromagnetic anomaly parallel to the regional strike over the upland areas of Exmoor. The potential for the occurrence of stratiform sulphide deposits in North Devon and the Central Area has been assessed by GIS-based prospectivity analysis software, Arc-Spatial <b>Data</b> <b>Modeller</b> (Arc-SDM), using knowledge-driven and data-driven modelling techniques. This analysis used both new and legacy multivariate datasets including geophysics (aeromagnetic and gravity), geochemical data, mineral occurrences, and digital 1 : 50 000 geological linework. This has confirmed known occurrences as wel...|$|E
40|$|The Bayesian {{probability}} model, {{using the}} weight-of-evidence method, {{was applied to}} the task of evaluating landslide susceptibility using GIS data (Arc Spatial <b>Data</b> <b>Modeller).</b> The location chosen for the study was the Itajaí Valley, SC in Brazil, which suffered substantial landslide damage following heavy rain in 2008. Landslide locations were identified in the study area from the interpretation of aerial photographs and satellite images, and a spatial database was extracted from ASTER GDEM (Global Digital Elevation Model), geological map and LANDSAT imagery. The factors that influence landslide occurrence, such as slope, aspect, curvature plan and profile, direction flow, topographic wetness index and stream power index of topography, were calculated from ASTER imagery. Lithology and NDVI were extracted from a geological database (CPRM) and LANDSAT imagery. The spatial association between the factors and the landslides was calculated as the contrast values, W+ and W- using the weights-of-evidence model. Tests of conditional independence were performed for the selection of the factors, allowing the large number of combinations of factors to be analyzed. Additionally, the receiver operating characteristics (ROC) curves for all landslide susceptibility models were calculated. Landslide locations were used to validate results of the landslide susceptibility map generated using the weight-of-evidence approach and the verification results showed a 74, 56 % accuracy. Conditional independency and performance tests indicate that the landslide susceptibility map yielded for the study area shows a suitable level of accuracy. Pages: 6010 - 601...|$|E
40|$|The {{application}} of diffuse pollution models included in EUROHARP encompassed {{varying levels of}} parameterisation and approaches to the preparation of input data depending on the model and modelling team involved. Modellers consistently faced important decisions in relation to data interpretation, especially in those catchments with unfamiliar physical or climatic characteristics, where catchment conditions were beyond the range for which a particular model was originally developed, or where only limited input data were available. In addition to a broad discussion of data issues, this paper compares {{the performance of the}} four sub-annual output models tested in EUROHARP (EveNFlow, NL-CAT, SWAT and TRK) in three test catchments without the modelling teams having sight of measured flow and nitrate concentration data. Model performance in this ""blind test"" indicate that the range of predictions generated by any individual models pre and post calibration exceed the differences between the estimates yielded by all four models. Comparison of Analysis of Variance (ANOVA) statistics for simulated and observed flow, concentration and loads underscores the benefits of calibration for these intermediate and complex model formulations. Interpretation of input data (e. g. rainfall interpolation method and pedotransfer functions selected) appeared equally (or more) important than process representation. In the absence of calibration <b>data,</b> <b>modeller</b> unfamiliarity with a particular catchment and its environmental processes sometimes resulted in questionable assumptions and input errors which highlight the problems facing modellers charged with implementing policies under the Water Framework Directive (2000 / 60 /EC) in poorly monitored catchments. Catchment data owners and modellers must therefore work more closely given that the output from diffuse pollution models is clearly modeller-limited as well as model-limited. © 2009 The Royal Society of Chemistry...|$|E
40|$|International audienceRegional chemistry-transport {{models are}} used for {{atmospheric}} composition studies in several contexts : analysis of past events, scenarios studies, trends or forecast. Modelled concentrations are sensitive to many inputs data like anthropogenic surface emissions of NOx, VOCs and particulate matter. These emissions are provided as annual masses of pollutants for several activity sectors and projected onto a spatial grid. To use these <b>data,</b> <b>modellers</b> must make important assumptions in order to estimate pollutants fluxes for their own model grid and time frequency. Among these hypotheses, the time resolution is crucial {{and the way to}} redistribute emissions from annual to hourly fluxes determines the modelled concentrations accuracy. The usual CTMs approaches handle the time distribution with averaged factors. The present study quantifies with the chemistry-transport model CHIMERE the benefit of improving the calculation of traffic emissions fluxes by using hourly NO 2 measurements nearby roadside areas as a proxy of road traffic sources. This work shows very different diurnal variation of emissions from country to country and suggests the use of a new hourly emission factor dataset for various countries. The induced changes are quantified for ozone, nitrogen dioxide and particulate matter surface concentrations over the whole Europe during the summer 2007. It is shown that the daily ozone peak remains relatively insensitive to this improvement whereas the pollutants concentrations during nighttime are closer to the measurements with the new profiles...|$|R
40|$|Within {{the working}} {{lives of many}} of our readers, little short of a {{scientific}} revolution has taken place. The first activity after graduation for one of us (MWC) was in using an English Electric Digital Electronic Universal Computing computer (DEUCE) to undertake design optimization studies for Magnox nuclear reactor systems. In terms of computing history, DEUCE was a venerable design; English Electric Co Ltd introduced it as a commercial version of the Pilot Automatic Computing Engine (ACE), which was a smaller version of Turing’s ACE (DEUCE, 2011). But what was a revelation to a new graduate is, looking back, archaic in the extreme. Perhaps, our reactions are dulled by familiarization, but the advances of computational facilities are in fact breathtaking and continually opening up new worlds of mathematical and numerical modelling. Further, {{if this is what}} constitutes a revolution that revolution should be seen as including the parallel developments in experimental capabilities. Nowhere is this more evident than in the field of non-invasive optical methods and their use of lasers. This may readily be appreciated by, say, viewing typical copies of Laser Focus World and Bio-Optics World, research-oriented trade magazines published by the Pennwell Corporation. We need to adjust our thinking sharply downscale, for example, in biomedicine, even to intracellular processes. So, whether in the push provided by computational power, or the pull generated by new ultrascale experimental <b>data,</b> <b>modellers</b> have to investigate smaller and smaller scales...|$|R
40|$|FRIEND (Flow Regimes from International Experimental and Network <b>Data)</b> Water <b>modellers</b> are {{commonly}} {{faced with a}} range of dilemmas due to the complex, uncertain and conflicting nature of the problems currently studied. The limitations of present techniques to deal with the variability and non-homogeneity of future data sets in complex water systems are examined. The main limitations are in part due to changing human behaviour and linked anthropogenic land- and water-use impacts, as well as the uncertainty of climatic variability. Suggestions and questions for future practice are raised, as are technical based methods which are more likely to provide successful outcomes for integrated river basin management. [URL]...|$|R
40|$|Habitat {{loss and}} {{fragmentation}} are major threats to biodiversity. Geostatistical methods, especially kriging, {{are widely used}} in ecology. Bird counts data often fail to show normal distribution over an area which is required {{for most of the}} kriging methods. Hence choosing an interpolation method without understanding the implications may lead to bias results. United Kingdom’s Exprodat Consulting Ltd had set an Exploratory Spatial Data Analysis (ESDA) workflow for optimising interpolation of petroleum dataset. This workflow was applied in this study to predict capercaillie bird species over whole Sweden. There was no trend found in the dataset. Also the dataset was not spatially auto-correlated. A completely regularized spline surface model was created with RMSE 1. 336. Medium to high occurrences (8 - 16) were found over two very small areas, within Västerbottens county and Västra Götlands county. Low occurrences (1 - 3) were found all over Sweden. Urban areas like Stockholm city and Malmö city had low occurrences. Another kriging prediction surface was created with RMSE 1. 314 to compare the results. There were no prediction values from 5 to 16 in kriging surface. In-depth studies were carried out by selecting three areas. The studies showed that the results of local kriging surfaces did not match with the results of global surface. Uncertainty in GIS may exist at any level. Having low RMSE value does not always mean a good result. Hence ESDA before choosing interpolation method is an effective way. And a post result field investigation could make it more valid. Regression analysis is also widely used in ecology and there are certain different methods that are available to be used. Ordinary Least Squares is the first method that was tested upon bird counts data set. Adjusted R-squared value was 0. 008616 which indicated that explanatory variables pine, spruce, roads, urban areas and wetlands were just contributing to 0. 8 % to the dependent variable bird counts. It was also found that there was no linear relationship between dependent and explanatory variables. Logistic regression was the next step as it had the capability to work with nonlinear data also. The Spatial <b>Data</b> <b>Modeller</b> (SDM) tool was used to perform logistic regression in ArcGIS 9. 3. Initially results of logistic regression were unexpected, hence focal statistics was performed upon all the independent variables. Logistic regression with these new independent variables generated meaningful results. This time the probability of occurrence of birds had weak positive relationship with all the independent variables. Coefficients of pine, spruce, roads, urban areas and wetlands were found to be 0. 39, 0. 23, 0. 13, 0. 24 and 0. 14 respectively. Pine and spruce are natural attractors for birds, hence results were quite acceptable. But the overall model performance remained poor. Positive coefficient for roads, urban areas and wetlands may well be due to redundancy in these datasets or observer bias in bird species reporting. IDRISI Andes also came up with almost the same results when logistic regression with same dependent and independent variables was performed. IDRISI Andes output contained the pseudo R-square value, found to be 0. 0416. This was an indication of biasness in the dataset also. The results of in-depth studies by selecting three areas also showed that LR with focal statistics were having better results than LR without focal statistics, but the overall performance remained poor. The SDM tool is a good choice for performing logistic regression on small scale datasets due to its limitation. Comparison of results between the two geostatistical methods, interpolation and regression depicts the similarity at discrete places; an unbiased dataset might have resulted in a better comparison of two methods...|$|E
40|$|The Dalradian terrane of {{north-western}} Northern Ireland is {{an attractive}} target for orogenic vein mineralisation. The current high price of gold and the release in 2007 of high-quality regional geochemical and geophysical datatsets from the Tellus project have stimulated a marked revival {{of interest in this}} area. The results of the Tellus project, funded chiefly by the Department of Enterprise Trade and Investment (DETI), have provided major new insights into the geology and mineral potential of Northern Ireland. Significant vein type orogenic gold mineralisation is known at several localities within this region. Previous studies of the most important deposits at Curraghinalt and Cavanacaw, and a review of similar mineralisation elsewhere in the world, allowed the definition of deposit models to underpin systematic prospectivity analysis of the Dalradian terrane in north-western Northern Ireland. Based on these models key exploration criteria were identified and, where appropriate data were available, these were extracted from various multivariate datasets (geology, geochemistry, geophysics and mineral occurrences). These criteria were then assigned significance weightings, zones and styles of influence based on the exploration model and the views of the team undertaking the analysis. A knowledge-based prospectivity analysis using Fuzzy Logic modelling was then applied to map the favourability for the occurrence of deposits of this type within the Dalradian terrane. ESRI’s Arc Spatial <b>Data</b> <b>Modeller</b> version 3. 1 (Arc SDM) software was employed for this purpose. The key exploration criteria are certain structural vectors and elevated values for gold and associated elements in Tellus stream-sediment geochemical data. On account of the apparent differences in the importance of structures of various orientations at Curraghinalt and Cavanacaw, and the contrast in the geochemicalmineralogical characteristics of the ores, two separate prospectivity models were applied in this study, one for Curraghinalt and the other for Cavanacaw. Mineral occurrence information from the GSNI MINLOCS database and the occurrence of gold grains observed during panning of stream sediments were also incorporated in the analysis. Regional prospectivity analysis, covering an area of 3074 km 2, using the two models identified 22 prospective targets for orogenic vein style gold mineralisation disposed in four groups. Many of the targets coincide with known orogenic gold occurrences while others occur in areas identified as prospective by previous workers. However in some areas the size of the targets has increased and elsewhere new targets have been identified beyond the main Lack – Curraghinalt zone. Of particular interest is a series of targets trending west-north-west, which passes through Curraghinalt and Golan Burn, which may relate to the major fault bounding the southern side of the Newtownstewart Basin. Another series of targets, disposed along a near-linear trend with a similar orientation, is identified {{to the north of the}} Omagh-Kesh Basin. A possible interpretation of these results is that both sets of targets are related to major structural features related to a westward extension of the Curraghinalt lateral ramp which is widely regarded as a critical control on the location of the Curraghinalt deposit. The two prospectivity models were also applied to a sub-area of the terrane, the Newtownstewart map sheet, covering 560 km 2. This analysis incorporated structural vectors derived from a more detailed evaluation of the Tellus geophysical datasets and from the revised 1 : 50 000 scale geological map for the Newtownstewart sheet published in 2007 which incorporated modified linework based in part on examination of the Tellus data. Comparison of the results of the regional and detailed analyses shows that, in the detailed analysis, the addition of structural data in areas devoid of such information in the regional analysis has led to an increase in the size of the targets identified. The detailed analysis has also identified extensive additional target areas, including zones of very high prospectivity, in the south-eastern quarter of the map sheet. This study has clearly demonstrated the value of knowledge-based prospectivity analysis in the Dalradian terrane of north-western Northern Ireland. The regional Tellus geochemical and geophysical datasets are critical to the application of this methodology. Further improvement of the technique is possible through implementation of a range of measures. The potential benefits from the use of a sample catchment approach to the geochemical drainage data should be investigated. A more detailed and comprehensive database of bedrock mineral occurrences would help validation of the prospectivity results and would also potentially allow the use of data-driven methods of analysis, removing the subjectivity inherent in knowledge-based methods. Addition of further high-resolution datasets would improve the utility of this approach for follow-up exploration...|$|E
3000|$|Hybrid eco-physiological/mensurational {{models of}} forest {{production}} generally require monthly meteorological estimates at local {{points in the}} landscape as inputs. Where to obtain these estimates {{and how best to}} localise them are important questions for <b>modellers.</b> <b>Data</b> collected from nine independent meteorological stations were compared with estimates from the nearest grid points of the Virtual Climate Station Network created by the New Zealand National Institute of Water and Atmospheric Research (NIWA) and also to estimates from NIWA’s nearest actual meteorological stations.|$|R
40|$|Today’s <b>data</b> {{analysts and}} <b>modellers</b> {{are in the}} luxurious {{position}} {{of being able to}} more closely describe, estimate, predict and infer about complex systems of interest, thanks to ever more powerful computational methods but also wider ranges of modelling distributions. Mixture models constitute a fascinating illustration of these aspects: while within a parametric family, they offer malleable approximations in non-parametric settings; although based on standard distributions, they pose highly complex computational challenges; and they are both easy to constrain to meet identifiability requirements and fall within the class of ill-posed problems. This note aims to introduce the reader to Bayesian modelling and inference difficulties on mixtures of distributions...|$|R
40|$|The CoCoMac {{database}} (Collations of Connectivity {{data on the}} Macaque brain) {{contains the}} results from hundreds of publications which report axonal tracing experiments in the Macaque brain. Combining these studies is a daunting task mainly due to the countless variations in brain region definitions {{that have been used}} across labs and throughout the past decades. The end goal is to output a connectivity matrix for a user-specified set of brain regions, and thereby map the legacy data onto these regions. I will show that this mapping process is surprisingly unreliable when applied to the raw CoCoMac data, but that this can be resolved with the aid of graphical tools. We are currently ingesting data from the Markov/Kennedy publications into CoCoMac, and the purpose of the visit is {{to bridge the gap between}} <b>data</b> consumers (computational <b>modellers)</b> and <b>data</b> producers (experimental facilities) by discussing citation and data sharing policies...|$|R
40|$|International audienceRisk {{assessment}} for groundwater resources requires {{most of the}} time the use of numerical models. These models are used to describe the migration and the transport of pollutants and to predict the contaminant evolution. the models and their parameters selection depend on the extension and the quality of the diagnosis <b>data.</b> the <b>modeller</b> has to know the limits of application of the modelling approach to improve the quality of the assessment. the research program TRANSPOL was undertaken in order to bring a better and a common practice of the use of transport models for various groups of pollutants with similar behaviour (PAH, metals, chlorine solvents. PAHs have a specific behaviour due to their strong affinity for the organic matter, their low solubility, and their density. These compounds are often associated with other contaminants and they are the major pollutants of disused coke plants, wood preservative treatment plants and ancient gas production plants. PAHs can also be found in the environment after major incidents or spills (fuels may indeed contain significant PAHs concentration). the particular behaviour of those compounds and their present and past uses justify to study the transport of the soluble and non-soluble phases. Naphthalene was especially studied with two complementary approaches. Naphthalene is used commonly as a tracer of pollution for the PAHs contamination due to its high solubility and its low capacity of adsorption, which increase its mobility...|$|R
40|$|This {{article was}} {{published}} in the Journal of the Operational Research Society [© Palgrave Macmillan] at: [URL] paper explores the model development process in discrete-event simulation (DES) by reporting on an empirical study that follows six expert modellers while building simulation models. DES is a widely used modelling approach, however {{little is known about the}} modelling processes and methodology adopted by modellers in practice. Verbal Protocol Analysis is used to collect data, where the participants are asked to speak aloud while modelling. The results show that the expert modellers spend a significant amount of time on model coding, verification & validation and <b>data</b> inputs. The <b>modellers</b> iterate often between modelling activities. Patterns of modelling behaviour are identified, suggesting that the modellers adopt distinct modelling styles. This study is useful in that it provides an empirical view of existing DES modelling practice, which in turn can inform existing research and simulation practice as well as teaching of DES modelling to novices...|$|R
40|$|ABSTRACT-This paper {{evaluates the}} present {{role of the}} XBT program and proposes a {{strategy}} for the future {{under the assumption that}} there are other direct and indirect contributions to sampling the temperature and salinity of the ocean. Since the focus is on XBT sampling the paper restricts its scope to the upper ocean, mostly above 1000 m. The conclusions of the paper are based on a study and workshop that were convened specifically to look at the design of the ship-of-opportunity network and to look at options for its implementation in the future under the assumption that Argo happens. The paper also addresses issues related to data distribution and management. The primary conclusion is that the network of the future should place greatest emphasis on line sampling, at intermediate to high densities, and assume that a proposed profiling float array, Argo will largely take over the role formerly occupied by area (broadcast) sampling. It is argued that line sampling exclusively addresses several needs of the ocean observing system that cannot easily be addressed by other forms of sampling. Further it is argued that such a mode complements other in situ components such as moorings and floats as well as remotely sensed surface topography. A new network is outlined with a strategy for implementation that ensures continuity between existing and planned networks. We conclude the data management system that was established around the SOOP program requires substantial renovation if it is to adequately address the needs of the data gatherers and suppliers, and the <b>data</b> users (<b>modellers,</b> scientists, operational applications) ...|$|R
40|$|Abstract: Environmental {{datasets}} grow in {{size and}} specialization while models designed for local scale are often unsuitable at regional/continental scale. At regional scale, data are usually available as georeferenced collections of spatially distributed despite semantically atomic information. Complex <b>data</b> intrinsically impose <b>modellers</b> to manipulate nontrivial information structures. For example, multi-dimensional arrays of time series may be composed by slices of raster spatial matrices for each time step, whilst heterogeneous collections of uneven arrays are common when dealing with data analogous to precipitation events, and these structures may ask for integration at several spatial scales, projections and temporal extents. Interestingly, it might be far more difficult to practically implement such a complexity rather than conceptually describe it: a subset of modelling generalizations may deal more with abstraction rather than with the explosion of lines of code. Many environmental modelling algorithms are composed by chains of data-transformations or trees of domain specific sub-algorithms. Concisely expressing them {{without the need for}} paying attention to the enormous set of spatio-temporal details is a highly recommendable practice in both mathematical formulatio...|$|R
40|$|As {{with most}} {{fractured}} rock formations, Chalk is highly heterogeneous. Therefore, meaningful estimates of model parameters must be obtained at a scale comparable {{with the process}} of concern. These are frequently obtained by calibrating an appropriate model to observed concentration-time data from radially convergent tracer tests (RCTT). Arguably, an appropriate model should consider radially convergent dispersion (RCD) and Fickian matrix diffusion. Such a model requires the estimation of at least four parameters. A question arises {{as to whether or not}} this level of model complexity is supported by the information contained within the calibration <b>data.</b> Generally <b>modellers</b> have not answered this question due to the calibration techniques employed. A dual-porosity model with RCD was calibrated to two tracer test datasets from different UK Chalk aquifers. A multivariate sensitivity analysis, which assumed only a priori upper and lower bounds for each model parameter, was undertaken. Rather than looking at measures of uncertainty, the shape of the multivariate objective function surface was used to determine whether a parameter was identifiable. Non-identifiable parameters were then removed and the procedure was repeated until all remaining parameters were identifiable. It was found that the single fracture model (SFM) (which ignores mechanical dispersion) obtained the best mass recovery, excellent model performance and best parameter identifiability in both the tests studied. However, there was no objective evidence suggesting that mechanical dispersion was negligible. Moreover, the SFM (with just two parameters) was found to be good at approximating the Single Fracture Dispersion Model SFDM (with three parameters) when different, and potentially erroneous parameters, were used. Overall, this study emphasises the importance of adequate temporal sampling of breakthrough curve data prior to peak concentrations, to ensure adequate characterisation of mechanical dispersion processes, and continued monitoring afterwards, to ensure adequate characterisation of fracture spacing (where possible), when parameterising dual-porosity solute transport models...|$|R
40|$|In 2012, the hydrogeological model (HM) LAMO of Latvia {{has been}} {{developed}} by scientists of Riga Technical University (RTU). LAMO comprises an active groundwater zone that provides drinking water. 3 D-body of LAMO is approximated by the 951 × 601 × 25 size finite difference grid. Its plane approximation step is 500 metres and HM accounts for 25 geological layers. The commercial software Groundwater Vistas (GV) is used to run LAMO. The GV system contains tools used worldwide for hydrogeological modeling. The most time consuming and troublesome part of developing a HM is to designate its geometry that is represented by thicknesses of geological layers composing the HM body. For LAMO, most of these layers are outcropping. They are not continuous and, for this reason, they are not present everywhere {{in the area of}} the HM. After emerging at the surface, such layers have zero thickness. Thicknesses of the layers are not used by GV as initial <b>data.</b> A <b>modeller</b> must prepare the set of z-maps that presents elevation surfaces of the layers. The GV system uses z−maps for obtaining thicknesses of the layers. To prepare z-maps for primary geological layers, three steps of data interpolation were performed: 1. selecting and checking borehole data that describe stratigraphy of the geological environment; the EXCEL system was applied, to carry out this step; 2. by using the graphical system SURFER, the preliminarily set of z-maps was prepared; 3. the final version of z-maps was obtained by the Geological Data Interpolation (GDI) system. It was rather difficult to create z-maps for the Quaternary layers, because borehole information describing their geometry was waste and contradictory. For this reason, mainly pointwise data and a few lines were applied as sources for obtaining z-maps of the Quaternary System...|$|R
40|$|International audienceAs {{with most}} {{fractured}} rock formations, Chalk is highly heterogeneous. Therefore, meaningful estimates of model parameters must be obtained at a scale comparable {{with the process}} of concern. These are frequently obtained by calibrating an appropriate model to observed concentration-time data from radially convergent tracer tests (RCTT). Arguably, an appropriate model should consider radially convergent dispersion (RCD) and Fickian matrix diffusion. Such a model requires the estimation of at least four parameters. A question arises {{as to whether or not}} this level of model complexity is supported by the information contained within the calibration <b>data.</b> Generally <b>modellers</b> have not answered this question due to the calibration techniques employed. A dual-porosity model with RCD was calibrated to two tracer test datasets from different UK Chalk aquifers. A multivariate sensitivity analysis, which assumed only a priori upper and lower bounds for each model parameter, was undertaken. Rather than looking at measures of uncertainty, the shape of the multivariate objective function surface was used to determine whether a parameter was identifiable. Non-identifiable parameters were then removed and the procedure was repeated until all remaining parameters were identifiable. It was found that the single fracture model (SFM) (which ignores mechanical dispersion) obtained the best mass recovery, excellent model performance and best parameter identifiability in both the tests studied. However, there was no objective evidence suggesting that mechanical dispersion was negligible. Moreover, the SFM (with just two parameters) was found to be good at approximating the Single Fracture Dispersion Model SFDM (with three parameters) when different, and potentially erroneous parameters, were used. Overall, this study emphasises the importance of adequate temporal sampling of breakthrough curve data prior to peak concentrations, to ensure adequate characterisation of mechanical dispersion processes, and continued monitoring afterwards, to ensure adequate characterisation of fracture spacing (where possible), when parameterising dual-porosity solute transport models...|$|R
40|$|International audienceFor {{hydrological}} modelling {{studies at}} the river basin scale, decision makers need guidance in assessing the implications of uncertain <b>data</b> used by <b>modellers</b> as an input to modelling tools. Simulated solute transport through the unsaturated zone is associated with uncertainty due to spatial variability of soil hydraulic properties and derived hydraulic model parameters. In general for modelling {{studies at the}} river basin scale spatially available data at various scales must be aggregated to an appropriate scale. Estimating soil properties at unsampled points by means of geostatistical techniques require reliable information on the spatial structure of soil data. In this paper this information is assessed by reviewing current developments {{in the field of}} soil physical data uncertainty and adopting a classification system. Then spatial variability and structure is inspected by reviewing experimental work on determining spatial length scales for soil physical (and soil chemical) data. Available literature on spatial length scales for soil physical- and chemical properties is reviewed and their use in facilitating change of spatial support discussed. Uncertainty associated to the derivation of hydraulic properties from soil physical properties in this context is also discussed...|$|R
40|$|This paper {{highlights}} {{the value of}} the relationship between Griffith Centre for Coastal Management and DHI Water and Environment and their development of a robust, coastal data collection program. This program aligns the needs of the coastal modeller with the methodology and data collection to ensure desired outcomes are achieved. Coastal managers rely heavily on the use of computer models to both understand and predict the behaviour of the ocean and coastal waterways. Computer models are a very useful tool, however can be limited by {{the quality and quantity of}} the field data incorporated into the model. A strong relationship between those collecting the <b>data</b> and <b>modellers</b> applying the <b>data</b> is important to ensure that requirements are met and reliable models are developed. Griffith Centre for Coastal Management and DHI Water and Environment are collaborating on a series of projects with the final aim of developing robust, computer models. The Seaway SmartRelease Project involved the development of a hydrodynamic and advection-dispersion model using the MIKE suite of software. These were developed to model and predict the extent of the recycled water plume on entry into the Gold Coast Seaway. Extensive data was collected over three field trips including up to 11 hours of water quality samples and a series of current data profiles collected across the Seaway Channel. Additionally short term current data was collected through the deployment of a series of acoustic Doppler current profilers within the Seaway channel. The team of researchers and modellers worked closely to ensure the most appropriate data collection methods were employed. With the success of this award winning project, GCCM and DHI are now working together to develop a coastal process model, using MIKE 21 and LITPACK, for specific sections of the Gold Coast's beaches. This will include a sediment transport model and a wave model, both focussing on the nearshore zone where limited data is currently available. No Full Tex...|$|R
40|$|Spatial {{analysis}} and modelling tools in GIScience have limitations in accurately representing environmental properties and processes at various scales. Often the way environmental data was gathered, processed, and modelled {{in the digital}} domain is insufficient to represent the true natural temporal and spatial variability. This research {{is an attempt to}} implement a newly developed data scaling scheme in a GIS and Environmental Modelling setting that consequently track the flow and optimize the transfer of information by minimizing uncertainty in the digital domain. Our case study is a process-based, continuous, spatially distributed runoff and soil erosion modelling approach for the semiarid rangeland environment of the Lucky Hills Watershed in Arizona. The Water Erosion Prediction Project (WEPP) model is a representative hillslope and watershed model that requires the aggregation of spatial information before a model run. As an alternative, we have developed two simulation methods that enable taking advantage of more spatially detailed information about the terrain. However, it has to be determined, if we necessarily achieve better results in using either of the three model simulations. The experience of coordinating scaling theory, implementation procedure, and application indicate that <b>Data</b> Collectors, GIScientists, <b>Modellers,</b> and Decision Makers have to effectively communicate and understand the importance of aggregating and disaggregating information to validate and produce useful simulation results...|$|R
40|$|Simplistic {{economic}} objectives such as maximisation of producer {{profits are}} of little relevance in generating information {{to assist in}} the management of natural resources beyond the individual firm level. To provide data and information to support decision-making in natural resource management, it is necessary {{to take into account the}} views of various stakeholder groups and the multiple objectives of each group, through the use of some form of multicriteria analysis (MCA). Important decisions arise in the choice of stakeholder, since this will influence the management advice generated. Many groups and individuals can be affected by resource management decisions, but it would be impractical to attempt to identify the objectives and estimate their importance for each group. Also, questions arise concerning whether or not to include government agencies (which represent the broader community) and researchers as stakeholders. A further issue concerns choosing representative samples of stakeholder groups, from which to obtain preference <b>data.</b> Discussions with <b>modellers</b> and a reading of the literature would suggest that the choice of stakeholder groups and representatives is conducted haphazardly and is perhaps biased, and that a more systematic approach is needed. This article explores the above issues with reference to a number of multicriteria analyses, including local studies. (C) 2000 United Nations. Published by Elsevier Science Ltd. All rights reserved...|$|R
40|$|This work {{presents}} a Feature Recognition system developed using a previously trained Artificial Neural Network. The part description {{is taken from}} a B-rep solid <b>modeller's</b> <b>data</b> base. This description refers only to topological information about the faces in the part {{in the form of}} an Attributed Adjacency Graph. A set of heuristics is used for breaking down this compound feature graph into subgraphs, that correspond to simple features. Special representation patterns are then constructed for each of these subgraphs. These patterns are presented to a Neural Network which classifies them into feature classes: pockets, slots, passages, protrusions, steps, blind slots, corner pockets, and holes. The scope of instances/variations of these features that can be recognised is very wide. A commercially available neural network modelling tool was used for training. The user interface to the neural network recogniser has been written in Pascal. The program can handle parts with up to 200 planar or curved faces. The performance of the recogniser in terms of speed is far better than that of any other rule-based system due to the Neural Network approach employed. The basic limitation is that of the heuristics used to break down compound features into simple ones which are fed to the ANN, but this is still a step ahead compared to other approaches. (C) 1997 Elsevier Science Ltd...|$|R
40|$|Understanding {{the effects}} of {{different}} types and quality of data on bioclimatic modeling predictions is vital to ascertaining the value of existing models, and to improving future models. Bioclimatic models were constructed using the CLIMEX program, using different data types – seasonal dynamics, geographic (overseas) distribution, and {{a combination of the}} two – for two biological control agents for the major weed Lantana camara L. in Australia. The models for one agent, Teleonemia scrupulosa Stål (Hemiptera: Tingidae) were based on a higher quality and quantity of data than the models for the other agent, Octotoma scabripennis Guérin-Méneville (Coleoptera: Chrysomelidae). Predictions of the geographic distribution for Australia showed that T. scrupulosa models exhibited greater accuracy with a progressive improvement from seasonal dynamics data, to the model based on overseas distribution, and finally the model combining the two data types. In contrast, O. scabripennis models were of low accuracy, and showed no clear trends across the various model types. These case studies demonstrate the importance of high quality data for developing models, and of supplementing distributional data with species seasonal dynamics data wherever possible. Seasonal dynamics <b>data</b> allows the <b>modeller</b> to focus on the species response to climatic trends, while distributional data enables easier fitting of stress parameters by restricting the species envelope to the described distribution. It is apparent that CLIMEX models based on low quality seasonal dynamics data, together with a small quantity of distributional data, are of minimal value in predicting the spatial extent of species distribution...|$|R
