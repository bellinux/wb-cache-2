5682|1716|Public
25|$|Prioritized {{test case}} generation: It is {{possible}} to assign weights to {{the elements of the}} classification tree in terms of occurrence and <b>error</b> <b>probability</b> or risk. These weights are then used during test case generation to prioritize test cases. Risk-based and statistical testing is also available.|$|E
25|$|India's first laser-guided bomb, Sudarshan is {{the latest}} weapon system {{developed}} indigenously to occupy the niche of a precision delivery mechanism. It can be fitted to a 1000-pound gravity bomb and can guide it to the target using lasers with a CEP (Circular <b>Error</b> <b>Probability)</b> of 10 metres.|$|E
25|$|A simple {{error model}} is to {{introduce}} a small error to the data probability term in the homozygous cases, allowing a small constant probability that nucleotides which don't match the A allele are observed in the AA case, and respectively a small constant probability that nucleotides not matching the B allele are observed in the BB case. However more sophisticated procedures are available which attempt to more realistically replicate the actual error patterns observed in real data in calculating the conditional data probabilities. For instance, estimations of read quality (measured as Phred quality scores) have been incorporated in these calculations, {{taking into account the}} expected error rate in each individual read at a locus. Another technique that has successfully been incorporated into error models is base quality recalibration, where separate error rates are calculated - based on prior known information about error patterns - for each possible nucleotide substitution. Research shows that each possible nucleotide substitution is not equally likely to show up as an error in sequencing data, and so base quality recalibration has been applied to improve <b>error</b> <b>probability</b> estimates.|$|E
3000|$|Finally, {{it can be}} {{seen from}} Figure 11, that for very high <b>error</b> <b>probabilities,</b> (13) leads to an underestimation, while for lower <b>error</b> <b>probabilities</b> q [...]...|$|R
40|$|Abstract. The article used Minimization of K-L Information in the Hypothesis Testing,the {{convergent}} {{problems for}} the <b>error</b> <b>probabilities</b> of likelihood ratio were deeply discussed. The results show that for the <b>error</b> <b>probabilities</b> of likelihood ratio is power series convergent under the Minimization of K-L Information...|$|R
40|$|This paper {{considers}} {{the problem of}} estimating the mean of a Poisson distribution when there are errors in observing the zeros and ones and obtains both the maximum likelihood and moments estimates of the Poisson mean and the <b>error</b> <b>probabilities.</b> It {{is interesting to note}} that either method fails to give unique estimates of these parameters unless the <b>error</b> <b>probabilities</b> are functionally related. However, it is equally interesting to observe that the estimate of the Poisson mean does not depend on the functional relationship between the <b>error</b> <b>probabilities...</b>|$|R
500|$|As for guidance, {{the basic}} version {{combines}} {{data from a}} Global Positioning System (GPS) receiver and an inertial navigation system (INS) unit through Kalman filtering, achieving a [...] circular <b>error</b> <b>probability</b> (CEP). This [...] "decametric" [...] all-weather variant is complemented by a [...] "metric" [...] day/night fair weather version which adds infrared homing (IIR) guidance that matches the target area with a target model stored in its memory for a [...] CEP.|$|E
500|$|In April 1967, the Australian {{government}} committed No.2 Squadron and its Canberra bombers {{to action}} in the Vietnam War. Operating from Phan Rang Air Base outside Saigon, {{under the direction of}} the US 35th Tactical Fighter Wing (TFW), the Canberras were initially engaged in medium-altitude missions against Viet Cong forces, guided by Sky Spot ground radar, usually at night. Promoted to wing commander, Evans assumed control of the squadron in December 1967. Having never heard a shot fired in anger in his 24 years of service, he was anxious for a combat assignment. The Japanese surrender in August 1945 had prevented him from seeing action in World WarII, and the Korean War had ended just as he was on the verge of a posting for active duty with No.77 Squadron. [...] "Vietnam", he reasoned, [...] "would be my last chance". By the time he took command, the Canberras were flying a greater proportion of their missions at lower levels in daylight, using visual bomb-aiming methods honed during their earlier service in Malaysia; this gave the bombers an average circular <b>error</b> <b>probability</b> (CEP) of 50metres. Evans introduced intensive post-mission analysis to refine their technique, and permitted his pilots to bomb at the lowest level possible at which the bombsight would operate. The CEP was eventually reduced to 20metres, making the Canberras the most accurate bombing force in the region. In January 1968, the unit participated in the air campaigns to defend Huế and Khe Sanh during the Tet Offensive. Phan Rang itself was often subjected to harassing attacks and mortar fire from the Viet Cong, requiring Evans to undertake improvements to the airfield's ground defences. He completed his posting to Vietnam in November 1968 and was awarded the Distinguished Service Order for his performance as commanding officer of No.2 Squadron. The decoration was gazetted on 2May 1969 and backdated to 13March.|$|E
2500|$|The bound 1/2 on the <b>error</b> <b>probability</b> of {{a single}} round of the Solovay–Strassen test holds for any input n, but those numbers n for which the bound is (approximately) {{attained}} are extremely rare. On the average, the <b>error</b> <b>probability</b> of the algorithm is significantly smaller: it is less than ...|$|E
30|$|Calculate the <b>error</b> <b>probabilities</b> of {{the video}} chunks using {{expression}} (2).|$|R
40|$|In theory, quantum {{computers}} can efficiently simulate quantum physics, factor large numbers and estimate integrals, thus solving otherwise intractable computational problems. In practice, quantum computers must operate with noisy devices called ``gates'' {{that tend to}} destroy the fragile quantum states needed for computation. The goal of fault-tolerant quantum computing is to compute accurately even when gates have a high <b>probability</b> of <b>error</b> each time they are used. Here we give evidence that accurate quantum computing is possible with <b>error</b> <b>probabilities</b> above 3 % per gate, which is significantly higher than what was previously thought possible. However, the resources required for computing at such high <b>error</b> <b>probabilities</b> are excessive. Fortunately, they decrease rapidly with decreasing <b>error</b> <b>probabilities.</b> If we had quantum resources comparable to the considerable resources available in today's digital computers, we could implement non-trivial quantum computations at <b>error</b> <b>probabilities</b> as high as 1 % per gate. Comment: 47 page...|$|R
40|$| distributions, the Type I <b>error</b> <b>probabilities</b> of {{both the}} t test and the|$|R
2500|$|One of {{his results}} is the strong {{converse}} to Claude Shannon's coding theorem. While Shannon could prove {{only that the}} block <b>error</b> <b>probability</b> can not become arbitrarily small if the transmission rate is above the channel capacity, Wolfowitz proved that the block error rate actually converges to one. As a consequence, Shannon's original result is today termed [...] "the weak theorem" [...] (sometimes also Shannon's [...] "conjecture" [...] by some authors).|$|E
2500|$|Recent {{enhancements}} to {{the classification}} tree method include the prioritized test case generation: It {{is possible to}} assign weights to {{the elements of the}} classification tree in terms of occurrence and <b>error</b> <b>probability</b> or risk. These weights are then used during test case generation to prioritize test cases. Statistical testing is also available (e.g. for wear and fatigue [...] tests) by interpreting the element weights as a discrete probability distribution.|$|E
5000|$|Since [...] has at most 0.1 <b>error</b> <b>probability,</b> [...] {{can have}} at most 0.2 <b>error</b> <b>probability.</b>|$|E
40|$|A simple {{combination}} of one-sided sequential probability ratio tests, called a 2 -SPRT, {{is shown to}} approximately minimize the expected sample size at a given point θ 0 among all tests with <b>error</b> <b>probabilities</b> controlled at two other points, θ 1 and θ 2. In the symmetric normal and binomial testing problems, this result applies directly to the Kiefer-Weiss problem of minimizing the maximum over θ of the expected sample size. Extensive computer calculations for the normal case indicate that 2 -SPRT's have efficiencies greater than 99 % regardless {{of the size of}} the <b>error</b> <b>probabilities.</b> Accurate approximations to the <b>error</b> <b>probabilities</b> and expected sample sizes of these tests are given...|$|R
40|$|SUMMARY. Testing of {{hypotheses}} for discrete distributions {{is considered}} in this paper. The {{goal is to}} develop conditional frequentist tests that allow the reporting of datadependent <b>error</b> <b>probabilities</b> such that the <b>error</b> <b>probabilities</b> have a strict frequentist interpretation and also reflect the actual amount of evidence in the observed data. The resulting randomized tests are also seen to be Bayesian tests, in the strong sense that the reported <b>error</b> <b>probabilities</b> are also the posterior probabilities of the hypotheses. new procedure is illustrated {{for a variety of}} testing situations, both simple and composite, involving discrete distributions. Testing linkage heterogeneity with the new procedure is given as an illustrative example. 1...|$|R
40|$|We {{obtain an}} {{explicit}} form of fine large deviation theorems for the log-likelihood ratio in testing models with observed Ornstein-Uhlenbeck processes and get explicit rates of decrease for <b>error</b> <b>probabilities</b> of Neyman-Pearson, Bayes, and minimax tests. We also give expressions for {{the rates of}} decrease of <b>error</b> <b>probabilities</b> of Neyman-Pearson tests in models with observed processes solving affine stochastic delay differential equations. ...|$|R
5000|$|Instead of {{analyzing}} the average <b>error</b> <b>probability,</b> we analyze the expectationof the average <b>error</b> <b>probability,</b> where {{the expectation is}} with respect to therandom choice of code: ...|$|E
5000|$|Note {{that each}} block of code for [...] is {{considered}} a symbol for [...] Now since the probability of error at any index [...] for [...] is at most [...] and the errors in [...] are independent, the expected number of errors for [...] is at most [...] by linearity of expectation. Now applying Chernoff bound, we have bound <b>error</b> <b>probability</b> of more than [...] errors occurring to be [...] Since the outer code [...] can correct at most [...] errors, this is the decoding <b>error</b> <b>probability</b> of [...] This when expressed in asymptotic terms, gives us an <b>error</b> <b>probability</b> of [...] Thus the achieved decoding <b>error</b> <b>probability</b> of [...] is exponentially small as Theorem 1.|$|E
50|$|The bit <b>error</b> <b>probability</b> pe is the {{expectation}} {{value of the}} BER. The BER {{can be considered as}} an approximate estimate of the bit <b>error</b> <b>probability.</b> This estimate is accurate for a long time interval and a high number of bit errors.|$|E
40|$|Let [lambda] > 0 be {{the unknown}} {{parameter}} {{of the common}} exponential distribution of the i. i. d. sequence X 1, X 2 [...] . Exact formulae of a simple from for the <b>error</b> <b>probabilities</b> and {{the average number of}} the SPRT of against H 1 : [lambda] [greater-or-equal, slanted] [lambda] 0 ([lambda] 0 exponential distribution sequential <b>probability</b> ratio test <b>error</b> <b>probabilities</b> average sample number...|$|R
30|$|The {{following}} proposition {{characterizes the}} impact of the threshold and the billing cycle length on the <b>error</b> <b>probabilities.</b>|$|R
3000|$|... with <b>error</b> <b>probabilities</b> {{going to}} zero as N→∞ {{and for some}} {{constant}} τ∈R independent of the channel gains.|$|R
5000|$|Pairwise <b>error</b> <b>probability</b> is the <b>error</b> <b>probability</b> {{that for}} a {{transmitted}} signal (...) its corresponding but distorted version (...) will be received. This type of probability is called ″pair-wise error probability″ because the probability exists {{with a pair of}} signal vectors in a signal constellation. It's mainly used in communication systems.|$|E
5000|$|The bound 1/2 on the <b>error</b> <b>probability</b> of {{a single}} round of the Solovay-Strassen test holds for any input n, but those numbers n for which the bound is (approximately) {{attained}} are extremely rare. On the average, the <b>error</b> <b>probability</b> of the algorithm is significantly smaller: it is less than ...|$|E
5000|$|... the {{technique}} {{allows for the}} direct quantification of human <b>error</b> <b>probability</b> (HEP) ...|$|E
40|$|Abstract. Ronald Fisher {{advocated}} testing using p-values, Harold Jeffreys proposed use {{of objective}} posterior probabilities of hypotheses and Jerzy Neyman recommended testing with fixed <b>error</b> <b>probabilities.</b> Each was quite {{critical of the}} other approaches. Most troubling for statistics and science is that the three approaches can lead to quite different practical conclusions. This article focuses on discussion of the conditional frequentist approach to testing, which is argued to {{provide the basis for}} a methodological unification of the approaches of Fisher, Jeffreys and Neyman. The idea is to follow Fisher in using p-values to define the “strength of evidence ” in data and to follow his approach of conditioning on strength of evidence; then follow Neyman by computing Type I and Type II <b>error</b> <b>probabilities,</b> but do so conditional on the strength of evidence in the data. The resulting conditional frequentist <b>error</b> <b>probabilities</b> equal the objective posterior probabilities of the hypotheses advocated by Jeffreys. Key words and phrases: p-values, posterior probabilities of hypotheses, Type I and Type II <b>error</b> <b>probabilities,</b> conditional testing...|$|R
40|$|Random Boolean {{networks}} (RBNs) were {{proposed as}} a model of genetic regulatory net-works by Kauffman in 1969. This paper addresses the stability of cycles of RBNs and Boolean networks. In particular we define the <b>error</b> <b>probabilities</b> of cycle states as the probability of leaving a cycle after a randomly occurring error. Therefore, we introduce the weight distri-butions, which are usually used to analyse <b>error</b> <b>probabilities</b> of error-correction codes, and show that these completely determine the <b>error</b> <b>probabilities</b> of the cycles. By simulations of RBNs with 20, 17 and 15 state bits and bias of 0. 5 we found that networks with connectivity 2 show less stable attractor cycles compared to RBNs with higher K, namely K ∈ { 3, 4, 5, 6 }. ...|$|R
40|$|Testing of a {{composite}} null hypothesis versus {{a composite}} alternative is con-sidered when {{both have a}} related invariance structure. The goal is to develop conditional frequentist tests that allow the reporting of data-dependent <b>error</b> <b>probabilities,</b> <b>error</b> <b>probabilities</b> that have a strict frequentist interpretation and that reflect the actual amount of evidence in the data. The resulting tests are also seen to be Bayesian tests, in the strong sense that the reported frequentist <b>error</b> <b>probabilities</b> are also the posterior probabilities of the hypotheses under default choices of the prior distribution. The new procedures are illustrated {{in a variety of}} applications to model selection and multivariate hypothesis testing. Key words and phrases. Conditional error probabilities; Bayes factors; Pos-terior probabilities; Default prior distributions; Nested hypotheses; Group in-variance...|$|R
5000|$|Binary {{symmetric}} channel (BSC), {{a discrete}} memoryless channel {{with a certain}} bit <b>error</b> <b>probability</b> ...|$|E
5000|$|It {{is often}} useful to convert between ETX and the packet <b>error</b> <b>probability</b> : ...|$|E
5000|$|Using these figures, {{an overall}} Human <b>Error</b> <b>Probability</b> (HEP) can be {{calculated}} with the formulation provided below: ...|$|E
3000|$|... where Pr(off)=Pr(on)= 1 / 2. In the following, the {{conditional}} <b>error</b> <b>probabilities</b> Pr(on|off) and Pr(off|on) will be analyzed, respectively.|$|R
5000|$|Phred quality scores [...] {{are defined}} as a {{property}} which is logarithmically related to the base-calling <b>error</b> <b>probabilities</b> [...]|$|R
3000|$|By substituting the <b>error</b> <b>probabilities</b> for the PAM-system, we {{can obtain}} the {{corresponding}} QAM-system BERs {{as a function}} of [...]...|$|R
