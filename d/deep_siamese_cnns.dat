0|23|Public
40|$|Deep neural {{networks}} {{have been investigated}} in learning latent representations of medical images, yet {{most of the studies}} limit their approach in a single supervised convolutional neural network (CNN), which usually rely heavily on a large scale annotated dataset for training. To learn image representations with less supervision involved, we propose a <b>deep</b> <b>Siamese</b> <b>CNN</b> (SCNN) architecture that can be trained with only binary image pair information. We evaluated the learned image representations on a task of content-based medical image retrieval using a publicly available multiclass diabetic retinopathy fundus image dataset. The experimental results show that our proposed deep SCNN is comparable to the state-of-the-art single supervised CNN, and requires much less supervision for training. Comment: Presented in NIPS 2017 Workshop on Machine Learning for Health (ML 4 H); add retrieval results; fix typo...|$|R
40|$|Matching {{pedestrians}} {{across multiple}} camera views, known as human re-identification, is a challenging research {{problem that has}} numerous applications in visual surveillance. With the resurgence of Convolutional Neural Networks (<b>CNNs),</b> several end-to-end <b>deep</b> <b>Siamese</b> <b>CNN</b> architectures have been proposed for human re-identification {{with the objective of}} projecting the images of similar pairs (i. e. same identity) to be closer to each other and those of dissimilar pairs to be distant from each other. However, current networks extract fixed representations for each image regardless of other images which are paired with it and the comparison with other images is done only at the final level. In this setting, the network is at risk of failing to extract finer local patterns that may be essential to distinguish positive pairs from hard negative pairs. In this paper, we propose a gating function to selectively emphasize such fine common local patterns by comparing the mid-level features across pairs of images. This produces flexible representations for the same image according to the images they are paired with. We conduct experiments on the CUHK 03, Market- 1501 and VIPeR datasets and demonstrate improved performance compared to a baseline <b>Siamese</b> <b>CNN</b> architecture. Comment: Accepted to ECCV 201...|$|R
40|$|We {{propose a}} new {{pipeline}} for optical flow computation, based on Deep Learning techniques. We suggest using a <b>Siamese</b> <b>CNN</b> to independently, and in parallel, compute the descriptors of both images. The learned descriptors are then compared efficiently using the L 2 norm {{and do not}} require network processing of patch pairs. The success of the method {{is based on an}} innovative loss function that computes higher moments of the loss distributions for each training batch. Combined with an Approximate Nearest Neighbor patch matching method and a flow interpolation technique, state of the art performance is obtained on the most challenging and competitive optical flow benchmarks. Comment: CVPR 201...|$|R
40|$|The {{problem of}} {{automatic}} accent identification {{is important for}} several applications like speaker profiling and recognition {{as well as for}} improving speech recognition systems. The accented nature of speech can be primarily attributed to the influence of the speaker's native language on the given speech recording. In this paper, we propose a novel accent identification system whose training exploits speech in native languages along with the accented speech. Specifically, we develop a <b>deep</b> <b>Siamese</b> network-based model which learns the association between accented speech recordings and the native language speech recordings. The Siamese networks are trained with i-vector features extracted from the speech recordings using either an unsupervised Gaussian mixture model (GMM) or a supervised deep neural network (DNN) model. We perform several accent identification experiments using the CSLU Foreign Accented English (FAE) corpus. In these experiments, our proposed approach using <b>deep</b> <b>Siamese</b> networks yield significant relative performance improvements of 15. 4 percent on a 10 -class accent identification task, over a baseline DNN-based classification system that uses GMM i-vectors. Furthermore, we present a detailed error analysis of the proposed accent identification system...|$|R
40|$|In {{this paper}} we {{describe}} learning of a descriptor {{based on the}} Siamese Convolutional Neural Network (CNN) architecture and evaluate our results on a standard patch comparison dataset. The descriptor learning architecture is composed of an input module, a <b>Siamese</b> <b>CNN</b> descriptor module and a cost computation module {{that is based on}} the L 2 Norm. The cost function we use pulls the descriptors of matching patches close to each other in feature space while pushing the descriptors for non-matching pairs away from each other. Compared to related work, we optimize the training parameters by combining a moving average strategy for gradients and Nesterov's Accelerated Gradient. Experiments show that our learned descriptor reaches a good performance and achieves state-of-art results in terms of the false positive rate at a 95 % recall rate on standard benchmark datasets...|$|R
40|$|Image {{similarity}} involves fetching similar looking images given {{a reference}} image. Our solution called SimNet, is a <b>deep</b> <b>siamese</b> network which is trained on pairs {{of positive and}} negative images using a novel online pair mining strategy inspired by Curriculum learning. We also created a multi-scale CNN, where the final image embedding is a joint representation of top as well as lower layer embedding's. We go on to show that this multi-scale siamese network is better at capturing fine grained image similarities than traditional CNN's. Comment: 9 pages, 6 figures, GHCI 17 conferenc...|$|R
40|$|We {{present an}} {{efficient}} representation for sketch based image retrieval (SBIR) {{derived from a}} triplet loss convolutional neural network (CNN). We treat SBIR as a cross-domain modelling problem, in which a depiction invariant embedding of sketch and photo data is learned by regression over a <b>siamese</b> <b>CNN</b> architecture with half-shared weights and modified triplet loss function. Uniquely, we demonstrate the ability of our learned image descriptor to generalise beyond the categories of object present in our training data, forming a basis for general cross-category SBIR. We explore appropriate strategies for training, and for deriving a compact image descriptor from the learned representation suitable for indexing data on resource constrained e. g. mobile devices. We show the learned descriptors to outperform {{state of the art}} SBIR on the defacto standard Flickr 15 k dataset using a significantly more compact (56 bits per image, i. e. ≈ 105 KB total) search index than previous methods...|$|R
40|$|A {{model for}} hit song {{prediction}} {{can be used}} in the pop music industry to identify emerging trends and potential artists or songs before they are marketed to the public. While most previous work formulates hit song prediction as a regression or classification problem, we present in this paper a convolutional neural network (CNN) model that treats it as a ranking problem. Specifically, we use a commercial dataset with daily play-counts to train a multi-objective <b>Siamese</b> <b>CNN</b> model with Euclidean loss and pairwise ranking loss to learn from audio the relative ranking relations among songs. Besides, we devise a number of pair sampling methods according to some empirical observation of the data. Our experiment shows that the proposed model with a sampling method called A/B sampling leads to much higher accuracy in hit song prediction than the baseline regression model. Moreover, we can further improve the accuracy by using a neural attention mechanism to extract the highlights of songs and by using a separate CNN model to offer high-level features of songs...|$|R
40|$|In this paper, {{we study}} the {{challenging}} problem of multi-object tracking {{in a complex}} scene captured by a single camera. Different from the existing tracklet association-based tracking methods, we propose a novel and efficient way to obtain discriminative appearance-based tracklet affinity models. Our proposed method jointly learns the convolutional neural networks (CNNs) and temporally constrained metrics. In our method, a Siamese convolutional neural network (CNN) is first pre-trained on the auxiliary data. Then the <b>Siamese</b> <b>CNN</b> and temporally constrained metrics are jointly learned online to construct the appearance-based tracklet affinity models. The proposed method can jointly learn the hierarchical deep features and temporally constrained segment-wise metrics under a unified framework. For reliable association between tracklets, a novel loss function incorporating temporally constrained multi-task learning mechanism is proposed. By employing the proposed method, tracklet association can be accomplished even in challenging situations. Moreover, a new dataset with 40 fully annotated sequences is created to facilitate the tracking evaluation. Experimental results on five public datasets and the new large-scale dataset show that our method outperforms several state-of-the-art approaches in multi-object tracking...|$|R
30|$|In this paper, {{we present}} a robust tool for {{classifying}} the remote sensing images which is accomplished by adopting a <b>Siamese</b> <b>CNN</b> architecture better adapted to the characteristics of remote sensing datasets. Our architecture has two identical CNN channels that combine the identification and verification models. The identification model accepts a single image as input and utilizes the CNN to extract the useful features and the final convolutional layer to predict its label. Meanwhile, the verification model compares the feature vectors of the two images extracted by their respective identification models and calculate their distance in feature space. These two models are complementary and their combination allows the method to learn discriminative feature representation. Considering the two CNN channels of our architecture, one channel input adopts data augmentation like random rotation while the other remains unchanged. This operation can let our network learn rotation-invariant features which positively affects the classification results. To {{make the most of}} this combination, a joint decision making is introduced in our work. That is, we encode the probabilistic relationships drawn by identification and verification models to mine the valuable information of input data.|$|R
40|$|We {{investigate}} the ℓ_∞-constrained representation which demonstrates robustness to quantization errors, utilizing the tool of deep learning. Based on the Alternating Direction Method of Multipliers (ADMM), we formulate the original convex minimization problem as a feed-forward neural network, named Deep ℓ_∞ Encoder, by introducing the novel Bounded Linear Unit (BLU) neuron and modeling the Lagrange multipliers as network biases. Such a structural prior {{acts as an}} effective network regularization, and facilitates the model initialization. We then {{investigate the}} effective use of the proposed model {{in the application of}} hashing, by coupling the proposed encoders under a supervised pairwise loss, to develop a <b>Deep</b> <b>Siamese</b> ℓ_∞ Network, which can be optimized from end to end. Extensive experiments demonstrate the impressive performances of the proposed model. We also provide an in-depth analysis of its behaviors against the competitors. Comment: To be presented at IJCAI' 1...|$|R
40|$|Most thermal {{infrared}} (TIR) tracking methods are discriminative, which treat the tracking problem as a classification task. However, {{the objective of}} the classifier (label prediction) is not coupled to {{the objective of the}} tracker (location estimation). The classification task focuses on the between-class difference of the arbitrary objects, while the tracking task mainly deals with the within-class difference of the same objects. In this paper, we cast the TIR tracking problem as a similarity verification task, which is well coupled to the objective of tracking task. We propose a TIR tracker via a hierarchical Siamese convolutional neural network (CNN), named HSNet. To obtain both spatial and semantic features of the TIR object, we design a <b>Siamese</b> <b>CNN</b> coalescing the multiple hierarchical convolutional layers. Then, we train this network end to end on a large visible video detection dataset to learn the similarity between paired objects before we transfer the network into the TIR domain. Next, this pre-trained Siamese network is used to evaluate the similarity between the target template and target candidates. Finally, we locate the most similar one as the tracked target. Extensive experimental results on the benchmarks: VOT-TIR 2015 and VOT-TIR 2016, show that our proposed method achieves favorable performance against the state-of-the-art methods. Comment: 11 pages, 6 figure...|$|R
40|$|Quantification of {{physiological}} changes in plants can capture different drought mechanisms and assist in selection of tolerant varieties {{in a high}} throughput manner. In this context, an accurate 3 D model of plant canopy provides a reliable representation for drought stress characterization in contrast to using 2 D images. In this paper, we propose a novel end-to-end pipeline including 3 D reconstruction, segmentation and feature extraction, leveraging deep neural networks at various stages, for drought stress study. To overcome {{the high degree of}} self-similarities and self-occlusions in plant canopy, prior knowledge of leaf shape based on features from <b>deep</b> <b>siamese</b> network are used to construct an accurate 3 D model using structure from motion on wheat plants. The drought stress is characterized with a deep network based feature aggregation. We compare the proposed methodology on several descriptors, and show that the network outperforms conventional methods. Comment: Appears in Workshop on Computer Vision Problems in Plant Phenotyping (CVPPP), International Conference on Computer Vision (ICCV) 201...|$|R
40|$|Building {{effective}} recommender {{systems for}} domains like fashion is challenging {{due to the}} high level of subjectivity and the semantic complexity of the features involved (i. e., fashion styles). Recent work has shown that approaches to `visual' recommendation (e. g. clothing, art, etc.) can be made more accurate by incorporating visual signals directly into the recommendation objective, using `off-the-shelf' feature representations derived from deep networks. Here, we seek to extend this contribution by showing that recommendation performance can be significantly improved by learning `fashion aware' image representations directly, i. e., by training the image representation (from the pixel level) and the recommender system jointly; this contribution is related to recent work using <b>Siamese</b> <b>CNNs,</b> though we are able to show improvements over state-of-the-art recommendation techniques such as BPR and variants that make use of pre-trained visual features. Furthermore, we show that our model can be used generatively, i. e., given a user and a product category, we can generate new images (i. e., clothing items) that are most consistent with their personal taste. This represents a first step towards building systems that go beyond recommending existing items from a product corpus, but which can be used to suggest styles and aid the design of new products. Comment: 10 pages, 6 figures. Accepted by ICDM' 17 as a long pape...|$|R
40|$|Selecting {{attractive}} {{photos from}} a human action shot sequence is quite challenging, because of the subjective nature of the "attractiveness", which is mainly a combined factor of human pose in action and the background. Prior works have actively studied high-level image attributes including interestingness, memorability, popularity, and aesthetics. However, none of them has ever studied the "attractiveness" of human action shot. In this paper, we present the first study of the "attractiveness" of human action shots by taking a systematic data-driven approach. Specifically, we create a new action-shot dataset composed of about 8000 high quality action-shot photos. We further conduct rich crowd-sourced human judge studies on Amazon Mechanical Turk(AMT) in terms of global attractiveness of a single photo, and relative attractiveness {{of a pair of}} photos. A <b>deep</b> <b>Siamese</b> network with a novel hybrid distribution matching loss was further proposed to fully exploit both types of ratings. Extensive experiments reveal that (1) the property of action shot attractiveness is subjective but predicable (2) our proposed method is both efficient and effective for predicting the attractive human action shots...|$|R
40|$|Knee {{osteoarthritis}} (OA) is {{the most}} common musculoskeletal disorder. OA diagnosis is currently conducted by assessing symptoms and evaluating plain radiographs, but this process suffers from subjectivity. In this study, we present a new transparent computer-aided diagnosis method based on the <b>Deep</b> <b>Siamese</b> Convolutional Neural Network to automatically score knee OA severity according to the Kellgren-Lawrence grading scale. We trained our method using the data solely from the Multicenter Osteoarthritis Study and validated it on randomly selected 3, 000 subjects (5, 960 knees) from Osteoarthritis Initiative dataset. Our method yielded a quadratic Kappa coefficient of 0. 83 and average multiclass accuracy of 66. 71 % compared to the annotations given by a committee of clinical experts. Here, we also report a radiological OA diagnosis area under the ROC curve of 0. 93. We also present attention maps [...] given as a class probability distribution [...] highlighting the radiological features affecting the network decision. This information makes the decision process transparent for the practitioner, which builds better trust toward automatic methods. We believe that our model is useful for clinical decision making and for OA research; therefore, we openly release our training codes and the data set created in this study...|$|R
5000|$|Siamese - A pair of twin assassins {{who always}} work {{together}} as one Noh agent unit, they are often sent on missions that require their unique synchronicity. Born conjoined at the arm, they each have a prosthetic replacement, other than this very little is known of their background. One sister (the left twin) is quite talkative and taunts her targets, whereas the other is more taciturn and chooses to talk only when situations require. Kabuki Volume 4: Skin <b>Deep</b> sees <b>Siamese</b> standing over Ukiko's corpse, presumably having killed her. With {{the entirety of the}} next two volumes acting as a flashback, Siamese take on a darkly, sinister tone, appearing briefly, but always after an act of brutal violence - the build-up to their confrontation is slow and full of dread. Ironically the quiet right twin of the two is revealed to be a fan of Kabuki's on-screen persona and despairs over their mission. Siamese's Noh costumes are sleeveless T-shirts bearing the Japanese sun and rotations of stockings and trousers. The prosthetic arms act as weapons by being fitted with various attachments, and their masks are halved and monochromatic.|$|R
40|$|This work proposes {{solutions}} for two different scenarios in face recognition and verification. The first scenario involves large scale unconstrained unsupervised face recognition. The proposed system for {{this scenario is}} a complete face recognition framework. The proposed system first studies the performance of unsupervised face recognition for frontalized captured faces in the wild under {{the effect of a}} single image super-resolution algorithm. The system also introduces new high dimensional features based on LBP and SURF that perform better than the state-of-the-art features for unconstrained unsupervised face recognition. To solve the large scale recognition process, a new algorithm has been designed to manipulate face images in the dataset. This new algorithm represents all training face images as a fully connected graph. The algorithm then divides the fully connected graph into simpler sub-graphs to enhance the overall recognition rate. The sub-graphs are generated dynamically, and a comparison between different sub-graph selection techniques including minimizing edge weight sums, random selection, and maximizing sum of edge weights inside the sub-graph is provided. Results show that the optimized hierarchical dynamic technique developed with sub-graphs selection increases the recognition rate in large benchmark image dataset by more than 40 % for rank 1 recognition rate compared to the original single large graph method. The approach developed in this research is tested on different datasets, especially if the number of images per person in the training data is low. Furthermore, in order to improve rank 1 recognition rates and to reduce the computation time of the recognition process, a new technique that combines the hierarchical face recognition algorithm and a deep learning neural network using Siamese structure for face verification is proposed. The second part of this work addresses the usage of neural generative models for 3 D faces with an application in face recognition when 3 D datasets are utilized separately without the existence of texture information scenarios. An improved technique is developed to construct new representations for point clouds containing 3 D information. The technique employs a regression neural network model trained using Levenberg-Marquardt (LM) algorithm. One of the advantages of this new representation is the significant reduction in storage space required for point clouds due to the utilization of a regression model for depth map regeneration. Moreover, the trained neural models can be used to generate a super-resolution version of the original 3 D point clouds. The proposed regression representation is also used with a <b>deep</b> <b>Siamese</b> neural system to implement a complete depth-based neural face recognition and verification framework. The results indicate that the proposed system provides highly accurate and efficient face recognition results with 3 D information only without texture information...|$|R
40|$|We {{report on}} an {{architecture}} for the unsupervised discovery of talker-invariant subword embeddings. It {{is made out}} of two components: a dynamic-time warping based spoken term discovery (STD) system and a <b>Siamese</b> <b>deep</b> neural network (DNN). The STD system clusters word-sized repeated frag-ments in the acoustic streams while the DNN is trained to mini-mize the distance between time aligned frames of tokens of the same cluster, and maximize the distance between tokens of dif-ferent clusters. We use additional side information regarding the average duration of phonemic units, as well as talker iden-tity tags. For evaluation we use the datasets and metrics of the Zero Resource Speech Challenge. The model shows improve-ment over the baseline in subword unit modeling. Index Terms: zero resource speech challenge, feature extrac-tion, deep learning 1...|$|R
40|$|A <b>Siamese</b> <b>Deep</b> Forest (SDF) is {{proposed}} in the paper. It {{is based on the}} Deep Forest or gcForest proposed by Zhou and Feng and {{can be viewed as a}} gcForest modification. It can be also regarded as an alternative to the well-known Siamese neural networks. The SDF uses a modified training set consisting of concatenated pairs of vectors. Moreover, it defines the class distributions in the deep forest as the weighted sum of the tree class probabilities such that the weights are determined in order to reduce distances between similar pairs and to increase them between dissimilar points. We show that the weights can be obtained by solving a quadratic optimization problem. The SDF aims to prevent overfitting which takes place in neural networks when only limited training data are available. The numerical experiments illustrate the proposed distance metric method...|$|R
40|$|As a discriminative {{method of}} {{one-shot}} learning, <b>Siamese</b> <b>deep</b> network allows recognizing an object {{from a single}} exemplar with the same class label. However, {{it does not take}} the advantage of the underlying structure and relationship among a multitude of instances since it only relies on pairs of instances for training. In this paper, we propose a quadruplet deep network to examine the potential connections among the training instances, aiming to achieve a more powerful representation. We design four shared networks that receive multi-tuple of instances as inputs and are connected by a novel loss function consisting of pair-loss and triplet-loss. According to the similarity metric, we select the most similar and the most dissimilar instances as the positive and negative inputs of triplet loss from each multi-tuple. We show that this scheme improves the training performance and convergence speed. Furthermore, we introduce a new weighted pair loss for an additional acceleration of the convergence. We demonstrate promising results for model-free tracking-by-detection of objects from a single initial exemplar in the Visual Object Tracking benchmark...|$|R
40|$|A {{robust and}} {{informative}} local shape descriptor {{plays an important}} role in mesh registration. In this regard, spectral descriptors that are based on the spectrum of the Laplace-Beltrami operator have gained a spotlight among the researchers for the last decade due to their desirable properties, such as isometry invariance. Despite such, however, spectral descriptors often fail to give a correct similarity measure for non-isometric cases where the metric distortion between the models is large. Hence, they are in general not suitable for the registration problems, except for the special cases when the models are near-isometry. In this paper, we investigate a way to develop shape descriptors for non-isometric registration tasks by embedding the spectral shape descriptors into a different metric space where the Euclidean distance between the elements directly indicates the geometric dissimilarity. We design and train a <b>Siamese</b> <b>deep</b> neural network to find such an embedding, where the embedded descriptors are promoted to rearrange based on the geometric similarity. We found our approach can significantly enhance the performance of the conventional spectral descriptors for the non-isometric registration tasks, and outperforms recent state-of-the-art method reported in literature. Comment: Submitted to Computer Graphics Foru...|$|R
40|$|In {{this paper}} {{we present a}} tracker, which is radically {{different}} from state-of-the-art trackers: we apply no model updating, no occlusion detection, no combination of trackers, no geometric matching, and still deliver state-of-the-art tracking performance, as demonstrated on the popular online tracking benchmark (OTB) and six very challenging YouTube videos. The presented tracker simply matches the initial patch of the target in the first frame with candidates in a new frame and returns the most similar patch by a learned matching function. The strength of the matching function comes from being extensively trained generically, i. e., without any data of the target, using a <b>Siamese</b> <b>deep</b> neural network, which we design for tracking. Once learned, the matching function is used as is, without any adapting, to track previously unseen targets. It {{turns out that the}} learned matching function is so powerful that a simple tracker built upon it, coined Siamese INstance search Tracker, SINT, which only uses the original observation of the target from the first frame, suffices to reach state-of-the-art performance. Further, we show the proposed tracker even allows for target re-identification after the target was absent for a complete video shot. Comment: This paper is accepted to the IEEE Conference on Computer Vision and Pattern Recognition, 201...|$|R

