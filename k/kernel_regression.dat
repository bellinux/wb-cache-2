748|576|Public
25|$|Class {{prediction}} analysis: This approach, called supervised classification, {{establishes the}} basis for developing a predictive model into which future unknown test objects can be input in order to predict the most likely class membership of the test objects. Supervised analysis for class prediction involves use of techniques such as linear regression, k-nearest neighbor, learning vector quantization, decision tree analysis, random forests, naive Bayes, logistic regression, <b>kernel</b> <b>regression,</b> artificial neural networks, support vector machines, mixture of experts, and supervised neural gas. In addition, various metaheuristic methods are employed, such as genetic algorithms, covariance matrix self-adaptation, particle swarm optimization, and ant colony optimization. Input data for class prediction are usually based on filtered lists of genes which are predictive of class, determined using classical hypothesis tests (next section), Gini diversity index, or information gain (entropy).|$|E
5000|$|R: the {{function}} [...] of the np package can perform <b>kernel</b> <b>regression.</b>|$|E
5000|$|Many {{types of}} models and {{techniques}} {{are subject to}} this formulation. A few examples are linear least squares, smoothing splines, regression splines, local regression, <b>kernel</b> <b>regression,</b> and linear filtering ...|$|E
40|$|A {{systematic}} {{comparison is}} made of parametric (i. e., ordinary leastsquares regressions and related generalizations) and nonparametric (i. e., <b>kernel</b> <b>regressions</b> and regression trees) log-linear gravity models for reproducing international trade. Experiments were conducted to estimate a log-linear gravity model reproducing import and export trade fows in quantity between Italy and 13 world economic zones, based on a panel estimation data set. The best parametric regression model was estimated to defne a baseline reference model. Some spec-ifcations of nonparametric models, belonging to the categories of <b>kernel</b> <b>regressions</b> and regression trees, were also estimated. The performance of parametric and nonparametric models is contrasted through a comparison of goodness-of-ft measures (R 2, mean absolute percentage error) both in estimation and in hold-out sample validation. To assess the differences in model elasticity and forecasts, both parametric and nonparametric models are applied to future scenarios and the corresponding results compared...|$|R
40|$|This paper puts forward <b>kernel</b> ridge <b>regression</b> as an {{approach}} for forecasting with many predictors {{that are related}} nonlinearly to the target variable. In <b>kernel</b> ridge <b>regression,</b> the observed predictor variables are mapped nonlinearly into a high-dimensional space, where estimation of the predictive regression model {{is based on a}} shrinkage estimator to avoid overfitting. We extend the <b>kernel</b> ridge <b>regression</b> methodology to enable its use for economic time-series forecasting, by including lags of the dependent variable or other individual variables as predictors, as is typically desired in macroeconomic and financial applications. Monte Carlo simulations as well as an empirical application to various key measures of real economic activity confirm that <b>kernel</b> ridge <b>regression</b> can produce more accurate forecasts than traditional linear methods for dealing with many predictors based on principal component regression. High dimensionality, nonlinear forecasting, ridge <b>regression,</b> <b>kernel</b> methods...|$|R
40|$|Ridge {{regression}} is {{a classical}} statistical technique {{that attempts to}} address the bias-variance trade-off {{in the design of}} linear regression models. A reformulation of ridge regression in dual variables permits a non-linear form of ridge regression via the well-known “kernel trick”. Unfortunately, unlike support vector regression models, the resulting kernel expansion is typically fully dense. In this paper, we introduce a reduced rank <b>kernel</b> ridge <b>regression</b> (RRKRR) algorithm, capable of generating an optimally sparse kernel expansion that is functionally identical to that resulting from conventional <b>kernel</b> ridge <b>regression</b> (KRR). The proposed method is demonstrated to out-perform an alternative sparse <b>kernel</b> ridge <b>regression</b> algorithm on the Motorcycle and Boston Housing benchmarks...|$|R
50|$|<b>Kernel</b> <b>regression</b> is a {{non-parametric}} {{technique in}} statistics {{to estimate the}} conditional expectation of a random variable. The objective {{is to find a}} non-linear relation between a pair of random variables X and Y.|$|E
5000|$|MATLAB A free MATLAB toolbox with {{implementation}} of <b>kernel</b> <b>regression,</b> kernel density estimation, kernel estimation of hazard function {{and many others}} is available on these pages (this toolbox {{is a part of}} the book [...] ).|$|E
5000|$|According to David Salsburg, the {{algorithms}} used in <b>kernel</b> <b>regression</b> were independently developed {{and used in}} fuzzy systems: [...] "Coming up with {{almost exactly the same}} computer algorithm, fuzzy systems and kernel density-based regressions appear to have been developed completely independently of one another." ...|$|E
40|$|Background : Age {{patterns}} {{are a key}} dimension to compare migration between countries and over time. Comparative metrics can be reliably computed only if data capture the underlying age distribution of migration. Model schedules, the prevailing smoothing method, fit a composite exponential function, but are sensitive to function selection and initial parameter setting. Although non-parametric alternatives exist, their performance {{is yet to be}} established. Objective : We compare cubic splines and <b>kernel</b> <b>regressions</b> against model schedules by assessingwhich method provides an accurate representation of the age profile and best performs on metrics for comparing aggregate age patterns. Methods : We use full population microdata for Chile to perform 1, 000 Monte-Carlo simulations for nine sample sizes and two spatial scales. We use residual and graphic analysis to assess model performance on the age and intensity at which migration peaks and the evolution of migration age patterns. Results : Model schedules generate a better fit when (1) the expected distribution of the age profile is known a priori, (2) the pre-determined shape of the model schedule adequately describes the true age distribution, and (3) the component curves and initial parameter values can be correctly set. When any of these conditions is not met, <b>kernel</b> <b>regressions</b> and cubic splines offer more reliable alternatives. Conclusions : Smoothing models should be selected according to research aims, age profile characteristics, and sample size. <b>Kernel</b> <b>regressions</b> and cubic splines enable a precise representation of aggregate migration age profiles for most sample sizes, without requiring parameter setting or imposing a pre-determined distribution, and therefore facilitate objective comparison...|$|R
40|$|Abstract. This paper {{describes}} {{a method for}} performing <b>kernel</b> smoothing <b>regression</b> in an incremental, adaptive manner. A simple and fast combination of incremental vector quantization with <b>kernel</b> smoothing <b>regression</b> using adaptive bandwidth is shown to be effective for online modeling of environmental datasets. The method is illustrated on openly available datasets corresponding to the Tropical Atmosphere Ocean array and the Helsinki Commission hydrographic database for the Baltic Sea. ...|$|R
40|$|<b>Kernel</b> {{logistic}} <b>regression</b> models, {{like their}} linear counterparts, {{can be trained}} using the efficient iteratively reweighted least-squares (IRWLS) algorithm. This approach suggests an approximate leave-one-out cross-validation estimator based on an existing method for exact leave-one-out cross-validation of least-squares models. Results compiled over seven benchmark datasets are presented for <b>kernel</b> logistic <b>regression</b> with model selection procedures based on both conventional k-fold and approximate leave-one-out cross-validation criteria, demonstrating the proposed approach to be viable...|$|R
5000|$|In kernel density {{estimation}} and <b>kernel</b> <b>regression</b> {{additional parameter}} — the bandwidth h — is assumed. In these models it is typically taken that h → 0 as n → ∞, however {{the rate of}} convergence must be chosen carefully, usually h ∝ n−1/5.|$|E
50|$|His work {{includes}} {{the development of}} fast and robust methods for super-resolution, statistical analysis of performance limits for inverse problems in imaging, {{and the development of}} adaptive non-parametric techniques (<b>kernel</b> <b>regression)</b> for image and video processing. He holds 7 US patents in the field of image and video processing.|$|E
5000|$|<b>Kernel</b> <b>regression</b> {{estimates}} the continuous dependent variable from a limited {{set of data}} points by convolving the data points' locations with a kernel function—approximately speaking, the kernel function specifies how to [...] "blur" [...] {{the influence of the}} data points so that their values can be used to predict the value for nearby locations.|$|E
5000|$|Regression: {{least squares}}, ridge regression, least angle <b>regression,</b> elastic net, <b>kernel</b> ridge <b>regression,</b> support vector {{machines}} (SVM), partial least squares (PLS) ...|$|R
40|$|In {{this paper}} we propose a method of {{variable}} selection for support vector machines based on the approximate relationship between a support vector machine and <b>kernel</b> logistic <b>regression.</b> First, we derive the generalized Bayesian information criterion (GBIC) of a <b>kernel</b> logistic <b>regression.</b> Then we select variables that minimize the GBIC, and propose to use them for a support vector machine. Finally, we apply the proposed method to identify peptides that could be related to pancreatic cancer...|$|R
40|$|Regional-based {{association}} analysis instead of individual testing of each SNP {{was introduced in}} genome-wide association studies to increase the power of gene mapping, especially for rare genetic variants. For regional association tests, the <b>kernel</b> machine-based <b>regression</b> approach was recently proposed as a more powerful alternative to collapsing-based methods. However, {{the vast majority of}} existing algorithms and software for the <b>kernel</b> machine-based <b>regression</b> are applicable only to unrelated samples. In this paper, we present a new method for the <b>kernel</b> machine-based <b>regression</b> {{association analysis}} of quantitative traits in samples of related individuals. The method is based on the GRAMMAR+ transformation of phenotypes of related individuals, followed by use of existing <b>kernel</b> machine-based <b>regression</b> software for unrelated samples. We compared the performance of kernel-based association analysis on the material of the Genetic Analysis Workshop 17 family sample and real human data by using our transformation, the original untransformed trait, and environmental residuals. We demonstrated that only the GRAMMAR+ transformation produced type I errors close to the nominal value and that this method had the highest empirical power. The new method can be applied to analysis of related samples by using existing software for kernel-based association analysis developed for unrelated samples...|$|R
50|$|In {{non-parametric}} statistics, a kernel is a {{weighting function}} used in non-parametric estimation techniques. Kernels {{are used in}} kernel density estimation to estimate random variables' density functions, or in <b>kernel</b> <b>regression</b> to estimate the conditional expectation of a random variable. Kernels are also used in time-series, {{in the use of}} the periodogram to estimate the spectral density where they are known as window functions. An additional use is in the estimation of a time-varying intensity for a point process where window functions (kernels) are convolved with time-series data.|$|E
5000|$|In {{his early}} work Joachim Engel {{specialized}} in nonparametric curve estimation and signal detection applying methods of Harmonic Analysis (Engel, 1994) (Engel & Kneip 1996) and <b>kernel</b> <b>regression</b> to biomedical growth curves and economics. Recently {{he is best}} known for his contributions in Statistics Education, investigating students’ comprehension of randomness and variability (Engel & Sedlmeier 2005) and introducing computer intensive methods, based, for instance, on bootstrap procedures (Engel & Grübel, 2008). His experience on didactical methods for explaining functions and their uses for modeling real world problems is reflected in his widely used highly successful textbook on applying functions for modeling based on data (cite Engel, J. Anwendungs...). He also wrote a wellknown textbook on Complex Variables (cite Engel, J. and Fest, A. [...] ) ...|$|E
5000|$|Class {{prediction}} analysis: This approach, called supervised classification, {{establishes the}} basis for developing a predictive model into which future unknown test objects can be input in order to predict the most likely class membership of the test objects. Supervised analysis for class prediction involves use of techniques such as linear regression, k-nearest neighbor, learning vector quantization, decision tree analysis, random forests, naive Bayes, logistic regression, <b>kernel</b> <b>regression,</b> artificial neural networks, support vector machines, mixture of experts, and supervised neural gas. In addition, various metaheuristic methods are employed, such as genetic algorithms, covariance matrix self-adaptation, particle swarm optimization, and ant colony optimization. Input data for class prediction are usually based on filtered lists of genes which are predictive of class, determined using classical hypothesis tests (next section), Gini diversity index, or information gain (entropy).|$|E
40|$|An {{estimating}} {{procedure is}} introduced for <b>kernel</b> Poisson <b>regression</b> when the input variables consist of numerical and categorical variables, {{which is based}} on the penalized negative log-likelihood and the component-wise product of two different types of kernel functions. The proposed procedure provides the estimates of the mean function of the response variables, where the canonical parameter is linearly and/or nonlinearly related to the input variables. Experimental results are then presented which indicate the performance of the proposed <b>kernel</b> Poisson <b>regression...</b>|$|R
40|$|In {{this paper}} {{we present a}} simple {{hierarchical}} Bayesian treatment of the sparse <b>kernel</b> logistic <b>regression</b> (KLR) model {{based on the evidence}} framework introduced by MacKay. The principal innovation lies in the re-parameterisation of the model such that the usual spherical Gaussian prior over the parameters in the kernel induced feature space also corresponds to a spherical Gaussian prior over the transformed parameters, permitting the straight-froward derivation of an efficient update formula for the regularisation parameter. The Bayesian framework also allows the selection of good values for kernel parameters through maximisation of the marginal likelihood, or evidence, for the model. Results obtained on a variety of benchmark datasets are provided indicating that the Bayesian <b>kernel</b> logistic <b>regression</b> model is competitive with <b>kernel</b> logistic <b>regression</b> models, where the hyper-parameters are selected via cross-validation and with the support vector machine and relevance vector machine...|$|R
40|$|Prediction of {{dynamical}} {{time series}} with additive noise using support vector machines or <b>kernel</b> based <b>regression</b> {{has been proved}} to be consistent for certain classes of discrete dynamical systems. Consistency implies that these methods are effective at computing the expected value of a point at a future time given the present coordinates. However, the present coordinates themselves are noisy, and therefore, these methods are not necessarily effective at removing noise. In this article, we consider denoising and prediction as separate problems for flows, as opposed to discrete time dynamical systems, and show {{that the use of}} smooth splines is more effective at removing noise. Combination of smooth splines and <b>kernel</b> based <b>regression</b> yields predictors that are more accurate on benchmarks typically by a factor of 2 or more. We prove that <b>kernel</b> based <b>regression</b> in combination with smooth splines converges to the exact predictor for time series extracted from any compact invariant set of any sufficiently smooth flow. As a consequence of convergence, one can find examples where the combination of <b>kernel</b> based <b>regression</b> with smooth splines is superior by even a factor of $ 100 $. The predictors that we compute operate on delay coordinate data and not the full state vector, which is typically not observable...|$|R
50|$|In mathematics, {{nonlinear}} modelling is empirical or semi-empirical modelling {{which takes}} at least some nonlinearities into account. Nonlinear modelling in practice therefore means modelling of phenomena in which independent variables affecting the system can show complex and synergetic nonlinear effects. Contrary to traditional modelling methods, such as linear regression and basic statistical methods, nonlinear modelling can be utilized efficiently in {{a vast number of}} situations where traditional modelling is impractical or impossible. The newer nonlinear modelling approaches include non-parametric methods, such as feedforward neural networks, <b>kernel</b> <b>regression,</b> multivariate splines, etc., which do not require a priori knowledge of the nonlinearities in the relations. Thus the nonlinear modelling can utilize production data or experimental results while taking into account complex nonlinear behaviours of modelled phenomena which are in most cases practically impossible to be modelled by means of traditional mathematical approaches, such as phenomenological modelling.|$|E
5000|$|Technical analysis, {{also known}} as [...] "charting", {{has been a part}} of {{financial}} practice for many decades, but this discipline has not received the same level of academic scrutiny and acceptance as more traditional approaches such as fundamental analysis. One of the main obstacles is the highly subjective nature of technical analysisthe presence of geometric shapes in historical price charts is often in the eyes of the beholder. In this paper, we propose a systematic and automatic approach to technical pattern recognition using nonparametric <b>kernel</b> <b>regression,</b> and apply this method to a large number of U.S. stocks from 1962 to 1996 to evaluate the effectiveness of technical analysis. By comparing the unconditional empirical distribution of daily stock returns to the conditional distributionconditioned on specific technical indicators such as head-and-shoulders or double-bottomswe find that over the 31-year sample period, several technical indicators do provide incremental information and may have some practical value.|$|E
5000|$|Most {{forms of}} {{cross-validation}} are straightforward to implement {{as long as}} an implementation of the prediction method being studied is available. In particular, the prediction method can be a [...] "black box" [...] - {{there is no need}} to have access to the internals of its implementation. If the prediction method is expensive to train, cross-validation can be very slow since the training must be carried out repeatedly. In some cases such as least squares and <b>kernel</b> <b>regression,</b> cross-validation can be sped up significantly by pre-computing certain values that are needed repeatedly in the training, or by using fast [...] "updating rules" [...] such as the Sherman-Morrison formula. However one must be careful to preserve the [...] "total blinding" [...] of the validation set from the training procedure, otherwise bias may result. An extreme example of accelerating cross-validation occurs in linear regression, where the results of cross-validation have a closed-form expression known as the prediction residual error sum of squares (PRESS).|$|E
40|$|We study a decomposition-based {{scalable}} {{approach to}} performing <b>kernel</b> ridge <b>regression.</b> The method is simple to describe: it randomly partitions a dataset of size N into m subsets of equal size, computes an independent <b>kernel</b> ridge <b>regression</b> estimator for each subset, then averages the local solutions {{into a global}} predictor. This partitioning leads to a substantial reduction in computation time versus the standard approach of performing <b>kernel</b> ridge <b>regression</b> on all N samples. Our main theorem establishes that despite the computational speed-up, statistical optimality is retained: if m is not too large, the partition-based estimate achieves optimal rates of convergence for the full sample size N. As concrete examples, our theory guarantees that m may grow polynomially in N for Sobolev spaces, and nearly linearly for finite-rank kernels and Gaussian kernels. We conclude with simulations complementing our theoretical results and exhibiting the computational and statistical benefits of our approach. 1...|$|R
40|$|International audienceWe {{consider}} {{supervised learning}} problems within the positive-definite kernel framework, such as <b>kernel</b> ridge <b>regression,</b> <b>kernel</b> logistic <b>regression</b> or the support vector machine. With kernels leading to infinite-dimensional feature spaces, a common practical limiting difficulty is {{the necessity of}} computing the kernel matrix, which most frequently leads to algorithms with running time at least quadratic {{in the number of}} observations n, i. e., O(n^ 2). Low-rank approximations of the kernel matrix are often considered as they allow the reduction of running time complexities to O(p^ 2 n), where p is the rank of the approximation. The practicality of such methods thus depends on the required rank p. In this paper, we show that in the context of <b>kernel</b> ridge <b>regression,</b> for approximations based on a random subset of columns of the original kernel matrix, the rank p may be chosen to be linear in the degrees of freedom associated with the problem, a quantity which is classically used in the statistical analysis of such methods, and is often seen as the implicit number of parameters of non-parametric estimators. This result enables simple algorithms that have sub-quadratic running time complexity, but provably exhibit the same predictive performance than existing algorithms, for any given problem instance, and not only for worst-case situations...|$|R
40|$|Abstract. We {{present the}} Explicit <b>Kernel</b> Rewards <b>Regression</b> (EKRR) approach, as an {{extension}} of <b>Kernel</b> Rewards <b>Regression</b> (KRR), for Optimal Policy Identification in Reinforcement Learning. The method uses the Structural Risk Minimisation paradigm to achieve a high generalisation capability. This explicit version of KRR offers at least two important advantages. On the one hand, finding a near-optimal policy is done by a quadratic program, hence no Policy Iteration techniques are necessary. And on the other hand, the approach allows for the usage of further constraints and certain regularisation techniques as e. g. in Ridge Regression and Support Vector Machines. ...|$|R
40|$|This paper {{presents}} a novel method for learning in domains with continuous target variables. The method integrates regression trees with <b>kernel</b> <b>regression</b> models. The integration {{is done by}} adding kernel regressors at the tree leaves producing what we call <b>kernel</b> <b>regression</b> trees. The approach is motivated by the goal of trying {{to take advantage of}} the different biases of the two regression methodologies. The presented method is implemented. <b>Kernel</b> <b>regression</b> trees are comprehensible and accurate regression models of the data. Experimental comparisons on both artificial and real world domains revealed the superiority of <b>kernel</b> <b>regression</b> trees when compared to the two individual approaches. The use of <b>kernel</b> <b>regression</b> at the trees leaves gives a significant performance gain. Moreover, a good performance level is achieved with much smaller trees than if <b>kernel</b> <b>regression</b> was not used. Compared to <b>kernel</b> <b>regression</b> our method improves its comprehensibility and execution time. Keywords [...] ...|$|E
40|$|We propose <b>kernel</b> <b>{{regression}}</b> for signals over graphs. The optimal {{regression coefficients}} are learnt using a constraint that the target vector is a smooth signal over an underlying graph. The constraint is imposed using a graph- Laplacian based regularization. We discuss how the proposed <b>kernel</b> <b>regression</b> exhibits a smoothing effect, simultaneously achieving noise-reduction and graph-smoothness. We further extend the <b>kernel</b> <b>regression</b> to simultaneously learn the un- derlying graph and the regression coefficients. We validate our theory by application to various synthesized and real-world graph signals. Our experiments show that <b>kernel</b> <b>regression</b> over graphs outperforms conventional regression, particularly for small sized training data and under noisy training. We also observe that <b>kernel</b> <b>regression</b> reveals {{the structure of}} the underlying graph even with a small number of training samples...|$|E
30|$|The kernel {{function}} in the classic <b>kernel</b> <b>regression</b> method only depends on spatial sample locations, but the adaptive {{kernel function}} in the adaptive <b>kernel</b> <b>regression</b> method proposed in [18] depends on sample locations and density, {{as well as the}} radiometric values of image data. Therefore, the adaptive <b>kernel</b> <b>regression</b> method has a greater ability to preserve image details, such as edges.|$|E
40|$|Due to the {{widening}} semantic gap of videos, computational tools to classify these videos into different genre are highly needed to narrow it. Classifying videos accurately demands good representation of video data and an {{efficient and effective}} model {{to carry out the}} classification task. <b>Kernel</b> Logistic <b>Regression</b> (KLR), <b>kernel</b> version of logistic regression (LR), proves its efficiency as a classifier, which can naturally provide probabilities and extend to multiclass classification problems. In this paper, Weighted <b>Kernel</b> Logistic <b>Regression</b> (WKLR) algorithm is implemented for video genre classification to obtain significant accuracy, and it shows accurate and faster good results...|$|R
40|$|We {{establish}} optimal convergence {{rates for}} a decomposition-based scalable approach to <b>kernel</b> ridge <b>regression.</b> The method is simple to describe: it randomly partitions a dataset of size N into m subsets of equal size, computes an independent <b>kernel</b> ridge <b>regression</b> estimator for each subset, then averages the local solutions {{into a global}} predictor. This partitioning leads to a substantial reduction in computation time versus the standard approach of performing <b>kernel</b> ridge <b>regression</b> on all N samples. Our two main theorems establish that despite the computational speed-up, statistical optimality is retained: as long as m is not too large, the partition-based estimator achieves the statistical minimax rate over all estimators using the set of N samples. As concrete examples, our theory guarantees {{that the number of}} processors m may grow nearly linearly for finite-rank kernels and Gaussian kernels and polynomially in N for Sobolev spaces, which in turn allows for substantial reductions in computational cost. We conclude with experiments on both simulated data and a music-prediction task that complement our theoretical results, exhibiting the computational and statistical benefits of our approach...|$|R
40|$|To {{enhance the}} {{accuracy}} of protein-protein interaction function prediction, a 2 -order graphic neighbor information feature extraction method based on undirected simple graph is proposed in this paper, which extends the 1 -order graphic neighbor featureextraction method. And the chi-square test statistical method is also involved in feature combination. To demonstrate the effectiveness of our 2 -order graphic neighbor feature, four logistic regression models (logistic regression (abbrev. LR), diffusion <b>kernel</b> logistic <b>regression</b> (abbrev. DKLR), polynomial <b>kernel</b> logistic <b>regression</b> (abbrev. PKLR), and radial basis function (RBF) based <b>kernel</b> logistic <b>regression</b> (abbrev. RBF KLR)) are investigated on the two feature sets. The experimental results of protein function prediction of Yeast Proteome Database (YPD) using the the protein-protein interaction data of Munich Information Center for Protein Sequences (MIPS) show that 2 -order graphic neighbor information of proteins can significantly improve the average overall percentage of protein function prediction especially with RBF KLR. And, with a new 5 -top chi-square feature combination method, RBF KLR can achieve 99. 05 % average overall percentage on 2 -order neighbor feature combination set...|$|R
