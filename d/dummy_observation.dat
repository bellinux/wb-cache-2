2|13|Public
40|$|Motivated {{by the way}} a small {{open economy}} should react to {{business}} cycles, we have estimated a small open economy (SOE) model for Nigeria. This is {{with a view to}} understanding how the Nigerian economy should be managed {{in the face of a}} cycle such as the current global meltdown. Our SOE model is used to generate <b>dummy</b> <b>observation</b> priors for the VAR in line with the BVAR-DSGE technique. We consider four monetary policy rules and estimate each of the resulting models using DYNARE 4. 0. 2. We find that the Central Bank of Nigeria (CBN) places little weight on the exchange rate behaviour in reacting to the cycles, resulting in overshooting and persistence in the exchange rate but strongly reacts to the behaviour of inflation and, to a lesser degree, of output, output gap or its growth following the shocks. We conclude that it will be important for the CBN to pursue a guided exchange rate policy by actively responding to the exchange rate movement to avoid overshooting and persistence, that the terms of trade must be endogenize and that there is scope for the CBN to learn from past policy outcome by building a much stronger feedback. ...|$|E
40|$|The project {{consists}} of three X-ray Multimirror Mission (XMM) observations of the hot, nearby galaxy cluster Triangulum Australis (one pointing to the cluster center and two offsets). The major goal of the project is to obtain an accurate radial gas temperature profile out to large radii and derive the total gravitating mass within the radius of over density 500. A secondary goal is to obtain a detailed two-dimensional gas temperature map of the central region. We have recently received the central pointing (of the two CDROMs received, one contains the central pointing and the other, a zero-exposure <b>dummy</b> <b>observation).</b> The {{analysis of the data}} at hand is underway with the aim of deriving the two-dimensional temperature map. At present, we have to limit quantitative analysis to the EPIC-MOS data because of the absence of the necessary calibration products for the Extended Full Frame mode used for EPIC-pn in these observations. We anticipate the arrival of the data from the remaining two offset pointings in the near future, after which {{we will be able to}} undertake the total mass derivation. Money from this year's installment of the grant was partially spent on an upgrade of the computer used for the above data analysis, and on travel to a conference that extensively discussed the XMM calibration issues...|$|E
40|$|By {{exploiting}} the insight that the misspecification of dynamic stochastic general equilibrium (DSGE) models is more prevalent at some frequencies than at others, we develop methods that enable {{different degrees of}} relaxation of the DSGE restrictions in different directions. We approximate the DSGE model by a vector autoregression. <b>Dummy</b> <b>observations</b> are constructed from the DSGE model and converted into the frequency domain. By re-weighting the frequency domain <b>dummy</b> <b>observations</b> we can control {{the extent to which}} the restrictions derived from economic theory are relaxed. Bayesian marginal data densities can then be used to obtain a data-driven procedure that determines the optimal degree of shrinkage toward the DSGE model restrictions. We provide several numerical illustrations of our procedure...|$|R
40|$|In applications, {{the linear}} {{multiple}} regression model is often modified {{to allow for}} nonlinearity in an independent variable. It is argued here that in practice it may often be desirable to specify a Bayesian prior that the unknown functional form is "simple" or "uncomplicated" rather than to parametize the nonlinearity. "Discrete smoothness priors" and "continuous smoothness priors" are defined and it is shown how posterior mean estimates can easily be derived using ordinary multiple linear regression modified with dummy variables and <b>dummy</b> <b>observations.</b> Relationships with spline and polynomial interpolation are pointed out. Illustrative examples of cost function estimation are provided. ...|$|R
40|$|ABSTRACT. Using {{the idea}} of {{generalized}} <b>dummy</b> <b>observations,</b> we extend the methods of Del Negro and Schorfheide (DS), who have proposed {{a way to use}} a dynamic stochastic general equilibrium (DSGE) model to generate a prior distribution for a structural time series model that relaxes the tight theoretical restrictions of the DSGE. The advantages of this paper’s approach over that of DS are that the time series model strictly nests the DSGE specification, that the prior information is more able to resolve weak identification, that uncertainty about identification is treated more like other forms of uncertainty, and that giving greater weight to the prior at particular frequencies is more straightforward. In every large scale macro modeling project we make compromises. Models whose properties we understand and can interpret behaviorally are generally not rich enough, either in number of variables or in dynamics, to fit the data well. Models that are rich enough to fit well become complex and can end up having implications that we Date: May 22, 2008. Key words and phrases. DSGE,macroeconomic model,Bayesian...|$|R
40|$|ABSTRACT. Using {{the idea}} of {{generalized}} <b>dummy</b> <b>observations,</b> we extend the methods of Del Negro and Schorfheide, who have proposed {{a way to use}} a dy-namic stochastic general equilibrium (DSGE) model to generate a prior distribu-tion for a structural vector autoregression (SVAR). The method proposed here is more explicit and systematic about the prior’s assertions about the SVAR identifi-cation, and it provides a mechanism for varying the tightness of the prior across frequencies, so that for example the long run properties of the DSGE can be as-serted more confidently than its short-run behavior. In every large scale macro modeling project we make compromises. Models whose properties we understand and can interpret behaviorally are generally not rich enough, either in number of variables or in dynamics, to fit the data well. Models that are rich enough to fit well become complex and can end up hav-ing implications that we believe are implausible. Practical macro modelers who face real-time demands for forecasts and policy projections have struggled contin-uously with these tradeoffs...|$|R
40|$|In some {{applications}} of the distributed lag model, theory requires that all lag coefficients have a positive sign. A distributed lag estimator which provides estimated coefficients with positive sign is developed here which is analogous to an earlier distributed lag estimator derived from "smoothness priors" which did not assure that all estimated coefficients be positive. The earlier estimator with unconstrained signs was a posterior mode of the coefficients based on a spherically normal "smoothness prior" in the d+l order differences of the coefficients. The newer estimator with constrained sign is a posterior mode of the logs of the coefficients based on spherically normal "smoothness prior" on the d+l order differences of the logs of the coefficients. The meaning of both categories of prior is discussed in this paper and they are compared to prior parameterizations of the lag curve. Both varieties of "smoothness prior", {{in contrast to the}} parameterizations, allow the coefficients to assume any "smooth" shape subject to the sign constraint. The sign-constrained estimator has the additional advantage that it easily forms asymptotes. Moreover, the sign con-strained estimator is easily implemented. The estimate can be obtained by an iterative procedure involving regressions with <b>dummy</b> <b>observations</b> similar to those used to find the unconstrained sign estimator. An illustrative example of the application of both estimators is given {{at the end of the}} paper. ...|$|R
40|$|We {{consider}} {{the problem of}} fitting mathematical models for bacterial growth and decline to experimental data. Using models which represent the phases of the growth and decline cycle in a piecewise manner, we describe how least-squares fitting can lead to potentially misleading parameter estimates. We show how these difficulties can be overcome by extending a data set to include hypothetical <b>observations</b> (<b>dummy</b> data points) which reflect biological beliefs, and the resulting stabilization of parameter estimates is analysed mathematically. The techniques are illustrated using real and simulated data sets...|$|R
30|$|The {{present study}} showed that nested {{imputation}} methods are suitable approaches to deal with missing values in both continuous and categorical background variables. Both SMI and MMI yield virtually unbiased estimates of subpopulation differences and regression coefficients for background variables with missing data. Concerning the estimation of standard errors, MMI more accurately reflect the uncertainty due to missing data in the background variables than SMI, resulting in slightly larger standard errors. However, the differences between imputation methods are small in non-extreme conditions of missing data. Thus, both SMI and MMI {{can be used to}} impute missing values on background variables in large-scale assessments to avoid the conceptual flaws and the possible biases associated with the common approach of <b>dummy</b> coding missing <b>observations.</b>|$|R
50|$|In May 1984, Fennia began {{service with}} Jakob Lines. Initially she {{was set on}} the new Jakobstad (Finland) - Örnsköldsvik (Sweden) route, later also on Jakob Lines' {{traditional}} Jakobstad-Skellefteå route. On 28 November two passengers were injured during a heavy storm. Fennia was from the beginning too large a ship for Jakob Lines and already in December 1985 she was sold to Vaasanlaivat / Vasabåtarna (in exchange for Fenno Express and 19.1 million Finnish marks). Between January and April 1986 Fennia was heavily rebuilt at Wärtsilä Turku shipyard. Her terraced rear superstructure was built in with cabins, the midship <b>dummy</b> funnel / <b>observation</b> lounge was removed, the two actual funnels to the rear were given a sleeker appearance and to support the additions rear sponsons were added.|$|R
40|$|Before being {{considered}} definitive, data currently produced by statistical agencies undergo a recurrent revision process resulting in different releases {{of the same}} phenomenon. The collection of all these vintages {{is referred to as}} a real-time data set. Economists and econometricians have realized the importance of this type of information for economic modeling and forecasting. This paper focuses on testing non-stationary data for forecastability, i. e., whether revisions reduce noise or are news. To deal with historical revisions which affect the whole vintage of time series due to redefinitions, methodological innovations etc., we employ the recently developed impulse indicator saturation approach, which involves potentially adding an indicator <b>dummy</b> for each <b>observation</b> to the model. We illustrate our procedures with the U. S. Real Gross National Product series from ALFRED and that revisions to this series neither reduce noise nor can be considered as news...|$|R
30|$|A {{potential}} {{limitation of}} working with grouped data is {{in the identification of}} fixed effects in the estimation of Eqs. (3) and (5). In the simplest specifications estimated below, I introduce origin and destination country fixed effects, and year dummies. Additionally, in several specifications I introduce country-pair or country of origin × year dummies. Destination country, time, and destination × time fixed effects are identified in all cases, as grouping only affects origin countries. To identify a dummy for an origin country, we need to observe, at least, one bilateral observation from that country, or that the country appears in a unique combination of grouped observations. 14 To identify a country of origin × year <b>dummy,</b> this bilateral <b>observation</b> or unique combination of groups has to be observed in each year. And the identification of a country-pair dummy requires the bilateral observation to be observed at least once for each destination country. When one of these situations is not satisfied, a single dummy for each unique combination of groups is identified.|$|R
40|$|We {{examine the}} {{interest}} rate elasticity of housing prices, advancingthe empirical literature in two directions. First, we take a commonly used cross-country panel dataset and evaluate the housing price equation using a consistent estimator {{in the presence of}} endogenous explanatory variables and a lagged dependent variable. Second, we carry-out a novel analysis of determinants of residential housing prices in a cross-section of countries. Our results show that the short-term interest rate, and hence monetary policy, has a sizable impact on residential housing prices. Housing prices;Real estate prices;Economic models;equation, real estate, equations, regression analysis, statistics, standard errors, autocorrelation, stata, survey, correlation, <b>dummy</b> variables, missing <b>observations,</b> housing construction, instrumental variables, supply equations, measurement errors, optimization, cointegration, real estate markets, predictions, econometrics, real estate market, covariance, land prices, explanatory power, construction costs, real estate tax, simultaneous equations, statistical techniques, property values, outlier, real estate price, property taxes, correlation analysis, consistent estimator, estimation method, office buildings, finite sample, estimation procedure, scatter plot...|$|R
40|$|We {{estimate}} {{a latent}} factor model that decomposes international stock returns into global, country-, and industry-specific shocks {{and allows for}} stock-specific exposures to these shocks. We find that across stocks there is substantial dispersion in these exposures, which is partly explained by {{the extent to which}} firms operate across countries. We show that portfolios consisting of stocks with low exposures to country shocks achieve substantial variance reduction relative to the global market, both in- and out-of-sample. The shock exposures are thus a stock-selection device for international portfolio diversification. Industrial structure;Risk premium;Export diversification;Stock markets;Economic models;stock returns, standard deviation, equation, standard deviations, correlation, covariance, samples, cash flow, sample mean, stock market, correlations, orthogonality, kurtosis, statistics, skewness, statistic, financial economics, financial markets, descriptive statistics, sampling error, forecasting, explanatory power, sampling, international financial markets, equity markets, standard errors, goodness of fit, dummy variable, independent variable, maximum likelihood estimator, international capital, global stock markets, random variable, optimization, bayes factors, computation, international capital markets, vector autoregression, bayesian information criterion, global stock market, data analysis, financial sector, normal distribution, covariances, number of parameters, national equity market, random variables, conditional expectation, logarithm, missing <b>observations,</b> <b>dummy</b> variables...|$|R
40|$|The main {{objective}} of the dissertation is to apply recent advances in modern econometric analysis, namely cointegrating Vector Autoregression (VAR) and Bayesian VAR (BVAR) to a small open regional economy like Hawaii. This is accomplished in three related yet independent essays demonstrating how regional modeling and forecasting can benefit from these latest developments. The first essay concentrates on the cointegrating VAR analysis, applying it to Hawaii's premier industry-tourism. Recent research {{in the literature on}} identified cointegrating VARs emphasizes the need to rely on economic theory to impose weak exogeneity assumptions, guide the search for long-run just (over) identifying restrictions and shrink the model to the most parsimonious representation. While cointegration analysis has gradually appeared in the empirical tourism literature, the focus has been exclusively on the demand side with no use of the latest identification techniques. A complete Hawaii tourism model is developed, exploiting Hall, Henry, and Greenslade's (2002) theory-directed sequential reduction methodology. Both demand and supply factors are emphasized in identifying long-run cointegrating relationships. The second essay applies the BVAR methodology to another key sector in regional modeling-construction. This essay represents the first application of priors on linear combinations of parameters-namely, sums of coefficients and <b>dummy</b> initial <b>observation</b> priors - in a BVAR construction forecasting model. I find that including these priors does not necessarily improve forecast accuracy at medium to long horizons, especially when the series are integrated and {{there is more than one}} cointegrating relationship. The third essay extends the second essay to deal with the entire regional economy. All regional models must deal with the inavailability of expenditure data at the state and local levels. This problem typically leads researchers to use either a single highly restricted VAR, or BVAR, or a model of pseudo theory driven equations. In contrast, my third essay makes use of BVAR blocks to model proxies for the expenditure categories in a traditional macro structure. Compared with existing regional BVAR models, the current setup is more complete in accounting for both the intra-action of sectors within the region and the inter-action of the region with external drivers...|$|R

