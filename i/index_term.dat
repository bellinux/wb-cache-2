745|10000|Public
25|$|The SafetyLit Thesaurus is {{the basis}} for hybrid textword+synonym search capacity. There are often many {{synonyms}} and spelling variants for potential search terms. New terms and synonyms are added weekly. Work on the SafetyLit thesaurus is not finished, so the full hierarchical search system is not yet complete. However, some term hierarchies such as geographic area names are available for searching. For example, an <b>index</b> <b>term</b> search using ‘Australia’ will find articles that contain the words ‘Canberra’, ‘Adelaide’, or ‘Perth’, even if the SafetyLit records do not contain the word ‘Australia’. The SafetyLit Thesaurus is built on a modified version of TemaTres open-source software.|$|E
5000|$|Let: [...] be the {{occurrence}} matrix [...] be {{the occurrence}} matrix without the <b>index</b> <b>term</b> [...] and [...] be density of [...] Then: The discrimination {{value of the}} <b>index</b> <b>term</b> [...] is: ...|$|E
50|$|An optimal <b>index</b> <b>term</b> is {{one that}} can {{distinguish}} two different documents {{from each other and}} relate two similar documents. On the other hand, a sub-optimal <b>index</b> <b>term</b> can not distinguish two different document from two similar documents.|$|E
40|$|Phrase {{browsing}} applications {{provide information}} seekers {{with access to}} text content via structured lists of <b>index</b> <b>terms.</b> The <b>index</b> <b>terms,</b> which may be identified {{by a variety of}} techniques, are phrases that have been automatically extracted from full text documents. Browsing applications support interactive navigation of <b>index</b> <b>terms</b> and provide direct access to the original documents via the <b>index</b> <b>terms.</b> Term...|$|R
50|$|Many {{journals}} and databases provides access (also) to <b>index</b> <b>terms</b> made by authors to the articles being published or represented. The relative quality of indexer-provided <b>index</b> <b>terms</b> and author provided <b>index</b> <b>terms</b> {{is of interest}} to research in information retrieval. The quality of both kinds of <b>indexing</b> <b>terms</b> depends, of course, on the qualifications of provider. In general authors have difficulties providing <b>indexing</b> <b>terms</b> that characterizes his document relative to the other documents in the database. Author keywords {{are an integral part}} of literature.|$|R
40|$|AbstractThe {{purpose of}} this study is to reveal the {{characteristics}} of the terminological structure formed by the <b>index</b> <b>terms</b> of junior-high school, high school and university-level textbooks. We first identify the types of concept of <b>index</b> <b>terms,</b> and then uncover the conceptual structure that underlies <b>index</b> <b>terms.</b> We found that, as the school level progresses, (1) the balance of concept types of <b>index</b> <b>terms</b> shift from concrete objects to their behaviours or features, (2) <b>index</b> <b>terms</b> as a whole shifts from a set of fragmented terms to a single indirectly-related group, (3) the core part of the <b>index</b> <b>terms</b> comes to be occupied by terms which represent concepts other than concrete objects...|$|R
5000|$|In fuzzy-set theory, {{an element}} has a varying degree of membership, say dA, {{to a given}} set A instead of the {{traditional}} membership choice (is an element/is not an element).In MMM each <b>index</b> <b>term</b> has a fuzzy set associated with it. A document's weight with respect to an <b>index</b> <b>term</b> A {{is considered to be}} the degree of membership of the document in the fuzzy set associated with A. The degree of membership for union and intersection are defined as follows in Fuzzy set theory: ...|$|E
5000|$|The {{index of}} a book may report any number of {{references}} for a given <b>index</b> <b>term,</b> and thus may be coded as a multimap from index terms to any number of reference locations or pages.|$|E
50|$|TREX {{supports}} {{various kinds}} of text search, including exact search, boolean search, wildcard search, linguistic search (grammatical variants are normalized for the index search) and fuzzy search (input strings that differ by a few letters from an <b>index</b> <b>term</b> are normalized for the index search). Result sets are ranked using term frequency-inverse document frequency (tf-idf) weighting, and results can include snippets with the search terms highlighted.|$|E
40|$|In {{this paper}} the GK# {{model for the}} {{construction}} of thesaurus classes based on fuzzy semantic association measure between <b>index</b> <b>terms</b> and concepts (thesaurus classes) is presented. The association measure is obtained on the basis of fuzzy semantic relations between <b>index</b> <b>terms,</b> and it is used to cluster <b>index</b> <b>terms</b> into concepts. A hierarchical algorithm is introduced which runs on a simple numerical example. # 2001 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved...|$|R
40|$|This paper {{describes}} a technique for automatically creating an index for handwritten notes captured as digital ink. No text recognition is performed. Rather, a dictionary of possible <b>index</b> <b>terms</b> is built by clustering groups of ink strokes corresponding roughly to words. Terms whose distribution varies significantly across note pages are {{selected for the}} index. An index page containing the <b>index</b> <b>terms</b> is created, and terms are hyper-linked back to their original location in the notes. Further, <b>index</b> <b>terms</b> occurring in a note page are highlighted to aid browsing. 1...|$|R
50|$|Each record {{contains}} a bibliographic citation, abstract, <b>index</b> <b>terms</b> from the Thesaurus of Psychological <b>Index</b> <b>Terms,</b> keywords, classification categories, population information, the geographical {{location of the}} research population, and cited references for journal articles, book chapters, and books, mainly from 2001 to present. Records of books include the book's table to contents.|$|R
5000|$|Bibliographic {{records are}} usually retrievable from bibliographic indexes (e.g., {{contemporary}} bibliographic databases) by author, title, <b>index</b> <b>term,</b> or keyword. Bibliographic records {{can also be}} referred to as surrogate records or metadata [...] Bibliographic records can represent a wide variety of published contents, including traditional paper, digitized, or born-digital publications. The process of creation, exchange, and preservation of bibliographic records are parts of a larger process, called bibliographic control.|$|E
5000|$|The Subject Headings Authority File (Schlagwortnormdatei) or SWD is a {{controlled}} vocabulary <b>index</b> <b>term</b> system used primarily for subject indexing in library catalogs. The SWD is {{managed by the}} German National Library (DNB) in cooperation with various library networks. The inclusion of keywords in the SWD is defined by [...] "Rules for the Keyword Catalogue" [...] (RSWK). Similar authority systems in other languages include the Library of Congress Subject Headings (LCSH) and the [...] (Répertoire d'autorité-matière encyclopédique et alphabétique unifié). Since April 2012 the SWD {{is part of the}} Integrated Authority File (Gemeinsame Normdatei or GND).|$|E
50|$|The SafetyLit Thesaurus is {{the basis}} for hybrid textword+synonym search capacity. There are often many {{synonyms}} and spelling variants for potential search terms. New terms and synonyms are added weekly. Work on the SafetyLit thesaurus is not finished, so the full hierarchical search system is not yet complete. However, some term hierarchies such as geographic area names are available for searching. For example, an <b>index</b> <b>term</b> search using ‘Australia’ will find articles that contain the words ‘Canberra’, ‘Adelaide’, or ‘Perth’, even if the SafetyLit records do not contain the word ‘Australia’. The SafetyLit Thesaurus is built on a modified version of TemaTres open-source software.|$|E
40|$|Most {{information}} retrieval systems use stopword lists and stemming algorithms. However, {{we have found}} that recognizing singular and plural nouns, verb forms, negation, and prepositions can produce dramatically different text classification results. We present results from text classification experiments that compare relevancy signatures, which use local linguistic context, with corresponding <b>indexing</b> <b>terms</b> that do not. In two different domains, relevancy signatures produced better results than the simple <b>indexing</b> <b>terms.</b> These experiments suggest that stopword lists and stemming algorithms may remove or conflate many words {{that could be used to}} create more effective <b>indexing</b> <b>terms.</b> Introduction Most {{information retrieval}} systems use a stopword list to prevent common words from being used as <b>indexing</b> <b>terms.</b> Highly frequent words, such as determiners and prepositions, are not considered to be content words because they appear in virtually every document. Stopword lists are almost univer [...] ...|$|R
40|$|When {{professional}} indexers independently assign {{terms to}} a given document, the term sets generally differ between indexers. Studies of inter-indexer consistency measure the percentage of matching <b>index</b> <b>terms,</b> {{but none of them}} consider the semantic relationships that exist amongst these terms. We propose to represent multiple-indexers data in a vector space and use the cosine metric as a new consistency measure that can be extended by semantic relations between <b>index</b> <b>terms.</b> We believe that this new measure is more accurate and realistic than existing ones and therefore more suitable for evaluation of automatically extracted <b>index</b> <b>terms...</b>|$|R
40|$|International audienceMost Information {{retrieval}} systems {{represent a}} query, also a document, as {{a bag of}} <b>indexing</b> <b>terms</b> without any relation between each other. This bag-based representation causes a problem for specialists when they deal with a specific domain like medical one. We present {{an alternative to the}} bag of <b>indexing</b> <b>terms</b> representation depending on semantic query structuring, in order to fulfill this need of precision in a specific domain. This structure of a query is obtained by grouping <b>indexing</b> <b>terms</b> using pre-defined categories called dimensions. These dimensions represent the different aspects that could appear in a query or a document. By using this notion, the relevant document to a given query should not only has a maximum number of shared <b>indexing</b> <b>terms</b> but also have a similar structure. Experimental results show precision improvement related to the granularity of dimensions and its distribution over the whole corpus...|$|R
5000|$|A quarry of PsycINFO {{database}} completed August 3, 2015 {{contained the}} key phrase: [...] "Peabody Picture Vocabulary Test" [...] and Year: [...] "1959 To 2015" [...] located 451 publications. Only publications that used Peabody Picture Vocabulary Test {{in the title}} were collected. The 451 PPVT publications from 1959 through 2015 were organized into the following groups: <b>index</b> <b>term</b> to find the article, name of author who published multiple articles, publication type, author affiliation, reference source, age group of individuals who took the test, research methodology used in article, names of specific tests and measures used in the article, and classification of article by topic. Not all of the references with [...] "Peabody Picture Vocabulary Test" [...] in the title would fall within the classifications.|$|E
50|$|An <b>index</b> <b>term,</b> subject term, subject heading, or descriptor, in {{information}} retrieval, {{is a term}} that captures {{the essence of the}} topic of a document. Index terms make up a controlled vocabulary for use in bibliographic records. They {{are an integral part of}} bibliographic control, which is the function by which libraries collect, organize and disseminate documents. They are used as keywords to retrieve documents in an information system, for instance, a catalog or a search engine. A popular form of keywords on the web are tags which are directly visible and can be assigned by non-experts. Index terms can consist of a word, phrase, or alphanumerical term. They are created by analyzing the document either manually with subject indexing or automatically with automatic indexing or more sophisticated methods of keyword extraction. Index terms can either come from a controlled vocabulary or be freely assigned.|$|E
50|$|Automated {{extraction}} indexing {{may lead}} to loss of meaning of terms by indexing single words as opposed to phrases. Although {{it is possible to}} extract commonly occurring phrases, it becomes more difficult if key concepts are inconsistently worded in phrases. Automated extraction indexing also has the problem that, even with use of a stop-list to remove common words, some frequent words may not be useful for allowing discrimination between documents. For example, the term glucose is likely to occur frequently in any document related to diabetes. Therefore use of this term would likely return most or all the documents in the database. Post-co-ordinated indexing where terms are combined at the time of searching would reduce this effect but the onus would be on the searcher to link appropriate terms as opposed to the information professional. In addition terms that occur infrequently may be highly significant for example a new drug may be mentioned infrequently but the novelty of the subject makes any reference significant. One method for allowing rarer terms to be included and common words to be excluded by automated techniques would be a relative frequency approach where frequency of a word in a document is compared to frequency in the database as a whole. Therefore a term that occurs more often in a document than might be expected based {{on the rest of the}} database could then be used as an <b>index</b> <b>term,</b> and terms that occur equally frequently throughout will be excluded.Another problem with automated extraction is that it does not recognise when a concept is discussed but is not identified in the text by an indexable keyword.|$|E
5000|$|... {{one or more}} {{preferred}} <b>index</b> <b>terms</b> (at most one in each natural language) ...|$|R
40|$|A {{method of}} drawing <b>index</b> <b>terms</b> from text is presented. The {{approach}} uses no stop list, stemmer, or other language-and domain-specific component, allowing operation {{in any language}} or domain with only trivial modification. The method uses n-gram counts, achieving a function similar to, but more general than, a stemmer. The generated <b>index</b> <b>terms,</b> which the author calls “highlights, ” are suitable for identifying the topic for perusal and selection. An extension is also described and demonstrated which selects <b>index</b> <b>terms</b> to represent a subset of documents, distinguishing them from the corpus. Some experimental results are presented, showing operation in English, Spanish, German, Georgian, Russian, and Japanese...|$|R
40|$|This paper {{explores the}} {{effectiveness}} of <b>index</b> <b>terms</b> more complex than single words in conventional information retrieval systems. Retrieval is performed in two phases. In the first phase, a conventional retrieval method (the Okapi system) is used {{and in the second}} phase, complex <b>index</b> <b>terms</b> such as syntactic relations and single words with part of speech information are introduced to rerank the results of the first phase. The effectiveness of the different types of <b>index</b> <b>terms</b> are evaluated through experiments, in which the TREC- 7 test collection and 50 queries are used. The experiments show that retrieval effectiveness was improved for 32 out of 50 queries...|$|R
40|$|The {{invention}} {{relates to}} a feature weighing computation method {{based on a}} structural constraint in a Chinese information retrieval. The method comprises the following steps: a. carrying out structuring processing to inquiry and obtaining a structuring inquiry result, wherein the structuring processing comprises one or a plurality of following steps: splitting words, carrying out part-of-speech tagging to splitted works, carrying out shallow parsing to inquiry or carrying out parsing to inquiry; b. determining an <b>index</b> <b>term</b> according to the structuring inquiry result and then determining an inquiry-context property set of the <b>index</b> <b>term</b> according to the structuring inquiry result which is adjacent to the <b>index</b> <b>term</b> and positioned in a word list; c. computing the weighing value of each property in the inquiry-context property set; d. combining the weighing values of all properties into property values of the index terms through a first composite function; and f. combining the property values of the index terms by a second composite function and obtaining the <b>index</b> <b>term</b> weighing. The method can compute the weighing accurately no matter whether the index terms exist in the word list or not. 本发明是有关于一种中文信息检索中基于结构约束的特征权重计算方法，包括以下步骤：a. 对查询进行结构化处理，得到结构化查询结果；结构化处理包括：分词、对切分出的词进行词性标注、对查询进行浅层句法分析或对查询进行句法分析中一个或几个；b. 根据述结构化查询结果确定索引词，然后根据与所述索引词相邻并位于词列表中的结构化查询的结果，确定所述索引词的查询—上下文属性集；c. 计算查询—上下文属性集中每个属性的权重值；d. 通过第一组合函数将各个属性的权重值组合成所述索引词的属性值；e. 使用第二组合函数对所述索引词的属性值组合，得到所述索引词权重。无论索引词是否在词列表中，本发明的方法都能准确计算出其权重。Department of ComputingInventor name used in this publication: 陆永邦, Lu YongbangTitle in Traditional Chinese: 中文信息檢索中基於結構約束的索引詞權重計算方法Chin...|$|E
40|$|Abstract [...] In this paper, {{an inverse}} optimal control problem is {{introduced}} with state function governed by fractional partial differential equation. The {{existence of the}} control and necessary optimality conditions are proved. <b>Index</b> <b>Term</b> [...] fractional calculus, partial differential equations, Optimal control. 1...|$|E
40|$|Abstract [...] In {{this paper}} {{we present a}} {{generalized}} method to obtain exact solutions of nonlinear partial differential equations. As a particular case, we obtain exact solutions for reaction diffusion equation. A new class of multiple-soliton or wave trains obtained. <b>Index</b> <b>Term</b> [...] Exact solution, Reaction diffusion equatio...|$|E
40|$|As an {{information}} retrieval system, PubMed® aims at providing efficient {{access to documents}} cited in MEDLINE®. For this purpose, it relies on matching representations of documents, as provided by authors and indexers to user queries. In this paper, we describe the growth of author keywords in biomedical journal articles and present a comparative study of author keywords and MeSH® <b>indexing</b> <b>terms</b> assigned by MEDLINE indexers to PubMed Central Open Access articles. A similarity metric is used to assess automatically the relatedness between pairs of author keywords and <b>indexing</b> <b>terms.</b> A set of 300 pairs is manually reviewed to evaluate the metric and characterize the relationships between author keywords and <b>indexing</b> <b>terms.</b> Results show that author keywords are increasingly available in biomedical articles and that over 60 % of author keywords {{can be linked to}} a closely related <b>indexing</b> <b>term.</b> Finally, we discuss the potential impact of this work on indexing and terminology development...|$|R
5000|$|... 2000 - Additional IV <b>Index</b> <b>terms</b> were added: 60, 90, 120, 150, 180, 360, 720 ...|$|R
5000|$|SSA (Salient Semantic Analysis) which <b>indexes</b> <b>terms</b> using salient {{concepts}} {{found in}} their immediate context.|$|R
40|$|IRRA (IR-Ra) group {{participated in}} the 2010 Web track. In this year, the major concern {{is to examine the}} effect of {{supplementary}} methods on the effectiveness of the new nonparametric <b>index</b> <b>term</b> weighting model, divergence from independence (DFI). Every written text document contains words, but the words used in individual documents may diffe...|$|E
40|$|Abstract [...] This paper {{reports the}} {{studies on the}} inversion-layer {{mobility}} in n-channel Poly-Si TFT’s with 1016 cm- 3 substrate impurity concentration. The validity and limitations of the universal relationship between the inversion layer mobility and the effective normal field (Eeff) was examined. <b>Index</b> <b>Term</b> – TFT, Grain boundaries, carrier mobility, polysilicon I...|$|E
30|$|Here, pi =[*]ri/r, qi =[*](fi −[*]ri)/(f[*]−[*]r), f {{refers to}} amount number of {{document}} {{in the training}} document set. r {{is the number of}} document related to user query in the training document set. fi represents a number of document, including <b>index</b> <b>term</b> ki in the training document set. Ri is the number of document, including ki in r relation documents.|$|E
5000|$|Extraction {{indexing}} involves taking words {{directly from}} the document. It uses natural language and lends itself well to automated techniques where word frequencies are calculated and those with a frequency over a pre-determined threshold are used as <b>index</b> <b>terms.</b> A stop-list containing common words (such as [...] "the", [...] "and") would be referred to and such stop words would be excluded as <b>index</b> <b>terms.</b>|$|R
40|$|Experimenting with {{different}} mathematical objects for text representation {{is an important}} step of building text classification models. In order to be efficient, such objects of a formal model, like vectors, have to reasonably reproduce language-related phenomena such as word meaning inherent in <b>index</b> <b>terms.</b> We introduce an algorithm for sense-based semantic ordering of <b>index</b> <b>terms</b> which approximates Cruse’s description of a sense spectrum. Following semantic ordering, text classification by support vector machines can benefit from semantic smoothing kernels that regard semantic relations among <b>index</b> <b>terms</b> while computing document similarity. Adding expansion terms to the vector representation can also improve effectiveness. This paper proposes a new kernel which discounts less important expansion terms based on lexical relatedness. ...|$|R
40|$|Abstract. This paper {{presents}} a new method that aims at improving semantic indexing while {{reducing the number}} of <b>indexing</b> <b>terms.</b> <b>Indexing</b> <b>terms</b> are determined using a minimum redundancy cut in a hierarchy of conceptual hypernyms provided by an ontology (e. g. WordNet, EDR). The results of some information retrieval experiments carried out on several standard document collections using the EDR ontology are presented, illustrating the benefit of the method. ...|$|R
