5|31|Public
40|$|Based on the {{relevant}} case law produced at the European level, this article attempts to examine three controversial aspects of price squeeze: (i) {{the identities of}} the infringer, (ii) the nature of the infringement (i. e. stand-alone or value-added), and (iii) the approach for the <b>imputation</b> <b>test.</b> status: publishe...|$|E
40|$|A {{vertically}} integrated {{firm that}} wholesales to its retail rivals can, {{if it has}} sufficient market power, set the margin between its retail and wholesale prices so as to harm its rivals. Conventionally, an <b>imputation</b> <b>test</b> is used to determine whether such behavior is being undertaken. Such tests are common in electronic communications, and the EC calls for their potentially intensive ex ante application in the supply of NGANs. This paper shows that while imputation tests are helpful analytical tools for understanding the nature of price squeezes, difficulties associated with implementation, which are sharp in an NGAN context, can make them misleading in practice. Instead, price squeezes are best dealt with through the rigorous comparison of expected outcomes, given the alleged anticompetitive behavior, with the outcomes expected in that behavior’s absence. Such analysis is not suited to ex ante application. price squeeze, imputation tests, next generation access networks, vertical discrimination, electronic communications, regulation...|$|E
40|$|Generative {{models of}} protein {{structure}} enable researchers {{to predict the}} behavior of proteins under different conditions. Continuous graphical models are powerful and efficient tools for modeling static and dynamic distributions, {{which can be used}} for learning generative models of molecular dynamics. In this thesis, we develop new and improved continuous graphical models, to be used in modeling of protein structure. We first present von Mises graphical mod-els, and develop consistent and efficient algorithms for sparse structure learning and parameter estimation, and inference. We compare our model to sparse Gaussian graphical model and show it outperforms GGMs on synthetic and Engrailed protein molecular dynamics datasets. Next, we develop algorithms to estimate Mixture of von Mises graphical models using Expectation Maximization, and show that these models outperform Von Mises, Gaussian and mixture of Gaussian graphical mod-els in terms of accuracy of prediction in <b>imputation</b> <b>test</b> of non-redundant protein structure datasets. We then use non-paranormal and nonparametric graphical mod...|$|E
5000|$|HPlus [...] - [...] A {{software}} package for <b>imputation</b> and <b>testing</b> of haplotypes in association studies using a modified method that incorporates the expectation-maximization algorithm and a Bayesian method known as progressive ligation.|$|R
40|$|Introduction. The {{microarray}} datasets {{from the}} MicroArray Quality Control (MAQC) project have enabled {{the assessment of}} the precision, comparability of microarrays, and other various microarray analysis methods. However, to date no studies that we are aware of have reported the performance of missing value imputation schemes on the MAQC datasets. In this study, we use the MAQC Affymetrix datasets to evaluate several imputation procedures in Affymetrix microarrays. Results. We evaluated several cutting edge imputation procedures and compared them using different error measures. We randomly deleted 5 % and 10 % of the data and imputed the missing values using <b>imputation</b> <b>tests.</b> We performed 1000 simulations and averaged the results. The results for both 5 % and 10 % deletion are similar. Among the imputation methods, we observe the local least squares method with is most accurate under the error measures considered. The k-nearest neighbor method with has the highest error rate among imputation methods and error measures. Conclusions. We conclude for imputing missing values in Affymetrix microarray datasets, using the MAS 5. 0 preprocessing scheme, the local least squares method with has the best overall performance and k-nearest neighbor method with has the worst overall performance. These results hold true for both 5 % and 10 % missing values...|$|R
40|$|In recent years, {{genome-wide}} association {{approaches have}} proven {{a powerful and}} successful strategy to identify genetic contributors to complex traits, including a number of endocrine disorders. Their success has meant that genome wide association studies (GWAS) are fast becoming the default study design for discovery of new genetic variants that influence a clinical trait or phenotype. This chapter focuses {{on a number of}} key elements that require consideration for the successful conduct of a GWAS. Although many of the considerations are common to any genetic study, the greater cost, extreme multiple testing, and greater openness to data sharing require specific awareness and planning by investigators. In the section on designing a GWAS, we reflect on ethical considerations, study design, selection of phenotype/s, power considerations, sample tracking and storage issues, and genotyping product selection. During execution, important considerations include DNA quantity and preparation, genotyping methods, quality control checks of genotype data, in silico genotyping (<b>imputation),</b> <b>tests</b> of association, and replication of associa-tion signals. Although the field of human genetics is rapidly evolving, recent experiences can help guide an investigator in making practical and methodological choices that will eventually determine the overall quality of GWAS results. Given the investment to recruit patient populations or cohorts that are powered for a GWAS, and the still substantial costs associated with genotyping, it is helpful to be aware of these aspects to maximize the likelihood of success, especially where there is an opportunity for implementing them prospectively...|$|R
40|$|Generative {{models of}} protein {{structure}} enable researchers {{to predict the}} behavior of proteins under different conditions. Continuous graphical models are powerful and efficient tools for modeling static and dynamic distributions, {{which can be used}} for learning generative models of molecular dynamics. In this thesis, we develop new and improved continuous graphical models, to be used in modeling of protein structure. We first present von Mises graphical models, and develop consistent and efficient algorithms for sparse structure learning and parameter estimation, and inference. We compare our model to sparse Gaussian graphical model and show it outperforms GGMs on synthetic and Engrailed protein molecular dynamics datasets. Next, we develop algorithms to estimate Mixture of von Mises graphical models using Expectation Maximization, and show that these models outperform Von Mises, Gaussian and mixture of Gaussian graphical models in terms of accuracy of prediction in <b>imputation</b> <b>test</b> of non-redundant protein structure datasets. We then use non-paranormal and nonparametric graphical models, which have extensive representation power, and compare several state of the art structure learning methods that can be used prior to nonparametric inference in reproducing kernel Hilbert space embedded graphical models. To be {{able to take advantage of}} the nonparametric models, we also propose feature space embedded belief propagation, and use random Fourier based feature approximation in our proposed feature belief propagation, to scale the inference algorithm to larger datasets. To improve the scalability further, we also show the integration of Coreset selection algorithm with the nonparametric inference, and show that the combined model scales to large datasets with very small adverse effect on the quality of predictions. Finally, we present time varying sparse Gaussian graphical models, to learn smoothly varying graphical models of molecular dynamics simulation data, and present results on CypA protei...|$|E
40|$|Defence date: 28 May 2014 Examining Board: Professor Heike Schweitzer (supervisor), Freie Universität Berlin Professor Thomas Fetzer, University of Mannheim Professor Pierre Larouche, Tilburg University Professor Giorgio Monti, EUI. Received the The Institute of Competition Law 2015 Concurrences PhD Award. A {{margin squeeze}} is an exclusionary form of abuse of a {{dominant}} position that a vertically integrated firm can implement when it sells its upstream bottleneck input to its downstream competitors. Because it is vertically integrated, the dominant incumbent {{can reduce the}} margin between the input price charged to competitors and the retail price charged to end-users by either raising {{the price of the}} input and/or lowering the price of its retail product/services {{to such an extent that}} the remaining margin of profit is insufficient for its rivals to remain competitive. Although the scenario of margin squeeze seems to be rather simple, the underlying economic and legal theories are not. Consequently, detecting a margin squeeze requires competition authorities to apply a complex <b>imputation</b> <b>test,</b> which in turn requires various methodological choices that can determine the outcome of the investigation. The principal purpose of the dissertation is to determine whether the European Commission's margin squeeze decisions are consistent with EU case law. The dissertation examines two alternative hypotheses. Under hypothesis A, margin squeeze is presented as a deviation from the essential facilities doctrine, which could be seen as an expression of regulatory competition law. Hypothesis B assumes that it constitutes another form of vertical foreclosure, the main question then being under what exact conditions foreclosure is likely in network industries where the margin squeeze doctrine traditionally applies. Two conclusions follow from the analysis. First, margin squeeze constitutes another theory of vertical foreclosure, and accordingly cannot be seen as an unjustified deviation from refusal to deal and essential facilities cases. Second, to ensure that the theory of harm in margin squeeze cases is credible, competition authorities could enhance their current analytical framework by regularly reviewing various additional elements, in particular the extent to which the wholesale product is important for downstream competition...|$|E
40|$|Nonparametric {{tests for}} the null {{hypothesis}} that a function has a prescribed form are developed and applied to data sets with missing observations. Omnibus nonparametric tests such as the order selection tests, {{do not need to}} specify a particular alternative parametric form, and have power against a large range of alternatives. More specifically, likelihood-based order selection tests are defined {{that can be used for}} multiply imputed data when the data are missing-at-random. A simulation study and data analysis illustrate the performance of the tests. In addition, an Akaike information criterion for model selection is presented that can be used with multiply imputed datasets. Akaike information criterion Hypothesis <b>test</b> Multiple <b>imputation</b> Lack-of-fit <b>test</b> Missing data Omnibus test Order selection...|$|R
40|$|In {{the past}} few years {{genome-wide}} association (GWA) studies have uncovered a large number of convincingly replicated associations for many complex human diseases. Genotype imputation has been used widely in the analysis of GWA studies to boost power, fine-map associations and facilitate the combination of results across studies using meta-analysis. This Review describes the details of several different statistical methods for imputing genotypes, illustrates and discusses the factors that influence imputation performance, and reviews methods {{that can be used to}} assess <b>imputation</b> performance and <b>test</b> association at imputed SNPs...|$|R
40|$|Of the {{two most}} common forms of genetic {{variation}} in the human genome, Single Nucleotide Polymorphisms (SNPs) and Variable Number Tandem Repeat Polymorphisms (VNTRs), SNPs are much more easily and inexpensively assayed in a high-throughput manner. For this reason, we seek to explore methods that can allow us to use the more readily available SNP genotype information to infer VNTR genotypes in nearby genomic regions. We focus in particular on imputing a VNTR polymorphism, 5 -HTTLPR, in the promoter region of the serotonin transporter gene in {{a small sample of}} individuals from an ongoing neuroimaging genetics study, a portion of whom have both manual 5 -HTTLPR genotypes and genome wide SNP data. We investigate four imputation methods: Tagger, Vertex Discriminant Analysis (VDA), IMPUTE 2, and BEAGLE. We achieve an accuracy of 93 % with VDA in our subsample of Caucasians with manual 5 -HTTLPR genotypes. Further, we find that for the entire Caucasian subsample without manual genotypes, a majority of the <b>imputation</b> methods <b>tested</b> make the same 5 -HTTLPR genotype call. Thesi...|$|R
40|$|Missing and {{incomplete}} information in surveys or databases can be imputed using different statistical and soft-computing techniques. This paper comprehensively compares auto-associative neural networks (NN), neuro-fuzzy (NF) {{systems and the}} hybrid combinations the above methods with hot-deck <b>imputation.</b> The <b>tests</b> are conducted on an eight category antenatal survey and also under principal component analysis (PCA) conditions. The neural network outperforms the neuro-fuzzy system for all tests {{by an average of}} 5. 8 %, while the hybrid method is on average 15. 9 % more accurate yet 50 % less computationally efficient than the NN or NF systems acting alone. The global impact assessment of the imputed data is performed by several statistical tests. It is found that although the imputed accuracy is high, the global effect of the imputed data causes the PCA inter-relationships between the dataset to become altered. The standard deviation of the imputed dataset is on average 36. 7 % lower than the actual dataset which may cause an incorrect interpretation of the results. Comment: 7 page...|$|R
40|$|Ukraine {{belongs to}} the group of {{countries}} which are known for the widespread phenomenon of subsistence and semi-subsistence farming. Individual farmers are not obliged to produce financial reports and their incomes belong to the category of unobservable incomes. When checking the eligibility for social assistance the level of their incomes needs to be estimated. In a country, where poverty rate is quite high, the coverage of the poor with financial aid is relatively low and public finances under constant control, the importance of a fair and justified methodology for income imputation is particularly strong. In this situation, an outdated and unfair current system of agriculture income estimation in Ukraine calls for immediate changes. This paper presents recommendations for the Ukrainian government in the area of agriculture income imputation, where several methods of estimating farm income were proposed (including the one based on Household Budget Survey). The recommendations were preceded with the analysis of five countries' practices in this area: Kazakhstan, Kyrgyzstan, Moldova, Russia, and Poland. A review of different means testing methods, including direct means testing and proxy means testing, served as an introduction to the topic. subsistence and semi-subsistence farming, hard to verify income, farm household income, income (agro-income) <b>imputation,</b> means <b>testing</b> methods...|$|R
40|$|Deciphering {{important}} {{genes and}} pathways from incomplete gene expression data could facilitate {{a better understanding}} of cancer. Different imputation methods can be applied to estimate the missing values. In our study, we evaluated various imputation methods for their performance in preserving significant genes and pathways. In the first step, 5 % genes are considered in random for two types of ignorable and non-ignorable missingness mechanisms with various missing rates. Next, 10 well-known imputation methods were applied to the complete datasets. The significance analysis of microarrays (SAM) method was applied to detect the significant genes in rectal and lung cancers to showcase the utility of imputation approaches in preserving significant genes. To determine the impact of different imputation methods on the identification of important genes, the chi-squared test was used to compare the proportions of overlaps between significant genes detected from original data and those detected from the imputed datasets. Additionally, the significant genes are tested for their enrichment in important pathways, using the ConsensusPathDB. Our results showed that almost all the significant genes and pathways of the original dataset can be detected in all imputed datasets, indicating that there is {{no significant difference in the}} performance of various <b>imputation</b> methods <b>tested.</b> The source code and selected datasets are available on [URL]...|$|R
40|$|Objectives To assess micro-simulation {{for testing}} policy options under {{demographic}} ageing. Methods Individual-level data {{were drawn from}} the New Zealand Health Survey (1996 / 7 and 2002 / 3), {{a national survey of}} ambulatory care in New Zealand (2001 / 2), and the Australian National Health Survey (1995). Health service effects assessed were visits to the family doctor, and rates of prescribing and referral. We created a representative set of synthetic health histories by <b>imputation</b> and <b>tested</b> the health service effects of different policy scenarios. These were created by varying ageing and morbidity trajectories, degree of social support available, and intensity of practitioner behaviour. Results The set of synthetic health histories created by combining the data sources generated outcomes reasonably close to external benchmarks. Altering the age distribution of 2002 to approximate settings for 2021 produced no change in rates of visiting, prescribing, or referral for the 65 -and-over population. Quantifying the health service effects of different scenarios showed no impact on visit rates by varying social support, but substantial differences for visits between high and low morbidity scenarios and for prescribing and referral rates according to practitioner behaviour. Conclusions There is potential for micro-simulation to assist in the synthesis of data and to help quantify scenario options for policy development. Health policy Demographic aging Computer simulation...|$|R
40|$|Knowledge of {{biological}} relatedness between samples {{is important for}} many genetic studies. In large-scale human genetic association studies, the estimated kinship is used to remove cryptic relatedness, control for family structure, and estimate trait heritability. However, estimation of kinship is challenging for sparse sequencing data, such as those from off-target regions in target sequencing studies, where genotypes are largely uncertain or missing. Existing methods often assume accurate genotypes at {{a large number of}} markers across the genome. We show that these methods, without accounting for the genotype uncertainty in sparse sequencing data, can yield a strong downward bias in kinship estimation. We develop a computationally efficient method called SEEKIN to estimate kinship for both homogeneous samples and heterogeneous samples with population structure and admixture. Our method models genotype uncertainty and leverages linkage disequilibrium through <b>imputation.</b> We <b>test</b> SEEKIN on a whole exome sequencing dataset (WES) of Singapore Chinese and Malays, which involves substantial population structure and admixture. We show that SEEKIN can accurately estimate kinship coefficient and classify genetic relatedness using off-target sequencing data down sampled to ~ 0. 15 X depth. In application to the full WES dataset without down sampling, SEEKIN also outperforms existing methods by properly analyzing shallow off-target data (~ 0. 75 X). Using both simulated and real phenotypes, we further illustrate how our method improves estimation of trait heritability for WES studies...|$|R
40|$|We {{introduce}} a general integer programming formulation {{for a class}} of combinatorial op-timization games, which include many interesting problems on graphs. The formulation im-mediately allows us to improve the algorithmic result for finding imputations in the core (an important solution concept in cooperative game theory) of the network flow game on unit networks. An important result is a general theorem that the core for this class of games is nonempty {{if and only if}} a related linear program has an integer optimal solution. We study the properties for this mathematical condition to hold for several problems on graphs, and apply them to resolve algorithmic and complexity issues for their cores: decide whether the core is empty; if the core is empty, find an imputation in the core; given an <b>imputation</b> $x $, <b>test</b> whether $x $ is in the core. ...|$|R
40|$|Results of the {{analyses}} of occupational and environmen-tal samples are frequently reported as “less than a specified value, ” a practice followed by many analytical laboratories. A left-censored distribution occurs when analytical laboratories do not report results that fall below their limits of detection or quantification. Approximately 37 % of the household interior dust lead loadings collected in a large-scale, multisite, longi-tudinal study of lead-based paint hazard controls were reported to be below the “method detection limit. ” These unreported val-ues are unusable in any statistical {{analysis of the data}} and must be replaced by a valid dust lead loading estimate, a process called data <b>imputation.</b> This investigation <b>tested</b> how well data imputed using a newly formulated procedure for estimating the data below the method detection limit were correlated with dust lead loadings reported by the participating laboratorie...|$|R
40|$|The {{new field}} of {{research}} called Knowledge Discovery in Databases (KDD) aims at tearing down the last barrier in enterprises' information flow, the data analysis step. This is done by developing and integrating data mining algorithms. A neuro-fuzzy-system can be such a data mining tool. After introducing the KDD process and differentiating it from data mining we explain the basics of neuro-fuzzy-systems. As one possible system we pick out the NEFCLASS architecture. After that we examine the appropriateness of such a neuro-fuzzy-system for data mining. Starting out from the problem areas identified, we develop a concept for an integrated KDD system based on NEFCLASS. One part of this concept, {{the solution of the}} missing-values-problem deserves closer examination. To this end we introduce different <b>imputation</b> algorithms and <b>test</b> them on real data. Finally, we propose a draft for an implementation of the overall concept worked out before. ...|$|R
40|$|Human genetic {{variation}} contributes {{to differences in}} susceptibility to HIV- 1 infection. To search for novel host resistance factors, we performed a genome-wide association study (GWAS) in hemophilia patients highly exposed to potentially contaminated factor VIII infusions. Individuals with hemophilia A and a documented history of factor VIII infusions before the introduction of viral inactivation procedures (1979 - 1984) were recruited from 36 hemophilia treatment centers (HTCs), and their genome-wide genetic variants were compared with those from matched HIV-infected individuals. Homozygous carriers of known CCR 5 resistance mutations were excluded. Single nucleotide polymorphisms (SNPs) and inferred copy number variants (CNVs) were tested using logistic regression. In addition, we performed a pathway enrichment analysis, a heritability analysis, and a search for epistatic interactions with CCR 5 Δ 32 heterozygosity. A total of 560 HIV-uninfected cases were recruited: 36 (6. 4 %) were homozygous for CCR 5 Δ 32 or m 303. After quality control and SNP <b>imputation,</b> we <b>tested</b> 1 081 435 SNPs and 3686 CNVs for association with HIV- 1 serostatus in 431 cases and 765 HIV-infected controls. No SNP or CNV reached genome-wide significance. The additional analyses did not reveal any strong genetic effect. Highly exposed, yet uninfected hemophiliacs form an ideal study group to investigate host resistance factors. Using a genome-wide approach, we did not detect any significant associations between SNPs and HIV- 1 susceptibility, indicating that common genetic variants of major effect are unlikely to explain the observed resistance phenotype in this populatio...|$|R
40|$|Background: Environmental epidemiology, when {{focused on}} the life course of {{exposure}} to a specific pollutant, requires historical exposure estimates {{that are difficult to}} obtain for the full time period due to gaps in the historical record, especially in earlier years. We show that these gaps can be filled by applying multiple imputation methods to a formal risk equation that incorporates lifetime exposure. We also address challenges that arise, including choice of imputation method, potential bias in regression coefficients, and uncertainty in age-at-exposure sensitivities. Methods: During time periods when parameters needed in the risk equation are missing for an individual, the parameters are filled by an imputation model using group level information or interpolation. A random component is added to match the variance found in the estimates for study subjects not needing imputation. The process is repeated to obtain multiple data sets, whose regressions against health data can be combined statistically to develop confidence limits using Rubin’s rules to account for the uncertainty introduced by the <b>imputations.</b> To <b>test</b> for possible recall bias between cases and controls, which can occur when historical residence location is obtained by interview, and which can lead to misclassification of imputed exposure by disease status, we introduce an “incompleteness index, ” equal to the percentage of dose imputed (PDI) for a subject. “Effective doses ” can be computed using different functional dependencies of relative risk on age of exposure, allowing intercomparison of different risk models. To illustrate our approach, we quantify lifetime exposure (dose) from traffic air pollution in a...|$|R
40|$|Inspired {{by recent}} successes of deep {{learning}} in computer vision, we propose a novel framework for encoding time series as {{different types of}} images, namely, Gramian Angular Summation/Difference Fields (GASF/GADF) and Markov Transition Fields (MTF). This {{enables the use of}} techniques from computer vision for time series classification and imputation. We used Tiled Convolutional Neural Networks (tiled CNNs) on 20 standard datasets to learn high-level features from the individual and compound GASF-GADF-MTF images. Our approaches achieve highly competitive results when compared to nine of the current best time series classification approaches. Inspired by the bijection property of GASF on 0 / 1 rescaled data, we train Denoised Auto-encoders (DA) on the GASF images of four standard and one synthesized compound dataset. The <b>imputation</b> MSE on <b>test</b> data is reduced by 12. 18 %- 48. 02 % when compared to using the raw data. An analysis of the features and weights learned via tiled CNNs and DAs explains why the approaches work. Comment: Accepted by IJCAI- 2015 ML trac...|$|R
40|$|One {{impediment}} to {{the statistical analysis}} of network data has been the difficulty in modeling the dependence among the observations. In the very simple case of binary (0 - 1) network data, some researchers have parameterized network dependence in terms of exponential family representations. Accurate parameter estimation for such models is quite difficult, and {{the most commonly used}} models often display a significant lack of fit. Additionally, such models are generally limited to binary data. In contrast, random effects models have been a widely successful tool in capturing statistical dependence for a variety of data types, and allow for prediction, <b>imputation,</b> and hypothesis <b>testing</b> within a general regression context. We propose novel random effects structures to capture network dependence, which can also provide graphical representations of network structure and variability. 1 Network Dependence Network data typically consist of a set of n nodes and a relational tie yi,j, measured on each ordered pair of nodes i, j = 1, [...] ., n. This framework has many applications, including the study of war, trade, the behavior of epidemics, the interconnectedness of the World Wid...|$|R
40|$|Severe early {{childhood}} caries (S-ECC) affects 17 % of 2 - 3 year old children in South Australia impacting on their general health and well-being. S-ECC is largely preventable by providing mothers with anticipatory guidance. Randomised controlled trials (RCTs) {{are the most}} decisive way to test this, but that approach suffers from near inevitable loss to follow-up that occurs with preventative strategies and distant outcome assessment. We re-examined {{the results of an}} RCT to prevent S-ECC using sensitivity analyses and multiple <b>imputation</b> to <b>test</b> different assumptions about violation of random allocation (1 %) and major loss to follow-up (32 %). Irrespective of any assumptions about missing outcomes, providing expectant mothers with anticipatory guidance during pregnancy and in the child’s first year of life, significantly reduced the incidence of S-ECC at 20 months of age. However, the relative risk of S-ECC varied from 0. 18 (95 % confidence interval (CI) : 0. 06 – 0. 52) to 0. 70 (95 % CI: 0. 56 – 0. 88). Also the ‘number needed to treat’ (NNT) to prevent one case of S-ECC varied 2. 5 -fold: from 8 to 20 women given anticipatory guidance. Multiple imputation provided a best estimate of 0. 25 (95 % CI: 0. 11 – 0. 56) for the relative risk and of 14 (95 % CI: 10 – 33) for the number needed to treat. Avoiding loss to follow-up is crucial in any RCT, but is difficult with preventative health care strategies. Instead of abandoning randomisation in such circumstances, sensitivity analyses and multiple imputation can consolidate the findings of an RCT and add extra value to the conclusions derived from it. Kamila Plutzer, Gloria C. Mejia, A. John Spencer and Marc J. N. C. Keirs...|$|R
40|$|Motivation: Microarray {{experiments}} have revolutionized {{the study of}} gene expression with their ability to generate large amounts of data. This article describes an alternative to existing approaches to clustering of gene expression profiles; the key idea is to cluster in stages using a hierarchy of distance measures. This method is motivated by {{the way in which}} the human mind sorts and so groups many items. The distance measures arise from the orthogonal breakup of Euclidean distance, giving us a set of independent measures of different attributes of the gene expression profile. Interpretation of these distances is closely related to the statistical design of the microarray experiment. This clustering method not only accommodates missing data but also leads to an associated imputation method. Results: The performance of the clustering and <b>imputation</b> methods was <b>tested</b> on a simulated dataset, a yeast cell cycle dataset and a central nervous system development dataset. Based on the Rand and adjusted Rand indices, the clustering method is more consistent with the biological classification of the data than commonly used clustering methods. The imputation method, at varying levels of missingness, outperforms most imputation methods, based on root mean squared error (RMSE). 8 page(s...|$|R
40|$|A common {{practice}} in pre-processing data for hydrological modeling is to ignore observations with any missing variable values {{at any given time}} step, even if it {{is only one of the}} independent variables that is missing. These rows of data are labeled incomplete and would not be used in either model building or subsequent testing and verification steps. This is not necessarily the best way of doing it as information is lost when incomplete rows of data are discarded. Learning algorithms are affected by such problems more than physically-based models as they rely heavily on the data to learn the underlying input/output relationships. In this study, the extent of damage to the performance of the learning algorithm due to missing data is explored in a field-scale application. We have tested and compared the performance of two well-known learning algorithms, namely Artificial Neural Networks (ANNs) and Support Vector Machines (SVMs) for short-term prediction of groundwater levels in a well field. A comparison of these two algorithms is made using various percentages of missing data. In addition to understanding the relative performance of these algorithms in dealing with missing data, a solution in the form of an imputation methodology is proposed for filling the data gaps. The proposed <b>imputation</b> methodology is <b>tested</b> against observed data...|$|R
40|$|Prediction {{models for}} multivariate spatio-temporal {{functions}} in geosciences are typically developed using supervised learning from attributes collected by remote sensing instruments collocated {{with the outcome}} variable provided at sparsely located sites. In such collocated data there are often large temporal gaps due to missing attribute values at sites where outcome labels are available. Our objective is to develop more accurate spatio-temporal predictors by using enlarged collocated data obtained by imputing missing attributes at time and locations where outcome labels are available. The proposed method for large gaps estimation {{in space and time}} (called LarGEST) exploits temporal correlation of attributes, correlations among multiple attributes collected {{at the same time and}} space, and spatial correlations among attributes from multiple sites. LarGEST outperformed alternative methods in imputing up to 80 % of randomly missing observations at a synthetic spatio-temporal signal and at a model of fluoride content in a water distribution system. LarGEST was also applied for imputing 80 % of nonrandom missing values in data from one of the most challenging Earth science problems related to aerosol properties. Using such enlarged data a predictor of aerosol optical depth is developed that was much more accurate than predictors based on alternative <b>imputation</b> methods when <b>tested</b> rigorously over entire continental US in year 2005. ...|$|R
40|$|Meaningful {{relationships}} between forest structure attributes measured in representative field plots {{on the ground}} and remotely sensed data measured comprehensively across the same forested landscape facilitate the production of maps of forest attributes such as basal area (BA) and tree density (TD). Because imputation methods can efficiently predict multiple response variables simultaneously, they may be usefully applied to map several structural attributes at the species-level. We compared several approaches for imputing the response variables BA and TD, aggregated at the plot-scale and species-level, from topographic and canopy structure predictor variables derived from discrete-return airborne LiDAR data. The predictor and response variables were associated using imputation techniques based on normalized and unnormalized Euclidean distance, Mahalanobis distance, Independent Component Analysis (ICA), Canonical Correlation Analysis (aka Most Similar Neighbor, or MSN), Canonical Correspondence Analysis (aka Gradient Nearest Neighbor, or GNN), and Random Forest (RF). To compare and evaluate these approaches, we computed a scaled Root Mean Square Distance (RMSD) between observed and imputed plot-level BA and TD for 11 conifer species sampled in north-central Idaho. We found that RF produced the best results overall, especially after reducing the number of response variables to the most important species in each plot with regard to BA and TD. We concluded that RF was the most robust and flexible among the <b>imputation</b> methods we <b>tested.</b> We also concluded that canopy structure and topographic metrics derived from LiDAR surveys can be very useful for species-level imputation...|$|R
40|$|The main {{objective}} {{of this research was}} to test alternative low density SNP panels to impute Illumina 50 K SNP panel genotypes in Braford and Hereford cattle. Genotypes from 3, 768 Hereford, Braford and Nellore animals were used for <b>testing</b> <b>imputation</b> from low density SNP panels (3 K, 6 K, 8 K, 15 K and 20 K) to the Illumina 50 K SNP panel, under four different scenarios: including or not Nellore genotypes in the reference population in combination with the use or not of pedigree information. There {{were no significant differences in}} imputation accuracy among these four scenarios within each panel. However, significant differences between panels were found. The best accuracy was given by a customized 15 K SNP panel, with an overall genotype concordance rate of 0. 977, with 93. 3 % of the animals imputed with a concordance rate above 0. 95. The concordance rates for the other SNP panels were 0. 872, 0. 952, 0. 957 and 0. 958 for 3 K, 6 K, 8 K and 20 K SNP panel, respectively. Therefore, in the Braford/Hereford population considered in this study, all the alternative panels denser than 3 K could be used for imputing to the 50 K SNP panel with an overall high imputation accuracy. However, the best results were obtained with the customized 15 K SNP instead of the alternative commercial panels. The use of Nellore sire genotypes and pedigree information did not increase accuracy of imputation in this population. 201...|$|R
40|$|Data editing {{plays an}} {{important}} role in the survey process. The National Agricultural Statistics Service currently uses, in addition to some manual editing, an interactive micro-level edit system or a batch micro-level edit system, and an interactive macro-level edit system to edit reported data. Advantages of using these two edit systems are that: 1) the most complex edits can be incorporated and 2) the impact of editing at aggregate levels can be readily evaluated. There are, however, disadvantages with the use of the two edit systems: 1) a considerable amount of time and resources may be expended and 2) editing may not always be performed in a consistent manner. This paper evaluates a generalized automated edit and imputation system developed by the author called the Agricultural Generalized Imputation and Edit System (AGGIES). The AGGIES is appealing for the following reasons: 1) editing and imputation are fully automated, 2) the system provides consistency in the edit and imputation process, and 3) the system can be easily applied to any number of surveys, thus conserving resources to the development and maintenance of a single system. Comparisons between the AGGIES and the current edit and imputation procedures are made for expanded totals and the number and magnitude of variable changes. The data used for these comparisons are obtained from the Quarterly Hog Survey. The results reveal that the expanded totals obtained from using the AGGIES are similar to those obtained from the current edit and <b>imputation</b> procedures. Further <b>testing</b> on more applications is recommended...|$|R
40|$|The {{sources and}} {{processes}} affecting ambient speciated mercury concentrations including gaseous elemental mercury (GEM), gaseous oxidized mercury (GOM) and particulate bound mercury (PBM) at Kejimukujik National Park were identified using positive matrix factorization (PMF) model and {{principal component analysis}} (PCA). Four factors, Combustion Emission, Industrial Source, Photochemistry and Re-emission of Hg, and Sea Salt, were identified in both 2009 and 2010 by PMF model. The factors Photochemistry and Sea Salt {{were found to have}} the largest and smallest impact on ambient speciated mercury concentrations using PMF model, respectively. The components derived from PCA using the same dataset were largely consistent with the factors identified by PMF. A shift of factor impact on mercury concentrations between 2009 and 2010 was observed using both methods. An additional PCA component Gas-particle Partitioning of Hg was identified in 2009 according to the negative relation between GOM and PBM. After including meteorological parameters in the input of PCA, mercury wet deposition, a new factor, was identified in both years. The reproduction of observed GEM concentrations by PMF model was the best among all three mercury forms followed by PBM and GOM. The sensitivity of PMF model to the different treatment to improve the data quality were <b>tested.</b> <b>Imputations</b> and combining or excluding GOM and PBM were found to have no obvious improvement on the model performances. However, increasing the low GOM and PBM concentrations by a scaling factor were effective in improving the model performances. Different treatments of input data had little impacts on factor profiles but factor contributions to Hg were affected to some extent...|$|R
40|$|DNA {{sequence}} variation within {{human leukocyte antigen}} (HLA) genes mediate {{susceptibility to}} a wide range of human diseases. The complex genetic structure of the major histocompatibility complex (MHC) makes it difficult, however, to collect genotyping data in large cohorts. Long-range linkage disequilibrium between HLA loci and SNP markers across the major histocompatibility complex (MHC) region offers an alternative approach through imputation to interrogate HLA variation in existing GWAS data sets. Here we describe a computational strategy, SNP 2 HLA, to impute classical alleles and amino acid polymorphisms at class I (HLA-A, -B, -C) and class II (-DPA 1, -DPB 1, -DQA 1, -DQB 1, and -DRB 1) loci. To characterize performance of SNP 2 HLA, we constructed two European ancestry reference panels, one based on data collected in HapMap-CEPH pedigrees (90 individuals) and another based on data collected by the Type 1 Diabetes Genetics Consortium (T 1 DGC, 5, 225 individuals). We imputed HLA alleles in an independent data set from the British 1958 Birth Cohort (N =  918) with gold standard four-digit HLA types and SNPs genotyped using the Affymetrix GeneChip 500 K and Illumina Immunochip microarrays. We demonstrate that the sample size of the reference panel, rather than SNP density of the genotyping platform, is critical to achieve high imputation accuracy. Using the larger T 1 DGC reference panel, the average accuracy at four-digit resolution is 94. 7 % using the low-density Affymetrix GeneChip 500 K, and 96. 7 % using the high-density Illumina Immunochip. For amino acid polymorphisms within HLA genes, we achieve 98. 6 % and 99. 3 % accuracy using the Affymetrix GeneChip 500 K and Illumina Immunochip, respectively. Finally, we demonstrate how <b>imputation</b> and association <b>testing</b> at amino acid resolution can facilitate fine-mapping of primary MHC association signals, giving a specific example from type 1 diabetes...|$|R
40|$|Imputation is a {{commonly}} used technique that exploits linkage disequilibrium to infer missing genotypes in genetic datasets, using a well characterized reference population. While there is {{agreement that the}} reference population has to match the ethnicity of the query dataset, it is common practice {{to use the same}} reference to impute genotypes {{for a wide variety of}} phenotypes. We hypothesized that using a reference composed of samples with a different phenotype than the query dataset would introduce <b>imputation</b> bias. To <b>test</b> this hypothesis we used GWAS datasets from amyotrophic lateral sclerosis, Parkinson disease, and Crohn disease. First, we masked and then performed imputation of 100 disease-associated markers and 100 non-associated markers from each study. Two references for imputation were used in parallel: one consisting of healthy controls and another consisting of patients with the same disease. We assessed the discordance (imprecision) and bias (inaccuracy) of imputation by comparing predicted genotypes to those assayed by SNP-chip. We also assessed the bias on the observed effect size when the predicted genotypes were used in a GWAS study. When healthy controls were used as reference for imputation, a significant bias was observed, particularly in the disease-associated markers. Using cases as reference significantly attenuated this bias. For nearly all markers, the direction of the bias favored the non-risk allele. In GWAS studies of the three diseases (with healthy reference controls from the 1000 genomes as reference), the mean OR for disease-associated markers obtained by imputation was lower than that obtained using original assayed genotypes. We found that the bias is inherent to imputation as using different methods did not alter the results. In conclusion, imputation is a powerful method to predict genotypes and estimate genetic risk for GWAS. However, a careful choice of reference population is needed to minimize biases inherent to this approac...|$|R
40|$|Functional trait {{databases}} {{are powerful}} tools in ecology, {{though most of}} them contain large amounts of missing values. The goal {{of this study was}} to test the effect of imputation methods on the evaluation of trait values at species level and on the subsequent calculation of functional diversity indices at community level using functional trait databases. Two simple imputation methods (average and median), two methods based on ecological hypotheses, and one multiple <b>imputation</b> method were <b>tested</b> using a large plant trait database, together with the influence of the percentage of missing data and differences between functional traits. At community level, the complete-case approach and three functional diversity indices calculated from grassland plant communities were included. At the species level, one of the methods based on ecological hypothesis was for all traits more accurate than imputation with average or median values, but the multiple imputation method was superior for most of the traits. The method based on functional proximity between species was the best method for traits with an unbalanced distribution, while the method based on the existence of relationships between traits was the best for traits with a balanced distribution. The ranking of the grassland communities for their functional diversity indices was not robust with the complete-case approach, even for low percentages of missing data. With the imputation methods based on ecological hypotheses, functional diversity indices could be computed with a maximum of 30 % of missing data, without affecting the ranking between grassland communities. The multiple imputation method performed well, but not better than single imputation based on ecological hypothesis and adapted to the distribution of the trait values for the functional identity and range of the communities. Ecological studies using functional trait databases have to deal with missing data using imputation methods corresponding to their specific needs and making the most out of the information available in the databases. Within this framework, this study indicates the possibilities and limits of single imputation methods based on ecological hypothesis and concludes that they could be useful when studying the ranking of communities for their functional diversity indices. (Résumé d'auteur...|$|R
40|$|In {{reliability}} theory, it {{is common}} that data are missing due to censoring. This results in an incomplete data set which {{is often difficult to}} analyze. Methods are tested in search of the missing values, creating a fictional complete data set, with the information of when the object tested is most likely to fail. Four methods were, for this purpose, tested in this report: The quick and dirty method, the maximum likelihood method, single imputation and multiple imputation. The quick and dirty method consists of setting the censored times equal to the censored limit. The maximum likelihood estimator calculates the likelihood, taking the censoring limits into account, whereas the imputation methods imputes values for the censored, missing values. Conditional distributions are assumed appropriate as this is a logical conclusion for missing data where the failure time is not observed. Scaled truncation is used in the coding for multiple imputation, helping with the imputation, and both <b>imputation</b> methods were <b>tested</b> with the quick and dirty approach as well as the maximum likelihood approach as a starting point. The methods were implemented in the programming language R, using both own coding and embedded functions available in R. Two numerical examples are tested for all methods, calculating and comparing the gross variances of each of the methods. The gross variance calculates the expected total mean square error, where low values are considered to reveal accurate methods. The maximum likelihood estimator and multiple imputation normally perform the best, giving the lowest gross variances in most of the cases. The quick and dirty method does well for some censoring limits and poorly for others, specifically for censoring limits set far from the censored failure times, and is therefore characterized as an unreliable choice. Single imputation fails rarely, but is usually less exact than the best methods. However, it is a stable method as it consistently gives low gross variances. The accuracy of the methods dealing with censored data could settle guaranty issues that legitimize products, where the importance of reliability studies is increasing. It is also used for the credibility of these products and manufacturers. </p...|$|R
40|$|BACKGROUND: Persons {{with severe}} mental illness (SMI) often get {{extensive}} informal care {{from family members}} and friends as well as substantial amounts of formal treatment from paid professionals. Both sources of care are well documented, but very little is known about how one affects the other. AIMS OF THE STUDY: This analysis estimates the extent of substitution between direct care provided by family and friends and formal treatment for people with severe mental illness and substance use disorders. Separate estimates are generated for short-term and long-term effects. METHODS: Data are from a randomized clinical trial conducted at seven mental health centers in New Hampshire between 1989 and 1995. The study includes detailed data for 193 persons with dual disorders measured at study entry and every six months for three years. Hours of informal care were compared with total treatment costs within each six-month period to measure short-term effects. Average amount of informal care over three years represented long-term caregiving practices. Measures of informal care are from interviews with informal caregivers. Treatment costs are based on combined data from management information systems, Medicaid claims, hospital records, and self reports. We used mixed effects repeated measures regression to estimate longitudinal effects and a multiple <b>imputation</b> technique to <b>test</b> the sensitivity of results to missing data. RESULTS: In the short-term, persons with bipolar disorder used significantly more formal care as informal care increased (complementarity). The relationship between short-term informal and formal care was significantly weaker for persons with schizophrenia. For both diagnostic groups there was a long-term substitution effect; a 4 - 6 % increase in informal care hours was associated with an approximate 1 % decrease in formal care costs. DISCUSSION: Although they must be confirmed by further research, these findings suggest that there is a significant and strong relationship between care given by family and friends and that supplied by formal treatment providers. The analysis indicates that the short-term relationship between informal care and formal treatment tends to be complementary, but differs according to diagnosis. Long-term effects, which are possibly related to changing role perceptions, show substitution between the two forms of care. Missing data for family care hours in some time periods was a concern in this study. However, the consistency in results between the analyses that used imputed data and the model using only original data increase our confidence in the findings. Although there may be some endogeneity between formal and informal care in other treatment settings we believe the unique characteristics of the service-rich environment in which this study was conducted limit that concern here. IMPLICATIONS FOR HEALTH CARE PROVISION AND USE: The amount of care provided by informal caregivers has a significant impact on formal treatment costs. Models of care that explicitly acknowledge the interplay between the two types of care are needed to ensure efficient combinations of formal and informal care. IMPLICATIONS FOR HEALTH POLICY FORMULATION: How to best to encourage informal support, without overburdening caregivers, is a key challenge facing policy makers and providers of mental health services. The merits of various approaches to reducing caregiver burden is a subject that needs more attention from researchers. In the interim, the demands on informal caregivers may mount as efforts to reduce health care spending continue. IMPLICATIONS FOR FURTHER RESEARCH: Informal care is not often included in economic evaluations of mental health treatment. Although additional research is needed to understand better the mechanisms by which informal care and formal treatment are related, we believe our results offer a strong argument for including measures of informal care in future economic evaluations...|$|R
