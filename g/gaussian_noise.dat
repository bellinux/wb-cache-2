9605|2240|Public
5|$|Phase {{truncation}} spurs can {{be reduced}} substantially {{by the introduction of}} white <b>gaussian</b> <b>noise</b> prior to truncation. The so-called dither noise is summed into the lower W+1 bits of the PA output word to linearize the truncation operation. Often the improvement can be achieved without penalty because the DAC noise floor tends to dominate system performance. Amplitude truncation spurs can not be mitigated in this fashion. Introduction of noise into the static values held in the PAC ROMs would not eliminate the cyclicality of the truncation error terms and thus would not achieve the desired effect.|$|E
25|$|A continuous-time analog {{communications}} channel {{subject to}} <b>Gaussian</b> <b>noise</b> — see Shannon–Hartley theorem.|$|E
25|$|Since W is orthogonal, the {{estimation}} problem amounts to recovery of a signal in iid <b>Gaussian</b> <b>noise.</b> As p is sparse, one {{method is to}} apply a Gaussian mixture model for p.|$|E
30|$|Pattern C: <b>Gaussian</b> white <b>noise.</b> <b>Gaussian</b> white <b>noise</b> {{is added}} to the {{original}} noiseless image I with zero mean and variance 5.|$|R
40|$|Directed {{transport}} of overdamped Brownian particles driven by fractional <b>Gaussian</b> <b>noises</b> is investigated in asymmetrically periodic potentials. By using Langevin dynamics simulations, {{we find that}} rectified currents occur {{in the absence of}} any external driving forces. Unlike white <b>Gaussian</b> <b>noises,</b> fractional <b>Gaussian</b> <b>noises</b> can break thermodynamical equilibrium and induce directed transport. Remarkably, the average velocity for persistent fractional noise is opposite to that for anti-persistent fractional noise. The velocity increases monotonically with Hurst exponent for the persistent case, whereas there exists an optimal value of Hurst exponent at which the velocity takes its maximal value for the anti-persistent case. Comment: 13 pages, 6 figure...|$|R
30|$|T). From (6), Ỹ is a noisy {{observation}} of the channel matrix H with white <b>Gaussian</b> <b>noises.</b>|$|R
25|$|Newer systems, however, usually now adopt near-optimal {{types of}} error {{correction}} codes {{that use the}} turbo decoding principle, where the decoder iterates towards the desired solution. Examples of such error correction coding types include turbo codes and LDPC codes, which perform close to the Shannon limit for the Additive White <b>Gaussian</b> <b>Noise</b> (AWGN) channel. Some systems that have implemented these codes have concatenated them with either Reed-Solomon (for example on the MediaFLO system) or BCH codes (on the DVB-S2 system) to improve upon an error floor inherent to these codes at high signal-to-noise ratios.|$|E
25|$|The Kalman {{filters are}} based on linear {{dynamical}} systems discretized in the time domain. They are modelled on a Markov chain built on linear operators perturbed by errors that may include <b>Gaussian</b> <b>noise.</b> The state {{of the system is}} represented as a vector of real numbers. At each discrete time increment, a linear operator is applied to the state to generate the new state, with some noise mixed in, and optionally some information from the controls on the system if they are known. Then, another linear operator mixed with more noise generates the observed outputs from the true ("hidden") state. The Kalman filter may be regarded as analogous to the hidden Markov model, with the key difference that the hidden state variables take values in a continuous space (as opposed to a discrete state space as in the hidden Markov model). There is a strong duality between the equations of the Kalman Filter and those of the hidden Markov model. A review of this and other models is given in Roweis and Ghahramani (1999), and Hamilton (1994), Chapter 13.|$|E
2500|$|This is an {{everyday}} {{situation in which}} a set of parameters is measured, and the measurements are corrupted by independent <b>Gaussian</b> <b>noise.</b> Since the noise has zero mean, it is very reasonable to use the measurements themselves as {{an estimate of the}} parameters. This is the approach of the least squares estimator, which is [...]|$|E
3000|$|... (j∈{S 1,S 2, SR, PD}) are modeled as {{additive}} white <b>Gaussian</b> <b>noises</b> (AWGN) with {{mean zero}} and variance N [...]...|$|R
3000|$|... → 0, and {{therefore}} the MU does not interfere on the femtocell. Moreover, the additive complex <b>Gaussian</b> <b>noises</b> are represented by the terms n [...]...|$|R
3000|$|... 2 are the {{additive}} complex <b>Gaussian</b> <b>noises</b> {{with zero}} mean and covariance σ _ 1 ^ 2 I_N_r and σ _ 2 ^ 2 I_N_s, respectively.|$|R
2500|$|Papers by Dr. Fernando Rosas and Dr. Christian Oberli [...] {{have shown}} that the entire MIMO SVD link can be {{approximated}} by the average of the SER of Nakagami-m channels. This leads to characterise the eigenchannels of N × N MIMO channels with N larger than 14, showing that the smallest eigenchannel distributes as a Rayleigh channel, the next four eigenchannels closely distributes as Nakagami-m channels with m = 4, 9, 25 and 36, and the N – 5 remaining eigenchannels have statistics similar to an additive white <b>Gaussian</b> <b>noise</b> (AWGN) channel within 1dB signal-to-noise ratio. It is also shown that 75% of the total mean power gain of the MIMO SVD channel goes to the top third of all the eigenchannels.|$|E
2500|$|The ACT (Adaptive Control of Thought) (and later ACT-R (Adaptive Control of Thought-Rational)) {{theory of}} {{cognition}} represents declarative memory (of which semantic memory is a part) with [...] "chunks", which {{consist of a}} label, a set of defined relationships to other chunks (i.e., [...] "this is a _", or [...] "this has a _"), {{and any number of}} chunk-specific properties. Chunks, then, can be mapped as a semantic network, given that each node is a chunk with its unique properties, and each link is the chunk's relationship to another chunk. In ACT, a chunk's activation decreases {{as a function of the}} time since the chunk was created and increases with the number of times the chunk has been retrieved from memory. Chunks can also receive activation from <b>Gaussian</b> <b>noise,</b> and from their similarity to other chunks. For example, if [...] "chicken" [...] is used as a retrieval cue, [...] "canary" [...] will receive activation by virtue of its similarity to the cue (i.e., both are birds, etc.). When retrieving items from memory, ACT looks at the most active chunk in memory; if it is above threshold, it is retrieved, otherwise an [...] "error of omission" [...] has occurred, i.e., the item has been forgotten. There is, additionally, a retrieval latency, which varies inversely with the amount by which the activation of the retrieved chunk exceeds the retrieval threshold. This latency is used in measuring the response time of the ACT model, to compare it to human performance.|$|E
50|$|A {{special case}} is white <b>Gaussian</b> <b>noise,</b> {{in which the}} values at any pair of times are identically {{distributed}} and statistically independent (and hence uncorrelated). In communication channel testing and modelling, <b>Gaussian</b> <b>noise</b> is used as additive white noise to generate additive white <b>Gaussian</b> <b>noise.</b>|$|E
3000|$|... {{are assumed}} to be {{independent}} <b>Gaussian</b> <b>noises.</b> This transformation that provides direct measurements of speed and course change gives what is usually referred to as odometry.|$|R
40|$|This paper {{presents}} two simple optimization techniques {{based on}} combining the Langevin Equation with the Hopfield Model. Proposed models - referred as Stochastic Model (SM) and Pulsed Noise Model (PNM) - {{can be viewed}} as straightforward stochastic extensions of the Hopfield optimization network. Optimization with SM, unlike in previous related models, in which ffi-correlated <b>Gaussian</b> <b>noises</b> were considered, is based on <b>Gaussian</b> <b>noises</b> with positive autocorrelation times. This is a reasonable assumption from a hardware implementation point of view. In the other model - PNM, <b>Gaussian</b> <b>noises</b> are injected to the system only at certain time instances, as opposite to continuously maintained ffi-correlated noises used in the previous related works. In both models (SM and PNM), intensities of noises added to the model are independent of neurons' potentials. Moreover, instead of impractically long inverse logarithmic cooling schedules, linear cooling is tested. With the above strong simplific [...] ...|$|R
3000|$|... denotes {{a static}} {{appearance}} model consisting of ground-truth manually or automatically detected {{from the first}} frame {{as well as its}} perturbations with small <b>Gaussian</b> <b>noises,</b> and Y [...]...|$|R
5000|$|... #Subtitle level 3: Unknown {{constant}} in additive white <b>Gaussian</b> <b>noise</b> ...|$|E
5000|$|Additive white <b>Gaussian</b> <b>noise</b> (AWGN) channel, {{a linear}} {{continuous}} memoryless model ...|$|E
5000|$|... {{in systems}} theory in {{connection}} with nonlinear operations on <b>Gaussian</b> <b>noise.</b>|$|E
3000|$|... are zero-mean, unit-variance <b>Gaussian</b> <b>noises</b> {{that are}} {{independent}} {{of each other and}} of the channel inputs. The difference with the model in [17] is the presence of the intra-cluster signal in (1).|$|R
50|$|Where wk and vk are {{the process}} and {{observation}} noises which are both assumed to be zero mean multivariate <b>Gaussian</b> <b>noises</b> with covariance Qk and Rk respectively. uk-1 is the control vector.|$|R
5000|$|Where wk and vk are {{the process}} and {{observation}} noises which are both assumed to be zero mean multivariate <b>Gaussian</b> <b>noises</b> with covariance Qk and Rk respectively. Then the covariance prediction and innovation equations become ...|$|R
5000|$|Contaminated <b>Gaussian</b> <b>noise,</b> whose PDF is {{a linear}} mixture of Gaussian PDFs ...|$|E
5000|$|For {{the simple}} {{case of the}} {{additive}} white <b>Gaussian</b> <b>noise</b> (AWGN) channel: ...|$|E
5000|$|... where [...] {{is white}} <b>Gaussian</b> <b>noise</b> with average [...] and second moment ...|$|E
40|$|In this paper, {{we propose}} a new partition-based filter for {{removing}} mixed high probability impulse and <b>Gaussian</b> <b>noises</b> by extraction of the signals. The proposed filter sorts {{the elements of}} sample vector and extracts the sig-nals around the median from sorted signals. The extraction vector is classified into one of M partitions, and a particu-lar filtering operation is then activated. The output of the filter is estimated by the current pixel and the extraction vector. Carrying out the simulation, we illustrate the peak {{signal to noise ratio}} of the proposed filter, and show that it is effective in removing mixed high probability impulse and <b>Gaussian</b> <b>noises.</b> 1...|$|R
30|$|In the investigation, we will {{introduce}} a less complex error-correcting code, that is PC to PLC systems and study its {{performance in the}} context of impulsive noises, which remain quite different from the widely assumed <b>Gaussian</b> <b>noises</b> of wireless communications.|$|R
40|$|Abstract — A {{new image}} {{segmentation}} using multi-hyperbolic and multi-Gaussian kernel based fuzzy c-means algorithm (MHMGFCM) is proposed for medical {{magnetic resonance image}} (MRI) segmentation. The integration of two hyperbolic tangent kernels and two Gaussian kernels {{are used in the}} proposed algorithm for clustering of images. The performance of the proposed algorithm is tested on OASIS-MRI image dataset. The performance is tested in terms of score, number of iterations (NI) and execution time (TM) under different <b>Gaussian</b> <b>noises</b> on OASIS-MRI dataset. The results after investigation, the proposed method shows a significant improvement as compared to other existing methods in terms of score, NI and TM under different <b>Gaussian</b> <b>noises</b> on OASIS-MRI dataset...|$|R
50|$|The {{error rates}} quoted here {{are those in}} {{additive}} white <b>Gaussian</b> <b>noise</b> (AWGN).|$|E
5000|$|A continuous-time analog {{communications}} channel {{subject to}} <b>Gaussian</b> <b>noise</b> — see Shannon-Hartley theorem.|$|E
5000|$|<b>Gaussian</b> <b>noise,</b> with a {{probability}} density function {{equal to that}} of the normal distribution ...|$|E
3000|$|... {{represent}} <b>Gaussian</b> <b>noises.</b> The same {{parameters for}} time interval, window, and noise strength {{are used as}} in the Example 2. The results for the standard SM, the L-estimate SM, the standard CTDN= 4, and the L-estimate RCTDN= 4 are shown in Figure 4.|$|R
30|$|Analyzing {{the results}} of the tree methods and taking into account the {{isotropy}} of the Fourier transform we can observe the effects of <b>Gaussian</b> white <b>noise</b> decomposition for the three methods. Figure 4 a depicts the results for the first’s six modes of a full two-dimensional <b>Gaussian</b> white <b>noise</b> decomposition using BEEMD, the results are in line with those obtained by Rilling et al. [12] and Damerval et al. [13]. Figure 4 b shows the results for the first’s six modes of a full two-dimensional <b>Gaussian</b> white <b>noise</b> decomposition using LSEEMD, resulting in a filter bank over each separated horizontal line (noise image). Figure 4 c shows the results for the first’s six modes of a full two-dimensional <b>Gaussian</b> white <b>noise</b> decomposition using our proposed method PHEEMD.|$|R
30|$|In CRN, {{wireless}} propagation {{suffers from}} white <b>Gaussian</b> <b>noises</b> and TVFF. In this investigation, we fully {{make use of}} the finite-state Markov channel (FSMC) [16] to specify the cognitive channels because its Markov property can effectively characterize the dynamic property of TVFF and match its statistical model.|$|R
