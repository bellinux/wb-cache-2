7|25|Public
40|$|Abstract — This paper {{presents}} new {{schemes for}} view-dependent continuous level-of-detail (LOD) rendering of terrain which update output meshes with sub-linear CPU processing. We use a {{directed acyclic graph}} (DAG) abstraction for the longest-edge-bisection based multiresolution model. The other component of our refinement framework is the saturated monotonic perspective-division based error function. We made the critical observation that, for a vertex, {{the difference between the}} reciprocals of this particular error function for two different viewpoints is bounded by the distance between the two viewpoints, times a per-vertex constant. We call this the bounded variation property. Utilizing this property, we introduce the distance deferral table, a circular array based structure that schedules <b>deferred</b> <b>processing</b> of DAG vertices according to viewpoint motion. We then use the distance deferral table to opti-mize the traditional threshold-based refinement algorithm and the dual-queue constrained optimization algorithm to allow sub-linear CPU run-time. Index Terms — Viewing algorithms, virtual reality, visu-alization techniques and methodologies, terrain visualiza-tion, continuous level-of-detail, view-dependent optimiza-tion, <b>deferred</b> <b>processing,</b> multiresolution representation I...|$|E
40|$|Finding {{connected}} components (CC) of an {{undirected graph}} {{is a fundamental}} computational problem. Various CC algorithms exist for PRAM models. An implementation of a PRAM CC algorithm on a coarse-grain MIMD machine with distributed memory brings many problems, since the communication overhead is substantial compared to the local computation. Several implementations of CC algorithms on distributed memory machines have been described in the literature, all in Split-C. We have designed and implemented a CC algorithm in C ++ and MPI, by combining {{the ideas of the}} previous PRAM and distributed memory algorithms. Our main optimization is based on replacing the conditional hooking by rules for reducing nontrivial cycles during the contraction of components. We have also implemented a method for reducing the number of exchanged messages which is based on buffering messages and on <b>deferred</b> <b>processing</b> of answers...|$|E
40|$|The ATLAS event store {{employs a}} {{persistence}} framework with extensive navigational capabilities. These include real-time back navigation to upstream processing stages, externalizable data object references, navigation from any data object {{to any other}} both within a single file and across files, and more. The 2013 - 2014 shutdown of the Large Hadron Collider provides an opportunity to enhance this infrastructure in several ways that both extend these capabilities and allow the collaboration to better exploit emerging computing platforms. Enhancements include redesign with efficient file merging in mind, content-based indices in optimized reference types, and support for forward references. The latter provide the potential to construct valid references to data before those data are written, a capability that is useful {{in a variety of}} multithreading, multiprocessing, distributed processing, and <b>deferred</b> <b>processing</b> scenarios. This paper describes the architecture and design of the next generation of ATLAS navigational infrastructure...|$|E
40|$|This article previews the Supreme Court case EEOC v. Commercial Office Products Co., 486 U. S. 107 (1988). The author {{expected}} the Court {{to address the}} following issue: When a state civil rights agency decides to <b>defer</b> <b>processing</b> an employment discrimination charge to the EEOC, has the agency 2 ̆ 2 terminated 2 ̆ 2 its proceedings so that the charge will be deemed filed with the EEOC for purposes of calculating the statute of limitations...|$|R
50|$|Examples of {{policies}} would be limits on credit allowance, limits on value of order based on given criteria, {{constraints on the}} use of personal information, 'know the customer' regulations for financial institutions, export controls, and weight limits for packages. VPEC-T goes beyond this by pushing on into policies that may be unwritten, and are implied in departmental customs ("we <b>defer</b> <b>processing</b> of orders for perishable items received after 14:00 on Friday until Monday morning, and then they have priority") and other informal agreements or instructions.|$|R
40|$|In this paper, {{we propose}} a 2 D based {{partition}} method for {{solving the problem}} of Ranking under Team Context(RTC) on datasets without a priori. We first map the data into 2 D space using its minimum and maximum value among all dimensions. Then we construct window queries with consideration of current team context. Besides, during the query mapping procedure, we can pre-prune some tuples which are not top ranked ones. This pre-classified step will <b>defer</b> <b>processing</b> those tuples and can save cost while providing solutions for the problem. Experiments show that our algorithm performs well especially on large datasets with correctness...|$|R
40|$|We {{present a}} sample of 485 photometrically {{identified}} Type Ia supernova candidates mined from {{the first three years}} of data of the CFHT SuperNova Legacy Survey (SNLS). The images were submitted to a <b>deferred</b> <b>processing</b> independent of the SNLS real-time detection pipeline. Light curves of all transient events were reconstructed in the g_M, r_M, i_M and z_M filters and submitted to automated sequential cuts in order to identify possible supernovae. Pure noise and long-term variable events were rejected by light curve shape criteria. Type Ia supernova identification relied on event characteristics fitted to their light curves assuming the events to be normal SNe Ia. The light curve fitter SALT 2 was used for this purpose, assigning host galaxy photometric redshifts to the tested events. The selected sample of 485 candidates is one magnitude deeper than that allowed by the SNLS spectroscopic identification. The contamination by supernovae of other types is estimated to be 4 %. Testing Hubble diagram residuals with this enlarged sample allows us to measure the Malmquist bias due to spectroscopic selections directly. The result is fully consistent with the precise Monte Carlo based estimate used to correct SN Ia distance moduli in the SNLS 3 -year cosmological analyses. This paper demonstrates the feasibility of a photometric selection of high redshift supernovae with known host galaxy redshifts, opening interesting prospects for cosmological analyses from future large photometric SN Ia surveys. Comment: (The SNLS collaboration) 23 pages, 28 figures, Accepted in A&...|$|E
40|$|Early-bactericidal-activity (EBA) studies {{measure the}} change in mycobacterial load in sputum over time to {{evaluate}} antituberculosis drugs. We investigated whether a delay in sputum processing influences the quantitative results of sputum mycobacterial culture. We identified pretreatment smear-positive sputum samples collected overnight and processed at a single laboratory. Sputum volume, time from sputum collection to processing, CFU counts/ml of sputum, and time to culture positivity (TTP) data were retrieved. We obtained 817 TTP and 794 CFU results from a total of 844 sputum samples. Contamination did not occur more frequently with prolonged storage (TTP, 2. 0 %; CFU, 2. 4 %). Sample volumes were 10 ml in 49 %. Delays to processing were 0, 1, 2, and 3 days in 696 (43. 2 %), 722 (44. 8 %), 128 (7. 9 %), and 65 (4. 0 %) samples, respectively. TTP and CFU did not significantly differ between days of delay to processing (P = 0. 098 and P = 0. 908, respectively), {{but there was a}} nonsignificant trend toward a prolonged TTP over time (P = 0. 052, Jonckheere-Terpstra trend test). Sputa of 5 ml (113 h versus 99 h; P < 0. 01) but no significant decrease in CFU. Sputum can be stored under refrigerated conditions for <b>deferred</b> <b>processing</b> for at least 3 days. This means that central laboratories can be used for quantitative mycobacterial study endpoints when delays to processing are not expected to exceed a few days. Care should be taken to collect sputum of sufficient volume...|$|E
40|$|Detection of supernovae and, more generally, of {{transient}} {{events in}} large surveys can provide numerous false detections. In {{the case of}} a <b>deferred</b> <b>processing</b> of survey images, this implies reconstructing complete light curves for all detections, requiring sizable processing time and resources. Optimizing the detection of transient events is thus an important issue for both present and future surveys. We present here the optimization done in the SuperNova Legacy Survey (SNLS) for the 5 -year data deferred photometric analysis. In this analysis, detections are derived from stacks of subtracted images with one stack per lunation. The 3 -year analysis provided 300, 000 detections dominated by signals of bright objects that were not perfectly subtracted. Allowing these artifacts to be detected leads not only to a waste of resources but also to possible signal coordinate contamination. We developed a subtracted image stack treatment {{to reduce the number of}} non SN-like events using morphological component analysis. This technique exploits the morphological diversity of objects to be detected to extract the signal of interest. At the level of our subtraction stacks, SN-like events are rather circular objects while most spurious detections exhibit different shapes. A two-step procedure was necessary to have a proper evaluation of the noise in the subtracted image stacks and thus a reliable signal extraction. We also set up a new detection strategy to obtain coordinates with good resolution for the extracted signal. SNIa MC generated images were used to study detection efficiency and coordinate resolution. When tested on SNLS 3 data this procedure decreases the number of detections by a factor of two, while losing only 10 % of SN-like events, almost all faint. MC results show that SNIa detection efficiency is equivalent to that of the original method for bright events, while the coordinate resolution is improved. Comment: 14 pages, 9 figures. Accepted for Publication in the Journal of Cosmology and Astroparticle Physics (JCAP). v 2 with a new discussion section and information. v 3 corrected numbers in Table 2. v 4 with correction on Table 2 where coordinate resolutions where underestimate...|$|E
30|$|Generally local {{methods are}} memory-based algorithms, {{in the sense}} that they <b>defer</b> data <b>processing</b> until a {{prediction}} is required. A database of observed data is always kept and the estimation is derived from a neighborhood of the query point/time.|$|R
40|$|Recent {{research}} has found that forced interruptions at points of higher mental workload are more disruptive than at points of lower workload. This paper investigates a complementary idea: when users experience deferrable interruptions at points of higher workload, they may tend to <b>defer</b> <b>processing</b> of the interruption until times of lower workload. In an experiment, users performed a mail-browser primary task while being occasionally interrupted by a secondary chat task, evenly distributed between points of higher and lower workload. Analysis showed that 94 % of the time, users switched to the interrupting task during periods of lower workload, versus only 6 % during periods of higher workload. The results suggest that when interruptions can be deferred, users have a strong tendency to “monotask ” until primary-task mental workload has been minimized. Author Keywords Multitasking, interruption, attention, problem state, chat...|$|R
3000|$|Whether or not {{to perform}} <b>processing</b> on-camera or <b>deferring</b> <b>processing</b> to {{external}} computers/systems is impacted by camera capability/programmability and network latency and bandwidth. For instance, a multihop network may be too slow to permit active tracking if video needs to first be passed through several sensors before reaching a processor, whose control commands then need to be relayed across several more sensors before the camera ever receives the command to [...] "pan left". Outside of basic scripting capabilities, most commercial cameras do not offer the flexibility or processing power to achieve processing tasks more complicated than basic motion detection or tracking. This issue often prompts network builders to develop custom programmable camera hardware for use in their systems [4 – 7]. On-camera processing can also reduce bandwidth consumption of the network (e.g, transmitting only areas of interest as opposed to full-frame video), while external processing allows a greater range of control and processing power.|$|R
40|$|Verbs {{are a major}} {{component}} of theories of language processing, partly because they exhibit systematic restrictions on their arguments. However, verbs follow their arguments in many constructions (particularly in verb-final languages), making it inefficient to <b>defer</b> <b>processing</b> until the verb. Computational modeling suggests that during sentence processing, nouns may activate information about subsequent lexical items, including verbs. We investigated this prediction using short stimulus onset asynchrony (SOA, the time between the onsets of the prime and target) priming. Robust priming obtained when verbs were named aloud following typical agents (nun- praying), patients (dice-rolled), instruments (shovel- digging), and locations (arena- skating). This research is the first to investigate systematically the priming of verbs from nouns. It suggests that event memory is organized so that entities and objects activate the class of events in which they typically play a role (Lancaster & Barsalou, 1997). These computations may be an important component of expectancy generation in sentence processing...|$|R
40|$|Mobile {{computing}} platforms {{are performing}} increasingly complex and computationally intensive tasks. To help lengthen useful battery life, these platforms often incorporate {{some form of}} hardware power-down that {{is controlled by the}} system software. Unfortunately, these often incur substantial transition latencies when switching between power-down and active states, making them difficult to use in time-critical embedded systems. This paper introduces a class of sprint-and-halt schedulers that attempt to maximize the en-ergy savings of software-controlled power-down mechanisms, while simultaneously maintain-ing hard real-time deadline guarantees. Several different algorithms are proposed to reclaim unused <b>processing</b> time, <b>defer</b> <b>processing,</b> and extend power-down intervals while respect-ing task deadlines. Sprint-and-halt schedulers are shown to reduce energy consumption by 40 – 70 % over typical operating parameters. For very large or small state transition latencies, simple approaches work very close to theoretical limits, but over a critical range of latencies, advanced schedulers show 10 – 20 % energy reduction over simpler methods...|$|R
40|$|Salvucci and Bogunovich (2010) {{described}} a multitasking experiment in which users {{had to look}} up {{information on the internet}} as a primary task while being occasionally interrupted by a secondary chat task. This experiment showed that users who experience deferrable interruptions at times of higher mental workload tend to <b>defer</b> <b>processing</b> until times of lower workload. Based on this we conducted an experiment in which we investigated if we could influence the process of deferring incoming interruptions, and what the potential interruption effects are on effciency in the primary task. In our experiment we introduced a short pause at certain points of higher mental workload. Analysis showed that users switched more often to the secondary task at points of higher mental workload when they experienced a pause in the primary task, while effciency remained relatively unchanged. The results suggest that while having a tendency to monotask, users can be seduced into multitasking without losing effciency. ...|$|R
3000|$|A common {{approach}} {{to deal with}} out of order consists in specifying the maximum delay {{for the arrival of}} elements and <b>defer</b> the <b>processing</b> until such a delay has elapsed. Yet, the semantics of processing in the case some element overcomes such maximum delay changes from system to system. For instance, as discussed in [...] "Analysis of stream processing engines", Azure Stream analytics discards input elements that arrive too late with respect to the UTC wall clock time. These behaviors cannot be captured within the SECRET model.|$|R
5000|$|Nearly all {{digital cameras}} can process the {{image from the}} sensor into a JPEG file using {{settings}} for white balance, color saturation, contrast, and sharpness that are either selected automatically or entered by the photographer before taking the picture. Cameras that produce raw files save these settings in the file, but <b>defer</b> the <b>processing.</b> This results in an extra step for the photographer, so raw is normally only used when additional computer processing is intended. However, raw has numerous advantages over JPEG such as: ...|$|R
40|$|Complex Event Processing (CEP) is an {{emerging}} field with important applications in many areas. CEP systems collect events arriving from input data streams {{and use them}} to infer more complex events according to predefined patterns. The Non-deterministic Finite Automaton (NFA) {{is one of the most}} popular mechanisms on which such systems are based. During the event detection process, NFAs incrementally extend previously observed partial matches until a full match for the query is found. As a result, each arriving event needs to be processed to determine whether a new partial match is to be initiated or an existing one extended. This method may be highly inefficient when many of the events do not result in output matches. We present a lazy evaluation mechanism that <b>defers</b> <b>processing</b> of frequent event types and stores them internally upon arrival. Events are then matched in ascending order of frequency, thus minimizing potentially redundant computations. We introduce a Lazy Chain NFA, which utilizes the above principle, and does not depend on the underlying pattern structure. An algorithm for constructing a Lazy Chain NFA for common pattern types is presented, including conjunction, negation and iteration. Finally, we experimentally evaluate our mechanism on real-world stock trading data. The results demonstrate a performance gain of two orders of magnitude over traditional NFA-based approaches, with significantly reduced memory resource requirements...|$|R
40|$|The Idaho National Engineering and Environmental Laboratory (Laboratory) stores nearly 65, 000 {{cubic meters}} of waste {{generated}} on site or brought to the State of Idaho (Idaho) from Department of Energy (DOE) sites across the country since 1970. This represents approximately 62 percent of the stored waste that DOE plans to ship and permanently dispose of at the Waste Isolation Pilot Plant (WIPP) in Carlsbad, New Mexico. In December 1996, DOE met the first milestone by awarding a fixed-price contract to a private company to construct and operate the Advanced Mixed Waste Treatment Facility (Treatment Facility) [...] nearly 6 months ahead of schedule. Because the Treatment Facility would not be available to meet the 3, 100 cubic meter milestone by December 31, 2002, DOE decided to dispose of untreated waste using the characterization process that was in place in 1989, and adapting it to meet new characterization requirements. The purpose of the audit {{was to determine whether}} it {{is in the best interest}} of the Government to <b>defer</b> <b>processing</b> the 3, 100 {{cubic meters of}} waste until the new Treatment Facility can do so. The analysis showed that waiting until the Treatment Facility can process the 3, 100 cubic meters of waste would be more economic and reduce the environmental risks to Laboratory employees. Therefore, a compromise between DOE and Idaho officials allowing such a deferral would be in the best interest of the Government...|$|R
40|$|Recent {{studies have}} showed that when {{computer}} users {{are faced with}} a deferrable task interruption while multitasking, users tend to <b>defer</b> the <b>processing</b> of such an interruption until times of lower workload. Also, users can be tempted to processing a deferrable task interruption at times of higher workload when faced with a forced pause during the primary task. In our experiments, we investigate the influence on multitasking behaviour and efficiency by varying: (1) the difficulty of the interrupting task, (2) forced pauses during the primary task and (3) a penalty for not maintaining information required to do the primary task. We found that the difficulty of the interrupting task has little or no influence on behaviour and efficiency, whereas forced pauses and a time penalty can have different influences on multitasking behaviour and task efficiency. ...|$|R
50|$|Digital signal {{processing}} algorithms typically require {{a large number}} of mathematical operations to be performed quickly and repeatedly on a series of data samples. Signals (perhaps from audio or video sensors) are constantly converted from analog to digital, manipulated digitally, and then converted back to analog form. Many DSP applications have constraints on latency; that is, for the system to work, the DSP operation must be completed within some fixed time, and <b>deferred</b> (or batch) <b>processing</b> is not viable.|$|R
40|$|Congress {{passed the}} Civil Rights Act of 1964 {{to provide a}} {{comprehensive}} scheme to battle discrimination. Title VII of the Act was enacted to prohibit employment {{discrimination on the basis}} of race, color, religion, sex, or national origin. This same Act established the Equal Employment Opportunity Commission (EEOC) to oversee the administration of title VII. While the EEOC has substantial powers, enforcement of the Act is generally through private civil action. A complementary scheme developed among the states with the implementation of state discrimination laws and the creation of state agencies to administer these laws. Thus, title VII was implemented with the intent to supplement rather than supplant state employment discrimination provisions and remedies. This intention is demonstrated by the deferral provisions of title VII, which require the EEOC to <b>defer</b> <b>processing</b> a complaint and to afford the aggrieved party an opportunity to bring her action under state proceedings. Under this scheme, it is possible that each employment discrimination claim will be heard by four separate forums: a state administrative agency, a state court, the EEOC, and finally a federal court. In 1982, the Supreme Court attempted to settle some of the remaining issues in Kremer v. Chemical Construction Corp. In Kremer, the Court held that a charging party loses the right to sue under title VII in federal court by pursuing a state court review of an adverse deferral agency determination. This decision raises a number of new issues as well as results that seem antithetical to the purposes and intentions behind title VII. It also has far-reaching implications for lawyers who pursue employment discrimination claims and try to ensure aggrieved parties an opportunity for a de novo judicial hearing on the merits of their discrimination claims. This Note addresses the issues raised by the Court in Kremer as well as outline the effect Kremer will have on legal strategy. It also states a proper rule the Court should have adopted to avoid some of the problems inherent in its decision...|$|R
40|$|As {{a widely}} used fault {{tolerance}} technique, checkpointing {{has evolved into}} several schemes: independent, coordinated, and communication-induced (CIC). Independent and coordinated checkpointing have been adopted in many works on fault tolerant mobile agent (MA) systems. However, CIC, a flexible, efficient, and scalable checkpointing scheme, has not been applied to MA systems. Based on {{the analysis of the}} behavior of mobile agent, we argue that CIC is a well suited checkpointing scheme for MA systems. CIC not only establishes the consistent recovery lines efficiently but also integrates well with the independent checkpointing for reliable MA migration. In this paper, we propose an important improvement to CIC, referred to as the <b>deferred</b> message <b>processing</b> based CIC algorithm (DM-CIC), which achieves higher efficiency by exempting the CIC algorithm from making the forced checkpoints in MA systems. Through simulation, we find out that DM-CIC is stable and better suited to large scale MA systems. Department of ComputingInternet and Mobile Computing Lab (in Department of Computing) Refereed conference pape...|$|R
40|$|Protocol {{processing}} of received packets in BSD Unix is interrupt-driven and may cause scheduling anomalies that are unacceptable in systems that provide {{quality of service}} (QoS) guarantees. We propose an alternative mechanism, Signaled Receiver Processing (SRP), that generates {{a signal to the}} receiving process when a packet arrives. The default action of this signal is to perform protocol processing asynchronously. However, a receiving process may catch, block, or ignore the signal and <b>defer</b> protocol <b>processing</b> until a subsequent receive call. In any case, protocol processing occurs {{in the context of the}} receiving process and is correctly charged. Therefore, SRP allows the system to enforce and honor QoS guarantees. SRP offers several advantages over Lazy Receiver Processing (LRP), a previous solution to BSD's scheduling anomalies: SRP is easily portable to systems that support neither kernel threads nor Resource Containers (e. g., FreeBSD); gives applications control over the scheduling of protocol processing; uses a demultiplexing strategy that is appropriate for both hosts and gateways; and easily enables real-time or proportional-share scheduling...|$|R
40|$|Consider {{the serial}} {{emulation}} of a parallel algorithm. The thesis {{presented in this}} paper is rather broad. It suggests that such a serial emulation has the potential advantage of running on a serial machine faster than a standard serial algorithm for the same problem. The main concrete observation is very simple: just before the serial emulation of a round of the parallel algorithm begins, the whole list of memory addresses needed during this round is readily available; and, we can start fetching all these addresses from secondary memories at this time. This permits prefetching the data that will be needed in the next "time window", perhaps by means of pipelining; these data will then be ready at the fast memories when requested by the CPU. The possibility of distributing memory addresses (or memory fetch units) at random over memory modules, as has been proposed in the context of implementing the parallel-random-access machine (PRAM) design space, is discussed. This work also suggests that a multi-stage effort to build a parallel machine may start with "parallel memories" and serial <b>processing,</b> <b>deferring</b> parallel <b>processing</b> to a later stage. The general approach has the following advantage: a user-friendly parallel programming language can be used already in its first stage. This is in contrast to a practice of compromising user-friendliness of parallel computer interfaces (i. e., parallel programming languages), and may offer a way for alleviating a so-called "parallel software crisis". It is too early to reach conclusions regarding the significance of the thesis of this paper. Preliminary experimental results with respect to the fundamental and practical problem of constructing suffix trees indicate that drastic improvements in running time might be possible. Serious attempts to follow it up are needed to determine its usefulness. Parts of this paper are intentionally written in an informal way, suppressing issues {{that will have to be}} resolved in the context of a concrete implementation. The intention is to stimulate debate and provoke suggestions and other specific approaches. 	Validity of our thesis would imply that a standard computer science curriculum, which prepares young graduates for a professional career of over forty years, will have to include the topic of parallel algorithms irrespective of whether (or when) parallel processing will succeed serial processing in the general purpose computing market. (Also cross-referenced as UMIACS-TR- 91 - 145. 1...|$|R
40|$|Advances in {{satellite}} and drone imagery and sensors allow researchers to capture vivid, comprehensive and detailed snapshots {{of the surrounding}} environment, be it from a satellite looking down on earth, from a unmanned underwater vehicle or from a medical fMRI scanner. All these devices generate large amounts of data that ends up as a N-dimensional table of numbers stored on a computer. Capturing the data is {{only half of the}} story. In order to draw a conclusion a doctor or a researcher must be able to inspect the data and to process it. So far the processing of data has been done using the general purpose Central Processing Unit (CPU) of a computer. However, the performance is not always satisfactory, the resource utilization is low and the system does not scale very well in terms of price and performance. We propose <b>deferring</b> the data <b>processing</b> part to one ore more Graphic Processing Units within the computer to help speed up the process and increase resource utilization using a patented solution. This document introduces the concepts neede...|$|R
40|$|The {{software}} product Intel ® Integrated Performance Primitives (IPP) and new features are described. The primitives are {{the building blocks}} that application developers can easily integrate into their products-applications, components like media plug-ins or high level libraries, in order to significantly increase their performance on Intel and compatible architectures with Windows, Linux or MacOSX operating system installed. The IPP library covers many functional domains: image and signal processing; image coding and data compression; data integrity and cryptography; speech, audio and video coding, and others. Besides of the libraries IPP product contains 50 IPP based Samples released in the source codes. Some of the samples, for example, JPEG 2000 image codec and H 264 video codec are competitive to the commercial products. Several important new features {{were added to the}} product in 2008 in version IPP 6. 0. We will consider two of them in more details- a <b>Deferred</b> Mode Image <b>Processing</b> (DMIP) framework exploiting the CPU cache and multi-core capability, and the IPP functions generated and optimized automatically by a special tool Spiral developed at Carnegie Mellon University...|$|R
5000|$|UML state {{machines}} provide a special mechanism for deferring events in states. In every state, you can include a clause [...] If {{an event in}} the current state’s deferred event list occurs, the event will be saved (<b>deferred)</b> for future <b>processing</b> until a state is entered that does not list the event in its deferred event list. Upon entry to such a state, the UML state machine will automatically recall any saved event(s) that are no longer deferred and will then either consume or discard these events. It is possible for a superstate to have a transition defined on an event that is deferred by a substate. Consistent with other areas in the specification of UML {{state machines}}, the substate takes precedence over the superstate, the event will be deferred and the transition for the superstate will not be executed. In the case of orthogonal regions where one orthogonal region defers an event and another consumes the event, the consumer takes precedence and the event is consumed and not deferred.|$|R
5000|$|In the Legal Department, {{the center}} offers U visa and Violence Against Women Act (VAWA) {{services}} and Temporary protected status (TPS) renewals. It has recently begun <b>processing</b> <b>Deferred</b> Action for Childhood Arrivals petitions and Unaccompanied minor screening from the 2014 American immigration crisis. CARECEN also established a parent center called Centro de Padres de CARECEN [...] "Raul G. Borbon", named after an education community activist {{who fought for}} equity and quality in the public education system of Los Angeles. In the parent center, it offers English classes to Spanish-speaking immigrants {{as well as a}} new addition called Plaza Comunitaria/Casa Universitaria that allows adults to continue their education by elementary and middle school curriculum. It has also established a Youth Center where it offers students from low income and immigrant families apply to college and a leadership program to get youth to know about organizing and civil engagement. CARECEN continues {{to be involved in the}} surrounding community through different actions and campaigns. The active campaigns that CARECEN is either spearheading or is involved with are TPS for Residency, Fix L.A., and ICE out of LA among many others.|$|R
40|$|Self-join, which joins a {{relation}} with itself, is a prevalent operation in relational database systems. Despite its wide applicability, {{there has been}} little attention devoted to improving its performance. In this paper, we present SCALE (Sort for Clustered Access with Lazy Evaluation), an efficient self-join algorithm, which takes advantage of the fact that both inputs of a self-join operation are instances of the same relation. SCALE first sorts the relation on one join attribute, say R. A. In this way, for every value of the other join attribute, say R. B, its matching R. A tuples are essentially clustered. As SCALE scans the sorted relation, each tuple is joined with its matching tuples co-existing in memory. For tuples where full-range clustered accesses to their matching tuples are not possible, they are buffered and the unfinished part of join <b>processing</b> <b>deferred.</b> Such lazy evaluation minimizes the need for “random ” access to the matching tuples. SCALE further optimizes the memory allocation for clustered access and lazy evaluation to keep the processing cost minimal. Our analytical study shows that SCALE degenerates gracefully to a Sort-Merge Join in the worst case. We have also implemented SCALE in PostgreSQL, and results of our extensive experimental study show that it outperforms both Sort-Merge Join and Hybrid Hash Join by a wide margin in (almost) all cases...|$|R
40|$|Biotelemetry has {{revealed}} daily 15 -h behavioral sleep periods in a cubomedusan jellyfish, Chironex fleckeri. Its sleep {{is expected to}} be phylogenetically most primitive, since jellyfish possess only two germ layers. They belong to the phylum Cnidaria, the 'simplest' multicellular organisms with an organized nervous system. Cubomedusae have a complex visual system with 24 eyes of four different types, each type specialized for a different task. Input to these eyes during visually guided fast-swimming predation requires enormous amounts of neural processing, possibly nearly saturating the capacity of their comparatively simple nervous system. These heavy neural demands may account for the need for fifteen hours of sleep. C. fleckeri is the only animal known for which sleep may be either present or absent, dependent on lifestyle. Limited knowledge of behavior of some other cubomedusae suggests that they also possess this faculty. The finding of sleep in C. fleckeri supports current proposals of sleep's origin and basic function. Evolutionary analyses link sleep to a conflict produced by excessive processing demands on multifunctional neural circuitry for detailed focal vision by complex lensed eyes. The conflict arises between the enormous demands of complex visual analysis and needs for split-second control of actions, on the one hand, and non-urgent processing of memories of ongoing and stored events, on the other. Conflict is resolved by <b>deferring</b> the non-urgent <b>processing</b> to periods of sleep. Without sleep, selection would favor the evolution of circuitry 'dedicated' to single or but few tasks, with corresponding lesser efficiency. Had complex lensed eyes of medusae originated as a consequence of selection for increased mating success of males pursuing females, it could have occurred before the evolution of fast-swimming bilateral (three-germ-layered) prey. But if it was a consequence of selection for increased prey-hunting success, the origin of such eyes probably awaited the coexistence of bilateral prey...|$|R

