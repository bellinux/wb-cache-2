260|10000|Public
5000|$|The <b>effective</b> <b>sample</b> <b>size</b> [...] is {{the unique}} value (not {{necessarily}} an integer) such that ...|$|E
50|$|In statistics, <b>effective</b> <b>sample</b> <b>size</b> is {{a notion}} defined for a sample from a {{distribution}} when the observations {{in the sample}} are correlated or weighted.|$|E
5000|$|If {{the data}} has been weighted, then several {{observations}} composing a sample have been {{pulled from the}} distribution with effectively 100% correlation with some previous sample. In this case, the effect is known as Kish's <b>Effective</b> <b>Sample</b> <b>Size</b> ...|$|E
40|$|It {{has been}} well known that the {{conventional}} confidence interval for population proportions does not perform well for large or small values of proportions. Several alternative methods have been proposed in the literature, where the sample data are independent and identically distributed. For finite populations the problem is further complicated due {{to the use of}} complex sampling designs and issues related to <b>effective</b> <b>sample</b> <b>sizes</b> and <b>effective</b> degrees of freedom. In this paper we investigate the performance of several confidence intervals for proportions and quantiles under two-stage sampling designs through simulatio...|$|R
30|$|We {{focus on}} the {{population}} interviewed in the period 2007 – 2010, aged between 25 and 60  years old. The models are estimated separately by country. The <b>effective</b> (balanced) <b>sample</b> <b>sizes</b> are 11, 786 in Poland, 3580 in Lithuania, 4996 in Czech Republic, 5524 in Slovakia, 6740 in Hungary, and 6388 in Romania.|$|R
40|$|Coalescent-based Bayesian Markov chain Monte Carlo (MCMC) {{inference}} generates {{estimates of}} evolutionary parameters and their posterior probability distributions. As {{the number of}} sequences increases, {{the length of time}} taken to complete an MCMC analysis increases as well. Here, we investigate an approach to distribute the MCMC analysis across a cluster of computers. To do this, we use bootstrapped topologies as fixed genealogies, perform a single MCMC analysis on each genealogy without topological rearrangements, and pool the results across all MCMC analyses. We show, through simulations, that although the standard MCMC performs better than the bootstrap-MCMC at estimating the effective population size (scaled by mutation rate), the bootstrap-MCMC returns better estimates of growth rates. Additionally, we find that our bootstrap-MCMC analyses are, on average, 37 times faster for equivalent <b>effective</b> <b>sample</b> <b>sizes...</b>|$|R
5000|$|The {{case where}} the {{correlations}} are not uniform is somewhat more complicated. Note that if the correlation is negative, the <b>effective</b> <b>sample</b> <b>size</b> may be larger than the actual sample size. Similarly, {{it is possible to}} construct correlation matrices that have an [...] even when all correlations are positive. Intuitively, [...] may be thought of as the information content of the observed data.|$|E
50|$|By far, {{the most}} common means of dealing with missing data is listwise {{deletion}} (also known as complete case), which is when all cases with a missing value are deleted. If the data are missing completely at random, then listwise deletion does not add any bias, but it does decrease {{the power of the}} analysis by decreasing the <b>effective</b> <b>sample</b> <b>size.</b> For example, if 1000 cases are collected but 80 have missing values, the <b>effective</b> <b>sample</b> <b>size</b> after listwise deletion is 920. If the cases are not missing completely at random, then listwise deletion will introduce bias because the sub-sample of cases represented by the missing data are not representative of the original sample (and if the original sample was itself a representative sample of a population, the complete cases are not representative of that population either). While listwise deletion is unbiased when the missing data is missing completely at random, this is rarely the case in actuality.|$|E
50|$|Statistician William S. Gosset in 1914 {{developed}} {{methods of}} eliminating spurious correlation due to how position in time or space affects similarities. Today's election polls {{have a similar}} problem: the closer the poll to the election, the less individuals make up their mind independently, and the greater the unreliability of the polling results, especially {{the margin of error}} or confidence limits. The effective n of independent cases from their sample drops as the election nears. Statistical significance falls with lower <b>effective</b> <b>sample</b> <b>size.</b>|$|E
30|$|Sample {{attrition}} {{can lead}} to biased estimates when conducting causal inference, especially when observations are not missing at random (RAM). However, when non-responses {{are assumed to be}} MAR, the attrition bias disappears albeit with an <b>effective</b> reduction in <b>sample</b> <b>size.</b>|$|R
40|$|In this paper, we {{establish}} several recurrence {{relations for}} the single and product moments of progressively Type-II right censored order statistics from a half-logistic distribution. The use of these relations in a systematic recursive manner would enable one to compute all the means, variances and covariances of progressively Type-II right censored order statistics from the half-logistic distribution for all <b>sample</b> <b>sizes</b> n, <b>effective</b> <b>sample</b> <b>sizes</b> m, and all progressive censoring schemes (R 1, [...] .,Rm). The results established here generalize the corresponding results for the usual order statistics due toÂ Balakrishnan (1985). These moments are then utilized to derive best linear unbiased estimators of the scale and location-scale parameters of the half-logistic distribution. A comparison of these estimators with the maximum likelihood estimates is then made. The best linear unbiased predictors of censored failure times is then discussed briefly. Finally, two numerical examples are presented to illustrate all the inferential methods developed here. Progressive Type-II right censored order statistics Single moments Product moments Recurrence relations Half-logistic distribution Best linear unbiased estimators (BLUEs) Maximum likelihood estimators (MLEs) Best linear unbiased predictors (BLUPs) ...|$|R
40|$|Particle {{smoothing}} {{methods are}} used for inference of stochastic processes based on noisy observations. Typically, the estimation of the marginal posterior distribution given all observations is cumbersome and computational intensive. In this paper, we propose a simple algorithm based on path integral control theory to estimate the smoothing distribution of continuous-time diffusion processes with partial observations. In particular, we use an adaptive importance sampling method to improve the <b>effective</b> <b>sampling</b> <b>size</b> of the posterior over processes given the observations and {{the reliability of the}} estimation of the marginals. This is achieved by estimating a feedback controller to sample efficiently from the joint smoothing distributions. We compare the results with estimations obtained from the standard Forward Filter/Backward Simulator for two diffusion processes of different complexity. We show that the proposed method gives more reliable estimations than the standard FFBSi when the smoothing distribution is poorly represented by the filter distribution. Comment: 16 pages, 13 figure...|$|R
50|$|For {{cross-cultural}} studies, Murdock and Whiteestimated {{the size}} of patches of similarities in their sample of 186 societies. The four variables they tested - language, economy, political integration, and descent - had patches of similarities that varied from size three to size ten. A very crude rule of thumb might be to divide the square root of the similarity-patch sizes into n, so that the effective sample sizes are 58 and 107 for these patches, respectively. Again, statistical significance falls with lower <b>effective</b> <b>sample</b> <b>size.</b>|$|E
50|$|The problem pops up in sample surveys when sociologists want {{to reduce}} the travel time to do their interviews, and hence they divide their {{population}} into local clusters and sample the clusters randomly, then sample again within the clusters. If they interview n people in clusters of size m the <b>effective</b> <b>sample</b> <b>size</b> (efs) would have a lower limit of 1 + (n − 1) / m if everyone in each cluster were identical. When there are only partial similarities within clusters, the m in this formula has to be lowered accordingly. A formula of this sort is 1 + d (n − 1) where d is the intraclass correlation for the statistic in question. In general, estimation of the appropriate efs depends on the statistic estimated, as for example, mean, chi-square, correlation, regression coefficient, and their variances.|$|E
50|$|With the {{availability}} of large computing facilities, scientists refocused from isolated parameters inference to complex functions inference, i.e. re sets of highly nested parameters identifying functions. In these cases we speak about learning of functions (in terms for instance of regression, neuro-fuzzy system or computational learning) {{on the basis of}} highly informative samples. A first effect of having a complex structure linking data is the reduction of the number of sample degrees of freedom, i.e. the burning of a part of sample points, so that the <b>effective</b> <b>sample</b> <b>size</b> to be considered in the central limit theorem is too small. Focusing on the sample size ensuring a limited learning error with a given confidence level, the consequence is that the lower bound on this size grows with complexity indices such as VC dimension or detail of a class to which the function we want to learn belongs.|$|E
40|$|Hamiltonian Monte Carlo (HMC) {{has become}} {{routinely}} used for sampling from posterior distributions. Its extension Riemann manifold HMC (RMHMC) modifies the proposal kernel through distortion of local distances by a Riemannian metric. The performance depends critically on {{the choice of}} metric, with the Fisher information providing the standard choice. In this article, we propose {{a new class of}} metrics aimed at improving HMC's performance on multi-modal target distributions. We refer to the proposed approach as geometrically tempered HMC (GTHMC) due to its connection to other tempering methods. We establish a geometric theory behind RMHMC to motivate GTHMC and characterize its theoretical properties. Moreover, we develop a novel variable step size integrator for simulating Hamiltonian dynamics to improve on the usual Störmer-Verlet integrator which suffers from numerical instability in GTHMC settings. We illustrate GTHMC through simulations, demonstrating generality and substantial gains over standard HMC implementations in terms of <b>effective</b> <b>sample</b> <b>sizes.</b> Comment: 44 papges, 3 figure...|$|R
40|$|Data {{from the}} Norwegian Barents Sea bottom trawl surveys in February from 1985 through 1996 {{are used to}} analyse {{variations}} in haddock (Melanogrammus aeglefinus L.) maturity. Several link functions to estimate maturity ogives are compared. Due to the clustering of samples, both the goodness of fit and various other test statistics are calculated using an <b>effective</b> <b>sampling</b> <b>size</b> lower than {{the total number of}} samples. Length and year-class strength are used as continuous explanatory variables while year, age, sex and area are class variables. A strong year effect was detected which appears {{to be related to the}} very large 1990 year-class. The year-class strength explains additional variation. That is: when abundance is high, the proportions mature are reduced for otherwise similar haddock (with respect to length, age, sex and area). Both sex and area effects were significant. Males mature at younger ages and shorter lengths than females and there is a tendency for proportions mature to be lower in the eastern part of the Barents Sea...|$|R
40|$|This paper {{provides}} {{a new approach}} for sequentially learning parameters and states in a wide class of state space models using particle filters. Our approach generates direct i. i. d. samples from a particle approximation to the joint posterior distribution of both parameters and latent states, avoiding the use of and the degeneracies inherent in sequential importance sampling. We illustrate the efficiency of our approach by sequentially learning parameters and filtering states in two models: a log-stochastic volatility model and robust version of the Kalman filter model with t-errors in both the observation and state equation. In both cases, we show using simulated data that our approach efficiently learns the parameters and states sequentially, generating higher <b>effective</b> <b>sample</b> <b>sizes</b> than existing algorithms. We use the approach for two real data examples, sequentially learning in a stochastic volatility model of Nasdaq stock returns and about predictable components in a model of core inflation...|$|R
40|$|In {{this paper}} I address the {{question}} - how large is a phylogenetic sample I propose a definition of a phylogenetic <b>effective</b> <b>sample</b> <b>size</b> for Brownian motion and Ornstein-Uhlenbeck processes - the regression <b>effective</b> <b>sample</b> <b>size.</b> I discuss how mutual information {{can be used to}} define an <b>effective</b> <b>sample</b> <b>size</b> in the non-normal process case and compare these two definitions to an already present concept of <b>effective</b> <b>sample</b> <b>size</b> (the mean <b>effective</b> <b>sample</b> <b>size).</b> Through a simulation study I find that the AICc is robust if one corrects for the number of species or effective number of species. Lastly I discuss how the concept of the phylogenetic <b>effective</b> <b>sample</b> <b>size</b> can be useful for biodiversity quantification, identification of interesting clades and deciding on the importance of phylogenetic correlations...|$|E
40|$|Correlated samples {{have been}} {{frequently}} avoided in case-control genetic association studies {{in part because}} the methods for handling them are either not easily implemented or not widely known. We advocate one method for case-control association analysis of correlated samples – the <b>effective</b> <b>sample</b> <b>size</b> method – as a simple and accessible approach that does not require specialized computer programs. The <b>effective</b> <b>sample</b> <b>size</b> method captures the variance inflation of allele frequency estimation exactly, and can be used to modify the chi-square test statistic, p-value, and 95 % confidence interval of odds-ratio simply by replacing the apparent number of allele counts with the effective ones. For genotype frequency estimation, although a single <b>effective</b> <b>sample</b> <b>size</b> is unable to completely characterize the variance inflation, an averaged one can satisfactorily approximate the simulated result. The <b>effective</b> <b>sample</b> <b>size</b> method is applied to the rheumatoid arthritis siblings data collected from the North American Rheumatoid Arthritis Consortium (NARAC) to establish a significant association with the interferon-induced helicasel gene (IFIH 1) previously being identified as a type 1 diabetes susceptibility locus. Connections between the <b>effective</b> <b>sample</b> <b>size</b> method and other methods...|$|E
40|$|Affected {{relatives}} {{are essential for}} pedigree linkage analysis, however, they cause {{a violation of the}} independent sample assumption in case-control association studies. To avoid the correlation between samples, a common practice is to take only one affected sample per pedigree in association analysis. Although several methods exist in handling correlated samples, they are still not widely used in part because these are not easily implemented, or because they are not widely known. We advocate the <b>effective</b> <b>sample</b> <b>size</b> method as a simple and accessible approach for case-control association analysis with correlated samples. This method modifies the chi-square test statistic, p-value, and 95 % confidence interval of the odds-ratio by replacing the apparent number of allele or genotype counts with the effective ones in the standard formula, without the need for specialized computer programs. We present a simple formula for calculating <b>effective</b> <b>sample</b> <b>size</b> for many types of relative pairs and relative sets. For allele frequency estimation, the <b>effective</b> <b>sample</b> <b>size</b> method captures the variance inflation exactly. For genotype frequency, simulations showed that <b>effective</b> <b>sample</b> <b>size</b> provides a satisfactory approximation. A gene which is previously identified as a type 1 diabetes susceptibility locus, the interferon-induced helicase gene (IFIH 1), is shown to be significantly associated with rheumatoid arthritis when the <b>effective</b> <b>sample</b> <b>size</b> method is applied. This significant association is not established if only one affected sib per pedigree were used in the association analysis. Relationship between the <b>effective</b> <b>sample</b> <b>size</b> method and other methods [...] the generalized estimation equation, variance of eigenvalues for correlation matrices, and genomic controls [...] are discussed. Comment: 3 figure...|$|E
40|$|Abstract: Central to {{the methodological}} {{quality of the}} first round of the European Social Survey (ESS) was the {{principal}} of equivalence in cross national measurement. The survey was therefore designed with equivalence as its driving force and included features such as the requirement for random probability <b>samples,</b> <b>effective</b> <b>sample</b> <b>sizes,</b> clear specifications for fieldwork institutes, clear rules for interviewers about the mode, number and timing of contact attempts with all sample units and the documentation of all contact attempts using standardised forms. The use of standardised, detailed contact forms has enabled equivalent cross-national comparisons of non-response as well as providing some indication of the potential bias in survey estimates. Most but not all of the participating countries in Round 1 successfully collected the information required. The main functions of the contact forms were to document, across several different but functionally equivalent sample designs, each attempt to contact the sample units. In addition, the forms provided information about the reasons of non-contacts and refusals, allowed the construction of comparable non-response rates across countries, provided information about fully co-operative and reluctant respondents and als...|$|R
40|$|Gaussian scale mixture priors are {{frequently}} employed in Bayesian analysis of high-dimensional models, and a theoretical literature exists showing optimal risk properties of {{several members of}} this family in p ≫ n settings when the truth is sparse. However, while implementations of frequentist methods such as the Lasso can scale to dimension {{in the hundreds of}} thousands, corresponding Bayesian methods that use MCMC for computation are often limited to problems at least an order of magnitude smaller. This is in large part due to convergence toward unity of the spectral gap of the associated Markov kernel as the dimension grows. Here we propose an MCMC algorithm for computation in these models that combines blocked Gibbs, Metropolis-Hastings, and slice sampling. Our algorithm has computational cost per step comparable to the best existing alternatives, but superior convergence properties, giving <b>effective</b> <b>sample</b> <b>sizes</b> of 50 to 100 fold larger for identical computation time. Moreover, the convergence rate of our algorithm deteriorates much more slowly than alternatives as the dimension grows. We illustrate the scalability of the algorithm in simulations with up to 20, 000 predictors...|$|R
40|$|Data {{partitioning}} {{has long}} been regarded as an important parameter for phylogenetic inference. The division of heterogeneous multigene data sets into partitions with similar substitution patterns is known to increase the performance of probabilistic phylogenetic methods. However, {{the effect of the}} partitioning scheme on divergence time estimates has generally been ignored. To investigate the impact of data partitioning on the estimation of divergence times, we have constructed two genomic data sets. The first one with 15 nuclear genes comprising 50, 928 bp were selected from the OrthoMam database; the second set was composed of complete mitochondrial genomes. We studied two partitioning schemes: concatenated supermatrices and partitioned gene analysis. We have also measured the impact of taxonomic sampling on the estimates. After drawing divergence time inferences using the uncorrelated relaxed clock in BEAST, we have compared the age estimates between the partitioning schemes. Our results show that, in general, both schemes resulted in similar chronological estimates, however the concatenated data sets were more efficient than the partitioned ones in attaining suitable <b>effective</b> <b>sample</b> <b>sizes...</b>|$|R
40|$|Background Historical {{information}} is always relevant when designing clinical trials, {{but it might}} also be incorporated in the analysis. It seems appropriate to exploit past information on comparable control groups. Purpose Phase IV and proof-of-concept trials are used to discuss aspects of summarizing historical control data as prior information in a new trial. The importance of a fair assessment of the similarity of control parameters is emphasized. Methods The methodology is meta-analytic-predictive. Heterogeneity of control parameters is expressed via the between-trial variation, which is the key parameter determining the prior <b>effective</b> <b>sample</b> <b>size</b> and its upper bound (prior maximum sample size). Results For a Phase IV trial (930 control patients in 11 historical trials) between-trial heterogeneity was fairly small, resulting in a prior <b>effective</b> <b>sample</b> <b>size</b> of approximately 90 patients. For a proof-of-concept trial (363 patients in four historical trials) heterogeneity was moderate to substantial, resulting in a prior <b>effective</b> <b>sample</b> <b>size</b> of approximately 20. For another proof-of-concept trial (14 patients in one historical trial), assuming substantial heterogeneity implied a prior <b>effective</b> <b>sample</b> <b>size</b> of 7. The prior <b>effective</b> <b>sample</b> <b>size</b> can only be large if the amount of historical data is large and between-trial heterogeneity is small. The prior <b>effective</b> <b>sample</b> <b>size</b> is bounded by the prior maximum sample size (ratio of within- to between-trial variance), irrespective of the amount of historical data. Limitations The meta-analytic-predictive approach assumes exchangeability of control parameters across trials. Due to the difficulty to quantify between-trial variability, sensitivity of conclusions regarding assumptions and type of inference should be assessed. Conclusions The use of historical control {{information is}} a valuable option and may lead to more efficient clinical trials. The proposed approach is attractive for nonconfirmatory trials, but under certain circumstances extensions to the confirmatory setting could be envisaged as well. Clinical Trials 2010; 7 : 5 – 18...|$|E
30|$|Considering that spatial {{autocorrelation}} modifies the <b>effective</b> <b>sample</b> <b>size</b> n’ (n’<n), {{the relation}} n’= n [(1 −?)/(1 +?)] {{could be used}} to estimate the <b>effective</b> <b>sample</b> <b>size</b> if the matrix of covariance among locations can be described by a first-order autoregressive correlation structure (Cressie 1991, Fortin and Dale 2005). Fortin and Dale (2005) indicate that this approach can be used for one- and two-sample t-tests and for ANOVA comparisons among means. The same correction could be applied to paired sample t-tests (Dale and Fortin 2002).|$|E
40|$|Marine trawl surveys catch {{a cluster}} of fish at each station and fish caught {{together}} {{tend to have more}} similar characteristics, such as length, age, stomach contents etc., than those in the entire population. When this is the case, the <b>effective</b> <b>sample</b> <b>size</b> of estimates of the frequency distribution of a population characteristic can be much smaller than the number of fish sampled during a survey. As examples, it is shown that the <b>effective</b> <b>sample</b> <b>size</b> for estimates of length-frequency distributions generated by trawl surveys conducted in the Barents Sea, off Namibia and off South Africa is on average approximately one fish per tow. It is concluded that many more fish than necessary are measured at each station and that one way to increase the <b>effective</b> <b>sample</b> <b>size</b> for these surveys and, hence, increase the precision of the length-frequency estimates, is to reduce tow duration and use the time saved to collect samples at more stations...|$|E
40|$|This paper {{explores the}} {{application}} of methods from information geometry to the sequential Monte Carlo (SMC) sampler. In particular the Riemannian manifold Metropolis-adjusted Langevin algorithm (mMALA) is adapted for the transition kernels in SMC. Similar to its function in Markov chain Monte Carlo methods, the mMALA is a fully adaptable kernel which allows for efficient sampling of high-dimensional and highly correlated parameter spaces. We set up the theoretical framework for its use in SMC {{with a focus on}} the application to the problem of sequential Bayesian inference for dynamical systems as modelled by sets of ordinary differential equations. In addition, we argue that defining the sequence of distributions on geodesics optimises the <b>effective</b> <b>sample</b> <b>sizes</b> in the SMC run. We illustrate {{the application of}} the methodology by inferring the parameters of simulated Lotka-Volterra and Fitzhugh-Nagumo models. In particular we demonstrate that compared to employing a standard adaptive random walk kernel, the SMC sampler with an information geometric kernel design attains a higher level of statistical robustness in the inferred parameters of the dynamical systems. Comment: 23 pages, 10 figure...|$|R
30|$|In the Bayesian phylogenetic analysis, {{the best}} {{substitution}} {{models for the}} ITS and rpl 16 data were selected as SYM[*]+[*]I and GTR[*]+[*]I, respectively, using KAKUSAN 4 (Tanabe [2011]) based on Bayesian information criterion (BIC). Two separate runs of Metropolis coupled Markov chain Monte Carlo analyses were performed, each with a random starting tree and four chains (one cold and three heated). The chain length was ten million generations, and the chain was sampled every one thousandth generation from the cold chain. The mixing and convergence of the chains of the two runs was assessed using Tracer ver. 1.5. 0 (Drummond & Rambaut [2007]). The first 10 % of the total 10, 000 sample trees were discarded as burn-in. After the burn-in, the <b>effective</b> <b>sample</b> <b>sizes</b> of all parameters were[*]>[*] 200, indicating that the analyses sampled the posterior distributions of each parameter satisfactorily, and the values of Average Standard Deviation of Split Frequency (ASDSF) were below 0.005. The 50 % majority rule consensus tree of all the post-burn-in trees with Bayesian posterior probabilities (PP) was visualized with FigTree ver. 1.3. 1 (Drummond & Rambaut [2007]).|$|R
40|$|A self-sampling {{reference}} fleet {{is employed}} by the Institute of Marine Research to estimate {{the characteristics of the}} Norwegian commercial catch for a number of species. The reference fleet is composed of commercial fishing vessels that are paid to measure a subsample of fish from selected catches and, less frequently, to take and preserve otolith, stomach, and genetic samples. In this study, the sampling design for monitoring the catches of herring, mackerel, and blue whiting used by the recently established purse-seine segment of the reference fleet is evaluated. The precision of the estimated mean lengths, and hence that of the estimated length distributions of the entire commercial catch, was bounded by the number of boats in the purse-seine reference fleet. Therefore, the only way to improve survey precision significantly is {{to increase the number of}} boats in the reference fleet. In addition, the <b>effective</b> <b>sample</b> <b>sizes</b> were much smaller than the total number of fish measured, from which it followed that too many fish were measured in each selected catch. Based on this analysis, the prescribed number of fish sampled from each selected catch has been reduced by more than 50 %...|$|R
40|$|In {{this study}} we {{address the problem of}} using <b>effective</b> <b>sample</b> <b>size</b> (ESS) to {{approximate}} the probability distributions of order statistics from correlated data. We present these approximations and determine their accuracy through simulation studies. More often than not, correlation exists between data points in a set of data. When we use the original sample size of the data in a derivation of a model of the data, we automatically assume that each data point contains one data point 2 ̆ 7 s worth of information. If the data are correlated, then each data point contains less than one data point 2 ̆ 7 s worth of information making our assumption false. This is especially true in the case of data with a very high level of correlation. <b>Effective</b> <b>sample</b> <b>size</b> represents essentially how many pieces of uncorrelated information the sample would compare to and this is often much smaller than the original sample size. Here we calculate <b>effective</b> <b>sample</b> <b>size</b> which we then use in place of the original sample size. We use a method discussed by Thiebaux and Zwiers [9] for the calculation of <b>effective</b> <b>sample</b> <b>size</b> and show its usefulness using an application to the approximation of the probability distributions of order statistics in correlated data, and finally, we compare our results with simulated data...|$|E
3000|$|Equations (1) and (2) can be {{estimated}} by a system generalized method of moments (GMM), a difference GMM, or a single-equation fixed-effect method, which {{is also used to}} estimate Eqs. (3) and (4). To estimate Eqs. (1), (2), (3) and (4), we used the state-level panel dataset that is balanced (384 observations[*]=[*] 32 states times 12 biannual observations from 1992 to 2014). The period t- 1 thus implies 2  years before period t. When a single-equation fixed-effect method is applied to Eqs. (1), (2) and (3), the <b>effective</b> <b>sample</b> <b>size</b> is 350, as we lose the first period observations due to the use of lagged dependent variables. When a difference GMM method is applied to Eqs. (1) and (2), the <b>effective</b> <b>sample</b> <b>size</b> is 320, as we further lose the second period observations due to the differencing for instruments. Finally, when a system GMM method is applied to Eqs. (1) and (2), the <b>effective</b> <b>sample</b> <b>size</b> becomes 350 again, because the additional condition E([...] Δ y_i,t - 1 ε_it) = 0 that is added allows incorporating the levels and use Δ y_i,t - 1 as an instrument.|$|E
40|$|We address {{an issue}} in the {{transition}} from genetic linkage analysis to genetic association analysis: how to correctly account for correlations between samples obtained from a pedigree for a case-control analysis. Since correlation does not affect the mean of genotype or allele frequency estimation (it only affects the variance), we introduce the concept of “effective sample size ” to account for this effect. The concept of <b>effective</b> <b>sample</b> <b>size</b> much simplifies the handling of complicated relationship between correlated samples. For example, for allele frequency estimation, sibpairs and parent-child pairs are equivalent to 1. 5 samples, first cousins and uncle-nephew pairs are equivalent 1. 6 samples, etc., without considering the affection status. For genotype frequency estimation, the <b>effective</b> <b>sample</b> <b>size</b> concept is perhaps less convenient because its value depends on a particular genotype and depends on the allele frequency. We present the formula for χ 2 test statistic and 95 % confidence interval of oddratio, the two most frequently used quantities in case-control analysis, for correlated samples using the <b>effective</b> <b>sample</b> <b>size...</b>|$|E
40|$|<b>Sample</b> <b>size</b> {{calculations}} in {{the planning}} of clinical trials depend on good estimates of the model parameters involved. When the estimates of these parameters have {{a high degree of}} uncertainty attached to them, it is advantageous to reestimate the <b>sample</b> <b>size</b> after an internal pilot study. For non-inferiority trials with binary outcome we compare the performance of Type I error rate and power between fixed-size designs and designs with <b>sample</b> <b>size</b> reestimation. The latter design shows itself to be <b>effective</b> in correcting <b>sample</b> <b>size</b> and power of the tests when misspecification of nuisance parameters occurs with the former design. ...|$|R
40|$|We tested 16 million SNPs, {{identified}} through whole-genome sequencing of 457 Icelanders, for association with gout and serum uric acid levels. Genotypes were imputed into 41, 675 chip-genotyped Icelanders and their relatives, for <b>effective</b> <b>sample</b> <b>sizes</b> of 968 individuals with gout and 15, 506 individuals for whom serum uric acid measurements were available. We identified a low-frequency missense variant (c. 1580 C>G) in ALDH 16 A 1 associated with gout (OR = 3. 12, P = 1. 5 x 10 (- 16), at-risk allele frequency = 0. 019) and serum uric acid levels (effect = 0. 36 s. d., P = 4. 5 x 10 (- 21)). We confirmed the association with gout by performing Sanger sequencing on 6, 017 Icelanders. The association with gout was stronger in males relative to females. We {{also found a}} second variant on chromosome 1 associated with gout (OR = 1. 92, P = 0. 046, at-risk allele frequency = 0. 986) and serum uric acid levels (effect = 0. 48 s. d., P = 4. 5 x 10 (- 16)). This variant is close to a common variant previously associated with serum uric acid levels. This work illustrates how whole-genome sequencing data allow the detection of associations between low-frequency variants and complex traits...|$|R
40|$|Linear {{area level}} models for small area estimation, {{such as the}} Fay-Herriot model, face {{challenges}} when applied to discrete survey data. Such data commonly arise as direct survey {{estimates of the number}} of persons possessing some characteristic, such as the number of persons in poverty. For such applications, we examine a binomial/logit normal (BLN) model that assumes a binomial distribution for rescaled survey estimates and a normal distribution with a linear regression mean function for logits of the true proportions. <b>Effective</b> <b>sample</b> <b>sizes</b> are defined so variances given the true proportions equal corresponding sampling variances of the direct survey estimates. We extend the BLN model to bivariate and time series (first order autoregressive) versions to permit borrowing information from past survey estimates, then apply these models to data used by the U. S. Census Bureau's Small Area Income and Poverty Estimates (SAIPE) program to predict county poverty for school-age children. We compare prediction results from the alternative models to see how much the bivariate and time series models reduce prediction error variances from those of the univariate BLN model. Standard conditional variance calculations for corresponding linear Gaussian models that suggest how much variance reduction will be achieved from borrowing information over time with linear models agree generally with the BLN empirical results...|$|R
