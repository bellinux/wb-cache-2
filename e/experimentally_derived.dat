1221|215|Public
25|$|Accessibility – {{the motif}} must be {{accessible}} for the binding partner. Intrinsic disorder prediction tools (such as IUPred or GlobPlot), domain databases (such as Pfam and SMART) and <b>experimentally</b> <b>derived</b> structural data (from {{sources such as}} PDB) {{can be used to}} check the accessibility of predicted motif instances.|$|E
25|$|It can be {{successfully}} applied to air flow in lung alveoli, for the flow through a drinking straw {{or through a}} hypodermic needle. It was <b>experimentally</b> <b>derived</b> independently by Jean Léonard Marie Poiseuille in 1838 and Gotthilf Heinrich Ludwig Hagen, and published by Poiseuille in 1840–41 and 1846.|$|E
25|$|However, {{this law}} is {{inaccurate}} at lower temperatures, due to quantum effects; {{it is also}} inconsistent with the <b>experimentally</b> <b>derived</b> third law of thermodynamics, according to which the molar heat capacity of any substance must go to zero as the temperature goes to absolute zero. A more accurate theory, incorporating quantum effects, was developed by Albert Einstein (1907) and Peter Debye (1911).|$|E
40|$|We {{present results}} of a {{systematic}} study of persistent, or residual, images that occur in charged-coupled device (CCD) detectors. A phenomenological model for these residual images, also known as 2 ̆ 2 ghosting, 2 ̆ 2 is introduced. This model relates the excess dark current in a CCD after exposure {{to the number of}} filled impurity sites which is tested for various temperatures and exposure times. We <b>experimentally</b> <b>derive</b> values for the cross section, density, and characteristic energy of the impurity sites responsible for the residual images...|$|R
40|$|Real-time {{multimedia}} applications require {{quality of}} service (QoS) provisioning in terms of bounds on delay and packet loss along with soft bandwidth guarantees. The shared nature of the wireless communication medium results in interference. Interference combined with the overheads, associated with a medium access control (MAC) protocol, and {{the implementation of a}} networking protocol stack limit the available bandwidth in IEEE 802. 15. 4 -based networks and can result in congestion, even if the transmission rates of nodes are well below the maximum bandwidth supported by an underlying communication technology. Congestion degrades the performance of admitted real-time multimedia flow(s). Therefore, in this paper, we <b>experimentally</b> <b>derive</b> the IEEE 802. 15. 4 channel capacity using an unslotted CSMA-CA MAC protocol. We <b>experimentally</b> <b>derive</b> channel capacity for two cases, that is, when the CSMA-CA protocol is working without ACKs and when it is working with ACKs. Moreover, for both cases, we plot the relationship of offered data load with delay and packet loss rate. Simulation results demonstrate that the parameters that affect the choice of a CSMA-CA MAC layer protocol are end-to-end delay and packet loss requirements of a real-time multimedia flow, data load within the interference range of transmitters along the forwarding path, and length of the forwarding path...|$|R
30|$|Offline {{profiling}} and {{staging area}} approaches are typically used to <b>experimentally</b> <b>derive</b> workload resource requirements. However this has an upfront overhead {{and is not}} practical to apply for every application that will be deployed on a IaaS. Several proposals have attempted online profiling and/or monitoring of workloads, however these typically require explicit knowledge of the application [85], or an output from the VM such as latency or response time [82, 86, 87], which is typically not available to IPs. More research is need into application agnostic mechanisms that can extract workload resource requirements, and impact of adaptation, dynamically at run time.|$|R
25|$|Typically {{the sweep}} takes the simple {{form of an}} {{advancement}} of the surface, such that the surface is expanded in a symmetric manner about its advancement axis, with the advancement rate set by a volume attributed to each ion detected and identified. This causes the final reconstructed volume to assume a rounded-conical shape, similar to a badminton shuttlecock. The detected events thus become a point cloud data with attributed experimentally measured values, such as ion time of flight or <b>experimentally</b> <b>derived</b> quantities, e.g. time of flight or detector data.|$|E
25|$|Various sources can {{contribute}} {{to a range of}} simulation results. The range of the simulation results is defined as model uncertainty. One of the most important sources not possible to quantify is the conceptual model, which is developed and defined by the modeller. Further sources are the parameterization of the model regarding the hydraulic (only when simulating transport) and mineralogical properties. The parameters used for the geochemical simulations can also contribute to model uncertainty. These are the applied thermodynamic database and the parameters for the kinetic minerals dissolution. Differences in the thermodynamic data (i.e. equilibrium constants, parameters for temperature correction, activity equations and coefficients) can result in large uncertainties. Furthermore, the large spans of <b>experimentally</b> <b>derived</b> rate constants for minerals dissolution rate laws can cause large variations in simulation results. Despite this is well-known, uncertainties are not frequently considered when conducting geochemical modelling.|$|E
2500|$|Like {{many other}} {{database}} that store protein association knowledge STRING imports data from <b>experimentally</b> <b>derived</b> protein–protein interactions through literature curation. [...] Furthermore, STRING also store computationally predicted interactions from: (i) text mining of scientific texts, (ii) interactions computed from genomic features, and ...|$|E
40|$|The (k-) nearest {{neighbour}} {{problem is}} well known {{in a wide range of}} areas. Many algorithms to tackle this problem suffer from the "curse of dimensionality" which means that the execution time grows exponentially with increasing dimension. Therefore, it is important to have efficient algorithms for the problem. In this report, some well known tree-based algorithms for the k-nearest neighbour are investigated and tested on speech data. We <b>experimentally</b> <b>derive</b> the time complexity as a function of the number of nearest neighbours k, the database size n and the bucket size b. nrpages: 12 status: publishe...|$|R
50|$|Predictions of {{any type}} are not allowed. Contributed data should be <b>derived</b> <b>experimentally</b> and should be {{accompanied}} with experimental evidence.|$|R
40|$|Lith developers, unlike {{conventional}} developers, lose alkalinity when aerially oxidized. EThis {{is attributed}} to the alkaline reaction of the formaldehyde-bisulfite compound (FBS) commonly employed in lith developers as a "sulfite buffer". Conventional chemical analysis methods are used to obtain data and <b>experimentally</b> <b>derive</b> an equilibrium constant for the alkaline disassociation of FBS. Chemical and sensi tometric data are presented that indicate that the alkalinity loss contributes significantly to the overall loss of developer activity. It is also shown that aeriating a lith developer quantitatively converts hydroquinone to hydroquinone monosulfonate that, in turn, is a measurably active developing agent. Additional data are presented that show that solution ionic strength strongly influences hydroquinone developer activity. This {{is attributed to}} increased ionization of hydroquinone. Finally, chemical analysis data are given that indicate that the accepte...|$|R
2500|$|Saturation {{decompression}} is {{a physiological}} process of {{transition from a}} steady state of full saturation with inert gas at raised pressure to standard conditions at normal surface atmospheric pressure. [...] It is a long process during which inert gases are eliminated at a very low rate limited by the slowest affected tissues, and a deviation can cause the formation of gas bubbles which can produce decompression sickness. Most operational procedures rely on <b>experimentally</b> <b>derived</b> parameters describing a continuous slow decompression rate, which may depend on depth and gas mixture.|$|E
5000|$|There is {{an array}} of <b>experimentally</b> <b>derived</b> {{evidence}} to support the above theories: ...|$|E
50|$|The {{structure}} of disordered proteins may {{be approximated by}} running constrained molecular dynamics (MD) simulations where the conformational sampling is being influenced by <b>experimentally</b> <b>derived</b> constraints.|$|E
40|$|This paper evaluates network caching as a {{means to}} improve the {{performance}} of cluster-based multiprocessors. A network cache, shared by all processors on each cluster, offers the potential benefits of increased intra-cluster sharing, reduced network traffic, and useful prefetching. Using simulation, we evaluate the feasibility, structure, and performance of a network cache implementation. Five well-known parallel scientific applications are used in this study. We <b>experimentally</b> <b>derive</b> the network cache working sets of these applications, and demonstrate that the size requirements of the network cache are feasible using current technology. Using cache sizes derived from this working set analysis, we compare a conservative network cache implementation to that of an aggressive directory-based scheme without a network cache. In all five applications, the inclusion of the network cache improves performance. Finally, we examine the effect on application performance of varying the network cac [...] ...|$|R
40|$|Abstract. We {{investigate}} {{the need and}} prospects for measuring dark matter properties at particle collider experiments. We discuss the connections between the inferred properties of particle dark matter and the physics {{that is expected to}} be uncovered by the Large Hadron Collider (LHC) and the International Linear Collider (ILC) and motivate the necessity of measuring detailed dark matter properties at a collider. We then investigate a model-independent signature of dark matter at a collider and discuss its observability. We next examine the prospects for making precise measurements of dark matter properties using two example points in minimal supergravity (mSUGRA) parameter space. One of the primary difficulties encountered in such measurements is lack of constraint on the masses of unobservable heavy states. We discuss a new method for <b>experimentally</b> <b>deriving</b> estimates for such heavy masses and then conclude...|$|R
40|$|We {{propose a}} fault-detection scheme for pipelined, multithreaded processors. The scheme {{is based on}} check-sums and {{improves}} on previous schemes in terms of fault coverage and detection latency by not using compres-sion but storing complete checksums from several pipeline stages. We validate the scheme <b>experimentally</b> and <b>derive</b> checksum polynomials that lead to perfect fault coverage...|$|R
5000|$|The Weiss magneton was an <b>experimentally</b> <b>derived</b> unit of {{magnetic}} moment equal to [...] joules per tesla, {{which is about}} 20% of the Bohr magneton. It was suggested in 1911 by Pierre Weiss.|$|E
5000|$|... #Caption: A z-projection of an {{osteosarcoma}} cell phalloidin stained to visualise actin filaments. The {{image was}} {{taken on a}} confocal microscope and the subsequent deconvolution was done using an <b>experimentally</b> <b>derived</b> point spread function.|$|E
5000|$|... #Caption: An {{example of}} an <b>experimentally</b> <b>derived</b> point spread {{function}} from a confocal microscope using a 63x 1.4NA oil objective. It was generated using Huygens Professional deconvolution software. Shown are views in xz, xy, yz and a 3D representation.|$|E
40|$|We empirically {{investigate}} {{a number of}} strategies for solving the clustering problem under the minimum variance error criterion. First, we compare the behavior of four algorithms, 1) randomized minimum spanning tree, 2) hierarchical grouping, 3) randomized maximum cut, and 4) standard k-means. We test these algorithms with a large corpus of both contrived and real-world data sets and find that standard k-means performs best. We found, however, that standard k-means can, with non-negligible probability, do a poor job optimizing the minimum variance criterion. We therefore investigate various randomized k-means modifications. We empirically find that by running randomized k-means only a modest number of times, {{the probability of a}} poor solution becomes negligible. Using a large number of CPU hours to <b>experimentally</b> <b>derive</b> the apparently optimal solutions, we also find that randomized k-means has the best rate of convergence to this apparent optimum. ICSI and CS Division, Department of [...] ...|$|R
40|$|Using {{a minimal}} model of metabolism, {{we examine the}} {{limitations}} of behavior that is (a) solely in response to environmental phenomena or (b) solely in response to metabolic dynamics, showing that basic forms {{of each of these}} kinds of behavior are incapable of driving survival-prolonging behavior in certain situations. Inspired by experimental evidence of concurrent metabolism-based and metabolism-independent chemotactic mechanisms in Escherichia coli and Rhodobacter sphaeroides, we then investigate how metabolism-independent and metabolism-based sensitivities can be integrated into a single behavioral response, demonstrating that a simple switching mechanism can be sufficient to effectively integrate metabolism-based and metabolism-independent behaviors. Finally, we use a spatial simulation of bacteria to show that the investigated forms of behavior produce different spatio-temporal patterns that are influenced by the metabolic-history of the bacteria. We suggest that these patterns could be a way to <b>experimentally</b> <b>derive</b> insight into the relationship between metabolism and chemotaxis in real bacteria...|$|R
40|$|The tests {{reported}} herein {{were made}} {{for the purpose of}} determining the high-speed load distribution on the wing of a 3 / 16 scale model of a scout-bomber airplane. Comparisons are made between the root bending-moment and section torsional-moment coefficients as obtained <b>experimentally</b> and <b>derived</b> analytically. The results show good correlation for the bending-moment coefficients but considerable disagreement for the torsional-moment coefficients...|$|R
50|$|Added {{resistance}} in waves (RAW) - This element represents the computationally or <b>experimentally</b> <b>derived</b> resistance {{due to the}} motion of a yacht in a seaway. This resistance can be either {{be considered to be}} a factor of true wind speed (VT) or of physical characteristics of the yacht.|$|E
5000|$|Accessibility - {{the motif}} must be {{accessible}} for the binding partner. Intrinsic disorder prediction tools (such as IUPred or GlobPlot), domain databases (such as Pfam and SMART) and <b>experimentally</b> <b>derived</b> structural data (from {{sources such as}} PDB) {{can be used to}} check the accessibility of predicted motif instances.|$|E
50|$|In 1838 he <b>experimentally</b> <b>derived,</b> and in 1840 and 1846 {{formulated}} and published, Poiseuille's law (now {{commonly known}} as the Hagen-Poiseuille equation, crediting Gotthilf Hagen as well), which applies to laminar flow that is, non-turbulent flow of liquids through pipes of uniform section, such as blood flow in capillaries and veins.|$|E
40|$|We {{investigate}} {{the need and}} prospects for measuring dark matter properties at particle collider experiments. We discuss the connections between the inferred properties of particle dark matter and the physics {{that is expected to}} be uncovered by the Large Hadron Collider (LHC) and the International Linear Collider (ILC) and motivate the necessity of measuring detailed dark matter properties at a collider. We then investigate a model-independent signature of dark matter at a collider and discuss its observability. We next examine the prospects for making precise measurements of dark matter properties using two example points in minimal supergravity (mSUGRA) parameter space. One of the primary difficulties encountered in such measurements is lack of constraint on the masses of unobservable heavy states. We discuss a new method for <b>experimentally</b> <b>deriving</b> estimates for such heavy masses and then conclude. Comment: 8 pages, 8 figures; Plenary talk given at PASCOS 05, Gyeongju, Republic of Korea, June 200...|$|R
40|$|I {{survey the}} {{theoretical}} and experimental information available for determination of |Vub| with inclusive and exclusive techniques. Using recent experimental and theoretical advances, I outline a procedure {{in which the}} inclusive information can be combined to obtain an inclusive |Vub| that includes <b>experimentally</b> [...] <b>derived</b> uncertainty estimates for outstanding theoretical corrections. Comment: 9 pages, 5 figures, uses aipproc. cls, submitted to Proceedings of Beauty 200...|$|R
40|$|Wireless {{sensor network}} {{protocols}} and applications, including those used for localization, topology control, link scheduling, and link quality estimation, make {{extensive use of}} Received Signal Strength Indication (RSSI) measurements. In this paper we show that inaccuracies in the RSSI values reported by widely used 802. 15. 4 radios, such as the CC 2420 and the AT 86 RF 230, have profound impact on these protocols and applications. Furthermore, we <b>experimentally</b> <b>derive</b> the response curves which translate actual RSSI values to the raw RSSI readings that the radios report and show that they contain non-linear and even non-injective regions. Fortunately, these curves are consistent across radios of the same model, making RSSI calibration practical. We present a calibration mechanism that removes the artifacts in the raw RSSI measurements, including ambiguities created by the non-injective regions in the response curves, and generates calibrated RSSI readings that are linear. This calibration removes many of the outliers generated when raw RSSI readings are used to estimate Signal to Noise (and Interference) ratios, estimate radio model parameters, and perform RF-based localization. ...|$|R
50|$|Like {{many other}} {{database}} that store protein association knowledge STRING imports data from <b>experimentally</b> <b>derived</b> protein-protein interactions through literature curation. Furthermore, STRING also store computationally predicted interactions from: (i) text mining of scientific texts, (ii) interactions computed from genomic features, and (iii) interactions transferred from model organisms based on orthology.|$|E
50|$|Another {{approach}} uses selection algorithms such as ENSEMBLE and ASTEROIDS. Calculation procedures {{start by}} generating {{a pool of}} random conformers sampling the conformation space. Theoretical parameters are calculated for each conformer and the selection algorithms create the final ensembles by selecting a set of conformers that are fitting to the <b>experimentally</b> <b>derived</b> constraints.|$|E
5000|$|By summing the <b>experimentally</b> <b>derived</b> {{critical}} bandwidths {{over the}} length of the human cochlea, Greenwood developed the following function that describes the relationship between the frequency of a pure tone and the position of the hair cells measured as the fraction of the total length of the cochlear spiral in which it resides: ...|$|E
40|$|We {{introduce}} {{a game in}} which the player navigates an avatar through a maze by using a brain-computer interface (BCI) that analyzes the steady-state visual evoked potential (SSVEP) responses recorded with electroencephalography (EEG) on the players scalp. The four command control game, called The Maze was specifically designed around a SSVEP BCI and validated in several EEG set-ups when using a traditional electrode cap with relocatable electrodes and a consumer-grade headset with fixed electrodes (Emotiv EPOC). We <b>experimentally</b> <b>derive</b> the parameter values that provide an acceptable trade-off between accuracy of game control and interactivity, and evaluate the control provided by the BCI during gameplay. As a final step in the validation of the game, a population study on a broad audience was conducted with the EPOC headset in a real-world setting. The study revealed that the majority (85 %) of the players enjoyed the game despite of its intricate control (mean accuracy 80. 37 %, mean mission time ratio 0. 90). We also discuss what to take into account while designing BCI-based games. status: publishe...|$|R
40|$|International audienceRoom {{temperature}} flexible heat harvesters {{based on}} conducting polymers are {{ideally suited to}} cover the energy demands of the modern nomadic society. The optimization of their thermoelectric efficiency is usually sought by tuning the oxidation levels of the conducting polymers, even if such methodology is detrimental to the Seebeck coefficient (S) as both the Seebeck coefficient and the electrical conductivity (σ) are antagonistically related to the carrier concentration. Here we report a concurrent increase of S and σ and we <b>experimentally</b> <b>derive</b> the dependence of Seebeck coefficient on charge carrier mobility {{for the first time}} in organic electronics. Through specific control of the conducting polymer synthesis, we enabled the formation of a denser percolation network that facilitated the charge transport and the thermodiffusion of the charge carriers inside the conducting polymer layer, while the material shifted from a Fermi glass towards a semi-metal, as its crystallinity increased. This work sheds light upon the origin of the thermoelectric properties of conducting polymers, but also underlines the importance of enhanced charge carrier mobility for the design of efficient thermoelectric polymers...|$|R
40|$|Abstract. Wireless {{sensor network}} {{protocols}} and applications, including those used for localization, topology control, link scheduling, and link quality estimation, make {{extensive use of}} Received Signal Strength Indication (RSSI) measurements. In this paper we show that inaccuracies in the RSSI values reported by widely used 802. 15. 4 radios, such as the CC 2420 and the AT 86 RF 230, have profound impact on these protocols and applications. Furthermore, we <b>experimentally</b> <b>derive</b> the response curves which translate actual RSSI values to the raw RSSI readings that the radios report and show that they contain non-linear and even noninjective regions. Fortunately, these curves are consistent across radios of the same model, making RSSI calibration practical. We present a calibration mechanism that removes the artifacts in the raw RSSI measurements, including ambiguities created by the non-injective regions in the response curves, and generates calibrated RSSI readings that are linear. This calibration removes many of the outliers generated when raw RSSI readings are used to estimate Signal to Noise (and Interference) ratios, estimate radio model parameters, and perform RF-based localization. ...|$|R
