0|41|Public
40|$|Three-dimensional nonseparable perfect {{reconstruction}} filter banks using three-dimensional nonseparable sampling by two, FCO, are proposed. Filter {{structures are}} derived {{and applied to}} digital video. Separation into two bands is obtained, and it is shown to perform better from the perceptual point of view than <b>interlaced</b> <b>sequences</b> resulting from the quincunx sampling of a progressively scanned signal in time-vertical dimension...|$|R
40|$|A motion {{compensated}} de-interlacing method wirh adaptive global {{motion estimation}} and compensation is proposed {{to recover the}} defects of <b>interlaced</b> video <b>sequence</b> with camera panning, rotating or zooming. GME and GMC are used Io recover the change of the whole picture due to camera motions. Two local motion compensated de-interlacing methods are proposed and applied to de-interlace <b>interlaced</b> video <b>sequences</b> with or without global motion respectively. SAD checking and global/local motion comparator {{can be used as}} a block based mode decision system for intrdMC/GMC modes. The Proposed algorithm could achieve higher image quality of <b>interlaced</b> video <b>sequences</b> than any other usual de-interlacing algorithm on progressive devices. 1. tNTRODUCTlO...|$|R
40|$|Given x(1) < x(2) < [...] . < x(n) and y(1) < y(2) < [...] . < y(n- 1), two <b>interlacing</b> <b>sequences</b> of real numbers, the {{rectangular}} diagram for these numbers is a continuous piecewise linear function with slopes +/- 1 and with n local minima at the points x(i) and n- 1 local maxima at the points y(j). Recently, S. Kerov determined the asymptotic behavior of {{the rectangular}} diagrams associated with the zeros of two consecutive orthogonal polynomials for which the coefficients in the three-term recurrence relation converge. The purpose of this note is to show how this result of S. Kerov {{and even some of}} its generalizations follow directly from certain (C, - 1) -summability results on distribution of zeros of orthogonal polynomials proved by us some time ago. status: publishe...|$|R
40|$|This paper {{introduces}} a new method of converting interlaced video to a progressively scanned video. The missing pixel {{values of the}} <b>interlaced</b> <b>sequence</b> are interpolated from past fields according to motion vectors found between the present and past. Hierarchical block-matching motion estimation is used in finding these motion vectors. This approach gives improved results over conventional de-interlacing methods such as median filtering and linear spatial interpolation. 1. INTRODUCTION Interlaced video is widely used in transmitting a sequence of images. Most television systems incorporate interlace for its many advantages, including the reduction of bandwith. However, some image processing tasks require that interlaced video be converted to progressively scanned video. Conversion of frame rates between different interlaced television standards is a good example. For television systems implementing progressively scanned display, de-interlacing is certainly a necessity. Motion estimatio [...] ...|$|R
40|$|Interlaced versus {{progressive}} scanning is {{an important}} issue when dealing with digital television. Not only because the change from analog to digital communication may be seen as an opportunity to move to other formats, but also because of the well-known artifacts of interlaced scanning (interline twitter, line crawling, and field aliasing) compared to the natural way of representing two-dimensional images as the progressive format does. However, digital broadcasting has to face the problem of transmitting twice the number of pels of the progressive format. It is {{the purpose of this article}} to study this problem, and especially to check if the increased vertical and temporal correlations of the progressive pictures provide a significant improvement in the bit-rate reduction efficiency. In that case, progressive scanning may also be used as an intermediate transmission format to improve the compression performances of <b>interlaced</b> <b>sequences.</b> 1. Introduction <b>Interlaced</b> scanning was introd [...] ...|$|R
30|$|In this paper, we have {{proposed}} a flexible video resampling method with the advantages of both the LFI and EDI methods: the capabilities of converting image formats, resizing images for arbitrary ratios and improving edge quality. The proposed method converted input <b>interlaced</b> <b>sequences</b> into upscaled progressive sequences simultaneously and improved the image quality of resampled images by correcting various interpolation artifacts, such as ringing, blurring, and jagged edge artifacts. In {{order to reduce the}} ringing artifacts, the proposed ringing reduction method was combined with Lanczos interpolation and spatio-temporal interpolation. Also, the proposed JEC method was applied to initially upscaled images to correct jagged edge artifacts and to improve the sharpness of the images. Especially, this JEC postprocessor can be very useful for various image resampling applications since it is often used in combination with other common LFI techniques. The proposed algorithm was applied to various test images to verify the performance. Simulation results show that the proposed method outperformed conventional methods both visually and numerically.|$|R
40|$|Abstract. The pel-recursive {{approach}} to motion estimation {{has been widely}} studied for compensating progressively scanned, moderate-resolution video. Although pel-recursive algorithms may not be suitable for application to interlaced high-definition television (HDTV), the underlying principle of backward motion compensation, upon which pel-recursive algorithms are based, can be exploited to improve xisting motion compensation algorithms. This paper proposes applying a backward {{approach to}} motion compensation to improve the performance of standard block-based algorithms for motion-compensated <b>interlaced</b> HDTV <b>sequences.</b> First, we describe a framework for motion compensation i which motion information is parameterized by a motion operator and a domain for that operator. Within this framework, we characterize the type of motion information represented by forward (e. g. block-based) and backward approaches to motion compensation. We propose a method for combining these two sources of motion information to form an optimal motion-compensated prediction. Simulations on two <b>interlaced</b> HDTV <b>sequences</b> demonstrate p rformance improvements between 1 and 2 dB over standard block-based methods...|$|R
40|$|In the MPEG- 2 Test Model 5, the down-conversion of an <b>interlaced</b> <b>sequence</b> is {{obtained}} by prefiltering and sub-sampling each {{field of the}} image sequence {{after it has been}} fully decoded. Although the quality is very good, the cost of such a system is quite high due to large memory requirements. As a result, low-resolution decoders have been proposed to reduce some of the costs incurred by this scheme. Here, incoming DCT blocks are subject to a down-conversion process within the decoding loop, hence the motion compensation is performed using the downconverted images. In past work, it has been proven that the optimal filters for performing this motion compensation are intimately related to the method of down-conversion. Therefore, the choice of down-conversion filter is viewed as the primary variable affecting the quality of the down-converted sequence when such an optimal motion compensation scheme is considered. In the conventional method of frequency domain down-conversion, the 4 x 4 low [...] ...|$|R
40|$|We {{present an}} MPEG- 2 to H. 263 {{transcoder}} that accepts an interlaced MPEG- 2 bitstream as the input and produces a lower-bitrate progressive H. 263 bitstream as the output. As both DVD and digital television may use MPEG- 2 <b>interlaced</b> <b>sequences,</b> a potential application {{of such a}} transcoder is the transmission of a digital television signal over a wireless medium. Another application is transcoding interlaced DVD content for use on lower-resolution thin clients with progressive displays. The proposed algorithm exploits {{the properties of the}} MPEG- 2 and H. 263 compression standards to perform interlaced to progressive (eld to frame) conversion with spatial downsampling and frame-rate reduction in a CPU and memory ecient manner, while additionally minimizing picture quality degradation as measured by PSNR. This is the rst algorithm to our knowledge that eectively uses both spatial and temporal downsampling in an MPEG- 2 to H. 263 eld to frame transcoder in order to achieve substantial bitrate reduction. This paper discusses recoding experiments used to determine appropriate source and target coding parameters for the transcoder, provides {{a detailed description of the}} transcoding algorithm, and describes the performance of a software implementation of the transcoder...|$|R
40|$|We {{study the}} Young graph with edge multiplicities arising in a Pieri-type formula for Jack {{symmetric}} polynomials P_μ(x;a) with a parameter a. Starting with the empty diagram, we define recurrently the `dimensions' _a {{in the same}} way as for the Young lattice or Pascal triangle. New proofs are given for two known results. The first is the a-hook formula for _a, first found by R. Stanley. Secondly, we prove (for all complex u and v) a generalization of the identity ∑ν(c(b) +u) (c(b) +v) ν/μ=(n+ 1) (n+uv), where ν runs over immediate successors of a Young diagram μ with n boxes. Here c(b) is the content of a new box b. The identity is known to imply the existence of an interesting family of positive definite central functions on the infinite symmetric group. The approach is based on the interpretation of a Young diagram as a pair of <b>interlacing</b> <b>sequences,</b> so that analytic techniques may be used to solve combinatorial problems. We show that when dealing with Jack polynomials P_μ(x;a), it makes sense to consider `anisotropic' Young diagrams made of rectangular boxes of size 1 × a. Comment: 16 pages, AmSTeX, uses EPSF, three EPS figure...|$|R
40|$|The {{literature}} on organizational learning has traditionally either conceptualized learning as an intraorganizational {{or as an}} interorganizational process without taking into consideration how these different levels of learning are interlaced. We propose an integrated conceptual framework of intra- and interorganizational learning processes that address the fundamental experiential learning problem of balancing exploitation and exploration – a central issue for organizational development and prosperity. The conceptual framework is empirically studied in two longitudinal case studies of {{the internal and external}} product development activities of Scandinavian and American software companies. The case findings indicate that learning at both the intra- and interorganizational levels tend towards exploitation, i. e., refinement, routinisation, and production; while switching between levels in both directions was associated with exploration, i. e., experimentation, innovation, and free association. By studying both levels of learning simultaneously, it was hence possible to discern the pattern of exploratory switching between levels from the traditional view of interorganizational learning itself being mainly exploratory and intraorganizational learning being mainly exploitative. It was also possible to systematically track how intra- and interorganizational processes follow specific, <b>interlaced</b> <b>sequences</b> of experiential learning within and between organizations. Organizational learning; strategic alliances; product development; case study...|$|R
40|$|This paper {{presents}} {{a new system}} for the conversion of interlaced formats to progressive ones. Like other proposals, it is motion compensation-based, but a substantial 2 -fold improvement is added. First, assuming translational motions, {{the problem in the}} vertical direction is studied as a generalized interpolation problem. As a result, we derive two sets of linear filters which take into account the aliasing existing inside the fields. The first set of filters allows one to efficiently perform the interpolation which is required for subpel motion estimation. The second set of filters improves the estimation of the lines needed to obtain the progressive format, namely the deinterlacing process itself. Second, an improved block matching algorithm based on a split and merge procedure is used to estimate the motion and correctly propagate the motion vectors from blocks with reliable motion to blocks with uncertain motion. In order to tackle the problem of covered/uncovered objects, the whole process of estimation and motion-compensated interpolation is applied forward and backward. Simulation results and objective measurements are provided for artificially moving <b>interlaced</b> <b>sequences</b> obtained from a fixed picture and for a progressive sequence first converted to interlaced. The global system has also been tested on other sequences and visually assessed...|$|R
50|$|VC-1 is an {{evolution}} of the conventional DCT-based video codec design also found in H.261, MPEG-1 Part 2, H.262/MPEG-2 Part 2, H.263, and MPEG-4 Part 2. It is widely characterized {{as an alternative to}} the ITU-T and MPEG video codec standard known as H.264/MPEG-4 AVC. VC-1 contains coding tools for <b>interlaced</b> video <b>sequences</b> as well as progressive encoding. The main goal of VC-1 Advanced Profile development and standardization was to support the compression of interlaced content without first converting it to progressive, making it more attractive to broadcast and video industry professionals.|$|R
50|$|The ATSC {{specification}} and MPEG-2 {{allow the}} use of progressive frames coded within an <b>interlaced</b> video <b>sequence.</b> For example, NBC stations transmit a 1080i60 video sequence, meaning the formal output of the MPEG-2 decoding process is sixty 540-line fields per second. However, for prime-time television shows, those 60 fields can be coded using 24 progressive frames as a base - actually, an 1080p24 video stream (a sequence of 24 progressive frames per second) is transmitted, and MPEG-2 metadata instructs the decoder to interlace these fields and perform 3:2 pulldown before display, as in soft telecine.|$|R
40|$|A triple of {{vertices}} in a graph is {{a frustrated}} triangle if it induces an odd number of edges. We study the set F_n⊂[0,n 3] of possible number of frustrated triangles f(G) in a graph G on n vertices. We prove that about {{two thirds of}} the numbers in [0,n^ 3 / 2] cannot appear in F_n, and we characterise the graphs G with f(G) ∈[0,n^ 3 / 2]. More precisely, our main result is that, for each n≥ 3, F_n contains two <b>interlacing</b> <b>sequences</b> 0 =a_ 0 ≤ b_ 0 ≤ a_ 1 ≤ b_ 1 ≤ [...] . ≤ a_m≤ b_m∼ n^ 3 / 2 such that F_n∩(b_t,a_t+ 1) =∅ for all t, where the gaps are |b_t-a_t+ 1 |=(n- 2) -t(t+ 1) and |a_t-b_t|=t(t- 1). Moreover, f(G) ∈[a_t,b_t] if and only if G can be obtained from a complete bipartite graph by flipping exactly t edges/nonedges. On the other hand, we show, for all n sufficiently large, that if m∈[f(n),n 3 -f(n) ], then m∈ F_n where f(n) is asymptotically best possible with f(n) ∼ n^ 3 / 2 for n even and f(n) ∼√(2) n^ 3 / 2 for n odd. Furthermore, we determine the graphs with the minimum number of frustrated triangles amongst those with n vertices and e≤ n^ 2 / 4 edges. Comment: 19 pages, 4 figures, submitte...|$|R
40|$|We use {{the method}} of mutual {{interlacing}} to prove two conjectures on the real-rootedness of Eulerian-like polynomials: Brenti's conjecture on $q$-Eulerian polynomials for Weyl groups of type $D$, and Dilks, Petersen, and Stembridge's conjecture on affine Eulerian polynomials for irreducible finite Weyl groups. For the former, we obtain a refinement of Brenti's $q$-Eulerian polynomials of type $D$, and then show that these refined Eulerian polynomials satisfy certain recurrence relation. By using the Routh [...] Hurwitz theory and the recurrence relation, we prove that these polynomials form a mutually <b>interlacing</b> <b>sequence</b> for any positive $q$, and hence prove Brenti's conjecture. For $q= 1 $, our result reduces to the real-rootedness of the Eulerian polynomials of type $D$, which were originally conjectured by Brenti and recently proved by Savage and Visontai. For the latter, we introduce a family of polynomials based on Savage and Visontai's refinement of Eulerian polynomials of type $D$. We show that these new polynomials satisfy the same recurrence relation as Savage and Visontai's refined Eulerian polynomials. As a result, we get the real-rootedness of the affine Eulerian polynomials of type $D$. Combining the previous results for other types, we completely prove Dilks, Petersen, and Stembridge's conjecture, which states that, for every irreducible finite Weyl group, the affine descent polynomial has only real zeros. Comment: 28 page...|$|R
40|$|In {{the early}} days of {{television}} as Cathode Ray Tube (CRT) screens became brighter, the level of flicker caused by progressive scanning became more noticeable. This is because the human visual system is sensitive to large-area flicker. Interlaced scanning was invented in 1932 as a redeem to this difficulty. In contrast to progressive scanning, where every line is drawn in <b>sequence,</b> <b>interlaced</b> scanning alternates lines of a frame in half a frame interval, called a field. The conversion process from interlace scan to progressive scan is called deinterlacing. In this thesis, two deinterlacing methods, which use motion information from the video sequence, were used in conjunction to obtain an improved result. Thus, the process of finding the true motion on the <b>interlaced</b> video <b>sequence</b> also had to be analyzed. The analysis was done on artificially generated test sequences, as well as true video sequences. The result was measured using the Mean Square Error between a progressive input sequence and the deinterlaced output sequence. This measurement was compared to a much simpler deinterlacing algorithm and showed large improvements, primarily in sense of aliasing. However, in some cases, the deinterlacer produced severe artifacts causing picture degradation. Validerat; 20101217 (root...|$|R
40|$|JVT Scalable Video Coding (SVC) {{provides}} high coding efficiency for progressive video sequences with combined scalability. However, interlaced SVC is {{only the}} straightforward extension of H. 264 /AVC interlaced coding with the similar FGS coding technique for progressive coding. Based on the particular temporal correlation of <b>interlaced</b> video <b>sequences,</b> this paper presents a novel and efficient FGS coding scheme for interlaced SVC, which is able to achieve higher compression efficiency and further temporal scalability by introducing additional temporal decomposition stage when coding key pictures. The advantages of our proposed interlaced FGS coding scheme are verified by integrating it into JVT-SVC reference software. Index Terms—FGS, SVC, interlace 1...|$|R
40|$|In this letter, {{we present}} a new method for the motion detection/compensation between {{opposite}} parity fields in <b>interlaced</b> video <b>sequences.</b> We introduce a phase-correction filter, which is applied to one type (even or odd) of fields before motion detection/compensation. By means of this phase-correction filter, the motion-compensated PSNR has been improved by more than 2 dB, on average. We also present a new deinterlacing algorithm based on the newly developed motion detection/compensation. This algorithm requires storing one field only, and the phase-corrected field is used for both motion detection/compensation and intrafield deinterlacing, thus making the proposed algorithm computationally very efficient. Excellent deinterlacing results have been obtained...|$|R
50|$|The ATSC {{specification}} and MPEG-2 {{allow the}} use of progressive frames, even within an <b>interlaced</b> video <b>sequence.</b> For example, a station that transmits 1080i60 video sequence can use a coding method where those 60 fields are coded with 24 progressive frames and metadata instructs the decoder to interlace them and perform 3:2 pulldown before display. This allows broadcasters to switch between 60 Hz interlaced (news, soap operas) and 24 Hz progressive (prime-time) content without ending the MPEG-2 sequence and introducing a several seconds of delay as the TV switches formats. This {{is the reason why}} 1080p30 and 1080p24 sequences allowed by the ATSC specification are not used in practice.|$|R
40|$|Three {{approaches}} of an MPEG- 2 compatible coding technique are presented for stereoscopic sequences. The rst method utilizes the spatial scalability {{structure and the}} second employs the temporal scalability syntax. The scalability extensions of the video coding standard make the processing easier to accommodate the transmission of a stereoscopic video stream. The left and right channels required for a stereo sequence are correspondingly supported in the base and enhancement layers of the scalability structure. The enhancement layer selects the best prediction combination of the spatial and temporal information. In the third technique, {{the left and right}} stereoscopic images are represented and coded as an interlaced video. For <b>interlaced</b> <b>sequences,</b> transmission in either the eld or frame picture structure can be chosen for each frame to maximize compression, and therefore improve image reconstruction. Selection of picture structures depends on the temporal changes of the stereoscopic scenery. Field picture structure is chosen for scenes with high stereoscopic activities, whereas frame structure is preferred for stereo images with little or no disparity variations. Experimental results are presented for the proposed approaches and performance comparisons between them are analyzed and interpreted. Simulations on several stereoscopic sequences illustrate that the proposed spatial and temporal scalability methods always achieve better SNR performances than the simulcast method. Experimental data on progressive stereo sequences with the left-right interlaced structure approach show improving performances, as compared to the simulcast and both scalability techniques, when transmitted with increasing bit rate. As a result, the interlaced eld structure approach, supported in the main pro le of MPEG- 2, is the recommended choice for high bit rate coding of progressive stereo sequences. ...|$|R
40|$|Abstract. We give {{a simple}} proof of an {{explicit}} formula for Kerov polynomials. This explicit formula {{is closely related}} to a recent formula of Goulden and Rattan. 1. Kerov polynomials Kerov polynomials are universal polynomials which express the characters of symmetric groups evaluated on cycles, in terms of quantities known as the free cumulants of a Young diagram. We now explain these notions. Let λ = λ 1 ≥ λ 2 ≥ · · · be a Young diagram, to which we associate a piecewise affine function ω: R → R, with slopes ± 1, such that ω(x) = |x | for |x | large enough, as in Figure 1 below, which corresponds to the partition 8 = 4 + 3 + 1. We can encode the Young diagram using the local minima and local maxima of the function ω, denoted by x 1, [...] ., xm and y 1, [...] ., ym− 1 respectively, which form two <b>interlacing</b> <b>sequences</b> of integers. These are (− 3, − 1, 2, 4) and (− 2, 1, 3) respectively in the picture. To the Young diagram we associate the rational fraction Gλ(z) = ∏m− 1 ∏i= 1 m i= 1 (z − yi) (z − xi) and the formal power series Kλ, inverse to G for composition, Rk(λ) z k− 1. Kλ(z) = G (− 1) λ (z) = z − 1 + The quantities Rk(λ), k = 2, 3, [...] ., are called the free cumulants of the diagram λ. Note that R 1 (λ) = 0 for any Young diagram, so we do not include it in the series of free cumulants. These quantities arise in the asymptotic study of representations of symmetric groups, see [B 1]. It turns out that there exist universal polynomials Σ 2, Σ 3, [...] . in th...|$|R
40|$|It {{has been}} {{hypothesized}} that the human visual system can use temporal synchrony for the perceptual grouping of image regions into unified objects, as proposed in some neural models. It is argued here, however, that previous psychophysical evidence for this hypothesis is due to stimulus artifacts, and that earlier studies do not, therefore, support the claims of synchrony sensitive grouping mechanisms or processes. The sequence of dots • • • • • • • • • • is typically perceived {{as a series of}} loosely spaced dot pairs ••. Why don’t we perceive this sequence as a series of the equally plausible tightly spaced dot pairs • •? Because elements in close proximity {{are more likely to be}} perceived as belonging together. In the sequence, • ◦ • ◦ • ◦ • ◦ • ◦, however, proximity and color compete against one another, so that we might perceive a series of dot pairs ◦•, or two <b>interlaced</b> <b>sequences</b> of •’s and ◦’s. Proximity and color are just two possible cues that play a role in perceptual grouping [1], that is, the ability of the visual system to organize a multitude of parts into a unified entity or object. In complex visual scenes similar cues play a role in the perceptual grouping of objects. For example the mostly uniform color and texture of the cheetah in Figure 1 helps us in organizing its various parts into a single object. In addition to such static cues, dynamic cues also play a role in perceptual grouping. For example, in the context of a stationary background, the largely uniform translational (common-fate) motion of the running cheetah’s body and head in Figure 1 help us in group...|$|R
40|$|We propose an {{algorithm}} for deinterlacing of <b>interlaced</b> video <b>sequences.</b> It successively builds approximations to the deinterlaced sequence by weighting various interpolation methods. A particular example given here, uses four interpolation methods, weighted {{according to}} the errors each one introduces. Due to weighting, it is an adaptive algorithm. It is also time-recursive since motion-compensated part uses the previously interpolated frame. Furthermore, bi-directional motion estimation and compensation allow for better performance in case of scene changes and covering /uncovering of objects. Experiments are run both on "real-world" and computer generated sequences. Finally, subjective testing is performed to evaluate {{the quality of the}} algorithm. 1 Introduction Interlaced video has been around for quite some time, and along the way many of the problems associated with it have been discovered, such as line crawl and inter-line flicker. Moreover, interlaced video makes motion-based pr [...] ...|$|R
40|$|This {{paper is}} {{primarily}} concerned with motion-compensated interpolation of video sequences using multiple images. Due to the extended temporal support of such motion compensation, linear (constant-velocity) trajectory model is often inappropriate, for example due to insufficient temporal sampling. Recently, we have proposed a quadratic (constant-acceleration) trajectory model and a framework for the computation of its parameters. The approach is based on Markov random field (MRF) models that lead to a regularized formulation solved by multiresolution deterministic relaxation. In this paper, we demonstrate advantages of using accelerated motion over linear trajectories in a plausible application using natural data. We apply the estimated trajectories to motioncompensated interpolation over multiple frames of progressive and <b>interlaced</b> video <b>sequences.</b> The experimental results for "Miss America" (CIF) and "Femme et arbre" (interlaced) show, respectively, a 4 and 2 dB average improvement [...] ...|$|R
40|$|This paper {{addresses}} how {{to utilize}} both the {{spatial and temporal}} information present in a short video sequence to create a high-resolution still. A novel observation model based on motion compensated subsampling is proposed for video data. Since the reconstruction problem is ill-posed, Bayesian restoration with an edge-preserving prior image model is used to extract a high-resolution frame given a low-resolution sequence. Estimates generated by the multiframe video extraction algorithm show dramatic improvements over single frame interpolation techniques. Simulation results are reported for an image sequence containing a subpixel camera pan and an <b>interlaced</b> video <b>sequence.</b> 1 Introduction Single frame interpolation techniques [1, 2] have been researched quite extensively, with the zeroorder hold, bilinear, and various cubic spline interpolation methods providing progressively more accurate solutions. However, these methods are limited {{by the number of}} constraints available within th [...] ...|$|R
40|$|The {{evolution}} of the television market is led by 3 DTV technology, and this tendency can accelerate during the next years according to expert forecasts. However, 3 DTV delivery by broadcast networks is not currently developed enough, and acts as a bottleneck for the complete deployment of the technology. Thus, increasing interest is dedicated to ste-reo 3 DTV formats compatible with current HDTV video equipment and infrastructure, as they may greatly encourage 3 D acceptance. In this paper, different subsampling schemes for HDTV compatible transmission of both progressive and interlaced stereo 3 DTV are studied and compared. The frequency characteristics and preserved frequency content of each scheme are analyzed, and a simple interpolation filter is specially designed. Finally, {{the advantages and disadvantages}} of the different schemes and filters are evaluated through quality testing on several progressive and <b>interlaced</b> video <b>sequences...</b>|$|R
40|$|Traditionally, ice I was {{considered}} to exist in two well-defined crystalline forms at ambient pressure: stable hexagonal ice (ice Ih) and metastable cubic ice (ice Ic). However, {{it is becoming increasingly}} evident that what has been called cubic ice in the past does not have a structure consistent with the cubic crystal system. Instead, it is a stacking-disordered material containing cubic <b>sequences</b> <b>interlaced</b> with hexagonal <b>sequences,</b> which is termed stacking-disordered ice (ice Isd). In this article, we summarise previous work on ice with stacking disorder including ice that was called cubic ice in the past. We also present new experimental data which shows that ice which crystallises after heterogeneous nucleation in water droplets containing solid inclusions also contains stacking disorder even at freezing temperatures of around - 15 °C. This supports the results from molecular simulations, that the structure of ice that crystallises initially from supercooled water is always stacking-disordered and that this metastable ice can transform to the stable hexagonal phase subject to the kinetics of recrystallization. We also show that stacking disorder in ice which forms from water droplets is quantitatively distinct from ice made via other routes. The emerging picture of ice I is that of a very complex material which frequently contains stacking disorder and this stacking disorder can vary in complexity depending on the route of formation and thermal history...|$|R
30|$|The encoder {{always used}} two {{interlaced}} slice groups of two macroblock lines. For error recovery, an intra image was forced every 24 frames and {{the ratio of}} intra macroblock refresh was 5 %. The video resolution was 1, 920 [*]×[*] 1, 080 pixels at 59.94 fields-per-second in <b>interlaced</b> format. The <b>sequences</b> have a duration of 10  s. In total, 24 naive observers viewed the content. The Absolute Category Rating with Hidden Reference (ACR-HR) conforming to ITU-T P. 910 with a five-point rating scale was used. The subjects viewed the content {{at a distance of}} 1.5  m corresponding to three times the picture height. More details about the subjective experiment can be found in [19].|$|R
30|$|Deinterlacing {{and image}} {{upscaling}} {{have been studied}} for decades. These methods can be roughly classified into linear filtering interpolation (LFI) and edge directional interpolation (EDI). Among the deinterlacing methods, LFI approaches[2 – 7] can be categorized into spatial (intra-field), temporal (inter-field), and spatio-temporal methods, according to the field information. In order to obtain progressive images, missing pixels have to be reconstructed using linear filters according to the spatial correlations, the temporal correlations, and both the spatial and temporal correlations in <b>interlaced</b> video <b>sequences.</b> Particularly, some algorithms[5 – 7] discover missing pixel values by interpolating the pixels along motion trajectories, because temporal correlation is dependent on motion information. Among the image upscaling methods, LFI approaches[8 – 12] design a particular interpolation kernel, which {{can be applied to}} the entire image. Especially, these methods can resize images with arbitrary ratios, which is one of the preferred features for image upscaling applications. LFI methods for both deinterlacing and upscaling are as old as image processing, and they are still popular because of their simple implementation. However, LFI approaches usually produce jagged edge artifacts (stair-like artifacts) in diagonal edge regions because they do not consider any edge information during the resampling process.|$|R
40|$|In practice, <b>interlaced</b> video <b>sequences</b> are {{typically}} coded {{with either a}} frame-only or fieldonly structure, irrespective of the content. However, coding in this way will not provide the best coding efficiency. This paper proposes an adaptive picture-level field/frame coding scheme with corresponding rate control. First, a two-pass field/frame decision scheme is proposed. In this scheme, we formulate the field/frame decision as a constrained optimization problem. The actual rate and distortion data are collected and the optimal picture-level coding decision is determined based on this data. An effective rate control for the proposed two-pass algorithm is also presented. However, since {{the complexity of the}} two-pass scheme is relatively large since motion estimation must be performed for both the frame-based picture and the field-based picture, we also propose a one-pass field/frame decision scheme. This one-pass scheme calculates the variance of each macroblock in a field and estimates the correlation between two fields. Based on the correlation, a decision to code the picture as a frame or as fields is made. A rate control method for the proposed one-pass scheme is also presented. Simulation results demonstrate that our scheme outperforms frame-only and field-only coding for several sequences coded at a wide range of bit-rates, and the proposed one-pass scheme obtains similar performance as the proposed two-pass scheme...|$|R
40|$|AbstractA linearly {{recursive}} {{sequence in}} n variables is a tableau of scalars (ƒi 1 …in) for i 1,i 2,…, in ⩾ 0, such that for each 1 ⩽ i ⩽ n, all rows {{parallel to the}} ith axis satisfy a fixed linearly recursive relation hi(x) with constant coefficients. We show that such a tableau is Hadamard invertible (i. e., the tableau (1 / ƒi 1 … in) is linearly recursive) {{if and only if}} all ƒi 1 … in≠ 0, and each row is eventually an <b>interlacing</b> of geometric <b>sequences.</b> The procedure is effective, i. e., given a linearly recursive sequence ƒ = (ƒi 1 … in), it can be tested for Hadamard invertibility by a finite algorithm. These results extend the case n = 1 of Larson and Taft...|$|R
40|$|This paper investigates a coding {{method for}} <b>interlaced</b> image <b>sequences</b> which uses deinterlaced images as an {{intermediate}} format. The purpose of this method is threefold: firstly, to achieve a higher coding gain, as the processing of progressive pictures raises much less difficulty than the processing of interlaced ones; secondly, to provide an intermediate step toward {{the development of a}} fully progressive production and transmission chain; and finally to open the way to efficient solutions for important image processing issues such as frame rate conversion and compatibility. In previous work by the authors (1994), an interlace-to-progressive converter was proposed. It was based on theoretically correct generalized interpolation formulas, working under the assumption of a uniform translational motion. The scheme proposed here makes use of this converter to produce the progressive images from the interlaced source, which are coded and then re-interlaced. Another solution is considered, with the purpose of improving the coding efficiency alone. It is based on an intermediate progressive format at half the original temporal sampling rate. However, in that case, some additional information needs to be transmitted in order to accurately recover the full rate interlaced signal. The two methods mentioned above are described in some detail and tested on actual sequences. The picture quality is assessed, both objectively and subjectively, and conclusions are drawn. Anglai...|$|R
40|$|This {{contribution}} investigates local differential {{techniques for}} estimating optical flow and its derivatives {{based on the}} brightness change constraint. By using the tensor calculus representation we build the Taylor expansion of the gray-value derivatives {{as well as of}} the iotical flow in a spatiotemporal neighborhood. Such a formulation provides a unifying framework for all existing local differential approaches and allows to derive new systems of equations for the estimation of the optical flow and of its derivatives. We also tested various optical flow estimation approaches on real image sequences recorded by a calibrated camera which was fixed on the arm of a robot. By moving the arm of the robot along a precisely defined trajectory, we can determine the true displacement rate of scene surface elements projected into the image plane and compare it quantitatively with the results of different optical flow estimators. Since the optical flow estimators are based on gray-value derivati ves of up to fourth-order, we were forced to develop modified Gaussian derivative filters to obtain acceptable estimates for the derivatives. Further, we show quantitatively that these filters contribute to a much more robust optical flow estimation. In addition, successive lines of TV-cameras have an offset in time due to the interlace technique. We demonstrate the adaption of filter kernels for estimating higher-order spatiotemporal derivatives in <b>interlaced</b> image <b>sequences...</b>|$|R
40|$|This paper {{proposes a}} {{hardware}} architecture {{of a video}} noise estima-tion algorithm capable of real-time processing. The objectives con-sist of adapting a computationally demanding noise estimation algo-rithm to a synthesizable VHDL implementation and achieving real-time performance. This Structure-oriented noise estimation method considers image structure to find intensity-homogeneous blocks. Sub-sequently, these blocks {{are included in the}} averaging process to es-timate the noise variance. Generating worst-case estimation error of 3 dB, this spatial noise reduction method is reliable for highly noisy and textured images. The proposed architecture provides a satisfac-tory compromise between area and processing speed. Furthermore, parameterization of the architecture allows additional flexibility with the scaling of mask sizes that can operate on 3 x 3 or 5 x 5 blocks of pixels. The proposed design is targeted to an FPGA device and esti-mates the noise variance over an <b>interlaced</b> PAL video <b>sequence...</b>|$|R
30|$|The {{first public}} {{database}} of video contents and related subjective quality scores {{was produced by}} the Video Quality Experts Group (VQEG) and {{used to compare the}} performance of Full-Reference objective metrics, targeting secondary distribution of television as application [3]. Unfortunately, only part of the subjective results and test materials used to perform the study has been made publicly available. Additionally, this dataset includes <b>interlaced</b> video <b>sequences</b> and focuses on MPEG- 2 compression. These distortions are not representative of the current video coding and transmission technologies. Thus, the usage of VQEG data by independent researchers to validate more recent and future metrics is limited. Recently, two video databases have been proposed by the Laboratory for Image and Video Engineering (LIVE) at the University of Texas at Austin. The LIVE Video Quality Database [4] consists of a set of video sequences corresponding to different contents, distorted by MPEG- 2 and H. 264 /AVC compression as well as by transmission over error-prone IP and wireless networks. The presence of diverse distortion types makes this database particularly useful to test the consistency of metrics performance. The LIVE Wireless Video Quality Assessment Database [5] focuses on distortions due to transmission over a wireless network and takes into account a set of video sequences having similar content concerning airplanes. These databases include the test video sequences and the processed subjective results and have been used to evaluate the performance of a set of Full-Reference video quality metrics in [4, 5].|$|R
