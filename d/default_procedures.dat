12|74|Public
50|$|Many of the <b>default</b> <b>procedures</b> of the MNCA are {{different}} from standard parliamentary procedure, {{though they may be}} superseded by a provision either in the articles of incorporation or in the bylaws of the corporation.|$|E
50|$|As the CCP {{concentrates}} {{the risk}} of settlement failures into itself {{and is able to}} isolate the effects of a failure of a market participant, it also needs to be properly managed and well-capitalized in order to ensure its survival {{in the event of a}} significant adverse event, such as a large clearing firm defaulting. Guarantee funds are capitalized with collateral from the member firms. In the event of a settlement failure, the defaulting firm may be declared to be in default and the CCP's <b>default</b> <b>procedures</b> utilized, which may include the orderly liquidation of the defaulting firm's positions and collateral. In the event of a significant clearing firm failure, the CCP may draw on its guarantee fund in order to settle trades on behalf of the failed clearing firm.|$|E
40|$|The Erie {{doctrine}} governs, {{among other}} things, when {{a federal court}} sitting in diversity jurisdiction may use a federal procedure that differs from the procedure a state court would use. Displacing the state procedure with the federal procedure (or not) may impact the substantive objectives of either state or federal law, but the current Erie doctrine provides little guidance. This Article argues that the Erie doctrine is best understood as governing a choice of enforcement defaults. As argued below, the primary function of civil liability is to protect a substantive entitlement to avoid the legal violation, either directly through specific performance remedies or through deterrence. Accordingly, procedures in federal and state court {{can be understood as}} <b>default</b> <b>procedures</b> to enforce this substantive entitlement, and these defaults are often abrogated by private contract or through legislation. Understood in this way, the Erie doctrine governs when a federal court may abrogate a state enforcement default {{and replace it with a}} federal one. This Article then uses the existing literature on default rules to argue that the Erie doctrine itself should use default rules to force information from both state and federal governments about the relationship of <b>default</b> <b>procedures</b> to substantive policies. This way, federal courts can make better choices between enforcement defaults...|$|E
30|$|These {{data report}} our early {{experience}} with a TLH and demonstrate a satisfactory record during its introduction to a new unit. This new procedure offers a safe alternative to TAH for many women with no increased morbidity in agreement with recent literature [2, 9]. There is an excepted bias in the reported cases as the route chosen was not randomised; however, since introducing TLH, {{it has become the}} <b>default</b> <b>procedure</b> for endometrial pathology, reserving TAH as the <b>default</b> <b>procedure</b> for a grossly enlarged uterus.|$|R
50|$|An {{application}} usually calls DefWindowProc {{at the end}} of its own WindowProc function so {{that whatever}} messages it has not processed are passed on to the <b>default</b> <b>procedure.</b>|$|R
5000|$|... : This {{variable}} specifies the directory, {{where the}} Windows [...] {{to be used}} by the DR-DOS [...] multitasker is located, overriding the <b>default</b> <b>procedure</b> to locate the file.|$|R
40|$|Zellner’s g prior {{remains a}} popular {{conventional}} prior {{for use in}} Bayesian variable selection, despite several undesirable consistency issues. In this article we study mixtures of g priors {{as an alternative to}} default g priors that resolve many of the problems with the original formulation while maintaining the computational tractability that has made the g prior so popular. We present theoretical properties of the mixture g priors and provide real and simulated examples to compare the mixture formulation with fixed g priors, empirical Bayes approaches, and other <b>default</b> <b>procedures...</b>|$|E
40|$|Robust {{statistics}} has slowly {{become familiar}} to all practitioners. Books entirely {{devoted to the}} subject are without any doubts responsible for the increased practice of robust statistics in all fields of applications. Even classical books often {{have at least one}} chapter (or parts of chapters) which develops robust methodology. The improvement of computing power has also contributed {{to the development of a}} wider and wider range of available robust procedures. However, this success story is now menacing to get backwards: non specialists interested in the application of robust methodology are faced with a large set of (assumed equivalent) methods and with over-sophistication of some of them. Which method should one use? How the (numerous) parameters should be optimaly tuned? These questions are not so easy to answer for non specialists! One could then argue that <b>default</b> <b>procedures</b> are available in most statistical softwares (Splus, R, SAS, Matlab, [...] .). However, using as illustration the detection of outliers in multivariate data, it is shown that, on one hand, it is not obvious that one would feel confident with the output of <b>default</b> <b>procedures,</b> and that, on the other hand, trying to understand thoroughly the tuning parameters involved in the procedures might require some extensive research. This is not conceivable when trying to compete with the classical methodology which (while clearly unreliable) is so straightfoward. The aim of the paper is to help the practitioners willing to detect in a reliable way outliers in a multivariate data set. The chosen methodology is the Minimum Covariance Determinant estimator being widely available and intuitively appealing. Peer reviewe...|$|E
40|$|The use of {{powerful}} low or {{very low frequency}} (LF/VLF) transmitter signals (mainly in the range 15 - 60 kHz) is a well established technique for remote sensing of the lower ionosphere. Standard tools for calculating the the world wide propagation conditions – like the Long Wave Propagation Capability (LWPC) code- usually rely on <b>default</b> <b>procedures</b> for modeling the day-night transition conditions that do not map reality sufficiently for modeling purposes, {{especially with regard to}} timing and shape of the terminators. We propose an improved method to cover with these problems by making use of the possibility to introduce charge density profiles into the LWPC that vary appropriately over the day-night cycle and additionally can model disturbances caused by forcing of the lower ionosphere from above (X-rays, particle precipitations) and below (atmospheric waves). 1...|$|E
50|$|The Ontario {{amendments}} {{changed the}} test for summary judgment from asking whether the case presents “a genuine issue for trial” to asking {{whether there is a}} “genuine issue requiring a trial”. The new rule, with its enhanced fact‑finding powers, demonstrates that a trial is not the <b>default</b> <b>procedure.</b>|$|R
40|$|The {{purpose of}} this work is to assess the {{accuracy}} at low frequencies of measurements performed using the <b>default</b> <b>procedure</b> of ISO 140 - 4 (the same of ISO/DIS 16283 - 1) in test rooms with volumes greater than 25 cubic meters. About 100 previous measurements were analyzed, referring to in-situ airborne sound insulation from 50 to 5000 Hz...|$|R
40|$|Aortic valve {{replacement}} (AVR) {{has been the}} <b>default</b> <b>procedure</b> for the surgical management of aortic valve disease, with repair techniques heterogeneously and infrequently used. However, surgical aortic valve repair has evolved with improved techniques. Yet many questions remain regarding the ideal techniques and real-world applicability and effectiveness of valve repair. The AORTA Great Debate highlighted and discussed the controversies regarding the surgical management of aortic valve disease...|$|R
40|$|In recent years, we {{have seen}} an {{explosion}} of data collected from individuals, firms, or countries across short or long periods of time. This type of data gives {{us an opportunity to}} study the dynamics of change while controlling for time-invariant unobserved heterogeneity. Unfortunately, this type of heterogeneity, which is usually in the form of individual-specific fixed effects, creates problems for identification, estimation, and inference, especially if we continue to use <b>default</b> <b>procedures</b> without modification or without critical exploration. This dissertation revolves around a common theme – what practices and methods can be considered appropriate responses to the incidental parameter problem in panel data models. My approach to research is firmly rooted in the examination of empirical and theoretical practices so that we can come to an understanding of what we can and cannot do. (ECGE - Sciences économiques et de gestion) [...] UCL, 201...|$|E
40|$|Rita Franceschini has edited a {{book that}} deals with various {{elements}} of auto-/ biographical and multicultural studies. Most of the authors' arguments draw on linguistic, literary and ethnological approaches. In several chapters attempts are made to exemplify the interdependencies between biography and culture, specifically between individual and collective constructions of reality. In addition, various chapters focus on biographical structural effects resulting from speakers’ experience with foreign language challenges in bilingual or even trilingual contact situations. The review of the volume is framed by a cultural heuristic of an understanding explaining concept (Max WEBER); the reviewer's criticism is supported by qualitative standards of socio-pragmalinguistic and textlinguistic work. From {{this point of view}} evidence is provided that explains why literary studies' approaches tend to employ individual sampling procedures as well as other <b>default</b> <b>procedures</b> in gaining methodically controlled insights into properties and dimensions at issue. The linguistic contributions are discussed at length because of their efforts to connect micro- and macro-theoretical perspectives to each other, as such efforts are viewed as building blocks of a cultural studies' approach to research on (language) biographies...|$|E
40|$|Schubert's {{treatment}} of the medial caesura differs on many levels {{from that of the}} Classical tradition. He problematizes many of its norms, introducing complications to the course of his sonata movements. Much research has been devoted to Schubert's approach to sonata form, his large-scale formal deformations as well as his innovative harmonic language. However, few of these writings have discussed the importance of the medial caesura to his sonata forms. Through the lens of Sonata Theory, this dissertation examines Schubert's handling of the MC, demonstrating how the complications derived from his unorthodox practice modify the structural and rhetorical layout of his pieces. I investigate Schubert's approach to three stages surrounding the MC articulation, TR and the energy-gaining process, the MC point of articulation, and the S-theme, discussing specific formal and rhetorical complications that arise from each of them. In chapter 1, I reconsider Schubert's MC practice from a dialogical perspective, demonstrating how some non-normative procedures (in Classical terms) became the norm within his own style. In chapter 2, I examine the impact of two common Schubertian procedures on the function, perception, and meaning of the MC: tonally over-determined TRs and the early arrival of the secondary key within TR. Finally, in chapter 3, I demonstrate how Schubert broadened the available cadential arrangements within MC pairs in declined-MC situations, exploring the expressive potential of normative/non-normative dual oppositions. The conclusion shows that 1) Schubert's stylistic preferences radically expand many of the <b>default</b> <b>procedures</b> posited by Sonata Theory, inviting refinements of the theory; and 2) that the Schubertian MC may incorporate two structural roles beyond its most fundamental function as a formal articulator: clarification of the function of a formally ambiguous passage, which is often connected to cases of tonal over-determination or the early arrival of the secondary key; and introduction of tonal and formal complications into the work's trajectory, invoking some kind of "correction" or compensation...|$|E
40|$|It {{has been}} the subject of debate in the {{translation}} process literature whether human translation is a sequential and iterative process of comprehension-transfer- production or whether and to what extent comprehension and production activities may occur in parallel. Tirkkonen-Condit (2005) suggests a “monitor model” according to which translators start in a literal default rendering mode, and a monitor interrupts the <b>default</b> <b>procedure</b> when a problem occurs. This paper proposes an extension of the monitor model in which comprehension and production are processed in parallel by the <b>default</b> <b>procedure.</b> The monitor supervises text production processes, and triggers disintegration of the translation activity into chunks of sequential reading and writing behavior. To investigate this hypothesis, we compare text copying with translation activities under the assumption that text copying represents a typical literal <b>default</b> rendering <b>procedure.</b> Both, translation and text copying, require decoding, retrieval and encoding of textual segments, but translation requires in addition a transfer step into the target language. Comparing reading and writing behaviour obtained in the copying and translation experiments, we observe surprisingly many similarities, which also suggests similarities in the underlying processes. Copyists deviate from the default literal text reproduction into more effortful text understanding, and much of the translators’ behaviour looks like simple text copying. During translation as well as during text copying we observe that translators and copyists resort to sequential reading and writing patterns which seem to be triggered through target text production problems...|$|R
2500|$|The general {{procedure}} (the [...] "7/50" [...] procedure) - section 38. [...] The amendment must be {{passed by the}} House of Commons, the Senate, and at least two-thirds of the provincial legislative assemblies representing at least 50% {{of the total population}} of the provinces. This is the <b>default</b> <b>procedure</b> and it covers any amendment procedure not covered more specifically in sections 41, 43, 44 or 45. The general formula must be used for any of the six situations identified in section 42.|$|R
50|$|SCL is {{designed}} to allow both line-at-a-time interactive use from a console or from a command file, and creation of executable scripts or programs (when the language is compiled into object module format {{in the same way}} as any other VME programming language). The declaration of a procedure within SCL also acts as the definition of a simple form or template allowing the procedure to be invoked from an interactive terminal, with fields validated according to the data types of the underlying procedure parameters or using the <b>default</b> <b>procedure</b> parameter values.|$|R
40|$|With {{its recent}} {{decisions}} in Ashcroft v. Iqbal and Bell Atlantic v. Twombly, the Supreme Court may be intentionally or unintentionally “throwing the fight,” {{at least in}} the legal contests between many civil rights claimants and institutional defendants. The most obvious feared effect is reduction of civil rights claimants’ access to the expressive and coercive power of the courts. Less obviously, the Supreme Court may be effectively undermining institutions’ motivation to negotiate, mediate - or even communicate with and listen to - such claimants before they initiate legal action. Thus, the Supreme Court’s recent decisions have the potential to deprive marginalized claimants - and our society - of alternative, effective avenues for the airing and resolution of disputes with powerful institutional players. Ironically, it was just this sort of deprivation that led the Supreme Court to announce its expansive vision of notice pleading in Conley v. Gibson. Conley foretells the need for our courts to maintain a robust public forum for those who are marginalized by the <b>default</b> <b>procedures</b> of normal life - not only to provide redress to the parties directly involved in particular disputes but because the viability of such a forum has the indirect and salutary effect of forcing institutional players {{to find a way to}} sufficiently approximate the fair dialogue and resolution modeled in our courts. In an attempt to acknowledge legitimate concerns regarding the inefficiency and costs of today’s civil litigation process in some cases, while still protecting the courts’ essential role in providing a forum for marginalized parties, this Article will suggest that courts take a second look at the summary jury trial, an expedited form of trial conducted before an advisory jury and followed by negotiation or mediation between the parties and their lawyers. Relatively early and appropriate use of this process could effectively prompt resolution and dialogue - i. e., private dialogue between the parties before the process is to occur; a stylized form of public dialogue during the trial phase of the process itself; and another private dialogue, potentially with assistance from a judge or mediator, after the advisory jury has been dismissed...|$|E
40|$|The {{parties in}} this case defend two sides of a many-sided circuit split. This brief argues that a third view is correct. If a {{contract}} requires suit in a particular forum, and the plaintiff sues somewhere else, how may the defendant raise the issue? Petitioner Atlantic Marine Construction Company suggests a motion under Federal Rule of Civil Procedure 12 (b) (3) or 28 U. S. C. § 1406, {{on the theory that}} the contract renders venue improper. Respondent J-Crew Management, Inc. contends that venue remains proper, and that the defendant¹s only remedy is a transfer motion under § 1404. Both sides are wrong. Forum-selection clauses have no effect on venue, which is defined by statute. While parties can waive their venue objections in advance, they cannot destroy proper venue by private agreement. At the same time, an exclusive forum-selection clause does more than just inform a court 2 ̆ 7 s discretion under § 1404. If the clause is valid and enforceable, it waives the plaintiff 2 ̆ 7 s right to sue in an excluded forum, offering the defendant an affirmative defense to liability in that forum and the right to have the suit dismissed. The Federal Rules already specify the correct method of raising this defense: it must be affirmatively stated in the answer, which the defendant may accompany with an immediate summary judgment motion. Often, as here, the parties 2 ̆ 7 agreement will be incorporated in the complaint. In that case, the defendant may alternatively raise the defense in a pre-answer Rule 12 (b) (6) motion to dismiss, or a post-answer Rule 12 (c) motion for judgment on the pleadings. The Rules 2 ̆ 7 <b>default</b> <b>procedures</b> are practical as well as correct. They enable defendants to obtain quick and decisive enforcement of their forum-selection clauses, through the same procedures used to enforce binding prior judgments, settlements, or arbitral awards. And while there may be some practical advantages to treating forum-selection clauses as if they affected venue, these advantages have been greatly exaggerated [...] and, in any case, provide no reason to misapply the Federal Rules. Here, the parties agreed that their disputes 2 ̆ 2 shall be litigated 2 ̆ 2 in state or federal court in Norfolk, Va. J-Crew violated that agreement by suing in the Western District of Texas. Assuming, as the Court should, that the clause at issue is valid and enforceable, the complaint could have been dismissed by motion under Rule 12 (b) (6). Instead, Atlantic Marine made this forum-selection defense under the label of Rule 12 (b) (3). That may have been good enough to raise the issue, but the Court should leave such preservation questions to the court of appeals in the first instance. Because that court (and the district court) proceeded on the erroneous assumption that § 1404 was the only available remedy, this Court should identify the correct procedure, vacate the judgment, and remand the case for further proceedings...|$|E
40|$|Abstract Crossdocking is a {{logistic}} strategy used {{to improve}} the effectiveness of goods distribution by aiming to decrease inventory and transportation costs. A distribution network is an integrated set of suppliers, distribution platforms and customers where strategic, tactical and operating decisions related to a single player could produce effects on some (or many) others. The {{state of the art}} on crossdocking logistic strategy highlights a mismatch between the description of a crossdocking platform and its mathematical formulation. Unloading, loading, sorting, consolidating, storing, labelling and handling are the activities performed within a crossdocking platform which are not included in the modelling part. Generally the authors include just the storing activity. These activities could be relevant for some problems such as scheduling, layout and distribution whereas they could be considered irrelevant for some other problems which do not reach the detail level on the internal functioning of the platform such as the location problems. Among these problems, the PhD dissertation deals with the distribution flow problem which consists in determining how to send products from suppliers to customers through crossdocking platforms. The activities performed at the platforms are associated to costs and capacity constraints for the available resources. For this reason a crossdocking platform cannot be represented by a single transshipment node. In order to take into account these features, the crossdocking platform is modelled with a transshipment nodes network: a receiving node which stands for the activities performed on the incoming products, a storing node, which stands for the inventory activity and a shipping node standing for the activities performed on the outgoing products. The literature review on distribution flow problems for the crossdocking strategy underlines another mismatch: the lack of a specific constraint for the transportation efficiency. A model is proposed to fill these gaps. The main idea is that the crossdocking, with appropriate differences, can be formulated as a Fixed Charge Network Flow Problem, well-known NP-hard problem. Two exact approaches, a Branch and Bound and a Branch and Cut algorithm, have been developed in order to solve the problem. These two procedures are customized on the network features and they are different from the <b>default</b> <b>procedures</b> embedded within the most diffused optimization software: Xpress and Cplex. Some pseudo-random instances have been generated with the aim to test the developed model and procedures. For the large instances, the obtained results have been compared with those obtained by the optimization software. The obtained results demonstrate the effectiveness of the developed procedures {{as well as of the}} crossdocking strategy in terms of average level of inventory. The second part of this PhD thesis deals with an application: the case of the distribution process through container terminal. A new model is proposed, extension of the previous one, which allows to take into account the specific features associated a container terminal. Once again, the model is formulated, with appropriate differences, as a Fixed Charge Network Flow Problem. In this case the model is validated with a real instance extracted by the current functioning of the container terminal of Naples (Italy), which represents the case study. Like the previous model, the obtained results validate the crossdocking strategy for the management of the terminal yard. This strategy, in fact, allows to drastically cut the average inventory time...|$|E
30|$|The {{data show}} {{that the success of}} {{achieving}} a TLH increased with time; as cases were unselected, this increase is likely to be due to increase in surgical expertise. Over the whole study, excluding cases where other clinicians chose the surgical route (2 cases), performed the surgery (9 cases) or the patient had a VH (17 cases), 95  % (138 / 145) of all cases offered a TLH had a TLH. The results show that TLH can justifiably be offered to all patients undergoing non-vaginal hysterectomy as the <b>default</b> <b>procedure</b> providing the surgeon is appropriately experienced in the technique.|$|R
40|$|We {{study the}} {{influence}} of minimum quality standards in a two-region partial-equilibrium model of vertical product differentiation and trade. Three alternative standard setting arrangements are considered: Full Harmonization, National Treatment and Mutual Recognition. The analysis integrates {{the choice of a}} particular standard setting alternative by governments into the model. We provide a set of sufficient conditions for which Mutual Recognition emerges as one regulatory alternative that always improves welfare in both regions when compared to the case without regulation. We show that Mutual Recognition, being the <b>default</b> <b>procedure</b> if governments do not reach a unanimous decision, is the only possible equilibrium of the game. product differentiation, oligopoly, trade, quality standards, policy coordination...|$|R
40|$|Projected body {{frontal area}} is used when {{estimating}} the parasite drag of bird flight. We investigated {{the relationship between}} projected frontal area and body mass among passerine birds, and compared it with an equation based on waterfowl and raptors, which is used as <b>default</b> <b>procedure</b> in a widespread software package for flight performance calculations. The allometric equation based on waterfowl/raptors underestimates the frontal area compared to the passerine equation presented here. Consequently, revising the actual frontal areas of small birds will concomitantly change {{the values of the}} parasite drag coefficient. We suggest that the new equation S-b = 0. 0129 m(B) (0. 61) (m(2)) where m(B) is body mass (kg) should be used when a value of frontal area is needed for passerines...|$|R
30|$|In LTE, {{the network}} {{architecture}} is flat such {{that when a}} UE is handed over, to improve latency and efficiency, the handover procedure is exclusively controlled by the source and destination eNBs [25]. For intra-LTE handover, the <b>default</b> <b>procedure</b> is that the source eNB buffers the data and passes it to the destination eNB over the X 2 interface. If no X 2 connection exists between the source and destination eNBs, the handover is performed over the S 1 interface. However, from the UE's viewpoint, {{there is no difference}} between the two types of handover [25]. In the case of closed-access femto-cells, where a handover is not possible between a source H/eNB and a destination HeNB, the proposed resource partitioning procedure requires that signaling information is conveyed from the source eNB to the destination HeNB.|$|R
50|$|The Hague Convention Abolishing the Requirement for Legalisation for Foreign Public Documents has supplanted legalization as the <b>default</b> <b>procedure</b> by {{a system}} of apostille. It is {{available}} if both the origin country of the document and the destination country are party to the treaty. The apostille is a stamp on which standard validating information is supplied. It is available (dependent on the document) from the competent authority of the origin country, and often the document has to be notarized {{before it can be}} apostilled. In the United States the Secretaries of State for the various states are the competent authorities who can apply an apostille. A list of the competent authorities designated by each country that has joined the treaty is maintained by the Hague Conference on Private International Law.|$|R
40|$|The {{international}} standard ISO 140 - 5 for {{the measurement of}} the sound insulation of building facades has been recently replaced by the new standard ISO 16283 - 3. The revised standard includes the procedure for measurements at low frequencies down to 50 Hz. The uncertainty of facade sound insulation, in particular at low frequencies, was evaluated by a Round Robin Test, conducted in a full-scale experimental building at the Construction Technologies Institute of the National Research Council of Italy (ITCCNR). Each of the 10 teams involved in the RRT replicated the tests 5 times, {{for a total of}} 50 measurements. The different measurement positions inside the receiving room were compared. In particular, all the teams involved in the RRT followed the low-frequency procedure, assessing corner and center room positions; the energy average values according to ISO 16283 - 3 were considered and the relative uncertainty, in terms of repeatability and in situ reproducibility standard deviations, was compared with the ones measured and calculated following the <b>default</b> measurement <b>procedure.</b> It was found that the uncertainty of the low-frequency procedure is higher than that of the <b>default</b> <b>procedure.</b> This would suggest the need to investigate further the reliability of the low-frequency procedure. At high frequency, the significant uncertainty values found are probably caused by the loudspeakers directivity and position; this aspect need to be investigated in greater detail, as wel...|$|R
5000|$|Maravall’s {{research}} {{has centered on}} time series analysis and modeling and their application to economic series. His main contribution (an important part of it coauthored with Victor Gómez) has been {{the development of a}} model-based procedure to jointly solve several statistical time series problems that affect analysis and interpretation of economic time series. The standard (<b>default)</b> <b>procedure</b> performs, first, automatic identification and forecasting of regression-ARIMA models (that includes adjusting for outliers and calendar effects) in the possible presence of missing observations.1 Then, the model is decomposed into models for the unobserved components (such as seasonal, trend, transitory, and cyclical components) and, from these models, filters are derived to estimate and forecast the components.234 The model-based structure provides the joint distribution of  the estimators, from which parametric tests and inferences (such as the standard errors of all estimators and forecasts) are derived.5a,b ...|$|R
50|$|Hundreds of {{different}} messages are produced {{as a result}} of various events taking place in the system, and typically, an application processes {{only a small fraction of}} these messages. In order to ensure that all messages are processed, Windows provides a <b>default</b> window <b>procedure</b> called DefWindowProc that provides default processing for messages that the application itself does not process.|$|R
40|$|The most {{important}} asset of semisupervised classification methods {{is the use}} of available unlabeled data combined with a clearly smaller set of labeled examples, so as to increase the classification accuracy compared with the <b>default</b> <b>procedure</b> of supervised methods, which on the other hand use only the labeled data during the training phase. Both the absence of automated mechanisms that produce labeled data and the high cost of needed human effort for completing the procedure of labelization in several scientific domains rise the need for semisupervised methods which counterbalance this phenomenon. In this work, a self-trained Logistic Model Trees (LMT) algorithm is presented, which combines the characteristics of Logistic Trees under the scenario of poor available labeled data. We performed an in depth comparison with other well-known semisupervised classification methods on standard benchmark datasets and we finally reached {{to the point that the}} presented technique had better accuracy in most cases...|$|R
40|$|NBBC) Study, {{published}} by Behan et al, 1 {{showed that the}} provi-sional single-stent approach is superior to the systematic 2 -stent approach in terms of safety and efficacy. Surprisingly, the incidence of the combined end point of all-cause death, myocardial infarction, and target vessel revascularization was significantly greater in bifurcation lesions with a side branch diameter of 2. 75 mm and lesion length of 5 mm treated by the complex 2 -stent approach compared with the single-stent approach. However, critical review of the NBBC study discloses several flaws in methodology that render {{the results of that}} study unjustifiable. In the NBBC study, the included lesions were supposed to be true bifurcation lesions. As many as 28 % of the bifurcation lesions were really non–true bifurcation lesions. 1 There is a general agreement that the simple 1 -stent approach should be the <b>default</b> <b>procedure</b> for non–true bifurcation lesions. 2 Consequently, in more than one-fourt...|$|R
40|$|Profile hidden Markov models (HMMs) {{are used}} to model protein {{families}} and for detecting evolutionary relationships between proteins. Such a profile HMM is typically constructed from a multiple alignment {{of a set of}} related sequences. Transition probability parameters in an HMM {{are used to}} model insertions and deletions in the alignment. We show here that taking into account unrelated sequences when estimating the transition probability parameters helps to construct more discrimina-tive models for the global/local alignment mode. After normal HMM training, a simple heuristic is employed that adjusts the transition probabilities between match and delete states according to observed transitions in the training set relative to the unrelated (noise) set. The method is called adaptive transition probabilities (ATP) and is based on the HMMER package implementation. It was benchmarked in two remote homology tests based on the Pfam and the SCOP classifications. Com-pared to the HMMER <b>default</b> <b>procedure,</b> the rate of misclassification was reduced significantly in both tests and across all levels of error rate...|$|R
30|$|Total laparoscopic {{hysterectomy}} (TLH) has well-established {{advantages over}} total abdominal hysterectomy in benign gynaecology. We evaluated {{the outcome of}} a single surgeon who offered TLH as the <b>default</b> surgical <b>procedure</b> for all non-vaginal hysterectomies in an unselected gynaecology clinic population. TLH was offered as the default method of hysterectomy for patients from September 1, 2006, and data were collected up to August 31, 2011. Data were collected on indication for surgery, previous surgery, pelvic pathology, intraoperative findings, uterine weight and/or size, complications and conversion to open hysterectomy. Primary outcomes were the proportion of hysterectomies performed laparoscopically, complications and conversion rates. A total of 173 hysterectomies were performed; 18 (10  %) were total abdominal hysterectomy (TAH), 17 (10  %) were vaginal hysterectomies (VH), and 138 (80  %) were TLH. TLH rates increased from 51  % in year 1 to 100  % in years 3, 4 and 5 for women that elected for laparoscopic approach. The median uterine weight for TLH increased each year from 110  g (range 58 – 209  g) in year 1 to 240  g (range 70 – 584  g) in year 5. All patients were deemed suitable for laparoscopic approach irrespective of the uterine size and comorbidities by year 3 with only a single conversion in year 4. There were 11 major surgical complications: VH 0 (0  %), TAH 1 (5.6  %) and TLH 10 (7.2  %) and three (2.2  %) conversions to laparotomy. Once a surgeon's laparoscopic expertise plateaus, TLH can be offered to patients as the <b>default</b> <b>procedure</b> for non-vaginal hysterectomy in an unselected UK population with benign disease.|$|R
5000|$|Entering a zero ("0") for menu {{meant that}} no menu would be displayed. The S/36 [...] "command display" [...] would appear with no menu options. Entering a zero for library would {{override}} the default library {{and use the}} system library (#LIBRARY.) Entering a zero for procedure would override the <b>default</b> sign-on <b>procedure</b> and no procedure would run at sign-on. Mandatory menus cannot be overridden or respecified in libraries other than the named library.|$|R
40|$|A {{software}} tool is presented which calculates the spatial properties azimuth, length, spacing, {{and frequency of}} lineaments that are defined by their starting and ending co‐ordinates in a two‐dimensional (2 ‐D) planar co‐ordinate system. A simple graphical interface with five display windows creates a user friendly interactive environment. All lineaments are considered in the calculations, and no secondary sampling grid is needed for the elaboration of the spatial properties. Several rule‐based decisions are made to determine the nearest lineament in the spacing calculation. As a <b>default</b> <b>procedure,</b> the programme defines a window {{that depends on the}} mode value of the length distribution of the lineaments in a study area. This makes the results more consistent, compared to the manual method of spacing calculation. Histograms are provided to illustrate and elaborate the distribution of the azimuth, length and spacing. The core of the tool is the spacing calculation between neighboring parallel lineaments, which gives direct information about the variation of block sizes in a given category of structures. The 2 ‐D lineament frequency is calculated for the actual area that is occupied by the lineaments...|$|R
40|$|Exoplanet {{searches}} with dedicated instrumentation {{have made}} 1 m/s radial velocity (RV) precision routine. Yet, RVs for large samples of stars generally {{remain at the}} 1 km/s level. The Immersion Grating Infrared Spectrometer (IGRINS) is a revolutionary instrument that exploits broad spectral coverage at high-resolution in the near-infrared. IGRINS on the 2. 7 meter Harlan J. Smith Telescope at McDonald Observatory is nearly as sensitive as CRIRES at the 8 meter Very Large Telescope. However, IGRINS at R= 45, 000 has more than 30 times the spectral grasp of CRIRES. The use of a silicon immersion grating facilitates a compact cryostat while providing simultaneous wavelength coverage from 1. 45 - 2. 45 microns. We have developed a pipeline to cross-correlate the more than 20, 000 resolution elements in two IGRINS exposures and provide relative RVs with uncertainties of ~ 50 m/s (< 1 % of a resolution element). Absolute RVs are limited by the zero point uncertainty, which is ~ 150 m/s. IGRINS RVs will be provided for thousands of objects per year as a <b>default</b> <b>procedure</b> of the data reduction pipeline, creating a legacy product for multi-epoch studies of low-mass, stellar and substellar multiplicity...|$|R
