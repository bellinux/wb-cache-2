87|6|Public
5000|$|... #Subtitle level 2: <b>Hot</b> <b>standby</b> {{operation}} in railway signalling ...|$|E
5000|$|<b>Hot</b> <b>Standby</b> Routing Protocol - A Cisco Systems {{proprietary}} router redundancy protocol ...|$|E
5000|$|BM Start-Up: {{mainstream}} major generation units {{maintained in}} either an energy readiness or <b>hot</b> <b>standby</b> state.|$|E
30|$|Failovers are {{participants}} {{which serve}} as <b>hot</b> <b>standbys</b> should the leader fail or elect to stand down.|$|R
50|$|To {{provide better}} control over {{transaction}} processing, {{significant improvements in}} fault tolerance, and richer support for networking, CCI developed PERPOS, a Unix derivative that provided integrated support for real-time transaction processing, load balancing, and fault tolerant features such as <b>hot</b> and cold <b>standby.</b>|$|R
50|$|IT Service Continuity {{planning}} {{may make}} use of any combination of recovery and/or restoration strategies including; <b>hot,</b> warm, cold <b>standby</b> data centres or servers; high-availability services within the same or across multiple data centres; services may be active/active or active/passive; it may utilise a ship-on-demand, shared services or cloud services, or other approach. IT Service Continuity, in itself, does not prescribe or refer to any one strategy.|$|R
5000|$|... 1996 Polyhedra 2.0: added <b>hot</b> <b>standby</b> {{configurations}} {{for use in}} applications needing high availability. First port to an RTOS (pSOS) ...|$|E
50|$|A hot spare or warm spare or <b>hot</b> <b>standby</b> {{is used as}} a {{failover}} {{mechanism to}} provide reliability in system configurations. The hot spare is active and connected as part of a working system. When a key component fails, the hot spare is switched into operation. More generally, a <b>hot</b> <b>standby</b> can be used to refer to any device or system that is held in readiness to overcome an otherwise significant start-up delay.|$|E
50|$|VRRP {{is based}} on Cisco's {{proprietary}} <b>Hot</b> <b>Standby</b> Router Protocol (HSRP) concepts. The protocols, while similar in concept, are not compatible.|$|E
40|$|High {{demands on}} {{advanced}} safety and driving functions, such as active safety and lane departure warnings, increase a vehicle 2 ̆ 7 s dependency on automotive electrical/electronic architectures. Hard real-time requirements and high reliability constraints must be satisfied for the correct functioning of these safety-critical features, {{which can be}} achieved by using the AUTOSAR (Automotive Open System Architecture) standard. The AUTOSAR standard was introduced to simplify automotive system design while offering inter-operability, scalability, extensibility, and flexibility. The current version of AUTOSAR does not assist in the replication of tasks for recovering from task failures. Instead, the standard assumes that architecture designers will introduce custom extensions to meet such reliability needs. The introduction of affordable techniques with predictable properties for meeting reliability requirements will prove to be very valuable in future versions of AUTOSAR. In this paper, we propose a new Software-Component (SW-C) allocation algorithm called R-FLOW (Reliable application-FLOW-aware SW-C partitioning algorithm) for fail-stop processors to support fault-tolerance with bounded recovery times, and we integrate the R-FLOW algorithm into AUTOSAR. R-FLOW leverages different types of replication schemes to satisfy reliability and timing constraints, while offering a high degree of resource utilization and flexibility. Specifically, R-FLOW classifies real-time periodic tasks into Hard Recovery tasks, Soft Recovery tasks, andBest-Effort Recovery tasks. <b>Hot</b> <b>Standbys</b> are used for recovering from failures of hard recovery tasks, whereas Cold Standbys are utilized for recovering from failures of soft recovery and best-effort recovery tasks. With this goal in mind, we design and implement our proposed architecture within the guidelines of the current AUTOSAR framework. We have built an at-scale prototyping platform, comprising of Freescale HCS 12 X processing boards, a dual-channel FlexRay bus, and a CAN network. Our proposed architecture is evaluated on this platform using reliability and timeliness metrics in the context of different fault scenarios...|$|R
25|$|A dock {{can allow}} some laptop {{computers}} {{to become a}} substitute for a desktop computer, without sacrificing the mobile computing functionality of the machine. Portable computers can dock and undock <b>hot,</b> cold or <b>standby,</b> depending on {{the capabilities of the}} system. In a cold dock or undock, one completely shuts the computer down before docking/undocking. In a hot dock or undock, the computer remains running when docked/undocked. Standby docking or undocking, an intermediate style used in some designs, allows the computer to be docked/undocked while powered on, but requires that it be placed into a sleep mode prior to docking/undocking.|$|R
50|$|Circuit {{restoration}} {{is usually}} performed {{in accordance with}} planned procedures and priorities. Restoration may be effected automatically, such as by switching to a <b>hot</b> <b>standby,</b> or manually, such as by manual patching.|$|E
50|$|Electrical {{generators}} may {{be held on}} <b>hot</b> <b>standby,</b> or a {{steam train}} may {{be held at the}} shed fired up (literally hot) ready to replace a possible failure of an engine in service.|$|E
50|$|In {{computer}} networking, the <b>Hot</b> <b>Standby</b> Router Protocol (HSRP) is a Cisco proprietary redundancy {{protocol for}} establishing a fault-tolerant default gateway. Version 1 of the protocol {{was described in}} RFC 2281. There is no RFC for version 2 of the protocol.|$|E
5000|$|... 2008 Polyhedra 8.0: Polyhedra Flash DBMS now {{supports}} <b>hot</b> <b>standby</b> configurations {{for use in}} applications needing high availability, in {{a similar}} way to Polyhedra IMDB. Polyhedra 8.1 added Linux/MIPS support, the ability to monitor active queries, and enhancements to the historian.|$|E
50|$|<b>Hot</b> <b>standby</b> {{may have}} a {{slightly}} different connotation of being active but not productive to hot spare, that is it is a state rather than object. For example, in a national power grid, the supply of power needs to be balanced to demand over a short term. It can take many hours to bring a coal-fired power station up to productive temperatures. To allow for load balancing, generator turbines may be kept running with the generators switched off so as peaks of demand occur, the generators can rapidly be switched on to balance the load. Being {{in the state of}} being ready to run is known as <b>hot</b> <b>standby.</b> Though it is not a modern phenomenon, steam train operators might hold a spare steam engine at a terminus fired up, as starting an engine cold would take a significant amount of time.|$|E
50|$|<b>Hot</b> <b>standby</b> nets are in {{constant}} operation 24/7/365 for International Emergency and Disaster Relief communications. The Ham Radio Global ALE High Frequency Network, which began service in June 2007, is the world's largest intentionally open ALE network for internet connectivity. It {{is a free}} open network staffed by volunteers, and utilized by amateur radio operators supporting disaster relief organizations.|$|E
5000|$|Organizations mirror storage {{resources}} from one data center {{to a remote}} data center, which {{can serve as a}} <b>hot</b> <b>standby</b> {{in the event of a}} prolonged outage. In particular, iSCSI SANs allow entire disk arrays to be migrated across a WAN with minimal configuration changes, in effect making storage [...] "routable" [...] in the same manner as network traffic.|$|E
5000|$|The spare may {{be similar}} {{component}} or system, {{or it may}} be a system of reduced performance, designed to cope {{for the duration of the}} time to repair and recover the original component. In high availability systems, it is common to design so that not only is there a spare that can quickly be switched in, but also that the failed component can be repaired or replaced without stopping the system - this is known as hot swapping. It may be considered that the probability of a second failure is low, and therefore the system is designed simply to allow operation to continue until a suitable maintenance period. The appropriate solution is normally determined by balancing the costs of implementing the availability against the likelihood of a problem and the severity of that problem. there are two types of hot standby:1. <b>hot</b> <b>standby</b> master - slave2. <b>hot</b> <b>standby</b> in shearing mode ...|$|E
50|$|The {{concept of}} hot spares {{is not limited}} to hardware, but also {{software}} systems can be held in a state of readiness, for example a database server may have a software copy on <b>hot</b> <b>standby,</b> possibly even on the same machine to cope with the various factors that make a database unreliable, such as the impact of disc failure, poorly written queries or database software errors.|$|E
50|$|SecureSafe stores customers’ data {{in three}} data centers using triple {{redundancy}} mirroring. The first data center {{is dedicated to}} production, the second is a <b>hot</b> <b>standby</b> and the third acts as the so-called disaster recovery center. The first two data centers {{are located in the}} greater area of Zürich at the company InterXion. The third center is located in a former military bunker in the mountains of central Switzerland.|$|E
5000|$|Configurations {{can also}} be defined with active, <b>hot</b> <b>standby,</b> and cold standby (or idle) subsystems, {{extending}} the traditional “active+standby” nomenclature to “active+standby+idle” (e.g. 5+1+1). Typically, “cold standby” or “idle” subsystems are active for lower priority work. Sometimes these systems are located far away from their redundant pair in a strategy called geographic redundancy. [...] This architecture seeks to avoid loss of service from physically-local events (fire, flood, earthquake) by separating redundant machines.|$|E
50|$|Using {{full-duplex}} Ethernet physical layers, the EtherCAT slave controllers close an open port automatically {{and return}} the Ethernet frame if no downstream device is detected. Slave devices may have one, two, or more ports. Due to these features EtherCAT enables a multitude of network topologies, including line, tree, ring, star, or any combination thereof. The protocol also enables a multitude of communication features such as cable redundancy, Hot Connect of segments, change of devices during operation, or even master redundancy with <b>Hot</b> <b>Standby.</b>|$|E
50|$|OTN nodes are {{interconnected}} using pluggable {{optical fibers}} in a dual counterrotating ring topology. The primary ring consists of fibers carrying data from node to node in one direction, the secondary ring runs {{parallel with the}} primary ring but carries data in the opposite direction. Under normal circumstances, only one ring carries active data. If a failure is detected in this data path, the secondary ring is activated. This <b>hot</b> <b>standby</b> topology results in a 1 + 1 path redundancy. The switchover mechanism is hardware based and results in ultrafast (50ms) switchover without service loss.|$|E
50|$|There {{are several}} {{scenarios}} {{for what happens}} when a disk fails. In a hot swap system, {{in the event of a}} disk failure, the system itself typically diagnoses a disk failure and signals a failure. Sophisticated systems may automatically activate a <b>hot</b> <b>standby</b> disk and use the remaining active disk to copy live data onto this disk. Alternatively, a new disk is installed and the data is copied to it. In less sophisticated systems, the system is operated on the remaining disk until a spare disk can be installed.|$|E
5000|$|The CPU {{contains}} two identical 16-bit processors running in <b>hot</b> <b>standby</b> mode. The original CPU core {{was referred to}} as the NT40 CPU and was implemented in approximately 250 discrete logic devices across several circuit boards running at 36 MHz. The NT40 core consisted mainly of the NT1X44 stack card, which provides some register and stack functions of the processor, the NT1X45 which contained the arithmetic and logic functions, the NT1X46 which provides more registers and the load-route read-only memory (ROM) and the NT1X47 timing and control card which provides the micro-cycle source and microstore decoding functions of the processor. The NT1X47 card also contained the 2-digit hexadecimal display to indicate test result codes and the condition of the core. The NT1X48 processor maintenance card contained a thumbwheel on the faceplate to enable various diagnostic tests of the CPU. A later modification of these same five circuit boards with faster pin-compatible discrete logic devices enabled the CPU to operate at 40 MHz allowing central offices to improve call throughput capacity by 10 percent. [...] When the CPU is configured in dual <b>hot</b> <b>standby</b> mode, a mate exchange bus (MEB) between the two CPUs enables the state of one CPU to be continuously compared to that of the other CPU on a cycle by cycle basis. Any discrepancy between the two CPUs results in maintenance circuitry determining which CPU is at fault and activity to change to the sane CPU.|$|E
50|$|In {{the late}} 1990s the Internet Engineering Task Force (IETF) {{began work on}} a {{protocol}} for router redundancy. In 1997, Cisco informed the IETF that it had patents in this area and, in 1998, pointed out its patent on HSRP (<b>Hot</b> <b>Standby</b> Router Protocol). Nonetheless, IETF continued work on VRRP (Virtual Router Redundancy Protocol). After some debate, the IETF VRRP working group decided to approve the standard, despite its reliance on patented techniques, as long as Cisco made the patent available to third parties under RAND (Reasonable and Non-Discriminatory) licensing terms. Because VRRP fixed problems with the HSRP protocol, Cisco began using VRRP instead, while still claiming it as its own.|$|E
5000|$|... 1AESS used memory with 26-bit words, {{of which}} two were for parity checking. The initial version had 32 Kilowords of core mats. Later {{versions}} used semiconductor memory. Program stores were arranged to feed two words (52 bits) {{at a time}} to the CPU via the Program Store Bus, while Call Stores only gave one word at a time via the Call Store Bus. 1A Program Stores were writable and not fully duplicated but were backed up by the File Stores. They were provided in multiplicity of N+2, i.e. as many as were needed for the size of the office, plus two <b>hot</b> <b>standby</b> units to be loaded from disk as needed.|$|E
50|$|The Moscow and St. Petersburg TCI nodes are {{connected}} by dual communication channels operating two different fiber optic systems, {{as well as}} with a fully independent access to the Internet and connection to the Moscow and St. Petersburg IXes. The system’s support is ensured by the proper service of systems administrators, {{as well as by the}} service of network monitoring operating in 24x7 mode. Each node’s equipment is duplicated. Only one node is operable at a time, and the second one is in <b>hot</b> <b>standby.</b> In the event the first node fails the second one steps in. In the event of maintenance works at one of the nodes, functioning proceeds as stated before.|$|E
50|$|WUPW's high definition-ready 8VSB digital {{transmitter}} {{was first}} turned on for broadcasting in 2003. The transmitter is a liquid-cooled solid state system and quickly {{became known as}} a very reliable signal as other stations in the Toledo area struggled with the Inductive output tube type of transmitter during the birth of digital television in northwest Ohio. The WUPW transmitter {{has a great deal}} of redundancy: sixteen amplifiers and <b>hot</b> <b>standby</b> back-up digital exciters. It also has its own on-board computer system to watch over critical systems. The WUPW transmitter site has an emergency diesel generator to keep the station on-the-air even if a total loss of electrical power were to occur in the city.|$|E
50|$|While {{operating}} at reduced power {{in anticipation of}} typhoon Talim, on , the Unit 2 main transformer differential protective relay tripped because a damaged support insulator caused one phase to flash to ground. The main transformer trip was followed by main turbine trip and then reactor scram. Following the subsequent repair work, on , as the unit was in a <b>hot</b> <b>standby</b> condition, a water hammer occurred {{in one of the}} main steam lines. It was determined that condensed water accumulated while the unit was in warm standby with the drain valve closed. When that steam line started to supply steam to the turbine, the slug of water was pushed downstream, causing a water hammer and subsequent low steam pressure, resulting in another reactor trip.|$|E
50|$|Burn-up {{compensation}} is achieved automatically by the LEMs, achieving 80% of the nominal {{power at the}} end-of-life of the fuel cartridge. The LEM is a thermometer-like device actuated by the volume expansion of the Li6. This “liquid control rod” can keep the reactor power almost constant throughout the design life.Partial load operation is possible by adjusting the primary coolant flow rate. Reactor power will be in proportional with the primary coolant flow rate due to LEMs reactivity feedback. The LRM is composed of an envelope divided by a frozen seal into two chambers. The lower chamber, within the active core, has a 95% enriched Li6 reservoir, while the upper chamber is a vacuum prior to reactor startup.Reactor startup can be done automatically if the primary coolant temperature reaches its standby temperature. Coolant heating {{can be achieved by}} heat release from the primary pump circulation. Then the frozen seal of the LRM will melt at the <b>hot</b> <b>standby</b> temperature (approximately 780 °C), and Li6 is released slowly from lower level (active core level) to the upper level to achieve a positive reactivity increase.It will take 7(11) hours to complete startup. The LIMs assure sufficient negative reactivity feedback in unprotected transients. The LRMs enable an automated reactor startup by detecting the <b>hot</b> <b>standby</b> temperature of the primary coolant. All these systems use Li6 and are actuated by highly reliable physical properties (volume expansion of Li6 for LEM, and frozen seal melting for LIM and LRM). A configuration with Quick LEMs requires 3+(1) LEMs of smaller size than a configuration with Slow LEMs requiring 24 LEMs.The RAPID-L is equipped with 28 LEMs, 16 LIMs and 16 LRMs in the design concept. Two of the 16 LRM are reserves or dummies. It is a very redundant system. Failure of some of these devices would result in only a slight temperature deviation of the coolant. In case that most of the LEMs fail, the burn-up compensation by the LEMs could be impossible, and the reactor would shut down.|$|E
50|$|In its {{original}} role for hauling local trains, class E 41 proved both reliable and efficient, especially with push-pull trains. Less successful was the usage with S-Bahn trains, as class E 41 was not {{equipped with an}} electric brake, which would have helped to reduce abrasion. Class E 41 service stayed largely unchanged until the early 1990s. Since then many units {{have been replaced by}} former Deutsche Reichsbahn units of class 143 especially in S-Bahn service. Furthermore, since mid-1990s EMUs and newer locomotives as class 146 replaced even more class 141 units. Since then, many have been scrapped. Unit 141 188 was the first to retire on October 31, 1987; the total number of engines has hence fallen since. The last four remaining units, which finally were held ready <b>hot</b> <b>standby</b> around Frankfurt and used in case of need for defective younger machines were dropped out of service in December 2006.|$|E
40|$|This paper {{deals with}} a {{stochastic}} model for a two-unit <b>hot</b> <b>standby</b> combined hardware-software system in which one unit is operative {{and the other is}} <b>hot</b> <b>standby.</b> The operative unit may have hardware or software failures and goes to repair. This leads to degradation of the system and then <b>hot</b> <b>standby</b> unit is under operation. Further on hardware or software failures of the <b>hot</b> <b>standby</b> unit the system goes to complete failures. Various measures of performance of the system are obtained using semi-Markov process and regenerative point techniques. Numerical results and graphs pertaining to a particular case are also included...|$|E
40|$|Abstract — The present paper {{deals with}} the study of a {{database}} system having Primary database and <b>hot</b> <b>standby</b> database unit which {{is provided by the}} system provider itself. There is an agreement with the system provider that on the failure of the <b>hot</b> <b>standby</b> unit, another similar unit is immediately provided by him. The primary unit is a production unit and synchronized with <b>hot</b> <b>standby</b> unit through online transfer of archive redo logs. Data being saved in the primary unit gets simultaneously stored in the <b>hot</b> <b>standby</b> unit. When the primary database unit fails, the <b>hot</b> <b>standby</b> database unit becomes the production database and primary database unit goes under repair. The system is analyzed by making use of semi-Markov processes and regenerative point technique. Expression for Mea...|$|E
40|$|The present paper {{deals with}} the study of a {{database}} system having Primary database and <b>hot</b> <b>standby</b> database unit which {{is provided by the}} system provider itself. There is an agreement with the system provider that on the failure of the <b>hot</b> <b>standby</b> unit, another similar unit is immediately provided by him. The primary unit is a production unit and synchronized with <b>hot</b> <b>standby</b> unit through online transfer of archive redo logs. Data being saved in the primary unit gets simultaneously stored in the <b>hot</b> <b>standby</b> unit. When the primary database unit fails, the <b>hot</b> <b>standby</b> database unit becomes the production database and primary database unit goes under repair. The system is analyzed by making use of semi-Markov processes and regenerative point technique. Expression for Mean Time to System Failure, Mean Time to Failure of Primary Database Unit and Availability of Primary Unit are obtained. Graphical study has also been done...|$|E
