2|8|Public
30|$|A final, {{alternative}} configuration {{known as}} the Distributed Nagios Executor (DNX) introduces the concept of worker nodes [36]. In this configuration, a master Nagios server dispatches the service checks from its own schedule {{to a series of}} worker nodes. The master maintains all configuration, workers only require the IP address of the master. Worker nodes can join and leave in an ad hoc manner without disrupting monitoring services. This is beneficial for cloud monitoring; allowing an <b>elastic</b> <b>pool</b> of workers to scale in proportion to the monitored servers. If, however, the master fails all monitoring will cease. Thus, for anything other than the most trivial deployments additional failover mechanisms are necessary.|$|E
40|$|Titin (also {{known as}} connectin) {{is a giant}} {{filamentous}} protein whose elastic properties greatly contribute to the passive force in muscle. In the sarcomere, the elastic I-band segment of titin may interact with the thin filaments, possibly affecting the molecule's elastic behavior. Indeed, several {{studies have indicated that}} interactions between titin and actin occur in vitro and may occur in the sarcomere as well. To explore the properties of titin alone, one must first eliminate the modulating effect of the thin filaments by selectively removing them. In the present work, thin filaments were selectively removed from the cardiac myocyte by using a gelsolin fragment. Partial extraction left behind approximately 100 -nm-long thin filaments protruding from the Z-line, whereas the rest of the I-band became devoid of thin filaments, exposing titin. By applying a much more extensive gelsolin treatment, we also removed the remaining short thin filaments near the Z-line. After extraction, the extensibility of titin was studied by using immunoelectron microscopy, and the passive force-sarcomere length relation was determined by using mechanical techniques. Titin's regional extensibility was not detectably affected by partial thin-filament extraction. Passive force, on the other hand, was reduced at sarcomere lengths longer than approximately 2. 1 microm, with a 33 +/- 9 % reduction at 2. 6 microm. After a complete extraction, the slack sarcomere length was reduced to approximately 1. 7 microm. The segment of titin near the Z-line, which is otherwise inextensible, collapsed toward the Z-line in sarcomeres shorter than approximately 2. 0 microm, but it was extended in sarcomeres longer than approximately 2. 3 microm. Passive force became elevated at sarcomere lengths between approximately 1. 7 and approximately 2. 1 microm, but was reduced at sarcomere lengths of > 2. 3 microm. These changes can be accounted for by modeling titin as two wormlike chains in series, one of which increases its contour length by recruitment of the titin segment near the Z-line into the <b>elastic</b> <b>pool...</b>|$|E
50|$|The {{resources}} available for Standalone databases {{are expressed in}} terms of Database Transaction Units (DTUs) and for <b>elastic</b> <b>pools</b> in terms of elastic DTUs or eDTUs. A DTU is defined as a blended measure of CPU, memory, and data I/O and transaction log I/O in a ratio determined by an OLTP benchmark workload designed to be typical of real-world OLTP workloads.|$|R
50|$|Azure SQL Database {{is offered}} {{either as a}} Standalone {{database}} or <b>Elastic</b> database <b>pool,</b> and is priced in three tiers: Basic, Standard and Premium. Each tier offers different performance levels to accommodate a variety of workloads.|$|R
40|$|Abstract—Cloud {{computing}} {{presents a}} new model for IT service delivery and it typically involves over-a-network, on-demand, self-service access, which is dynamically scalable and <b>elastic,</b> utilising <b>pools</b> of often virtualized resources. Through these features, cloud computing {{has the potential to}} improve the way businesses and IT operate by offering fast start-up, flexibility, scalability and cost efficiency. Even though cloud computing provides compelling benefits and cost-effective options for IT hosting and expansion, new risks and opportunities for security exploits are introduced. Standards, policies and controls are therefore of the essence to assist management in protecting and safeguarding systems and data. Management should understand and analyse cloud computing risks in order to protect systems and data from security exploits. The focus of this paper is on mitigation for cloud computing security risks as a fundamental step towards ensuring secure cloud computing environments...|$|R
40|$|Enabling data- and compute-intensive {{applications}} that require realtime in-the-field {{data collection and}} processing using mobile platforms is still a significant challenge due to i) the insufficient computing capabilities and unavailability of complete data on individual mobile devices and ii) the prohibitive communication cost and response time involved in offloading data to remote computing resources such as clouds for centralized computation. A novel resource provisioning framework is proposed for organizing the heterogeneous sensing, computing, and communication capabilities of static and mobile devices in the vicinity in order to form an <b>elastic</b> resource <b>pool</b> (a heterogeneous mobile computing grid) that can be harnessed to collectively process massive amounts of locally generated data in parallel. The proposed framework is imparted with autonomic capabilities, namely, self-optimization and self-organization, {{in order to be}} energy and uncertainty aware, respectively, in the dynamic mobile environment...|$|R
40|$|Abstract—Interference Alignment and Cancelation (IAC) aims at {{significantly}} {{improving the}} wireless channel capacity. Existing algorithms for IAC are computationally intensive, which {{may lead to}} long execution times. A practical implementation of IAC is infeasible for fast-varying channels (when the coherence time is small, e. g., less than 0. 5 s). This is because {{a significant amount of}} time has to be spent on channel estimation as IAC techniques are extremely sensitive to the degree of accuracy of channel estimates, thus leaving a very small portion of time for actual data transmission. The collective computational capabilities of nodes in the neighborhood can be exploited (for parallelism) to facilitate the practical realization of compute-intensive IAC techniques. A novel resource provisioning framework, which organizes the mobile devices in the neighborhood to form an <b>elastic</b> resource <b>pool</b> – a heterogeneous mobile computing grid – is presented. This framework enables distributed execution of compute-intensive communication algorithms like IAC. The effectiveness of the approach is studied under different operational scenarios. Index Terms—Interference alignment, wireless channel estimation, distributed algorithms, mobile grid computing. I...|$|R
40|$|Cloud {{computing}} is {{an evolving}} paradigm that is radically {{changing the way}} humans store, share and access their digital files. Despite the many benefits, such as {{the introduction of a}} rapid <b>elastic</b> resource <b>pool,</b> and on-demand service, the paradigm also creates challenges for both users and providers. In particular, there are issues related to security and privacy, such as unauthorised access, loss of privacy, data replication and regulatory violation that require adequate attention. Nevertheless, and despite the recent research interest in developing software engineering techniques to support systems based on the cloud, the literature fails to provide a systematic and structured approach that enables software engineers to identify security and privacy requirements and select a suitable cloud service provider based on such requirements. This paper presents a novel framework that fills this gap. Our framework incorporates a modelling language and it provides a structured process that supports elicitation of security and privacy requirements and the selection of a cloud provider based on the satisfiability of the service provider to the relevant security and privacy requirements. To illustrate our work, we present results from a real case study...|$|R
40|$|Abstract — Cloud {{computing}} {{has certainly}} created a buzz {{around the world}} {{of it is the}} next big thing after internet in the field of information technology; some say it’s a metaphor for internet. It is a computing technology based on internet, Cloud computing presents a new model for IT service delivery and it typically involves over-a-network, on-demand, self-service access, which is dynamically scalable and <b>elastic,</b> utilizing <b>pools</b> of often virtualized resources. Even though the cloud continues to grow in popularity, Usability and respectability, Questions about cloud data protection, data privacy and other Security issues may continue to linger and are cited as the most substantial roadblock for cloud computing uptake. Privacy and security are the key issue for cloud storage. As promising as it is, this paradigm also brings into limelight many new challenges for data security and access control when users outsource sensitive data for sharing on cloud servers, which are not within the same trusted domain as data owners. To keep sensitive user data confidential against untrusted servers, we suggest applying cryptographic methods by disclosing data decryption keys only to authorized users. The paper analyzes the feasibility of the applying RSA encryption algorithm for data security and privacy in cloud Storage...|$|R
40|$|The {{simulations}} {{and potential}} forecasting of dust storms are of significant interest {{to public health}} and environment sciences. Dust storms have interannual variabilities and are typical disruptive events. The computing platform for a dust storm forecasting operational system should support a disruptive fashion by scaling up to enable high-resolution forecasting and massive public access when dust storms come and scaling down when no dust storm events occur to save energy and costs. With the capability of providing a large, <b>elastic,</b> and virtualized <b>pool</b> of computational resources, cloud computing becomes a new and advantageous computing paradigm to resolve scientific problems traditionally requiring a large-scale and high-performance cluster. This paper examines the viability for cloud computing to support dust storm forecasting. Through a holistic study by systematically comparing cloud computing using Amazon EC 2 to traditional high performance computing (HPC) cluster, we find that cloud computing is emerging as a credible solution for (1) supporting dust storm forecasting in spinning off {{a large group of}} computing resources in a few minute...|$|R

