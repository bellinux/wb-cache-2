0|10000|Public
40|$|A {{technique}} {{is indicated by}} which approximations to the <b>factor</b> loadings <b>of</b> a new test may be obtained if <b>factor</b> loadings <b>of</b> a given group of tests and the correlations of the new test with the other tests are known. The {{technique is}} applicable to any orthogonal system and is especially adapted to cases in which Σ a ji a jk = 0 when i ≠ k. Application is also made to the simultaneous <b>determination</b> <b>of</b> the <b>factor</b> <b>weights</b> <b>of</b> a group of tests in which no additional common factor is present. The technique is useful in adding tests to a completed factorial solution and in using factorial solutions involving errors to give results which are approximately correct...|$|R
40|$|The {{sustainable}} development {{of community and}} environment {{can be understood as}} a multi criteria optimization problem. All sub criteria are not of the same importance all the time. In this contribution an approach is introduced, which makes the situation dependent <b>determination</b> <b>of</b> the <b>weighting</b> <b>factors</b> <b>of</b> the sub criteria possible. For two practice examples solutions for a sustainable management of complex hydrologic and water supply systems are presente...|$|R
40|$|We {{propose a}} new method for the <b>determination</b> <b>of</b> the <b>weight</b> <b>factor</b> for the {{simulated}} tempering method. In this method a short replica-exchange simulation is performed and the simulated tempering <b>weight</b> <b>factor</b> is {{obtained by the}} multiple-histogram reweighting techniques. The new algorithm is particularly useful for studying frustrated systems with rough energy landscape where the <b>determination</b> <b>of</b> the simulated tempering <b>weight</b> <b>factor</b> by the usual iterative process becomes very difficult. The effectiveness of the method is illustrated by taking an example for protein folding. Comment: 8 pages, (ReVTeX), 5 figures, Chem. Phys. Lett., submitte...|$|R
40|$|A new {{approach}} is presented {{that uses a}} weighted least squares fit to analyze wind tunnel strain-gage balance calibration data. The weighted least squares fit is specifically designed to increase the influence of single-component loadings during the regression analysis. The weighted least squares fit also reduces the impact of calibration load schedule asymmetries on the predicted primary sensitivities of the balance gages. A <b>weighting</b> <b>factor</b> between zero and one is assigned to each calibration data point that depends on a simple count of its intentionally loaded load components or gages. The greater the number of a data point's intentionally loaded load components or gages is, the smaller its <b>weighting</b> <b>factor</b> becomes. The proposed approach is applicable to both the Iterative and Non-Iterative Methods that are used {{for the analysis of}} strain-gage balance calibration data in the aerospace testing community. The Iterative Method uses a reasonable estimate of the tare corrected load set as input for the <b>determination</b> <b>of</b> the <b>weighting</b> <b>factors.</b> The Non-Iterative Method, on the other hand, uses gage output differences relative to the natural zeros as input for the <b>determination</b> <b>of</b> the <b>weighting</b> <b>factors.</b> Machine calibration data of a six-component force balance is used to illustrate benefits of the proposed weighted least squares fit. In addition, a detailed derivation of the PRESS residuals associated with a weighted least squares fit is given in the appendices of the paper as this information could not be found in the literature. These PRESS residuals may be needed to evaluate the predictive capabilities of the final regression models that result from a weighted least squares fit of the balance calibration data...|$|R
30|$|Many {{decision-making}} problems {{involve a}} number <b>of</b> <b>factors</b> and subfactors. For difficult decisions, a quantitative approach is recommended. In this paper both {{qualitative and quantitative}} approaches are used. All <b>of</b> the important <b>factors</b> can then be given appropriate weights. AHP process uses pair-wise comparisons and then computes the <b>weighting</b> <b>factors</b> and evaluation. This process was developed by Satty (1980) and published in his book The Analytic Hierarchy Process. The decision maker starts by laying out the overall hierarchy of the decision. This hierarchy reveals the factors to be considered {{as well as the}} various alternatives in the decision, in this paper only the objectives are considered to prioritize the <b>factors.</b> A number <b>of</b> pair-wise comparisons are done, which result in the <b>determination</b> <b>of</b> <b>factor</b> and subfactor <b>weights</b> and <b>factor</b> evaluations. The AHP is a structured method to elicit preference opinion from decision makers. Its methodological procedure can easily be incorporated into multiple objective programming formulations with interactive solution process. If number <b>of</b> <b>factors</b> are less then, an excel sheet can be used to find out the priority.|$|R
40|$|This paper {{describes}} {{a method for}} transforming measured optical and infrared filter data for use with optical systems of arbitrary f-number and angle of incidence. Although it is generally desirable to have normal incidence at the filter (i. e., collimated light where an optical filter is used), other system design considerations may take precedence. In {{the case of a}} multispectral sensor under development at Sandia National Laboratories, system constraints require optical filter placement very near the focal plane. The light rays incident on the filters are therefore converging as determined by the system f-number while the chief ray of each ray bundle varies with focal plane position. To analyze the system`s spectral response at different points on the focal plane, a method was devised to transform the filter vendor`s measured data to account for the optical system design. The key to the transformation is the <b>determination</b> <b>of</b> <b>weighting</b> <b>factors</b> and shift factors for each angle of incidence making up a ray bundle. A computer worksheet was developed using a popular mathematical software package which performs this transformation for 75 key points on the focal plane...|$|R
40|$|International audienceThe {{sources of}} {{evidence}} {{may have different}} reliability and importance in real applications for decision making. The estimation <b>of</b> the discounting (<b>weighting)</b> <b>factors</b> when the prior knowledge is unknown have been regularly studied until recently. In the past, the <b>determination</b> <b>of</b> the <b>weighting</b> <b>factors</b> focused only on reliability discounting rule and it was mainly dependent on the dissimilarity measure between basic belief assignments (bba's) represented by an evidential distance. Nevertheless, {{it is very difficult}} to characterize efficiently the dissimilarity only through an evidential distance. Thus, both a distance and a conflict coefficient based on probabilistic transformations BetP are proposed to characterize the dissimilarity. The distance represents the difference between bba's, whereas the conflict coefficient reveals the divergence degree of the hypotheses that two belief functions strongly support. These two aspects of dissimilarity are complementary in a certain sense, and their fusion is used as the dissimilarity measure. Then, a new estimation method <b>of</b> <b>weighting</b> <b>factors</b> is presented by using the proposed dissimilarity measure. In the evaluation of weight of a source, both its dissimilarity with other sources and their <b>weighting</b> <b>factors</b> are considered. The <b>weighting</b> <b>factors</b> can be applied in the both importance and reliability discounting rules, but the selection of the adapted discounting rule should depend on the actual application. Simple numerical examples are given to illustrate the interest of the proposed approach...|$|R
40|$|In {{statistical}} physics, {{the efficiency}} of tempering approaches strongly depends on ingredients such {{as the number of}} replicas R, reliable <b>determination</b> <b>of</b> <b>weight</b> <b>factors</b> and the set of used temperatures, T_R = {T_ 1, T_ 2, [...] ., T_R}. For the simulated tempering (SP) in particular [...] useful due to its generality and conceptual simplicity [...] the latter aspect (closely related to the actual R) may be a key issue in problems displaying metastability and trapping in certain regions of the phase space. To determine T_R's leading to accurate thermodynamics estimates and still trying to minimize the simulation computational time, here it is considered a fixed exchange frequency scheme for the ST. From the temperature of interest T_ 1, successive T's are chosen so that the exchange frequency between any adjacent pair T_r and T_r+ 1 has a same value f. By varying the f's and analyzing the T_R's through relatively inexpensive tests (e. g., time decay toward the steady regime), an optimal situation in which the simulations visit much faster and more uniformly the relevant portions of the phase space is determined. As illustrations, the proposal is applied to three lattice models, BEG, Bell-Lavis, and Potts, in the hard case of extreme first-order phase transitions, always giving very good results, even for R= 3. Also, comparisons with other protocols (constant entropy and arithmetic progression) to choose the set T_R are undertaken. The fixed exchange frequency method is found to be consistently superior, specially for small R's. Finally, distinct instances where the prescription could be helpful (in second-order transitions and for the parallel tempering approach) are briefly discussed. Comment: 10 pages, 14 figure...|$|R
30|$|The {{eleventh}} {{generation of}} the International Geomagnetic Reference Field (IGRF) was agreed in December 2009 by a task force appointed by the International Association of Geomagnetism and Aeronomy (IAGA) Division V Working Group V-MOD. New spherical harmonic main field models for epochs 2005.0 (DGRF- 2005) and 2010.0 (IGRF- 2010), and predictive linear secular variation for the interval 2010.0 – 2015.0 (SV- 2010 - 2015) were derived from weighted averages of candidate models submitted by teams led by DTU Space, Denmark (team A); NOAA/NGDC, U.S.A. (team B); BGS, U.K. (team C); IZMIRAN, Russia (team D); EOST, France (team E); IPGP, France (team F); GFZ, Germany (team G) and NASA-GSFC, U.S.A. (team H). Here, we report the evaluations of candidate models {{carried out by the}} IGRF- 11 task force during October/November 2009 and describe the weightings used to derive the new IGRF- 11 model. The evaluations include calculations of root mean square vector field differences between the candidates, comparisons of the power spectra, and degree correlations between the candidates and a mean model. Coefficient by coefficient analysis including <b>determination</b> <b>of</b> <b>weighting</b> <b>factors</b> used in a robust estimation of mean coefficients is also reported. Maps of differences in the vertical field intensity at Earth’s surface between the candidates and weighted mean models are presented. Candidates with anomalous aspects are identified and efforts made to pinpoint both troublesome coefficients and geographical regions where large variations between candidates originate. A retrospective analysis of IGRF- 10 main field candidates for epoch 2005.0 and predictive secular variation candidates for 2005.0 – 2010.0 using the new IGRF- 11 models as a reference is also reported. The high quality and consistency of main field models derived using vector satellite data is demonstrated; based on internal consistency DGRF- 2005 has a formal root mean square vector field error over Earth’s surface of 1.0 nT. Difficulties nevertheless remain in accurately forecasting field evolution only five years into the future.|$|R
40|$|The {{eleventh}} {{generation of}} the International Geomagnetic Reference Field (IGRF) was agreed in December 2009 by a task force appointed by the International Association of Geomagnetism and Aeronomy (IAGA) Division VWorking Group V-MOD. New spherical harmonic main field models for epochs 2005. 0 (DGRF- 2005) and 2010. 0 (IGRF- 2010), and predictive linear secular variation for the interval 2010. 0 – 2015. 0 (SV- 2010 - 2015) were derived from weighted averages of candidate models submitted by teams led by DTU Space, Denmark (team A); NOAA/NGDC, U. S. A. (team B); BGS, U. K. (team C); IZMIRAN, Russia (team D); EOST, France (team E); IPGP, France (team F); GFZ, Germany (team G) and NASA-GSFC, U. S. A. (team H). Here, we report the evaluations of candidate models {{carried out by the}} IGRF- 11 task force during October/November 2009 and describe the weightings used to derive the new IGRF- 11 model. The evaluations include calculations of root mean square vector field differences between the candidates, comparisons of the power spectra, and degree correlations between the candidates and a mean model. Coefficient by coefficient analysis including <b>determination</b> <b>of</b> <b>weighting</b> <b>factors</b> used in a robust estimation of mean coefficients is also reported. Maps of differences in the vertical field intensity at Earth’s surface between the candidates and weighted mean models are presented. Candidates with anomalous aspects are identified and efforts made to pinpoint both troublesome coefficients and geographical regions where large variations between candidates originate. A retrospective analysis of IGRF- 10 main field candidates for epoch 2005. 0 and predictive secular variation candidates for 2005. 0 – 2010. 0 using the new IGRF- 11 models as a reference is also reported. The high quality and consistency of main field models derived using vector satellite data is demonstrated; based on internal consistency DGRF- 2005 has a formal root mean square vector field error over Earth’s surface of 1. 0 nT. Difficulties nevertheless remain in accurately forecasting field evolution only five years into the future...|$|R
40|$|The {{research}} {{focuses on}} providing reliable spatial information in support of tsunami risk and vulnerability assessment {{within the framework of}} the German-Indonesian Tsunami Early Warning System (GITEWS) project. It contributes to three major components of the project: (1) the provision of spatial information on surface roughness as an important parameter for tsunami inundation modeling and hazard assessment; (2) the modeling of population distribution, which is an essential factor in tsunami vulnerability assessment and local disaster management activities; and (3) the settlement detection and classification from remote sensing radar imagery to support the population distribution research. Regarding the surface roughness determination, research analyses on surface roughness classes and their coefficients have been conducted. This included the development of remote sensing classification techniques to derive surface roughness classes, and integration of the thus derived spatial information on surface roughness conditions to tsunami inundation modeling. This research determined 12 classes of surface roughness and their respective coefficients based on analyses of published values. The developed method for surface roughness classification of remote sensing data considered density and neighborhood conditions, and resulted in more than 90 % accuracy. The classification method consists of two steps: main land use classification and density and neighborhood analysis. First, the main land uses were defined and a classification was performed applying decision tree modeling. Texture parameters played an important role in increasing the classification accuracy. The density and neighborhood analysis further substantiated the classification result towards identifying surface roughness classes. Different classes such as residential areas and trees were combined to new surface roughness classes, as “residential areas with trees”. The density and neighborhood analysis led to an appropriate representation of real surface roughness conditions. This was used as an important input for tsunami inundation modeling. By using Tohoku University’s Analysis Model for Investigation Near-field Tsunami Number 3 (TUNAMI N 3), the spatially distributed surface roughness information was integrated in tsunami inundation modeling and compared to the modeling results applying a uniform surface roughness condition. An uncertainty analysis of tsunami inundation modeling based on the variation of surface roughness coefficients in the Cilacap study area was also undertaken. It was demonstrated that the inundation modeling results applying uniform and spatially distributed surface roughness resulted in high differences of inundation lengths, especially in areas far from the coastline. This result showed the important role of surface roughness conditions in resisting tsunami flow, which must be considered in tsunami inundation modeling. With respect to the second research focus, the population distribution, a concept of population distribution modeling was developed. Within the modeling process, <b>weighting</b> <b>factor</b> determination, multi-scale disaggregation and a comparative study to other methods were conducted. The basis of the developed method was a combination of census and land use data, which led to an improved spatial resolution and accuracy of the population distribution. Socio-economic data were used to derive <b>weighting</b> <b>factors</b> to distributing people to land use classes. Moreover, in case of missing input data, an approach was developed that allows for the <b>determination</b> <b>of</b> generalized <b>weighting</b> <b>factors.</b> The approach to use specific weightings, where possible and generalized ones, where necessary, led to a flexible methodology with respect to the achievable accuracy and availability of data. A comparative study was performed by comparing this new model with previously developed population distribution models. The newly developed model showed a higher accuracy. The detailed population distribution information was a valuable input for the vulnerability assessment being the main data source for human exposure assessment and an important contribution to evacuation time modeling. In support of the population distribution research, settlement classification using TerraSAR-X imagery was conducted. A current classification method of speckle divergence analysis on SAR imagery was further developed and improved by including the neighborhood concept. The settlement classification provided highly accurate results in dense urban areas, whereas the method needs to be further developed and improved for rural settlement areas. Finally, it has been shown how the results of this research can be applied. These applications cover the integration of surface roughness conditions into the tsunami inundation modeling and hazard mapping. The contributions to tsunami vulnerability assessment and evacuation planning were shown. Additionally, the results were integrated into the decision support system of the Tsunami Early Warning Center in Jakarta...|$|R
30|$|Based on the {{background}} literature, the <b>determination</b> <b>of</b> <b>factors</b> {{that contributes to}} adoption of IMVs in a population is imperative {{for the implementation of}} policy control measures as well as to improve livelihoods through sustainable increased productivity of maize.|$|R
40|$|AbstractDetermination <b>of</b> <b>factor</b> <b>weights</b> for {{landslide}} susceptibility mapping problem {{should be}} performed by some intelligent approaches instead of personal choices when a large number <b>of</b> <b>factors</b> are available. In this study, the quality <b>of</b> <b>factors</b> and their effects on the production of landslide susceptibility maps were assessed using Chi-square and Fisher <b>weighting</b> methods. Process <b>of</b> <b>factor</b> <b>weight</b> <b>determination</b> was automatized employing feature weighting algorithm with user-based Analytical Hierarchy Process (AHP) approach. In order to produce the most accurate and precise susceptibility maps, factors were integrated into the GIS environment using the factor-weighted overlay method. In this study, Arakli district of Trabzon Province, Turkey is considered as the study area. The primary focus {{in this study is}} to determine the <b>weights</b> <b>of</b> landslide causative <b>factors</b> using Chi-square and Fisher algorithms. On the other hand, AHP method was used as a benchmark method to compare and validate the performances <b>of</b> the landslide <b>factor</b> <b>weights.</b> All weighted <b>factor</b> sets were tested on factor-weighted overlay method for producing landslide susceptibility maps. The quality of susceptibility maps was assessed using overall accuracy measure and success rate curve analysis. Results showed that the weights determined by Chi-square and Fisher methods outperformed the conventional AHP method by about 6 %...|$|R
40|$|We {{prove that}} the problem <b>of</b> <b>determination</b> <b>of</b> <b>factor</b> gauge groups given the rank of the gauge group at any given vacuum in the Landscape is in the {{computational}} complexity class NPC. This extends a result of Denef and Douglas on the computational complexity <b>of</b> <b>determination</b> <b>of</b> {{the value of the}} cosmological constant in the Landscape. Comment: 6 pages, uses package palatin...|$|R
50|$|Matricization may {{be applied}} in {{connection}} with <b>determination</b> <b>of</b> the <b>factors</b> in the PARAFAC model.|$|R
3000|$|... = 1. To {{facilitate}} {{discussion of}} the impact <b>of</b> the <b>weight</b> <b>factor</b> on the overall transmission power and leased time, five sets <b>of</b> <b>weight</b> <b>factor</b> are supposed as following, {ω [...]...|$|R
30|$|Our {{results are}} {{comparable}} to recent published series. This observational study has several limitations but most importantly is heterogeneity in the sample; {{there is a large}} number of differing variables in patient, injury characteristics and treatment pathways that make <b>determination</b> <b>of</b> <b>factors</b> that influence healing and clinical outcome difficult. There is, in addition, a lack of a control group.|$|R
40|$|The {{purpose of}} this bachelor´s thesis is <b>determination</b> <b>of</b> <b>factors</b> {{affecting}} the price construction work. The determination factors are reached by using survey construction companies which will indicate and evaluate the factors. In {{the end of this}} bachelor´s thesis are budgets of buildings which provided building companies and budget created to compare price (are compared from standard items) ...|$|R
40|$|Provisions for {{controlling}} <b>factor</b> <b>weights</b> constitute a significant {{extension of the}} data envelopment analysis (DEA) methodology, as an effective tool for measuring efficiency. This paper suggests a conceptual framework for the treatment <b>of</b> <b>factor</b> <b>weights</b> in DEA. First, the paper proposes general guidelines for setting bounds on <b>factor</b> <b>weights.</b> Then, it develops and presents alternative methods to limit the range within which these <b>factor</b> <b>weights</b> are allowed to vary. All of these methods involve additional information which is entered into the analysis {{in the form of}} constraints, bounds or different objective functions. Finally, the implications of the various approaches is discussed. DEA efficiency multidimensional scaling...|$|R
40|$|This article {{presents}} {{the use of}} system analysis methodology and Graph Theory for validation of the relationships among descriptive factors, which impact the selection of oil materials for wood finishing. Matrix {{analysis was used to}} determine the results of the pair wise comparisons <b>of</b> <b>factor</b> <b>weights</b> and optimization <b>of</b> the <b>factor</b> values. Modelling theories have been developed, which allow for construction of models of primary impacts <b>of</b> the <b>factors</b> on oil materials selection in protective and decorative wood coatings creatio...|$|R
40|$|A {{prototype}} GIS-based {{procedure for}} setting priority sites for upland development {{based on a}} hierarchical integration of ecological, social and economic criteria and involvement of stakeholders in the selection process was developed for the Okoy-Banica watershed area in the Philippines. The basic steps involve the preparation of digitised data sets, derivation and classification of ecological, social and economic <b>factor</b> maps, elicitation <b>of</b> <b>factor</b> <b>weights</b> among stakeholders <b>of</b> the study area and encoding the <b>factor</b> <b>weights</b> into the <b>factor</b> maps, and overlaying and computing the factor maps to generate the decision/priority maps for upland development {{in accordance with the}} study model. The key aspect of the study is the investigation of the impact <b>of</b> <b>factor</b> <b>weights</b> being assigned by different stakeholder groups as a departure from the usual practice where experts assign the <b>factor</b> <b>weights.</b> <b>Factor</b> <b>weights</b> produce the numerical scale of judgement in order to quantify and use mathematical operations to produce objective decisions using GIS. In a GIS-assisted DSS, <b>factor</b> <b>weights</b> link the human dimension of decisions with the physical site attributes. Results of the study reveal, that the statistical test on priority rating of selection criteria by different stakeholder groups yielded a generally significantly different responses and that the resulting <b>factor</b> <b>weights</b> when encoded and overlaid in GIS produced variations on the extent and location of selected priority sites. The study shows the potential use of the consensus or multi-sectoral approach to <b>factor</b> <b>weight</b> assignment as a key towards generating multi-sectoral involvement in ensuring success of upland development projects. It also illustrates the important role the <b>factor</b> <b>weights</b> play in a GIS-assisted decision making and in the integration of people's participation towards ensuring sustainable development consideration in the planning and allocation process of upland development projects...|$|R
40|$|A new unified {{formulation}} of two dimensional limit equilibrium slope stability analysis method is developed in this paper. Based {{on the present}} formulation, Bishop's simplified Method, Janbu's simplified and rigorous methods, Lowe and Karafiath's method, Corps of Engineer's Method, Load transfer factor method, Blocking equilibrium Method, Spencer's Method and Morgenstern-Price's method and Leshchinsky's method can all be derived as a special case of the more general formulation. The authors have also investigated into several interesting examples in the <b>determination</b> <b>of</b> <b>factor</b> <b>of</b> safety. To solve for the <b>factor</b> <b>of</b> safety under difficult problem, Gauss-Newton method with a line search step selection strategy is proposed. Department of Civil and Environmental Engineerin...|$|R
40|$|We {{determine}} the composition <b>factors</b> <b>of</b> the polynomial representation of DAHA, conjectured by M. Kasatani. We reduce the <b>determination</b> <b>of</b> composition <b>factors</b> <b>of</b> polynomial representations of DAHA to the <b>determination</b> <b>of</b> the composition <b>factors</b> <b>of</b> the Weyl module W^(1 ^n) for the v-Schur algebra. By using the LLT-Ariki type theorem of v-Schur algebra proved by Varagnolo-Vasserot, we {{determine the}} composition <b>factors</b> <b>of</b> W^(1 ^n) by calculating the upper global basis and crystal basis of Fock space of U_q(ŝl̂_ℓ). Comment: 34 page...|$|R
30|$|The {{accurate}} {{prediction of}} JT coefficient, the accurate <b>determination</b> <b>of</b> gas compressibility <b>factor</b> (Z) <b>of</b> desired gas mixture and the variation <b>of</b> Z <b>factor</b> with temperatures {{at a constant}} pressure play a crucial role. In the light of available field and laboratory data plus whether the gas mixture compositions are known or unknown, different approaches such as equation of states (EOSs), empirical Z factor correlations {{can be used for}} the <b>determination</b> <b>of</b> gas compressibility <b>factor</b> (Z) and its variations due to change in temperature and pressure conditions which are required for the <b>determination</b> <b>of</b> JT coefficient. For instance, when the gas mixture compositions are known, any of the equation of states (EOSs) such as van der Waals (vdW), Soave–Redlich–Kwong (SRK), Peng–Robinson (PR) can be used for the <b>determination</b> <b>of</b> Z <b>factor</b> and its variations. When the compositions of gas mixture are unknown, the empirical Z factor correlations such as Beggs and Brill (1973), Bahadori et al. (2007) correlation, Heidaryan et al. (2010) correlation, Hall and Yarborough (1973) correlation and Dranchuk and Abou-Kassem (1975) are widely used as routine industry practice for the <b>determination</b> <b>of</b> Z <b>factor.</b>|$|R
40|$|This thesis {{presents}} a web application {{as a result}} of procedure for computerization of standards. The application allows for <b>determination</b> <b>of</b> behaviour <b>factor</b> q and response spectrum. The aim of the work was to develop a solution that will facilitate the work of engineers, and reduce a time-consuming procedure for the <b>determination</b> <b>of</b> the behaviour <b>factor</b> q and design response spectrum. The user interface is designed as a single web page with dynamic diagrams and runs completely in a web browser. The thesis describes all parameters that affect the <b>determination</b> <b>of</b> the behaviour <b>factor</b> q and the design response spectrum as defined by the European standard SIST EN 1998. We also describe decision support system and its application for the computerization of standards. According to the specifics of parameters affecting the <b>determination</b> <b>of</b> the behaviour <b>factor</b> q and design response spectrum, and facts that affect the implementation in a computer system, the most appropriate system is chosen. The web application is scalable, extendable, and internationalized for localization...|$|R
40|$|Ascorbic acid is an {{essential}} micronutrient in the diet of teleost fish, including salmonids, which do not have gulonolactone oxidase activity (Moreau and Dabrowski, 2001). The ascorbic acid requirement for different teleost fish {{has been well documented}} (Dabrowski, 2001). This requirement may also vary in fish ontogeny, for instance larval metamorphosis or gonad maturation (Ciereszko and Dabrowski, 1995). <b>Determination</b> <b>of</b> <b>factors</b> that affect egg and larval quality remains difficult as good criteria are lacking (Kjorsvik e...|$|R
40|$|To {{determine}} to {{what extent}} environmental factors and anthropogenic disturbances dictate N dynamics in tropical forest soils, changes of concentrations of inorganic N in soil were investigated {{during a period of}} extreme climatic conditions caused by El Niño and La Niña. This allowed the <b>determination</b> <b>of</b> <b>factors</b> driving the N-dynamics in tropical soils more clearly than during normal seasonal cycles. Three N-limited pine forests in Central Java, Indonesia, were studied monthly for over a year. N-N...|$|R
40|$|In {{this work}} we have {{proposed}} a method for automatic indexing and retrieval. This method will provide {{as a result the}} most likelihood document which is related to the input query. The technique used in this project is known as singular-value decomposition, in this method a large term by document matrix is analyzed and decomposed into 100 factors. Documents are represented by 100 item vector <b>of</b> <b>factor</b> <b>weights.</b> On the other hand queries are represented as pseudo-document vectors, which are formed from weighed combinations of terms...|$|R
30|$|The results {{represent}} {{the potential of}} GIS-based evaluation for urban planning purpose. However, {{it needs to be}} emphasized that the reliability of the assessment results depends on a multitude <b>of</b> <b>factors</b> ranging from the quality of the database to the potential errors in the GIS. Meanwhile, the modeling results are highly significant to the weights applied. The <b>determination</b> <b>of</b> a multitude <b>of</b> <b>factors</b> and <b>weights</b> for the various <b>factors</b> is one <b>of</b> the most important challenges in the future.|$|R
40|$|A purpose <b>of</b> work is <b>determination</b> <b>of</b> <b>factors,</b> {{providing}} realization <b>of</b> functional {{possibilities of}} skilled sportsmen in {{the conditions of}} the strained physical work on different distances. A 51 sportsman (age of 19 - 24) and 12 sportsmen (age 16 - 18 years) took part in researches. Exposed, that the degree of realization of aerobic potential of sportsmen depended on the level <b>of</b> trained. The <b>factors</b> <b>of</b> functional preparedness of sportsmen are selected. The features of their influence are set on realization of functional possibilities of sportsmen in the conditions of competitions...|$|R
40|$|Relaxing the {{assumption}} <b>of</b> internationally identical <b>factor</b> intensity techniques in the HOV model creates two challenges. First, computing actual <b>factor</b> intensity techniques <b>of</b> different countries requires detailed input-output tables and factor usage data, {{which are not}} always available. Second, determinants <b>of</b> the <b>factor</b> intensity technique differences across countries need to be identified. This paper explores the role <b>of</b> relative <b>factor</b> price differences in the <b>determination</b> <b>of</b> <b>factor</b> intensity technique differences across countries and proposes an inferring method that infers <b>factor</b> intensity techniques <b>of</b> different countries based on relative factor price differences. The HOV model is then modified accordingly. ...|$|R
2500|$|... the {{correlation}} between a docking score and the experimental response or <b>determination</b> <b>of</b> the enrichment <b>factor</b> (EF); ...|$|R
30|$|We have {{provided}} a review of an area of particular interest in conservation studies, namely the <b>determination</b> <b>of</b> museum <b>factors</b> affecting the aging process of organic materials. Our review {{has been based on}} the following pillars.|$|R
40|$|This paper {{undertakes}} {{a critical}} review of existing spillover analyses and proposes a unique analytical framework for examining technological spillovers in a manufacturing industry setting. The proposed framework overlaps three different literature strands; cluster and network dynamics, technological innovations; and spillover literature. It enables <b>determination</b> <b>of</b> {{the extent to which}} multinational presence in a host country stimulates spillover occurrence to local firms as well as their nature. Using this framework, the kind and the channels through which spillovers occur most can be equally determined – this is particularly relevant for policy intervention in a technically backward country. Lastly, it allows <b>determination</b> <b>of</b> <b>factors</b> and conditions under whic...|$|R
30|$|<b>Determination</b> <b>of</b> {{the control}} <b>factors,</b> noise factors and quality or {{performance}} measure {{responses of the}} product or process.|$|R
40|$|For the <b>determination</b> <b>of</b> {{accurate}} {{quantity of}} impurities in the samples authentic impurity standards or response factors {{at a given}} wavelength must be known. In the presented work a convenient method for <b>determination</b> <b>of</b> relative response <b>factors</b> <b>of</b> impurities has been described without using an authentic impurities standard. An approach for the <b>determination</b> <b>of</b> response <b>factors</b> <b>of</b> the impurities where impurity standard is physically not available was developed and verified using different approaches. One such method was developed and verified by RP-HPLC using Cosmosil C 18 MS-II column at UV 238 nm. Two different approaches were employed and the verification of correctness of approach was done using a known related substance <b>of</b> known response <b>factor...</b>|$|R
