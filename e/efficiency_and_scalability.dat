954|10000|Public
25|$|Efficiency: The {{source code}} and {{software}} architecture attributes are {{the elements that}} ensure high performance once the application is in run-time mode. Efficiency {{is especially important for}} applications in high execution speed environments such as algorithmic or transactional processing where performance and scalability are paramount. An analysis of source code <b>efficiency</b> <b>and</b> <b>scalability</b> provides a clear picture of the latent business risks and the harm they can cause to customer satisfaction due to response-time degradation.|$|E
5000|$|Automatic {{optimization}} {{based on}} data and cluster characteristics to ensure both <b>efficiency</b> <b>and</b> <b>scalability.</b>|$|E
5000|$|Trickle Up {{integrates}} {{learning objectives}} into all programs {{in order to}} increase the effectiveness, <b>efficiency,</b> <b>and</b> <b>scalability</b> of anti-poverty programs. These evaluations are designed to build on the existing evidence base about the Graduation Approach, including: ...|$|E
5000|$|Design of data-intensive {{computing}} platforms {{to provide}} {{high levels of}} reliability, <b>efficiency,</b> availability, <b>and</b> <b>scalability.</b>|$|R
30|$|Because of {{its high}} <b>efficiency,</b> flexibility, <b>and</b> <b>scalability,</b> graph {{databases}} {{have been widely used}} in various fields, such as social network, recommendation system, power system, and so on.|$|R
30|$|Qualitative {{retrieval}} {{results from}} the two YouTube datasets are shown in Fig.  10. Although the dataset is large and noisy, we can successfully retrieve geometrically relevant videos. This underlines the <b>efficiency,</b> effectiveness, <b>and</b> <b>scalability</b> of our proposed representation for large-scale video retrieval tasks.|$|R
50|$|Given that MPI has now {{emerged as}} the de facto {{standard}} on computer clusters, {{the increase in the}} number of cluster nodes has resulted in continued research to improve the <b>efficiency</b> <b>and</b> <b>scalability</b> of MPI libraries. These efforts have included research to reduce the memory footprint of MPI libraries.|$|E
50|$|Argonne National Laboratory {{pioneered the}} {{development}} of pyrochemical processing, or pyroprocessing, a high-temperature method of recycling reactor waste into fuel. Today, Argonne National Laboratory researchers are developing and refining several pyroprocessing technologies for both light water and fast reactors, working to improve the technologies’ commercial viability by increasing their process <b>efficiency</b> <b>and</b> <b>scalability.</b>|$|E
50|$|Linear network coding is a {{mathematical}} technique {{which may be}} used to improve a network's throughput, <b>efficiency</b> <b>and</b> <b>scalability,</b> as well as resilience to attacks and eavesdropping. Instead of simply relaying the packets of information they receive, the nodes of a network take several packets and combine them together for transmission. This {{may be used to}} attain the maximum possible information flow in a network.|$|E
40|$|Authentication is {{the most}} {{important}} component to protect information system from unauthorized access. Because mobile devices have resource limitations, current existing authentication methods experience security, <b>efficiency,</b> flexibility <b>and</b> <b>scalability</b> problems in wireless network services. Although many access control methods utilize both individuals and groups while validating authorization, there has been up to date no authentication mechanism supporting both group and individual. To overcome the existing problems, an authentication model for large scale wireless network is proposed in this paper. It provides secure, efficient, flexible and scalable authentication for wireless network users and services. To exhibit the security <b>and</b> <b>efficiency</b> characteristics, a realization of the authentication model using dynamic key cryptography and group key management for individual and group of users and services is also proposed. Its analysis demonstrates the advantages in security, <b>efficiency,</b> flexibility <b>and</b> <b>scalability</b> of both individual and group authentication to existing authentication methods...|$|R
30|$|In this work, {{we study}} the speedup, the <b>efficiency,</b> <b>and</b> the <b>scalability</b> of a {{parallel}} algorithm for {{the calculation of}} correntropy. Also, {{a study of the}} sequential fraction of the algorithm is performed in order to provide insights on how much time is spent with communication, synchronization, and load balancing.|$|R
30|$|From {{the above}} {{security}} analysis and latency evaluation results, the overhead {{in terms of}} the number of policies for preserving the publish/subscribe system is easy to observe, but the overhead is reasonable and acceptable. The overall latency comparison shows that our access control framework has higher policy matching <b>efficiency</b> <b>and</b> higher <b>scalability.</b>|$|R
5000|$|Efficiency: The {{source code}} and {{software}} architecture attributes are {{the elements that}} ensure high performance once the application is in run-time mode. Efficiency {{is especially important for}} applications in high execution speed environments such as algorithmic or transactional processing where performance and scalability are paramount. An analysis of source code <b>efficiency</b> <b>and</b> <b>scalability</b> provides a clear picture of the latent business risks and the harm they can cause to customer satisfaction due to response-time degradation.|$|E
5000|$|CGI is a {{protocol}} for interfacing external applications to web servers. CGI applications run in separate processes, which are created {{at the start}} of each request and torn down at the end. This [...] "one new process per request" [...] model makes CGI programs very simple to implement, but limits <b>efficiency</b> <b>and</b> <b>scalability.</b> At high loads, the operating system process creation and destruction overhead becomes significant. In addition, the CGI process model limits resource reuse techniques (such as reusing database connections, in-memory caching, etc.).|$|E
50|$|To {{address the}} {{problems}} of bottlenecks, gnutella developers implemented a tiered system of ultrapeers and leaves. Instead of all nodes being considered equal, nodes entering into the network were kept at the 'edge' of the network as a leaf, not responsible for any routing, and nodes which were capable of routing messages were promoted to ultrapeers, which would accept leaf connections and route searches and network maintenance messages. This allowed searches to propagate further through the network, and allowed for numerous alterations in the topology which have improved the <b>efficiency</b> <b>and</b> <b>scalability</b> greatly.|$|E
40|$|Mesh optical {{resilience}} solution {{has many}} requirements, {{some of which}} are conflicting with others. This article highlights the issues relevant to mesh optical resilience and the challenges of meeting the myriad requirements. A series of Hamiltonian cycle based solutions with different <b>efficiency,</b> complexity, <b>and</b> <b>scalability</b> tradeoffs are introduced. Solutions for moderate-sized and large networks are differentiated with the former emphasizing simplicity <b>and</b> <b>efficiency</b> <b>and</b> the latter stressing resilience from multiple simultaneous failures and isolation of failure events...|$|R
40|$|This paper {{presents}} an evolutionary algorithm approach {{to solve the}} allocation, binding and scheduling problems of VLSI in system level. From a behavioral description using specification graph, the genetic algorithm generates a cheapest and fastest heterogeneous hardware or software architecture. The evaluation of the proposed approach is performed on a video decoder. The experimental results show the <b>efficiency,</b> accuracy <b>and</b> <b>scalability</b> of our approach...|$|R
40|$|Laser {{heat engine}} concepts, {{proposed}} for satellite applications, are analyzed {{to determine which}} engine concept best meets the requirements of high efficiency (50 percent or better), continuous operation in space using near-term technology. The analysis of laser heat engines includes the thermodynamic cycles, engine design, laser power sources, collector/concentrator optics, receiving windows, absorbers, working fluids, electricity generation, and heat rejection. Specific engine concepts, optimized according to thermal efficiency, are rated by their technological availability and scaling to higher powers. A near-term experimental demonstration of the laser heat engine concept appears feasible utilizing an Otto cycle powered by CO 2 laser radiation coupled into the engine through a diamond window. Higher cycle temperatures, higher <b>efficiencies,</b> <b>and</b> <b>scalability</b> to larger sizes appear to be achievable from a laser heat engine design based on the Brayton cycle and powered by a CO laser...|$|R
5000|$|OpenLB is an {{object-oriented}} {{implementation of}} the Lattice Boltzmann methods (LBM). It is the first implementation of a generic platform for LBM programming, which is shared with the open source community (GPLv2).The code is written in C++ and is used by application programmers as well as developers, {{with the ability to}} implement custom modelsOpenLB supports complex data structures that allow simulations in complex geometries and parallel execution using MPI and OpenMP on high-performance computers.The source code uses the concepts of interfaces and Templates, so that efficient, direct and intuitive implementations of the LBM become possible.The <b>efficiency</b> <b>and</b> <b>scalability</b> has been checked and proved by code reviews.A user manual and a source code documentation by DoxyGen are available on the project page.|$|E
30|$|An {{appropriate}} set {{of performance}} evaluation metrics are presented, {{related to the}} feasibility/ operational <b>efficiency</b> <b>and</b> <b>scalability</b> of the proposed framework.|$|E
40|$|<b>Efficiency</b> <b>and</b> <b>scalability</b> are {{fundamental}} issues concerning data mining in large databases. Although classification {{has been studied}} extensively, few of the known methods take serious consideration of efficient induction in large databases and the analysis of data at multiple abstraction levels. This paper addresses the <b>efficiency</b> <b>and</b> <b>scalability</b> issues by proposing a data classification method which integrates attribute-oriented induction, relevance analysis, and the induction of decision trees. Such an integration leads to efficient, high-quality, multiple-level classification of large amounts of data, the relaxation of the requirement of perfect training sets, and the elegant handling of continuous and noisy data. 1. Introduction Computational <b>efficiency</b> <b>and</b> <b>scalability</b> are two important and challenging issues in data mining. Data mining is the automated discovery of non-trivial, previouslyunknown, and potentially useful patterns embedded in databases [9]. The increasing computeri [...] ...|$|E
40|$|This paper {{describes}} a novel and competitive complete conformant planner. Key to the enhanced performance is an efficient encoding of belief states as {{disjunctive normal form}} formulae and an efficient procedure for computing the successor belief state. We provide experimental comparative evaluation on a large pool of benchmarks. The novel design provides great <b>efficiency</b> <b>and</b> enhanced <b>scalability,</b> along with the intuitive structure of disjunctive normal form representations...|$|R
40|$|We {{present a}} novel {{approach}} for the analysis and design of selfsupporting simplicial masonry structures. A finite-dimensional formulation of their compressive stress field is derived, offering a new interpretation of thrust networks through numerical homogenization theory. We further leverage geometric properties of the resulting force diagram to identify a set of reduced coordinates characterizing the equilibrium of simplicial masonry. We finally derive computational form-finding tools that improve over previous work in <b>efficiency,</b> accuracy, <b>and</b> <b>scalability...</b>|$|R
40|$|This work {{proposes a}} {{recently}} approach called the optimization of Intrusion Detection System in ATM Network using some evolutionary algorithm; Bacterial Foraging algorithm. However, the existing Intrusion detection systems {{have a number}} of barriers in system <b>efficiency,</b> flexibility <b>and</b> <b>scalability.</b> An Evolution algorithm approach/s can be used for network intrusion detection purpose for constructing a real-time packet-filtering module. Important focus of our work will be on the development of fast pattern-matching algorithms (for packet filter) that are insensitive to the number of patterns...|$|R
30|$|In this paper, {{we propose}} a {{strategy}} to calculate the correntropy metric {{in the form of}} a parallel algorithm for multi-core architectures. We provide an analysis of its parallel <b>efficiency</b> <b>and</b> <b>scalability.</b> The simulation results were obtained on a shared memory system with 24 processing cores for input images of different dimensions. The results show that correntropy has a large potential as a metric for image analysis in the multi-core era due to its high parallel <b>efficiency</b> <b>and</b> <b>scalability.</b>|$|E
40|$|<b>Efficiency</b> <b>and</b> <b>scalability</b> {{have always}} been {{important}} concerns {{in the field of}} data mining, and are even more so in the multi-relational context, which is inherently more complex. The issue has been receiving an increasing amount of attention during the last few years, and quite a number of theoretical results, algorithms and implementations have been presented that explicitly aim at improving the <b>efficiency</b> <b>and</b> <b>scalability</b> of multi-relational data mining approaches. With this article we attempt to present a structured overview...|$|E
30|$|We {{conduct an}} {{additional}} experiment {{about the effect}} of the top-k set size on experimental results to evaluate the <b>efficiency</b> <b>and</b> <b>scalability</b> of the proposed method.|$|E
40|$|We {{develop a}} {{parallel}} computational algorithm for simulating models of gel dynamics where the gel {{is described by}} two phases, a networked polymer and a fluid solvent. The models consist of transport equations for the two phases, two coupled momentum equations, and a volumeaveraged incompressibility constraint. Multigrid with Vanka-type box relaxation scheme is used as preconditioner for the Krylov subspace solver (GMRES) to solve the momentum and incompressibility equations. Through numerical experiments of a model problem, the <b>efficiency,</b> robustness <b>and</b> <b>scalability</b> of the algorithm are illustrated...|$|R
40|$|Abstract – We {{present a}} Grid based {{application}} which works {{in collaboration with}} natural language processing (NLP), {{to act as a}} virtual assistant. The application can answer queries in a conversational manner and is capable of being deployed in various scenarios. Given the prevalence of large data sources in natural language engineering and the need for raw computational power in analysis of such data, the Grid Computing paradigm provides <b>efficiencies</b> <b>and</b> <b>scalability</b> otherwise unavailable to researchers. In our work we explore the integration of Grid with NLP, to mine relevant answers from these distributed resources. Our system receives queries from various interfaces and then uses NLP to understand the domain of the question. The Grid then routes the queries to the correct knowledge farm, depending on the domain found. Knowledge farms are distributed components which have large annotated domain specific datasets. We propose a novel method which involves the working of the Grid and NLP in concert to mine relevant information quickly. We have also created a working model of our system deploying various existing frameworks as our subsystems. I...|$|R
50|$|The private mailbox {{database}} design {{gives the}} server considerable advantages in <b>efficiency,</b> <b>scalability,</b> <b>and</b> administratability. Multiple concurrent read/write {{connections to the}} same mailbox are permitted. The server supports access control lists on mailboxes and storage quotas on mailbox hierarchies.|$|R
30|$|The results {{indicate}} that correntropy has a large potential as a metric for image analysis in the multi-core era due to its high parallel <b>efficiency</b> <b>and</b> <b>scalability.</b>|$|E
40|$|Many {{organizations}} have {{large quantities of}} data collected in various application areas. Classification of data is a major issue which leads less <b>efficiency</b> <b>and</b> <b>scalability.</b> In this paper, we developed a new method for decision tree for classification of data using a data structure called Peano Count Tree (P-tree) which enhances the <b>efficiency</b> <b>and</b> <b>scalability.</b> We apply Data Smoothing and Attribute Relevance techniques along with a classifier. Experimental {{results show that the}} P-tree method is significantly faster than existing classification methods, making it the preferred method for mining on data to be classified...|$|E
30|$|As {{shown in}} Fig.  4, we vary {{the number of}} {{monitoring}} nodes m with diverse window unit size w to demonstrate the <b>efficiency</b> <b>and</b> <b>scalability</b> of our resolution algorithm by using synthetic dataset.|$|E
50|$|Lustre File System {{has been}} {{designed}} and implemented {{to deal with the}} issue of bottlenecks traditionally found in distributed systems. Lustre is characterized by its <b>efficiency,</b> <b>scalability,</b> <b>and</b> redundancy. GPFS was also designed with the goal of removing such bottlenecks.|$|R
40|$|When XML {{documents}} are modeled as graphs, many challenging research issues arise. In particular, query processing for graph-structured XML data brings new challenges because traditional structural join methods cannot be directly applied. In this paper, we propose a labeling scheme for graph-structured XML data. With this labeling scheme, the reachability relationship of two nodes can be judged efficiently without accessing other nodes. Based on this labeling scheme, we design efficient structural join algorithms to evaluate reachability queries. Experiments show that our algorithms have high <b>efficiency</b> <b>and</b> good <b>scalability...</b>|$|R
40|$|Abstract—Different trust {{models have}} been {{developed}} for deal-ing with possible dishonest behavior and attacks from malicious peer Intrusion Detection Systems (IDSs) in a collaborative In-trusion Detection Network (IDN). For evaluating and comparing these models, this paper introduces a simulation framework that incorporates different components namely expertise model, deception model, attack model, and evaluation metrics. The proposed framework offers flexibility for users to adjust the simulation parameters according to their needs. We then compare three existing trust models in this domain to demonstrate the effectiveness of our framework when used in analyzing their <b>efficiency,</b> robustness <b>and</b> <b>scalability.</b> I...|$|R
