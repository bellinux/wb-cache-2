28|79|Public
50|$|Spatially dense <b>data</b> <b>condensation</b> applications: Arruda, J.R.F. 1993 {{applied the}} RDFS to {{condense}} spatially dense spatial measurements {{made with a}} laser Doppler vibrometer prior to applying modal analysis parameter estimation methods. More recently, Vanherzeele et al. (2006,2008a) proposed a generalized and an optimized RDFS for {{the same kind of}} application. A review of optical measurement processing using the RDFS was published by Vanherzeele et al. (2009).|$|E
40|$|Pattern Recognition Algorithms for Data Mining {{addresses}} different {{pattern recognition}} (PR) tasks in a unified framework with both theoretical and experimental results. Tasks covered include <b>data</b> <b>condensation,</b> feature selection, case generation, clustering/classification, and rule generation and evaluation. This volume presents various theories, methodologies, and algorithms, using both classical approaches and hybrid paradigms. The authors emphasize large datasets with overlapping, intractable, or nonlinear boundary classes, and datasets that demonstrate granular computing in soft frameworks. Organized into eight chapters, the book {{begins with an}} introduction to PR, data mining, and knowledge discovery concepts. The authors analyze the tasks of multi-scale <b>data</b> <b>condensation</b> and dimensionality reduction, then explore the problem of learning with support vector machine (SVM). They conclude by highlighting the significance of granular computing for different mining tasks in a soft paradigm. Pattern Recognition Algorithms for Data Mining addresses different pattern recognition (PR) tasks in a unified framework with both theoretical and experimental results. Tasks covered include <b>data</b> <b>condensation,</b> feature selection, case generation, clustering/classification, and rule generation and evaluation. This volume presents various theories, methodologies, and algorithms, using both classical approaches and hybrid paradigms. The authors emphasize large datasets with overlapping, intractable, or nonlinear boundary classes, and datasets that demonstrate granular computing in soft frameworks...|$|E
40|$|Application Scenario � Governmental and {{commercial}} organizations need to disseminate data for research or business-related applications. � Data owners {{are concerned about}} the privacy of their data, and not willing to release it in plain. � Data perturbation (randomization) strives to provide a solution to this dilemma. � Existing Perturbation Approach � Additive noise perturbation, <b>data</b> <b>condensation,</b> data anonymization, data swapping, sampling, etc. � They do not preserve Euclidean distance of the original data exactly. ...|$|E
40|$|Abstract: <b>Data,</b> {{concerning}} <b>condensation</b> {{reactions of}} salicylaldehydes and 2 -methylfuran are generalized. During {{the course of}} investigation a sequence of proceeding transformations is established. It is shown {{that it is possible}} to obtain 2 -hydroxyarylbis(5 methylfur- 2 -yl) methane, benzofuran or oxazulene derivatives selectively by simple changing of the reaction conditions...|$|R
40|$|This paper {{investigates the}} effect of vapour super-heating on {{hydrocarbon}} refrigerant 600 a (Isobutane), 290 (Propane) and 1270 (Propylene) condensation inside a brazed plate heat exchanger. Vapour super-heating increases heat transfer coefficient with respect to saturated vapour, whereas no effect was observed on pressure drop. The super-heated vapour <b>condensation</b> <b>data</b> shows the same trend vs. refrigerant mass flux as the saturated vapour <b>condensation</b> <b>data,</b> but with higher absolute values. A transition point between gravity controlled and forced convection condensation has been found for a refrigerant mass flux around 15 - 18 kg m- 2 s- 1 depending on refrigerant type. The super-heated vapour heat transfer coefficients are from 5 to 10...|$|R
40|$|We {{discuss the}} {{generation}} of dynamical scalar fields within pure Yang-Mills theories, {{with an eye on}} possible implications for the actual Higgs physics. Such scalars are routinely introduced, in particular, within the dual-superconductor model of confinement and there is plenty of lattice <b>data</b> on <b>condensation</b> of magnetic degrees of freedom in the Yang-Mills vacuum. We emphasize that the dynamical scalar fields of this type turn to be "soft" and avoid the problems of ultraviolet divergences. Moreover, generically their properties are close to the stringy scalars, like the thermal scalar. No immediate proposals for the Higgs particles are made...|$|R
40|$|The K-Modes {{clustering}} algorithm [1] {{has shown}} great promise for clustering large data sets with categorical attributes. K-Mode clustering algorithm {{suffers from the}} drawback of choosing random selection of initial points (modes) of the cluster. Different initial points leads to different cluster formations. In this paper Density-based Multiscale <b>Data</b> <b>Condensation</b> [2] approach with hamming distance [1] is used to extract K-initial points. Experiments show that K-modes clustering algorithm using these initial points produce improved and consistent results then the random selection method...|$|E
40|$|Abstract—The {{requirement}} {{to reduce the}} computational cost of evaluating a point probability density estimate when employing a Parzen window estimator is a well-known problem. This paper presents the Reduced Set Density Estimator that provides a kernelbased density estimator which employs {{a small percentage of}} the available data sample and is optimal in the L 2 sense. While only requiring OðN 2 Þ optimization routines to estimate the required kernel weighting coefficients, the proposed method provides similar levels of performance accuracy and sparseness of representation as Support Vector Machine density estimation, which requires OðN 3 Þ optimization routines, and which has previously been shown to consistently outperform Gaussian Mixture Models. It is also demonstrated that the proposed density estimator consistently provides superior density estimates for similar levels of data reduction to that provided by the recently proposed Density-Based Multiscale <b>Data</b> <b>Condensation</b> algorithm and, in addition, has comparable computational scaling. The additional advantage of the proposed method is that no extra free parameters are introduced such as regularization, bin width, or condensation ratios, making this method a very simple and straightforward approach to providing a reduced set density estimator with comparable accuracy to that of the full sample Parzen density estimator. Index Terms—Kernel density estimation, Parzen window, <b>data</b> <b>condensation,</b> sparse representation. ...|$|E
40|$|An {{algorithm}} for <b>data</b> <b>condensation</b> using support vector machines (SVM’s) is presented. The algorithm extracts {{data points}} lying {{close to the}} class boundaries, which form a much reduced but critical set for classification. The problem of large memory requirements for training SVM’s in batch mode is circumvented by adopting an active incremental learning algorithm. The learning strategy is motivated from the condensed nearest neighbor classification technique. Experimental results presented show that such active incremental learning enjoy superiority in terms of computation time and condensation ratio, over related methods...|$|E
40|$|An {{approximate}} {{theoretical model}} is derived for laminar film condensation {{on the inside}} of a rotating, truncated cone, and is used to predict the heat transfer performance of rotating, non-capillary heat pipes {{for a wide variety of}} parametric conditions. Experimental results are presented for water, ethyl alcohol, and freon- 113 in a stainless steel heat pipe rotating to speeds of 2800 rpm. Results show that these devices can be used effectively to transfer large quantities of heat in rotating systems. Predicted results agree to within + or - 20 percent of the experimental <b>data.</b> Dropwise <b>condensation,</b> instead of film condensation, improves heat pipe performance while the presence of non-condensible gases impairs performance...|$|R
40|$|To {{substantiate}} {{the geometry}} of finning condenser tubes for intensifying vapour condensation {{the dimensions of the}} flooding areas of the bottom part of hoizontal tubes as well as the parameters of a film flowing down in the grooves of vertical tubes are determined. The theoretical solution of the problem on vapour condensation from the profiled surfaces is presented. The experimental <b>data</b> on the <b>condensation</b> of vapours of various liquids have proved the solution obtained...|$|R
40|$|Abstract: Phenylazo- and thiazolylazo- 2, 4 -pentanediones on {{reaction}} with 2 -aminophenol and 2 -aminothiophenol {{yielded a}} new series of polydentate Schiff’s base ligands. The structure and tautomeric nature of these compounds and their metal complexes were established {{on the basis of their}} IR, 1 H-NMR and mass spectral data. The spectral and analytical <b>data</b> revealed the <b>condensation</b> of both carbonyl groups of 3 -(2 -thiazolylazo) - 2, 4 -pentanedione with 2 -aminophenol to form an N 2 O 2 tetradentate ligand...|$|R
40|$|Abstract. An {{approach}} {{is presented to}} guide the benchmarking of invoice analysis systems, a specific, applied subclass of document analysis systems. The {{state of the art}} of benchmarking of document analysis systems is presented, based on the processing levels: Document Page Segmentation, Text Recognition, Document Classification, and Information Extraction. The restriction to invoices enables and requires a more purposeful, i. e. detailed, targetting of the benchmarking procedures (acquisition of ground truth data, system runs, comparison of <b>data,</b> <b>condensation</b> into meaningful numbers). Therefore the processing of invoices is dissected. The involved data structures are elicited and presented. These are provided, being the building blocks of the actual benchmarking of invoice analysis systems. ...|$|E
40|$|In this paper, typical {{learning}} task including <b>data</b> <b>condensation,</b> binary classification, {{identification of the}} independence between random variables and conditional density estimation is described from a unified perspective of a linear combination of densities, and accordingly a direct estimation framework based on a linear combination of Gaussian components (i. e., Gaussian basis functions) under integrated square error criterion is proposed to solve these {{learning task}}s. The proposed direct estimation framework has three advantages. Firstly, different {{from most of the}} existing state-of-the-art methods in which estimating each component’s density in this linear combination of densities and then combining them linearly are required, it can directly estimate the linear combination of densities as a whole, and it has at least comparable to or even better approximation accuracy than the existing density estimation methods. Secondly, the time complexity of the proposed direct estimation framework is O(l 3) in which l is the number of Gaussian components in this framework which are generally viewed as the Gaussian distributions of the clusters in a dataset, and hence l is generally much less than the size of the dataset, so it is very suitable for large datasets. Thirdly, this proposed framework can be typically used to develop alternative approaches to classification, <b>data</b> <b>condensation,</b> identification of the independence between random variables, conditional density estimation and the similarity identification between multiple source domains and a target domain. Our preliminary results about experiments on several typical applications indicate the power of the proposed direct estimation framework. Department of ComputingSchool of Nursin...|$|E
40|$|AbstractThis paper {{gives an}} {{overview}} of the new ‘Fuel Rod Analysis Toolbox’, which is a program for the pre-processing of input data for fuel rod performance codes with a graphical user interface. It consists of three different modules that can handle several tasks such as <b>data</b> <b>condensation,</b> merging and synchronization. The ‘Fuel Rod Analysis Toolbox’ helps:•to reduce the amount of input data,•to simplify the setup of input files for complex data sets with input from experimental data as well as input resulting from neutronics or thermo-hydraulics codes, and•to reduce computation time. These advantages are already evident for complex fuel rod analyses employing a conventional one-and-a-half-dimensional code but they become even more important for two- or three-dimensional approaches...|$|E
40|$|FIELD GROUP SUB-GROUP filmwise condensation, {{integral}} finned tubes, vapor velocity 19 ABSTRACT (Continue on reverse if-ecessary {{and identify}} by block number) Over the years, {{there has been}} significant variation in the filmwise steam <b>condensation</b> <b>data</b> at NPS on horizontal low-integral finned tubes. With a view to increasing {{the accuracy of the}} data, inserts were used inside the tubes to reduce inside thermal resistance; however, significant discrepancies then occurred in the calculated outside heat-transfer coefficient when compared to data taken without a...|$|R
40|$|We {{consider}} {{a system of}} N bosons interacting through a singular two-body potential scaling with N and having the form N^ 3 β- 1 V (N^β x), for an arbitrary parameter β∈ (0, 1). We provide a norm-approximation for the many-body evolution of initial <b>data</b> exhibiting Bose-Einstein <b>condensation</b> {{in terms of a}} cubic nonlinear Schrödinger equation for the condensate wave function and of a unitary Fock space evolution with a generator quadratic in creation and annihilation operators for the fluctuations. Comment: 37 page...|$|R
40|$|This paper {{reports on}} a new sensor, based on a porous silicon oxide microcavity, for the {{determination}} of the alcoholic strength of white and red wines. The shift of the cavity mode due to interaction with ethanol is monitored in continuous way by means of Fourier transform infrared (FT-IR) spectroscopy comparing the <b>data</b> obtained in <b>condensation</b> and evaporation mode. The results demonstrate the advantage of working in evaporation mode and the possibility to determine the alcoholic strength of wine with a good reproducibility and selectivity...|$|R
40|$|A {{multicomponent}} discrete-sectional {{model was}} used to simulate the fate of lead in a high temperature system. The results show {{the ability of the}} developed model to simulate metallic aerosol systems at high temperatures. The PbO reaction and nucleation rate can be determined by comparing the simulations and the experimental <b>data.</b> <b>Condensation</b> on SiO{sub 2 } particle surfaces is found important for removing the PbO vapor. The value of the accommodation factor that is applied to account for nonidealities in the condensation process are determined. The differences between the nanosized particles and the bulk particles are elucidated. The use of such a model helped to understand the effects of various mechanisms in determining the metal oxide vapor concentration profile and in establishing the ultimate particle size distribution...|$|E
40|$|Abstract — A {{cooperative}} {{framework is}} {{presented in this paper}} where multiple Cores in host and multiple GPUs cooperate to compute the Voronoi adjacency relationship in multidimensional Machine Learning datasets. Voronoi adjacency plays a very important role in neighbor based procedures in classification and <b>data</b> <b>condensation.</b> The proposal includes a system of Polytope Inclusion Agents, which computes the Delaunay polytope that contains a defined point, and a Coordination System among the agents that deals with the scheduling and load balance. The Polytope Inclusion Agent uses the Dual Simplex algorithm to solve a Linear Programming problem. The results show that for small datasets the use of GPU is a drawback, while for larger ones the GPUs take advantages of their massive parallelism...|$|E
40|$|This {{research}} aims to know: {{patterns of}} supervision {{conducted by the}} supervisor of PI in the workplace. This research took place at PT. Jaringan Multimedia Indonesia (PT. JMI) as place for Industrial Practice Program. The key informants in this research were the industrial mentorsand {{students who participated in}} the industrial practice program. The data were collected through in-depth interviews, and documentation. The technical analysis of the data refered to the analysis of Miles & Huberman interactive model, including data collection, <b>data</b> <b>condensation,</b> data display, and drawing and verifying conclusions. The result shows that methods of supervision to students through mentoring that is focused on the handling of the case or case studies. So students are asked directly involved in work in the industry...|$|E
40|$|One of the {{engineered}} {{safety systems}} in the advanced boiling water reactor is a passive containment cooling system (PCCS) which is composed {{of a number of}} vertical heat exchanger. After a loss of coolant accident, the pressurized steam discharged from the reactor and the noncondensable (NC) gases mixture flows into the PCCS condenser tube. The PCCS condenser must be able to remove sufficient energy from the reactor containment to prevent containment from exceeding its design pressure. The efficient performance of the PCCS condenser is thus vital to the safety of the reactor. In PCCS condenser tube three flow conditions are expected namely the forced flow, cyclic venting and complete condensation modes. The PCCS test facility consists of steam generator (SG), instrumented condenser with secondary pool boiling section, condensation tank, suppression pool, storage tank, air supply line, and associated piping and instrumentation [...] The specific design of condensing tube is based on scaling analysis from the PCCS design of ESBWR. The scaled PCCS is made of four tubes of diameter 52. 5 mm and height 1. 8 m arranged in square pitch. Steam air mixture condensation tests were carried out in a through flow mode of operation where the mixture flows through the condenser tube with some steam <b>condensation.</b> <b>Data</b> on <b>condensation</b> heat transfer were obtained for two nominal pressures, 225 kPa and 275 kPa and for air concentration fraction from 0 to 13 %. Test results showed that with increase in pressure the condensation heat transfer increased. The presence of the air in the steam decreased the condensation heat transfer coefficient from 10 to 45 % depending on air fraction in the steam...|$|R
40|$|Four total {{pressure}} probes {{were used to}} measure the growth of condensation down the test section of the Langley 0. 3 -m tunnel, and the <b>condensation</b> <b>data</b> were employed to verify a mathematical model which assumes condensation results from heterogeneous nucleation on preexisting seed particles. The onset of effects occurs throughout the test section at the same total temperature but the magnitude of the effects increases with increasing length down the test section. Condensation is important because it determines the minimum operating temperature of transonic cryogenic wind tunnels...|$|R
40|$|This work {{presents}} new correlations {{to compute}} the heat transfer coefficient during refrigerant condensation {{both inside and outside}} enhanced tubes. Inside augmented tubes considered include low-fin, micro-fin and cross-grooved, while four different three-dimensional finned tubes have been considered for outside condensation. The models proposed are able to reproduce available experimental <b>data</b> relative to <b>condensation</b> of pure refrigerants and also refrigerant mixtures with a fair accuracy. The paper also presents an experimental rig specifically designed to measure heat transfer coefficients and pressure drop during condensation of zeotropic refrigerant mixtures inside enhanced tubes...|$|R
40|$|Intelligent data {{analytics}} gradually becomes a day-to-day reality of today's businesses. However, despite rapidly increasing storage and computational power current state-of-the-art predictive models {{still can not}} handle massive and noisy corporate data warehouses. What is more adaptive and real-time operational environment requires multiple models to be frequently retrained which fiirther hinders their use. Various data reduction techniques ranging from data sampling up to density retention models attempt to address this challenge by capturing a summarised data structure, yet they either do not account for labelled data or degrade the classification performance of the model trained on the condensed dataset. Our response is a proposition of a new general framework for reducing the complexity of labelled data by means of controlled spatial redistribution of class densities in the input space. On the example of Parzen Labelled Data Compressor (PLDC) we demonstrate a simulatory <b>data</b> <b>condensation</b> process directly inspired by the electrostatic field interaction where the data are moved and merged following the attracting and repelling interactions with the other labelled data. The process {{is controlled by the}} class density function built on the original data that acts as a class-sensitive potential field ensuring preservation of the original class density distributions, yet allowing data to rearrange and merge joining together their soft class partitions. As a result we achieved a model that reduces the labelled datasets much further than any competitive approaches yet with the maximum retention of the original class densities and hence the classification performance. PLDC leaves the reduced dataset with the soft accumulative class weights allowing for efficient online updates and as shown in a series of experiments if coupled with Parzen Density Classifier (PDC) significantly outperforms competitive <b>data</b> <b>condensation</b> methods in terms of classification performance at the comparable compression levels...|$|E
30|$|In {{response}} {{to the problems of}} analyzing large-scale data, quite a few efficient methods [2], such as sampling, <b>data</b> <b>condensation,</b> density-based approaches, grid-based approaches, divide and conquer, incremental learning, and distributed computing, have been presented. Of course, these methods are constantly used to improve the performance of the operators of data analytics process. 1 The results of these methods illustrate that with the efficient methods at hand, {{we may be able to}} analyze the large-scale data in a reasonable time. The dimensional reduction method (e.g., principal components analysis; PCA [3]) is a typical example that is aimed at reducing the input data volume to accelerate the process of data analytics. Another reduction method that reduces the data computations of data clustering is sampling [4], which can also be used to speed up the computation time of data analytics.|$|E
40|$|Abstract—A simple yet {{effective}} {{unsupervised classification}} rule {{to discriminate between}} normal and abnormal data is based on accepting test objects whose nearest neighbors ’ distances in a reference data set, assumed to model normal behavior, lie within a certain threshold. This work investigates the effect of using {{a subset of the}} original data set as the reference set of the classifier. With this aim, the concept of a reference-consistent subset is introduced and it is shown that finding the minimum-cardinality referenceconsistent subset is intractable. Then, the Condensed Nearest Neighbor Domain Description (CNNDD) algorithm is described, which computes a reference-consistent subset with only two reference set passes. Experimental results revealed the advantages of condensing the data set and confirmed the effectiveness of the proposed approach. A thorough comparison with related methods was accomplished, pointing out {{the strengths and weaknesses of}} one-class nearest-neighbor-based training-set-consistent condensation. Index Terms—Classification, data domain description, <b>data</b> <b>condensation,</b> nearest neighbor rule, novelty detection. Ç...|$|E
40|$|The {{mechanism}} of the deep earthquake that occurred south of Honshu, Japan, on February 18, 1956, has been studied by several seismologists. In the present paper, all the available <b>data</b> on the <b>condensation</b> and rarefaction of the P waves and the polarization angle of the S waves are presented. The ample data gives a fine example {{for the study of}} the source mechanism. And it is confirmed clearly that the {{mechanism of}} the earthquake can be explained by the double couple hypothesis. 1956 年 2 月 18 日に本州南方沖に起こつた深発地震のメカニズムはすでに多くの人々によつて研究されている. 本研究においては本邦および外国の観測所におけるこの地震のP波およびS波に関する観測結果をできるだけ多く集め,その豊富な材料に基いてこの地震のメカニズムはdoub lecouple(type II) の震原モデルによつて十分によく説明できることを確めた...|$|R
40|$|The {{influence}} of return bends on the downstream pressure drop and {{heat transfer coefficient}} of condensing refrigerant R- 12 was studied experimentally. Flow patterns in glass return bends of 1 / 2 to 1 in. radius and 0. 315 in. I. D. were examined visually and photographically using a high frequency xenon light source. Local pressure drop and heat transfer measurements were made along a horizontal 14 1 / 2 ft. test section immediately following the return bend. The refrigerant mass flux ranged from 1. 32 X 105 to 4. 58 X 105 lbm/hr-ft 2, saturation temperature from 90 to 107 *F, and return bend quality from 0. 24 to 1. 0. The pressure drop and heat transfer data were compared to previous <b>data</b> for <b>condensation</b> without return bends. Effects on the downstream pressure drop and heat transfer {{were found to be}} small, if not negligible. Sponsored by Technical Committee 1. 3, American Society of Heating, Refrigeration, and Air Conditioning Engineers DSR Projec...|$|R
40|$|Prediction of heat {{transfer}} during film condensation in mini and microchannels is of much practical interest. No well-verified method {{for this purpose}} is available. The applicability of the author’s well-validated general correlation (Shah 2009) for condensation in tubes to small channels is investigated in this paper. A wide range of <b>data</b> for <b>condensation</b> in horizontal micro and mini channels were compared with it. This correlation was found to predict 500 data points from 15 studies on small diameter channels with a mean deviation of 15. 9 percent. These data included single round and rectangular channels as well as multiport channels with round and rectangular ports with equivalent diameters from 0. 49 to 5. 3 mm, 8 fluids, reduced pressures from 0. 048 to 0. 52, and mass flux from 50 to 1400 kg/m 2 s. This indicates its applicability to minichannels. However, {{a large amount of}} data for diameters from 0. 114 to 2. 6 mm showed large deviations from this correlation. The discrepancy in the overlapping range of data could be due to difficulties in accurate measurements on small channels...|$|R
40|$|This paper {{gives an}} {{overview}} of the new ‘Fuel Rod Analysis Toolbox’, which is a program for the pre-processing of input data for fuel performance codes with a graphical user interface under Windows. It consists of three different modules that can handle several tasks such as <b>data</b> <b>condensation,</b> merging and synchronization. It can reduce the amount of data, hence the computation time of fuel performance codes and reduce the time required to prepare input files for complex experimental data sets, or prepare the input files on the basis of detailed code calculations carried out by neutronics or thermo-hydraulics codes applied for safety analysis reports. It is proposed {{to be used as a}} common basis to generate input files for fuel performance code validation on the basis of the experimental data included in the International Fuel Performance Experiments database of the IAEA and the OECD-NEA in the frame of benchmarks such as FUMAC. JRC. E. 3 -Materials researc...|$|E
40|$|Quantification of {{functional}} connectivity in physiological networks is frequently performed {{by means of}} time-variant partial directed coherence (tvPDC), based on time-variant multivariate autoregressive models. The principle advantage of tvPDC lies in the combination of directionality, time variance and frequency selectivity simultaneously, offering a more differentiated view into complex brain networks. Yet the advantages specific to tvPDC also cause {{a large number of}} results, leading to serious problems in interpretability. To counter this issue, we propose the decomposition of multi-dimensional tvPDC results into a sum of rank- 1 outer products. This leads to a <b>data</b> <b>condensation</b> which enables an advanced interpretation of results. Furthermore it is thereby possible to uncover inherent interaction patterns of induced neuronal subsystems by limiting the decomposition to several relevant channels, while retaining the global influence determined by the preceding multivariate AR estimation and tvPDC calculation of the entire scalp. Finally a comparison between several subjects is considerably easier, as individual tvPDC results are summarized within a comprehensive model equipped with subject-specific loading coefficients. A proof-of-principle of the approach is provided by means of simulated data; EEG data of an experiment concerning visual evoked potentials are used to demonstrate the applicability to real data...|$|E
40|$|This study {{describes}} {{the findings of}} a qualitative study examining the understanding about the beliefs and practices about assessment of faculty who teach English as a foreign language. I analyzed the beliefs and practices of EFL faculty members as described in their responses upon the instructional process of the target language following a three-part approach proposed by Miles, Huberman and Saldana (2014), which is a concurrent flow of three activities: (a) <b>data</b> <b>condensation,</b> (b) data display, and (c) conclusion drawing/ verification. For the collection of data, I used two instruments: qualitative interviews, and documents. The following research questions guided this study: 1) what are the beliefs of faculty members about the role of assessment in the teaching of English as a foreign language in undergraduate programs in a higher educational institution in Colombia? 2) How do EFL faculty members describe the practice of assessment in a Colombian higher educational institution, both in terms of their teaching EFL pre service teachers and in their interactions with departmental colleagues? 3) How do EFL faculty members interpret the implications of the guiding principles about assessment of a Colombian university, contained in its undergraduate student manual, for their instructional practice of English as Foreign Language...|$|E
40|$|The RACORO aerosol <b>data</b> (cloud <b>{{condensation}}</b> nuclei (CCN), condensation nuclei (CN) and aerosol size distributions) need {{further processing}} {{to be useful}} for model evaluation (e. g., GCM droplet nucleation parameterizations) and other investigations. These tasks include: (1) Identification and flagging of 'splash' contaminated Twin Otter aerosol data. (2) Calculation of actual supersaturation (SS) values in the two CCN columns flown on the Twin Otter. (3) Interpolation of CCN spectra from SGP and Twin Otter to 0. 2 % SS. (4) Process data for spatial variability studies. (5) Provide calculated light scattering from measured aerosol size distributions. Below we first briefly describe the measurements and then describe the results of several data processing tasks that which have been completed, {{paving the way for}} the scientific analyses for which the campaign was designed. The end result of this research will be several aerosol data sets which can be used to achieve some of the goals of the RACORO mission including the enhanced understanding of cloud-aerosol interactions and improved cloud simulations in climate models...|$|R
40|$|A simple semi-empirical {{theory is}} {{developed}} for the ionic strength dependence of the flexible polymer-induced condensation of semiflexible polyelectrolytes such as DNA and F-actin filaments. Critical concentrations of flexible polymer needed for condensation are calculated by comparing the free energies of inserting the semiflexible polyelectrolytes in a solution of flexible polymers, respectively, in their free state, and in their condensed state. Predictions of the theory are compared to experimental <b>data</b> on the <b>condensation</b> of DNA and F-actin filaments induced by the flexible polymer poly(ethylene oxide). The theory also predicts that reentrant decollapse is possible at low ionic strength and high concentrations of flexible polymer, as observed for DNA...|$|R
40|$|Much {{attention}} has been paid in the recent years to the possible use of fluorinated propene isomers for the substitution of high-GWP refrigerants. Among the fluorinated propene isomers, R 1234 ze(E) may be a substitute of R 134 a for refrigeration applications. R 1234 ze(E) has a GWP equal to 6 (considering a period time of 100 years), much lower than that of R 134 a. Unfortunately, R 1234 ze(E) displays poorer heat transfer characteristics during condensation as compared to R 134 a. In this paper, to get better performance, a mixture of R 1234 ze(E) and R 32 is under study at two mass compositions (23 / 77 % and 46 / 54 %). In particular the local heat transfer coefficient during condensation of this mixture in a single minichannel with 0. 96 mm diameter is measured and analyzed. Tests are carried out with the experimental apparatus available at the Two Phase Heat Transfer Lab of the University of Padova. The new experimental data are compared to <b>condensation</b> <b>data</b> of pure R 1234 ze(E) and R 32. This allows to analyze the heat transfer penalization due to the mass transfer resistance occurring during condensation of this zeotropic mixture and to assess available predicting models for condensation of zeotropic mixtures. It is worth mentioning here that very limited <b>data</b> of mixture <b>condensation</b> in minichannel are available in the literature. Therefore, the present paper provides interesting information with regard to the applicability of available models in mini and microchannels...|$|R
