37|9|Public
25|$|The first {{well-known}} public {{presentation of}} markup languages in computer text processing {{was made by}} William W. Tunnicliffe {{at a conference in}} 1967, although he preferred to call it generic coding. It {{can be seen as a}} response to the emergence of programs such as RUNOFF that each used their own control notations, often specific to the target typesetting device. In the 1970s, Tunnicliffe led the development of a standard called GenCode for the publishing industry and later was the first chair of the International Organization for Standardization committee that created SGML, the first standard <b>descriptive</b> <b>markup</b> language. Book designer Stanley Rice published speculation along similar lines in 1970. Brian Reid, in his 1980 dissertation at Carnegie Mellon University, developed the theory and a working implementation of <b>descriptive</b> <b>markup</b> in actual use.|$|E
25|$|Another major {{publishing}} {{standard is}} TeX, created and refined by Donald Knuth in the 1970s and '80s. TeX concentrated on detailed layout of text and font descriptions to typeset mathematical books. This required Knuth to spend considerable time investigating {{the art of}} typesetting. TeX is mainly used in academia, where it is a de facto standard in many scientific disciplines. A TeX macro package known as LaTeX provides a <b>descriptive</b> <b>markup</b> system on top of TeX, and is widely used.|$|E
2500|$|Markup is used {{to label}} parts of the {{document}} rather than to provide specific instructions {{as to how they}} should be processed. Well-known examples include LaTeX, HTML, and XML. The objective is to decouple the inherent structure of the document from any particular treatment or rendition of it. Such markup is often described as [...] "semantic". An example of <b>descriptive</b> <b>markup</b> would be HTML's <cite> tag, which {{is used to}} label a citation. Descriptive markup—sometimes called logical markup or conceptual markup—encourages authors to write in a way that describes the material conceptually, rather than visually.|$|E
40|$|Abstract. ADDS (Approach to Document-based Development of Software) is an {{approach}} {{to the development of}} applications based on a document-oriented paradigm. According to this paradigm, applications are described by means of documents that are marked up using <b>descriptive</b> domain-specific <b>markup</b> languages. Afterwards, applications are produced processing these marked up documents. Formulation of domain-specific markup languages in ADDS is a dynamic and eminently pragmatic activity since these languages evolve in accordance with the authoring needs of the main actors that participate in the development process (i. e. domain experts and developers). OADDS (Operationalization in ADDS) is a processing model that promotes the construction of modular language processors and their incremental evolution. Thus, OADDS is specifically designed to cope with the evolutionary nature of the domain-specific markup languages encouraged by ADDS. ADDS and OADDS have successfully been applied to the development of applications in knowledge-intensive domains (i. e. transport networks and educational hypermedias). This paper also describes the advantages (incremental development and maintenance improvement) that this approach supposes for the development of knowledge-based systems. ...|$|R
40|$|Abstract: This paper {{proposes a}} {{document}} oriented paradigm {{to the development}} of contentintensive, document-based applications (e. g. educational and hypermedia applications, and knowledge based systems). According to this paradigm, the main aspects of this kind of applications can be described by means of documents. Afterwards, these documents are marked up using <b>descriptive</b> domain-specific <b>markup</b> languages and applications are produced by the automatic processing of these marked documents. We have used this paradigm to improve the maintenance and portability of content-intensive educational and hypermedia applications. ADDS (Approach to Document-based Development of Software) is an approach to software development based on the document oriented paradigm. A key feature of ADDS is that formulation of domain-specific markup languages is a dynamic and eminently pragmatic activity, and markup languages evolve according to the authoring needs of the different participants in the development process (domain experts and developers). The evolutionary nature of markup languages in ADDS leads to OADDS (Operationalization in ADDS), the proposed operationalization model for the incremental development of modular markup language processors. Finally, the document-oriented paradigm can also be applied in th...|$|R
40|$|This article {{presents}} a detailed {{overview of the}} principal languages for the representation, interchange and exploitation of data, both textual and graphical. In particular, a detailed discussion is made of the procedure of text encoding. The approach taken in the article emphasises {{the importance of the}} World Wide Web for data dissemination and the fundamental issue of standards: HTML, XML and its derivate languages are analysed in detail. Importance has been given to the languages that represent not only the characters that textual sources contain but also the structure, content and appearance of the data. Two types of markup languages are presented: procedural and <b>descriptive.</b> A procedural <b>markup</b> specifies how the document should be presented. <b>Descriptive</b> (or logical) <b>markup</b> languages describe the structure of a document, such as SGML. The article considers the topics of international standards as the TEI Guidelines for Electronic Text Encoding and Interchange for the description of marked-up electronic texts and the RDF metadata recommendation. The first section concludes with a presentation of the innovative aspects of the Semantic Web. The second part focuses on spatial, graphical and multimedia data, and their display and exchange over the Web. The development of the Geography Markup Language (GML) is introduced and discussed, as well as other vector formats derived by XML, such as SVG, to construct structured spatial and non-spatial information for data sharing over the Web. Importance has also been given to the virtual reality languages such as VRML, an ISO standard, and the XML-based X 3 D. In conclusion the article aims to present a broad view not only of the technical aspects of data encoding but also the analysis of the standards, which are fundamental in the light of data interoperability and exchange...|$|R
2500|$|Berners-Lee {{considered}} HTML an SGML application. The Internet Engineering Task Force (IETF) formally {{defined it}} as such with the mid-1993 publication of the first proposal for an HTML specification: [...] by Berners-Lee and Dan Connolly, which included an SGML Document Type Definition to define the grammar. Many of the HTML text elements {{are found in the}} 1988 ISO technical report TR 9537 Techniques for using SGML, which in turn covers the features of early text formatting languages such as that used by the RUNOFF command developed in the early 1960s for the CTSS (Compatible Time-Sharing System) operating system. These formatting commands were derived from those used by typesetters to manually format documents. Steven DeRose argues that HTML's use of <b>descriptive</b> <b>markup</b> (and influence of SGML in particular) was {{a major factor in the}} success of the Web, because of the flexibility and extensibility that it enabled. HTML became the main markup language for creating web pages and other information that can be displayed in a web browser, and is quite likely the most used markup language in the world today.|$|E
50|$|Scribe is a markup {{language}} and word processing system which pioneered {{the use of}} <b>descriptive</b> <b>markup.</b> Scribe was revolutionary when it was proposed, because it involved {{for the first time}} a clean separation of presentation and content.|$|E
50|$|The first {{well-known}} public {{presentation of}} markup languages in computer text processing {{was made by}} William W. Tunnicliffe {{at a conference in}} 1967, although he preferred to call it generic coding. It {{can be seen as a}} response to the emergence of programs such as RUNOFF that each used their own control notations, often specific to the target typesetting device. In the 1970s, Tunnicliffe led the development of a standard called GenCode for the publishing industry and later was the first chair of the International Organization for Standardization committee that created SGML, the first standard <b>descriptive</b> <b>markup</b> language. Book designer Stanley Rice published speculation along similar lines in 1970. Brian Reid, in his 1980 dissertation at Carnegie Mellon University, developed the theory and a working implementation of <b>descriptive</b> <b>markup</b> in actual use.|$|E
40|$|DRAFT DRAFT Abstract DRAFT DRAFT {{shortcomings}} of this DRAFT approach. A DSL formulation DRAFT implies an in-depth {{analysis of the}} application domain, In this paper we present ADDS (Approach to and strong usability considerations regarding the DRAFT DRAFT DRAFT DRAFT DRAFT DRAFT Document-oriented Development of Software), our language’s end users (i. e. domain experts). solution to software construction based on Domain- Furthermore, {{the complexity of the}} DSL’s DRAFT Specific Languages DRAFT (DSLs). DSLs DRAFT in ADDS are DRAFT operationalization process DRAFT must be addressed. DRAFT Lastly, formulated as <b>descriptive</b> Domain-Specific <b>Markup</b> during the development of applications, new aspects, Languages (DSMLs) that are used for marking up the not initially covered by the DSL, could be discovered. DRAFT documents DRAFT that describe the relevant DRAFT aspects of the DRAFT Therefore the costs of DRAFT the DSL maintenance DRAFT must be applications (e. g. data and some aspects of the also considered. behavior). Final running applications are obtained by This paper describes ADDS (Approach to DRAFT the processing DRAFT of these documents DRAFT with suitable DRAFT Document-oriented DRAFT Development of Software), DRAFT our processors. ADDS promotes the incremental approach to the development of software applications DRAFT development DRAFT of DSMLs and their DRAFT processors, so they based on DSLs. ADDS promotes the description of DRAFT DRAFT DRAFT can evolve according to the authoring needs of the relevant aspects of an application by means of different participants in the development process documents. These documents are marked up with DRAFT (domain experts DRAFT and developers). DRAFT The incremental DRAFT appropriate descriptive DRAFT Domain-Specific DRAF...|$|R
40|$|Electronic {{publishing}} {{is confronted}} with a multitude of demands and hopes, expressed by users on one hand, and by institutions on the other. One of the key issues concerns long-term availability of digital information. In addition, research findings indicate that users would like to place more detailed full-text information retrieval requests. Due to differing interests, some users focus their attention to figure captions, others in tables or bibliographies, {{to name just a}} few examples. Furthermore, there is a wish to deliver publications on many platforms, which asks for suitable mechanisms of combining information with different sets of output specifications. In all these three cases, the capabilities of todays desktop editors fall short. Yet they are among the most frequently used tools to produce scientific publications. It is claimed that the answer would lie in the use of structure-oriented editors and <b>descriptive,</b> platform-independent <b>markup.</b> But the move is not a trivial one. One of the first big challenges is the author himself. To what extent is he willing to modify his working habits? Does he accept the possibility of letting someone else define the layout of his work? Another major issue is the publication process. The nature of changes in workflow are as much organizational as they are technical. This paper describes some of the lessons learned in HUTpubl, a project conducted by the Helsinki University of Technology (HUT) Library. The goal of the project is to establish an SGML-based (Standard Generalized Markup Language) publishing model for HUT scientific publication series. The paper further elaborates on findings in other related projects and research activities. reviewe...|$|R
40|$|The Standard Generalized Markup Language (SGML) {{has been}} the International Organization of Standardization (ISO) {{published}} standard for text interchange for nearly a decade. Since 1986, SGML based publishing has been successfully implemented in many fields, notably those industries with massive and mission-critical publishing operations such as the military, legal, medical, and heavy industries. SGML based publishing differs from the WYSIWYG paradigm of desktop publishing in that an SGML document contains <b>descriptive,</b> structural <b>markup</b> rather than specific formatting markup. Specific markup describes {{the appearance of a}} document and is usually a proprietary code which makes the document difficult to re-use or interchange to different systems. The structurally generic markup codes in an SGML document allow the fullest exploitation of the information. An SGML document exhibits more re-usability than a document created and stored in a proprietary formatting code. In many cases, workflow and production are greatly improved by the implementation of SGML based publishing. Historical and anecdotal case studies of many applications clearly delineate the benefits of an SGML based publishing system. And certainly, the boom in Web publishing has spurred interest in enabling a publishing system with multi-output functionality. However, implementation is associated with high costs. The acquisition of new tools and new skills is a costly investment. A careful cost-benefit analysis must determine that the current publishing needs would be satisfied by moving to SGML. Increased productivity is the measure by which SGML is adopted. The purpose of this thesis project is to investigate the relative benefits and requirements of a simple SGML based publishing implementation. The graduate thesis for most of the School of Printing Management and Sciences at the Rochester Institute of Technology was used as an example. The author has expanded the requirements for the publication process of a graduate thesis with factors which do not exist in reality. The required output has been expanded from mere print output to include publishing on the World Wide Web (WWW) in the Hypertext Markup Language (HTML), and to some proprietary electronic browser such as Folio Views for inclusion in a searchable collection of graduate theses on CD-ROM. A proposed set of tools and methods are discussed in order to clarify the requirements of such an SGML implementation...|$|R
50|$|IBM's Generalized Markup Language (GML) Starter Set is a macro {{language}} encapsulating {{a set of}} SCRIPT commands. GML is a <b>descriptive</b> <b>markup</b> layer {{describing the}} logical structure of a document. Both SCRIPT/VS and the GML Starter Set are part of IBM's Document Composition Facility (DCF), used in the System/370 platform and successors. The tag sets of the BookMaster and BookManager BUILD/MVS products are built on a foundation of the GML Starter Set syntax and implementation.|$|E
50|$|Another major {{publishing}} {{standard is}} TeX, created and refined by Donald Knuth in the 1970s and '80s. TeX concentrated on detailed layout of text and font descriptions to typeset mathematical books. This required Knuth to spend considerable time investigating {{the art of}} typesetting. TeX is mainly used in academia, where it is a de facto standard in many scientific disciplines. A TeX macro package known as LaTeX provides a <b>descriptive</b> <b>markup</b> system on top of TeX, and is widely used.|$|E
5000|$|Berners-Lee {{considered}} HTML an SGML application. The Internet Engineering Task Force (IETF) formally {{defined it}} as such with the mid-1993 publication of the first proposal for an HTML specification: [...] "Hypertext Markup Language (HTML)" [...] Internet-Draft by Berners-Lee and Dan Connolly, which included an SGML Document Type Definition to define the grammar. Many of the HTML text elements {{are found in the}} 1988 ISO technical report TR 9537 Techniques for using SGML, which in turn covers the features of early text formatting languages such as that used by the RUNOFF command developed in the early 1960s for the CTSS (Compatible Time-Sharing System) operating system. These formatting commands were derived from those used by typesetters to manually format documents. Steven DeRose argues that HTML's use of <b>descriptive</b> <b>markup</b> (and influence of SGML in particular) was {{a major factor in the}} success of the Web, because of the flexibility and extensibility that it enabled. HTML became the main markup language for creating web pages and other information that can be displayed in a web browser, and is quite likely the most used markup language in the world today.|$|E
40|$|This paper {{describes}} HUTpubl, an SGML-based project {{conducted by}} the HUT [Helsinki University of Technology] Library. The goal of the project {{is to establish a}} working SGML/XML-based publishing model for HUT scientific publication series. In-house project partners include the Department of Automation and Systems Technology, and the Department of Computer Science and Engineering. HUTpubl {{is also a member of}} an umbrella project RAJU (Rakenteinen julkaiseminen yliopistoissa), a newly formed consortium of three Finnish universities. RAJU seeks to define guidelines and to give recommendations for the use of structured documents in higher education in Finland. The paper elaborates on lessons learned in HUTpubl, both technical and those more closely related to finance and workforce. Findings in other related projects and research activities are also included. Electronic publishing in general is confronted with a multitude of demands and hopes, expressed by users on one hand, and by institutions on the other. One of the key issues concerns long-term availability of digital information. In addition, research findings indicate that users would like to place more detailed full-text information retrieval requests. Due to differing interests, some users focus their attention to figures, others in tables or bibliographies, to name just a few examples. Furthermore, there is a wish to deliver publications on many platforms, which asks for suitable mechanisms of combining information with different sets of output specifications. In all these three cases, the capabilities of todays desktop editors fall short. Yet they are among the most frequently used tools to produce scientific publications. It is claimed that the answer would lie in the use of structure-oriented editors and <b>descriptive,</b> platform-independent <b>markup.</b> But the move is not a trivial one. One of the first big challenges is the author himself. To what extent is he willing to modify his working habits? Does he accept the possibility of letting someone else define the layout of his work? Another major issue is the publication process. The nature of changes in workflow are as much organizational as they are technical. There is no easy way to copy existing publishing models from one university to another, as lucrative as it might seem...|$|R
40|$|An {{increasing}} {{amount of}} data is published as semistructured documents formatted with presentational markup. Examples include data objects such as mathematical expressions encoded with MathML or web pages encoded with XHTML. Our intention {{is to improve the}} state of the art in retrieving, manipulating, or mining such data. We focus first on mathematics retrieval, which is appealing in various domains, such as education, digital libraries, engineering, patent documents, and medical sciences. Capturing the similarity of mathematical expressions also greatly enhances document classification in such domains. Unlike text retrieval, where keywords carry enough semantics to distinguish text documents and rank them, math symbols do not contain much semantic information on their own. Unfortunately, considering the structure of mathematical expressions to calculate relevance scores of documents results in ranking algorithms that are computationally more expensive than the typical ranking algorithms employed for text documents. As a result, current math retrieval systems either limit themselves to exact matches, or they ignore the structure completely; they sacrifice either recall or precision for efficiency. We propose instead an efficient end-to-end math retrieval system based on a structural similarity ranking algorithm. We describe novel optimization techniques to reduce the index size and the query processing time. Thus, with the proposed optimizations, mathematical contents can be fully exploited to rank documents in response to mathematical queries. We demonstrate the effectiveness and the efficiency of our solution experimentally, using a special-purpose testbed that we developed for evaluating math retrieval systems. We finally extend our retrieval system to accommodate rich queries that consist of combinations of math expressions and textual keywords. As a second focal point, we address the problem of recognizing structural repetitions in typical web documents. Most web pages use presentational markup standards, in which the tags control the formatting of documents rather than semantically describing their contents. Hence, their structures typically contain more irregularities than <b>descriptive</b> (data-oriented) <b>markup</b> languages. Even though applications would greatly benefit from a grammar inference algorithm that captures structure to make it explicit, the existing algorithms for XML schema inference, which target data-oriented markup, are ineffective in inferring grammars for web documents with presentational markup. There is currently no general-purpose grammar inference framework that can handle irregularities commonly found in web documents and that can operate with only a few examples. Although inferring grammars for individual web pages has been partially addressed by data extraction tools, the existing solutions rely on simplifying assumptions that limit their application. Hence, we describe a principled approach to the problem by defining a class of grammars that can be inferred from very small sample sets and can capture the structure of most web documents. The effectiveness of this approach, together with a comparison against various classes of grammars including DTDs and XSDs, is demonstrated through extensive experiments on web documents. We finally use the proposed grammar inference framework to extend our math retrieval system and to optimize it further...|$|R
40|$|UnrestrictedPatents are {{structured}} documents that contain important {{information related to}} a certain invention and purport to describe the invention in very precise terms. Patent search is often conducted by inventors, patent attorneys, technical and business experts to find the prior art and mitigate risks. Prior art searches {{are the most common}} searches and are performed before filing an application to ascertain patentability of an invention, during the application process to determine novelty of the invention, to invalidate a patent’s claim of originality or to learn about a specific field of invention. Due to the complex nature of the Patent document, traditional information retrieval approaches (IR) do not perform very well.; In this thesis, I investigate methods to improve patent document information retrieval. I propose a tunable and parametric citation based algorithm “FindCite”, {{that can be used for}} easily collecting relevant prior art patents from a patent dataset. Using citations, this algorithm can discover relevant patents even if they do not contain the key words or phrases issued as search queries. Our experiments demonstrate that FindCite results in better precision and recall as compared to other publicly available patent search systems including USPTO patent search and Google patent search.; Additionally, I use “Problem – Solution approach” for patent document information retrieval, in which I investigate methods to identify the problem that a certain invention solves. I propose new methods to automatically extract problem solved concepts, which is a statement of “What problem does the invention solve” from the patent documents. The proposed approaches require no training, thus are domain independent and can be used in any text based information extraction or information retrieval system.; Finally, I propose Patent Document Markup Language (PDML), a <b>descriptive</b> and referential <b>markup</b> which allows marking up and linkage of the problems and their solutions described in the patent documents. PDML not only facilitates navigation from a specific problem to its solution and the claims within a patent document, but also facilitates discovery of relevant patents across the patent dataset; for example, it makes it possible to find all patents that provide different solutions for a certain problem...|$|R
40|$|More {{and more}} large {{repositories}} of texts {{which must be}} automatically processed represent their content {{through the use of}} <b>descriptive</b> <b>markup</b> languages. This method has been diffused by the availability of widely adopted standards like SGML and, later, XML, which made possible the definition of specific format...|$|E
40|$|Markup {{practices}} {{can affect}} the move toward systems that support scholars {{in the process of}} thinking and writing. Whereas procedural and presentational markup systems retard that movement, <b>descriptive</b> <b>markup</b> systems accelerate the pace by simplifying mechanical tasks and allowing the authors to focus their attention on the content...|$|E
40|$|I {{describe}} a synthesis withn TEX of <b>descriptive</b> <b>markup</b> and object-oriented programming. An underlying formatting system may use {{a number of}} different collections of user-level markup, such as LATEX or SGML. I give an extension of WX's markup scheme that more effectively addresses the needs of a production environment. The implementation of such a system benefits from the use of the model of object-oriented programming. LATEX environments {{can be thought of as}} objects, and several environments may share functionality donated by a common, more general object. Ths article is a companion to William Baxter's "An Object-Oriented Programming System in TEX. " I believe that the key to cost-effective production of T $ documents in a commercial setting is <b>descriptive</b> <b>markup.</b> That is, the document being processed contains content organized by codes, the latte...|$|E
40|$|In {{this paper}} we {{describe}} {{an approach to}} the production of learning resources where authors (students and instructors) are actively involved in the production process. This active involvement is achieved by using <b>descriptive</b> <b>markup</b> technologies. Authors are compelled to produce learning resources in the form of documents, and to make the structure of these documents explicit by creating and using <b>descriptive</b> <b>markup</b> languages. This lets developers formalize these author-designed markup languages and provide suitable transformation specifications for translating these marked documents into their final presentations. We exemplify this approach with the production of DHTML pages by Ph. D. students in archaeology, oriented to be used as resources owned by reusable learning objects in Chasqui, an authoring and deployment tool used in the virtualization of academic museums at the Complutense University o...|$|E
40|$|The paper {{discusses}} {{the role of}} <b>descriptive</b> <b>markup</b> languages {{in the development of}} digital humanities, a new research discipline that is part of social sciences and humanities, which focuses on the use of computers in research. A chronological review of the development of digital humanities, and then <b>descriptive</b> <b>markup</b> languages is exposed, through several developmental stages. It is shown that the development of digital humanities since the mid- 1980 s and the appearance of SGML, markup language that was the foundation of TEI, a key standard for the encoding and exchange of humanities texts in the digital environment, is inseparable from the development of markup languages. Special attention is dedicated to the presentation of the Text Encoding Initiative – TEI development, a key organization that developed the titled standard, both from organizational and markup perspectives. By this time, TEI standard is published in five versions, and during 2000 s SGML is replaced by XML markup language. Key words: markup languages, digital humanities, text encoding, TEI, SGML, XM...|$|E
40|$|Abstract—Patterns are {{reusable}} entities {{of knowledge}} {{that need to be}} suitably represented. In this paper, we propose an approach for representing software patterns based on the notions from <b>descriptive</b> <b>markup,</b> semiotics, and knowledge representation. The requirements for representation of software patterns are presented. A pattern representation framework within the Semantic Web architecture is proposed. An example from Web Application patterns is given. The social and technical prospects and challenges of pattern representations are discussed. The significance of tools in automated processing and reasoning is emphasized. I...|$|E
40|$|This paper {{presents}} a 204 -item digital wordlist of Mono, an Ubangian language spoken in the Democratic Republic of the Congo. 1 The wordlist includes orthographic and broad phonetic transcriptions of each word, French and English glosses, an individual WAV recording of each item, GIF {{images of the}} original field transcriptions, and metadata for resource discovery. An archival form of the wordlist was deposited into an institutional archive (the SIL Language and Culture Archives) and includes the original WAV digital recording, <b>descriptive</b> <b>markup</b> encoding of the wordlist in XML employing Unicode 5. 1 transcription, TIFF images of the original field transcriptions, and the metadata record. The presentation form was then generated directly from the archival form. 1...|$|E
40|$|This paper {{presents}} a 207 -item digital wordlist of Buhi’non, an Austronesian language spoken in the Philippines. The wordlist includes a broad phonetic transcription of each word, an English gloss, an individual WAV recording of each item, and metadata for resource discovery. An archival {{form of the}} wordlist was deposited into an institutional archive (Kaipuleohone, the University of Hawai‘i Digital Ethnographic Archive) and includes the original WAV file of the entire wordlist recording session, <b>descriptive</b> <b>markup</b> encoding of the wordlist in XML employing Unicode transcription, TIFF images of the original field transcriptions, and the metadata record. The presentation form was then generated directly from the archival form. National Foreign Language Resource Cente...|$|E
40|$|Functional {{programming}} fits {{well with}} the use of <b>descriptive</b> <b>markup</b> in HTML and XML. There is also a good fit between S-expressions in Lisp and the means of expression in HTML and XML. These similarities are exploited in LAML which is a software package for Scheme. LAML supports exact mirrors of HTML 4. 01, the three variants of XHTML 1. 0, SVG 1. 0, and a number of more specialized XML languages. The mirrors are all synthesized automatically from document type definitions (DTDs). Each element in a mirror is represented by a named function in Scheme. The mirror functions validate the XML document while it is generated. The validation is based on final state automata that are automatically derived from the DTD...|$|E
40|$|Functional {{programming}} fits {{well with}} the use of <b>descriptive</b> <b>markup</b> in HTML and XML. There is also a good fit between S-expressions in Lisp and the means of expression in HTML and XML. These similarities are exploited in LAML (Lisp Abstracted Markup Language) which is a software package for Scheme. LAML supports exact mirrors of different versions of HTML. In the mirrors each HTML element is represented by a named Scheme function. The mirror functions guarantee that the generated HTML code is valid. LAML has been used for both server side CGI programming and programmatic authoring of non-trivial static web materials. The programmatic LAML author can use the power of functional programming for the production of everyday web documents. Equally important, it is straightforward to define domain-specific web languages in Scheme syntax which parallel the advantages of XML. ...|$|E
40|$|For some time, {{theorists and}} {{practitioners}} of <b>descriptive</b> <b>markup</b> {{have been aware}} that the strict hierarchical organization of elements provided by SGML and XML represents a potentially problematic abstraction. The nesting structures of SGML and XML capture an important property of real texts and represent a successful combination of expressive power and tractability. But not all textual phenomena appear in properly nested form, and {{for more than twenty}} years students of markup have been exploring methods of recording overlapping (non-hierarchical) structures. Useful surveys include (Barnard et al. 1995), (DeRose 2004), and (Witt et al. 2005). Some approaches to the overlap problem take the form of non-SGML, non-XML syntaxes and non-tree-like data structures. One example is offered by the TexMecs syntax and Goddag data structures proposed by the project Markup Languages for Complex Documents (MLCD) based at the University of Bergen. Another i...|$|E
40|$|For a {{document}} collection in which structural elements are identified with markup, {{it is often}} necessary to construct a grammar retrospectively that constrains element nesting and ordering. This has been addressed by others as an application of grammatical inference. We describe an approach based on stochastic grammatical inference which scales more naturally to large data sets and produces models with richer semantics. We adopt an algorithm that produces stochastic finite automata and describe modifications that enable better interactive control of results. Our experimental evaluation uses four document collections with varying structure. Keywords: stochastic grammatical inference, text database structure 1. Introduction 1. 1. Text structure For electronically stored text, there are well known advantages to identifying structural elements (e. g., chapters, titles, paragraphs, footnotes) with <b>descriptive</b> <b>markup</b> [4, 5, 12]. Most commonly, markup {{is in the form}} of labeled tags interl [...] ...|$|E
40|$|AbstractIn this paper, {{we propose}} a documental {{approach}} {{to the development of}} graphical adventure videogames. This approach is oriented to the production and maintenance of adventure videogames using the game’s storyboard as the key development element. The videogame storyboard is marked up with a suitable domain-specific <b>descriptive</b> <b>markup</b> language, from which the different art assets that are needed are referred to, and then the final executable videogame itself is automatically produced by processing the marked storyboard with a suitable processor for such a language. This document-oriented approach opens new authoring possibilities in videogame development and allows a rational collaboration between the different communities that participate in the development process: game writers, artists, and programmers. We have implemented the approach {{in the context of the}} project, by defining a suitable markup language for the storyboards (the language), and by building a suitable processor for this language (the engine) ...|$|E
40|$|Sustained {{growth in}} the Internet has lead to a {{dramatic}} increase in the amount of electronic information available to users. Tools designed to help users discover new information are the key factors in deriving the maximum benefit from a large-scale information sharing system. However, many tools currently available lack the features necessary for coping with a burgeoning volume of information, such as selectively filtering interesting information, and automatically organizing it for efficient browsing. In this paper we describe the Selective USENET Retrieval Facility (SURF), a prototype Netnews information filter. We discuss methods of applying <b>descriptive</b> <b>markup</b> techniques to provide a consistent organization for the filtered information. We also describe the information retrieval models used in the filter, as well as domain specific methods of improving filtering effectiveness. 1 Introduction Since its initial deployment 25 years ago, the Internet has become an indispensable tool f [...] ...|$|E
40|$|Introduction L aT E X {{is not a}} word processor, in the {{conventional}} sense. It is a typesetting system, or document processing system, developed by Leslie Lamport [1]. L aT E X uses a set of special commands [...] - or "macros" [...] - based on Donald E. Knuth's T E X program [2, 3]. L aT E X is also not a WYSIWYG (What You See Is What You Get) system. It is a <b>descriptive</b> <b>markup</b> system. The user creates an input file (with any text editor) which consists only of ASCII characters. This input file contains {{the text of the}} document and the L aT E X formatting commands [3]. Then, the file is processed by the L aT E X software into a recognizable form which may be printed. L aT E X is a powerful typesetting system, which can produce very complex documents. This manual, however, will be concerned with only the relatively simple commands needed to produce a `mid-term paper' sized repo...|$|E
40|$|More {{and more}} large {{repositories}} of texts {{which must be}} automatically processed rep-resent their content {{through the use of}} <b>descriptive</b> <b>markup</b> languages. This method has been diffused by the availability of widely adopted standards like SGML and, later, XML, which made possible the definition of specific formats for many kinds of text, from literary texts (TEI) to web pages (XHTML). The markup approach has, however, several noteworthy shortcomings. First, we can encode easily only texts with a hierarchical structure, then extra-textual information, like metadata, can be tied only to the same structure of the text and must be expressed as strings of the markup language. Third, queries and programs for the retrieval and pro-cessing of text must be expressed in terms of languages where every document is represented as a tree of nodes; for this reason, in documents where parallel, overlap-ping structures exists, the complexity of such programs becomes significantly higher. Consider, for instance, a collection of classical lyrics, with two parallel hierarchies lyric> stanzas> verses> words, and lyric> sentences> words, with title and in...|$|E
40|$|In {{working on}} the New OED project, we, like many other researchers, have {{wrestled}} with large, intricate bodies of text. Based on this exposure, we have begun to investigate {{the similarities and differences}} between managing conventional business data and managing reference text data. The paper begins with the claim that text can support complex models of the real world that cannot be captured more formally. Thus important information resources must be held as text, but the very absence of a formal model makes it difficult to identify the structures present in a text. A common text structuring technique is <b>descriptive</b> <b>markup,</b> which introduces tags into a text stream. We present three views of tagged text: one based on tags as text, one on arbitrarily interleaved tags with text, and one on constrained tag placement in the text. Throughout the discussion, examples are drawn from our experience with the OED. 1. Text as a model The role of a database is to model an enterprise, so that whe [...] ...|$|E
40|$|Language {{documentation}} faces {{challenges of}} data preservation and accessibility. Data can be lost due to physical deterioration (e. g. field notes or tape recordings) or outdated format (e. g. Microsoft Word 3. 0). Archived data is typically difficult to access, {{and it is}} sometimes found that the archived information is inadequate for research purposes. Increased interest in language documentation has coincided with advancements in digital technologies, offering hope for meeting these challenges. This paper discusses the archiving of a 204 -item wordlist of Ngbugu, an Ubangian language spoken in Central African Republic, employing best practice recommendations. Our solution includes: TIFF digital imaging of the original handwritten transcription, WAV digital recording of the wordlist, <b>descriptive</b> <b>markup</b> encoding of the wordlist in XML employing Unicode transcription, viewing and playback via an XSLT style sheet that renders the information in HTML, publishing metadata for resource discovery with the Open Language Archives Community (OLAC), and depositing the original materials and digital representations in an institutional archive committed to long-term preservation and access...|$|E
