29|10000|Public
5000|$|Data {{in transit}} is also {{referred}} to as <b>data</b> <b>in</b> <b>motion,</b> [...] and data in flight [...]|$|E
50|$|Other {{applications}} of CGI computer simulations {{are being developed}} to graphically display large amounts of <b>data,</b> <b>in</b> <b>motion,</b> as changes occur during a simulation run.|$|E
50|$|Data encryption, which {{prevents}} data visibility {{in the event}} of its unauthorized access or theft, is commonly used to protect <b>data</b> <b>in</b> <b>motion</b> and increasingly promoted for protecting data at rest.|$|E
5000|$|Secure Islands Technologies {{offers for}} {{large-scale}} data sources {{a set of}} built-in data interceptors which capture <b>data</b> at creation, <b>in</b> <b>motion,</b> <b>in</b> use, and at rest - identifying <b>data</b> <b>in</b> need of protection based on content-driven, pre-defined policies, and submitting for automatic classification and protection by IQProtector Suite.|$|R
40|$|In this work, {{a highly}} {{parallel}} and flexible framework {{based on a}} multicore processor which is especially optimized for computation-intensive execution is proposed to accelerate motion estimation for HEVC. Using multilevel on-chip communication mechanism greatly enhances efficiency and flexibility of <b>data</b> exchange <b>in</b> <b>motion</b> estimation. Experimental results not only validate the feasibility of the framework, but also show that ME achieves 8. 5 times speedup comparing to typical frameworks while 16 cores are utilized...|$|R
50|$|The central {{object of}} a motion chart is a blob (or bubble), {{which is a}} solid object homeomorphic to a disc. Blobs have 3 {{important}} characteristics - size, position and appearance. Using variable mapping, motion charts allow control over {{the appearance of the}} blobs at different time points. This mechanism enhances the dynamic appearance of the <b>data</b> <b>in</b> the <b>motion</b> chart and facilitates the visual inspection of associations, patterns and trends in multivariate datasets.|$|R
5000|$|For {{the most}} part, iSCSI {{operates}} as a cleartext protocol that provides no cryptographic protection for <b>data</b> <b>in</b> <b>motion</b> during SCSI transactions. As a result, an attacker who can listen in on iSCSI Ethernet traffic can: ...|$|E
50|$|Encryption, which {{prevents}} data visibility {{in the event}} of its unauthorized access or theft, is commonly used to protect <b>Data</b> <b>in</b> <b>Motion</b> and Data at Rest and increasingly recognized as an optimal method for protecting Data in Use.|$|E
5000|$|Data architecture: The data {{structures}} used by {{a business}} and/or its applications. Descriptions of data in storage and <b>data</b> <b>in</b> <b>motion.</b> Descriptions of data stores, data groups and data items. Mappings of those data artifacts to data qualities, applications, locations etc.|$|E
40|$|Abstract: <b>Data</b> hiding <b>in</b> {{compressed}} video. We target, {{finding the}} macro blocks randomly by Pseudo Random Generation Algorithm. Then encoded {{the video and}} reconstructed the frames both forward predictive (P) -frame and bidirectional (B) -frame in compressed video, which are very secured and Compared to <b>data</b> hiding <b>in</b> <b>motion</b> vectors of compressed video. A every frame is searched for achieve to robustness. The secret message bit stream {{is embedded in the}} random macro block in each frame of compressed video. The method is implemented and tested for hiding <b>data</b> <b>in</b> order of multiple GOP’s and the results are evaluated. The proposed method is performing well...|$|R
40|$|Abstract:-This paper aims {{to explain}} the <b>data</b> hiding concept <b>in</b> <b>motion</b> vector of {{compressed}} video. <b>In</b> this <b>data</b> hiding <b>in</b> <b>motion</b> vector is done by stenography Technique, <b>data</b> is compressed <b>in</b> different frames of video. The process starts with Mailing system such as sending and receiving secret <b>data.</b> <b>In</b> that hiding <b>data</b> <b>in</b> natural sequence of multiple groups of pictures. The RSA algorithm is used for encryption of message in video and use edge detection mechanism for selecting pixel, Data is encoded as a region where motion estimation is allowed to generate motion vector. The sender first uses the stenographic application for encrypting the secret message. For this encryption, the sender uses text document <b>in</b> which the <b>data</b> is written and the image as a carrier file in which the secret message or text document to be hidden. The sender sends the carrier image and text document to the encryption phase for <b>data</b> embedding, <b>in</b> which the text document is embedded into the image file. In encryption phase, the data is embedded into carrier file which was protected. The decryption phase decrypts the original text document using the least significant bit decoding and decrypts the original message. The performance analysis shows that the algorithm ensures better security against attackers Keywords—stenography, RSA algorithm, secret message, carrier file, video. I...|$|R
40|$|International audienceOver {{the past}} decade, many {{research}} ﬁelds have real- ized the beneﬁts of motion capture data, {{leading to an}} ex- ponential growth {{of the size of}} motion databases. Conse- quently indexing, querying, and retrieving motion capture data have become important considerations in the usability of such databases. Our aim is to efﬁciently retrieve mo- tion from such databases in order to produce real-time an- imation. For that purpose, we propose a new database ar- chitecture which structures both the semantic and raw <b>data</b> contained <b>in</b> <b>motion</b> <b>data.</b> The performance of the overall architecture is evaluated by measuring the efﬁciency of the <b>motion</b> retrieval process, <b>in</b> terms of the mean time access to the data...|$|R
50|$|Network (<b>data</b> <b>in</b> <b>motion)</b> {{technology}} is typically installed at network egress points near the perimeter. It analyzes network traffic to detect sensitive data {{that is being}} sent in violation of information security policies. Multiple security control points may report activity to be analyzed by a central management server.|$|E
5000|$|VNS3 is a software-only virtual {{appliance}} that {{allows users to}} control access and network topology and secure <b>data</b> <b>in</b> <b>motion</b> across public and private clouds VNS3 is a virtual router, switch, firewall, protocol re-distributor, and SSL/IPSec VPN concentrator. The Network Virtualization Software creates a customer controlled overlay network over top of the underlying network backbone ...|$|E
5000|$|Velocity of Data: The {{velocity}} of data {{in social media}} {{can be divided into}} two categories: data at rest and <b>data</b> <b>in</b> <b>motion.</b> Dimensions of {{velocity of}} <b>data</b> <b>in</b> <b>motion</b> can answer questions such as: How the sentiment of the general population is changing about the players during the course of match? Is the crowd conveying positive sentiment about the player who is actually losing the game? In these cases, the analysis is done as arrives. In this analysis, the amount of detail produced is directly correlated to the complexity of the analytical tool or system. A highly complex tool produces more amounts of details. The second type of analysis in the context of velocity is an analysis of data at rest. This analysis is performed once the data is fully collected. Performing this analysis can provide insights such as; which of your company's products has the most mentions as compared to others? What is the relative sentiment around your products as compared to a competitor's product? ...|$|E
40|$|Purpose: The {{concept of}} open-architecture control, {{which has been}} used as a main method of control in many {{branches}} such as automatic systems of machining, robotics, testing and control, causes a great increment in efficiency and precision of control systems and dynamic capability to industrial control networks. Design/methodology/approach: The high execution costs in one hand and unnecessary usage of all OAC standard components on the other hand, makes the modeling of OAC concept impossible for such systems. Findings: This paper tries to represent a simple system based on elimination of unessential elements of open-architecture to reach a limited and efficient control system. Research limitations/implications: In hierarchical architecture systems, regarding to the synchronous processes of calculation and controlling, the total execution time is very low in comparison with similar serial system. In spite of this, the execution time of synchronous processes in serial architecture is lower than hierarchical one. Practical implications: The utilization of one processor instead of multiple one, merging the industrial control network layer into system management layer and omission of processor execution layer in each axis and optimized using of multi-tasking capability of processor. Originality/value: The analysis of simulated <b>data</b> <b>in</b> <b>motions</b> with higher degrees of freedom shows that the usage of CNC machines with serial architecture for higher than 3 -degrees of freedom is unaccepted due to large increment in total machining time...|$|R
40|$|The {{outline of}} a {{research}} effort in probabilistic approaches to robot motion determination is presented. A heuristic path planner that has met with considerable success in the JPL telerobot testbed is presented together with an interpretation of its performance. The relevance of techniques from stochastic geometry and stochastic diffusion is addressed, as {{is the possibility of}} using sensor <b>data</b> directly <b>in</b> the <b>motion</b> determination process...|$|R
50|$|NNGs for {{points in}} the plane {{as well as in}} multidimensional spaces find applications, e.g., <b>in</b> <b>data</b> compression, <b>motion</b> planning, and {{facilities}} location. In statistical analysis, the nearest-neighbor chain algorithm based on following paths in this graph can be used to find hierarchical clusterings quickly. Nearest neighbor graphs are also a subject of computational geometry.|$|R
5000|$|IBM Virtualization Engine TS7700 series - The TS7700 is {{a virtual}} tape library for System z (mainframe) that uses disk drives for cache to {{accelerate}} backup operations. The design is intended to protect data while having shorter backup windows. End-to-end encryption protects <b>data</b> <b>in</b> <b>motion,</b> on cache hard drives and on tape. TS7740 and TS7720 are designed to speed up tape backups and restores by using a tiered hierarchy of disk and tape to make more efficient use of tape drives.|$|E
50|$|Because of its nature, Data in Use is of {{increasing}} concern to businesses, {{government agencies and}} other institutions. Data in use, or memory, can contain sensitive data including digital certificates, encryption keys, intellectual property (software algorithms, design data), and personally identifiable information. Compromising data in use enables access to encrypted data at rest and <b>data</b> <b>in</b> <b>motion.</b> For example, someone with access to random access memory can parse that memory to locate the encryption key for data at rest. Once they have obtained that encryption key, they can decrypt encrypted data at rest. Threats to data in use can {{come in the form}} of cold boot attacks, malicious hardware devices, rootkits and bootkits.|$|E
30|$|Besides {{the data}} {{abundance}} and different formats, the rapid flow of data has also attracted {{the researchers to}} find mechanisms to manage the <b>data</b> <b>in</b> <b>motion.</b> Typically this is to consider that, how quickly the data is produced and stored, and its associated rates of retrieval and processing. This idea of <b>data</b> <b>in</b> <b>motion</b> is evoking far more interest than the conventional definitions, and needs {{a new way of}} thinking to solve the problem [11]. This is not associated only with the growth rate at the data acquisition end, but also data-flow rate during transmission; as well the speed at which data is processed and stored in the data repositories. Any way, we are {{aware of the fact that}} today’s enterprises have to deal with petabytes instead of terabytes; and the increase in smart object technology alongside the streaming information has led the constant flow of data at a pace that has threatened the traditional data management systems [11]. RDBMS use two-dimensional tables to represent data and use multi-join transactional queries for the database consistency. Although they are mature and still useful for many applications, but processing of volumes of data using multi-joins is prone to performance issues [12, 13]. This problem is evident when extensive data processing is required to find hidden useful information in huge data volumes; but such data mining techniques are not in our current focus [14 – 17], as we limit our discussion to NoSQL temporal modeling and schema based data integration.|$|E
40|$|This paper {{proposes a}} {{parallel}} architecture for {{estimation of the}} motion of an underwater robot. It {{is well known that}} image processing requires a huge amount of computation, mainly at low-level processing where the algorithms are dealing with a great number of <b>data.</b> <b>In</b> a <b>motion</b> estimation algorithm, correspondences between two images have to be solved at the low level. In the underwater imaging, normalised correlation can be a solution in the presence of non-uniform illumination. Due to its regular processing scheme, parallel implementation of the correspondence problem can be an adequate approach to reduce the computation time. Taking into consideration the complexity of the normalised correlation criteria, a new approach using parallel organisation of every processor from the architecture is propose...|$|R
30|$|Prospective {{conformal}} <b>data</b> sorting <b>in</b> PET: <b>Motion</b> correction {{is commonly}} employed via sorting data {{based on a}} patient’s motion characterization, and the sorting windows, for example, an “optimal bin,” are usually generally defined and/or based on population-derived motion characteristic studies. On the other hand, real-time motion characterization can enable full-time motion assessment, {{which can be used}} for data-driven conformal sorting [17]. This approach implements data-driven processing after acquisition of the motion signal and has the advantage of being a more patient/scan-specific solution.|$|R
40|$|Abstract — Tracking of <b>motion</b> <b>in</b> weightlifting is a {{challenging}} task. This paper presents {{a method for}} tracking Weightlifting bar movement of an athlete. The proposed method uses three different template creation techniques for tracking and applies Wilcoxon Signed rank test to evaluate the results. The method was tested with normally captured data (30 fps) and <b>data</b> captured <b>in</b> slow <b>motion</b> (100 fps) separately. For the normally captured data, combined dynamic templates method has the highest accuracy of 95 % in tracking the trajectory. For slow motion data Wilcoxon Signed Rank generated equal results for all three template creation methods...|$|R
40|$|Big Data, {{more than}} ever, {{is playing a}} vital role in IT {{decision}} making, with such decisions increasingly moving towards being made in real-time. Organisations are optimizing service performance, better handling capacity across overall organisations, and effectively making decisions utilising operational analytics. Realizing the full value of business data is a key challenge for today’s operational analytics. Complex Event Processing (CEP) is a technique for tracking, analyzing, and processing data as an event happens and is useful for Big Data because it is intended to manage <b>data</b> <b>in</b> <b>motion.</b> <b>Data</b> <b>in</b> <b>motion</b> is processed and communicated based on business rules and processes. For decisions to be better-informed, data used for decision making has to be timely, complete, accurate, trusted, valid, reliable, and relevant. CEP utilizes data generated from moment-to-moment from different emerging sources such as sensor, sentiment, geo-locational, etc… There is a need {{to bridge the gap between}} traditional business intelligence with new Big Data technologies such as CEP. Bridging of this gap will enable organisations to become agile and data-driven so that business outcomes can be maximized by delivering better-informed decisions about a customer and delivering a better service to them. In this paper we discuss the architecture developed for CEP using open source technologies and show how CEP is applied to the use case of an Electronic Coupon Distribution Service (ECDS), using location information, past shopping/travel history, gender, likes/dislikes, etc… We further explore how different types of data such as static information (gender, age, etc.), previous history (where the person travelled to, what they bought, etc.), as well as real-time information about a customer (current location, current shopping habits, etc.) would all be utilised in CEP...|$|E
40|$|The {{emergence}} {{cloud computing}} in the computing arena has had major effect in way we utilize computing resources. It is being heralded {{by many as}} the new computing paradigm, coming with disruptive technologies which are expected to foster all sorts of innovations. However, further investigations suggest that cloud computing it is nothing new, rather an evolution of different existing technologies creatively integrated together. Therefore, it has inherited strengths and weaknesses of existing technologies, but has lowered the entry bar to computing making it an interesting proposition. In this paper we propose a security wrapper, which affords enough protection to the classified data as it flows in the cloud. The wrapper offers security to <b>data</b> <b>in</b> <b>motion</b> and at rest, and incorporates adaptive SLA negotiation...|$|E
40|$|Rapid {{growth in}} the {{automation}} gives rise to emergence of the database having different flavors. Taking consideration of various domains; Database {{is the most important}} component in the world of software. As today’s world is moving on the factors of dependant on the other technology or with other organization which gives rise to the concept of ‘Hybrid’ terminology. In order to keep the data secure and preventing from unauthorized use, we need some of the technology likewise one of them is Encryption technology. In this paper, we likely to give stress on the concept of moving data or in other terminology <b>data</b> <b>in</b> <b>motion</b> either retrieved or transitive data to keep secure saving of data in the database having different location and different combination of databas...|$|E
40|$|Increasingly {{pervasive}} {{networks are}} leading towards {{a world where}} <b>data</b> is constantly <b>in</b> <b>motion.</b> <b>In</b> such a world, conventional techniques for query processing, which were developed under the assumption of a far more static and predictable computational environment, will not be sufficient. Instead, query processors based on adaptive dataflow will be necessary. The Telegraph project has developed a suite of novel technologies for continuously adaptive query processing. The next generation Telegraph system, called TelegraphCQ, is focused on meeting the challenges that arise in handling large streams of continuous queries over high-volume, highly-variable <b>data</b> streams. <b>In</b> this paper, we describe the system architecture and its underlying technology, and report on our ongoing implementation effort, which leverages the PostgreSQL open source code base. We also discuss open issues and our research agenda. ...|$|R
40|$|The {{accuracy}} of measuring voxel intensity changes between stimulus and rest images in fMRI echo-planar imaging (EPI) data is severely degraded {{in the presence}} of head <b>motion.</b> <b>In</b> addition, EPI is sensitive to susceptibility-induced geometric distortions. Head motion causes image shifts and associated field map changes that induce different geometric distortion at different time points. Conventionally, geometric distortion is “corrected” with a static field map independently of image registration. That approach ignores all field map changes induced by head motion. This work evaluates the improved motion correction capability of mapping slice to volume with concurrent iterative field corrected reconstruction using updated field maps derived from an initial static field map that has been spatially transformed and resampled. It accounts for motion-induced field map changes for translational and in-plane rotation motion. The results from simulated EPI time series <b>data,</b> <b>in</b> which <b>motion,</b> image intensity and activation ground truths are available, show improved accuracy in image registration, field corrected image reconstruction and activation detection...|$|R
40|$|Abstract: This paper {{presents}} a compensation method for an accelerometer to measure acceleration data accurately when a robot manipulator moves slowly. Although the accelerometer works fine under the fast movement of a robot manipulator, low cost accelerometers provide relatively inaccurate acceleration data under slow movements. In order {{to correct the}} error of the sensor <b>data</b> <b>in</b> the slow <b>motion,</b> correction factors are obtained experimentally. Then those corrected data are used for the disturbance observer. Experimental studies of the position control of a robot manipulator are conducted by applying the DOB (Disturbanc...|$|R
40|$|Modern {{analytics}} solutions succeed {{to understand}} and predict phenomenons in a large diversity of software systems, from social networks to Internet-of-Things platforms. This success challenges analytics algorithms to deal {{with more and more}} complex data, which can be structured as graphs and evolve over time. However, the underlying data storage systems that support large-scale data analytics, such as time-series or graph databases, fail to accommodate both dimensions, which limits the integration of more advanced analysis taking into account the history of complex graphs, for example. This paper therefore introduces a formal and practical definition of temporal graphs. Temporal graphs pro- vide a compact representation of time-evolving graphs {{that can be used to}} analyze complex <b>data</b> <b>in</b> <b>motion.</b> In particular, we demonstrate with our open-source implementation, named GREYCAT, that the performance of temporal graphs allows analytics solutions to deal with rapidly evolving large-scale graphs...|$|E
40|$|Abnormal crowd {{behaviour}} detection {{attracts a}} large interest {{due to its}} importance in video surveillance scenarios. However, the ambiguity {{and the lack of}} sufficient "abnormal" ground truth data makes end-to-end training of large deep networks hard in this domain. In this paper we propose to use Generative Adversarial Nets (GANs), which are trained to generate only the "normal" distribution of the data. During the adversarial GAN training, a discriminator "D" is used as a supervisor for the generator network "G" and vice versa. At testing time we use "D" to solve our discriminative task (abnormality detection), where "D" has been trained without the need of manually-annotated abnormal data. Moreover, in order to prevent "G" learn a trivial identity function, we use a cross-channel approach, forcing "G" to transform raw-pixel <b>data</b> <b>in</b> <b>motion</b> information and vice versa. The quantitative results on standard benchmarks show that our method outperforms previous state-of-the-art methods in both the frame-level and the pixel-level evaluation...|$|E
40|$|With {{the systems}} and {{applications}} migration {{from the traditional}} enterprise data center infrastructures to the virtualisation of cloud computing infrastructure, there are changes required in term {{of the way of}} system security and data security management. There are more various sources of threats to the data integrity preservation which may come from internal employees, external users, cloud providers and the vendor of cloud providers. Among the way to ensure the data is not leaked out is by looking at data loss prevention tool. The tool should be protecting data at all the three common states namely <b>data</b> <b>in</b> <b>motion,</b> data in transmit and data at rest. The thesis intention is to find out the effectiveness of using open source data loss prevention in cloud computing infrastructure. In addition, the thesis would also study the security vulnerabilities on the open source DLP deployment architecture system and propose the improved architecture setup. While the effectiveness evaluation is done using open source, there is also a need to find the market leading commercial data loss prevention tool and identified the market strength...|$|E
40|$|The {{primary focus}} of {{existing}} secure cloud storage solutions have been on securing <b>data</b> both <b>in</b> <b>motion</b> and at rest. These storage solutions mostly focus on three essential properties: confidentiality, integrity and availability. However, modern enterprise applications demand data can be shared within or across organizations. The challenge is how to securely share <b>data</b> <b>in</b> public clouds using federated identities without increasing data movement and computation costs. Furthermore, the consumer {{should be able to}} delete their <b>data</b> <b>in</b> the cloud in the context of collaboration without leaving any traces behind. This problem has been addressed in recent times by utilizing or developing new data encryption techniques such as identitybased encryption, attribute-based encryption and proxy-re-encryption. However, these techniques suffer from scalability and flexibility problems when dealing with big data and support for dynamic and federated access control. This paper presents a novel architecture and corresponding protocols to provide secure sharing and deletion of documents on public cloud services: CloudDocs. This system uses AES for data encryption to achieve scalability, supports identity-based access control rules using private-public key pairs to provide flexibility, and uses independent key management services to support secure deletion, whereby the data is irrecoverable once the keys are destroyed. The key management service also supports dynamic and federated access control. fals...|$|R
30|$|Four {{different}} {{studies using}} static (ST), gated (GA), non-motion-corrected (NMC), and motion-corrected (MC) data were performed. For the ST study, the PET <b>data,</b> <b>in</b> which neither <b>motion</b> nor noise was introduced, were reconstructed using filtered back-projection (FBP) for each time frame. For the GA study, only the data {{corresponding to the}} reference motion phase were reconstructed using FBP. For the NMC study, the PET <b>data</b> generated <b>in</b> each <b>motion</b> phase were first reconstructed using FBP in each time frame. The resulting PET images across all the motion phases were then summed for the time frame. For the ST, GA, and NMC studies, the attenuation map <b>in</b> the reference <b>motion</b> phase was used during the PET reconstruction. For the MC study, the attenuation map for a given motion phase was obtained by transforming the reference attenuation map to the motion phase using the corresponding motion fields measured by MRI. The reconstructed PET image by FBP for each motion phase was transformed to the reference motion phase using the motion fields measured by MRI in each time frame. The final motion-corrected PET image in the time frame was then obtained by summing up all the transformed PET images (including the one <b>in</b> the reference <b>motion</b> phase). Because the goal of our study is to assess only the impact of motion correction (including correction on both the emission data and attenuation map) on dynamic PET, we used the same attenuation map, which was generated by the XCAT phantom, for both the simulations and reconstructions.|$|R
40|$|One of the {{limitations}} of Original Lucid is its inability to deal easily with multidimensional data structures such as arrays and trees. Over the past ten years, Lucid has evolved sufficiently to express and manipulate multidimensional data structures that change [...] - thus making multidimensional problem solving naturally possible in the latest version of Lucid, called Indexical Lucid. In this report, we trace the evolution of Lucid from 1984 to its current form. We discuss the novel aspects of Indexical Lucid by contrasting it with Original Lucid. We also illustrate the expressiveness of Indexical Lucid by solving selected multidimensional problems. 1 Background Lucid (Original Lucid) was invented in 1976 by Ashcroft and Wadge as a system for writing and proving properties about programs [2, 3]. The most unusual aspect of Lucid, in addition to being non-procedural, is its dynamic view of computation: one <b>in</b> which <b>data</b> is <b>in</b> <b>motion</b> that is generated and consumed by stationary operation [...] ...|$|R
