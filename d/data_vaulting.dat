1|109|Public
50|$|Hybrid on-site and {{off-site}} <b>data</b> <b>vaulting,</b> sometimes {{known as}} Hybrid Online Backup, involve {{a combination of}} Local backup for fast backup and restore, along with Off-site backup for protection against local disasters. According to Liran Eshel, CEO of CTERA Networks, this ensures that the most recent data is available locally {{in the event of}} need for recovery, while archived data that is needed much less often is stored in the cloud.|$|E
50|$|<b>Data</b> <b>Vault</b> 2.0 {{is the new}} {{specification}}, it is an open standard. The new specification contains components which {{define the}} implementation best practices, the methodology (SEI/CMMI, Six Sigma, SDLC, etc..), the architecture, and the model. <b>Data</b> <b>Vault</b> 2.0 has a focus on including new components such as Big Data, NoSQL - and also focuses on performance of the existing model. The old specification (documented here for the most part) is highly focused on <b>data</b> <b>vault</b> modeling. It is documented in the book: Building a Scalable Data Warehouse with <b>Data</b> <b>Vault</b> 2.0.|$|R
40|$|Efficient {{management}} and exploration of high-volume scientific file repositories have become pivotal for advancement in science. We propose {{to demonstrate the}} <b>Data</b> <b>Vault,</b> {{an extension of the}} database system architecture that transparently opens scientific file repositories for efficient in-database processing and exploration. The <b>Data</b> <b>Vault</b> facilitates science <b>data</b> analysis using high-level declarative languages, such as the traditional SQL and the novel array-oriented SciQL. Data of interest are loaded from the attached repository in a just-in-time manner without need for up-front data ingestion. The demo is built around concrete implementations of the <b>Data</b> <b>Vault</b> for two scientific use cases: seismic time series and Earth observation images. The seismic <b>Data</b> <b>Vault</b> uses the queries submitted by the audience to illustrate the internals of <b>Data</b> <b>Vault</b> functioning by revealing the mechanisms of dynamic query plan generation and on-demand external data ingestion. The image <b>Data</b> <b>Vault</b> shows an application view from the perspective of data mining researchers...|$|R
50|$|Teams {{using the}} <b>data</b> <b>vault</b> {{methodology}} will automatically adopt to the repeatable, consistent, and measurable {{projects that are}} expected at CMMI Level 5. Data that flow through the EDW <b>data</b> <b>vault</b> system will begin to follow the TQM (total quality management) life-cycle {{that has long been}} missing from BI (business intelligence) projects.|$|R
5000|$|Another way {{to think}} of a <b>data</b> <b>vault</b> model is as a graph model. The <b>data</b> <b>vault</b> model {{actually}} provides a [...] "graph based" [...] model with hubs and relationships in a relational database world. In this manner, the developer can use SQL to get at graph based relationships with sub-second responses.|$|R
50|$|The <b>data</b> <b>vault</b> {{modeling}} components follow {{hub and spokes}} architecture. This modeling {{style is}} a hybrid design, consisting of the best practices from both third normal form and star schema. The <b>data</b> <b>vault</b> model is not a true third normal form, and breaks some of its rules, {{but it is a}} top-down architecture with a bottom up design. The <b>data</b> <b>vault</b> model is geared to be strictly a data warehouse. It is not geared to be end-user accessible, which when built, still requires the use of a data mart or star schema based release area for business purposes.|$|R
5000|$|... #Subtitle level 2: Connection between exchanging, authoring/illustrating {{environment}} and <b>data</b> <b>vault</b> ...|$|R
50|$|Personal {{spent two}} years {{building}} the Personal Platform before launching its <b>Data</b> <b>Vault</b> product in beta in November 2011. Following Privacy by Design principles, Personal only enabled users to see or share the sensitive data and all the files they stored in their <b>Data</b> <b>Vault.</b> Such information was encrypted, and could only be decrypted with a user’s password. Only users could choose and know their passwords to their vault because Personal did not store user passwords - and therefore could not reset them without deleting a user’s sensitive data and all files stored in their vault. All Personal apps and services were linked to a user’s private <b>Data</b> <b>Vault.</b>|$|R
5000|$|<b>Data</b> <b>vaults</b> {{philosophy}} {{is that all}} data is relevant data, {{even if it is}} not in line with established definitions and business rules. If data are not conforming to these definitions and rules then that is a problem for the business, not the data warehouse. The determination of data being [...] "wrong" [...] is an interpretation of the data that stems from a particular point of view that may not be valid for everyone, or at every point in time. Therefore the <b>data</b> <b>vault</b> must capture all data and only when reporting or extracting data from the <b>data</b> <b>vault</b> is the <b>data</b> being interpreted.|$|R
40|$|The <b>data</b> <b>vault</b> model {{natively}} supports {{data and}} schema evolution, {{so it is}} often adopted to create operational data stores. However, it can hardly be directly used for OLAP querying. In this paper we propose an approach called Starry Vault for finding a multidimensional structure in <b>data</b> <b>vaults.</b> Starry Vault builds on the specific features of the <b>data</b> <b>vault</b> model to automate multidimensional modeling, and uses approximate functional dependencies to discover out of data the information necessary to infer the structure of multidimensional hierarchies. The manual intervention by the user is limited to some editing of the resulting multidimensional schemata, which makes the overall process simple and quick enough to be compatible with the situational analysis needs of a data scientist...|$|R
40|$|MSc (Computer Science), North-West University, Potchefstroom Campus, 2014 This study {{compares the}} impact of {{dimensional}} modelling and <b>data</b> <b>vault</b> modelling on the performance and maintenance effort of data warehouses. Dimensional modelling is a data warehouse modelling technique pioneered by Ralph Kimball in the 1980 s that is much more effective at querying large volumes of data in relational databases than third normal form <b>data</b> models. <b>Data</b> <b>vault</b> modelling {{is a relatively new}} modelling technique for data warehouses that, according to its creator Dan Linstedt, was created in order to address the weaknesses of dimensional modelling. To date, no scientific comparison between the two modelling techniques have been conducted. A scientific comparison was achieved in this study, through the implementation of several experiments. The experiments compared the data warehouse implementations based on dimensional modelling techniques with data warehouse implementations based on <b>data</b> <b>vault</b> modelling techniques in terms of load performance, query performance, storage requirements, and flexibility to business requirements changes. An analysis of the results of each of the experiments indicated that the <b>data</b> <b>vault</b> model outperformed the dimensional model in terms of load performance and flexibility. However, the dimensional model required less storage space than the <b>data</b> <b>vault</b> model. With regards to query performance, no statistically significant differences existed between the two modelling techniques. Master...|$|R
50|$|The <b>data</b> <b>vault</b> {{methodology}} {{is based}} on SEI/CMMI Level 5 best practices. It includes multiple components of CMMI Level 5, and combines them with best practices from Six Sigma, TQM, and SDLC. Particularly, it is focused on Scott Ambler's agile methodology for build out and deployment. <b>Data</b> <b>vault</b> projects have a short, scope-controlled release cycle and should consist of a production release every 2 to 3 weeks.|$|R
50|$|Data {{are never}} deleted from the <b>data</b> <b>vault,</b> {{unless you have}} a {{technical}} error while loading data.|$|R
50|$|<b>Data</b> <b>vault</b> {{modeling}} {{was originally}} conceived by Dan Linstedt in 1990 {{and was released}} in 2000 as a public domain modeling method. In a series of five articles on The Data Administration Newsletter the basic rules of the <b>Data</b> <b>Vault</b> method are expanded and explained. These contain a general overview, {{an overview of the}} components, a discussion about end dates and joins, link tables, and an article on loading practices.|$|R
50|$|Another {{issue to}} which <b>data</b> <b>vault</b> is a {{response}} is {{that more and more}} {{there is a need for}} complete auditability and traceability of all the data in the data warehouse. Due to Sarbanes-Oxley requirements in the USA and similar measures in Europe this is a relevant topic for many business intelligence implementations, hence the focus of any <b>data</b> <b>vault</b> implementation is complete traceability and auditability of all information.|$|R
5000|$|Data Vault-Code Gen Bundle: [...] Helps {{automate}} the code-generation {{process for}} building the <b>Data</b> <b>Vault</b> through Code automation Templates (CAT's).|$|R
50|$|The Personal Platform was a privacy- and security-by-design {{platform}} {{for individuals to}} manage and reuse their own data and information. The Fill It app was a 1-click form-filling solution for web and mobile logins, checkouts and forms, and the <b>Data</b> <b>Vault</b> app served as the main cloud-based repository for a user's data. Personal helped individuals take control and benefit from their information while knowing that the information in their <b>Data</b> <b>Vault</b> remained legally theirs {{and could not be}} used without their permission.|$|R
50|$|Reference {{tables are}} {{referenced}} from Satellites, but never bound with physical foreign keys. There is no prescribed structure for reference tables: use what works best in your specific case, ranging from simple lookup tables to small <b>data</b> <b>vaults</b> or even stars. They can be historical or have no history, {{but it is}} recommended that you stick to the natural keys and not create surrogate keys in that case. Normally, <b>data</b> <b>vaults</b> have a lot of reference tables, just like any other Data Warehouse.|$|R
50|$|SciQL an SQL-based query {{language}} for science applications with arrays as first class citizens. SciQL allows MonetDB to effectively {{function as an}} array database. SciQL {{is used in the}} European Union PlanetData and TELEIOS project, together with the <b>Data</b> <b>Vault</b> technology, providing transparent access to large scientific <b>data</b> repositories. <b>Data</b> <b>Vaults</b> map the <b>data</b> from the distributed repositories to SciQL arrays, allowing for improved handling of spatio-temporal data in MonetDB. SciQL will be further extended for the Human Brain Project.|$|R
5000|$|As a result, {{even very}} large file {{repositories}} can be efficiently analyzed, as only the required data is processed in the database. The {{data can be}} accessed through either the MonetDB SQL or SciQL interfaces. The <b>Data</b> <b>Vault</b> technology {{was used in the}} European Union's TELEIOS project, which was aimed at building a virtual observatory for Earth observation <b>data.</b> <b>Data</b> <b>Vaults</b> for FITS files have also been used for processing astronomical survey data for the The INT Photometric H-Alpha Survey (IPHAS) ...|$|R
50|$|Note {{that while}} it is {{relatively}} straightforward to move data from a <b>data</b> <b>vault</b> model to a (cleansed) dimensional model, the reverse is not as easy.|$|R
5000|$|<b>Data</b> <b>vault</b> {{modeling}} {{makes no}} distinction between good and bad data ("bad" [...] meaning not conforming to business rules). This is summarized in the statement that a <b>data</b> <b>vault</b> stores [...] "a single version of the facts" [...] (also expressed by Dan Linstedt as [...] "all the data, all of the time") {{as opposed to the}} practice in other data warehouse methods of storing [...] "a single version of the truth" [...] where data that does not conform to the definitions is removed or [...] "cleansed".|$|R
50|$|In 2007 KeepVault was {{an early}} OEM vendor for backup when Microsoft {{launched}} Windows Home Server. KeepVault was shipped on the HP <b>Data</b> <b>Vault</b> and HP Mediasmart servers.|$|R
50|$|<b>Data</b> <b>Vault</b> is a database-attached {{external}} file repository for MonetDB, {{similar to the}} SQL/MED standard. The <b>Data</b> <b>Vault</b> technology allows for transparent integration with distributed/remote file repositories. It is designed for scientific data data exploration and mining, specifically for remote sensing data. There is support for the GeoTIFF (Earth observation), FITS (astronomy), MiniSEED (seismology) and NetCDF formats.The data is stored in the file repository in the original format, and loaded in the database in a lazy fashion, only when needed. The system can also process the data upon ingestion, if the data format requires it.|$|R
5000|$|Reference {{tables are}} {{a normal part}} of a healthy <b>data</b> <b>vault</b> model. They are there to prevent {{redundant}} storage of simple reference data that is referenced a lot. More formally, Dan Linstedt defines reference data as follows:Any information deemed necessary to resolve descriptions from codes, or to translate keys in to (sic) a consistent manner. Many of these fields are [...] "descriptive" [...] in nature and describe a specific state of the other more important information. As such, reference data lives in separate tables from the raw <b>Data</b> <b>Vault</b> tables.|$|R
50|$|<b>Data</b> <b>Vault</b> 2.0 {{has arrived}} on the scene as of 2013 and brings to the table Big Data, NoSQL, unstructured, semi-structured {{seamless}} integration, along with methodology, architecture, and implementation best practices.|$|R
50|$|<b>Data</b> <b>vault</b> {{modeling}} is {{a database}} modeling method {{that is designed}} to provide long-term historical storage of data coming in from multiple operational systems. It is also a method of looking at historical data that deals with issues such as auditing, tracing of data, loading speed and resilience to change as well as emphasizing the need to trace where all the data in the database came from. This means that every row in a <b>data</b> <b>vault</b> must be accompanied by record source and load date attributes, enabling an auditor to trace values back to the source.|$|R
40|$|In {{this short}} paper we outline the <b>data</b> <b>vault,</b> a database-attached {{external}} file repository. It provides a true symbiosis between a DBMS and existing file-based repositories. Data {{is kept in}} its original format while scalable processing functionality is provided through the DBMS facilities. In particular, it provides transparent access to all data kept in the repository through an (array-based) query language using the file-type specific scientific libraries. The design space for <b>data</b> <b>vaults</b> is characterized by requirements coming from various fields. We present a reference architecture for their realization in (commercial) DBMSs and a concrete implementation in MonetDB for remote sensing data geared at content-based image retrieval...|$|R
50|$|This is {{an example}} of a {{reference}} table with risk categories for drivers of vehicles. It can be referenced from any satellite in the <b>data</b> <b>vault.</b> For now we reference it from satellite S_DRIVER_INSURANCE. The reference table is R_RISK_CATEGORY.|$|R
50|$|Another {{view is that}} a <b>data</b> <b>vault</b> model {{provides}} an ontology of the Enterprise {{in the sense that}} it describes the terms in the domain of the enterprise (Hubs) and the relationships among them (Links), adding descriptive attributes (Satellites) where necessary.|$|R
50|$|The ETL for {{updating}} a <b>data</b> <b>vault</b> {{model is}} fairly straightforward (see <b>Data</b> <b>Vault</b> Series 5 - Loading Practices). First {{you have to}} load all the hubs, creating surrogate IDs for any new business keys. Having done that, you can now resolve all business keys to surrogate ID's if you query the hub. The second step is to resolve the links between hubs and create surrogate IDs for any new associations. At the same time, you can also create all satellites that are attached to hubs, since you can resolve {{the key to a}} surrogate ID. Once you have created all the new links with their surrogate keys, you can add the satellites to all the links.|$|R
40|$|Abstract—Today, {{third-party}} applications {{provide a}} variety of rich services to smartphone users. There are compelling reasons to share personal data with third parties, such as social networking applications, but the benefits of sharing data must be balanced against the corresponding risks to personal privacy. Prior work has proposed personal <b>data</b> <b>vaults</b> to separate the capturing and sharing of data. We argue that to express realistic access control policies, personal <b>data</b> <b>vaults</b> must support the ability to flexibly transform data before release (e. g., converting location data to the zip code level). We propose a simple, practical personal <b>data</b> <b>vault</b> design that supports full-fledged computation by both trusted and untrusted entities. Our design includes a small scripting language called TRANSMUTE which pipes data streams through sandboxed filters written in the Lua programming language. A straightforward analysis of TRANSMUTE scripts provides a strong guarantee of noninterference, and our approach admits a flexible mechanism for audit that provides finer-grained information about data leakage. We formalize our approach and prove a noninterference theorem. We also describe our implementation and evaluate its expressiveness and performance through several case studies. I...|$|R
50|$|Conversent Communications was a Marlboro, Massachusetts-based CLEC and {{was founded}} in 1999 as New England Voice and Data. The company name was changed to Conversent in 2000. Conversent {{acquired}} REON broadband corporation, Fibernet of West Virginia and Northeast <b>Data</b> <b>Vault</b> before the One Communications merger.|$|R
50|$|<b>Data</b> <b>vault</b> {{attempts}} {{to solve the}} problem of dealing with change in the environment by separating the business keys (that do not mutate as often, because they uniquely identify a business entity) and the associations between those business keys, from the descriptive attributes of those keys.|$|R
5000|$|Dan Linstedt, {{creator of}} the <b>Data</b> <b>Vault</b> methodology, says {{business}} metadata [...] "...provides definition of the functionality, definition of the data, definition of the elements, and definition of how the data is used within business...business metadata includes business requirements, time-lines, business metrics, business process flows, and business terminology." ...|$|R
50|$|The <b>data</b> <b>vault</b> {{modelled}} {{layer is}} normally used to store data. It is not optimized for query performance, {{nor is it}} easy to query by the well-known query-tools such as Cognos, SAP Business Objects, Pentaho et al. Since these end-user computing tools expect or prefer their data to be contained in a dimensional model, a conversion is usually necessary.|$|R
