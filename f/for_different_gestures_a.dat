0|10000|Public
40|$|Abstract—Real time data {{acquisition}} from a cyber glove based hand gesture recognition system and data transmission through internet is proposed in this paper. The joint angles {{of the fingers}} are recognized <b>for</b> <b>different</b> <b>gestures</b> using <b>a</b> Cyber glove. Data from cyber glove is obtained in real time by a Client module developed using Windows Application Program Interface (WINAPI), from Virtual Hand Suite (VHS). TCP protocol is used for transmission of extracted data to the Server. Microcontroller (AT 89 S 52) is used to control {{the speed of the}} motor depending upon the real time data received at the server end. Real time working of the prototype is demonstrated with successful data tapping, transmission, reception and speed control...|$|R
50|$|Interaction {{techniques}} are the glue between physical I/O devices and interaction tasks or domain objects. Different types of interaction techniques {{can be used}} to map a specific device to a specific domain object. <b>For</b> example, <b>different</b> <b>gesture</b> alphabets exist <b>for</b> pen-based text input.|$|R
50|$|Researchers at Deutsche Telekom {{have used}} magnetometers {{embedded}} in mobile devices to permit touchless 3D interaction. Their interaction framework, called MagiTact, tracks {{changes to the}} magnetic field around a cellphone to identify <b>different</b> <b>gestures</b> made by <b>a</b> hand holding or wearing a magnet.|$|R
30|$|Both {{objective}} and subjective evaluation results showed improvements with motion synthesis method. The objective evaluation results {{showed a significant}} increase in shape and end timing performance of the synthesized motion curves <b>for</b> <b>different</b> <b>gestures</b> in unstable tracking environments. The subjective evaluation results also supported the viability of motion synthesis based on haptic feedback when tracking stability was poor. When the executed gesture was fast, {{as is the case in}} tapping, the effect of tracking instability was minimal, and motion synthesis had no significant improvements in {{objective and}} subjective scores. The subjective evaluation results showed that participants could better perceive synchronization of vibrotactile feedback with hand motion when synthesized motion data was used.|$|R
40|$|Abstract — A novel {{method for}} {{detecting}} muscle contraction is presented. This method is further developed <b>for</b> identifying four <b>different</b> <b>gestures</b> to facilitate <b>a</b> hand <b>gesture</b> controlled robot system. It is achieved based on surface Electromyograph (EMG) measurements of groups of arm muscles. The crossinformation is preserved through a simultaneous processing of EMG channels using a recent multivariate extension of Empirical Mode Decomposition (EMD). Next, phase synchrony measures are employed {{to make the}} system robust to different power levels due to electrode placements and impedances. The multiple pairwise muscle synchronies are used as features of <b>a</b> discrete <b>gesture</b> space comprising four gestures (flexion, extension, pronation, supination). Simulations on real-time robot control illustrate the enhanced accuracy and robustness of the proposed methodology. I...|$|R
40|$|Abstract: This paper {{proposes a}} method of {{actuating}} robotic cylindrical manipulator model in OpenGL based on hand gestures recognized using OpenCV (Open Source Computer Vision Library) in VC++. A model of cylindrical manipulator is developed using OpenGL (Open Graphics library) in VC++, which is actuated for the recognized hand gestures. The fingers and their orientation {{with respect to a}} reference base plane are recognized <b>for</b> <b>different</b> <b>gestures</b> using OpenCV. The actuator in each joint of cylindrical manipulator is selected by specific hand gesture. The movement of the actuator is controlled by orientation of the hand with respect to base plane. The hand gesture recognition and cylindrical manipulator module are developed using Windows Application Program Interface (WINAPI), standard C++ libraries, OpenCV libraries and OpenGL libraries in visual studio 2010. Real time simulation of the prototype model developed in VC++ is demonstrated with successful gesture recognition and actuation of manipulator...|$|R
40|$|International audienceThis study {{addresses}} {{the development of}} a testing system for pattern-recognition- based strategies of myoelectric control. This text describes the structure and components of the proposed system, as well as a process of its testing. The latter included an acquisition of an accompanying EMG, using MyoTM armband by Thalmic Labs Inc. TM, <b>for</b> six <b>different</b> <b>gestures</b> (classes) from seven subjects, as well as its processing, feature extraction, training the classifier and further real-time validation. The results show that system provides acceptable classification rates...|$|R
40|$|Part 2 : Full PapersInternational audienceThe {{description}} of <b>a</b> <b>gesture</b> requires temporal analysis of values generated by input sensors {{and does not}} fit well the observer pattern traditionally used by frameworks to handle user input. The current solution is to embed particular gesture-based interactions, such as pinch-to-zoom, into frameworks by notifying when <b>a</b> whole <b>gesture</b> is detected. This approach suffers {{from a lack of}} flexibility unless the programmer performs explicit temporal analysis of raw sensors data. This paper proposes a compositional, declarative meta-model for gestures definition based on Petri Nets. Basic traits are used as building blocks for defining gestures; each one notifies the change of a feature value. <b>A</b> complex <b>gesture</b> is defined by the composition of other sub-gestures using a set of operators. The user interface behaviour can be associated to the recognition of the whole gesture or to any other sub-component, addressing the problem of granularity for the notification events. The meta-model can be instantiated <b>for</b> <b>different</b> <b>gesture</b> recognition supports and its definition has been validated through a proof of concept library. Sample applications have been developed for supporting multitouch gestures on iOS and full body gestures with Microsoft Kinect...|$|R
40|$|A robot {{control system}} using four <b>different</b> <b>gestures</b> from <b>an</b> arm is presented. This is {{achieved}} based on surface Electromyograph (EMG) measurements of groups of arm muscles. The cross-information is preserved through a simultaneous processing of EMG channels using a recent multivariate extension of Empirical Mode Decomposition (EMD). Next, phase synchrony measures are employed {{to make the}} system robust to different power levels due to electrode placements and impedances. The multiple pairwise muscle synchronies are used as features of <b>a</b> discrete <b>gesture</b> space comprising four gestures (flexion, extension, pronation, supination). Simulations on real-time robot control illustrate the enhanced accuracy and robustness of the proposed methodology...|$|R
30|$|In this paper, {{a method}} for {{synthetic}} motion element synthesis for haptic rendering using hidden Markov models (HMM) is proposed. The proposed method {{was inspired by the}} embodied motion pattern generation for robots as detailed in [9, 10]. In the aforementioned work, the authors generated self-motion elements from recognized motion patterns in robots to replicate human motion patterns in robots. Here we use mimesis theory to recreate stable, real-time motion patterns <b>for</b> <b>different</b> <b>gestures.</b> The unstable motion patterns are fed to <b>an</b> HMM-based <b>gesture</b> recognition algorithm which recognizes the hidden states corresponding to <b>an</b> identified <b>gesture.</b> Primitive motion elements associated with each states are synthesized to recreate the ideal motion paths associated with each gesture. An algorithm for adaptive modulation of primitive motion element compared with changes in the real-time execution speed by users is also proposed. An objective analysis of the comparative performance of the synthesized motion data with the stable motion data obtained from a reference sensor is conducted to estimate the viability of the proposed model. Further, a subjective evaluation of vibrotactile feedback based on the proposed model was conducted to confirm the performance of the proposed methodology.|$|R
40|$|International audienceIn {{this paper}} a multi-classifier method for early {{recognition}} of handwritten gesture is presented. Unlike the other works which study the early recognition problem {{related to the}} time, we propose to make the recognition according to the quantity of incremental drawing of handwritten <b>gestures.</b> We train <b>a</b> segment length based multi-classifier for the task of recognizing the handwritten touch gesture as early as possible. To deal with potential similar parts {{at the beginning of}} <b>different</b> <b>gestures,</b> we introduce <b>a</b> reject option to postpone the decision until ambiguity persists. We report results on two freely available datasets: MGSet and ILG. These results demonstrate the improvement we obtained by using the proposed reject option for the early recognition of handwritten gestures...|$|R
40|$|We {{present an}} {{interface}} design for video browsing on mobile {{devices such as}} tablets {{that is based on}} storyboards and optimized with respect to content visualization and interaction design. In particular, we consider scientific results from our previous studies on mobile visualization (e. g., about optimum image sizes) and interaction (e. g., human perception and classification performance <b>for</b> <b>different</b> scrolling <b>gestures)</b> in order to create an interface for intuitive and efficient video content access. Our work aims at verifying if and to what degree optimized small screen designs utilizing touch screen gestures can compete with browsing methods on desktop PCs featuring significantly larger screen estate as well as more sophisticated input devices and interaction modes...|$|R
40|$|It is {{hard for}} most {{people who are not}} {{familiar}} with a sign language to communicate without an interpreter. Thus, a system that transcribes symbols in sign languages into plain text can help with real-time communication, and it may also provide interactive training for people to learn a sign language. A sign language uses manual communication and body language to convey meaning. The depth data <b>for</b> five <b>different</b> <b>gestures</b> corresponding to alphabets Y, V, L, S, I was obtained from online database. Each segmented gesture is represented by its timeseries curve and feature vector is extracted from it. To recognise the class of input noisy hand shape, distance metric for hand dissimilarity measure, called Finger-Earth Mover’s Distance (FEMD) is used. As it only matches fingers while not the complete hand shape, it can distinguish hand gestures of slight differences better...|$|R
40|$|This paper {{explores the}} use of Laban’s Movement Analysis {{from the field of}} dance and drama as a means to {{document}} user response to physical product interaction. A range of ‘traditional’ and ‘modern’ product pairs were identified and reviewed in a Theatre Studies workshop, where participants were required to discuss and complete worksheets on their emotional response. The results provide qualitative feedback on their reactions to the <b>different</b> <b>gestures,</b> and form <b>an</b> ‘emotional vocabulary’ that we plan to use in the development of semantic differentials for future studies. Key factors in emotional response to gesture have been identified, including the differences of mechanical versus electronic activation and the ‘framing’ of sequences of gestures in product interaction...|$|R
40|$|In this paper, we have {{proposed}} a system based on K-L Transform to recognize <b>different</b> hand <b>gestures.</b> The system consists of five steps: skin filtering, palm cropping, edge detection, feature extraction, and classification. Firstly the hand is detected using skin filtering and palm cropping was performed to extract out only the palm portion of the hand. The extracted image was then processed using the Canny Edge Detection technique to extract the outline images of palm. After palm extraction, the features of hand were extracted using K-L Transform technique and finally the input gesture was recognized using proper classifier. In our system, we have tested <b>for</b> 10 <b>different</b> hand <b>gestures,</b> and recognizing rate obtained was 96 %. Hence we propose an easy approach to recognize <b>different</b> hand <b>gestures.</b> Comment: 7 pages, 9 figure...|$|R
40|$|Abstract — This paper {{presents}} an approach for human activity recognition focusing on <b>gestures</b> in <b>a</b> teaching scenario, {{together with the}} setup and results of user studies on human gestures exhibited in unconstrained human-robot interaction (HRI). The user studies analyze several aspects: the distribution of gestures, relations, and characteristics of these gestures, and the acceptability of <b>different</b> <b>gesture</b> types in <b>a</b> human-robot teaching scenario. The results are then evaluated {{with regard to the}} activity recognition approach. The main effort is {{to bridge the gap between}} human activity recognition methods on the one hand and naturally occuring or at least acceptable gestures for HRI on the other. The goal is two-fold: To provide recognition methods with information and requirements on the characteristics and features of human activities in HRI, and to identify human preferences and requirements for the recognition of gestures in human-robot teaching scenarios. I...|$|R
40|$|In this paper, we {{investigate}} efficient recognition of human gestures / movements from multimedia and multimodal data, including the Microsoft Kinect and translational and rotational acceleration and velocity from wearable inertial sensors. We firstly present {{a system that}} automatically classifies a large range of activities (17 <b>different</b> <b>gestures)</b> using <b>a</b> random forest decision tree. Our system can achieve near real time recognition by appropriately selecting the sensors {{that led to the}} greatest contributing factor for a particular task. Features extracted from multimodal sensor data were used to train and evaluate a customized classifier. This novel technique is capable of successfully classifying var- ious gestures with up to 91 % overall accuracy on a publicly available data set. Secondly {{we investigate}} a wide range of different motion capture modalities and compare their results in terms of gesture recognition accu- racy using our proposed approach. We conclude that gesture recognition can be effectively performed by considering an approach that overcomes many of the limitations associated with the Kinect and potentially paves the way for low-cost gesture recognition in unconstrained environments...|$|R
40|$|About 70 million deaf {{people use}} sign {{language}} as {{their first language}} or mother tongue, {{but the lack of}} a common language between the deaf and hearing individuals makes the general communication difficult. This thesis aims to explore the potential of utilizing electromyography to improve the general communication for deaf people. The Myo armband, developed by Thalmic Labs, is <b>a</b> wearable <b>gesture</b> and motion control device that use a set of electromyographic sensors, combined with a gyroscope, accelerometer and magnetometer, to detect movements and gestures. This thesis presents a development of a prototype-level system that utilize the Myo armband's electromyographic sensors to detect and translate sign language signs to something intelligible for the hearing individuals. Based on the previous work and the associated framework developed for gesture recognition using the Inertial Measurement Units (IMU) and Electromyography (EMG) sensors from the Myo armband, this thesis focuses on the EMG feature extraction and using machine learning for gestures classification. This thesis propose <b>a</b> framework for <b>gesture</b> recognition, which achieved an accuracy of 85 % <b>for</b> 10 <b>different</b> <b>gestures...</b>|$|R
40|$|With over 250 million {{devices to}} be sold, {{wearable}} devices revenue is forecasted to hit $ 2 billion by 2018. Wearables {{are expected to}} {{play a key role}} in achieving ubiquitous com-puting in the future. The ease of use and comfort in ac-cessing online content comes however at the expense of po-tential privacy and security issues. This represents today a challenging and growing concern. In their current design, wearables are prone to several risks ranging from the pos-sible exposure of data when communicating with tethering devices to the ease of access to sensitive data in case of com-promised devices. In this paper, we study the feasibility of finger movement pattern based behavioural biometrics for implicit authentication of users on Google Glass. Using data collected from 20 volunteers, we employ two different clas-sification methods: statistical inference (white box) and ma-chine learning (black box). Our experimental study shows that such authentication is both effective, in terms of clas-sification accuracy, and feasible, in terms of computational load, on wearable devices. Our results show classification accuracy of up to 99 % with only 75 training samples using behavioural biometric data <b>for</b> four <b>different</b> <b>gestures</b> types. Our implemented classifier on Glass takes between 25 to 110 milliseconds execution time for prediction depending on the <b>gesture</b> type, once <b>an</b> adjustable threshold number of ges-tures have been performed...|$|R
40|$|ABSTRACT:- In this paper, we {{introduced}} a novel and simple methods of extracting the general {{features of the}} hand gestures from surface EMG signal patterns: Hand Extension (H. E), Hand Grasp(H. G),Wrist Extension(W. E),Wrist Flexion(W. F) Pinch(P),Thumb Flexion (T. F). Hand gesture EMG signal classification is demonstrated as a method for prosthesis applications. Recorded electrode signals from the Abductor Pollicies longus above the elbow are noise filtered and transformed into features using wavelet transforms. Feature sets <b>for</b> six <b>different</b> hand <b>gestures</b> are classified by minimum distance classifier technique. Features construction, recognition accuracy and an approach for {{an extension of the}} technique to a variety of real world application areas are presented...|$|R
40|$|The {{design of}} remote <b>gesturing</b> {{technologies}} is <b>an</b> area of growing interest. Current technologies have taken differing {{approaches to the}} representation of remote gesture. It is not clear which approach has the most benefit to task performance. This study therefore compared performance in a collaborative physical (assembly) task using remote gesture systems constructed with combinations of three <b>different</b> <b>gesture</b> formats (unmediated hands only, hands and sketch and digital sketch only) and two <b>different</b> <b>gesture</b> output locations (direct projection into a worker's task space or on an external monitor). Results indicated that <b>gesturing</b> with <b>an</b> unmediated representation of the hands leads to faster performance with no loss of accuracy. Comparison of gesture output locations did not find {{a significant difference between}} projecting gestures and presenting them on external monitors. These results are discussed in relation to theories of conversational grounding and the design of technologies from a 'mixed ecologies' perspective...|$|R
40|$|Brain {{computer}} interfaces (BCIs) offer individuals {{suffering from}} major disabilities an alternative method {{to interact with}} their environment. Sensorimotor rhythm (SMRs) based BCIs can successfully perform control tasks; however, the traditional SMR paradigms intuitively disconnect the control and real task, making them non-ideal for complex control scenarios. In this study, we design a new, intuitively connected motor imagery (MI) paradigm using hierarchical common spatial patterns (HCSP) and context information to effectively predict intended hand grasps from electroencephalogram (EEG) data. Experiments with 5 participants yielded an aggregate classification accuracy [...] intended grasp prediction probability [...] of 64. 5 % <b>for</b> 8 <b>different</b> hand <b>gestures,</b> more than 5 times the chance level. Comment: This work has been submitted to EMBC 201...|$|R
40|$|This paper {{presents}} <b>a</b> <b>gesture</b> recognition {{system based}} on Hidden Markov Models. It has several user friendly capabilities like person-independent and background independent recognition. It can distinguish between up to 24 <b>different</b> <b>gestures.</b> An improved system is able to recognize gestures continuously and output the result with no noticeable delay...|$|R
40|$|ENGELSK: This thesis {{investigates the}} use of hand <b>gestures</b> as <b>an</b> {{additional}} modality in authentication schemes, to thwart the risk of observation (shoulder surfing) attacks. We used the accelerometer already embedded in the iPod Touch to gather accelerometer signals, which were used to conduct experiments on how accurately we could recognize and differentiate <b>different</b> <b>gestures.</b> We restricted ourselves to a pre-defined set of <b>gestures,</b> and achieved <b>an</b> EER of 5 % on the controlled wrist movements, and 8 % after including two circular motions. The algorithms we used were tailored to fit the limited computational power of the iPod Touch, as we needed a recognition module {{that could be used}} in real time for our authentication schemes. After assessing the characteristics of the <b>different</b> hand <b>gestures,</b> we developed two unique authentication schemes that incorporate hand <b>gestures</b> as <b>an</b> additional modality for authentication. We developed suitable attack scenarios, and found that both schemes adds additional entropy to the scheme, as well as a significant amount of shoulder surfing resistance...|$|R
40|$|This {{thesis is}} {{submitted}} in partial {{fulfillment of the}} requirements for the degree of Bachelor of Arts in English and Humanities, 2013. Cataloged from PDF version of thesis. Includes bibliographical references (page 49 - 56). Children need gestures to communicate with others. They communicate using <b>different</b> <b>gestures.</b> Unlike adults children have limited number of gestures as they have not acquired the capability of using <b>a</b> variety of <b>gestures.</b> It is seen that every <b>gesture</b> as <b>a</b> purpose of its own. <b>Different</b> <b>gestures</b> are used in different situations and also children’s gesture varies according to some factors associated with it. While gesturing children not only use gestures {{but there is an}} amalgamation of speech as well. It is seen that there are <b>different</b> combinations of <b>gesture</b> and speech available which facilitates the communication. This paper examined the <b>different</b> types of <b>gestures</b> that the children use while communicating with others also the use of these gestures is overviewed along with the integration of speech. Two groups of children are studied, 14 months and 18 months. Sharjah IbrahimB. A. in Englis...|$|R
5000|$|... #Caption: Female Corsair {{trying out}} <b>different</b> <b>gestures</b> at Klaipedia.|$|R
40|$|In this paper, {{we present}} <b>a</b> <b>gesture</b> {{recognition}} {{system for the}} development of a human-robot interaction (HRI) interface. Kinect cameras and the OpenNI framework are used to obtain real-time tracking of a human skeleton. Ten <b>different</b> <b>gestures,</b> performed by <b>different</b> persons, are defined. Quaternions of joint angles are first used as robust and significant features. Next, neural network (NN) classifiers are trained to recognize the <b>different</b> <b>gestures.</b> This work deals with different challenging tasks, such as the real-time implementation of <b>a</b> <b>gesture</b> recognition system and the temporal resolution of gestures. The HRI interface developed in this work includes three Kinect cameras placed at different locations in an indoor environment and an autonomous mobile robot that can be remotely controlled by one operator standing in front of one of the Kinects. Moreover, the system is supplied with a people re-identification module which guarantees that only one person at a time has control of the robot. The system's performance is first validated offline, and then online experiments are carried out, proving the real-time operation of the system as required by a HRI interface...|$|R
30|$|Gestures that {{conveyed}} relevant semantic content {{were coded}} and analyzed. <b>A</b> <b>gesture</b> {{was defined as}} a movement of hand(s) accompanied by speech to express an idea or meaning. <b>A</b> <b>gesture</b> unit was defined as “the period of time between successive rests of the limbs (McNeill, 1992).” Movement of the hand(s) starting from a resting position and returning to a resting position was regarded as one gesture. If the hands did not return to a resting position between two gestures, the boundary was defined by a pause in motion and an obvious change in shape or trajectory. When a participant used both hands simultaneously to describe one object, concept, or part, it was regarded as one <b>gesture.</b> If <b>a</b> participant used both hands and one described an object, a concept, or a part and the other hand <b>a</b> <b>different</b> concept, the <b>gestures</b> were coded as two <b>different</b> <b>gestures.</b> Beats, which serve to advance the discourse, and emblems, which have conventionalized meanings like “OK,” were excluded as were {{a very small number of}} metaphoric gestures expressing abstract ideas.|$|R
40|$|The {{purpose of}} this paper is twofold. First, we {{introduce}} our Microsoft Kinect–based video dataset of American Sign Language (ASL) signs designed for body part detection and tracking research. This dataset allows researchers to exper-iment with using more than 2 -dimensional (2 D) color video information in gesture recognition projects, as it gives them access to scene depth information. Not only can this make it easier to locate body parts like hands, but without this ad-ditional information, two completely <b>different</b> <b>gestures</b> that share <b>a</b> similar 2 D trajectory projection can be difficult to distinguish from one another. Second, as an accurate hand locator is a critical element in any automated gesture or sign language recognition tool, this paper assesses the efficacy of one popular open source user skeleton tracker by examining its performance on random signs from the above dataset. We compare the hand positions as determined by the skeleton tracker to ground truth positions, which come from manual hand annotations of each video frame. The {{purpose of this}} study is to establish a benchmark for the assessment of more advanced detection and tracking methods that utilize scene depth data. For illustrative purposes, we compare the re-sults of one of the methods previously developed in our lab for detecting a single hand to this benchmark...|$|R
40|$|Depending {{upon their}} nature, Outdoor AR {{applications}} can be deployed on head mounted displays (HMD) like Google glass or handheld Displays (HHD) like smartphones. This master’s thesis investigates novel gesture-based interaction techniques and applications for a HMD-HHD hybrid system {{that account for}} advantages presented by each platform. Prior research in HMD-HHD hybrid systems and gestures used in VR and surface computing were taken into account while designing the applications and interaction techniques. A prototype system combining a HMD and HHD was developed and four applications were created for the system. For evaluating the <b>gestures,</b> <b>an</b> application that compared four of the proposed gestures for selection tasks was developed. The results showed {{a significant difference between}} the <b>different</b> <b>gestures</b> and that the choice of gesture for selection tasks using a hybrid system depended upon application requirements like speed and accuracy...|$|R
2500|$|... (left) Double-click: {{clicking}} the button two times {{in quick succession}} counts as <b>a</b> <b>different</b> <b>gesture</b> than two separate single clicks.|$|R
50|$|Mudra is the <b>different</b> <b>gestures</b> {{the hands}} and body take when the Kundalini is {{activated}} and pass up through the central channel.|$|R
40|$|This {{material}} {{is presented to}} ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected {{to adhere to the}} terms and constraints invoked by each author's copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder. [...] Copyright IEEE. Personal use of this {{material is}} permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE. DOI : 10. 1109 /ROMAN. 2006. 314402 This paper presents an approach for human activity recognition focusing on <b>gestures</b> in <b>a</b> teaching scenario, together with the setup and results of user studies on human gestures exhibited in unconstrained human-robot interaction (HRI). The user studies analyze several aspects: the distribution of gestures, relations, and characteristics of these gestures, and the acceptability of <b>different</b> <b>gesture</b> types in <b>a</b> human-robot teaching scenario. The results are then evaluated with regard to the activity recognition approach. The main effort is {{to bridge the gap between}} human activity recognition methods on the one hand and naturally occuring or at least acceptable gestures for HRI on the other. The goal is two-fold: To provide recognition methods with information and requirements on the characteristics and features of human activities in HRI, and to identify human preferences and requirements for the recognition of gestures in human-robot teaching scenarios...|$|R
40|$|Abstract — This paper {{presents}} {{a method to}} evaluate medical gestures. The objective is to objectively assess <b>a</b> <b>gesture</b> carried out by novice doctors. The proposed method {{is based on the}} study of the curvature of the 3 D <b>gesture</b> and provide <b>a</b> global performance index for one manipulation. The study of the number of peaks on the curvature indicates if the gesture is smooth or not. The application is the obstetric gestures linked to the forceps use but the method can be applied to <b>different</b> <b>gestures</b> without loss of generality. Seven residents carried out 30 forceps blade placements. The results clearly show a difference between the gestures carried out. This highlights the difficulty of the gesture according to the fetal head presentation. I...|$|R
40|$|Skweezees {{are soft}} objects filled with {{conductive}} padding, which {{are capable of}} detecting <b>different</b> squeeze <b>gestures</b> using electrodes dispersed all over the object. This paper presents a case-study on the design and development of a tangible product based on the Skweezee system, namely a cushion remote. Squeeze-based gestures for soft user interfaces have rarely been explored. Therefore, we have worked on establishing squeeze-based <b>gestures</b> for <b>a</b> soft cushion interface for controlling a television, {{by means of a}} user-centered approach. The user study has brought out appropriate <b>gestures</b> for controlling <b>a</b> cushion remote. A prototype was designed using these gestures. The end result is a cushion remote that uses the Skweezee system and <b>a</b> <b>gesture</b> set for <b>a</b> Skweezee-based cushion remote control. status: accepte...|$|R
40|$|Abstract—Our {{real-time}} continuous gesture recognition sys-tem addresses {{problems that}} have previously been neglected: handling both gestures that are characterized by distinct paths and gestures characterized by distinct hand poses; and determin-ing how and when the system should respond to gestures. Our probabilistic recognition framework based on hidden Markov models (HMMs) unifies {{the recognition of the}} two forms of gestures. Using information from the hidden states in the HMM, we can identify <b>different</b> <b>gesture</b> phases: the pre-stroke, the nucleus and the post-stroke phases. This allows the system to respond appropriately to both <b>gestures</b> that require <b>a</b> discrete response and those needing a continuous response. Our system is extensible: in only a few minutes, users can define their own <b>gestures</b> by giving <b>a</b> few examples rather than writing code. We also collected <b>a</b> new <b>gesture</b> dataset that contains the two forms of <b>gestures,</b> and propose <b>a</b> new hybrid performance metric for evaluating gesture recognition methods for real-time interaction. Keywords—Gesture recognition, real-time, hidden Markov mod-els I...|$|R
