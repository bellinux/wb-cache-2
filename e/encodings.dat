3797|17|Public
25|$|XML {{allows the}} use of any of the Unicode-defined <b>encodings,</b> and any other <b>encodings</b> whose {{characters}} also appear in Unicode. XML also provides a mechanism whereby an XML processor can reliably, without any prior knowledge, determine which encoding is being used. <b>Encodings</b> other than UTF-8 and UTF-16 are not necessarily recognized by every XML parser.|$|E
25|$|Within text strings, {{characters}} are shown using character codes (integers) that map to glyphs {{in the current}} font using an encoding. There {{are a number of}} predefined <b>encodings,</b> including WinAnsi, MacRoman, and a large number of <b>encodings</b> for East Asian languages, and a font can have its own built-in encoding. (Although the WinAnsi and MacRoman <b>encodings</b> are derived from the historical properties of the Windows and Macintosh operating systems, fonts using these <b>encodings</b> work equally well on any platform.) PDF can specify a predefined encoding to use, the font's built-in encoding or provide a lookup table of differences to a predefined or built-in encoding (not recommended with TrueType fonts). The encoding mechanisms in PDF were designed for Type 1 fonts, and the rules for applying them to TrueType fonts are complex.|$|E
25|$|There {{are various}} {{character}} <b>encodings</b> for representing Ukrainian with computers.|$|E
25|$|There {{are other}} Start Of Frame markers that {{introduce}} {{other kinds of}} JPEG <b>encodings.</b>|$|E
25|$|SAMPA was devised as a hack to {{work around}} the {{inability}} of text <b>encodings</b> to represent IPA symbols. Consequently, as Unicode support for IPA symbols becomes more widespread, the necessity for a separate, computer-readable system for representing the IPA in ASCII decreases. However, text input relies on specific keyboard <b>encodings</b> or input devices. For this reason, SAMPA and X-SAMPA are still widely used in computational phonetics and in speech technology.|$|E
25|$|GB 2312 – Principally {{simplified}} Chinese <b>encodings,</b> {{but there}} are also the basic 33 Russian Cyrillic letters (in upper- and lower-case).|$|E
25|$|Also {{note that}} the NCBI have {{converted}} this FASTQ data from the original Solexa/Illumina encoding to the Sanger standard (see <b>encodings</b> below).|$|E
25|$|JIS and Shift JIS – Principally Japanese <b>encodings,</b> {{but there}} are also the basic 33 Russian Cyrillic letters (in upper- and lower-case).|$|E
25|$|For {{the usage}} in computers, {{a variety of}} <b>encodings</b> {{have been used for}} Greek online, many of them {{documented}} in RFC 1947.|$|E
25|$|The {{advantage}} of SLP1 over other <b>encodings</b> {{is that a}} single ASCII character is used for each Devanagari letter, a peculiarity that eases reverse transliteration.|$|E
25|$|Although {{often said}} to be obsolete, in fact the {{half-width}} katakana are still used in many systems and <b>encodings.</b> For example, the titles of mini discs can only be entered in ASCII or half-width katakana, and half-width katakana are commonly used in computerized cash register displays, on shop receipts, and Japanese digital television and DVD subtitles. Several popular Japanese <b>encodings</b> such as EUC-JP, Unicode and Shift JIS have half-width katakana code as well as full-width. By contrast, ISO-2022-JP has no half-width katakana, and is mainly used over SMTP and NNTP.|$|E
25|$|UTF-8 {{will take}} more space than a multi-byte {{encoding}} designed for a specific script. East Asian legacy <b>encodings</b> generally used two bytes per character yet take three bytes per character in UTF-8.|$|E
25|$|With early desktop {{computers}} {{it was possible}} to modify existing 8-bit Latin fonts to accommodate specialized character needs. This was done without any kind of system or standardization, meaning incompatibility of <b>encodings.</b>|$|E
25|$|The {{preferred}} character <b>encodings</b> (writing codes) for Slovene {{texts are}} UTF-8 (Unicode), UTF-16, and ISO/IEC 8859-2 (Latin-2), which generally supports Central and Eastern European languages that {{are written in}} the Latin script.|$|E
25|$|The ISO-8859-1 and Windows-1252 {{character}} <b>encodings</b> {{include the}} letters á, é, í, ó, ú, ý, {{and their respective}} capital forms. Dozens more letters with the acute accent are available in Unicode.|$|E
25|$|Some letters {{can occur}} in variant shapes, mostly {{inherited}} from medieval minuscule handwriting. While their use in normal typography of Greek is purely a matter of font styles, some such variants have been given separate <b>encodings</b> in Unicode.|$|E
25|$|Support for the typographic {{apostrophe}} (’) {{was introduced}} in several 8-bit character <b>encodings,</b> such as the Apple Macintosh operating system’s Mac Roman character set (in 1984), {{and later in the}} CP1252 encoding of Microsoft Windows. There is no such character in ISO-8859-1.|$|E
25|$|Intrinsity's main {{selling point}} was its Fast14 technology, {{a set of}} design tools {{implemented}} in custom EDA software, for using dynamic logic and novel signal <b>encodings</b> to permit greater processor speeds in a given process than naive static design can offer.|$|E
25|$|For large fonts or fonts with {{non-standard}} glyphs, {{the special}} <b>encodings</b> Identity-H (for horizontal writing) and Identity-V (for vertical) are used. With such fonts {{it is necessary}} to provide a ToUnicode table if semantic information about the characters is to be preserved.|$|E
25|$|For {{the range}} A0–FF (hex) it follows the Unicode range 370–3CF (see below) except that some symbols, like ©, ½, § etc. are used where Unicode has unused locations. Like all ISO-8859 <b>encodings</b> it {{is equal to}} ASCII for 00–7F (hex).|$|E
25|$|The euro is {{represented}} in the Unicode character set with the character name EURO SIGN and the code position U+20AC (decimal 8364) {{as well as in}} updated versions of the traditional Latin character set <b>encodings.</b> In HTML, the &euro; entity can also be used.|$|E
25|$|Because many <b>encodings</b> {{have only}} a limited {{repertoire}} of characters, they are often only usable to represent text in a limited subset of human languages. Unicode {{is an attempt to}} create a common standard for representing all known languages, and most known character sets are subsets of the very large Unicode character set. Although there are multiple character <b>encodings</b> available for Unicode, the most common is UTF-8, which has the advantage of being backwards-compatible with ASCII; that is, every ASCII text file is also a UTF-8 text file with identical meaning. UTF-8 also has the advantage that it is easily auto-detectable. Thus, a common operating mode of UTF-8 capable software, when opening files of unknown encoding, is to try UTF-8 first and fall back to a locale dependent legacy encoding when it definitely isn't UTF-8.|$|E
25|$|WTF-8 (Wobbly Transformation Format – 8-bit) is an {{extension}} of UTF-8 where the <b>encodings</b> of the surrogate halves (U+D800 through U+DFFF) are allowed. This is necessary to store possibly-invalid UTF-16, such as Windows filenames. Many systems that deal with UTF-8 work this way without considering it a different encoding, as it is simpler.|$|E
25|$|The {{main source}} of {{problems}} was confusion between hexadecimal number encoding and binary-coded decimal <b>encodings</b> of numbers. Both hexadecimal and BCD encode the numbers 0–9 as 0x0–0x9. But BCD encodes the number 10 as 0x10, whereas hexadecimal encodes the number 10 as 0x0A; 0x10 interpreted as a hexadecimal encoding represents the number 16.|$|E
25|$|Telegraphy {{was driven}} {{by the need to}} reduce sending costs, either in hand-work per message or by {{increasing}} the sending rate. While many experimental systems employing moving pointers and various electrical <b>encodings</b> proved too complicated and unreliable, a successful advance in the sending rate was achieved through the development of telegraphese.|$|E
25|$|The Incan Empire ran a large {{command economy}} using quipu, tallies made by {{knotting}} colored fibers. Knowledge of the <b>encodings</b> of the knots and colors was suppressed by the Spanish conquistadors in the 16thcentury, {{and has not}} survived although simple quipu-like recording devices are still used in the Andean region.|$|E
25|$|This {{argument}} {{can be applied to}} any class of computable (total) functions that can be enumerated in this way, as explained in the article Machines that always halt. Note however that the partial computable functions (those that need not be defined for all arguments) can be explicitly enumerated, for instance by enumerating Turing machine <b>encodings.</b>|$|E
25|$|It is {{possible}} in UTF-8 (or any other multi-byte encoding) to split or truncate a string {{in the middle of}} a character. This can result in an invalid string which some software refuses to accept. A good parser should ignore a truncated character at the end, which is easy in UTF-8 but tricky in some other multi-byte <b>encodings.</b>|$|E
25|$|With {{the header}} {{extensions}} and the Base64 and Quoted-Printable MIME <b>encodings,</b> {{there was a}} new generation of binary transport. In practice, MIME has seen increased adoption in text messages, but it is avoided for most binary attachments. Some operating systems with metadata attached to files use specialized encoding formats. For Mac OS, both Binhex and special MIME types are used.|$|E
25|$|In modern {{typography}} {{there was}} insufficient space on typewriters and later computer keyboards {{to allow for}} both an O-with-dots (also representing Ö) and an O-with-bars. Since they looked nearly identical, the two glyphs were combined, which was also done in computer character <b>encodings</b> such as ISO 8859-1. As a result {{there was no way}} to differentiate between the different characters.|$|E
25|$|UTF-8 is self-synchronizing: {{character}} {{boundaries are}} easily identified by scanning for well-defined bit patterns in either direction. If bytes are lost due to error or corruption, one can always locate the next valid character and resume processing. If {{there is a}} need to shorten a string to fit a specified field, the previous valid character can easily be found. Many multi-byte <b>encodings</b> are much harder to resynchronize.|$|E
25|$|Some software, such as text editors, will {{refuse to}} {{correctly}} display or interpret UTF-8 unless the text {{starts with a}} byte order mark, and will insert such a mark. This {{has the effect of}} making it impossible to use UTF-8 with any older software that can handle ASCII-like <b>encodings</b> but cannot handle the byte order mark. This, however, is no problem of UTF-8 itself but one of bad software implementations.|$|E
25|$|UTF-8 {{has been}} the {{dominant}} character encoding for the World Wide Web since 2009, and as of November 2017 accounts for 90.1% of all Web pages. (The next-most popular multibyte <b>encodings,</b> Shift JIS and GB 2312, have 0.8% and 0.6% respectively). The Internet Mail Consortium (IMC) recommended that all e-mail programs be able to display and create mail using UTF-8, and the W3C recommends UTF-8 as the default encoding in XML and HTML.|$|E
25|$|Both the dakuten and handakuten glyphs {{are drawn}} identically in hiragana and {{katakana}} scripts. The combining characters are rarely used in full-width Japanese characters, as Unicode and all common multibyte Japanese <b>encodings</b> provide precomposed glyphs for all possible dakuten and handakuten character combinations {{in the standard}} hiragana and katakana ranges. However, combining characters are required in half-width kana, which does not provide any precomposed characters in order to fit within a single byte.|$|E
25|$|Any byte {{oriented}} string searching algorithm can be {{used with}} UTF-8 data, since the sequence of bytes for a character cannot occur anywhere else. Some older variable-length <b>encodings</b> (such as Shift JIS) did not have this property and thus made string-matching algorithms rather complicated. In Shift JIS the end byte of a character and the first byte of the next character could look like another legal character, something that can't happen in UTF-8.|$|E
25|$|WCF {{is capable}} of using SOAP for {{communication}} between two processes, thereby making WCF based applications interoperable with any other process that communicates via SOAP. When a WCF process communicates with a non-WCF process, XML based encoding {{is used for the}} SOAP messages but when it communicates with another WCF process, the SOAP messages are encoded in an optimized binary format, to optimize the communication. Both the <b>encodings</b> conform to the data structure of the SOAP format, called Infoset.|$|E
