111|1152|Public
2500|$|It can {{be noted}} {{that each of the}} first two rules has a support of 1 (i.e., the {{antecedent}} matches two objects), while each of the last two rules has a support of 2. [...] To finish writing the rule set for this knowledge system, the same procedure as above (starting with writing a new decision matrix) should be followed for the case of , thus yielding a new set of implications for that <b>decision</b> <b>value</b> (i.e., a set of implications with [...] as the consequent). In general, the procedure will be repeated for each possible value of the decision variable.|$|E
2500|$|To {{read this}} {{decision}} matrix, look, for example, {{at the intersection}} of row [...] and column , showing [...] in the cell. [...] This means that with regard to <b>decision</b> <b>value</b> , object [...] differs from object [...] on attributes [...] and , and the particular values on these attributes for the positive object [...] are [...] and [...] This tells us that the correct classification of [...] as belonging to decision class [...] rests on attributes [...] and [...] although one or the other might be dispensable, we know {{that at least one of}} these attributes is indispensable.|$|E
5000|$|The {{algorithm}} proceeds in {{rounds and}} uses a rotating coordinator: in each round r, the process whose identity is given by r mod n is chosen as the coordinator. Each process keeps track of its current preferred <b>decision</b> <b>value</b> (initially equal to the input of the process) and the last round where it changed its <b>decision</b> <b>value</b> (the value's timestamp). The actions carried out in each round are: ...|$|E
5000|$|An {{alternative}} is to derive an analytical [...] function by assuming a particular parametric distribution for the underlying <b>decision</b> <b>values.</b> For example, a binormal precision-recall curve {{can be obtained by}} assuming <b>decision</b> <b>values</b> in both classes to follow a Gaussian distribution.|$|R
5000|$|<b>Decisions,</b> <b>values,</b> {{and free}} will: How does the brain enable <b>decisions</b> based on <b>values</b> {{and how can}} {{decision-making}} be improved? What can neuroscience reveal {{about the nature of}} human freedom? ...|$|R
3000|$|... be {{the binary}} <b>decision</b> <b>values</b> {{of the current}} and the {{previous}} time instants, respectively. We have the following: [...]...|$|R
50|$|Nozick views human {{rationality}} as {{an evolutionary}} adaptation. Its delimited purpose and function {{may be responsible}} for biases and blind spots, possibly accounting for philosophy's difficulty with perennial questions that are remote from the exigencies that drive natural selection. He offers a reformulation of the decision theory that was developed in the twentieth century to explain rational action. It should include the symbolic meaning of actions as well as a new rule of rational decision that maximizes <b>decision</b> <b>value.</b> These have implications for long-standing issues such as the Prisoner's Dilemma and for Newcomb's Problem. His proposal about rational belief has an internalist element of support by reasons that make the belief credible, and an externalist element of generation by a process that reliably produces true beliefs. Rational belief has an intellectual component, for one should not believe any statement less credible than some incompatible alternative. It also has a practical component, for one should believe a statement only if the expected utility (or <b>decision</b> <b>value)</b> of doing so is greater than that of not believing it.|$|E
5000|$|To {{read this}} {{decision}} matrix, look, for example, {{at the intersection}} of row [...] and column , showing [...] in the cell. This means that with regard to <b>decision</b> <b>value</b> , object [...] differs from object [...] on attributes [...] and , and the particular values on these attributes for the positive object [...] are [...] and [...] This tells us that the correct classification of [...] as belonging to decision class [...] rests on attributes [...] and although one or the other might be dispensable, we know {{that at least one of}} these attributes is indispensable.|$|E
5000|$|It can {{be noted}} {{that each of the}} first two rules has a support of 1 (i.e., the {{antecedent}} matches two objects), while each of the last two rules has a support of 2. To finish writing the rule set for this knowledge system, the same procedure as above (starting with writing a new decision matrix) should be followed for the case of , thus yielding a new set of implications for that <b>decision</b> <b>value</b> (i.e., a set of implications with [...] as the consequent). In general, the procedure will be repeated for each possible value of the decision variable.|$|E
30|$|As Kong et al. {{approach}} {{works only}} on probability, ignoring any association between parameters might result probably in different values from actual. Secondly, it predicts missing values in [0, 1] range, while the actual value must be either 0 or 1 in standard soft set (Boolean information system). In contrast, DFIS prefer to predict actual values through association and use probability when the association is not strong. Secondly, in both cases, it calculates binary values maintaining {{the integrity of}} standard soft set. Thirdly, compare to Zou et al. results; its <b>decision</b> <b>values</b> results are much closer to actual values as shown in experimental results (Qin et al. 2012). The average of mean absolute percentage error (MAPE) of DFIS is 0.07, while that of Zou et al. approach is 0.11 for all five data sets used in DFIS. If we convert this average of MAPE to percent accuracy of both approaches then the average accuracy of DFIS is 93.17  % while that of Zou et al. approach is 89.12  % in calculating <b>decision</b> <b>values.</b> It is notable that Zou et al. and Kong et al. approaches have same results of <b>decision</b> <b>values</b> (Kong et al. 2014); consequently, the average accuracy of DFIS in <b>decision</b> <b>values</b> comes to be 4.04  % higher than Kong et al. technique. Hence DFIS is more suitable than Kong et al. approach.|$|R
50|$|Each {{received}} symbol may {{be represented}} in vector form as vr = {r0, r1}, where r0 and r1 are soft <b>decision</b> <b>values,</b> whose magnitudes signify the joint reliability of the received vector, vr.|$|R
40|$|We {{propose a}} new ensemble-based {{classifier}} for multi-source human action recognition called Multi-Max-Margin Support Vector Machine (MMM-SVM). This ensemble method incorporates the <b>decision</b> <b>values</b> of multiple sources and makes an informed final prediction by merging multi-source feature's intrinsic decision strength. Experiments {{performed on the}} benchmark IXMAS multi-view dataset (Weinland) demonstrate that the performance of our multi-view system can further improve the accuracy over single view by 3 – 13 % and consistently outperform the direct-concatenation method. We further apply this ensemble technique for combining the <b>decision</b> <b>values</b> of contextual and motion information in the UCF Sports dataset (Liu, 2009) {{and the results are}} comparable to the state-of-the-art, which exhibits our algorithm's potential for further extension in other areas of feature fusion problems...|$|R
5000|$|In DRSA, {{data are}} often {{presented}} using a {{particular form of}} decision table. Formally, a DRSA decision table is a 4-tuple , where [...] is a finite set of objects, [...] is a finite set of criteria, [...] where [...] is {{the domain of the}} criterion [...] and [...] is an information function such that [...] for every [...] The set [...] is divided into condition criteria (set [...] ) and the decision criterion (class) [...] Notice, that [...] is an evaluation of object [...] on criterion , while [...] is the class assignment (<b>decision</b> <b>value)</b> of the object. An example of decision table is shown in Table 1 below.|$|E
30|$|Nowadays, in a {{very often}} {{changing}} market, the business of a single item does not pay much profit to a retailer. For this reason, almost all businessmen {{in the fields of}} transportation (Sancak and Salman [10]) do the business of several items. Generally, in all the cases of STP (multi-objective, multi-item and multi-objective multi-item ones), the inequality has been considered as a general inequality. But we can consider this inequality in the fuzzy environment named fuzzy inequality [11, 12]. Fuzzy inequality means it will essentially satisfy that inequality condition. Flexible index is used (Cao [13]) to convert it into the general inequality, so that it will {{give you a chance to}} choose the appropriate <b>decision</b> <b>value.</b> Two algorithms were given by Cao [13] to find the <b>decision</b> <b>value.</b> We have taken one of them to find the <b>decision</b> <b>value.</b> That <b>decision</b> <b>value</b> will give us a more general optimal solution and an optimal value to minimize the objectives.|$|E
30|$|Service QoE {{evaluation}} {{score is}} to provide services for media transmission control. In the process of media transmission, there are some new technologies, such as media streaming payload distribution on the transport path (Ning et al. 2012; Song et al. 2012). Service QoE evaluation algorithm will give guidance to meeting the transport needs of media. In this paper, policy <b>decision</b> <b>value</b> 1 means that the server will directly give user experience score of media service, and feed {{it back to the}} client. Policy <b>decision</b> <b>value</b> 2 signifies network redirection, i.e. adjust of the media transport payload or paths. Policy <b>decision</b> <b>value</b> 3 suggests that the transmission of the media service should be stopped.|$|E
5000|$|Classification: A {{process that}} classifies the input signal range into [...] {{non-overlapping}} intervals , by defining [...] boundary (<b>decision)</b> <b>values</b> , such that [...] for , with the extreme limits defined by [...] and [...] All the inputs [...] that fall {{in a given}} interval range [...] {{are associated with the}} same quantization index [...]|$|R
40|$|A {{concept of}} interval-valued {{triangular}} fuzzy soft set is presented, and some operations of “AND,” “OR,” intersection, union and complement, {{and so forth}} are defined. Then some relative properties are discussed and several conclusions are drawn. A dynamic decision making model {{is built based on}} the definition of interval-valued triangular fuzzy soft set, in which period weight is determined by the exponential decay method. The arithmetic weighted average operator of interval-valued triangular fuzzy soft set is given by the aggregating thought, thereby aggregating interval-valued triangular fuzzy soft sets of different time-series into a collective interval-valued triangular fuzzy soft set. The formulas of selection and <b>decision</b> <b>values</b> of different objects are given; therefore the optimal decision making is achieved according to the <b>decision</b> <b>values.</b> Finally, the steps of this method are concluded, and one example is given to explain the application of the method...|$|R
40|$|Analyzes the {{traditional}} methods of extracting decision rules in Rough Sets, defines {{the concept of the}} decision dependability and proposes a novel algorithm of extracting short decision rules. Only the length of decision rules is extended when the current decision rules can’t classify all the samples in the decision table. At the same time, three methods are proposed to reduce the computational complexity: 1) defines the concept of bound coefficient, 2) only classify the samples with the same <b>decision</b> <b>values</b> at a time thus averting the time-consuming classification of the equivalence classes with different <b>decision</b> <b>values,</b> 3) defines the Remain set and only classify the samples in the Remain set, so the computational complexity will decrease proportional with the reduction of the samples in the Remain set. Above-mentioned methods can be used directly for incomplete information systems and have great practicability...|$|R
40|$|Previous {{work has}} {{demonstrated}} that human adolescents may be hypersensitive to rewards; it is unknown which aspect of reward processing this reflects. We separated <b>decision</b> <b>value</b> and prediction error signals and found that neural prediction error signals in the striatum peaked in adolescence, whereas neural <b>decision</b> <b>value</b> signals varied depending upon how value was modeled. This suggests that one contributor to adolescent reward-seeking may be heightened dopaminergic prediction error responsivity. Adolescence is a unique period in psychological development, characterized by increased risky choices and actions as compared to children and adults. This may reflect the relatively early functional development of limbic affective and reward systems in comparison to prefrontal cortex 1, such that adolescents tend to make poor decisions and risky choices more often than both children (who are not yet fully sensitive to rewards) and adults (who are sensitive to rewards, but {{have the ability to}} exert control over reward-driven urges). According to behavioral decision theories, choices are driven by the value assigned to each potential choice (<b>decision</b> <b>value)</b> 2. <b>Decision</b> <b>value</b> is computed by a system in the media...|$|E
3000|$|As Zou et al. {{approach}} calculates only <b>decision</b> <b>value</b> of incomplete soft set and {{the missing}} data remains still missing. While, Kong et al. approach has same results of d [...]...|$|E
30|$|A branch connects two nodes {{together}} {{and can also}} connect a node and a leaf. Each node {{is made up of}} branches labelled as the possible value of attributes in the parent node [23]. The leaves are labelled as the <b>decision</b> <b>value</b> of classification.|$|E
3000|$|... [...]) is {{estimated}} by LIBSVM [16]. LIBSVM uses the method in [21] to estimate class probability for classification problem. The basic {{idea of the}} method is to estimate pairwise (i.e., one-against-one) class probability from <b>decision</b> <b>values</b> (or cost values) and then solve an optimization problem to get multi-class probability. Compared to the simple potential form E(l [...]...|$|R
40|$|Abstract—A {{cooperative}} protocol called soft-decision-andforward (SDF) is introduced, which exploits {{the soft}} <b>decision</b> <b>values</b> of the received signal at the relay node. Alamouti code {{is used for}} orthogonal transmission and distributed spacetime codes are designed for non-orthogonal transmission. The maximum likelihood decoders with low decoding complexity are proposed. From simulations, {{it can be seen}} that SDF protocol outperforms AF protocol. S...|$|R
40|$|Abstract:- Level {{crossing}} rate, {{outage probability}} and average time of fade {{duration of the}} SSC combiner output signal are determined in this paper. Log-normal fading at the input is present. The results are shown graphically for different variance <b>values,</b> <b>decision</b> threshold <b>values</b> and fading parameters values...|$|R
40|$|Abstract — We {{consider}} stationary consensus protocols for {{networks of}} dynamic agents with fixed and switching topologies. At each time instant, each agent knows only its {{and its neighbors}} ’ state, but must reach consensus on a group <b>decision</b> <b>value</b> that is function of all the agents ’ initial state. We show that our protocol design is the solution of individual optimizations performed by the agents. This notion suggests a game theoretic interpretation of consensus problems as mechanism design problems. Under this perspective a supervisor entails the agents to reach a consensus by imposing individual objectives. We prove that such objectives can be chosen so that rational agents have a unique optimal protocol, and asymptotically reach consensus on a desired group <b>decision</b> <b>value.</b> I...|$|E
40|$|To {{overcome}} {{the disadvantage of}} CV-ACC method that the high-density sample region may {{be close to the}} optimal hyper-plane, a parameter selection method for support vector machine (SVM) based on the <b>decision</b> <b>value,</b> named as CV-SNRMDV method, is proposed in this paper. SNRMDV is used as the criterion of cross-validation (CV) in our method, which is defined as the ratio between the difference of medians of decision values and the sum of the standard deviations from the medians. Compared with the traditional cross-validation accuracy (CV-ACC) method, CV-SNRMDV makes use of the information of sample distribution and <b>decision</b> <b>value.</b> Consequently CV-SNRMDV overcomes the disadvantage of CV-ACC. The experiments show our method obtains a better test accuracy on the simulated dataset, while the test accuracies on benchmark datasets are close to CV-ACC...|$|E
40|$|Even {{a single}} night of total sleep {{deprivation}} (SD) can have dramatic effects on economic decision making. Here we tested the novel hypothesis that SD influences economic decisions by altering the valuation process. Using {{functional magnetic resonance imaging}} we identified value signals related to the anticipation and the experience of monetary and social rewards (attractive female faces). We then derived <b>decision</b> <b>value</b> signals that were predictive of each participant’s willingness to exchange money for brief views of attractive faces in an independent market task. Strikingly, SD altered <b>decision</b> <b>value</b> signals in ventromedial prefrontal cortex (VMPFC) in proportion to the corresponding change in economic preferences. These changes in preference were independent of the effects of SD on attention and vigilance. Our results provide novel evidence that signals in VMPFC track {{the current state of the}} individual, and thus reflect not static but constructed preferences...|$|E
30|$|Support vector machine (SVM) {{modeling}} is a machine-learning-based method. It {{involves a}} training phase with associated input and a predicting phase with target output <b>decision</b> <b>values.</b> In recent years, the method {{has become increasingly}} popular. The {{aim of this study}} is to carry out prediction of earthquake-induced landslides distribution in the area affected by the April 20 2013 Lushan earthquake based on GIS and the SVM model.|$|R
40|$|This paper {{introduces}} {{the notion of}} veto number that {{can be associated with}} agreement problems. An agreement problem has veto number # when # is the minimal number of processes that control the allowed <b>decision</b> <b>values,</b> i. e., if each of them changes its mind on the value it proposes, then it forces deciding on a di#erent value. The paper presents and investigates this concept...|$|R
5000|$|Unlike the UCC, the EA is a dues-paying organization, {{which means}} {{that it is not}} reliant on congregational benevolent support, as the UCC's {{conferences}} and national entities are; there is also no [...] "free-riding," [...] e.g., a congregation receiving services from the organization without paying for them. This, in turn, limits the size of the national staff, a <b>decision</b> <b>valued</b> by the politically conservative constituency of the EA, who frequently equate bureaucracy with liberalism.|$|R
30|$|The {{calculation}} of the parameter value of background payload bandwidth can be achieved using a sub-path transmission stream and the original background idle bandwidth ratio method. When the payload bandwidth ratio is between (0, 30  %), the transmission state is excellent. When it is between (30, 70  %), this situation can meet the transmission requirements. Nevertheless, network congestion may emerge at any time. When it is higher than 70  %, the server will automatically consider the transmission media to have consumed the payload bandwidth of path, which {{does not meet the}} demand. Under this condition, it needs to make payload distribution adjustments according to Formula (4). Policy <b>decision</b> <b>value</b> 2 in Table  3 indicates that the server needs to make payload distribution adjustments according to Formula (4), or redirect network to choose other transport paths. Policy <b>decision</b> <b>value</b> 3 suggests that none of the transmission schemes can meet the demand of media transmission, which should be stopped consequently.|$|E
40|$|Abstract: We {{consider}} an asynchronous distributed system prone to crash failures and present a protocol designed to solve several consecutive consensus instances. After specifying the Multiple Integrated Consensus problem, we propose {{a solution that}} follows the Paxos approach, but relies on another flexible interaction scheme. A subset of processes (namely the coordinators and the acceptors) ensures that eventually a single value is selected to become the <b>decision</b> <b>value.</b> Moreover, these processes also act to guarantee {{the persistence of the}} previous decisions and to regulate the sequence of consensus instances. In a recent past, two different protocols, namely FastPaxos by Boichat et al. and Fast Paxos (with a space) by Lamport, have been designed to reduce the latency of learning a <b>decision</b> <b>value</b> to respectively, three and two communication steps, in favorable circumstances. Our protocol unifies these two different strategies, in order to obtain the best performance gain, in some frequent scenarios. Key-words...|$|E
40|$|OBJECTIVE: We {{examine the}} use of {{information}} theory applied to a single cardiac troponin T (cTnT) (first generation monoclonal; Boehringer Mannheim Corp., Indianapolis, Indiana) used with the character of chest pain, electrocardiography (ECG) and serial ECG changes {{in the evaluation of}} acute myocardial infarction (AMI). We combined a single measure of cTnT (blinded to the investigators) with a creatine kinase MB isoenzyme (CK-MB) measurement to discover the best <b>decision</b> <b>value</b> for this test in a study of 293 consecutive patients presenting to the emergency department with symptoms warranting exclusion of AMI. METHODS: The <b>decision</b> <b>value</b> for determining whether cTnT is positive or negative was determined independently of the final diagnosis by examining the information in the cTnT and CKMB data. Using information theory, an autocorrelation matrix with a one-to-one pairing of the CKMB and troponin T was constructed. The effective information, also known as Kullback entropy, assigned the values for troponin T and for CKMB that have the lowest frequency of misclassification error. The Kullback entropy is determined by subtracting the data entropy from the maximum entropy of the data set in which the information has been destroyed. The assignment of the optimum decision values was made independently of the clinical diagnoses without the construction of a receiver-operator characteristic curve (ROC). The final diagnosis of AMI was independently determined by the clinicians and entered into the medical record. RESULTS: The <b>decision</b> <b>value</b> for cTnT was 0. 1 ng/ml as determined by the the information in the data. The method was validated within the same study by mapping the results so obtained into the diagnoses obtained independently by the clinicians using all of the methods at their disposal. The cTnT was different in AMI (n = 60) compared with non-AMI patients (n = 233) (2. 08 +/- 0. 21 vs. 0. 07 +/- 0. 10; p <. 0001). CONCLUSION: Information theory provides a strong framework and methodology for determining the <b>decision</b> <b>value</b> for cTnT which minimizes misclassification errors at 0. 1 ng/ml. The result has a strong correlation with other features in detecting AMI in patients presenting with chest pain...|$|E
40|$|This paper {{discusses}} {{a method}} to empirically determine and a use for reporting a <b>decision</b> level <b>value</b> for {sup 137 }Cs using a Canberra Accuscan II, direct radiobioassay (in vivo) system. The <b>decision</b> level <b>value</b> is {{used to determine the}} upper 5 % of in vivo measurements for the purpose of recounting individuals. The paper overviews decision level concepts, the applicability of ANSI N 13. 30 and ANSI N 42. 2 and describes the specific process employed...|$|R
40|$|Support vector {{machines}} Both {{the occurrence}} {{and intensity of}} facial expressions are critical to what the face reveals. While much {{progress has been made}} towards the automatic detec-tion of facial expression occurrence, controversy exists about how to estimate expression intensity. The most straight-forward approach is to train multiclass or regression models using intensity ground truth. However, collecting inten-sity ground truth is even more time consuming and expensive than collecting binary ground truth. As a shortcut, some researchers have proposed using the <b>decision</b> <b>values</b> of binary-trained maximum margin classifiers as a proxy for ex-pression intensity. We provide empirical evidence that this heuristic is flawed in practice as well as in theory. Unfortunately, there are no shortcuts when it comes to estimating smile intensity: researchers must take the time to collect and train on intensity ground truth. However, if they do so, high reliability with expert human coders can be achieved. Intensity-trained multiclass and regres-sion models outperformed binary-trained classifier <b>decision</b> <b>values</b> on smile in-tensity estimation across multiple databases and methods for feature extraction and dimensionality reduction. Multiclass models even outperformed binary– trained classifiers on smile occurrence detection. c © 2014 Elsevier Ltd. All rights reserved. 1...|$|R
40|$|Communicating {{about issues}} or crises {{has become one}} of the most {{important}} tasks for managers who must address various stakeholders that have become more influential in the <b>decisions,</b> <b>values,</b> policies, and practices of the organization. This article describes an issue communication exercise for upper-level undergraduate and graduate business students. The students learn to examine public issues that companies face and to use traditional communication formats to address internal audiences (via memos, flyers, and reports) and external audiences (via personal letters, public letters, and reports) by using discourse appropriate to the rhetorical contexts of the issues...|$|R
