53|39|Public
25|$|Director Robert Zemeckis drew {{inspiration}} for the visual effects of Beowulf from experience with The Polar Express, which used motion capture technology to create three-dimensional images of characters. Appointing Jerome Chen, whom Zemeckis worked with on The Polar Express, the two decided to chart realism as their foremost goal. Over 450 graphic designers were chosen for the project, the largest team ever assembled for an Imageworks-produced movie as of 2007. Designers at Imageworks generated new animation tools for facial, body, and cloth design especially for the movie, and elements of <b>keyframe</b> <b>animation</b> were incorporated into the movie to capture the facial expressions of the actors and actresses. The mead hall battle scene {{near the beginning of}} the film, among others, required numerous props that served as additional markers; these markers allowed for a more accurate manifestation of a battlefield setting as the battle progressed. However, the data being collected by the markers slowed down the studios' computer equipment, and five months were spent developing a new save/load system that would increase the efficiency of the studios' resources. To aid in the process of rendering the massive quantities of information, the development team used cached data. In the cases that using cached data was not possible, the scenes were rendered using foreground occlusion, which involves the blurring of different overlays of a single scene in an attempt to generate a single scene film.|$|E
5000|$|<b>Keyframe</b> <b>{{animation}}</b> with linear, Bézier, and TCB animation curves.|$|E
5000|$|A module {{provides}} {{support for}} JavaScript, CSS3 transition and CSS3 <b>keyframe</b> <b>animation</b> hooks within existing core and custom directives.|$|E
5000|$|<b>Animation</b> - <b>Keyframed</b> <b>animation</b> and raw channel {{manipulation}} (CHOPs), {{motion capture}} support ...|$|R
5000|$|<b>Keyframed</b> <b>animation</b> tools {{including}} inverse kinematics, armature (skeletal), hook, {{curve and}} lattice-based deformations, shape animations, non-linear animation, constraints, and vertex weighting.|$|R
40|$|We {{present a}} novel multi-resolution point sample {{rendering}} algorithm for <b>keyframe</b> <b>animations.</b> The algorithm accepts triangle meshes of arbitrary topology as input which are animated by specifying {{different sets of}} vertices at keyframe positions. A multi-resolution representation consisting of prefiltered point samples and triangles is built to represent the animated mesh {{at different levels of}} detail. We introduce a novel sampling and stratification algorithm to efficiently generate suitable point sample sets for moving triangle meshes. Experimental results demonstrate that the new data structure can be used to render highly complex <b>keyframe</b> <b>animations</b> like crowd scenes in real-time...|$|R
50|$|The {{generation}} of facial animation {{data can be}} approached in different ways: 1.) marker-based motion capture on points or marks {{on the face of}} a performer, 2.) markerless motion capture techniques using different type of cameras, 3.) audio-driven techniques, and 4.) <b>keyframe</b> <b>animation.</b>|$|E
5000|$|<b>Keyframe</b> <b>animation</b> is {{the least}} {{automated}} of the processes to create animation data although it delivers {{the maximum amount of}} control over the animation. It is often used in combination with other techniques to deliver the final polish to the animation. The keyframe data can be made of scalar values defining the morph targets coefficients or rotation and translation values of the bones in models with a bone based rig. Often to speed up the <b>keyframe</b> <b>animation</b> process a control rig is used by the animation. The control rig represents a higher level of abstraction that can act on multiple morph targets coefficients or bones at the same time. For example, a [...] "smile" [...] control can act simultaneously on the mouth shape curving up and the eyes squinting.|$|E
50|$|MotionBuilder is a {{professional}} 3D character animation software produced by Autodesk. It is used for virtual production, motion capture, and traditional <b>keyframe</b> <b>animation.</b> It was originally named Filmbox {{when it was first}} created by Canadian company Kaydara, later acquired by Alias and renamed to MotionBuilder. Alias in turn was acquired by Autodesk.|$|E
40|$|We {{present a}} sketch-based {{rotation}} editing system for enriching rotational motion in <b>keyframe</b> <b>animations.</b> Given {{a set of}} keyframe orientations of a rigid object, the user first edits its angular velocity trajectory by sketching curves, and then the system computes the altered rotational motion by solving a variational curve fitting problem. The solved rotational motion not only satisfies the orientation constraints at the keyframes, but also fits well the user-specified angular velocity trajectory. Our system is simple and easy to use. We demonstrate its usefulness by adding interesting and realistic rotational details to several <b>keyframe</b> <b>animations.</b> © 2011 Journal of Zhejiang University Science Editorial Office and Springer-Verlag Berlin Heidelberg. link_to_subscribed_fulltex...|$|R
40|$|This paper {{presents}} an integrated {{set of tools}} helping to describe movements of objects in a framework of <b>keyframed</b> <b>animation,</b> both in simple and complex scenes. We {{want to have a}} powerful, robust and fast tool for designing either quickly prototyped or finely tuned motions. To achieve that, we will bring together Hermite splines (with an optional simplified interface for tangents, extended from [KB 84]) and logarithms of quaternions for orientations [Han 93]...|$|R
40|$|In this paper, an {{interactive}} facial animation system is described. The system {{is built on}} top of the facial animation system developed by Keith Waters, which uses a muscle-based face model. We built {{an interactive}} facial animation system on top of it to produce <b>keyframe</b> <b>animations</b> of a synthetic face model. The user creates keyframes interactively by either giving predefined universal expressions to the face (or a meaningful combination of these expressions blended naturally on the face model), or giving expressions to the face by moving the muscle vectors defined on the face model. The user might also change the orientation of the face for a keyframe by rotating it in x- and y-directions. To create intermediate frames, we use cosine interpolation. Keywords: interpolation, <b>keyframing,</b> facial <b>animation,</b> expression blending...|$|R
50|$|At its inception, MicroStation {{was used}} in the {{engineering}} and architecture fields primarily for creating construction drawings; however, it has evolved through its various versions to include advanced modeling and rendering features, including boolean solids, raytracing, and <b>keyframe</b> <b>animation.</b> It can provide specialized environments for architecture, civil engineering, mapping, or plant design, among others.|$|E
50|$|In 2002, CinéGroupe {{acquired}} {{the rights to}} the five-minute short Love and Darph and approached animator Bernie Denk to direct the series, which was produced in association with Sci Fi US. Bernie Denk's team worked in Montreal on preproduction (character design, modeling and textures) while both Montreal and Malaysian teams worked on animation, lighting and compositing. <b>Keyframe</b> <b>animation</b> was chosen for its quality and animating control capabilities.|$|E
50|$|As well as {{supporting}} traditional <b>keyframe</b> <b>animation,</b> Motion {{introduced a}} system of pre-set 'behaviors' which can be combined to create realistic animations. For instance, the 'throw' behaviour will move an object across the screen. Combined with the 'gravity' behavior, it will simulate a realistic arc of motion. The effects can be tweaked utilizing various parameters, varying {{the strength of the}} bounces, the amount of gravity to apply and so on.|$|E
5000|$|... scenes can {{be placed}} as <b>keyframes</b> along an <b>animation</b> {{timeline}} ...|$|R
40|$|Keywords—animation, {{motion capture}} We {{describe}} a simple technique for editing captured or <b>keyframed</b> <b>animation</b> based on warping {{of the motion}} parameter curves. The animator interactively defines a set of keyframe-like constraints which are used to derive a smooth deformation that preserves the fine structure of the original motion. Motion clips are combined by overlapping and blending of the parameter curves. We show that whole families of realistic motions {{can be derived from}} a single captured motion sequence using only a few keyframes to specify the motion warp. Our technique makes it feasible to create libraries of reusable “clip motion. ”...|$|R
5000|$|The {{on-screen}} {{representation of}} the T-3000 was made by British effects company Double Negative. Footage combines Jason Clarke filmed on set, <b>keyframed</b> <b>animation,</b> and motion capture. Supervisor Peter Bebb said that the company tried to design the T-3000 like a computer would do it, focusing on design and battle efficiency, [...] "form follows function". Given it is a Terminator built out of a human, the result is [...] "a pure robot that sits under flesh structure", still retaining an overall human shape. The mechanical cells tried to resemble the material on stealth aircraft, with a result described as [...] "more matte than metal", resembling a slightly iridescent ceramic carbon.|$|R
50|$|StrataVision 3D was a {{comprehensive}} 3D computer {{graphics software package}} by Strata. Features include primitives-based modeling with texturising, <b>keyframe</b> <b>animation,</b> raytrace and later radiosity rendering.It is notable for {{being part of the}} first wave of 3D graphics in desktop publishing. One particular milestone was rendering the environment in the blockbuster game Myst entirely using StrataVision. The MultiMedia Novel, Sinkha, and the Warner Bros 3D Looney Tunes Project were also created initially using this software.|$|E
5000|$|The Image Metrics {{animation}} process captures facial details like eye movement, lip {{and tongue}} synching, subtle expressions and skin textures {{that can be}} compromised {{in the use of}} traditional motion capture methods. Advanced mapping technology allows Image Metrics to deliver facial animation results up to five times faster than motion capture methods and ten times faster than key frame animation (although this figure is subjective, since <b>Keyframe</b> <b>Animation</b> can be used to produce very different results.) ...|$|E
50|$|Imagica DigitalScape Bauhaus Entertainment (株式会社イマジカデジタルスケープ・バウハウスエンタテインメント) is a Japanese {{video game}} developer.Created in 2006 {{by the human}} {{resource}} agency Imagica DigitalScape (part of Imagica Robot Holdings Inc.), both merged in 2009 {{in order to be}} able to diversify its services spectrum.Bauhaus Entertainment has helped with the creation of over a dozen videogames from well known franchises.The company usually takes part in the creation of character modelling, background modelling, real-time animation and cutscene animation of large games. Bauhaus specializes in <b>keyframe</b> <b>animation</b> and motion capture of triple-A games.|$|E
40|$|Figure 1 : Conversational gesture used {{to compare}} the quality of {{animation}} methods (motion captured movements, close-up view). In this paper, we explore the perception of finger motions of virtual characters. In three experiments, designed to investigate finger animations, we asked the following questions: When are errors in finger motion noticeable? What are the consequences of these errors? What animation method should we recommend? We found that synchronization errors of as little as 0. 1 s can be detected, but that the perceptibility of errors is highly dependent on the type of motion. Errors in finger animations can change the interpretation of a scene even without altering its perceived quality. Finally, out of the four conditions tested – original motion capture, no motions, <b>keyframed</b> <b>animation</b> and randomly selected motions – the original motion captured movements were rated as having the highest quality...|$|R
40|$|In this thesis, {{a system}} called Video Looping is {{developed}} to analyze human cyclic motions. Video Looping {{allows users to}} extract human cyclic motion from a given video sequence. This system analyzes similarities from {{a large amount of}} live footage to find the point of smooth transition. The final cyclic loop is created using only a few output images. Video Looping is useful not only to learn and understand human movements, but also to apply the cyclic loop to various artistic applications. To provide practical animation references, the output images are presented as photo plate sequences to visualize human cyclic motion similar to Eadweard Muybridge's image sequences. The final output images can be used to create experimental projects such as composited multiple video loops or small size of web animations. Furthermore, they can be imported into animation packages, and animators can create <b>keyframe</b> <b>animations</b> by tracing them in 3 D software...|$|R
40|$|We {{describe}} a simple technique for editing captured or <b>keyframed</b> <b>animation</b> based on warping {{of the motion}} parameter curves. The animator interactively defines a set of keyframe-like constraints which are used to derive a smooth deformation that preserves the fine structure of the original motion. Motion clips are combined by overlapping and blending of the parameter curves. We show that whole families of realistic motions {{can be derived from}} a single captured motion sequence using only a few keyframes to specify the motion warp. Our technique makes it feasible to create libraries of reusable "clip motion. " 1 Introduction Systems for real-time 3 -D motion capture have recently become commercially available. These systems hold promise as a means of producing highly realistic human figure animation with more ease and efficiency than traditional techniques afford. Motion capture can be used to create custom animation, or to create libraries of reusable clip-motion. Clip-motion libraries c [...] ...|$|R
5000|$|Powerful, full-featured {{graphical}} {{user interface}}. VisIt’s {{graphical user interface}} allows novice users to quickly get started visualizing their data, as well as allowing power users access to advanced features. It automatically creates time-based animations from data sets that contain multiple time steps. In addition, {{it also has a}} <b>keyframe</b> <b>animation</b> capability that allows users to create sophisticated animations. VisIt allows users to pan, zoom, and rotate objects interactively using the mouse. It also gives users the ability to interactively size and position geometric objects such as planes and spheres.|$|E
50|$|According to Qube, Q {{ships with}} a range of {{features}} including: arbitrary scene rendering algorithm support, arbitrary shader program support (HLSL 2 - 4, GLSL, Cg, shader states), <b>keyframe</b> <b>animation,</b> simultaneous n-dimensional animation blending, animation state machines, multi-gigabyte texture manager, background data streaming, hierarchical LOD and scene management schemes, collision detection, network-enabled media pipeline, live editing of game content, scripting across all core and custom components, cross-platform data formats and APIs, platform-specific extended data formats and APIs, 2D and 3D audio with effects, background texture compression / decompression, user input, hardware accelerated math, Max and Maya exporters, application framework, command line tool framework, and cross-platform build.|$|E
50|$|Each {{method has}} its {{advantages}} {{and as of}} 2007, games and films are using either or both of these methods in productions. <b>Keyframe</b> <b>animation</b> can produce motions that would be difficult or impossible to act out, while motion capture can reproduce the subtleties of a particular actor. For example, in the 2006 film Pirates of the Caribbean: Dead Man's Chest, Bill Nighy provided the performance for the character Davy Jones. Even though Nighy doesn't appear in the movie himself, the movie benefited from his performance by recording the nuances of his body language, posture, facial expressions, etc. Thus motion capture is appropriate in situations where believable, realistic behavior and action is required, but the types of characters required exceed {{what can be done}} throughout the conventional costuming.|$|E
40|$|Motion {{control is}} one of the {{important}} components of animation. The process of creating motion of an animated character usually involves articulated model as shown in Figure 5. 1. Character animation can be described as the movement of multiple articulated joints hierarchically. All the articulated joints are connected by segments in a hierarchical form. Then, it is combined with a 3 D geometric model such as polygonal mesh. The trajectories or orientations of the joints are specified in the skeleton. The change of a root joint propagates to other sub-joints. For example, a rotation of hip joint made the entire leg to rotate (Jing 2005). There are four main techniques for creating 3 D character animation: keyframing, kinematics, dynamics and motion capture. In <b>keyframing</b> <b>animation,</b> the key values for the animated DOFs are specified by animator while the computer interpolates between these values. Motion capture is the process that is used to record the movements of the objects and to map the movements to a virtual character. These three techniques will be discussed in this chapter...|$|R
40|$|Recent {{advances}} in computer graphics techniques and increasing power of graphics hardware {{made it possible}} to display and animate large crowds in real-time. Most of the research efforts have been directed towards improving rendering or behavior control; the question how to author crowd scenes in an efficient way is usually not addressed. We introduce a novel approach to create complex scenes involving thousands of animated individuals in a simple and intuitive way. By employing a brush metaphor, analogous to the tools used in image manipulation programs, we can distribute, modify and control crowd members in real-time with immediate visual feedback. We define concepts of operators and instance properties that allow to create and manage variety in populations of virtual humans. An efficient technique allowing to render up to several thousands of fully three-dimensional polygonal characters with <b>keyframed</b> <b>animations</b> at interactive framerates is presented. The potential of our approach is demonstrated by authoring a scenario of a virtual audience in a theater and a scenario of a pedestrian crowd in a city. Categories and Subject Descriptors (according to ACM CCS) : I. 3. 3 [Picture/Image Generation]: Display algorithm...|$|R
40|$|Streamlining {{the process}} of editing motion capture data and <b>keyframe</b> {{character}} <b>animation</b> is a fundamental problem in the animation field. This paper explores a new method for editing character animation, by using a data-driven pose distance as a falloff to interpolate new poses seamlessly into the sequence. This pose distance is the measure given by Green's function of the pose space Laplacian. The falloff shape and timing extent are naturally suited to the skeleton's range of motion, replacing {{the need for a}} manually customized falloff spline. This data-driven falloff is somewhat analogous to the difference between a generic spline and the ``magic wand'' selection in an image editor, but applied to the animation domain. It also supports powerful non-local edit propagation in which edits are applied to all similar poses in the entire animation sequence...|$|R
50|$|VEGAS {{features}} {{integration with}} 24p DV. It {{is also one}} of the few NLEs which can convert other formats to 24p (or any format to any other format) without any kind of a plugin or third-party application support and is the only proprietary NLE that allows for multiple instances of the application to be opened simultaneously. Clips and sequences can be copied and pasted between instances of Vegas. One instance can be rendering a sequence in the background while the user continues to edit in a different instance of Vegas in the foreground. VEGAS provides sophisticated compositing including green screen, masking, and <b>keyframe</b> <b>animation.</b> Nesting allows a prior project to be included in another project modularizing the editing process so that an array of tracks and edits become one track for further editing. Any changes to the previous project become reflected in the later project. Nesting is especially helpful in large or complex or special effects projects as the final rendering suffers no generation loss.|$|E
50|$|Director Robert Zemeckis drew {{inspiration}} for the visual effects of Beowulf from experience with The Polar Express, which used motion capture technology to create three-dimensional images of characters. Appointing Jerome Chen, whom Zemeckis worked with on The Polar Express, the two decided to chart realism as their foremost goal. Over 450 graphic designers were chosen for the project, the largest team ever assembled for an Imageworks-produced movie as of 2007. Designers at Imageworks generated new animation tools for facial, body, and cloth design especially for the movie, and elements of <b>keyframe</b> <b>animation</b> were incorporated into the movie to capture the facial expressions of the actors and actresses. The mead hall battle scene {{near the beginning of}} the film, among others, required numerous props that served as additional markers; these markers allowed for a more accurate manifestation of a battlefield setting as the battle progressed. However, the data being collected by the markers slowed down the studios' computer equipment, and five months were spent developing a new save/load system that would increase the efficiency of the studios' resources. To aid in the process of rendering the massive quantities of information, the development team used cached data. In the cases that using cached data was not possible, the scenes were rendered using foreground occlusion, which involves the blurring of different overlays of a single scene in an attempt to generate a single scene film.|$|E
40|$|The {{demand of}} 3 D digital {{animation}} production nowadays had led animators and producers not to limit to only <b>keyframe</b> <b>animation</b> but other tools and technologies. Amongst the key factors in determining such tools may refer to content, data accuracy, sampling rate, {{the freedom of}} motion, flexibility, skill sets required and cost. This research provides a comprehensive study on two animation techniques namely <b>keyframe</b> <b>animation</b> and motion -capture...|$|E
40|$|Figure 1 : Our method augments hand-crafted {{character}} animations {{such as this}} sumo wrestler with high-quality secondary motion, {{using an}} efficient rig-space simulation method. We present an efficient method for augmenting <b>keyframed</b> charac-ter <b>animations</b> with physically-simulated secondary motion. Our method achieves a performance improvement of one to two orders of magnitude over previous work without compromising on qual-ity. This performance {{is based on a}} linearized formulation of rig-space dynamics that uses only rig parameters as degrees of free-dom, a physics-based volumetric skinning method that allows our method to predict the motion of internal vertices solely from de-formations of the surface, as well as a deferred Jacobian update scheme that drastically reduces the number of required rig evalua-tions. We demonstrate the performance of our method by compar-ing it to previous work and showcase its potential on a production-quality character rig...|$|R
40|$|Abstract. This work {{proposes a}} {{real-time}} virtual human multimodal expression model. Five modalities explore the affordances of the body: deterministic, non-deterministic, gesticulation, facial and vocal expression. Deterministic expression is <b>keyframe</b> body <b>animation.</b> Non-deterministic expression is robotics-based procedural body animation. Vocal expression is voice synthesis, through Festival, and parameterization, through SABLE. Facial expression is lip-synch and emotion expression through a parametric muscle-based face model. Inspired by psycholinguistics, gesticulation expression is unconventional, idiosyncratic and unconscious hand gestures animation described as sequences of Portuguese Sign Language hand shapes, position and orientation. Inspired by the arts, one modality {{goes beyond the}} body to explore the affordances {{of the environment and}} express emotions through camera, lights and music. To control multimodal expression, this work proposes a high-level integrated synchronized markup language – Expressive Markup Language. Finally, three studies, involving a total of 197 subjects, evaluated the model in storytelling contexts and produced promising results...|$|R
40|$|This paper {{proposes a}} new {{technique}} to support making 3 D character animations with DID (Dinosaur or Direct Input Device). The key idea is to superimpose {{the image of a}} CG model on a physical armature using video see-through Head-Mounted Display (HMD). Using this technique, animators can easily recreate the posture of the physical armature. In addition, the system shows candidates of postures for animators who are not familiar with target motions. We have implemented a prototype system for making character animations using this technique. Keywords: 3 D <b>animation,</b> <b>Keyframe,</b> Head-Mounted Display, Physical Armature...|$|R
