148|506|Public
25|$|When both {{parameters}} {{are less than}} one (α, β < 1), this is the anti-mode: the lowest point of the probability <b>density</b> <b>curve.</b>|$|E
5000|$|Typically, {{values of}} between 2 and 3.5 {{are used for}} [...] and {{have the effect of}} {{flattening}} the <b>density</b> <b>curve.</b> This is useful for highly-skewed distributions where the distances [...] and [...] are of very different sizes.|$|E
5000|$|One of {{the results}} of Fourier {{analysis}} is Parseval's theorem which states that the area under the energy spectral <b>density</b> <b>curve</b> is equal to the area under the square of the magnitude of the signal, the total energy: ...|$|E
5000|$|... #Caption: Comparing <b>density</b> <b>curves</b> for the PERT and {{triangular}} probability distributions ...|$|R
5000|$|... #Caption: Comparing <b>density</b> <b>curves</b> for the {{modified}} PERT distributions with different weights ...|$|R
40|$|Students who are {{studying}} quantum physics often find the graphing of wave functions and probability <b>density</b> <b>curves</b> difficult and time consuming. The common spreadsheet of Microsoft ExcelTM {{is an excellent}} tool for the teaching of physics. It {{makes it easy to}} construct illustrative examples, to manipulate variables, and to plot results in graphical form. In this article efforts have been made to generate spreadsheet for wave functions and probability <b>density</b> <b>curves</b> of quantum harmonic oscillator. A dynamic spreadsheet presentation can help students better appreciate how the graphs are produced from the relationship between the variables...|$|R
5000|$|The {{probability}} density {{function of the}} four parameter beta distribution {{is equal to the}} two parameter distribution, scaled by the range (c-a), (so that the total area under the <b>density</b> <b>curve</b> equals a probability of one), and with the [...] "y" [...] variable shifted and scaled as follows: ...|$|E
50|$|The above theorem {{holds true}} in the {{discrete}} cases as well. A similar result holds for power: the area under the power spectral <b>density</b> <b>curve</b> {{is equal to}} the total signal power, which is , the autocorrelation function at zero lag. This is also (up to a constant which depends on the normalization factors chosen in the definitions employed) the variance of the data comprising the signal.|$|E
50|$|The G7 Method is a {{printing}} procedure used for visually accurate color reproduction by putting emphasis on matching grayscale colorimetric measurements between processes. G7 stands for grayscale plus seven colors: the subtractive colors typically used in printing (Cyan, Magenta, Yellow, and Black) and the additive colors (Red, Green, and Blue). The method {{is used in}} many applications of printing such as offset lithography, flexography, and gravure since it uses a one-dimensional neutral print <b>density</b> <b>curve</b> (NPDC) to match neutral tonality between two G7 calibrated printing systems. The G7 method is not a completely accurate color management system nor is it officially standardized by the International Color Consortium (ICC).|$|E
30|$|Polarization {{curves and}} {{respective}} power <b>density</b> <b>curves</b> for DEFCs with membranes electrodes assemblies containing Nafion recast and Nafion-MPS composite membranes were measured at 80  °C (Fig.  4 a) and 130  °C (Fig.  4 b).|$|R
30|$|To give {{graphical}} {{view of the}} skewness {{and tail}} behaviors of the skew-slash t distributions, we plot the <b>density</b> <b>curves</b> of the univariate standard skew-slash t and contours of the bivariate standard skew-slash t below.|$|R
50|$|The G7 method solved {{this problem}} by {{creating}} Neutral Print <b>Density</b> <b>Curves</b> (NPDC) that related neutral density to the half-tone dot percentages of a print image rather than a TVI curve relating the input dot percentage to the output gain relative to the input percentage.|$|R
50|$|The CATS did {{extensive}} {{land use}} and activity surveys, {{taking advantage of the}} City work done by the Chicago Planning Commission. Hock’s work forecasting activities said what the land uses—activities were that would be accommodated under the <b>density</b> <b>curve.</b> Existing land-use data were arrayed in cross section. Land uses were allocated {{in a manner consistent with}} the existing pattern. The study area was divided into transportation analysis zones: small zones where there was a lot of activity, larger zones elsewhere. The original CATS scheme reflected its Illinois State connections. Zones extended well away from the city. The zones were defined to take advantage of Census data at the block and minor civil division levels. They also strived for homogeneous {{land use and}} urban ecology attributes.|$|E
50|$|Under {{increasing}} pressure, ice {{undergoes a}} number of transitions to other allotropic forms with higher density than liquid water, such as ice II, ice III, high-density amorphous ice (HDA), and very-high-density amorphous ice (VHDA).The unusual <b>density</b> <b>curve</b> and lower density of ice than of water is vital to life—if water was most dense at the freezing point, then in winter the very cold water at the surface of lakes and other water bodies would sink, the lake could freeze from the bottom up, and all life in them would be killed. Furthermore, given that water is a good thermal insulator (due to its heat capacity), some frozen lakes might not completely thaw in summer. The layer of ice that floats on top insulates the water below. Water at about 4 °C (39 °F) also sinks to the bottom, thus keeping {{the temperature of the}} water at the bottom constant (see diagram).|$|E
30|$|The log plot in Fig.  4 shows higher {{values of}} the caliper reading in the {{interval}} 3385 – 3393  m. The measured data in this interval may {{be affected by the}} bad hole. However, hole is good in the interval 3393 – 3420  m. Hence, measured data in this interval is suitable to establish regression relation of density with other available curves and thereby in construction of a synthetic <b>density</b> <b>curve.</b> In this case, deep resistivity and P-sonic showed a good correlation with the measured density and have been used to synthesize the <b>density</b> <b>curve.</b>|$|E
40|$|EN]Laser {{diffraction}} customarily used in {{the building}} industry for granulometric analysis of cement, has the additional advantage {{of being able to}} determine the behaviour of materials subjected to reactive liquids by monitoring the granulometric distribution curves. Through the distribution <b>density</b> <b>curves</b> obtained for different samples that show different affinities for calcium hydroxide the results on granulometry permit the differentiation of the materials {{as a function of the}} presence or absence of pozzolanic activity thus underscoring the advantage of using the laser technique in the study of this property. There is also a relationship among the reaction rate between the material and the lime, the degree of activity of the pozzolanic materials, and the displacement in the granulometric <b>density</b> <b>curves.</b> Peer reviewe...|$|R
50|$|Wavelet {{amplitude}} and phase spectra {{are estimated}} statistically {{from either the}} seismic data alone or {{from a combination of}} seismic data and well control using wells with available sonic and <b>density</b> <b>curves.</b> After the seismic wavelet is estimated, it is used to estimate seismic reflection coefficients in the seismic inversion.|$|R
30|$|In Fig.  4, track- 3, the red {{curve is}} the {{measured}} density data {{in which some}} poor quality data are observed against the wash-out section. The synthetic <b>density</b> <b>curves</b> are generated for this interval using the multi-linear regression equation established using deep resistivity (LLD) and DT data in the interval 3393 – 3420  m.|$|R
3000|$|... where A is {{the vector}} of CV values {{generating}} the <b>density</b> <b>curve</b> and M is the mode value. The indices ‘high’ and ‘low’ indicate {{the level of}} activity concentration.|$|E
3000|$|... (3) VaR is a {{straightforward}} risk measure tool, {{like in the}} service contract, according to the fitted probability <b>density</b> <b>curve,</b> it can be inferred that {{the probability that the}} NPV is large than 2.40 million US dollars is 60 [...]...|$|E
40|$|A {{method to}} {{generate}} digital random time histories is described. A random number sequence is shaped {{to give the}} desired spectral <b>density</b> <b>curve.</b> This finite set of numbers is then Inverse Fast Fourier Transformed (IFFT). The result is a pseudo random time history which has given spectral characteristics. An application of this technique is described. ...|$|E
3000|$|... c, %) was {{calculated}} {{based on a}} COD removal (CODin − CODef) and a measured current as described previously (Miyahara et al. 2013). Polarization and power <b>density</b> <b>curves</b> were drawn using a potentiostat (HZ- 5000, Hokuto Denko, Tokyo, Japan) as described previously (Miyahara et al. 2013), and the maximum power density (the peak in a power curve; P [...]...|$|R
40|$|We {{investigate}} filling {{dependence of}} the zigzag Hubbard ladder, using density matrix renormalization group method. We illustrate the chemical-potential vs. electron-density and spin gap vs. electron <b>density</b> <b>curves,</b> which reflect characteristic properties of the electron state. On {{the basis of the}} obtained phase diagram, we discuss the connection to a novel quasi-one-dimensional superconductor Pr_ 2 Ba_ 4 Cu_ 7 O_ 15 -δ. Comment: 5 pages, 6 figures, fig. 4 is adde...|$|R
40|$|The {{electric}} field amplitude (Eo) dependent dynamic ferroelectric hysteresis and polarization current <b>density</b> <b>curves</b> measured {{at room temperature}} for Na 0. 5 Bi 0. 5 TiO 3 (NBT), showed three different stages of polarization reversal mechanism. The scaling relationship confirmed the dominance of domain wall motion at stage-I (i. e., upto Eo o o o-dependent longitudinal piezoelectric response (d 33 and g 33 ) values match closely with our proposed polarization reversal mechanism...|$|R
3000|$|... where Acompensated is {{the vector}} of CV values {{generating}} the compensated <b>density</b> <b>curve</b> and Ashifted is the shifted CV vector. The mode {{value of the}} mean curve defined in the ‘Results’ section is 9.5 %. The shift is negative if the subject has a lower activity concentration in its healthy regions compared to the norm.|$|E
30|$|The <b>density</b> <b>curve</b> method applied {{has been}} {{described}} in detail previously [8]. Briefly, the coefficient of variation (CV) is calculated for overlapping cubic volumes (1  cm 3) covering the 3 D reconstructed activity distribution in the segmented lung, creating a 3 D CV matrix. The variation of activity uptake between subjects was compensated for in the calculation of CV values, {{based on the assumption}} that each subject had a healthy lung volume and that the healthy volumes in all subjects should generate equivalent CV values (for a detailed description, see the Appendix). The compensated CV values of the matrix were plotted as density curves (normalised CV distributions) with an area under the curve (AUC) of 100 %, for each subject. A CV-threshold value was defined, CVT. The area under the <b>density</b> <b>curve</b> (AUC) for CV values greater than the threshold value was then calculated for all subjects, i.e. AUC(CV[*]>[*]CVT).|$|E
40|$|ABSTRACT: The {{objective}} {{of this study is}} to present a risk analysis method for flood protection level decision. The concept of “risk ” is here defined as the product of flood damage and its occurrence probability. The study also presents a flood damage prediction model FDPM using GIS to calculate flood damages for any design storms with different return periods. The calculated monetary damages for the design storms with their occurrence probabilities enable us to quantify flood inundation risk as an Annual Risk <b>Density</b> <b>Curve</b> based on the concept of “risk. ” FDPM and the risk analysis method were applied to the storm design level decision of the Kanda River in the Tokyo Metropolis. One example of the applications of the risk analysis method to optimal storm design level decision is presented with cost curves: Risk Cost Reduction Curve and Capital Cost Curve given by the Annual Risk <b>Density</b> <b>Curve...</b>|$|E
40|$|We {{conducted}} a surreptitious, prospective, cohort study {{to explore how}} often physicians nod off during scientific meetings and to examine risk factors for nodding off. After {{counting the number of}} heads falling forward during 2 days of lectures, we calculated the incidence <b>density</b> <b>curves</b> for nodding-off episodes per lecture (NOELs) and assessed risk factors using logistic regression analysis. In this article we report our eye-opening results and suggest ways speakers can try to avoid losing their audience...|$|R
50|$|The density tool, on {{the other}} hand, {{measures}} {{the total number of}} formation electrons. Like the neutron tool, water-filled formations are used in its calibration process. Under these conditions, a lower number of electrons is equivalent to a lower formation density, or a higher formation porosity. Therefore, logging a gas-filled formation, results in a porosity estimate that is higher than the true porosity. Overlaying the neutron and <b>density</b> <b>curves</b> in a gas-bearing zone results in the classic crossover separation.|$|R
5000|$|... #Caption: Magnetization (J) or flux <b>density</b> (B) <b>curve</b> as a {{function}} of magnetic field intensity (H) in ferromagnetic material. The inset shows Barkhausen jumps.|$|R
40|$|The {{output signal}} of the {{electromagnetic}} flowmeter measuring the slurry flow will be disturbed by the slurry noise. It is necessary to study the characteristics of slurry noise {{in order to reduce}} the interference. The methodology for the estimation of the slurry noise power spectrum based on the ARMA model is presented in this paper, and the relation among the slurry noise, the velocity, and the concentration is obtained by means of the methodology above according to the 1 /f characteristics of the slurry noise. The results by computer simulation experiment show that if the concentration or flow velocity is increased, then slurry noise power spectral <b>density</b> <b>curve</b> in the logarithmic coordinate system will move to the upper right; if the concentration or flow velocity is reduced, then slurry noise power spectral <b>density</b> <b>curve</b> in the logarithmic coordinate system will move to the lower left...|$|E
30|$|Fourteen healthy {{subjects}} without documented {{lung disease}} or respiratory symptoms, and two patients with documented airway disease, inhaled on average approximately 90  MBq 99 mTc-Technegas {{immediately prior to}} the 20 -min SPECT acquisition. Variation in activity uptake between subjects was compensated for in resulting CV values. The area under the compensated CV <b>density</b> <b>curve</b> (AUC), for CV values greater than a threshold value CVT, AUC(CV[*]>[*]CVT), {{was used as the}} measure of ventilation heterogeneity.|$|E
40|$|We {{present a}} proof for the {{quantile}} normalization method proposed by Bolstad- 03 {{which has become}} one of the most popular methods to align density curves in microarray data analysis. We prove consistency of this method which is viewed as an application to <b>density</b> <b>curve</b> registration of the new method proposed in Dupuy-Loubes-Maza- 11, the structural expectation. Moreover, when this method fails in some case of mixture, we propose a new methodology to cope with this issue...|$|E
40|$|Level densities {{have been}} {{extracted}} from primary gamma spectra for 56, 57 -Fe and 96, 97 -Mo nuclei using (3 -He,alpha gamma) and (3 -He, 3 -He') reactions on 57 -Fe and 97 -Mo targets. The level <b>density</b> <b>curves</b> reveal step structures above the pairing gap {{due to the}} breaking of nucleon Cooper pairs. The location of the step structures in energy and their shapes arise from the interplay between single-particle energies and seniority-conserving and seniority-non-conserving interactions. Comment: 9 pages, including 5 figure...|$|R
30|$|Regional {{heterogeneity}} in {{lung ventilation}} SPECT images may {{be determined by}} calculating the coefficient of variation (CV) in small elements of the lung [8],[18]. Heterogeneity maps and <b>density</b> <b>curves</b> can then be generated based on the CV values. This method was used in earlier work, whereby <b>density</b> <b>curves</b> were evaluated using our proposed CVT method [8]. Using the revised method, the proportion of CV values greater than a threshold value CVT may be determined. The purpose of the CVT-method is to discriminate between activity distributions in the lungs of healthy subjects and subjects with affected ventilation. The method {{has been shown to}} be capable of identifying simulated mild COPD in an anthropomorphic phantom, and to differentiate patients with severe COPD from healthy subjects. The next step is to evaluate the ability of the CVT method to detect genuine abnormalities in subjects who are ‘borderline normal’, according to lung function tests. Since the outcome of a lung function test is a summation of the status of the whole lung, quite significant changes in lung function may occur before a deviation from the norm can be identified. A method that can reliably detect lung abnormalities earlier than lung function tests would therefore be beneficial.|$|R
40|$|We {{summarize}} the sensitivity {{achieved by the}} LIGO and Virgo gravitational wave detectors for compact binary coalescence (CBC) searches during LIGO's fifth science run and Virgo's first science run. We present noise spectral <b>density</b> <b>curves</b> {{for each of the}} four detectors that operated during these science runs which are representative of the typical performance achieved by the detectors for CBC searches. These spectra are intended for release to the public as a summary of detector performance for CBC searches during these science runs. Comment: 12 pages, 5 figure...|$|R
