12|13|Public
40|$|A {{family of}} satisfiable {{benchmark}} instances in {{conjunctive normal form}} is introduced. The instances are constructed by transforming a random regular graph into a system of linear equations followed by clausification. Schemes for introducing nonlinearity to the instances are developed, making the instances suitable for benchmarking solvers with <b>equivalence</b> <b>reasoning</b> techniques. An extensive experimental evaluation shows that state-ofthe-art solvers scale exponentially in the instance size. Compared with other well-known families of satisfiable benchmark instances, the present instances are among the hardest. benchmarking, Boolean satisfiability, <b>equivalence</b> <b>reasoning,</b> satisfiable in-Keywords: stance...|$|E
40|$|Parity constraints, {{common in}} {{application}} domains such as circuit verification, bounded model checking, and logical cryptanalysis, {{are not necessarily}} most efficiently solved if translated into conjunctive normal form. Thus, specialized parity reasoning techniques have been developed {{in the past for}} propagating parity constraints. This paper studies the questions of deciding whether unit propagation or <b>equivalence</b> <b>reasoning</b> is enough to achieve full propagation in a given parity constraint set. Efficient approximating tests for answering these questions are developed. It is also shown that <b>equivalence</b> <b>reasoning</b> can be simulated by unit propagation by adding a polynomial amount of redundant parity constraints to the problem. It is proven that without using additional variables, an exponential number of new parity constraints would be needed in the worst case. The presented classification and propagation methods are evaluated experimentally...|$|E
40|$|Abstract. This paper {{discusses}} several {{techniques to}} make the lookahead architecture for satisfiability (Sat) solvers more competitive. Our contribution consists of reduction of the computational costs to perform look-ahead and a cheap integration of both <b>equivalence</b> <b>reasoning</b> and local learning. Most proposed techniques are illustrated with experimental results of their implementation in our solver march eq. ...|$|E
40|$|In his {{habilitation}} theses Steve Kremer presents some selected {{research results}} {{in the area of}} formal analysis of security protocols. His contributions include application of formal methods to electronic voting protocols and security APIs, automated methods for verifying <b>equivalence</b> properties, compositional <b>reasoning</b> for security protocols and computational soundness results...|$|R
40|$|This paper {{presents}} a process calculus for reconfig-urable communicating systems which has broadcast as ba-sic communication primitive, and we provide an opera-tional semantics for this calculus. We illustrate the calculus through some examples, and we propose three behavioural <b>equivalences</b> for <b>reasoning</b> about systems of broadcasting processes, namely, barbed equivalence, step-equivalence and labelled bisimilarity. An important result, {{is that all}} these relations coincide, providing different ways to study the equivalence/non-equivalence of two systems. Then, we provide a direct characterization for the strong congruence relation induced by these equivalences. Finally, we give a complete axiomatisation for strong congruence. 1...|$|R
50|$|Support is {{provided}} for cryptographic primitives including: symmetric & asymmetric cryptography; digital signatures; hash functions; bit-commitment; and signature proofs of knowledge. The tool {{is capable of}} evaluating reachability properties, correspondence assertions and observational <b>equivalence.</b> These <b>reasoning</b> capabilities are particularly useful to the computer security domain since they permit the analysis of secrecy and authentication properties. Emerging properties such as privacy, traceability and verifiability can also be considered. Protocol analysis is considered with respect to an unbounded number of sessions and an unbounded message space. The tool is capable of attack reconstruction: when a property cannot be proved, an execution trace which falsifies the desired property is constructed.|$|R
40|$|In this paper, several {{techniques}} are discussed {{that make the}} lookahead architecture for satisfiability (SAT) solvers more competing. Our contribution consists of reduction of the computational costs to perform lookahead and the cheap integration of both <b>equivalence</b> <b>reasoning</b> and local learning. Most proposed {{techniques are}} illustrated with experimental results of their implementation in our solver march_eq...|$|E
40|$|Preprocessing {{has proven}} {{important}} in enabling efficient Boolean satisfiability (SAT) solving. For many real application scenarios of SAT {{it is important}} to be able to extract a full satisfying assignment for original SAT instances from a satisfying assignment for the instances after preprocessing. We show how such full solutions can be efficiently reconstructed from solutions to the conjunctive normal form (CNF) formulas resulting from applying a combination of various CNF preprocessing techniques implemented in the PrecoSAT solver—especially, blocked clause elimination combined with SatElite-style variable elimination and <b>equivalence</b> <b>reasoning...</b>|$|E
40|$|Propositional satisfiability (SAT) solvers, which {{typically}} operate using {{conjunctive normal form}} (CNF), have been successfully applied in many domains. However, in some application areas such as circuit verification, bounded model checking, and logical cryptanalysis, instances can have many parity (xor) constraints {{which may not be}} handled efficiently if translated to CNF. Thus, extensions to the CNF-driven search with various parity reasoning engines ranging from <b>equivalence</b> <b>reasoning</b> to incremental Gaussian elimination have been proposed. This paper studies how stronger parity reasoning techniques in the DPLL(XOR) framework can be simulated by simpler systems: resolution, unit propagation, and parity explanations. Such simulations are interesting, for example, for developing the next generation SAT solvers capable of handling parity constraints efficiently...|$|E
40|$|We {{show that}} any {{institution}} I satisfying some reasonable conditions {{can be transformed}} into another institution, Ib, which captures formally and abstractly the intuitions of adding support for behavioral <b>equivalence</b> and <b>reasoning</b> to an existing, particular algebraic framework. We call our transformation an ``extension'' because Ib has the same sentences as I and because its entailment relation includes that of I. Many properties of behavioral equivalence in concrete hidden logics follow as special cases of corresponding institutional results. As expected, the presented constructions and results can be instantiated to other logics satisfying our requirements as well, thus leading to novel behavioral logics, such as partial or infinitary ones, that have the desired properties...|$|R
40|$|PhDAlthough human causal {{reasoning}} is widely acknowledged {{as an object}} of scientific enquiry, there is little consensus on an appropriate measure of progress. Up-to-date evidence of the standard method of research in the field shows that this method has been rejected at the birth of modern science. We describe an instance of the standard scientific method for modelling {{causal reasoning}} (causal calculators). The method allows for uniform proofs of three relevant computational properties: correctness of the model with respect to the intended model, full abstraction of the model (function) with respect to the <b>equivalence</b> of <b>reasoning</b> scenarios (input), and formal relations of equivalence and subsumption between models. The method extends and exploits the systematic paradigm [Handbook of Logic in Artificial Intelligence and Logic Programming, volume IV, p. 439 - 498, Oxford 1995] to fit with our interpretation of it. Using the described method, we present results for some major models, with an updated summary spanning seventy-two years of research in the field...|$|R
40|$|Abstract: We {{propose a}} logic and a {{deductive}} system for stating and automatically proving the equivalence of programs in deterministic languages having a rewriting-based operational semantics. The deductive system is circular {{in nature and}} is proved sound and weakly complete; together, these results say that, when it terminates, our system correctly solves the program-equivalence problem as we state it. We show that our approach is suitable for proving the equivalence of both terminating and non-terminating programs, and also the equivalence of both concrete and symbolic programs. The latter are programs in which some statements or expressions are symbolic variables. By proving the equivalence between symbolic programs, one proves in one shot the equivalence of (possibly, infinitely) many concrete programs obtained by replacing the variables by concrete statements or expressions. We also report on a prototype implementation of the proposed deductive system in the K Framework. Key-words: Program <b>Equivalence,</b> Circular <b>Reasoning,</b> K framework...|$|R
40|$|Abstract. Structural logical {{formulas}} sometimes yield {{a substantial}} fraction of so called equivalence clauses after translating to CNF. The best known {{example of this}} feature is probably provided by the parity-family. The larger such CNF formulas cannot be solved in reasonable time if no extra reasoning with- and detection of- these clauses is incorporated. That is, in solving these formulas, {{there is a more}} or less separated algorithmic device in dealing with these equivalence clauses, called <b>equivalence</b> <b>reasoning,</b> and another dealing with the remaining clauses. In this paper we propose a way to align these two reasoning devices by introducing parameters for which we establish optimal settings over a variety of existing benchmarks. We obtain a truly convincing speed up in solving such formulas with respect to the best solving methods existing so far. ...|$|E
40|$|The march ks Sat solver is an {{upgraded}} {{version of the}} successful march dl and march eq Sat solvers, which won several awards at the Sat 2004 and Sat 2005 competitions. For the latest detailed description, we refer to [2]. Like its predecessors, march ks integrates <b>equivalence</b> <b>reasoning</b> into a DPLL architecture and uses lookahead heuristics to determine the branch variable in all nodes of the DPLL search-tree. The main improvements in march ks are: • renewed pre-processing techniques: Removal of the 3 -Sat translator and therefore a new procedure for the addition of resolvents. • an improved adaptive algorithm to trigger the DoubleLook procedure- inspired by the one used int satz by Li [1]. • a guided jumping strategy: Instead of the conventional depth-first search, march ks uses a jumping strategy based {{on the distribution of}} solutions measured on random 3 -Sat instances [3]. 2 pre-processing The pre-processor of march dl, reduces the formula at hand prior to calling the main solving (DPLL) procedure. Earlier versions already contained unit-clause and binary equivalence propagation, as well as <b>equivalence</b> <b>reasoning,</b> a 3 -Sat translator, and finally a full- using all free variables- iterative root look-ahead. However, march ks is the first version of march which does not use a 3 -Sat translator by default (although it is still optional). The motivation for its removal is to examine the effect of (not) using a 3 -Sat translator on the performance. Because the addition of resolvents was only based on the ternary clauses in the formula (after the translation) we developed a new algorithm for this addition which uses all clauses with at least three literals. 3 the architecture As a look-ahead Sat solver, the branch rule of march ks is based on a look-ahead evaluation function (Diff). The applied Diff measures the reduction of CNF- and equivalence-clauses between two formulas F and F ′ in a weighted manner. The solver differs from the straightforward look-ahead architecture in two aspects: (1) look-ahead is performed on a subset of the free (unfixed) variables, and (2) if a certain look-ahead significantly reduces the formula, the DoubleLookhead procedure is called to check whether this look-ahead will eventually result in a conflict. Algorithms below show the pseudocode of this architecture...|$|E
40|$|Current {{methods for}} solving Boolean satisfiability problem (SAT) are {{scalable}} enough to solve discrete nonlinear problems involving {{hundreds of thousands}} of variables. However, modern SAT solvers scale poorly with problems involving parity constraints (linear equations modulo 2). Gaussian elimination can be used to solve a system of linear equation effectively but it cannot be applied as such when the problem description also contains nonlinear constraints. A SAT solver typically reads the problem description in conjunctive normal form (CNF). Representing parity constraints in CNF results in an inefficient encoding which partly makes solving the problem harder. Also, the deduction methods used in SAT solvers are not efficient when solving problems involving parity constraints. This report develops an efficient xor-reasoning module for deciding the satisfiability of a conjunction of parity constraints and studies alternative ways to integrate the xor-reasoning module into a modern conflict-driven clause learning SAT solver. The novelty of the approach is the combination of conflict-driven clause learning techniques (CDCL) with <b>equivalence</b> <b>reasoning</b> enhancing deduction capabilities of CDCL SAT solvers beyond unit propagation on the CNF formula. The presented proof system and the abstract model for computing clausal explanations for inconsistent valuations of variables allow for different possible implementations. Key design principles along with alternative ways to compute clausal explanations and to integrate the xor-reasoning module into a SAT solver are presented, analyzed, and compared. We have integrated the xor-reasoning module into a state-of-the-art CDCL SAT solver, minisat. The applicability of the hybrid approach is evaluated experimentally and compared with a number of modern SAT solvers on three challenging benchmarks: randomly generated regular linear problems, a known keystream attack on the stream cipher Trivium, and a known plaintext attack on the block cipher DES. The results are promising: the number of heuristics decisions needed typically decreases without causing unbearable computational overhead. Larger problem instances may even be solved faster by minisat with the xor-reasoning module than by the unmodified version...|$|E
40|$|AbstractIn {{a process}} calculus, {{we say that}} a name x is uniformly {{receptive}} for a process P if:(1) at any time P is ready to accept an input at x, at least {{as long as there}} are processes that could send messages at x;(2) the input offer at x is functional, that is, all messages received by P at x are applied to the same continuation. In the π-calculus this discipline is employed, for instance, when modeling functions, objects, higher-order communications, or remote-procedure calls. We formulate the discipline of uniform receptiveness by means of a type system, and then we study its impact on behavioural <b>equivalences</b> and process <b>reasoning.</b> We develop some theory and proof techniques for uniform receptiveness, and illustrate their usefulness on some non-trivial examples...|$|R
40|$|Because {{contemporary}} large software {{systems are}} pervasively inconsistent, {{it is not}} safe to reason about them using classical logic. The goal of Direct Logic {{is to be a}} minimal fix to classical mathematical logic that meets the requirements of large-scale Internet applications (including sense making for natural language) by addressing the following issues: inconsistency robustness, contrapositive inference bug, and direct argumentation. Direct Logic makes the following contributions over previous work: * Direct Inference (no contrapositive bug for inference) * Direct Argumentation (inference directly expressed) * Inconsistency-robust deduction without artifices such as indices (labels) on propositions or restrictions on reiteration * Intuitive inferences hold including the following: * Boolean <b>Equivalences</b> * <b>Reasoning</b> by splitting for disjunctive cases * Soundness * Inconsistency-robust Proof by Contradiction Since the global state model of computation (first formalized by Turing) is inadequate to the needs of modern large-scale Internet applications the Actor Model was developed to meet this need. Using, the Actor Model, this paper proves that Logic Programming is not computationally universal in that there are computations that cannot be implemented using logical inference. Consequently the Logic Programming paradigm is strictly less general than the Procedural Embedding of Knowledge paradigm. Comment: Corrected: all types are stric...|$|R
40|$|In {{a process}} calculus, {{we say that}} a name x is uniformly {{receptive}} for a process P if: (1) at any time P is ready to accept an input at x, at least {{as long as there}} are processes that could send messages at x; (2) the input offer at x is functional, that is, all messages received by P at x are applied to the same continuation. In the -calculus this discipline is employed, for instance, when modeling functions, objects, higher-order communications, remote-procedure calls. We formulate the discipline of uniform receptiveness by means of a type system, and then we study its impact on behavioural <b>equivalences</b> and process <b>reasoning.</b> We develop some theory and proof techniques for uniform receptiveness, and illustrate their usefulness on some non-trivial examples...|$|R
40|$|Constraint {{satisfaction}} {{deals with}} developing automated techniques for solving computationally hard {{problems in a}} declarative fashion. The main emphasis of this thesis is on constraint satisfaction techniques for the propositional satisfiability problem (SAT). As solving techniques for propositional satisfiability have rapidly progressed during the last 15 years, implementations of decision procedures for SAT, so called SAT solvers, {{have been found to}} be extremely efficient as back-end search engines in solving large industrial-scale combinatorial problems. Since SAT solvers have become a standard tool for solving various real-world problem instances of increasing size and difficulty, there is a demand for more and more robust and efficient solvers. For understanding the success (and failures) of SAT solving in specific problem domains, it is important to investigate how different types of structural properties of SAT instances are related to the efficiency of solving the instances with different SAT-based constraint satisfaction techniques. This is the underlying motivation for this thesis. The emphasis of the thesis is on search-based SAT solving techniques for solving structured real-world problems. The work focuses on selected topics related to the analysis and development of both complete and stochastic local search methods for SAT. Both experimental and proof complexity theoretic approaches are applied. The main contributions are three-fold. The work contributes to the analysis of structure-based branching heuristics. A proof complexity theoretic power hierarchy is established for DPLL and clause learning SAT solvers with various structure-based branching restrictions. The proof complexity theoretic results are complemented by a detailed experimental evaluation of the effects of structure-based branching on state-of-the-art SAT solvers. In connection with structure-based branching in SAT, the work introduces the Extended ASP Tableaux proof system in the context of answer set programming, which is a field closely related to SAT solving. The work also contributes to the development of stochastic local search methods for structured real-world SAT instances. A novel non-clausal local search method for SAT is developed by incorporating the concept of justification frontiers previously applied in the context of complete non-clausal solvers. Variants of the method are analyzed with respect approximate completeness, and adaptive noise mechanisms aimed specifically for the method are developed. As a third point of view to structure in SAT, the work addresses the problem of generating hard satisfiable SAT instances for both DPLL-based and local search solvers by introducing the regular XORSAT model. Additionally, techniques for applying XORSAT instances specifically for benchmarking <b>equivalence</b> <b>reasoning</b> techniques in SAT solvers are developed...|$|E
40|$|Abstract. We {{show that}} any {{institution}} I satisfying some reasonable conditions {{can be transformed}} into another institution, Ibeh, which captures formally and abstractly the intuitions of adding support for behavioral <b>equivalence</b> and <b>reasoning</b> to an existing, particular algebraic framework. We call our transformation an &quot;extension &quot; because Ibeh has the same sentences as I and because its entailment relation includes that of I. Many properties of behavioral equivalence in concrete hidden logics follow as special cases of corresponding institutional results. As expected, the presented constructions and results can be instantiated to other logics satisfying our requirements as well, thus leading to novel behavioral logics, such as partial or infinitary ones, that have the desired properties. 1 Introduction Many approaches to behavioral equivalence are defined as extensions of morestandard algebraic frameworks, following relatively well understood methodologies. For example, hidden algebra is defined {{as an extension of}} algebraic specifi-cation: it adds appropriate machinery for experiments and then uses it to define behavioral equivalence as &quot;indistinguishability under experiments&quot;, also knownto be the largest behavioral congruence consistent with the visible data. Here we explore this problem from an abstract model theoretical perspective. We investigate conditions under which an institution admits behavioral extensions. The intuition of a behavioral signature extending an algebraic signature iscaptured categorically in a general way covering all cases of operations in current use, including the ones that tend to be problematic: constants of hidden sorts andoperations with multiple arguments of hidden sort. Let the original institution be I = (Sign, Sen, Mod, |=), let Ψ be a fixed signature in Sign called the visiblesignature, and le...|$|R
40|$|Security {{protocols}} {{play more}} and more important roles with wide use in many applications nowadays. Currently, there are many tools for specifying and verifying security protocols such as Casper/FDR, ProVerif, or AVISPA. In these tools, the intruder’s ability, which either needs to be specified explicitly or set by default, is not flexible in some circumstances. Moreover, whereas most of the existing tools focus on secrecy and authentication properties, few supports privacy properties like anonymity, receipt freeness, and coercion resistance, which are crucial in many applications such as in electronic voting systems or anonymous online transactions. In this paper, we introduce a framework for specifying security protocols in the labeled transition system (LTS) semantics model, which embeds {{the knowledge of the}} participants and parameterizes the ability of an attacker. Using this model, we give the formal definitions for three types of privacy properties based on trace <b>equivalence</b> and knowledge <b>reasoning.</b> The formal definitions for some other security properties, such as secrecy and authentication, are introduced under this framework, and the verification algorithms are also given. The results of this paper are embodied in the implementation of a SeVe module in a process analysis toolkit (PAT) model checker, which supports specifying, simulating, and verifying security protocols. The experimental results show that a SeVe module is capable of verifying many types of security protocols and complements the state-of-the-art security verifiers in several aspects. Moreover, it also proves the ability in building an automatic verifier for security protocols related to privacy type, which are mostly verified by hand now. No Full Tex...|$|R
40|$|Advances in {{networking}} and database technologies {{have made the}} concept of global information sharing possible. A rapidly growing number of applications require access to and manipulation of the data residing in multiple pre-existing database systems, which are usually autonomous and heterogeneous. A promising approach {{to the problems of}} interoperating multiple heterogeneous database systems is the construction of multidatabase systems. Among all of the research issues concerning multidatabase systems, schema management which involves with the management of various schemas at different levels in a dynamic environment has been largely overlooked in the previous research. Two most important research in schema management have been identified: schema translation and schema integration. The need for declarative and extensible approach to schema translation and the support for schema integration are accentuated in a large-scale environment. This dissertation presents a construct-equivalence-based methodology based on the implications of semantics characteristics of data models for schema translation and schema integration. The research was undertaken for the purposes of (1) overcoming the methodological inadequacies of existing schema translation approaches and the conventional schema integration process for large-scale MDBSs, (2) providing an integrated methodology for schema translation and schema normalization whose similarities of problem formulation has not been previously recognized, (3) inductively learning model schemas that provide a basis for declaratively specifying construct equivalences for schema translation and schema normalization. The methodology is based on a metamodel (Synthesized Object-Oriented Entity-Relationship (SOOER) model), an inductive metamodeling approach (Abstraction Induction Technique), a declarative construct equivalence representation (Construct Equivalence Assertion Language, CEAL), and its associated transformation and reasoning methods. The results of evaluation studies showed that Abstraction Induction Technique inductively learned satisfactory model schemas. CEAL's expressiveness and adequacy in meeting its design principles, well-defined construct <b>equivalence</b> transformation and <b>reasoning</b> methods, as well as the advantages realized by the construct-equivalence-based schema translation and schema normalization suggested that the construct-equivalence-based methodology be a promising approach for large-scale MDBSs...|$|R

