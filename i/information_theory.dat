10000|2718|Public
5|$|In {{the realm}} of science, notable UND alumni include {{important}} contributor to <b>information</b> <b>theory</b> Harry Nyquist, pioneer aviator Carl Ben Eielson, Arctic explorer Vilhjalmur Stefansson, engineer and NASA astronaut Karen L. Nyberg, and leading NASA manager John H. Disher.|$|E
5|$|In <b>information</b> <b>theory,</b> the {{definition}} of the amount of self-information and information entropy is often expressed with the binary logarithm, corresponding to making the bit the fundamental unit of information. However, the natural logarithm and the nat are also used in alternative notations for these definitions.|$|E
5|$|Miller's Language and Communication {{was one of}} {{the first}} {{significant}} texts in the study of language behavior. The book was a scientific study of language, emphasizing quantitative data, and was based on the mathematical model of Claude Shannon's <b>information</b> <b>theory.</b> It used a probabilistic model imposed on a learning-by-association scheme borrowed from behaviorism, with Miller not yet attached to a pure cognitive perspective. The first part of the book reviewed <b>information</b> <b>theory,</b> the physiology and acoustics of phonetics, speech recognition and comprehension, and statistical techniques to analyze language. The focus was more on speech generation than recognition. The second part had the psychology: idiosyncratic differences across people in language use; developmental linguistics; the structure of word associations in people; use of symbolism in language; and social aspects of language use.|$|E
40|$|This paper {{presents}} {{the advantages of}} <b>information</b> foraging <b>theory</b> matched with traditional <b>information</b> retrieval <b>theory</b> and user behavior analysis theory, a search content framework for <b>information</b> foraging <b>theory</b> is described, on a thor- ough review of the two research branches i. e. the basic concept of <b>information</b> foraging <b>theory</b> and the elementary mod- els of <b>information</b> foraging <b>theory,</b> an extended framework is proposed,. Several problems for future research are also identified through...|$|R
25|$|This analogy has {{withstood}} <b>information</b> <b>theory's</b> {{utility in}} simple, technical applications, but in biology, the social sciences, human language and behavior, {{it has been}} historically less successful. These attempts foundered by misunderstanding the conceptual framework of the theory rather than its mathematics. Reliance on ordinary language has made the <b>information</b> <b>theory's</b> insights less clear.|$|R
5000|$|Mathematical {{foundations}} of cybernetics, systems <b>theory,</b> communication and <b>information</b> <b>theories,</b> Structural models of qualitative data, and cyberspace ...|$|R
5|$|The modern form of {{a binary}} logarithm, {{applying}} to any number (not just powers of two) was considered explicitly by Leonhard Euler in 1739. Euler established the application of binary logarithms to music theory, long before their more significant applications in <b>information</b> <b>theory</b> and computer science became known. As part of his work in this area, Euler published a table of binary logarithms of the integers from 1 to 8, to seven decimal digits of accuracy.|$|E
5|$|In electronics, {{magnetic}} core memory, radar, single electron transistors, and inertial guidance controls were invented or substantially developed by MIT researchers. Harold Eugene Edgerton {{was a pioneer}} in high speed photography and sonar. Claude E. Shannon developed much of modern <b>information</b> <b>theory</b> and discovered the application of Boolean logic to digital circuit design theory. In the domain of computer science, MIT faculty and researchers made fundamental contributions to cybernetics, artificial intelligence, computer languages, machine learning, robotics, and cryptography. At least nine Turing Award laureates and seven recipients of the Draper Prize in engineering have been or are currently associated with MIT.|$|E
5|$|Reviewing {{the book}}, Charles E. Osgood {{classified}} {{the book as}} a graduate-level text based more on objective facts than on theoretical constructs. He thought the book was verbose on some topics and too brief on others {{not directly related to}} the author's expertise area. He was also critical of Miller's use of simple, Skinnerian single-stage stimulus-response learning to explain human language acquisition and use. This approach, per Osgood, made it impossible to analyze the concept of meaning, and the idea of language consisting of representational signs. He did find the book objective in its emphasis on facts over theory, and depicting clearly application of <b>information</b> <b>theory</b> to psychology.|$|E
5000|$|In research, {{scientists}} can use cloud-based quantum resources to test quantum <b>information</b> <b>theories,</b> perform experiments, compare architectures, amongst other things.|$|R
50|$|Time {{consistency}} {{is a key}} {{concept of}} information economics. Time consistency in information economics is tied to <b>information</b> <b>theory's</b> key finding that the information value of something is the greater the less of it is known in advance.|$|R
40|$|Recent studies {{claim that}} the more translators know about the {{structure}} and the dynamics of discourse, the more readily and accurately they can translate both the content {{and the spirit of}} a text. Similarly, international research projects highlight directions of research which aim at helping translators make reasonable and consistent decisions as to the relevance and reliability of source text features in the target text. Other recent studies stress the importance of developing <b>information</b> structure <b>theories</b> for translation. In line with such current research desiderata, the aim {{of this article is to}} test the relevance of <b>information</b> <b>theories</b> for translation. In the first part, <b>information</b> <b>theories</b> are presented from different linguistic perspectives. In the second part, their relevance for translation is tested on a series of texts by examining how they have been or can be applied to translation. The last part presents the conclusions of the analysis...|$|R
5|$|Szilard {{initially}} attended Palatine Joseph Technical University in Budapest, but his {{engineering studies}} were interrupted by {{service in the}} Austro-Hungarian Army during World War I. He left Hungary for Germany in 1919, enrolling at Technische Hochschule (Institute of Technology) in Berlin-Charlottenburg, but became bored with engineering and transferred to Friedrich Wilhelm University, where he studied physics. He wrote his doctoral thesis on Maxwell's demon, a long-standing puzzle {{in the philosophy of}} thermal and statistical physics. Szilard was the first to recognize the connection between thermodynamics and <b>Information</b> <b>theory.</b>|$|E
5|$|The {{earliest}} {{research into}} thinking machines {{was inspired by}} a confluence of ideas that became prevalent in the late 30s, 40s and early 50s. Recent research in neurology had shown that the brain was an electrical network of neurons that fired in all-or-nothing pulses. Norbert Wiener's cybernetics described control and stability in electrical networks. Claude Shannon's <b>information</b> <b>theory</b> described digital signals (i.e., all-or-nothing signals). Alan Turing's theory of computation showed that any form of computation could be described digitally. The close relationship between these ideas suggested that {{it might be possible to}} construct an electronic brain.|$|E
5|$|The {{invention}} of electronic {{computers in the}} 1940s, along {{with the development of}} mathematical <b>information</b> <b>theory,</b> led to a realization that brains can potentially be understood as information processing systems. This concept formed the basis of the field of cybernetics, and eventually gave rise to the field now known as computational neuroscience. The earliest attempts at cybernetics were somewhat crude in that they treated the brain as essentially a digital computer in disguise, as for example in John von Neumann's 1958 book, The Computer and the Brain. Over the years, though, accumulating information about the electrical responses of brain cells recorded from behaving animals has steadily moved theoretical concepts in the direction of increasing realism.|$|E
5000|$|Prediction {{market can}} be {{utilized}} to improve forecast {{and has a}} potential application to test lab-based <b>information</b> <b>theories</b> based on its feature of information aggregation. Researchers have applied prediction markets to assess unobservable information in Googleâ€™s IPO valuation ahead of time.|$|R
40|$|His main {{areas of}} {{research}} interest are corporate financial policy under asymmetric <b>information,</b> <b>theories</b> and regulation of banking activities cum crises, sharing of intellectual property under alternative institutional structures, and applied game theory. He has published around forty academic papers on these themes. Kjell G...|$|R
5000|$|... #Caption: Schematic {{diagram of}} <b>information</b> {{integration}} <b>theory</b> ...|$|R
5|$|As older {{problems}} are solved, more complexities are being introduced, like adding additional parts to an engine. Interaction {{of these new}} parts, or niches, create emergent properties, like time-saving office devices (e.g. email, mobile computers, etc.) make communication more efficient but also expand the network of contacts and {{increase the amount of}} time spent on such duties. Homer-Dixon relates this to complexity theory explaining that as new niches are filled there is a synergistic burst of simplicity. However, this can also lead to less control or freedom as emergent properties are created, like a new government program leading to a sprawling bureaucracy. <b>Information</b> <b>theory</b> is touched upon relating the amount of information required to describe a system and the degree of that system's complexity. Chaos theory is used to describe how small changes can lead to widely varying results and path dependence.|$|E
5|$|Szilard was {{appointed}} as assistant to von Laue at the Institute for Theoretical Physics in 1924. In 1927 {{he finished his}} habilitation and became a Privatdozent (private lecturer) in physics. For his habilitation lecture, he produced a second paper on Maxwell's Demon, Ãœber die Entropieverminderung in einem thermodynamischen System bei Eingriffen intelligenter Wesen (On the reduction of entropy in a thermodynamic system by the intervention of intelligent beings), that had actually been written soon after the first. This introduced the thought experiment now called the Szilard engine and became important {{in the history of}} attempts to understand Maxwell's demon. The paper is also the first equation of negative entropy and information. As such, it established Szilard as {{one of the founders of}} <b>information</b> <b>theory,</b> but he did not publish it until 1929, and did not pursue it further. Claude E. Shannon, who took it up in the 1950s, acknowledged Szilard's paper as his starting point.|$|E
25|$|The Shannon Centenary, 2016, {{marked the}} life and {{influence}} of Claude Elwood Shannon on the hundredth anniversary of his birth on April 30, 1916. It was inspired {{in part by the}} Alan Turing Year. An ad hoc committee of the IEEE <b>Information</b> <b>Theory</b> Society including Christina Fragouli, RÃ¼diger Urbanke, Michelle Effros, Lav Varshney and Sergio VerdÃº, coordinated worldwide events. The initiative was announced in the History Panel at the 2015 IEEE <b>Information</b> <b>Theory</b> Workshop Jerusalem and the IEEE <b>Information</b> <b>Theory</b> Society Newsletter.|$|E
5000|$|For related <b>information</b> see:Chaos <b>theory</b> and General {{relativity}} ...|$|R
5000|$|Journal of <b>Information</b> Technology <b>Theory</b> and Application (JITTA) ...|$|R
30|$|AK {{is among}} worldâ€™s leading {{industrial}} researchers and inventors. Through {{his career in}} technology, he contributed numerous technologies with worldwide impact. As with all inventors, his interests span beyond engineering and technology. He has a background in evolutionary computation, polymers, photochemistry, physics, thermodynamics, <b>information</b> <b>theories,</b> evolutionary biology and economics.|$|R
25|$|<b>Information</b> <b>theory</b> {{involves}} the quantification of information. Closely related is coding theory {{which is used}} to design efficient and reliable data transmission and storage methods. <b>Information</b> <b>theory</b> also includes continuous topics such as: analog signals, analog coding, analog encryption.|$|E
25|$|Coding {{theory is}} one of the most {{important}} and direct applications of <b>information</b> <b>theory.</b> It can be subdivided into source coding theory and channel coding theory. Using a statistical description for data, <b>information</b> <b>theory</b> quantifies the number of bits needed to describe the data, which is the information entropy of the source.|$|E
25|$|This {{provides}} {{a connection between}} quantum <b>information</b> <b>theory</b> and thermodynamics.|$|E
5000|$|... #Subtitle level 2: <b>Information</b> Processing: <b>Theory,</b> Model and {{technologies}} ...|$|R
50|$|The {{ultimate}} goal of neurocybernetic research is the technological implementation of major principles of information processing in biological organisms by probing cellular and network mechanisms of brain functions. To unravel the biological design principles, computer-aided analyses of neuronal structure and signal transmission based on modern <b>information</b> <b>theories</b> and engineering methods are employed.|$|R
40|$|Abstract â€” Separation kernels are {{the holy}} grail of secure systems, {{remaining}} elusive despite years of research into their design, implementation, and analysis. Though separation kernel research has achieved many successes, the disconnect between <b>information</b> flow <b>theory</b> and system implementation is a significant barrier to further progress. In this paper, we show how a particular branch of <b>information</b> flow <b>theory,</b> noninterference, can be utilized to formulate correctness and security properties of a microkernelstyle hypervisor. Thus, we not only provide a first step towards a formally verified separation kernel, but also reduce the gap between <b>information</b> flow <b>theory</b> and operating systems practice. I...|$|R
25|$|Gray, R. M. (2011), Entropy and <b>Information</b> <b>Theory,</b> Springer.|$|E
25|$|One early {{commercial}} {{application of}} <b>information</b> <b>theory</b> {{was in the}} field of seismic oil exploration. Work in this field made it possible to strip off and separate the unwanted noise from the desired seismic signal. <b>Information</b> <b>theory</b> and digital signal processing offer a major improvement of resolution and image clarity over previous analog methods.|$|E
25|$|Henri Theil, Economics and <b>Information</b> <b>Theory,</b> Rand McNally & Company - Chicago, 1967.|$|E
5000|$|Based on his {{research}} into computer-mediated communication, Walther introduced social <b>information</b> processing <b>theory</b> in 1992. Social <b>information</b> processing <b>theory</b> {{finds that the}} development of relationships via computer-mediated communication depends on sufficient time and message exchanges, and on the application of available communicative cues by users. The lack of nonverbal cues means that computer-mediated communications contain less information than face-to-face communications, however social <b>information</b> processing <b>theory</b> finds that longer and/or more frequent communication [...] as well as the use of other cues (i.e. spelling ability) [...] while participating in computer-mediated communication help address the issue of information exchange.|$|R
50|$|Bettman, James R. <b>Information</b> {{processing}} <b>theory</b> {{of consumer}} choice. Addison-Wesley Pub. Co., 1979.|$|R
40|$|Technics and Dialectics of the Information Society: Japanese Origins of <b>Information</b> Society <b>Theory</b> is a {{historical}} and theoretical analysis {{of the development of}} Japanese <b>Information</b> Society <b>Theory</b> from the origins of postwar Japanese capitalism to the present day. Making use of the methods of Political Economy and Critical Theory, it examines the contradictions of Japanese capitalism within a global context, and considers how <b>Information</b> Society <b>Theory</b> constituted both a strategy used by the Japanese capitalist class to overcome barriers to accumulation in the crisis of the late 1960 s â€“ early 1970 s, and an ideology of legitimation, the utopian dimension of which points beyond the theoryâ€Ÿs own limitations to possibilities for a renewal of socialist politics. In its concluding section the thesis briefly considers how <b>Information</b> Society <b>Theory</b> adapted to address the changing circumstances of Japan following the bubble economy of the 1980 s, and how it addressed the challenge of global neoliberalism...|$|R
