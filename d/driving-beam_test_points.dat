0|3996|Public
40|$|Abstract—An {{efficient}} {{method to}} select an optimum set of <b>test</b> <b>points</b> for dictionary techniques in analog fault diagnosis is proposed. This {{is done by}} searching for the minimum of the entropy index based on the available <b>test</b> <b>points.</b> First, the two-dimensional integer-coded dictionary is constructed whose entries are measurements associated with faults and <b>test</b> <b>points.</b> The problem of optimum <b>test</b> <b>points</b> selection is, thus, transformed to {{the selection of the}} columns that isolate the rows of the dictionary. Then, the likelihood for a column to be chosen based on the size of its ambiguity set is evaluated using the minimum entropy index of <b>test</b> <b>points.</b> Finally, the <b>test</b> <b>point</b> with the minimum entropy index is selected to construct the optimum set of <b>test</b> <b>points.</b> The proposed entropy-based method to select a local minimum set of <b>test</b> <b>points</b> is polynomial bounded in computational cost. The comparison between the proposed method and other reported <b>test</b> <b>points</b> selection methods is carried out by statistical experiments. The results indicate that the proposed method more efficiently and more accurately finds the locally optimum set of <b>test</b> <b>points</b> and is practical for large scale analog systems. Index Terms—Analog fault diagnosis, fault dictionary, rough set, <b>test</b> <b>point.</b> I...|$|R
40|$|Abstract—Test pointsprovide additionalcontrol todesignlogic and {{can improve}} circuit testability. Traditionally, <b>test</b> <b>points</b> are {{activated}} by a global test enable signal, and routing {{the signal to}} the <b>test</b> <b>points</b> can be costly. To address this problem, we propose a new <b>test</b> <b>point</b> structure that utilizes controllability don’t-cares to generate local <b>test</b> <b>point</b> activation signals. To support the structure, we propose new methods for extracting don’t-cares from assertions and finite state machines in the design. Our empirical evaluation shows that don’t-cares exist in many designs {{and can be used}} for reducing <b>test</b> <b>point</b> overhead. I...|$|R
40|$|By {{simplifying}} tolerance {{problem and}} treating faulty voltages on different <b>test</b> <b>points</b> as independent variables, integer-coded table technique is proposed {{to simplify the}} <b>test</b> <b>point</b> selection process. Usually, simplifying tolerance problem may induce a wrong solution while the independence assumption will result in over conservative result. To address these problems, the tolerance problem is thoroughly considered in this paper, and dependency relationship between different <b>test</b> <b>points</b> is considered at the same time. A heuristic graph search method is proposed to facilitate the <b>test</b> <b>point</b> selection process. First, the information theoretic concept of entropy is {{used to evaluate the}} optimality of <b>test</b> <b>point.</b> The entropy is calculated by using the ambiguous sets and faulty voltage distribution, determined by component tolerance. Second, the selected optimal <b>test</b> <b>point</b> is used to expand current graph node by using dependence relationship between the <b>test</b> <b>point</b> and graph node. Simulated results indicate that the proposed method more accurately finds the optimal set of <b>test</b> <b>points</b> than other methods; therefore, it is a good solution to minimize the size of the <b>test</b> <b>point</b> set. To simplify and clarify the proposed method, only catastrophic and some specific parametric faults are discussed in this paper...|$|R
40|$|Recently, Pomeranz and Reddy [7], {{presented}} a <b>test</b> <b>point</b> insertion method to improve path delay fault testability in large combinational circuits. A test application scheme was developed that allows <b>test</b> <b>points</b> to be utilized as primary inputs and primary outputs during testing. The placement of <b>test</b> <b>points</b> {{was guided by}} the number of paths and was aimed at reducing this number. Indirectly, this approach achieved complete robust path delay fault testability in very low computation times. In this paper, we use their test application scheme, however, we use more exact measures for guiding <b>test</b> <b>point</b> insertion like <b>test</b> generation and RD fault identification. Thus, we reduce the number of <b>test</b> <b>points</b> needed to achieve complete testability by ensuring that <b>test</b> <b>points</b> are inserted only on paths associated with path delay faults that are necessary to be tested and that are not robustly testable. Experimental results show that an average reduction of about 70 % in the number of <b>test</b> <b>points</b> ov [...] ...|$|R
3000|$|... are the ‘n’ <b>test</b> <b>points</b> [33]. The {{number of}} <b>test</b> <b>points</b> ‘n’, in each {{iteration}} step, determines {{the rate of}} convergence of the algorithm.|$|R
40|$|EP 0636895 (A 1) The present {{invention}} {{relates to}} a test grid for an unpopulated printed circuit, comprising <b>test</b> <b>points</b> (T) linked to measurement circuits. Each measurement circuit {{is linked to}} a group (G) of <b>test</b> <b>points,</b> the <b>test</b> <b>points</b> of each group being linked together by resistors (r) of non-zero values...|$|R
40|$|This paper {{presents}} an approach for embedding of test concepts into high-level synthesis. Symbolic <b>test</b> <b>points</b> declared by a test concept specification are mapped on physical registers. This {{is done by}} an assignment of <b>test</b> <b>points</b> to symbolic registers and by a usual register binding tool. The experiments show that this way of integration does not result in larger designs. 1 Introduction The use of built-in self test methods or partial scan paths requires the selection of <b>test</b> <b>points.</b> The selection of <b>test</b> <b>points</b> is a problem which is mostly considered after {{the design of the}} RT-structure. But it can be very difficult or even impossible to find a configuration of <b>test</b> <b>points,</b> which can realize a given test concept. In that cases, where it is impossible to find a configuration of <b>test</b> <b>points,</b> new registers are inserted in the design. These new registers are only used as <b>test</b> <b>points.</b> This paper {{presents an}} approach for embedding of test concepts using the design space. The test concep [...] ...|$|R
40|$|This paper {{presents}} an experimental investigation {{on the impact}} of <b>test</b> <b>point</b> insertion on circuit size and performance. Often <b>test</b> <b>points</b> are inserted into a circuit in order to improve the circuit’s testability, which results in smaller test data volume, shorter test time, and higher fault coverage. Inserting <b>test</b> <b>points</b> however requires additional silicon area and influences the timing of a circuit. The paper shows how placement and routing is affected by <b>test</b> <b>point</b> insertion during layout generation. Experimental data for industrial circuits show that inserting 1 % <b>test</b> <b>points</b> in general increases the silicon area after layout by less than 0. 5 % while the performance of the circuit may be reduced by 5 % or more. 1...|$|R
40|$|We {{propose a}} new {{algorithm}} for <b>test</b> <b>point</b> selection for scan-based BIST. The new algorithm combines {{the advantages of}} both explicit-testability-calculation and gradient techniques. The <b>test</b> <b>point</b> selection is guided bya cost function which is partially based on explicit testability recalculation and partially on gradients. With an event-driven mechanism, it can quickly identify a set of nodes whose testability need to be recalculated due to a <b>test</b> <b>point,</b> and then use gradients to estimate {{the impact of the}} rest of the circuit. In addition, by incorporating timing information into the cost function, timing penalty caused by <b>test</b> <b>points</b> can be easily avoided. We present the results to illustrate that high fault coverages for both area- and timing-driven <b>test</b> <b>point</b> insertions can be obtained with a small number of <b>test</b> <b>points.</b> The results also indicate a signi#cant reduction of computational complexity while the qualities are similar to the explicitly-testability-calculation method. 1 I [...] ...|$|R
3000|$|Because {{the optimal}} plan is not unique, {{to obtain a}} {{determined}} plan, one should restrict the arrangement mode of the stress level combinations (called <b>test</b> <b>points)</b> in the feasible region of the test (called test region), and restrict the sample location ratio on <b>test</b> <b>points.</b> Escobar and Meeker [44] proposed a method of obtaining the optimal non-degenerated plan (called splitting plan): find the <b>test</b> <b>point</b> ξ [...]...|$|R
40|$|This {{research}} {{was designed to}} determine a small set of low-beam <b>test</b> <b>points</b> for recommendation as common <b>test</b> <b>points</b> throughout the world. Our recommendation is a compromise among the following three set of inputs: (1) expert opinion, based on a worldwide survey of 119 experts in lighting and vision, (2) current practice, based on an analysis of candela matrices of 150 production low beams, and (3) scientific evidence concerning visibility and glare under nighttime driving conditions. Expert opinion and scientific evidence did not fully converge on the same <b>test</b> <b>points,</b> with the main difference being {{in the amount of}} light recommended for points at which objects need to be seen. While experts suggested light levels comparable to current production outputs, the recommendations based exclusively on scientific evidence would call for light levels of more than ten times the current levels. Therefore, the <b>test</b> <b>points</b> based exclusively on scientific evidence should be viewed only as ideal <b>test</b> <b>points,</b> but we should aim in the future to explore technologies that would make approximations to these <b>test</b> <b>points</b> feasible...|$|R
5000|$|After {{moving the}} <b>test</b> <b>points,</b> the linear {{equation}} part is repeated, {{getting a new}} polynomial, and Newton's method is used again to move the <b>test</b> <b>points</b> again. This sequence is continued until the result converges to the desired accuracy. The algorithm converges very rapidly.Convergence is quadratic for well-behaved functions—if the <b>test</b> <b>points</b> are within [...] of the correct result, they will be approximately within [...] of the correct result after the next round.|$|R
40|$|The <b>test</b> <b>point</b> {{insertion}} {{problem is}} that of select-ing t nodes in a combinational network as candi-dates for inserting observable <b>test</b> <b>points,</b> so as to minimize the number of test vectors needed to detect all single stuck-at faults in the network. In this paper we describe a dynamic programming approach to selecting the <b>test</b> <b>points</b> and provide an algorithm that inserts the <b>test</b> <b>points</b> optimally for fanout-free networks. We further extend this algo-rithm to general combinational networks with recon-vergent fanout. We also analyze the time complex-ity of the algorithm and show that it runs in O(n. t) time, where n {{is the size of}} the network and t is the number of <b>test</b> <b>points</b> to be inserted. As a side result we show that we can compute minimal test sets for a restricted class of networks that includes fanout. This extends previous results which were limited to fanout-free networks. 1...|$|R
5000|$|This {{intuitive}} {{approach can}} be made quantitative by defining the normalized distance between the <b>test</b> <b>point</b> and the set to be [...] By plugging this into the normal distribution we can derive the probability of the <b>test</b> <b>point</b> belonging to the set.|$|R
40|$|Golden section search {{strategies}} (GSSS), dichotomous search strategies (DSS), and Z-score strategies (ZSS) {{are simple}} and robust computerized adaptive testing strategies. GSSS, DSS, and {{one version of}} ZSS are similar in that statistical hypothesis testing occurs at each successive <b>testing</b> <b>point</b> in determining the current ability estimates. After each item is administered, the examinee 2 ̆ 7 s obtained score is compared with the expected score at successive <b>testing</b> <b>points.</b> If the examinee 2 ̆ 7 s obtained score does not exceed a confidence interval of an expected score at a <b>testing</b> <b>point,</b> the examinee 2 ̆ 7 s current ability estimate {{is assumed to be}} equal to that of the <b>testing</b> <b>point.</b> Otherwise, a hypothesis testing will be conducted at the next <b>testing</b> <b>point</b> and the process is continued until the examinee 2 ̆ 7 s current ability estimate is determined and the next item is selected. The three strategies differ in the successive <b>testing</b> <b>points</b> 2 ̆ 7 allocation. Each middle point of successive golden search regions is a <b>testing</b> <b>point</b> in GSSS; each middle point of successive dichotomous search regions is a <b>testing</b> <b>point</b> in DSS; each Z-score estimate evaluated at the previous <b>testing</b> <b>point</b> is the next <b>testing</b> <b>point</b> in ZSS. No hypothesis testing is involved in another version of ZSS in which the current ability estimate is the Z-score estimate evaluated at the previous ability estimate. Results of Monte Carlo studies in three hypothetical item pools and one SAT Verbal item pool showed that GSSS, DSS, and ZSS were computationally efficient, precise throughout the ability continuum, and robust against aberrant responses. Optimal measurement occurs using moderate size confidence intervals. Both versions of ZSS measured well under general conditions. GSSS, DSS, and ZSS provided more accurate and efficient ability estimates than did maximum likelihood estimate strategies (MLES) whenever guessing effect existed. GSSS, DSS, and ZSS were more efficient but not more accurate than MLES whenever guessing was not a factor. For GSSS, DSS, and ZSS, there were no differences in measurement accuracy, but occasional differences in measurement efficiency. GSSS, DSS and ZSS were more robust against guessing and inaccuracy of item parameters and took less time to execute than did MLES...|$|R
40|$|This paper {{presents}} a procedure for inserting <b>test</b> <b>points</b> at the outputs of scan {{elements of a}} full-scan circuit {{in such a manner}} that the peak power during scan testing is kept below a specified limit while maintaining the original fault coverage. If the power in a clock cycle during scan testing excee& a specified limit (which depen& on the peak power the chip has been designed to supply), a "peak power violation" is said to occur. Given a set of vectors, simulation is used to identify the cycles in which peak power violations occur (called "violating cycles"). For each violating cycle, the reduction in power caused by a control-O and control- 1 <b>test</b> <b>point</b> at each scan element is determined by simulation. The optimization problem then is to select as few <b>test</b> <b>points</b> as possible to eliminate all violating cycles. We present a heuristic procedure for minimizing the number of <b>test</b> <b>points</b> using integer linear programming techniques. The <b>test</b> <b>points</b> are activated and deactivated in a manner such that there is neither any loss in fault coverage nor peak power violations in the capture cycle. Experimental results indicate that the proposed procedure is very effective in controlling peak power during scan testing using a small number of <b>test</b> <b>points...</b>|$|R
5000|$|A {{technique}} {{has been described}} which opens up windows in the solder mask to create <b>test</b> <b>points</b> located directly on PCB tracks. This technique uses a conductive rubber tipped probe to contact the <b>test</b> <b>point</b> which could have a conductive Hot Air Solder Levelling (HASL) finish.|$|R
40|$|A {{method of}} testing the {{electrical}} functionality of an optically controlled switch in a reconfigurable antenna is provided. The method includes configuring one or more conductive paths between one or more feed points and one or more <b>test</b> <b>point</b> with switches in the reconfigurable antenna. Applying one or more test signals to {{the one or more}} feed points. Monitoring the one or more <b>test</b> <b>points</b> in response to the one or more test signals and determining the functionality of the switch based upon the monitoring of the one or more <b>test</b> <b>points...</b>|$|R
40|$|The set of test {{patterns}} {{applied to}} a circuit during built-in self-test (BIST) may not provide sufficiently high fault coverage due {{to the presence of}} hard-to-detect faults. This paper presents an innovative method for inserting <b>test</b> <b>points</b> in a way that complete (100 %) single stuck-at fault coverage is obtained for a specified set of test patterns. A very different approach is taken compared with previous <b>test</b> <b>point</b> insertion methods. Instead of using probabilistic techniques for <b>test</b> <b>point</b> placement, a path tracing procedure is used to place both control and observation points...|$|R
40|$|Abstract—This paper {{presents}} a novel <b>test</b> <b>point</b> insertion method for pseudorandom built-in self-test (BIST) {{to reduce the}} area overhead. The proposed method replaces dedicated flip-flops for driving control points by existing functional flip-flops. For each control point, candidate functional flip-flops are identified by using logic cone analysis that investigates the path inversion parity, logical distance, and reconvergence from each control point. Four types of new control point structures are introduced based on the logic cone analysis results to avoid degrading the testability. Experimental {{results indicate that the}} proposed method significantly reduces <b>test</b> <b>point</b> area overhead by replacing the dedicated flip-flops and achieves essentially the same fault coverage as conventional <b>test</b> <b>point</b> implementations using dedicated flip-flops driving the control points. Index Terms—Dedicated flip-flop, functional flip-flop, logic cone analysis, <b>test</b> <b>point</b> insertion. Ç...|$|R
40|$|AbstractWe {{consider}} {{the problem of}} verifying a simple polygon in the plane using “test points”. A <b>test</b> <b>point</b> is a geometric probe that takes as input a point in Euclidean space, and returns “+” if the point is inside the object being probed or “−” if it is outside. A verification procedure takes as input {{a description of a}} target object, including its location and orientation, and it produces a set of <b>test</b> <b>points</b> that are used to verify whether a test object matches the description. We give a procedure for verifying an n-sided, non-degenerate, simple target polygon using 5 n <b>test</b> <b>points.</b> This <b>testing</b> strategy works even if the test polygon has n + 1 vertices, and we show a lower bound of 3 n + 1 <b>test</b> <b>points</b> for this case. We also give algorithms using O(n) <b>test</b> <b>points</b> for simple polygons that may be degenerate and for test polygons that may have up to n + 2 vertices. All of these algorithms work for polygons with holes. We also discuss extensions of our results to higher dimensions...|$|R
40|$|In this paper, an {{automatic}} test pattern generator (ATPG) -based scan-path <b>test</b> <b>point</b> insertion technique, which can achieve high delay fault coverage for scan designs, is proposed. In the proposed technique, shift dependency between adjacent scan flip-flops that causes some delay faults to be untestable in standard scan environment, is broken by inserting <b>test</b> <b>points,</b> {{which can be}} combinational gates as well as flip-flops. Instead of topology-based approaches used in prior publications, the proposed technique uses a special ATPG to identify pairs of adjacent scan flip-flops between which <b>test</b> <b>points</b> are inserted to improve fault coverage. Since the proposed technique inserts <b>test</b> <b>points</b> only where they are necessary, it can drastically reduce hardware overhead compared to circuit topology-based techniques. 100 % transition delay coverage was attained for all ISCAS 89 benchmark circuits except one. This is achieved with very small numbers of <b>test</b> <b>points.</b> On an average, about 40 % reduction in scan chain length against a prior approach was achieved by the proposed method for benchmark circuits with default scan chain order...|$|R
40|$|While {{previous}} {{research has focused on}} deterministic testing of bridging faults, this paper studies pseudo-random testing of bridging faults and describes a means for achieving high fault coverage in a built-in self-test (BIST) environment. Bridging faults are generally more random pattern testable than shown to illustrate tha less random pattem tes method for identifying these random-pattem-resistant bridging faults is described. State-of-the-art <b>test</b> <b>point</b> insertion techniques, are based on the stuck-at fault model, are inadequa ta is presented which indicates that even after inserting <b>test</b> <b>points</b> that result in 100 % single stuck-ut fault coverage, many bridging faults are still not detected. A <b>test</b> <b>point</b> insertion procedure that targets both single stuck-at faults and non-feedback bridging faults is presented. It is shown that by considering both types of faults when selecting the location for <b>test</b> <b>points,</b> higher fault coverage can be obtained with little or no increase in overhead. Thus, the <b>test</b> <b>point</b> insertion procedure described here is a low-cost way {{to improve the quality of}} built-in self-test. 1...|$|R
30|$|Our cube {{determination}} {{experiment was}} conducted in room 517, with <b>test</b> <b>points</b> 1 to 12. In each <b>test</b> <b>point,</b> the target broadcasts a cube determination request every six seconds and about 300 requests are sent in total. Note that anchors in neighboring places (e.g., rooms) do not reply to these requests.|$|R
5000|$|Note {{that the}} error graph does indeed {{take on the}} values [...] at the six <b>test</b> <b>points,</b> {{including}} the end points, but that those points are not extrema. If the four interior <b>test</b> <b>points</b> had been extrema (that is, the function P(x)f(x) had maxima or minima there), the polynomial would be optimal.|$|R
40|$|Noise {{test has}} been done for {{a certain type of}} diesel {{locomotive}} under different working conditions for outfield and cab, and analyzed the noise characteristic of each <b>test</b> <b>point.</b> Through the analysis of the data :The sound pressure level of point D, E, I, J of 12 <b>test</b> <b>points</b> of outfield <b>test</b> is bigger due to the effect of noise source; The changing of working condition of locomotive mainly makes <b>test</b> <b>points</b> of the outfield produce big change in the high frequency after 400 Hz; The opening of cooling fan makes sound pressure level of <b>test</b> <b>points</b> of outfield improved obviously; The noise of cab is mainly produced by combined action of vibration of locomotive wall and external noise source: External noise source influences the whole frequency band of the cab, vibration makes individual frequency have big increased...|$|R
5000|$|Nelder-Mead in n {{dimensions}} {{maintains a}} set of n+1 <b>test</b> <b>points</b> arranged as a simplex. It then extrapolates {{the behavior of the}} objective function measured at each <b>test</b> <b>point,</b> in order to find a new <b>test</b> <b>point</b> and to replace one of the old <b>test</b> <b>points</b> with the new one, and so the technique progresses. The simplest approach is to replace the worst point with a point reflected through the centroid of the remaining n points. If this point is better than the best current point, then we can try stretching exponentially out along this line. On the other hand, if this new point isn't much better than the previous value, then we are stepping across a valley, so we shrink the simplex towards a better point. An intuitive explanation of the algorithm is presented in ...|$|R
50|$|The {{drawback}} of {{the above}} approach was that we assumed that the sample points are distributed about the center of mass in a spherical manner. Were the distribution to be decidedly non-spherical, for instance ellipsoidal, then we would expect the probability of the <b>test</b> <b>point</b> belonging to the set to depend {{not only on the}} distance from the center of mass, but also on the direction. In those directions where the ellipsoid has a short axis the <b>test</b> <b>point</b> must be closer, while in those where the axis is long the <b>test</b> <b>point</b> can be further away from the center.|$|R
5000|$|Most <b>test</b> <b>points</b> against England in one match(22 points - 1998) ...|$|R
5000|$|... #Caption: Percy Montgomery {{holds the}} South African record for <b>test</b> <b>points.</b>|$|R
50|$|Putting {{this on a}} {{mathematical}} basis, the ellipsoid that best represents the set's probability distribution can be estimated by building the covariance matrix of the samples. The Mahalanobis distance is {{the distance of the}} <b>test</b> <b>point</b> from the center of mass divided by the width of the ellipsoid {{in the direction of the}} <b>test</b> <b>point.</b>|$|R
40|$|This paper studies {{pseudo-random}} pattern {{testing of}} bridging faults. Although bridging faults {{are generally more}} random pattern testable than stuck-at faults, examples are shown to illustrate that some bridging faults can be much less random pattern testable than stuck-at faults. A fast method for identifying these random-pattern-resistant bridging faults is described. It is shown that state-of-the-art <b>test</b> <b>point</b> insertion techniques, {{which are based on}} the stuck-at fault model, are inadequate. Data is presented which indicates that even after inserting <b>test</b> <b>points</b> that result in 100 % single stuck-at fault coverage, many bridging faults are still not detected. A <b>test</b> <b>point</b> insertion procedure that targets both single stuck-at faults and non-feedback bridging faults is presented. It is shown that by considering both types of faults when selecting the location for <b>test</b> <b>points,</b> higher fault coverage can be obtained with little or no increase in overhead. Thus, the <b>test</b> <b>point</b> insertion procedure described here is a lowcost way {{to improve the quality of}} built-in self-test. While this paper considers only non-feedback bridging faults, the techniques that are described can be applied to feedback bridging faults in a straightforward manner...|$|R
30|$|The {{salinity}} {{varied from}} 1.5 to 200  g/L, with eight <b>test</b> <b>points.</b>|$|R
30|$|Relative error (RE) {{and root}} {{mean squared error}} (RMSE) were used to {{evaluate}} the error of the approximation models at <b>test</b> <b>points</b> other than those used in building the model. The <b>test</b> <b>points</b> comprises of ten new sample points within the sample space. These points are shown as green doted points augmented in the original design in Fig.  13.|$|R
40|$|Many {{theoretical}} and experimental {{studies have shown}} that aircraft flying in formation could experience significant reductions in fuel use compared to solo flight. To date, formation flight for aerodynamic benefit has not been thoroughly explored in flight for large transport-class vehicles. This paper summarizes flight data gathered during several two ship, C- 17 formation flights at a single flight condition of 275 knots, at 25, 000 ft MSL. Stabilized <b>test</b> <b>points</b> were flown with the trail aircraft at 1, 000 and 3, 000 ft aft of the lead aircraft at selected crosstrack and vertical offset locations within the estimated area of influence of the vortex generated by the lead aircraft. Flight data recorded at <b>test</b> <b>points</b> within the vortex from the lead aircraft are compared to data recorded at tare flight <b>test</b> <b>points</b> outside of the influence of the vortex. Since drag was not measured directly, reductions in fuel flow and thrust for level flight are used as a proxy for drag reduction. Estimated thrust and measured fuel flow reductions were documented at several trail <b>test</b> <b>point</b> locations within the area of influence of the leads vortex. The maximum average fuel flow reduction was approximately 7 - 8 %, compared to the tare points flown before and after the <b>test</b> <b>points.</b> Although incomplete, the data suggests that regions with fuel flow and thrust reduction greater than 10 % compared to the tare <b>test</b> <b>points</b> exist within the vortex area of influence...|$|R
40|$|In {{this paper}} {{we present a}} {{computational}} methodology {{to solve the problem}} of the proper design of the test matrix for an envelope expansion test campaign, where both flutter and systems testing are required (i. e. a new store integration). There are two different stakeholders involved: Structural Engineers (StE), who want to verify their predictions about the flutter free area, and the Systems Engineers (SyE), who want to investigate environmental aspects in the entire operational flight envelope. The test matrix, representing the <b>test</b> <b>points</b> distribution in the flight envelope, can be found solving an optimization problem with hard constraints (flight envelope boundaries) and different objective functions for the two stakeholders: the aim of the StE is to optimize distribution in Mach (M) range; the aim of SyE is to optimize distribution in Pressure Altitude (Hc) range; both of them want to maximize <b>test</b> <b>points</b> density near maximum equivalent airspeed (VE) area. Given the goals of the two stakeholders, the problem was formulated as a potential game, where StE control M distribution and SyE control Hc distribution, according to their respective strategies. The two players make their decision about <b>test</b> <b>points</b> location simultaneously, playing a spatial competition game. A simple Newton-Raphson method is sufficient to numerically solve the single <b>test</b> <b>point</b> location problem; a genetic algorithm is adopted to estimate the Nash equilibrium solutions to the multiple <b>test</b> <b>points</b> location problem. Results for the single, double and multiple <b>test</b> <b>points</b> location problems are shown...|$|R
