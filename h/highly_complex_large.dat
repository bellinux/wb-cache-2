5|10000|Public
40|$|Abstract. We present HyperGraphDB, a novel graph {{database}} {{based on}} generalized hypergraphs where hyperedges can contain other hyper-edges. This generalization automatically reifies every entity {{expressed in the}} database thus removing many of the usual difficulties in dealing with higher-order relationships. An open two-layered architecture of the data organization yields a highly customizable system where specific do-main representations can be optimized while remaining within a uniform conceptual framework. HyperGraphDB is an embedded, transactional database designed as a universal data model for <b>highly</b> <b>complex,</b> <b>large</b> scale knowledge representation applications such as found in artificial intelligence, bioinformatics and natural language processing...|$|E
40|$|This {{issue of}} the Journal is devoted to {{physical}} integration in Latin America. The Editorial Committee set about designing the contents, with a call for papers, which suggested topics and questions {{that focused on the}} subject while covering a broad spectrum. Regional physical integration is supported by a wide range of instruments; such as infrastructure works to reduce transportation costs and enhance connectivity, and financial structuring of technically, <b>highly</b> <b>complex</b> <b>large</b> scale capital investments. But no less relevant are government cooperation mechanisms that provide the necessary coordination for trade facilitation policies and measures, or the local impacts associated with enterprises, especially those located in border areas. ...|$|E
40|$|A general {{methodology}} for {{the generation of}} dipropionate functionalities using cyclic 7 -membered vinylsulfones has been devised {{for the purpose of}} synthesizing polyketide natural products such as aplyronine A. Final oxidative cleavage via ozonolysis {{has been shown to be}} difficult providing variable yields. Furthermore, elegant synthesis of the C 28 -C 34 segment of the aplyronine A actin binding tail has proved elusive. Utilization of OsO 4 and catalytic citric acid has led to a methodology whereby harsh ozonolysis procedures can be mostly avoided. A reengineering of the vinylsulfone polypropionate methodology in conjunction with osmylation has been found to provide the actin binding tail under mild high yielding conditions. These discoveries will be key in developing a methodology for providing <b>highly</b> <b>complex</b> <b>large</b> scale substrates for the synthesis of asymmetric natural products. ...|$|E
30|$|The ideal {{solution}} to combat interference {{is to allow}} all the BSs to coordinate and to perform MCP on a global scale. This is however <b>highly</b> <b>complex</b> as a <b>large</b> amount of data and CSI exchange is required. In this paper, we focus on finding a scalable {{solution to}} overcoming interference on the uplink.|$|R
40|$|Information {{systems have}} been {{introduced}} to accumulate real-time tracking data on containers and transporters at container terminals in ports. Logistics managers of container terminals need an intelligent tool to analyze the performance of <b>highly</b> <b>complex</b> and <b>large</b> logistics systems using the accumulated real-time tracking data. In this paper, all of the operational activities of an actual container terminal in Japan are simulated to analyze the processing time and the bottlenecks of the operations flows. The method for collecting the required data for performing simulation is described, especially by making use of electronic real-time tracking data that is accumulated from the information systems. The procedure is applied to an actual container terminal in a port. It is found that the information obtained by performing simulation is effective for analyzing {{the performance of the}} operation. ...|$|R
40|$|Dealing with {{uncertainty}} in dynamic environments {{is a basic}} challenge to au-tonomous systems. These systems are <b>highly</b> <b>complex,</b> involving <b>large</b> numbers of stochastic variables and many interacting nonlinear subsystems that operate at mul-tiple time granularities. An autonomous system {{must be able to}} monitor the state of its components and environment to form informed plans of intelligent action. But monitoring is a challenge for large-scale systems, because the cost of exact inference grows exponentially with the number of state variables. Thus, approximate monitor-ing algorithms are needed. Complex systems typically consist of loosely coupled subsystems. One approach to approximate monitoring exploits this idea by factoring the joint distribution of the system state into a product of marginal distributions over the subsystem states. However, sometimes this factoring approach is still computationally infeasible. An alternative approach to approximate monitoring is particle filtering (pf), in which the joint distribution over the system state is approximated by a set of samples o...|$|R
40|$|The {{performance}} of a chromatographic technique for polyester analysis at a moderate temperature was investigated. Sample dissolution at room temperature in phenol- 1, 1, 2, 2 -tetrachloroethane (TCE) and analysis at 70 degrees C in m-cresol, despite the efficiency for pure polyesters, proved to be inadequate for molecular mass determination of polyester/phosphite systems since it causes a rapid degradation of bonds created by the phosphite in the molten state. Furthermore, the presence of three different solvents makes the technique <b>highly</b> <b>complex.</b> <b>Large</b> modifications of the chromatographic conditions result, partly irreversibly, from the injection of samples dissolved in phenol-TCE. A new procedure, working close to room temperature with phenol-TCE eluent, was therefore developed, thus avoiding any polymer degradation and allowing {{for the study of}} polyester/phosphite samples by size exclusion chromatography. status: publishe...|$|E
40|$|A Manufacturing Execution System (MES) is a <b>highly</b> <b>complex,</b> <b>large,</b> {{multi-task}} {{application that}} is used to manage production in companies and factories. It monitors and tracks every aspect of all factory-based manufacturing processes. One of the challenges of a MES is to find ways of integrating it with other information techology (IT) systems; i. e., business process management (BPM) systems, so that compatible information may be shared between both systems. This work studies the integration of a local company MES into a BMP to assist with budgeting, in which a data set is gathered from the MES and a soft computing model helps the expert with cost-level estimation. Various modelling methods are used, such as fuzzy rule based ones, in order to determine whether white box or black box models are suitable for the task. The results of the study show how information may be integrated between manufacturing and business management software...|$|E
40|$|Within the {{electric}} power industry, the average company’s enterprise system- i. e. the over-all system of IT related entities- is today <b>highly</b> <b>complex.</b> Technically, <b>large</b> organizations posses {{hundreds or thousands}} of extensively interconnected and heterogeneous single IT systems performing tasks that varies from enterprise resource planning to real-time control and monitoring of industrial processes. Moreover are these systems storing a wide variety of sometimes redundant data, and typically they are deployed on several different plat-forms. IT does, however, not execute in splendid isolation. Organizationally, the enterprise system embraces business processes and business units using as well as maintaining and acquiring the IT systems. The interplay between the organization and the IT systems are further determined by for instance business goals, ownership and governance structures, strategies, individual system users, documentation, and cost. Lately, Enterprise Architecture (EA) has evolved with the mission to take a holistic ap-proach to managing the above depicted enterprise system. The discipline’s presumption is that architectural models are the key to succeed in understanding and administrating enter...|$|R
40|$|Automated {{chemistry}} laboratories {{dependent on}} robotic processes are the standard in both academic and large community hos-pital settings. Diagnostic microbiology manufacturers are betting that robotics {{will be used}} for specimen processing, plate read-ing, and organism identification in the near future. These systems are <b>highly</b> <b>complex</b> and have <b>large</b> footprints and hefty price tags. However, they are touted as being more efficient, rapid, and accurate than standard processes. Certain features, such as im-age collection, are highly innovative. Hospital administrators may be swayed to institute these new systems because of the prom-ise of the need for fewer skilled workers, higher throughput, and greater efficiency. They also may be swayed by the fact that workers with the requisite clinical microbiology skills are becomingmore difficult to find, and this technology should allow fewer skilled workers to handle larger numbers of cultures. In this Point-Counterpoint, Nate Ledeboer, Medical Director, Clini...|$|R
40|$|Actin {{filament}} networks play {{an active}} role in cytokinesis of eukaryotic cells. These networks, linked mainly by myosin, are concentrated below the cell membrane forming a spherical supporting shell. During cytokinesis, this network is modified such that a contractile ring is formed along the diameter of the shell. We present a realistic three-dimensional simulation model to study the dynamics of this spherical shell of elastic actin filaments and myosin motors. The results show compelling evidence that this fibre-spring model, with the motors activated in a narrow region around the division plane, is sufficient to reproduce most of the essential mechanics of cytokinesis: A spontaneous formation of a contractile ring, a characteristic filament orientation structure, and realistic cleavage furrow dynamics. These results demonstrate that, though cytokinesis is a <b>highly</b> <b>complex</b> process with <b>large</b> variation in intricate details, the fundamental dynamics are largely generic. In particular, motor mediated contraction of an unstructured filament mesh is sufficient to undergo division without concentrated and directed polymerization in the cleavage zone...|$|R
40|$|The {{structure}} of collagen {{has been a}} matter of curiosity, investigation, and debate {{for the better part of}} a century. There has been a particularly productive period recently, during which much progress has been made in better describing all aspects of collagen structure. However, there remain some questions regarding its helical symmetry and its persistence within the triple-helix. Previous considerations of this symmetry have sometimes confused the picture by not fully recognizing that collagen structure is a <b>highly</b> <b>complex</b> and <b>large</b> hierarchical entity, and this affects and is effected by the super-coiled molecules that make it. Nevertheless, the symmetry question is not trite, but of some significance as it relates to extracellular matrix organization and cellular integration. The correlation between helical structure in the context of the molecular packing arrangement determines which parts of the amino acid sequence of the collagen fibril are buried or accessible to the extracellular matrix or the cell. In this study, we concentrate primarily on the triple-helical {{structure of}} fibrillar collagens I and II, the two most predominant types. By comparing X-ray diffraction data collected from type I and type II containing tissues, we point to evidence for a range of triple-helical symmetries being extant in the molecules native environment. The possible significance of helical instability, local helix dissociation and molecular packing of the triple-helices is discussed in the context of collagen’s supramolecular organization, all of which must affect the symmetry of th...|$|R
40|$|The {{progression}} of liver fibrosis {{in response to}} chronic injury varies considerably among individual patients. The underlying genetics is <b>highly</b> <b>complex</b> due to <b>large</b> numbers of potential genes, environmental factors and cell types involved. Here, we provide the first toxicogenomic analysis of liver fibrosis induced by carbon tetrachloride in the murine 'genetic reference panel' of recombinant inbred BXD lines. Our aim was to define the core of risk genes and gene interaction networks that control fibrosis progression. Liver fibrosis phenotypes and gene expression profiles were determined in 35 BXD lines. Quantitative trait locus (QTL) analysis identified seven genomic loci influencing fibrosis phenotypes (pQTLs) with genome-wide significance on chromosomes 4, 5, 7, 12, and 17. Stepwise refinement was based on expression QTL mapping with stringent selection criteria, {{reducing the number of}} 1, 351 candidate genes located in the pQTLs to a final list of 11 cis-regulated genes. Our findings demonstrate that the BXD reference population represents a powerful experimental resource for shortlisting the genes within a regulatory network that determine the liver's vulnerability to chronic injury...|$|R
40|$|Time {{alignment}} of complex LC-MS data remains {{a challenge in}} proteomics and metabolomics studies. This work describes modifications of the Dynamic Time Warping (DTW) and the Parametric Time Warping (PTW) algorithms that improve the alignment quality for <b>complex,</b> <b>highly</b> variable LC-MS data sets. Regular DTW or PTW use one-dimensional profiles such as the Total Ion Chromatogram (TIC) or Base Peak Chromatogram (BPC) resulting in correct alignment if the signals have a relatively simple structure. However, when aligning the TICs of chromatograms from <b>complex</b> mixtures with <b>large</b> concentration variability such as serum or urine, both algorithms often lead to mis{{alignment of}} peaks and thus incorrect comparisons in the subsequent statistical analysis. This is mainly {{due to the fact}} that compounds with different m/z values but similar retention times are not considered separately but confounded in the benefit function of the algorithms using only one-dimensional information. Thus, it is necessary to treat the information of different mass traces separately in the warping function to ensure that compounds having the same m/z value and retention time are aligned to each other. The Component Detection Algorithm (CODA) is widely used to calculate the quality of an LC-MS mass trace. By combining CODA with the warping algorithms of DTW or PTW (DTW-CODA or PTW-CODA), we include only high quality mass traces measured by CODA in the benefit function. Our results show that using several CODA selected high quality mass traces in DTW-CODA and PTW-CODA significantly improves the alignment quality of three different, <b>highly</b> <b>complex</b> LC-MS data sets. Moreover, DTW-CODA leads to better preservation of peak shape as compared to the original DTW-TIC algorithm, which often suffers from a substantial peak shape distortion. Our results show that combination of CODA selected mass traces with different time alignment algorithm is a general principle that provide accurate alignment for <b>highly</b> <b>complex</b> samples with <b>large</b> concentration variability...|$|R
40|$|Cavitation and {{turbulence}} {{inside a}} diesel injector {{play a critical}} role in primary spray breakup and development processes. The study of cavitation in realistic injectors is chal-lenging, both theoretically and experimentally, since the associated two-phase flow field is turbulent and <b>highly</b> <b>complex,</b> characterized by <b>large</b> pressure gradients and small orifice geometries. We report herein a computational investigation of the internal nozzle flow and cavitation characteristics in a diesel injector. A mixture based model in FLUENT V 6. 2 software is employed for simulations. In addition, a new criterion for cavitation inception based on the total stress is implemented, and its effectiveness in predicting cavitation is evaluated. Results indicate that under realistic diesel engine conditions, cavitation patterns inside the orifice are influenced by the new cavitation criterion. Simu-lations are validated using the available two-phase nozzle flow data and the rate of injection measurements at various injection pressures (800 – 1600 bar) from the present study. The computational model is then used to characterize the effects of important injector parameters on the internal nozzle flow and cavitation behavior, as well as on flow properties at the nozzle exit. The parameters include injection pressure, needle lif...|$|R
50|$|Phanes are abstractions of <b>highly</b> <b>complex</b> organic {{molecules}} introduced for simplification of {{the naming of}} these <b>highly</b> <b>complex</b> molecules.|$|R
40|$|AbstractThe Jinping I {{hydropower}} {{station is}} a huge water conservancy project consisting of the highest concrete arch dam to date {{in the world and}} a <b>highly</b> <b>complex</b> and <b>large</b> underground powerhouse cavern. It is located on the right bank with extremely high in-situ stress and a few discontinuities observed in surrounding rock masses. The problems of rock mass deformation and failure result in considerable challenges related to project design and construction and have raised a wide range of concerns in the fields of rock mechanics and engineering. During the excavation of underground caverns, high in-situ stress and relatively low rock mass strength in combination with large excavation dimensions lead to large deformation of the surrounding rock mass and support. Existing experiences in excavation and support cannot deal with the large deformation of rock mass effectively, and further studies are needed. In this paper, the geological conditions, layout of caverns, and design of excavation and support are first introduced, and then detailed analyses of deformation and failure characteristics of rocks are presented. Based on this, the mechanisms of deformation and failure are discussed, and the support adjustments for controlling rock large deformation and subsequent excavation procedures are proposed. Finally, the effectiveness of support and excavation adjustments to maintain the stability of the rock mass is verified. The measures for controlling the large deformation of surrounding rocks enrich the practical experiences related to the design and construction of large underground openings, and the construction of caverns in the Jinping I hydropower station provides a good case study of large-scale excavation in highly stressed ground with complex geological structures, as well as a reference case for research on rock mechanics...|$|R
40|$|Abstract. Game-theoretic {{approaches}} {{have been proposed}} for addressing the complex problem of assigning limited security resources to protect a critical set of targets. However, many of the standard assumptions fail to address human adversaries who security forces will likely face. To address this challenge, previous research has attempted to integrate models of human decision-making into the game-theoretic algorithms for security settings. The current leading approach, based on experimental evaluation, is derived from a wellfounded solution concept known as quantal response and is known as BRQR. One critical difficulty with opponent modeling in general is that, in security domains, information about potential adversaries is often sparse or noisy and furthermore, the games themselves are <b>highly</b> <b>complex</b> and <b>large</b> in scale. Thus, we chose to examine a completely new approach to addressing human adversaries that avoids the complex task of modeling human decision-making. We leverage and modify robust optimization techniques {{to create a new}} type of optimization where the defender’s loss for a potential deviation by the attacker is bounded by the distance of that deviation from the expected-value-maximizing strategy. To demonstrate the advantages of our approach, we introduce a systematic way to generate meaningful reward structures and compare our approach with BRQR in the most comprehensive investigation to date involving 104 security settings where previous work has tested only up to 10 security settings. Our experimental analysis reveals our approach performing as well as or outperforming BRQR in over 90 % of the security settings tested and we demonstrate significant runtime benefits. These results are in favor of utilizing an approach based on robust optimization in these complex domains to avoid the difficulties of opponent modeling. ...|$|R
40|$|The Jinping I {{hydropower}} {{station is}} a huge water conservancy project consisting of the highest concrete arch dam to date {{in the world and}} a <b>highly</b> <b>complex</b> and <b>large</b> underground powerhouse cavern. It is located on the right bank with extremely high in-situ stress and a few discontinuities observed in surrounding rock masses. The problems of rock mass deformation and failure result in considerable challenges related to project design and construction and have raised a wide range of concerns in the fields of rock mechanics and engineering. During the excavation of underground caverns, high in-situ stress and relatively low rock mass strength in combination with large excavation dimensions lead to large deformation of the surrounding rock mass and support. Existing experiences in excavation and support cannot deal with the large deformation of rock mass effectively, and further studies are needed. In this paper, the geological conditions, layout of caverns, and design of excavation and support are first introduced, and then detailed analyses of deformation and failure characteristics of rocks are presented. Based on this, the mechanisms of deformation and failure are discussed, and the support adjustments for controlling rock large deformation and subsequent excavation procedures are proposed. Finally, the effectiveness of support and excavation adjustments to maintain the stability of the rock mass is verified. The measures for controlling the large deformation of surrounding rocks enrich the practical experiences related to the design and construction of large underground openings, and the construction of caverns in the Jinping I hydropower station provides a good case study of large-scale excavation in highly stressed ground with complex geological structures, as well as a reference case for research on rock mechanics...|$|R
5000|$|... (96360-96549) hydration, therapeutic, prophylactic, {{diagnostic}} injections and infusions, {{and chemotherapy}} and other <b>highly</b> <b>complex</b> drug or <b>highly</b> <b>complex</b> biologic agent administration ...|$|R
40|$|In {{the present}} work a “cork substrate” is {{developed}} aiming {{the achievement of}} an innovative material with ideal properties to respond to clothing and footwear applications. The preparation phases of leather processing are <b>highly</b> <b>complex,</b> with a <b>large</b> level of environmental aggressiveness. Garment and footwear industries are widely looking for new materials and applications able to reduce industrial pollutant charge, {{as well as new}} processes with lower water and energy consumption and higher economic advantages. Nowadays the conventional material to produce footwear is leather because it combines excellent properties such as: breathability, softness and thermal conductivity. A cork skin is laminated with membranes and textile fabrics and a comparison with leather properties has been done. The inner layer is made with a twill fabric that can be dyed and finished to confer functionalized properties. The results obtained are very promising and the possibility of using this laminated is demonstrated. We gratefully acknowledge the financial support from QREN (Quadro de Referência Estratégico Nacional – National Strategic Reference Framework), for this study “COLTEC”, Project no 2011 / 19280, co-financiado pelo FEDER, Programa Operacional Regional Norte. The authors wish to express their acknowledgment to FCT and FEDER-COMPETE funding, under the project PEst-C/CTM/UI 0264 / 2011...|$|R
40|$|Two {{measures}} of unemployment duration {{were obtained for}} each individual from data collected by Istat through the Quarterly Labour Force Survey. One measure was the length of on-going spells of unemployment, declared by the interviewed individual in a first wave and suitably adjusted considering information in a second wave (Spell Answers). The other was the time elapsed between the dates {{of the end of}} the previous job and the beginning of the current job in the second wave (Spell Reconstructions). They were modelled by traditional duration models via a mixture distribution of the answered and reconstructed spells, instead of using their minimum. The likelihood of the mixture is <b>highly</b> <b>complex</b> with a <b>large</b> number of covariates. Hence, classical optimisation methods do not provide robust parameter estimations. A stochastic search algorithm, called Differential Evolution, is proposed to tackle the numerical optimisation problem. In fact, even if DE is still rather unknown, it proves to be consistently and clearly superior to other search heuristic and classical optimisation approaches in many applications. Moreover, it is very simple to implement and requires little or no parameter tuning. A comparison between the ordinary duration models based on a sole unemployment spell for individuals (spell answers or spell reconstructions) is also reported. JEL classification: C 41, J 6...|$|R
40|$|Recently, {{more and}} more {{attention}} is drawn {{to the field of}} medical image synthesis across modalities. Among them, the synthesis of computed tomography (CT) image from T 1 -weighted magnetic resonance (MR) image is of great importance, although the mapping between them is <b>highly</b> <b>complex</b> due to <b>large</b> gaps of appearances of the two modalities. In this work, we aim to tackle this MR-to-CT synthesis by a novel deep embedding convolutional neural network (DECNN). Specifically, we generate the feature maps from MR images, and then transform these feature maps forward through convolutional layers in the network. We can further compute a tentative CT synthesis from the midway of the flow of feature maps, and then embed this tentative CT synthesis back to the feature maps. This embedding operation results in better feature maps, which are further transformed forward in DECNN. After repeat-ing this embedding procedure for several times in the network, we can eventually synthesize a final CT image in the end of the DECNN. We have validated our proposed method on both brain and prostate datasets, by also compar-ing with the state-of-the-art methods. Experimental results suggest that our DECNN (with repeated embedding op-erations) demonstrates its superior performances, in terms of both the perceptive quality of the synthesized CT image and the run-time cost for synthesizing a CT image...|$|R
50|$|Glass' {{behaviour}} when heated is <b>highly</b> <b>complex.</b>|$|R
50|$|A woven silk lamba {{featuring}} <b>highly</b> <b>complex</b> geometric designs.|$|R
40|$| that a cell is a <b>highly</b> <b>complex</b> and {{organized}} system. |$|R
50|$|The {{position}} of Christians affected by Nazism is <b>highly</b> <b>complex.</b>|$|R
50|$|The {{position}} of Christians in Nazi Fascism is <b>highly</b> <b>complex.</b>|$|R
50|$|However, if {{the process}} is <b>highly</b> <b>complex</b> and nonlinear, subject to {{frequent}} disturbances, a nonlinear model will be required. Biologically motivated intelligent controllers have been increasingly employed in these situations. Amongst them, fuzzy logic, neural networks and genetic algorithms {{are some of the}} most widely employed tools in control applications with <b>highly</b> <b>complex,</b> nonlinear settings.|$|R
2500|$|The Natchez verb is <b>highly</b> <b>complex</b> {{and has the}} {{following}} morphological structure: ...|$|R
5000|$|Defiance Future Shock <b>Highly</b> <b>Complex</b> Machinery (CD, Album, Dig) Jarring Effects 2010 ...|$|R
5000|$|The Natchez verb is <b>highly</b> <b>complex</b> {{and has the}} {{following}} morphological structure: ...|$|R
5000|$|... {{generation}} is distributed across a vast geographical area (e.g., a country), {{and therefore the}} response of the electrical grid, itself a <b>highly</b> <b>complex</b> system, has to be taken into account: even if the production levels of all units are known, checking whether the load can be sustained and what the losses are requires <b>highly</b> <b>complex</b> power flow computations.|$|R
40|$|Yield under drought {{stress is}} a <b>highly</b> <b>complex</b> trait with <b>large</b> {{influence}} to even a minor fluctuation {{in the environmental}} conditions. Genomics-assisted breeding holds great promise for improving such complex traits more efficiently in less time, but requires markers associated with the trait of interest. In this context, a recombinant inbred line mapping population (TAG 24 × ICGV 86031) was used to identify markers associated with quantitative trait loci (QTLs) for yield and yield related traits at two important locations of West Africa under well watered and water stress conditions. Among the traits analyzed under WS condition, the harvest index (HI) and the haulm yield (HYLD) were positively correlated with the pod yield (PYLD) and showed intermediate broad sense heritability. QTL analysis using phenotyping and genotyping data resulted in identification of 52 QTLs. These QTLs had low phenotypic variance (< 12 %) for all the nine traits namely plant height, primary branching, SPAD chlorophyll meter reading, percentage of sound mature kernels, 100 kernel weight, shelling percentage, HI, HYLD and PYLD. Interestingly, few QTLs identified {{in this study were}} also overlapped with previously reported QTLs detected for drought tolerance related traits identified earlier in Indian environmental conditions using the same mapping population. Accumulating these many small-effect QTLs into a single genetic background is nearly impossible through marker-assisted backcrossing and even marker-assisted recurrent selection. Under such circumstances, the deployment of genomic selection is the most appropriate approach for improving such complex traits with more precision and in less time...|$|R
40|$|International audienceLake and {{reservoir}} {{managers are}} in need of simple tools for predicting harmful algal blooms. But the process-based models described in the literature are often <b>highly</b> <b>complex</b> and require <b>large</b> data sets. Beyond a certain degree, adding processes was shown to reduce model predictive capabilities. In this work, we assess the performance of a simple biological model to describe the succession of cyanobacterial blooms in a reservoir. Our study site is Karaoun Reservoir, Lebanon, a eutrophic reservoir where cyanobacteria Aphanizomenon ovalisporum and Microcystis aeruginosa alternatively bloom. We used the following configuration of the one-dimensional hydrodynamic-ecological model Dyresm-Caedym: both cyanobacteria were simulated, with constant buoyancy for Aphanizomenon ovalisporum, vertical migration dependent on light and internal nitrogen for Microcystis aeruginosa and growth limited by light and temperature for both species. The model was calibrated in summer and autumn 2012 and validated in spring and summer 2013. It showed a good performance for the water level (RMSE 1 m, annual variation 25 m), water temperature profiles (RMSE 1 °C, range 13 - 28 °C) and cyanobacteria biomass (RMSE 48 µg L- 1 equivalent chlorophyll a, range 0 - 206 µg L- 1). This shows that simple model configurations can be sufficient when few major processes can be identified. This approach could be transposed on other eutrophic lakes and reservoirs to describe the competition between dominant phytoplankton species, contribute to early warning systems or be used to predict the impact of climate change and management scenarios...|$|R
50|$|Aircraft {{stability}} {{control is}} a <b>highly</b> <b>complex</b> example using multiple inputs and outputs.|$|R
