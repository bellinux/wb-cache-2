6|46|Public
50|$|<b>Error</b> <b>guessing</b> has no {{explicit}} {{rules for}} testing; test cases {{can be designed}} depending on the situation, either drawing from functional documents or when an unexpected/undocumented error is found while testing operations.|$|E
50|$|In {{software}} testing, <b>error</b> <b>guessing</b> is a {{test method}} in which test cases used to find bugs in programs are established based on experience in prior testing. The scope of test cases usually rely on the software tester involved, who uses past experience and intuition to determine what situations commonly cause software failure, or may cause errors to appear. Typical errors include divide by zero, null pointers, or invalid parameters. <b>Error</b> <b>guessing</b> has no explicit rules for testing; test cases can be designed depending on the situation, either drawing from functional documents or when an unexpected/undocumented error is found while testing operations.|$|E
50|$|It is {{performed}} by improvisation: the tester seeks to find bugs {{by any means}} that seem appropriate. Ad hoc testing {{can be seen as}} a light version of <b>error</b> <b>guessing,</b> which itself is a light version of exploratory testing.|$|E
5000|$|... #Subtitle level 3: Brute force, {{trial and}} <b>error,</b> {{inspired}} <b>guess</b> ...|$|R
5000|$|Lehel later hacked Colin Powell's {{website and}} {{accessed}} years' worth of his correspondence from another AOL account. The correspondence included personal financial information {{as well as}} e-mails to George Tenet, Richard Armitage, and John Negroponte. [...] Through six months of trial and <b>error,</b> Lehel <b>guessed</b> the password of Romanian politician Corina Crețu and gained access to her correspondence with Powell.|$|R
40|$|Most {{previously}} proposed mining methods on data streams make {{an unrealistic}} assumption that “labelled” data stream {{is readily available}} and can be mined at anytime. However, in most real-world problems, labelled data streams are rarely immediately available. Due to this reason, models are reconstructed only when labelled data become available periodically. This passive stream mining model has several drawbacks. We propose a new concept of demand-driven active data mining. In active mining, {{the loss of the}} model is either continuously guessed without using any true class labels or estimated, whenever necessary, from a small number of instances whose actual class labels are verified by paying an affordable cost. When the estimated loss is more than a tolerable threshold, the model evolves by using a small number of instances with verified true class labels. Previous work on active mining concentrates on <b>error</b> <b>guess</b> and estimation. In this paper, we discuss several approaches on decision tree evolution. ...|$|R
50|$|In {{software}} testing, <b>error</b> <b>guessing</b> is a {{test method}} in which test cases used to find bugs in programs are established based on experience in prior testing. The scope of test cases usually rely on the software tester involved, who uses past experience and intuition to determine what situations commonly cause software failure, or may cause errors to appear. Typical errors include divide by zero, null pointers, or invalid parameters.|$|E
50|$|Competitive demands require quicker, more {{effective}} and innovative technical problem solving. Creative thinking (Divergent thinking) is a critical skill required by employees, engineers, scientists and inventors, yet typically we receive no training in how to think creatively. We generally rely on our own talents to provide random inspiration or use trial and <b>error</b> (<b>guessing)</b> and Brainstorming. TRIZICS provides a repeatable method for creating new inventive ideas by systematically deriving them {{rather than relying on}} inspiration.|$|E
3000|$|The third {{shortcoming}} of CTT is {{that the}} theory assumes that errors of measurement are equal for all persons. This is problematic because persons with different levels of ability will show different levels of <b>error</b> (<b>guessing)</b> in a test that evaluates intelligence or any other construct, for example. The fourth shortcoming of CTT {{is that it does}} not allow accurate predictions about possible results for a respondent or for a sample on an item, using only their ability scores. This information would be important for a test designer interested in developing a test for a population with specific characteristics. All these points can be addressed effectively with IRT, if of course an IRT model can be found that fits the test data. 1 [...]...|$|E
40|$|Five {{classes of}} up to 13 a posteriori error estimators compete in three second-order model cases, namely the {{conforming}} and non-conforming firstorder approximation of the Poisson-Problem plus some conforming obstacle problem. Since it is the natural first step, the error is estimated in the energy norm exclusively. The competition allows merely guaranteed error control and excludes {{the question of the}} best <b>error</b> <b>guess.</b> The former a posteriori error estimators apply to the obstacle problem as well and lead to surprisingly accurate guaranteed upper error bounds. This approach allows an extension to more general boundary conditions and a discussion of efficiency for the affine benchmark examples. The Luce-Wohlmuth and the least-square error estimators win the competition in several computational benchmark problems. Novel equilibration of nonconsistency residuals and novel conforming averaging error estimators win the competition for Crouzeix-Raviart nonconforming finite element methods. Our numerical results provide sufficient evidence that guaranteed error control in the energy norm is indeed possible with efficiency indices between one and two...|$|R
5000|$|Therefore, if the SPD detects no photons, it is {{not certain}} which state was sent. The <b>error</b> of <b>guessing</b> wrong {{is equal to the}} {{probability}} [...] that the [...] state was sent, which is 50% as it is assumed either state is equally likely to be sent, times the probability that a zero detection result can occur when the displaced state is [...] The probability of error [...] becomes, ...|$|R
40|$|An {{iterative}} method is introduced for solving noisy, ill-conditioned inverse problems. Analysis of the semi-convergence behavior identifies three error components - iteration error, noise <b>error,</b> and initial <b>guess</b> <b>error.</b> A derived expression explains how the three errors {{are related to}} each other relative to the number of iterations. The Standard Tikhonov regularization method is just the first iteration of the {{iterative method}} and the derived noise damping filter is a generalization of the Standard Tikhonov filter. The derived filter is a function two parameters, a regularization parameter and the iteration number parameter. The new method is tested on image reconstruction from projections simulated data set...|$|R
30|$|It {{is perhaps}} worth {{emphasizing}} that the observed prior, to infer regions as extending straight back, is not trivial. The system could have no prior, with wide-ranging estimates of interior structures across or within individuals. Alternatively, no structure {{could be seen}} when there is insufficient evidence to determine the 3 D form. If indeed the phenomenon is perceptual in nature, seeing a structure as extending straight down may be analogous to seeing the shortest path in apparent motion; it may be “sensible” and minimize <b>error</b> between <b>guess</b> and reality, but nonetheless it requires a theoretical explanation.|$|R
40|$|The {{displacement}} threshold (DT) during voluntary visual {{tracking of}} a line and during fixation of a stable spot located at the initial stimulus position was determined {{to help in the}} modeling pilot manual dynamics during the nose-up 'flare' maneuver. Forty-six observers made paired comparison, forced choice judgments of the maximal downward displacements of a horizontal line moving at various speeds for the durations of 0. 25, 0. 50, and 1. 00 sec. The results indicated that the DT is significantly lower, the confidence is higher, and the number of <b>errors</b> and <b>guesses</b> is smaller when a fixation dot is present; moreover, increasing both the stimulus velocity and duration significantly improves the accuracy of displacement discrimination and increases mean confidence...|$|R
30|$|This value {{corresponds}} {{exactly to}} the quadratic <b>error</b> in our <b>guess</b> if (1) {{there is an}} instance of the exact pattern in the image, and (2) that instance is the closest one in the image, in a covariance factor sense, to our guess. The use of this extra term permits a less shape-selective weighting of candidate patterns when {{the quality of the}} estimated pattern is still relatively low.|$|R
40|$|Under the {{supervision}} of Professors W. Beckman, J. Mitchell, and R. Engelstad; 72 pp. As the semiconductor industry continues to shrink the size microelectronic components, sources of critical dimension error that were unimportant {{in the past have}} surfaced, and must be resolved. Among these errors is the proximity heating effect. As an optical mask is patterned using an e-beam, there is heat diffusion away from the area being patterned. Due to the increase in temperature, the resist surrounding the patterned area increases in sensitivity and becomes more prone to development from scattered electrons. The unexpected development of resist and distortions due to thermal gradients can cause the final pattern to differ from the intended pattern. Unfortunately, there is no method to predict the magnitude of these <b>errors.</b> <b>Guess</b> and check methods are not feasible in the production environment due to the limited number of chip manufacturing tools, and the need to produce saleable products on these tools. Consequently, a method is needed to predict the magnitude and location of these errors. The topic of this thesis is to investigate the thermal response of the optical mask due to direct patterning using a finite element program, ANSYS. The results from this thesis, resist temperature as a function of position and time, can then be combined with experimental data relating the temperature history of the resist with its sensitivity, and Monte Carlo simulations that predict the scattering of electrons as they penetrate an optical substrate to yield the percentage of resist development at every point on the mask. The results of this analysis can then be compared with the desired pattern. Any regions containing unacceptable errors can then be redesigned. Supported by International SEMATECH, the Semiconductor Research Corporation, DARPA/ARL, and the Unversity of Wisconsin-Madison Graduate School...|$|R
40|$|The Gain-Loss Model (GaLoM) is a {{probabilistic}} skill multimap {{model for}} assessing learning processes proposed {{within the framework}} of knowledge space theory. Model parameters are initial probabilities of the skills, effects of learning object on gaining and losing the skills, careless <b>error</b> and lucky <b>guess</b> probabilities of problems. When the skills are assessed through a small number of problems or the data are noisy, model identifiability and goodness of recovery are a concern. An extension of the GaLoM is proposed in which the parameter space of careless <b>error</b> and lucky <b>guess</b> probabilities is constrained. Different ratios between number of problems and skills, and levels of noise in the data are considered in a simulation study. Advantages of the constrained GaLoM with respect to identifiability and goodness of recovery are presented and discussed. The Gain-Loss Model (GaLoM, Robusto, Stefanutti, & Anselmi, in press) is a probabilistic model for assessing learning processes proposed {{within the framework of}} knowledge space theory (Doignon & Falmagne, 1985, 1999). It assesses the effect of learning objects on the attainment of skills required to solve problems in a given knowledge domain. Via th...|$|R
6000|$|Is {{not this}} common impulse rather curious? And is {{suspicion}} of forgery to fall, in Portugal, on respectable priests, {{or on the}} very uncultured wags of Traz os Montes? Mortillet, educated by priests, hated and suspected all of them. M. Cartailhac suspected [...] "clericals," [...] as to the Spanish cave paintings, but acknowledged his <b>error.</b> I can <b>guess</b> no motive for the ponderous bulk of Portuguese forgeries, and am a little suspicious of the tendency to shout [...] "Forgery" [...] {{in the face of}} everything unfamiliar.|$|R
40|$|Abstract In this paper, the Laplace {{problem with}} a free surface is solved by using the hypersingular equation. In the {{conventional}} boundary element method, only sin-gular integral equation was used. The boundary of free surface can be determined by trial and <b>error</b> after initial <b>guess</b> and iterations. By introducing the hyper-singular equation, the convergence rate of free surface can be accelerated. It is found that the result by using a higher-order singularity approach for the kernel is more accurate than introducing a higher-order element for the boundary density. Finally, numerical examples were demonstrated to show {{the validity of the}} present method...|$|R
40|$|Two {{experiments}} investigated criterion {{setting and}} metacognitive processes underlying the strategic regulation of accuracy on the Scholastic Aptitude Test (SAT) using type- 2 signal detection theory (SDT). In Experiment 1, report bias was manipulated by penalizing participants either 0. 25 (low incentive) or 4 (high incentive) points for each <b>error.</b> Best <b>guesses</b> to unanswered items were obtained so that type- 2 signal-detection indices {{of discrimination and}} bias could be calculated. The same incentive manipulation was used in Experiment 2, only the test was computerized, confidence ratings were taken so that receiver-operating characteristic (ROC) curves could be generated, and feedback was manipulated. The results of both experiments demonstrated that SDT provides {{a viable alternative to}} Koriat and Goldsmith’s (1996 c) framework of monitoring and control and reveals information about the regulation of accuracy that their framework does not. For example, ROC analysis indicated that the threshold model implied by formula scoring is inadequate. Instead, performance on the SAT should be modelled with an equalvariance, Gaussian, type- 2, signal-detection model...|$|R
40|$|Within the {{framework}} of knowledge space theory, a probabilistic skill multimap model for assessing learning processes is proposed. The learning process of a student is modeled {{as a function of}} the interaction between his competence state and the effect of a learning object on specific skills. Model parameters are initial probabilities of the skills, effects of learning objects on gaining and losing the skills, careless <b>error,</b> and lucky <b>guess</b> probabilities of the problems. A simulation study assessed model identifiability and goodness-of-recovery under several conditions. Practi- cal implications of using the model are discussed, and the MATLAB code for simulating, estimating and testing it is available in the Psychonomic Society supplemental archive...|$|R
40|$|Abstract—Fano’s {{inequality}} {{relates the}} <b>error</b> probability of <b>guessing</b> a finitely-valued random variable given another random variable and the conditional entropy of given. It {{is not necessarily}} tight when the marginal distribution of is fixed. This paper gives a tight upper bound on the conditional entropy of given {{in terms of the}} error probability and the marginal distribution of. A new lower bound on the conditional entropy for countably infinite alphabets is also found. The relationship between the reliability criteria of vanishing error probability and vanishing conditional entropy is also discussed. A strengthened form of the Schur-concavity of entropy which holds for finite or countably infinite random variables is given...|$|R
40|$|Within the {{theoretical}} framework of knowledge space theory, a probabilistic skill multimap model for assessing learning processes is proposed. The learning process {{of a student}} is modeled {{as a function of}} the student’s knowledge and of an educa- tional intervention on the attainment of specific skills required to solve problems in a knowledge domain. Model parameters are initial probabilities of the skills, effects of learning objects on gaining and losing the skills, and careless <b>error</b> and lucky <b>guess</b> probabilities of the problems. An empirical application shows that the model is effective in assessing knowledge and effectiveness of educational intervention at both classroom and student levels. Practical implications for teaching and learning are discussed...|$|R
40|$|Capitalization on {{chance is}} a huge problem in {{computerized}} adaptive testing (CAT) when Fisher information is used to select the items. Maximizing Fisher information tends to favor items with positive estimation errors in the discrimination parameter and negative estimation <b>errors</b> in the <b>guessing</b> parameter. As a result, information in the resulting tests is overestimated and measurement precision is lower than expected. Since reduction of test length {{is one of the}} most important selling points of CAT, this is a serious threat to both the validity and viability of this test administration mode. In this chapter, robust test assembly is presented as an alternative method that accounts for uncertainty in the item parameters during test assembl...|$|R
40|$|Prey {{consumption}} by Northwest Atlantic harp seals, Phoca groenlandica, depends on population size, seasonal and spatial distribution, energy requirements, energy content of prey and diet composition. There is uncertainty in {{our knowledge of}} all these components. This carries through into uncertainty in any estimate of prey consumption. Available information ranges from sample estimates, sometimes with conventional measures of precision (standard <b>errors),</b> to <b>guesses</b> based on unquantified observation. An attempt is made here to quantify the effect {{of some of the}} major sources of uncertainty, particularly with respect to the amount of Atlantic cod (Gadus morhua) eaten in NAFO Division 2 J and 3 KL (off southern Labrador and northeast Newfoundland). The primary objective is to determine which components contribute most to the uncertainty, as a guide for research planning. However, a thorough quantification of uncertainty would also be useful in evalu-ating alternative management options for harp seals which have the objective of reducing possible impacts on prey. This work examines the effect on consumption estimates of un-certainty associated with the harp seal populatoin size as well as the effect of additional sources of uncertainty attributable to residency of harp seals, energy requirements, spe-cies composition of the diet in the inshore and offshore, and the calorific value of prey...|$|R
40|$|This report {{describes}} the {{work being done}} in developing the MESAN system for analyzing surface parameters and clouds. The following parameters are being analyzed:- 2 meter temperature,- precipitation in mm water for 1, 3, 12 and 12 hours accumulation time, and for new snow-cover in cm,- wind speed and direction and gust speed at 10 meter elevation,- visibility,- relative humidity,- total cloud cover,- amount and low clouds,- significant cloud base,- probability of observing significant cloud base,- higt of cloud top,- depth of snow cover, and- sea surface temperature. Hirlam data are normally used as first guess fields. Observatons are taken from synop, metar, Swedish climate stations, satellites, radars and automatic stations. Much work {{has been devoted to}} minimizing systematic errors in observations and investigating structure functions of first <b>guess</b> <b>errors.</b> The analys method used is optimal interpolation...|$|R
40|$|There are {{two common}} {{settings}} in a quantum-state discrimination problem. One is minimum-error discrimination where a wrong <b>guess</b> (<b>error)</b> is allowed and the discrimination success probability is maximized. The other is unambiguous discrimination where errors {{are not allowed}} but the inconclusive result "I don't know" is possible. We investigate discrimination problem with a finite margin imposed on the error probability. The two common settings correspond to the error margins 1 and 0. For arbitrary error margin, we determine the optimal discrimination probability for two pure states with equal occurrence probabilities. We also consider the case where the states to be discriminated are multipartite, and show that the optimal discrimination probability {{can be achieved by}} local operations and classical communication. Comment: 7 pages, 1 figure, typos corrected, references and an appendix added, to appear in Phys. Rev. A 7...|$|R
40|$|ABSTRACT: Synchronization is a {{critical}} operation in many parallel applications. Conservative Synchronization mechanisms are failing {{to keep up with}} the increasing demand for well-organized management operations as systems grow larger and network latency increases. The charity of this paper is threefold. First, we revisit some delegate bringing together algorithms in light of recent architecture innovation and provide an example of how the simplify assumption made by typical logical models of management mechanism can lead to significant performance <b>guess</b> <b>errors.</b> Second, we present an architectural modernism called active memory that enables very fast tiny operations in a shared-memory multiprocessor. Third, we use execution-driven simulation to quantitatively compare the performance of a variety of Synchronization mechanisms based on both existing hardware techniques and active memory operations. To the best of our knowledge, management based on active memory out forms all existing spinlock and non-hardwired difficulty implementations by a large border...|$|R
40|$|This study {{evaluated}} three algorithms of the iterative ensemble Kalman filter (EnKF). They are Confirming EnKF, Restart EnKF, {{and modified}} Restart EnKF developed {{to resolve the}} inconsistency problem (i. e., updated model parameters and state variables do not follow the Richards equation) in vadose zone data assimilation due to model nonlinearity. While Confirming and Restart EnKF were adapted from literature, modified Restart EnKF was developed in this study to reduce computational costs by calculating only the mean simulation, not all the ensemble realizations, from time t = 0. A total of 11 cases were designed to investigate the performance of EnKF, Confirming EnKF, Restart EnKF, and modified Restart EnKF with different types and spatial configurations of observations (pressure head and water content) and different values of observation <b>error</b> variance, initial <b>guess</b> of ensemble mean and variance, ensemble size, and damping factor. The numerical study showed that Confirming EnKF produced considerable inconsistency for the nonlinear unsaturated flow problem, which differs fro...|$|R
40|$|Abstract. We {{determine}} the imbalances of the keystreams produced by Achterbahn- 80 and Achterbahn- 128 in two dierent ways. The number of cyclically inequivalent keystreams pro-duced by the keystream generators of Achterbahn- 80 and Achterbahn- 128 is determined. An abstract {{model for the}} keystream generator of a primitive NLFSR combination generator is used to justify the correlation attack introduced in [6] and generalized in [8]. A common <b>error</b> in a <b>guess</b> and determine attack is discussed. The optimal decision rule for nding the correct initial state of the target shift register in the guess and determine attack is described in the coin tossing model. The reliability of results derived from the abstract keystream generator model and the coin tossing model is demonstrated by running an actual guess and determine attack on a cipher that could be called Baby-Achterbahn. Two attacks against Achterbahn- 128 / 80 found by Naya-Plasencia [8] and Hell and Johansson [5] are shown to be equivalent...|$|R
40|$|Agents {{existing}} {{in a continuous}} domain require {{a great deal of}} specific, detailed knowledge. Providing that information to the agent manually is time-consuming and error-prone, so it is desirable to enable the agent to automatically learn to accomplish its goals. However, performance {{in the early stages of}} learning is characterized by many <b>errors</b> and random <b>guessing,</b> slowing the learning rate. This research attempts to avoid both problems by providing the agent an approximate model of its environment and enabling it to learn more detailed knowledge autonomously. The agent uses a two-level framework to first classify its sensory information into regions, and then map the set of regions to an action it can take in the world. The agent monitors the results of the action and learns by either changing its classification of input or its action mapping. A preliminary implementation of the framework in the domain of simulated flight is presented. Open questions for future research and a summar [...] ...|$|R
40|$|Existing {{algorithms}} for wideband direction finding {{are mainly}} based on local approximations of the Gaussian log-likelihood around the true directions of arrival (DOAs), assuming negligible array calibration errors. Suboptimal and costly algorithms, such as classical or sequential beamforming, {{are required to}} initialize a local search that eventually furnishes DOA estimates. This multistage process may be nonrobust {{in the presence of}} even small <b>errors</b> in prior <b>guesses</b> about angles and number of sources generated by inherent limitations of the preprocessing and may lead to catastrophic errors in practical applications. In this paper, a new approach to wideband direction finding is introduced and described. The proposed strategy combines a robust near-optimal data-adaptive statistic, called the weighted average of signal subspaces (WAVES), with an enhanced design of focusing matrices to ensure a statistically robust preprocessing of wideband data. The overall sensitivity of WAVES to various error sources, such as imperfect array focusing, is also reduced with respect to traditional CSSM algorithms, as demonstrated by extensive Monte Carlo simulations...|$|R
40|$|In {{a current}} {{analysis}} procedure at the National Meteorological Center (NMC), a first guess analysis (from the latest 6 or 12 hour forecast) is updated by new data. In the newest analysis procedures, {{which are based}} on optimum interpolation, the observational correction depends on the size of preassigned, expected observational errors versus the size of expected errors in the first guess forecast. In the case of Northern Hemisphere ocean temperatures, the latter are around 2 degrees. The infrared satellite retrievals have a similar size "error". If (as assumed) their errors are uncorrelated with the first <b>guess</b> <b>error,</b> they will improve the analysis. The larger errors of the microwave retrievals, however, mean that they will be given little weight in the analysis. An evaluation of the TIROS-N retrievals is currently underway at NMC to determine the impact of satellite derived data within operational analysis schemes, to isolate possible problems within current retrieval methods, and to offer possible solutions for these problems...|$|R
40|$|In {{an earlier}} paper, we studied {{the problem of}} {{guessing}} a random vector X within distortion D, and characterized the best attainable exponent E(D, p) of the pth moment {{of the number of}} required guesses G(X) until the <b>guessing</b> <b>error</b> falls below D. In this correspondence, we extend these results to a multistage, hierarchical guessing model, which allows for a faster search for a codeword vector at the encoder of a rate-distortion codebook. In the two-stage case of this model, if the target distortion level is D 2, the guesser first makes guesses with respect to (a higher) distortion level D 1, and then, upon his/her first success, directs the subsequent guesses to distortion DI. As in the abovementioned earlier paper, we provide a single-letter characterization of the best attainable guessing exponent, which relies heavily on well-known results on the successive refinement problem. We also relate this guessing exponent function to the source-coding error exponent function of the two-step coding process. © 1999 IEEE...|$|R
40|$|Cataloged from PDF {{version of}} article. In an earlier paper, we studied {{the problem of}} {{guessing}} a random vector X within distortion D, and characterized the best attainable exponent E(D, rho) of the rho th moment {{of the number of}} required guesses G(X) until the <b>guessing</b> <b>error</b> falls below D. In this correspondence, we extend these results to a multistage, hierarchical guessing model, which allows for a faster search for a codeword vector at the encoder of a rate-distortion codebook. In the two-stage case of this model, if the target distortion level is D- 2, the guesser first makes guesses with respect to (a higher) distortion level D- 1, and then, upon his/her first success, directs the subsequent guesses to distortion D- 2. As in the above-mentioned earlier paper, we provide a single-letter characterization of the best attainable guessing exponent, which relies heavily on well-known results on the successive refinement problem. We also relate this guessing exponent function to the source-coding error exponent function of the two-step coding process...|$|R
40|$|Erdal Arikan z In {{an earlier}} paper, we studied {{the problem of}} {{guessing}} a random vector X within distortion D, and characterized the best attainable exponent E(D �) of the th moment {{of the number of}} required guesses G(X) until the <b>guessing</b> <b>error</b> falls below D. In this paper, we extend these results to a multi-stage, hierarchical guessing model, which allows for a faster search for a codeword vector at the encoder of a rate-distortion codebook. In the two-stage case of this model, if the target distortion level is D 2, the guesser rst makes guesses w. r. t. (a higher) distortion level D 1, and then, upon his/her rst success, directs the subsequent guesses to distortion D 2. As in the above-mentioned earlier paper, we provide a single-letter characterization of the best attainable guessing exponent, which relies heavily on well-known results on the successive re nement problem. We also relate this guessing exponent function to the source coding error exponent function of the two-step coding process. Index Terms: rate-distortion theory, successive re nement, <b>guessing,</b> source coding <b>error</b> exponent...|$|R
