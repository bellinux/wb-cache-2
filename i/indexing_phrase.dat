2|89|Public
40|$|Neuroscientific and musicological {{approaches}} to music cognition indicate that listeners familiarized in the Western tonal tradition expect a musical phrase boundary at predictable time intervals. However, phrase boundary prediction processes in music remain untested. We analyzed event-related potentials (ERPs) and event-related induced power {{changes at the}} onset and offset of a boundary pause. We made comparisons with modified melodies, where the pause was omitted and filled by tones. The offset of the pause elicited a closure positive shift (CPS), <b>indexing</b> <b>phrase</b> boundary detection. The onset of the filling tones elicited significant increases in theta and beta powers. In addition, the P 2 component was larger when the filling tones started than when they ended. The responses to boundary omission suggest that listeners expected to hear a boundary pause. Therefore, boundary prediction seems to coexist with boundary detection in music segmentation...|$|E
40|$|This {{dissertation}} {{introduces a}} new theoretical model for text classification systems, including systems for document retrieval, automated indexing, electronic mail filtering, and similar tasks. The Concept Learning model emphasizes {{the role of}} manual and automated feature selection and classifier formation in text classification. It enables drawing on results from statistics and machine learning in explaining the effectiveness of alternate representations of text, and specifies desirable characteristics of text representations. ^ The use of syntactic parsing to produce indexing phrases has been widely investigated as a possible route to better text representations. Experiments with syntactic phrase indexing, however, have never yielded significant improvements in text retrieval performance. The Concept Learning model suggests that the poor statistical characteristics of a syntactic <b>indexing</b> <b>phrase</b> representation negate its desirable semantic characteristics. The application of term clustering to this representation to improve its statistical properties while retaining its desirable meaning properties is proposed. ^ Standard term clustering strategies from information retrieval (IR), based on cooccurrence of indexing terms in documents or groups of documents, were tested on a syntactic <b>indexing</b> <b>phrase</b> representation. In experiments using a standard text retrieval test collection, small effectiveness improvements were obtained. ^ As a means of evaluating representation quality, a text retrieval test collection introduces a number of confounding factors. In contrast, the text categorization task allows much cleaner determination of text representation properties. In preparation {{for the use of}} text categorization to study text representation, a more effective and theoretically well-founded probabilistic text categorization algorithm was developed, building on work by Maron, Fuhr, and others. ^ Text categorization experiments supported a number of predictions of the Concept Learning model about properties of phrasal representations, including dimensionality properties not previously measured for text representations. However, in carefully controlled experiments using syntactic phrases produced by Church 2 ̆ 7 s stochastic bracketer, in conjunction with reciprocal nearest neighbor clustering, term clustering was found to produce essentially no improvement in the properties of the phrasal representation. New cluster analysis approaches are proposed to remedy the problems found in traditional term clustering methods. ...|$|E
40|$|A {{syntactic}} {{approach is}} described for generating <b>indexing</b> <b>phrases</b> usable for the content identification of natural-language texts. The phrase generation method {{is based on}} a simple language analysis system that determines the syntactic function of individual text words {{with a high degree of}} accuracy, and chooses of <b>indexing</b> <b>phrases</b> based on weights assigned to the phrase components. The proportion of phrases that appear to be acceptable for content identification ranges from 96 to 98 percent...|$|R
40|$|In {{this paper}} we {{describe}} the results of experiments contrasting syntactic <b>phrase</b> <b>indexing</b> with statistical <b>phrase</b> <b>indexing</b> for Dutch texts. Our results showed that we at least need a compound split- ting algorithm for good quality retrieval for Dutch texts. If we then add either syntactic or statistical phrases, performance generally improves, but this effect is never statistically significant. If we compare syntactic vs. statistical <b>phrase</b> <b>indexing,</b> syntactic <b>phrases</b> are slightly superior to statistical phrases, particularly at high precision. At higher recall levels syntactic and statistical phrases are equally effective. However, since a compound splitting algorithm requires a dictionary and knowledge about constraints on compound formation, a purely non-linguistic indexing strategy, with or without phrases, {{does not seem to}} be very effective for Dutch...|$|R
40|$|The {{evaluation}} experiments of the JSCB {{team are}} described {{with a focus}} on noun <b>phrase</b> <b>indexing</b> and its weighting issues in ad hoc text retrieval. Experiments on the effects of supplemental noun <b>phrase</b> <b>indexing</b> in view of the effect of various length of queries are reported. The results show that the noun <b>phrase</b> <b>indexing</b> outperforms single word only indexing with long queries while single word only indexing performs slightly better with short queries. A new weighting method for phrasal terms is also evaluated and improvement is observed. Keywords Phrasal <b>indexing,</b> noun <b>phrase</b> <b>indexing,</b> phrasal terms, weighting, vector space model. 1. INTRODUCTION Automatic indexing of modern information retrieval systems typically adopts bag-of-word representation, in which each word is considered as a dimension of the vector representing an information item, as internal representation of "aboutness". It is well known that such simple representation usually performs, as well as, if not bett [...] ...|$|R
40|$|Search engines need to {{evaluate}} queries extremely fast, a challenging task given the quantities of data being indexed. A {{significant proportion of}} the queries posed to search engines involve phrases. In this article we consider how phrase queries can be efficiently supported with low disk overheads. Our previous research has shown that phrase queries can be rapidly evaluated using nextword indexes, but these indexes are twice as large as conventional inverted files. Alternatively, special-purpose <b>phrase</b> <b>indexes</b> can be used, but it is not feasible to <b>index</b> all <b>phrases.</b> We propose combinations of nextword <b>indexes</b> and <b>phrase</b> <b>indexes</b> with inverted files as a solution to this problem. Our experiments show that combined use of a partial nextword, partial phrase, and conventional inverted index allows evaluation of phrase queries in a quarter the time required {{to evaluate}} such queries with an inverted file alone; the additional space overhead is only 26 % {{of the size of the}} inverted file...|$|R
40|$|We combine {{techniques}} of XML Mining and Text Mining {{for the benefit}} of Information Retrieval. By manipulating the word sequence according to the XML structure of the marked-up text, we strengthen phrase boundaries so that they are more obvious to the algorithms that extract multiword sequences from text. Consequently, the quality of the <b>indexed</b> <b>phrases</b> improves, which has a positive effect on the average precision measured by the INEX 2007 standards. Categories and Subject Descriptors H. 3. 1 [Information Storage and Retrieval]: Analysis and Indexing—Indexing method...|$|R
40|$|This work {{describes}} a methodology to <b>index</b> anatomical <b>phrases</b> to the 2005 AA {{release of the}} Unified Medical Language System (UMLS). A phrase chunking tool based on Natural Language Processing (NLP) was developed to identify semantically coherent phrases within medical reports. Using this phrase chunker, a set of 2, 551 unique anatomical phrases was extracted from brain radiology reports. These phrases were mapped to the 2005 AA release of the UMLS using a vector space model. Precision for the task of <b>indexing</b> unique <b>phrases</b> was 0. 87...|$|R
2500|$|Here, the ei is a trace element. This {{constraint}} {{is called}} the [...] "CC-Constraint". It states that, in the underlying structure, the quantifier cannot appear inside another, differently <b>indexed</b> Noun <b>Phrase.</b> This is a stronger version of his previously stated [...] "C-Constraint", and he proposes that while Mandarin must always follow the CC-Constraint, English can at times relax this constraint to follow the C-Constraint instead. This, he claims, leads to the difference in interpretation possibilities in the English and Mandarin versions of example (35), since the quantifier shei appears within the differently <b>indexed</b> Noun <b>Phrase</b> shei de muchin, and so it cannot be reindexed {{to have the same}} index as ta.|$|R
40|$|Abstract. In this paper, {{we propose}} a common <b>phrase</b> <b>index</b> as an {{efficient}} index structure to support phrase queries {{in a very}} large text database. Our structure {{is an extension of}} previous <b>index</b> structures for <b>phrases</b> and achieves better query efficiency with negligible extra storage cost. In our experimental evaluation, a common <b>phrase</b> <b>index</b> has 5 % and 20 % improvement in query time for the overall and large queries (queries of long phrases) respectively over an auxiliary nextword index. Moreover, it uses only 1 % extra storage cost. Compared with an inverted index, our improvement is 40 % and 72 % for the overall and large queries respectively. ...|$|R
5000|$|Here, the ei is a trace element. This {{constraint}} {{is called}} the [...] "CC-Constraint". It states that, in the underlying structure, the quantifier cannot appear inside another, differently <b>indexed</b> Noun <b>Phrase.</b> This is a stronger version of his previously stated [...] "C-Constraint", and he proposes that while Mandarin must always follow the CC-Constraint, English can at times relax this constraint to follow the C-Constraint instead. This, he claims, leads to the difference in interpretation possibilities in the English and Mandarin versions of example (35), since the quantifier shei appears within the differently <b>indexed</b> Noun <b>Phrase</b> shei de muchin, and so it cannot be reindexed {{to have the same}} index as ta.|$|R
40|$|Information {{retrieval}} is {{an important}} application area of natural-language processing where one encounters the genuine challenge of processing large quantities of unrestricted natural-language text. This paper reports on {{the application of a}} few simple, yet robust and efficient nounphrase analysis techniques to create bet- ter <b>indexing</b> <b>phrases</b> for information retrieval. In particular, we describe a hybrid approach to the extraction of meaningful (continuous or discontinuous) subcompounds from complex noun phrases using both corpus statistics and linguistic heuristics. Results of experiments show that indexing based on such extracted sub- compounds improves both recall and precision in an information retrieval system. The noun-phrase analysis techniques are also potentially useful for book indexing and automatic thesaurus extraction...|$|R
40|$|In this chapter, we {{describe}} the key indexing components of today’s web search engines. As the World Wide Web has grown, the systems and methods for indexing have changed significantly. We present the data structures used, the features extracted, the infrastructure needed, and the options available for designing a brand new search engine. We highlight techniques that improve relevance of results, discuss trade-offs to best utilize machine resources, and cover distributed processing concept in this context. In particular, we delve into the topics of <b>indexing</b> <b>phrases</b> instead of terms, storage in memory vs. on disk, and data partitioning. We will finish with some thoughts on information organization for the newly emerging data-forms...|$|R
40|$|In {{order for}} an {{automatic}} information retrieval system to effectively retrieve {{documents related to}} a given subject area, the content of each document in the system's database must be represented accurately. This study examines the hypothesis that better representations of document content can be constructed if the content analysis method takes into consideration the syntactic structure of document and query texts. Two methods of automatically generating phrases for use as content indicators have been implemented and tested experimentally. The non-syntactic (or statistical) method is based on simple text characteristics such as word frequency and the proximity of words in text. The syntactic method uses augmented phrase structure rules (production rules) to selectively extract phrases from parse trees generated by an automatic syntactic analyzer. Experimental {{results show that the}} effect of non-syntactic <b>phrase</b> <b>indexing</b> is inconsistent. For the five collections tested, increases in average precision ranged from 22. 7 % to 2. 2 % over simple, single term <b>indexing.</b> The syntactic <b>phrase</b> <b>indexing</b> method was tested on two collections. Precision figures averaged over all test queries indicate that non-syntactic <b>phrase</b> <b>indexing</b> performs significantly better than syntactic <b>phrase</b> <b>indexing</b> for one collection, but that the difference is insignificant for the other collection. More detailed analysis of individual queries, however, indicates that the performance of both methods is highly variable, and that there is evidence that syntax-based indexing has certain benefits not available with the non-syntactic approach. Possible improvements of both methods of <b>phrase</b> <b>indexing</b> are considered. It is concluded that the prospects for improving the syntax-based approach to document indexing are better than for the non-syntactic approach. The PLNLP system was used for syntactic analysis of document and query texts, and for implementing the syntax-based phrase construction rules. The SMART information retrieval system was used for retrieval experimentation...|$|R
25|$|According to the <b>indexing</b> theory, each <b>phrase</b> in a {{sentence}} can be given a unique index, which is a number (or letter) that identifies that phrase as picking out a particular entity in the world. It is possible to modify the <b>indexes</b> on these <b>phrases</b> so that two or more phrases have the same index. This is called co-indexation. If co-indexation occurs, the phrases with the same-numbered index will all refer to the same entity. This phenomenon is called co-reference.|$|R
40|$|Concord {{adverbial}} constructions in Korean show {{unbounded dependency}} relationships between two non-empty entities. There {{are two different}} types of unboundedness involved: one between a concord adverbial and a verbal ending and the other between the adverbial as a modifier and a predicate. In addition, these unboundedness relationships exhibit properties of “downward movement ” phenomena. In this paper, we examine the <b>Indexed</b> <b>Phrase</b> Structure Grammar analysis of the constructions presented in Chae (2003, 2004), and propose to introduce a new feature to solve its conceptual problem. Then, we provide an analysis of conditional-concessive constructions, which is a subtype of concord adverbial constructions. These constructions are special {{in the sense that they}} contain a seemingly incompatible combination of a conditional adverbial and a concessive verbal ending. We argue that they are basically conditional constructions despite their concessive meaning. ...|$|R
40|$|Abstract. Document {{clustering}} techniques mostly rely on single term {{analysis of}} text, {{such as the}} vector space model. To better capture the structure of documents, the underlying data model {{should be able to}} represent the phrases in the document as well as single terms. We present a novel data model, the Document Index Graph, which indexes Web documents based on phrases rather than on single terms only. The semistructured Web documents help in identifying potential phrases that when matched with other documents indicate strong similarity between the docu-ments. The Document Index Graph captures this information, and finding significant matching phrases between documents becomes easy and efficient with such model. The model is flexi-ble in that it could revert to a compact representation of the vector space model if we choose not to <b>index</b> <b>phrases.</b> However, using <b>phrase</b> <b>indexing</b> yields more accurate document similar-ity calculations. The similarity between documents is based on both single term weights and matching phrase weights. The combined similarities are used with standard document cluster-ing techniques to test their effect on the clustering quality. Experimental results show that our phrase-based similarity, combined with single-term similarity measures, gives a more accurate measure of document similarity and thus significantly enhances Web document clustering qual-ity...|$|R
40|$|The {{effect of}} {{selecting}} varying numbers and kinds of features {{for use in}} predicting category membership was investigated on the Reuters and MUC- 3 text categorization data sets. Good categorization performance was achieved using a statistical classifier and a proportional assignment strategy. The optimal feature set size for word-based indexing {{was found to be}} surprisingly low (10 to 15 features) despite the large training sets. The extraction of new text features by syntactic analysis and feature clustering was investigated on the Reuters data set. Syntactic <b>indexing</b> <b>phrases,</b> clusters of these phrases, and clusters of words were all found to provide less effective representations than individual words. 1. Introduction Text categorization [...] -the automated assigning of natural language texts to predefined categories based on their content [...] -is a task of increasing importance. Its applications include indexing texts to support document retrieval [1], extracting data from texts [2], and ai [...] ...|$|R
40|$|This paper {{outlines}} {{a possible}} {{approach to the}} use of advanced information storage and retrieval techniques for business correspondence. The present situation in the Office Automation field is surveyed, and the role of information retrieval is an integrated office information system is discussed. Business letter characteristics that might be useful for analysis and retrieval are described. The idea of a generalized business thesaurus containing standard phrases and locutions as well as synonyms and <b>index</b> <b>phrases</b> is presented. A technical solution to the analysis, storage and retrieval of business letters based on the concepts from the SMART-system is outlined. A description of the experiments performed so far follows. These include decomposition, analysis, storage, automatic classification and utilization of inter-letter references. The results from the experiments show that the use of automatic indexing and retrieval in the office is feasible and provides a viable alternative to existing manual business files. In conclusion, several future experiments are outlined...|$|R
5000|$|Most web {{search engines}} are {{designed}} to search for words anywhere in a document—the title, the body, and so on. This being the case, a keyword can be any term that exists within the document. However, priority is given to words {{that occur in the}} title, words that recur numerous times, and words that are explicitly assigned as keywords within the coding. Index terms can be further refined using Boolean operators such as [...] "AND, OR, NOT." [...] "AND" [...] is normally unnecessary as most search engines infer it. [...] "OR" [...] will search for results with one search term or another, or both. [...] "NOT" [...] eliminates a word or phrase from the search, getting rid of any results that include it. Multiple words can also be enclosed in quotation marks to turn the individual index terms into a specific <b>index</b> <b>phrase.</b> These modifiers and methods all help to refine search terms, to better maximize the accuracy of search results.|$|R
40|$|Dealing {{effectively}} with {{large volumes of}} semistructured data requires efficient retrieval. Finding all occurrences of a given word in a large static text is a well-studied problem. Most solutions, however, are not well-suited for phrase-searching. In this paper, we investigate a new algorithm to find all occurrences of a given phrase in a large, static text, based on the data structure known as a suffix array. Using this algorithm, phrases of bounded length can be found with expected search time of one disk access to the text and one disk access to an index. To achieve this performance for phrases of up to five words in length requires an index having total size of approximately 120 % {{of the size of}} the text. The algorithm guarantees a worst case search performance of 2 disk accesses to the text per phrase search. The method augments a suffix array with a parallel signature array, so that every <b>indexed</b> <b>phrase</b> has an associated signature. To search for a phrase, we search a block of the [...] ...|$|R
40|$|Large-scale web-search {{engines are}} {{generally}} designed for linear text. The linear text representation is suboptimal for audio search, where accuracy can be significantly improved if the search includes alternate recognition candidates, commonly represented as word lattices. This paper proposes {{a method for}} indexing word lattices that is suitable for large-scale web-search engines, requiring only limited code changes. The proposed method, called Time-based Merging for Indexing (TMI), first converts the word lattice to a posterior-probability representation and then merges word hypotheses with similar time boundaries to reduce the index size. Four alternative approximations are presented, which differ in index size and the strictness of the phrase-matching constraints. Results are presented for three types of typical web audio content, podcasts, video clips, and online lectures, for phrase spotting and relevance ranking. Using TMI indexes that are only five times larger than corresponding lineartext <b>indexes,</b> <b>phrase</b> spotting was improved over searching top- 1 transcripts by 25 - 35 %, and relevance ranking by 14 %, at only a small loss compared to unindexed lattice search. ...|$|R
40|$|Syntactic <b>phrase</b> <b>indexing</b> and term {{clustering}} {{have been}} widely explored as text representation techniques for text retrieval. In this paper we study the properties of phrasal and clustered in-dexing languages on a text categorization task, enabling us to study their properties in isolation from query interpretation issues. We show that optimaJ effectiveness occurs when using only {{a small proportion of}} the indexing terms available, and that effectiveness peaks at a higher feature set size and lower effectiveness level for a syntac-tic <b>phrase</b> <b>indexing</b> than for word-based indexing. We also present results suggesting that traditional term clustering method are unlikely to provide sig-nificantly improved text representations. An im-proved probabilistic text categorization method is also presented. ...|$|R
40|$|Document {{clustering}} techniques mostly rely on single term {{analysis of}} the document data set, such as the Vector Space Model. To achieve more accurate document clustering, more informative features including phrases and their weights are particularly important in such scenarios. Document clustering is particularly useful in many applications such as automatic categorization of documents, grouping search engine results, building a taxonomy of documents, and others. This paper presents two key parts of successful document clustering. The first part is a novel phrase-based document index model, the Document Index Graph, which allows for incremental construction of a phrase-based index of the document set {{with an emphasis on}} efficiency, rather than relying on single-term indexes only. It provides efficient phrase matching that is used to judge the similarity between documents. The model is flexible in that it could revert to a compact representation of the vector space model if we choose not to <b>index</b> <b>phrases.</b> The second part is an incremental document clustering algorithm based on maximizing the tightness of clusters by carefully watching the pair-wise document similarity distribution inside clusters. The combination of these two components creates an underlying model for robust and accurate document similarity calculation that leads to much improved results in Web document clustering over traditional methods...|$|R
40|$|Language {{samples were}} {{elicited}} in six different situations from 20 five-year-old children (10 boys, 10 girls) {{in their first}} six months at Primary School. The samples of language were analyzed using a number of quantitative and qualitative measures. The qualitative measures used were the noun <b>phrase</b> <b>index</b> (NPI), verb <b>phrase</b> <b>index</b> (VPI), and length complexity index (LCI) developed by Shriner (1967). The data was also analyzed according to number of noun phrases, number of verb phrases, and total utterances. In the statistical treatment of the analyzed data, correlation coefficients were obtained between all possible pairs of situations and conditions {{on each of the}} quantitative and qualitative measures. A series of t tests was also computed between pairs of situations on each of these measures. The analysis {{of the results of the}} quantitative and qualitative measures indicated that the samples of language elicited in the six different situations were not comparable. Discussion of these results considered a range of factors that may have contributed to the differences noted. Factors related to the nature of the situations, the setting within which the language was elicited, and person factors, such as the effect of the experimenter and the effect of the mother, were considered. It was concluded that such factors as those mentioned above were integral aspects of the language situation worthy of equal consideration with the language elicited...|$|R
40|$|Phrase searching in text indexes Compare {{different}} approaches to perform phrase searching, and consider a new approach whereas bigrams is considered as index term. This master thesis focus at the challenges within phrase searching in large text indexes, and to assess alternative approaches to cope with such indexes. This goal was achieved by performing an experiment, based on the theory of using bigrams consisting of stopwords as additional index terms. Realizing the characteristics within inverted index structures, we utilized stopwords as indicators for severe long posting lists. The characteristics of stopwords proved valuable, and they were collected based on a already established index for {{a subset of the}} TREC GOV 2 collection. In alternative approaches we outlined two 9 ̆ 3 state of the art 9 ̆ 4 index structures, speciﬁcally designed to cope with phrase searching challenges. The ﬁrst structure - nextword index - followed a modiﬁcation of the inverted index structure. The second structure - <b>phrase</b> <b>index</b> - utilized the inverted structure in using complete <b>phrases</b> as <b>index</b> terms. Our bigram index focused on the same manipulation of the inverted index structure as the <b>phrase</b> <b>index,</b> using bigrams of words to rastically cut posting lists lengths. This was one of our main goals, as we identiﬁed stopwords posting list lengths {{to be one of the}} primary challenges with phrase searching in inverted index structures. Using stopwords to create and select bigrams proved successful to enhance phrase searching, as response times substantially improved. We conclude that our bigram index provides a signiﬁcant performance in crease in terms of query evaluation time, and outperforms the standard inverted <b>index</b> within <b>phrase</b> searching...|$|R
40|$|Tiré du site Internet de Onestar Press: "Consummation Breakdown is a {{book with}} two front covers which THEREFORE offers {{two-directional}} reading. If read following one of the covers, this book shows the facsimile of a chronological arrangement : ordinary till receipts collected by the artist AND folded over {{the bottom of the}} right page, concealing the last bit of information WHICH IN TURN shows on the the left page overleaf. Flicking the pages backwards and upside down (following the other cover), one can read an <b>index</b> of <b>phrases</b> terminating each transaction, on the corresponding upright ends of each receipt. - Gabriel Kuri. "...|$|R
40|$|The {{factor of}} failure in Arabic {{language}} learning in Indonesia {{is the lack}} of activities in language learning. One of language learning activities is identifying meaning or word meaning and phrase in dictionary. However, word and <b>phrase</b> <b>index</b> developed based on teaching materials at madrasah non-pesantren was not developed yet. Therefore, Research and Development is needed to apply. After having collected the word and <b>phrase</b> <b>index</b> that were collected from teaching materials of Arabic language at MI non-pesantren in Indonesia, then it was tested at MIN Malang. The result showed that: there was a development in Arabic language learning through word and phrase index; that was students’ competence to identify meaning and collaboration in understanding text and context in a sentence. After having applied the experiment, students’ average score increased from 52 with 19 failed students to 81, 26 with 5 failed students. DOI: 10. 24865 /ajas. v 1 i 2. ...|$|R
40|$|Phrase {{matching}} is {{a common}} IR technique to search text and identify relevant documents in a document collection. Phrase matching in XML presents new challenges as text may be interleaved with arbitrary markup, thwarting search techniques that require strict contiguity or close proximity of keywords. We present a technique for phrase matching in XML that permits dynamic specification of both the phrase to be matched and the markup to be ignored. We develop an effective algorithm for our technique that utilizes inverted <b>indices</b> on <b>phrase</b> words and XML tags. We describe experimental results comparing our algorithm to an indexed-nested loop algorithm that illustrate our algorithm's efficiency...|$|R
40|$|We {{address the}} problem of {{identifying}} words and phrases that accurately capture, or contribute to, the semantic gist of deci-sions made in multi-party human-human meetings. We first de-scribe our approach to modelling decision discussions in spo-ken meetings and then compare two approaches to extracting information from these discussions. The first one uses an open-domain semantic parser that identifies candidate phrases for decision summaries and then employs machine learning tech-niques to select from those candidate phrases. The second one uses categorical and sequential classifiers that exploit simple syntactic and semantic features to identify words and phrases relevant for decision summarization. <b>Index</b> Terms: <b>phrase</b> extraction, human-human meetings, deci-sion detection and summarizatio...|$|R
40|$|In {{this paper}} we {{describe}} an experiment with syntactic <b>phrase</b> <b>indexing</b> for Dutch texts. We compare different choices for combining terms to form head-modifier pairs {{and we also}} investigate {{the effect of adding}} none, one, or all constituent parts of the pair as a separate index term. The results of our experiments show that using head-modifier pairs as index terms can improve both recall and precision significantly but only if all constituent parts are also added separately. We found that using both Noun-Adjective and Noun-Noun head-modifier pairs produced the best results. Keywords Natural language processing; syntactic phrase indexing; head-modifier pairs; Dutch. 1 Introduction The work described in this paper is part of the UPLIFT project 1. UPLIFT investigates whether linguistic tools can improve and extend the functionality of vector space text retrieval systems. This paper describes an experiment with syntactic <b>phrase</b> <b>indexing</b> for Dutch texts. The basic idea behind phrase i [...] ...|$|R
40|$|Second edition, 1898 [...] . 17 copies [...] . " cf. Chettle, H. The {{tragedy of}} Hoffmann, 1917. Imprint fictitious. Printed by the editor for private circulation. cf. Jaggard, p. 552. Introduction [...] Arden of Feversham [...] Appendix: Jacob's preface (to 1770 reprint) [...] Ballad on Arden of Feversham [...] <b>Phrase</b> <b>index</b> [...] Note on the surreptitious issue of Arden of Feversham. Jaggard, W. Shakespeare bibl.,Mode of access: Internet...|$|R
40|$|An edition, {{containing}} {{about one-third}} of the present collection, was published in 2 parts, without music, 1843 - 44. cf. Pref. Edited, with <b>indexes</b> of Irish <b>phrases,</b> names of persons, and places, by Thomas O. Davis, who contributed many songs. Contains 17 original airs, and 22 old Irish airs. Issued in monthly parts, July-December 1844. Lettered on cover : National ballads and songs. Aded t. -p. illustrated. Mode of access: Internet...|$|R
40|$|Within the autosegmental-metrical {{theory of}} {{intonation}} [1, 2], {{there is only}} weak evidence {{for the existence of}} the intermediate phrase (ip) for French. Our proposal is that the emergence of an intermediate prosodic level is not merely linked to a specific focus or marked syntactic structure, while predicting that an alignment constraint (ALING-XP,R; ip,R) conspires to place an ip boundary at the right edge of a maximal projection, such as at an NP/VP boundary, when the maximal projection can be parsed into at least two accentual phrases. Alos, an ip boundary appears to be signaled by prosodic cues that are stronger than the ones associated to (ipinternal) AP boundaries. The alignment between major syntactic constituents and prosodic structure appears to be signaled by a H- tone aligned at the right edge of the ip (blocking recursive downstep of ip-internal LH * rises) as well as preboundary lengthening. Finally, partial reset of the first LH * following the ip boundary is taken as evidence for an internal structuring of the Intonation <b>Phrase.</b> <b>Index</b> Terms: intermediate <b>phrase,</b> prosodic phrasing, preboundary lengthening, downstep, pitch reset, embedde...|$|R
40|$|Database Tomography (DT) is a textual {{database}} analysis system {{consisting of}} two major components: 1) algorithms for extracting multi-word phrase frequencies and phrase proximities (physical closeness of the multi-word technical phrases) from any type of large textual database, to augment 2) interpretative capabilities of the expert human analyst. DT was used to derive technical intelligence from a Power Sources database derived from the Science Citation <b>Index</b> (SCI). <b>Phrase</b> frequency analysis by the technical domain experts provided the pervasive technical themes of the Power Sources database, and the phrase proximity analysis provided the relationships among the pervasive technical themes. Bibliometric analysis of the Power Sources literature supplemented the DT results with author / journal / institution / country publication and citation data...|$|R
40|$|In this paper, {{we present}} a new phrase break {{prediction}} method that integrates second-order information into general maximum entropy model. The phrase break prediction problem was mapped into a classification problem in our research. The features we used for the prediction of phrase breaks are of several layers such as local features (part-of-speech (POS) tags, a lexicon, lengths of eojeols 1 and location of juncture in the sentence), global features (chunk label derived from a eojeol parse tree) and second-order features (distance probability of previous and next phrase break). These three features were combined and used in the experiments, {{and we were able}} to generate good performance especially in the major <b>phrase</b> break prediction. <b>Index</b> Terms: <b>phrase</b> break, prosodic phrasing, speech synthesis, ToB...|$|R
