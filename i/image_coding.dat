1895|1485|Public
2500|$|High Efficiency Image File Format, image {{container}} {{format for}} HEVC and other <b>image</b> <b>coding</b> formats ...|$|E
5000|$|I. Sadeh, “Properties of <b>image</b> <b>coding</b> by {{polynomial}} representation,” ...|$|E
5000|$|The JPEG 2000 <b>image</b> <b>coding</b> system (ISO/IEC 15444) {{consists}} of following parts: ...|$|E
40|$|The {{algorithms}} for bit-level erasure decoding of (n,k) Reed-Solomon codes over GF(2 m) {{beyond the}} design erasure decoding distance of n-k are considered. Erasure decoding {{takes place in}} the (mk, nk) binary <b>image</b> <b>code.</b> When the number of erasures exceeds n-k, then additional parity check equations must be developed. We describe how these equations are obtained and some of their properties. Decoding algorithms are discussed which tradeoff decoding performance for computational complexity. Simulation results for several families of codes are presented which provide information about good binary <b>image</b> <b>codes...</b>|$|R
40|$|Low-density parity-check (LDPC) codes {{have been}} known for their {{outstanding}} error-correction capabilities. With low- complexity decoding algorithms and a near capacity performance, these codes {{are among the most}} promising forward error correction schemes. LDPC decoding algorithms are generally sub-optimal and their performance not only depends on the codes, but also on many other factors, such as the code representation. In particular, a given non- binary code can be associated with a number of different field or ring <b>image</b> <b>codes.</b> Additionally, each LDPC code can be described with many different Tanner graphs. Each of these different images and graphs can possibly lead to a different performance when used with iterative decoding algorithms. Consequently, in this dissertation we try to find better representations, i. e., graphs and <b>images,</b> for LDPC <b>codes.</b> We take the first step by analyzing LDPC codes over multiple-input single-output (MISO) channels. In an n_T by 1 MISO system with a modulation of alphabet size 2 M̂, each group of n_T transmitted symbols are combined and produce one received symbol at the receiver. As a result, we consider the LDPC-coded MISO system as an LDPC code over a 2 ̂{M n_T}-ary alphabet. We introduce a modified Tanner graph to represent MISO-LDPC systems and merge the MISO symbol detection and binary LDPC decoding steps into a single message passing decoding algorithm. We present an efficient implementation for belief propagation decoding that significantly reduces the decoding complexity. With numerical simulations, we show that belief propagation decoding over modified graphs outperforms the conventional decoding algorithm for short length LDPC codes over unknown channels. Subsequently, we continue by studying images of non-binary LDPC codes. The high complexity of belief propagation decoding has been proven to be a detrimental factor for these codes. Thereby, we suggest employing lower complexity decoding algorithms over <b>image</b> <b>codes</b> instead. We introduce three classes of binary <b>image</b> <b>codes</b> for a given non-binary code, namely: basic, mixed, and extended binary <b>image</b> <b>codes.</b> We establish upper and lower bounds on the minimum distance of these binary <b>image</b> <b>codes,</b> and present two techniques to find binary <b>image</b> <b>codes</b> with better performance under belief propagation decoding algorithm. In particular, we present a greedy algorithm to find optimized binary <b>image</b> <b>codes.</b> We then proceed by investigation of the ring <b>image</b> <b>codes.</b> Specifically, we introduce matrix-ring-image codes for a given non-binary code. We derive a belief propagation decoding algorithm for these codes, and with numerical simulations, we demonstrate that the low-complexity belief propagation decoding of optimized <b>image</b> <b>codes</b> has a performance very close to the high complexity BP decoding of the original non-binary code. Finally, in a separate study, we investigate the performance of iterative decoders over binary erasure channels. In particular, we present a novel approach to evaluate the inherent unequal error protection properties of irregular LDPC codes over binary erasure channels. Exploiting the finite length scaling methodology, that has been used to study the average bit error rate of finite-length LDPC codes, we introduce a scaling approach to approximate the bit erasure rates in the waterfall region of variable nodes with different degrees. Comparing the bit erasure rates obtained from Monte Carlo simulation with the proposed scaling approximations, we demonstrate that the scaling approach provides a close approximation {{for a wide range of}} code lengths. In view of the complexity associated with the numerical evaluation of the scaling approximation, we also derive simpler upper and lower bounds and demonstrate through numerical simulations that these bounds are very close to the scaling approximatio...|$|R
40|$|Example-based {{learning}} of codes that statistically encode general image classes is of vital importance for computational vision. Recently, non-negative matrix factorization (NMF) was suggested to provide <b>image</b> <b>codes</b> {{that are both}} sparse and localized, in contrast to established nonlocal methods like PCA. In this paper we adopt and generalize this approach to develop a novel learning framework that allows to efficiently compute sparsity-controlled invariant <b>image</b> <b>codes</b> by a well-defined sequence of convex conic programs. Applying the corresponding parameter-free algorithm to various image classes results in semantically relevant and transformation-invariant image representations that are remarkably robust against noise and quantization. 1. Introduction and Relate...|$|R
50|$|More {{theoretical}} and practical results about <b>Image</b> <b>Coding</b> based on Polynomial approximation of images.|$|E
5000|$|Liaison {{representative}} to MPEG and JPEG from ITU-T on video and <b>image</b> <b>coding</b> topics.|$|E
5000|$|High Efficiency Image File Format, image {{container}} {{format for}} HEVC and other <b>image</b> <b>coding</b> formats ...|$|E
40|$|We {{present a}} {{modified}} version of an embedded wavelet coding scheme, first suggested by Shapiro (see IEEE Transactions on Signal Processing, vol. 41, no. 12, p. 3445 - 3462, 1993), that improves the performance of the original algorithm in a visual subjective distortion sense. We preserve the features of the original Shapiro's embedded coder. It is possible to choose a fixed target bit rate, as the information needed to represent an <b>image</b> <b>coded</b> at some rate always contains the needed information for the same <b>image</b> <b>coded</b> at lower rates. Therefore, the decoder can cease decoding the bit stream at any point, simulating an <b>image</b> <b>coded</b> at a lower rate corresponding to the truncated bit stream. We also introduce some perceptive improvements by adopting different (more regular) filters with respect to the original QMF pyramid filters proposed by Simoncelli, Hingorani et al. (1987) and used by Shapiro. These filters are synthesized using a “wavelet approach” instead of a “subband approach”, and this leads to a better control on their regularity properties, jointly with better perceptual performanc...|$|R
5000|$|There {{are several}} {{theories}} {{as to how}} mental images are formed in the mind. These include the dual-code theory, the propositional theory, and the functional-equivalency hypothesis. The dual-code theory, created by Allan Paivio in 1971, is the theory that we use two separate codes to represent information in our brains: <b>image</b> <b>codes</b> and verbal <b>codes.</b> <b>Image</b> <b>codes</b> are things like thinking of {{a picture of a}} dog when you are thinking of a dog, whereas a verbal code would be to think of the word [...] "dog". Another example is the difference between thinking of abstract words such as justice or love and thinking of concrete words like elephant or chair. When abstract words are thought of, it is easier to think of them in terms of verbal codes—finding words that define them or describe them. With concrete words, it is often easier to use <b>image</b> <b>codes</b> and bring up a picture of a human or chair in your mind rather than words associated or descriptive of them.|$|R
40|$|The {{objective}} {{of this paper is}} to perform the innovation design for improving the recognition of a captured QR <b>code</b> <b>image</b> with blur through the Pillbox filter analysis. QR <b>code</b> <b>images</b> can be captured by digital video cameras. Many factors contribute to QR code decoding failure, such as the low quality of the image. Focus is an important factor that affects the quality of the image. This study discusses the out-of-focus QR <b>code</b> <b>image</b> and aims to improve the recognition of the contents in the QR <b>code</b> <b>image.</b> Many studies have used the pillbox filter (circular averaging filter) method to simulate an out-of-focus image. This method is also used in this investigation to improve the recognition of a captured QR <b>code</b> <b>image.</b> A blurred QR <b>code</b> <b>image</b> is separated into nine levels. In the experiment, four different quantitative approaches are used to reconstruct and decode an out-of-focus QR <b>code</b> <b>image.</b> These nine reconstructed QR <b>code</b> <b>images</b> using methods are then compared. The final experimental results indicate improvements in identification...|$|R
50|$|Although {{the best-known}} {{application}} of lapped transforms {{has been for}} audio coding, they have also been used for video and <b>image</b> <b>coding</b> and various other applications. They are used in video coding for coding I-frames in VC-1 and for <b>image</b> <b>coding</b> in the JPEG XR format. More recently, a form of lapped transform has also {{been used in the}} development of the Daala video coding format.|$|E
50|$|Presented a novel {{method for}} <b>Image</b> <b>Coding</b> based on Polynomial {{approximation}} of images. Theoretical and practical results were presented.|$|E
50|$|In video, time {{is often}} {{considered}} {{as the third}} dimension. Still <b>image</b> <b>coding</b> techniques can be expanded to an extra dimension.|$|E
50|$|Picture stimuli have an {{advantage}} over word stimuli because they are dually encoded; they generate a verbal and <b>image</b> <b>code,</b> whereas word stimuli only generate a verbal code. Pictures are likely to generate a verbal label, whereas words {{are not likely to}} generate image labels.|$|R
40|$|Recent {{algorithms}} for sparse coding {{and independent}} component analysis (ICA) have demonstrated how localized features {{can be learned}} from natural images. However, these approaches do not take image transformations into account. As a result, they produce <b>image</b> <b>codes</b> that are redundant because the same feature is learned at multiple locations. We describe an algorithm for sparse coding based on a bilinear generative model of images. By explicitly modeling the interaction between image features and their transformations, the bilinear approach helps reduce redundancy in the <b>image</b> <b>code</b> and provides a basis for transformationinvariant vision. We present results demonstrating bilinear sparse <b>coding</b> of natural <b>images.</b> We also explore an extension of the model that can capture spatial relationships between the independent features of an object, thereby providing a new framework for parts-based object recognition. ...|$|R
40|$|We apply a {{probabilistic}} method for learning efficient <b>image</b> <b>codes</b> {{to the problem}} of unsupervised classification, segmentation and de-noising of images. The method is based on the Independent Component Analysis (ICA) mixture model proposed for unsupervised classification and automatic context switching in blind source separation [I]. In this paper, we demonstrate that this algorithm is effective in classifying complex image textures such as trees and rocks in natural scenes. The algorithm is useful for de-noising and filling in missing pixels in images with complex structures. The advantage of this model is that <b>image</b> <b>codes</b> can be learned with increasing numbers of basis function classes. Our results suggest that the ICA mixture model provides greater flexibility in modeling structure and in finding more image features than in either Gaussian mixture models or standard ICA algorithms...|$|R
5000|$|Starting in late 2006, VCEG {{has also}} been {{responsible}} for the ITU-T work on still <b>image</b> <b>coding</b> standards including the following: ...|$|E
50|$|Proceedings of {{the conference}} on Information Sciences and Systems 1992 Princeton UniversityConference paper - New {{theoretical}} and practical results about <b>Image</b> <b>Coding</b> based on Polynomial approximation of images.|$|E
50|$|The {{artifacts}} at block boundaries can {{be reduced}} by applying a deblocking filter. As in still <b>image</b> <b>coding,</b> {{it is possible to}} apply a deblocking filter to the decoder output as post-processing.|$|E
5000|$|Use <b>images,</b> symbols, <b>codes,</b> and {{dimensions}} {{throughout your}} mind map.|$|R
40|$|MorphoLibJ v 1. 2. 1 List of changes: 	several {{corrections}} {{added to}} user manual, with updated cross-references and new examples 	added DOI 	implementation of hybrid algorithm for morphological reconstruction of 3 D <b>images</b> <b>coded</b> with short. 	add static methods in Images 3 D for checking {{type and size}} consistency of two image...|$|R
40|$|In this paper, we {{dealt with}} an unequal error {{protection}} (UEP) solution for visual-based quantized JPEG <b>images</b> <b>coded</b> and transmitted over time-varying channels by focusing our purpose {{on the question of}} how to improve the total rate-distortion performances of a given UEP scheme. In consequence, over an UEP scheme, a packet transmission solution is proposed with a retransmission protocol using rate-compatible punctured convolutional codes (RCPC codes). The simulated time-varying channel is a Rayleigh fading type where the transmitted packets do not always experience the channel distortion with the same intensity. Attractive results are issued from our simulations. Key words: ARQ/FEC, JPEG <b>images,</b> RCPC <b>codes,</b> time-varying channels...|$|R
5000|$|Due {{to camera}} {{problems}} {{and the need to}} reduce file size, there is a slight modification to the <b>image</b> <b>coding</b> scheme so that each compressed line is effectively bandwidth limited on the number of bits available to encode it.|$|E
50|$|In July 2006, {{the video}} coding {{work of the}} ITU-T led by VCEG was voted as the most {{influential}} area of the standardization work of the CCITT and ITU-T in their 50-year history. The <b>image</b> <b>coding</b> work that {{is now in the}} domain of VCEG was also highly ranked in the voting, placing third overall.|$|E
50|$|New {{work has}} been started in the JPEG {{committee}} to enable the use of JPEG XR <b>image</b> <b>coding</b> within the JPX file storage format — enabling use of the JPIP protocol, which allows interactive browsing of networked images. Additionally, a Motion JPEG XR specification was approved as an ISO standard for motion (video) compression in March 2010.|$|E
3000|$|..., which {{meets the}} {{requirements}} of HVS. Thus, {{the quality of the}} reconstructed images is improved. While evaluated by the regional selective image quality metrics, ΔPMOS_SSIM is 0.78 and ΔPMOS_PSNR is − 0.70. It means the difference between the qualities of reconstructed <b>images</b> <b>coded</b> by the proposed MVC scheme and JMVM is tiny and imperceptible. However, the important and interesting fact is that [...]...|$|R
40|$|In this paper, I {{propose a}} method for {{efficient}} <b>coding</b> of <b>images</b> using cellular automata. This method allows us to describe each selected group of neighboring cells of bend points in the contour. This method enables us to compress the <b>image</b> <b>code.</b> These groups will be separated to objects in an image by using cellular automata which uses bend-points determination...|$|R
40|$|This paper {{presents}} a cryptographic technique that encrypts secret information using a <b>coding</b> <b>image</b> by transforming the pixels {{of this image}} from the intensity domain to the characters domain using a hash function. In the proposed technique, the <b>coding</b> <b>image</b> {{will be used to}} encrypt the secret information at the sender and decrypt it at the receiver using the pixels whose intensity values are transformed to characters. A matrix of characters corresponding to the <b>coding</b> <b>image</b> is generated where each character in this matrix corresponds to a pixel in the <b>coding</b> <b>image</b> and each character in the secret information is mapped to a character in the matrix of characters. The locations of characters in the matrix of characters that correspond to pixels in the <b>coding</b> <b>image</b> and correspond to characters in the secret information forms the pixels map. The pixels map is encrypted using a secret key before being sent to the receiver on a secure communication channel different from that used to send the <b>coding</b> <b>image</b> and at different times. Upon receiving the <b>coding</b> <b>image</b> and the encrypted pixels map the receiver uses the secret key to decrypt the pixels map and uses the <b>coding</b> <b>image</b> and the hash function to generate the matrix of characters. Each location in the pixels map is used to retrieve a character from the matrix of characters in order to decrypt the secret information. Experimental results showed the effectiveness and the efficiency of the proposed algorithm where a message was encrypted using a <b>coding</b> <b>image</b> without modifying its pixels and it was decrypted without errors...|$|R
50|$|Jayant's {{personal}} {{research has}} been in the field of digital coding, secure voice, and transmission of information signals. His {{research has been}} on techniques for speech encryption, packet voice, signal enhancement, robust vector quantization and <b>image</b> <b>coding.</b> He has made pioneering contributions to waveform quantization. He is the author of five books, and he has received 40 patents.|$|E
5000|$|OpenIllusionist {{is closely}} {{connected}} with the Media Engineering Group (MEG) of the Department of Electronics at the University of York, UK - specifically the Visual Systems subgroup. This group was formed when John Robinson took up a professorship in the Department in 2000/2001, bringing with him a background in <b>image</b> <b>coding</b> and an interest in augmented reality.|$|E
5000|$|Huang {{went on to}} the United States {{to study}} at the Massachusetts Institute of Technology (MIT). At MIT he worked {{initially}} with Peter Elias, who was interested in information theory and <b>image</b> <b>coding,</b> and then with William F. Schreiber. At that time scanning equipment was not commercially available, so it was necessary to build a scanner for digitizing and reproducing images. Computer programs were written in assembly language using a prototype Lincoln Lab TX-0 computer. Descriptions of digitized images were stored on paper tape with punched holes. [...] Huang was supervised by Schreiber for both his M.S. thesis, Picture statistics and linearly interpolative coding (1960), and his Sc.D. thesis, Pictorial noise (1963). [...] His master's work focused on algorithms for <b>image</b> <b>coding</b> using adaptive techniques for interpolation with sensitivity to edges. His doctorate included work on the subjective effects of pictorial noise across a spectrum.|$|E
50|$|Using HEVC's intra frame encoding, a still-image coded format called Better Portable Graphics (BPG) {{has been}} {{proposed}} by the programmer Fabrice Bellard. It is essentially a wrapper for <b>images</b> <b>coded</b> using the HEVC Main 4:4:4 16 Still Picture profile with up to 14 bits per sample, although it uses an abbreviated header syntax and adds explicit support for Exif, ICC profiles, and XMP metadata.|$|R
40|$|<b>Image</b> {{and video}} <b>coding</b> is an {{optimization}} problem. A successful <b>image</b> and video <b>coding</b> algorithm delivers a good tradeo# between visual quality and other coding performance measures, such as compression, complexity, scalability, robustness, and security. In this paper, we follow two recent trends in <b>image</b> and video <b>coding</b> research. One is to incorporate human visual system (HVS) models {{to improve the}} current state-of-the-art of <b>image</b> and video <b>coding</b> algorithms by better exploiting {{the properties of the}} intended receiver. The other is to design rate scalable image and video codecs, which allow the extraction of coded visual information at continuously varying bit rates from a single compressed bitstream...|$|R
40|$|To publish {{documents}} we {{must take}} care of documents' structure and their formal layout. If a large document is composed {{by a lot of}} parts including text, <b>images,</b> <b>code</b> and so on, they will lay in different files; so file organization should also be a concern {{in the mind of the}} publisher. Normally, we focus on the structure and design, and forget file organization until this becomes a trouble, at production stage...|$|R
