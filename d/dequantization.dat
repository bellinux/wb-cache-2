79|3|Public
25|$|These assertions {{are tested}} {{on a large}} set of {{randomized}} input images, to handle the worst cases. The former IEEE 1180–1990 standard contained some similar precision requirements. The precision has a consequence on the implementation of decoders, and it is critical because some encoding processes (notably used for encoding sequences of images like MPEG) {{need to be able}} to construct, on the encoder side, a reference decoded image. In order to support 8-bit precision per pixel component output, <b>dequantization</b> and inverse DCT transforms are typically implemented with at least 14-bit precision in optimized decoders.|$|E
2500|$|The {{encoding}} {{description in}} the JPEG standard does not fix the precision {{needed for the}} output compressed image. However, the JPEG standard (and the similar MPEG standards) includes some precision requirements for the decoding, including {{all parts of the}} decoding process (variable length decoding, inverse DCT, <b>dequantization,</b> renormalization of outputs); the output from the reference algorithm must not exceed: ...|$|E
50|$|These assertions {{are tested}} {{on a large}} set of {{randomized}} input images, to handle the worst cases. The former IEEE 1180-1990 standard contained some similar precision requirements. The precision has a consequence on the implementation of decoders, and it is critical because some encoding processes (notably used for encoding sequences of images like MPEG) {{need to be able}} to construct, on the encoder side, a reference decoded image. In order to support 8-bit precision per pixel component output, <b>dequantization</b> and inverse DCT transforms are typically implemented with at least 14-bit precision in optimized decoders.|$|E
40|$|We {{present a}} {{reformulation}} {{of quantum mechanics}} in terms of probability measures and functions on a general classical sample space and in particular in terms of probability densities and functions on phase space. The basis of our proceeding is the existence of so-called statistically complete observables and the duality between the state spaces and the spaces of the observables, the latter holding in the quantum {{as well as in}} the classical case. In the phase-space context, we further discuss joint position-momentum observables, Hilbert spaces of infinitely differentiable functions on phase space, and <b>dequantizations.</b> Finally, the relation of quantum dynamics to the classical Liouville dynamics is investigated. Comment: 118 pages, no figures. This work was published 1997 as a book by Wissenschaft und Technik Verlag Berlin. Since it has proved to be of interest to some researchers, it is published here for an easier acces...|$|R
40|$|To assess if {{a digital}} image has been (or not) doubly {{compressed}} is a challenging issue especially in forensics domain where could be fundamental clarify if, {{in addition to}} the compression at the time of shooting, the picture was decompressed (in some way) and then resaved. This is not a clear indication of forgery, but it guarantees that the image, probably, is not the original one. In this paper we propose a novel technique able to recover the coefficients of the first compression in a double compressed JPEG image under some assumptions. The proposed approach exploits how successive quantizations followed by <b>dequantizations</b> introduce some regularities (e. g., sequence of zero and not zero values) on the histograms of coefficient distributions that could be analyzed to recover properly the original compression parameters. Experimental results and comparisons with state of the art methods confirm the effectiveness of the proposed approach...|$|R
40|$|One of {{the most}} common {{problems}} in the image forensics field is the reconstruction of the history of an image or a video. The data related to the characteristics of the camera that carried out the shooting, together with the reconstruction of the (possible) further processing, allow us to have some useful hints about the originality of the visual document under analysis. For example, if an image has been subjected to more than one JPEG compression, we can state that the considered image is not the exact bitstream generated by the camera at the time of shooting. It is then useful to estimate the quantization steps of the first compression, which, in case of JPEG images edited and then saved again in the same format, are no more available in the embedded metadata. In this paper, we present a novel algorithm to achieve this goal in case of double JPEG compressed images. The proposed approach copes with the case when the second quantization step is lower than the first one, exploiting the effects of successive quantizations followed by <b>dequantizations.</b> To improve the results of the estimation, a proper filtering strategy together with a function devoted to find the first quantization step, have been designed. Experimental results and comparisons with the state-of-the-art methods, confirm the effectiveness of the proposed approach...|$|R
40|$|For {{functions}} defined on C^n or (R_+) ^n we {{construct a}} <b>dequantization</b> transform, which {{is closely related}} to the Maslov <b>dequantization.</b> The subdifferential at the origin of a dequantized polynomial coincides with its Newton polytope. For the semiring of polynomials with nonnegative coefficients, the <b>dequantization</b> transform is a homomorphism of this semiring to the idempotent semiring of convex polytopes with the well-known Minkowski operations. Using the <b>dequantization</b> transform we generalize these results to a wide class of functions and convex sets. Comment: 7 page...|$|E
40|$|Abstract. For {{functions}} defined on Cn or Rn + we {{construct a}} <b>dequantization</b> transform f ↦ → ˆ f; this transform {{is closely related}} to the Maslov <b>dequantization.</b> If f is a polynomial, then the subdifferential ∂ ˆ f of ˆ f at the origin coincides with the Newton polytope of f. For the semiring of polynomials with nonnegative coefficients, the <b>dequantization</b> transform is a homomorphism of this semiring to the idempotent semiring of convex polytopes with respect to the well-known Minkowski operations. Using the <b>dequantization</b> transform we generalize these results to a wide class of functions and convex sets. 1. Introduction. The Maslo...|$|E
40|$|An {{attempt to}} {{ascertain}} reasons of the quantum logic efficiency was made. To do this, we reduce the {{complete set of}} elementary quantum logic operations to the classical one via taking the semi-classical limit (<b>dequantization).</b> We estimate {{the amount of information}} loss for any element of the set and obtain general expression for the loss estimation of any quantum algorithm under <b>dequantization.</b> We demonstrate the technique on quantum discrete fast Fourier transform and Grover search algorithms. Comment: 14 page...|$|E
40|$|AbstractWe {{strengthen}} {{the connection between}} information theory and quantum-mechanical systems using a recently developed <b>dequantization</b> procedure whereby quantum fluctuations latent in the quantum momentum are suppressed. The <b>dequantization</b> procedure results in a decomposition of the quantum kinetic energy as the sum of a classical term and a purely quantum term. The purely quantum term, which results from the quantum fluctuations, is essentially identical to the Fisher information. The classical term is complementary to the Fisher information and, in this sense, it plays a role analogous {{to that of the}} Shannon entropy. We demonstrate the kinetic energy decomposition for both stationary and nonstationary states and employ it to shed light on the nature of kinetic energy functionals...|$|E
40|$|We {{present a}} <b>dequantization</b> {{procedure}} {{based on a}} variational approach whereby quantum fluctuations latent in the quantum momentum are suppressed. This is done by modeling generic quantum fluctuations by means of deformations of the momentum operator, which in turn give rise to deformed kinetic terms quantifying the amount of ``fuzzyness'' caused by such fluctuations. Considered as a functional of such deformations, the deformed kinetic term is shown to possess a unique minimum {{which turns out to}} be the classical kinetic energy. Thus a variational procedure determines the particular deformation that removes the quantum fluctuations, resulting in <b>dequantization</b> of the system. As a byproduct, we obtain a natural decomposition for the quantum kinetic term which has possible relevance for the construction of kinetic-energy functionals...|$|E
40|$|Tropical and {{idempotent}} {{analysis with}} their relations to the Hamilton-Jacobi and matrix Bellman equations are discussed. Some <b>dequantization</b> procedures {{are important in}} tropical and idempotent mathematics. In particular, the Hamilton-Jacobi-Bellman equation is treated {{as a result of}} the Maslov <b>dequantization</b> applied to the Schrödinger equation. This leads to a linearity of the Hamilton-Jacobi-Bellman equation over tropical algebras. The correspondence principle and the superposition principle of idempotent mathematics are formulated and examined. The matrix Bellman equation and its applications to optimization problems on graphs are discussed. Universal algorithms for numerical algorithms in idempotent mathematics are investigated. In particular, an idempotent version of interval analysis is briefly discussed. Comment: 70 pages, 5 figures, CIME lectures (2011), to be published in Lecture Notes in Mathematics (Springer...|$|E
40|$|We {{strengthen}} {{the connection between}} Information Theory and quantum-mechanical systems using a recently developed <b>dequantization</b> procedure whereby quantum fluctuations latent in the quantum momentum are suppressed. The <b>dequantization</b> procedure results in a decomposition of the quantum kinetic energy as the sum of a classical term and a purely quantum term. The purely quantum term, which results from the quantum fluctuations, is essentially identical to the Fisher information. The classical term is complementary to the Fisher information and, in this sense, it plays a role analogous {{to that of the}} Shannon entropy. We demonstrate the kinetic energy decomposition for both stationary and nonstationary states and employ it to shed light on the nature of kinetic-energy functionals. Comment: 13 pages, 3 figures. To appear in J. Comput. Appl. Mat...|$|E
40|$|We use Berezin's <b>dequantization</b> {{procedure}} {{to define a}} formal *-product on the algebra of smooth functions on the bounded symmetric domains. We prove that this formal *-product is convergent on a dense subalgebra of the algebra of smooth functions. © 1995 Kluwer Academic Publishers. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|E
40|$|We use Berezin’s <b>dequantization</b> {{procedure}} {{to define a}} formal *- product on a dense subalgebra of the algebra ofsmooth functions on a compact homogeneous Kähler manifold M. We prove that this formal *-product isconvergent when M isa hermitian symmetric space. © 1993 American Mathematical Society. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|E
40|$|In this Letter {{we propose}} two path {{integral}} approaches {{to describe the}} classical mechanics of spinning particles. We show how these formulations {{can be derived from}} the associated quantum ones via a sort of geometrical <b>dequantization</b> procedure proposed in a previous paper. Comment: 13 pages, Latex, title change...|$|E
40|$|We {{present in}} this paper two <b>dequantization</b> {{procedures}} of coadjoint orbits {{in the setting of}} exponential solvable Lie groups. The first one consists in considering generalized moment sets of unitary representations. The second one concerns characteristic and Poisson characteristic varieties of some topological unitary modules over a deformed algebra appropriately associated with a given representation...|$|E
40|$|We {{present a}} <b>dequantization</b> {{procedure}} {{based on a}} variational approach whereby quantum fluctuations latent in the quantum momentum are suppressed. This is done by adding generic local deformations to the quantum momentum operator which {{give rise to a}} deformed kinetic term quantifying the amount of ``fuzzyness'' caused by such fluctuations. Considered as a functional of such deformations, the deformed kinetic term is shown to possess a unique minimum which is seen to be the classical kinetic energy. Furthermore, we show that extremization of the associated deformed action functional introduces an essential nonlinearity to the resulting field equations which are seen to be the classical Hamilton-Jacobi and continuity equations. Thus, a variational procedure determines the particular deformation that has the effect of suppressing the quantum fluctuations, resulting in <b>dequantization</b> of the system. Comment: 6 pages, 1 figure. v 2 : changes in presentation and conten...|$|E
40|$|The goal of {{this thesis}} is to {{demonstrate}} practical application of sparse data representation in the processing of sparse signals. For solving several example problems - denoising, <b>dequantization,</b> and sparse signal decomposition - convex optimization was used. The solutions were implemented in the Matlab environment. For each of the problems, there are two solutions - one for one-dimensional, and one for two-dimensional signal...|$|E
40|$|Many {{image and}} video {{compression}} schemes perform the discrete cosine transform to represent images in frequency space. An {{analysis of a}} suite of images confirms the previous finding of a Laplacian distribution to model the luminance AC coefficients. This model is used in two ways: to reduce <b>dequantization</b> error and to speed up optimal quantization matrix generation. 1 1. INTRODUCTION Many popular digital video compression schemes use the Discrete Cosine Transform (DCT) for transform coding [1]. In particular JPEG and MPEG [2] {{make use of the}} DCT to concentrate image information. It has been generally believed [1, 3] that the distribution of the luminance AC components is Laplacian (a double sided exponential, see Figure 1) : p(x) = 2 e We confirm that the Laplacian distribution is a good approximation for the distribution of AC coefficients. Modeling the coefficient distribution can be used to reduce mean-squared error (MSE) in <b>dequantization</b> and to speed rate-distortio [...] ...|$|E
40|$|LNS) {{arithmetic}} {{can reduce}} the power consumption for MPEG decoding compared to conventional fixed-point techniques. Although this introduces small numeric errors which violate the IEEE- 1180 standard for the Inverse Discrete Cosine Transform (IDCT), the visual effects of such error may be tolerable for portable battery-powered devices, like video phones, that have limited-resolution displays. The MPEG standard achieves video compression by quantization of the data fed to the IDCT. The MPEG decoder must multiply this data by <b>dequantization</b> factors. Such multiplication, by itself, is trivial with LNS since adding logarithms is equivalent to multiplication. The IEEE- 1180 standard suggests oddification, where fixed-point data is forced to become odd after <b>dequantization</b> to minimize IDCT mismatch between the encoder and the decoder. Oddification poses an implementation problem for data in LNS format. This paper suggests that the visual effect of LNS without oddification is nearly indistinguishable from LNS with oddification, meaning {{that the benefits of}} LNS in MPEG are even greater than previously expected...|$|E
3000|$|... is reconstructed by the <b>dequantization</b> process. Although the {{performance}} of the proposed decoder depends {{on the quality of the}} side information, the proposed decoder outperforms the conventional decoder significantly, especially at low bit rates. This is shown in Performance evaluation. The performance of the proposed decoder is much better than that of the conventional decoder, as the side information gets closer to the original Wyner-Ziv frame.|$|E
40|$|Abstract. To each natural {{deformation}} quantization on a Poisson manifold M {{we associate}} a Poisson morphism from the formal neighborhood of the zero section of T ∗ M {{to the formal}} neighborhood of the diagonal of the product M × ˜ M, where ˜ M is a copy of M with the opposite Poisson structure. We call it <b>dequantization</b> of the natural deformation quantization. Then we “dequantize” Fedosov’s quantization. 1...|$|E
40|$|Abstract- <b>Dequantization</b> through {{coherent}} {{states is}} used to study moment maps and symplectic manifolds for nuclear collective motion. The theory of collective motion in nuclei has as its geometric origin the comparison of certain nuclear phenomena with properties of a liquid drop. Bohr and Mottelson (1952, 53) / 1, 2 / introduced {{the idea of a}} irrotational flow and explained a variety of collective phenomena by the deformations and vibrations of a nuclear fluid. Th...|$|E
40|$|Abstract. We {{remark that}} the power {{diagrams}} from computer science are the spines of amoebas in algebraic geometry, or the hypersurfaces in tropical geometry. Our concept of a Morse poset generalizes to power diagrams. We show that there exists a discrete Morse function on the coherent triangulation, dual to the power diagram, such that its critical set equals the Morse poset of the power diagram. In the final section we use Maslov <b>dequantization</b> to compute the medial axis. 1...|$|E
40|$|This paper was/will be {{published}} in SPIE Electronic Imaging 2006 and is made available as an electronic preprint with permission of SPIE and IS&T. One print or electronic copy may be made for personal use only. Systematic or multiple reproduction, distribution to multiple locations via electronic or other means, duplication of any material in this paper for a fee or for commercial purposes, or modification {{of the content of}} the paper are prohibited. Color image <b>dequantization</b> by Constrained Diffusio...|$|E
40|$|The Weyl {{quantization}} {{of classical}} observables on the torus (as phase space) without regularity assumptions is explicitly computed. The equivalence class of symbols yielding the same Weyl operator is characterized. The Heisenberg equation for {{the dynamics of}} general quantum observables is written through the Moyal brackets on the torus {{and the support of}} the Wigner transform is characterized. Finally, a <b>dequantization</b> procedure is introduced that applies, for instance, to the Pauli matrices. As a result we obtain the corresponding classical symbols...|$|E
40|$|This {{paper is}} about the {{logarithmic}} limit sets of real semi-algebraic sets, and, more generally, about the logarithmic limit sets of sets definable in an o-minimal, polynomially bounded structure. We prove {{that most of the}} properties of the logarithmic limit sets of complex algebraic sets hold in the real case. This include the polyhedral structure and the relation with the theory of non-archimedean fields, tropical geometry and Maslov <b>dequantization.</b> Comment: 35 pages, 15 figures; some examples added, sections 2. 3 and 4. 1 shortened, some typos correcte...|$|E
40|$|To each natural {{deformation}} quantization on a Poisson manifold M {{we associate}} a Poisson morphism from the formal neighborhood of the zero {{section of the}} cotangent bundle to M to the formal neighborhood of the diagonal of the product M x M~, where M~ is a copy of M with the opposite Poisson structure. We call it <b>dequantization</b> of the natural deformation quantization. Then we "dequantize" Fedosov's quantization. Comment: 16 pages, latex; references, terminology, notation, and several typos corrected; to appear in "Letters in Math. Phys. ...|$|E
40|$|We derive some {{important}} {{features of the}} standard quantum mechanics from a certain classical-like (hidden variable) statistical model (it was called prequantum classical statistical field theory, PCSFT). The correspondence between classical and quantum quantities is asymptotic, so we call our approach asymptotic <b>dequantization.</b> The approach {{might be seen as}} a kind of retain, through post-von-Neumannian formalism, to the approach of the old quantum theory of Bohr and Sommerfeld, {{except for the fact that}} (the standard) quantum mechanics is now seen as a limit of a new type of classical statistical theory configured in an infinite-dimensional Hilbert space (over real numbers). One of unexpected consequences of PCSFT is the infinite dimension of physical space on the prequantum scale. Another important physical consequence is that PCSFT induces not only conventional linear Schrödinger's equation, but also nonlinear generalizations of Schrödinger's equations that have been studied, e. g., by De Broglie, Bialynicki-Birula and Mycielski, Davidson, Doebner, Gisin, Weinberg. On one hand, the natural derivation of such equations through the Hamiltonian formalism on the infinite dimensional phase space could be used as an argument in favor of "nonlinear quantum mechanics. " On the other hand, we can apply the well known experimental tests of "nonlinear quantum mechanics" to estimate the small parameter α of asymptotic <b>dequantization...</b>|$|E
40|$|<b>Dequantization</b> {{is a set}} {{of rules}} which turn quantum {{mechanics}} (QM) into classical mechanics (CM). It is not the WKB limit of QM. In this paper we show that, by extending time to a 3 -dimensional "supertime", we can dequantize the system in the sense of turning the Feynman path integral version of QM into the functional counterpart of the Koopman-von Neumann operatorial approach to CM. Somehow this procedure is the inverse of geometric quantization and we present it in three different polarizations: the Schrödinger, the momentum and the coherent states ones...|$|E
40|$|ReportAccurate {{prediction}} of wavelet coefficients relies on {{an understanding of}} the phase effects of edge alignment. This research examines techniques for uncovering edge information based on the available coefficients. These techniques are evaluated in the context of reconstructing an image from quantized wavelet coefficients. A predictor is described which can be trained on the coefficients to capture relationships among the pixels. Another method is presented where the quantized coefficients are interpolated to recreate an underlying continuous function. Based on this research, the interpolation process offers more promise in solving the <b>dequantization</b> problem...|$|E
40|$|Abstract: Shape placing {{problems}} on 2 d plane, {{also known as}} nesting geometrical problems, consist in managing shape co-arrangement, so all shapes are placed in a given way (e. g. they use minimal area). In the paper, an application of quantization to digitize all shapes is proposed, what allows performing arrangement with using algorithm based on digital structures. When nesting operation is finished, <b>dequantization</b> procedure can be run to return shapes to their original form. The properties of the proposed algorithm are evaluated {{on the basis of}} computer simulation. Key Words: 2 d allocation, quantization, nesting, simulation...|$|E
40|$|This {{paper is}} a brief {{introduction}} to idempotent and tropical mathematics. Tropical mathematics can {{be treated as a}} result of the so-called Maslov <b>dequantization</b> of the traditional mathematics over numerical fields as the Planck constant ħ tends to zero taking imaginary values. Comment: English and Russian versions. This article, a result of a very considerable expansion of math. GM/ 0501038, is to appear in Journal of Mathematical Sciences (Notes of Scientific Seminars of the V. A. Steklov Mathematical Institute), see. Dedicated to Anatoly Vershik with admiration and gratitude. Russian version to be published in "Zapiski Nauchnykh Seminarov POMI...|$|E
40|$|This paper {{shows that}} an n X 1 integer vector can be exactly {{recovered}} from its Hadamard transform coefficients, even when 0. 5 n log(2) (n) of the (less significant) bits of these coefficients are removed. The paper introduces a fast "lossless" <b>dequantization</b> algorithm for this purpose. To investigate {{the usefulness of}} the procedure in data compression, the paper investigates an embedded block image coding technique called the "LHAD" based on the algorithm. The results show that lossless compression ratios close to {{the state of the art}} can be achieved, but that techniques such as CALIC and S+P still perform better...|$|E
40|$|We study a novel type of {{extensions}} of the Standard Model which include a hard mass term for the U(1) gauge field and, optionally, the additional scalar multiplets spontaneously violating the electric charge conservation. Contrary {{to the case of}} abelian massive electrodynamics, in these theories the massiveness of photon necessarily implies non-conservation (and also <b>dequantization)</b> of the electric charge (even in the absence of spontaneous breakdown of the electromagnetic symmetry). On the other hand, unexpectedly, there exist models with charge non-conservation where it is possible to keep the photon mass zero (at least, at the tree level) ...|$|E
