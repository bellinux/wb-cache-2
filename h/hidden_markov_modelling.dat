60|10000|Public
40|$|This {{document}} {{presents a}} comprehensive approach to <b>Hidden</b> <b>Markov</b> <b>Modelling.</b> Concepts are first introduced in a one-dimensional context. Both model evaluation and training are considered and theoretical developments which lead to algorithms are presented. Major implementation issues are also addressed. Then, results are extended to two-dimensional <b>Hidden</b> <b>Markov</b> <b>Modelling.</b> Again, model evaluation and training are considered and corresponding algorithms sketched. Proofs and detailed justifications of results presented {{can be found in}} the referenced bibliography. This report is the first part of a three-fold document. Part Two [8] details the implementation of the tools presented in this document and part Three [9] specialises in applying these procedures to colour image analysis...|$|E
40|$|This paper {{presents}} a scheme of speaker-independent isolated word recognition in which <b>Hidden</b> <b>Markov</b> <b>Modelling</b> is used with Vector Quantization codebooks constructed using the Expectation-Maximization (EM) algorithm for Gaussian mixture models. In comparison with conventional vector quantization, the EM algorithm results in greater recognition accuracy...|$|E
40|$|In this paper, we {{investigate}} {{the relationship between}} speech trajectories and the hidden Markov model. The speech utterances were transformed into speech feature vectors and the trajectories displayed on a two dimensional space. The hidden Markov models were also displayed on a two dimensional space. By visual examination, {{we think that the}} state of the HMM is related to a sustained sound. Further experiments showed that each state seem to be associated with a distinct phoneme of the utterance. Therefore, the number of states required in the continuous HMM is related to the number of phonemes in the word to be modelled. In the semi-continuous HMM, {{it is also possible that}} the same gaussian probability density function is shared by the same phoneme sound in different semi-continuous HMMs. 1. 0 INTRODUCTION Recently, <b>hidden</b> <b>Markov</b> <b>modelling</b> (HMM) (Rabiner, 1989) has been widely used in many state-of-the-art speech recognition systems. <b>Hidden</b> <b>Markov</b> <b>modelling</b> is a doubly stochastic proce [...] ...|$|E
40|$|This paper {{proposes a}} novel method for {{decoding}} any high-order <b>hidden</b> <b>Markov</b> <b>model.</b> First, the high-order <b>hidden</b> <b>Markov</b> <b>model</b> {{is transformed into}} an equivalent first-order <b>hidden</b> <b>Markov</b> <b>model</b> by Hadar’s transformation. Next, the optimal state sequence of the equivalent first-order <b>hidden</b> <b>Markov</b> <b>model</b> is recognized by the existing Viterbi algorithm of the first-order <b>hidden</b> <b>Markov</b> <b>model.</b> Finally, the optimal state sequence of the high-order <b>hidden</b> <b>Markov</b> <b>model</b> is inferred from the optimal state sequence of the equivalent first-order <b>hidden</b> <b>Markov</b> <b>model.</b> This method provides a unified algorithm framework for decoding <b>hidden</b> <b>Markov</b> <b>models</b> including the first-order <b>hidden</b> <b>Markov</b> <b>model</b> and any high-order <b>hidden</b> <b>Markov</b> <b>model...</b>|$|R
40|$|Abstract. The <b>hidden</b> <b>markov</b> <b>model</b> {{is a kind}} of {{important}} probability model of series data processing and statistical learning and it has been successfully applied in many engineering tasks. This paper introduces the basic principle of <b>hidden</b> <b>markov</b> <b>model</b> firstly, and then discusses the limitations of <b>hidden</b> <b>markov</b> <b>model,</b> as well as the improved <b>hidden</b> <b>markov</b> <b>model</b> which is put forward to solve these problems...|$|R
40|$|Abstract-The <b>hidden</b> <b>markov</b> <b>model</b> {{is a kind}} of {{important}} probability model of series data processing and statistical learning and it has been successfully applied in many engineering tasks. This paper introduces the basic principle of <b>hidden</b> <b>markov</b> <b>model</b> firstly, and then discusses the limitations of <b>hidden</b> <b>markov</b> <b>model,</b> as well as the improved <b>hidden</b> <b>markov</b> <b>model</b> which is put forward to solve these problems. Keywords- HMM, <b>Markov</b> family <b>model,</b> DDBHMM...|$|R
40|$|In {{order to}} {{facilitate}} proper interpretation of long measurements involving large data sets a novel data classifier was implemented and tested. The method is based on <b>Hidden</b> <b>Markov</b> <b>modelling</b> technique, the basic tool in speech recognition. Tests performed used motion data and yielded a activity recognition score of up to 95. 5 ± 1. 9 % {{on a set of}} 8 different human activities...|$|E
40|$|This paper {{presents}} an HMM-based recognition system for perceptive relevant pitch movements of spontaneous German speech. The pitch movements are defined {{according to the}} perceptively and phonetically motivated IPO-approach to intonation. For recognition we use a hybrid approach combining polynomial classification with <b>Hidden</b> <b>Markov</b> <b>Modelling.</b> The recognition is based only on the speech signal, its fundamental frequency and eleven derived features. We evaluate the system on a speaker independant recognition task...|$|E
30|$|It is {{difficult}} for general <b>Hidden</b> <b>Markov</b> <b>modelling</b> methods to discriminate between utterances that are acoustically very similar [43]. Therefore, in the final CALL system we will probably use the following procedure: {{the output of the}} first step is a cluster of similar responses (e.g., according to a phonetically-based distance measure), and a more detailed analysis is carried out in the second (error detection) step to determine what was actually uttered and where to give feedback on.|$|E
40|$|We {{describe}} pair <b>hidden</b> <b>Markov</b> <b>models,</b> with {{an emphasis}} on their relationship to evolutionary <b>models</b> and <b>hidden</b> <b>Markov</b> <b>models.</b> We then explain the statistical interpretation of alignment with pair <b>hidden</b> <b>Markov</b> <b>models,</b> and highlight connections to the Needleman–Wunsch algorithm and other dynamic programming–based alignment algorithms...|$|R
40|$|Abstract: In this paper, the second-order <b>hidden</b> <b>Markov</b> <b>models</b> (HMM 2 s) {{have been}} used to enhance the {{recognition}} performance of isolated-word text-dependent speaker authentication systems under the neutral talking condition. Our results show that HMM 2 s enhance the speaker authentication performance under such a condition compared to the firstorder <b>hidden</b> <b>Markov</b> <b>models</b> (HMM 1 s). The speaker authentication performance has been improved by 6 %. Key words: First-order <b>hidden</b> <b>Markov</b> <b>models,</b> neutral talking condition, second-order <b>hidden</b> <b>Markov</b> <b>models,</b> speaker authentication performance...|$|R
50|$|The layered <b>hidden</b> <b>Markov</b> <b>model</b> (LHMM) is a {{statistical}} model {{derived from the}} <b>hidden</b> <b>Markov</b> <b>model</b> (HMM). A layered <b>hidden</b> <b>Markov</b> <b>model</b> (LHMM) consists of N levels of HMMs, where the HMMs on level i + 1 correspond to observation symbols or probability generators at level i.Every level i of the LHMM consists of Ki HMMs running in parallel.|$|R
40|$|In {{this report}} {{we discuss the}} problem of {{combining}} spatially-distributed predictions from neural networks. An example of this problem is the prediction of a wind vector-eld from remote-sensing data by combining bottom-up predictions (wind vector predictions on a pixel-by-pixel basis) with prior knowledge about wind-eld congurations. This task can be achieved using the scaled-likelihood method, which {{has been used by}} Morgan and Bourlard (1995) and Smyth (1994), in the context of <b>Hidden</b> <b>Markov</b> <b>modelling.</b> Combining Spatially Distributed Predictions From Neural Networks...|$|E
40|$|This paper {{describes}} two speaker verification algorithms, a text-independent method {{based on}} a second order statistical measure and a text-dependent method based on <b>hidden</b> <b>Markov</b> <b>modelling.</b> We investigate the effect of different features, sampling rates, and threshold setting methods and introduce a N-best words pruning method that aims {{to compensate for the}} effect of poorly trained client models. Experimental evaluation is performed on the publicly available XM 2 VTS database according to a published protocol for three different operating points and a priori threshold setting...|$|E
40|$|We present two machine {{learning}} approaches to information extraction from semi-structured documents {{that can be used}} if no annotated training data are available, but there does exist a database filled with information derived from the type of documents to be processed. One approach employs standard supervised learning for information extraction by artificially constructing labelled training data from the contents of the database. The second approach combines unsupervised <b>Hidden</b> <b>Markov</b> <b>modelling</b> with language models. Empirical evaluation of both systems suggests {{that it is possible to}} bootstrap a field segmenter from a database alone. The combination of Hidden Markov and language modelling was found to perform best at this task. ...|$|E
40|$|Hidden <b>Markov</b> <b>models</b> are {{extensions}} of <b>Markov</b> <b>models</b> where each observation {{is the result}} of a stochastic process in one of several unobserved states. Though favored by many scientists because of its unique and applicable mathematical structure, its independence assumption between the consecutive observations hampered further application. Autoregressive <b>hidden</b> <b>Markov</b> <b>model</b> is a combination of autoregressive time series and <b>hidden</b> <b>Markov</b> chains. Observations are generated by a few autoregressive time series while the switches between each autoregressive time series are controlled by a <b>hidden</b> <b>Markov</b> chain. In this thesis, we present the basic concepts, theory and associated approaches and algorithms for <b>hidden</b> <b>Markov</b> <b>models,</b> time series and autoregressive <b>hidden</b> <b>Markov</b> <b>models.</b> We have also built a bivariate autoregressive <b>hidden</b> <b>Markov</b> <b>model</b> on the temperature data from the Pacific Ocean to understand the mechanism of El Nino. The parameters and the state path of the model are estimated through the Segmental K-mean algorithm and the state estimations of the autoregressive <b>hidden</b> <b>Markov</b> <b>model</b> have been compared with the estimations from a conventional <b>hidden</b> <b>Markov</b> <b>model.</b> Overall, the results confirm the strength of the autoregressive <b>hidden</b> <b>Markov</b> <b>models</b> in the El Nino study and the research sets an example of ARHMM's application in the meteorology...|$|R
40|$|An on-line {{unsupervised}} algorithm for {{estimating the}} <b>hidden</b> <b>Markov</b> <b>models</b> (HMM) parame-ters is presented. The problem of <b>hidden</b> <b>Markov</b> <b>models</b> adaptation to emotional speech is solved. To increase {{the reliability of}} estimated HMM parameters, a mechanism of forgetting and updating is proposed. A functional block diagram of the <b>hidden</b> <b>Markov</b> <b>models</b> adaptation algorithm is also provided with obtained results, which improve the efficiency of emotional speech recognition...|$|R
40|$|The biggest {{difficulty}} of <b>hidden</b> <b>Markov</b> <b>model</b> applied to multistep attack is {{the determination of}} observations. Now the research of the determination of observations is still lacking, and it shows {{a certain degree of}} subjectivity. In this regard, we integrate the attack intentions and <b>hidden</b> <b>Markov</b> <b>model</b> (HMM) and support a method to forecasting multistep attack based on <b>hidden</b> <b>Markov</b> <b>model.</b> Firstly, we train the existing <b>hidden</b> <b>Markov</b> <b>model(s)</b> by the Baum-Welch algorithm of HMM. Then we recognize the alert belonging to attack scenarios with the Forward algorithm of HMM. Finally, we forecast the next possible attack sequence with the Viterbi algorithm of HMM. The results of simulation experiments show that the <b>hidden</b> <b>Markov</b> <b>models</b> which have been trained are better than the untrained in recognition and prediction...|$|R
40|$|By mapping {{acoustic}} parameters onto phonetic features, it {{is possible}} to explicitly address the linguistic information in the signal. For the experiments presented in this paper, we mapped cepstral parameters onto two sets of phonetic features, one based on the IPA chart and the other on SPE. As a result, the phoneme identification rates in a <b>hidden</b> <b>Markov</b> <b>modelling</b> framework increase from 15. 6 % for the cepstral parameters to 42. 3 % and 31. 7 % for the IPA and SPE features, respectively. Furthermore, for phonetic features the resulting confusions between phonemes are often less severe from a phonetic point of view. The theoretical implications of the differences are addressed. 1...|$|E
40|$|In this paper, {{by using}} the {{formulation}} of the missing-data problem, a general framework for statistical acoustic modelling of speech is presented. With the motivation of utilizing bi-directional contextual dependence in acoustic modelling, a bi-directional <b>hidden</b> <b>Markov</b> <b>modelling</b> approach for speech recognition is studied {{and the importance of}} the bi-directional contextual dependence for speech recognition is identified by a series of comparative experiments. Furthermore, hidden Markov random field (MRF) -based acoustic modelling techniques using our previously proposed contextual vector quantization (CVQ) method and iterated conditional modes (ICM) algorithm, which is very suitable for parallel processing implementation, are also attempted. Their viability is confirmed by a series of preliminary experiments in a speaker-independent isolated English letter recognition task...|$|E
40|$|The {{basic problem}} of target motion {{analysis}} (TMA) is {{to estimate the}} trajectory of an object (Le. position and velocity for a rectilinear movement) from noise corrupted sensor data. The problem is quite easy for a rectilinear movement of the source, but numerous problem arise when it maneuvers. Our main goal is to apply <b>hidden</b> <b>markov</b> <b>modelling</b> to this, and to optimize the estimability of the source trajectory via the observer motion. 1. INTRODUCTION. The basic problem of target motion analysis (TMA) is to estimate the trajectory of an object (i. e. position and velocity for a rectilinear movement) from noise corrupted sensor data. The problem is not new and numerous approache...|$|E
40|$|Master of ScienceDepartment of Computing and Information SciencesDavid A. GustafsonThe use of <b>hidden</b> <b>Markov</b> <b>models</b> in autism {{recognition}} and analysis is investigated. More speciﬁcally, {{we would like}} to be able to determine a person's level of autism (AS, HFA, MFA, LFA) using <b>hidden</b> <b>Markov</b> <b>models</b> trained on observations taken from a subject's behavior in an experiment. A preliminary model is described that includes the three mental states self-absorbed, attentive, and join-attentive. Futhermore, observations are included that are more or less indicative of each of these states. Two experiments are described, the ﬁrst on a single subject and the second on two subjects. Data was collected from one individual in the second experiment and observations were prepared for input to <b>hidden</b> <b>Markov</b> <b>models</b> and the resulting <b>hidden</b> <b>Markov</b> <b>models</b> were studied. Several questions subsequently arose and tests, written in Java using the JaHMM <b>hidden</b> <b>Markov</b> <b>model</b> tool- kit, were conducted to learn more about the <b>hidden</b> <b>Markov</b> <b>models</b> being used as autism recognizers and the training algorithms being used to train them. The tests are described along with the corresponding results and implications. Finally, suggestions are made for future work. It turns out that we aren't yet able to produce <b>hidden</b> <b>Markov</b> <b>models</b> that are indicative of a persons level of autism and the problems encountered are discussed and the suggested future work is intended to further investigate the use of <b>hidden</b> <b>Markov</b> <b>models</b> in autism recognition...|$|R
40|$|AbstractThe basic {{theory of}} <b>hidden</b> <b>Markov</b> <b>models</b> was {{developed}} and applied to problems in speech recognition in the late 1960 s, and has since then been applied to numerous problems, e. g. biological sequence analysis. Most applications of <b>hidden</b> <b>Markov</b> <b>models</b> are based on efficient algorithms for computing the probability of generating a given string, or computing the most likely path generating a given string. In this paper we consider the problem of computing the most likely string, or consensus string, generated by a given model, and its implications on the complexity of comparing <b>hidden</b> <b>Markov</b> <b>models.</b> We show that computing the consensus string, and approximating its probability within any constant factor, is NP-hard, and that the same holds for the closely related labeling problem for class <b>hidden</b> <b>Markov</b> <b>models.</b> Furthermore, we establish the NP-hardness of comparing two <b>hidden</b> <b>Markov</b> <b>models</b> under the L∞- and L 1 -norms. We discuss {{the applicability of the}} technique used for proving the hardness of comparing two <b>hidden</b> <b>Markov</b> <b>models</b> under the L 1 -norm to other measures of distance between probability distributions. In particular, we show that it cannot be used for proving NP-hardness of determining the Kullback–Leibler distance between two <b>hidden</b> <b>Markov</b> <b>models,</b> or of comparing them under the Lk-norm for any fixed even integer k...|$|R
40|$|This paper reviews recent {{advances}} in Bayesian nonparametric techniques for constructing and performing inference in infinite <b>hidden</b> <b>Markov</b> <b>models.</b> We focus on variants of Bayesian nonparametric <b>hidden</b> <b>Markov</b> <b>models</b> that enhance a posteriori state-persistence in particular. This paper also introduces a new Bayesian nonparametric framework for generating left-to-right and other structured, explicit-duration infinite <b>hidden</b> <b>Markov</b> <b>models</b> that we call the infinite structured hidden semi-Markov model. Comment: 23 pages, 10 figure...|$|R
40|$|Speech dynamic feature are {{routinely}} used in current speech recognition systems {{in combination with}} short-term (static) spectral features. The aim {{of this paper is}} to propose a method to automatically estimate the optimum ponderation of static and dynamic features in a speech recognition system. The recognition system considered in this paper is based on Continuous-Density <b>Hidden</b> <b>Markov</b> <b>Modelling</b> (CDHMM) widely used in speech recognition. Our approach consists basically in 1) adding two new parameters for each state of each model that weight both kinds of speech features, and 2) estimating those parameters by means of a Maximum Likelihood training. Experimental results in speaker independent digit recognition show an important increase of recognition accuracy. Peer ReviewedPostprint (published version...|$|E
40|$|A Doctoral Thesis. Submitted in partial {{fulfilment}} of {{the requirements}} for the award of Doctor of Philosophy of Loughborough University. Recognition of printed and hand printed characters has received much attention {{over the past decade}} as the need for automated 'document entry' systems assumes a commanding role in office automation. Although, present Optical Character Recognition(OCR) systems have reached a high degree of sophistication as compared to early systems, the design of a robust system which can separate text from images accurately and cope reliably with noisy input and frequent change of font is a formidable task. In this thesis, a novel method of character recognition based on <b>Hidden</b> <b>Markov</b> <b>Modelling</b> (HMM) is initially described. The scheme first describes a training set of characters by their outer contours using Freeman codes; next, the HMM method is applied to capture topological variation of the characters automatically, by looking at typical samples of the different characters. Fonts of similar topology can also be incorporated in one hidden Markov model. Once the model of a character in upright position is derived, the character can be recognized, even, when it has been rotated by multiples of 90 degrees. This technique is further extended to combine structural analysis/description of characters with <b>hidden</b> <b>Markov</b> <b>modelling.</b> In this scheme, a character is first skeletonized and then split to primitives; each primitive is described by hidden Markov models while its Corresponding position with respect to nodes(junctions) where the primitives meet, are recorded. This scheme is virtually font and size independent. A new document classification algorithm based on Fuzzy theory is also proposed which provides an indication of a document's contents in terms of 'text' and 'nontext' portions...|$|E
3000|$|Cognitive radio {{technologies}} {{are being developed}} which allow heterogeneous systems to share spectrum access while minimizing interference to improve the overall efficiency of spectrum usage. Thus, one important function of a cognitive radio is dynamically to avoid transmitting in occupied spectrum by detecting signals received from unknown competing systems. Robust operation requires the detection of multiple wideband interferers of unknown statistics from a single received sample vector. This paper describes the hypothesis tests that must be evaluated to perform detection of such signals and discusses several methods for performing detection. Computer simulation results are presented to show that <b>hidden</b> <b>Markov</b> <b>modelling</b> and power spectral analysis with edge enhancement are more robust than a simple [...] "interference temperature-" [...] based energy detector.|$|E
40|$|Automatic {{detection}} of semantic events in sport videos is a challenging task. In this paper, we propose a multimodal multilayer statistical inference framework for semantic sports video analysis using Dynamic Bayesian Networks (DBNs). Based on this framework, three instances including factorial hierarchical <b>hidden</b> <b>Markov</b> <b>model</b> (FHHMM), coupled hierarchical <b>hidden</b> <b>Markov</b> <b>model</b> (CHHMM), and product hierarchical <b>hidden</b> <b>Markov</b> <b>model</b> (PHHMM), are constructed and compared. Play-break detection in soccer videos {{is used as}} a testbed with hierarchical <b>hidden</b> <b>Markov</b> <b>model</b> (HHMM) as a baseline. Experimental results indicate the superior capability of the PHHMM, because it not only effectively models dynamic interactions between different modalities, but also sufficiently utilizes context constraints in multilayer structures...|$|R
40|$|In this paper, {{we study}} {{the problem of}} {{learning}} phylogenies and <b>hidden</b> <b>Markov</b> <b>models.</b> We call a <b>Markov</b> <b>model</b> nonsingular if all transition matrices have determinants bounded away from 0 (and 1). We highlight {{the role of the}} nonsingularity condition for the learning problem. Learning <b>hidden</b> <b>Markov</b> <b>models</b> without the nonsingularity condition is at least as hard as learning parity with noise. On the other hand, we give a polynomial-time algorithm for learning nonsingular phylogenies and <b>hidden</b> <b>Markov</b> <b>models...</b>|$|R
40|$|In {{the current}} thesis several {{selected}} {{aspects of the}} two related latent class models; finite mixtures and <b>hidden</b> <b>Markov</b> <b>models,</b> were considered. The problem of calculating the MLE of a Gaussian mixture with Newton's method and the consistency of penalized MLE were investigated. A penalized MLE procedure for univariate Gaussian <b>hidden</b> <b>Markov</b> <b>models</b> was introduced and shown to be consistent. Furthermore, a result on identifiability of nonparametric <b>hidden</b> <b>Markov</b> <b>models</b> is derived...|$|R
40|$|In today’s working {{world the}} elderly are often {{classified}} {{as a set of}} dependent people and are sometimes neglected by society. One of the ways to determine whether an elderly person is safe in their home is to find out what activities an elderly person is carrying out and give appropriate assistance or institute safeguards. This paper describes the lower tier of a two tiered approach that is being adopted. The higher tier consists of hierarchical sets of plans that model common goals and sub-goals associated with activities in daily life. The lower tier deals with recognition of tasks from the stream of sensor events. Tasks are the lowest level component of a plan. The tasks are modelled using a form of <b>hidden</b> <b>Markov</b> <b>modelling...</b>|$|E
40|$|This paper {{addresses}} {{the problem of}} automatic speech recognition {{in the presence of}} interfering signals and noise with statistical characteristics ranging from stationary to fast changing and impulsive. A technique of signal decomposition using hidden Markov models, [lj, is described. This is a generalisation of conventional <b>hidden</b> <b>Markov</b> <b>modelling</b> that provides an optimal method of decomposing simultaneous processes. The technique exploits the ability of hidden Markov models to model dynamically varying signals in order to accomodate concurrent processes, including interfering signals as complex as speech. This form of signal decomposition has wide implications for signal separation in general and improved speech modelling in particular. However. this paper concentrates on the application of decomposition to the problem of recognition of speech contaminated with noise. ...|$|E
40|$|We report {{results of}} a {{numerical}} analysis of the memory effects in two-dimensional Abelian sandpiles. It is found that a sandpile forgets its instantaneous configuration in two distinct stages: a fast stage and a slow stage, whose durations roughly scale as NN and N 2 N 2 respectively, where NN is the linear size of the sandpile. We confirm {{the presence of the}} longer time-scale by an independent diagnostic based on analysing emission probabilities of a hidden Markov model applied to a time-averaged sequence of avalanche sizes. The application of <b>hidden</b> <b>Markov</b> <b>modelling</b> to the output of sandpiles is novel. It discriminates effectively between a sandpile time series and a shuffled control time series with the same time-averaged event statistics and hence deserves further development as a pattern-recognition tool for Abelian sandpiles...|$|E
40|$|Background: Baum-Welch {{training}} is an expectation-maximisation algorithm for training the emission and transition probabilities of <b>hidden</b> <b>Markov</b> <b>models.</b> Until know, no memory-efficient algorithm was available. Methods and results: We introduce a linear space algorithm for Baum-Welch training. For a <b>hidden</b> <b>Markov</b> <b>model</b> with M states, an input sequence of length L and |T | transition and |E | emission parameters, it requires O(M) memory and O(L(|T | + |E|) |T |) time rather than O(LM) memory and O(L|T | + L(|T | + |E|)) time. For a pair <b>hidden</b> <b>Markov</b> <b>model</b> with input sequences Lx and Ly, {{the requirement of}} O(LxLyM) memory and O(LxLy|T | + LxLy(|T | + |E|)) time is reduced to O(min{Lx, Ly}M) memory and O(LxLy(|T | + |E|) |T |) time. For <b>hidden</b> <b>Markov</b> <b>models</b> used on long sequences, for example in gene finding, this means a huge memory saving compared to the best available current algorithms which need O (√ LM) (<b>hidden</b> <b>Markov</b> <b>models)</b> or O (√ LxLyM) (pair <b>hidden</b> <b>Markov</b> <b>models)</b> memory. Additionally, our new algorithm is very simple {{in the sense that}} its implementation does not require sophisticated programming techniques like checkpoints or recursive functions. Conclusions: Using our algorithm, Baum-Welch training can now be performed on standard personal computers, even for sophisticated pair <b>hidden</b> <b>Markov</b> <b>models</b> and very long biological sequences. ...|$|R
40|$|This {{paper is}} an {{exploration}} of <b>hidden</b> <b>Markov</b> <b>models</b> and their applications. After a brief discussion of ordinary <b>Markov</b> <b>models</b> (<b>Markov</b> chains) {{and some of their}} applications in mathematics, I will focus on <b>hidden</b> <b>Markov</b> <b>models.</b> I will explain several algorithms which are used in conjunction with <b>hidden</b> <b>Markov</b> models: the Viterbi algorithm, the forward algorithm, and the backward algorithm. I will also discuss the estimation of parameters of a <b>hidden</b> <b>Markov</b> <b>model.</b> The main applications this paper examines are DNA sequence analysis, speech recognition, and protein family alignment and sequencing...|$|R
40|$|We {{propose a}} dynamic {{graphical}} model which generalizes nonhomogeneous <b>hidden</b> <b>Markov</b> <b>models.</b> Inference and forecast procedures are developed. A comparison with an exact propagation algorithm is established and equivalence is stated. Graphical <b>models</b> Nonhomogeneous <b>hidden</b> <b>Markov</b> <b>models</b> Inference and forecast procedures...|$|R
