287|165|Public
5000|$|Peterson's {{algorithm}} is the step 2 of the generalized BCH <b>decoding</b> <b>procedure.</b> Peterson's {{algorithm is}} used to calculate the error locator polynomial coefficients [...] of a polynomial ...|$|E
50|$|It was a 4/2/4 {{photographic}} sound encoding and <b>decoding</b> <b>procedure</b> {{compatible with}} (and {{using the same}} technical basic structure, with identical sound quality as) its competitor Dolby Stereo Matrix.|$|E
5000|$|This <b>decoding</b> <b>procedure</b> {{works because}} A [...] A = 0 for any bit string A. After d &minus; 1 {{distinct}} blocks have been exclusive-ored into {{a packet of}} degree d, the original unencoded content of the unmatched block is all that remains. In symbols we have ...|$|E
40|$|Geometric {{perspectives}} of test-based decoding acceptance criteria are developed. Test-based <b>decoding</b> <b>procedures</b> employ simple algebraic tests conducted on candidate codewords, efficiently generated by low-complexity sub-optimal decoding strategies, and channel reliability measures. Test-based <b>decoding</b> <b>procedures</b> typically yield significant performance improvements over sub-optimal <b>decoding</b> <b>procedures</b> while maintaining low average complexity. Specific contributions are: (1) A geometric {{interpretation of a}} single codeword acceptance criterion for binary codes transmitted over a coherent additive white Gaussian noise channel provides the methodology to evaluate the performance of test-based decoding. (2) Test-based <b>decoding</b> <b>procedures</b> and acceptance criterion performance bounds are extended to other memoryless channels, the random phase and the Rayleigh fast fading channels. (3) Test-based <b>decoding</b> <b>procedures</b> and acceptance criterion performance bounds are extended to non-binary codes. (4) Single codeword test-based <b>decoding</b> <b>procedures</b> and acceptance criterion performance bounds are extended to multi-codeword test-based decoding, with specific emphasis placed on the two-codeword case. ...|$|R
3000|$|... [...]. The <b>decoding</b> <b>procedures</b> at {{the other}} borders can be {{obtained}} by exploiting the symmetry of the system.|$|R
3000|$|... {{parallel}} SISO channels. This way {{the analog}} JSCC encoding and <b>decoding</b> <b>procedures</b> {{described in the}} previous section can be applied.|$|R
5000|$|In 1960, Peterson {{developed}} an algorithm for decoding BCH codes. His algorithm solves the important {{second stage of}} the generalized BCH <b>decoding</b> <b>procedure</b> and {{is used to calculate}} the error locator polynomial coefficients that in turn provide the error locator polynomial. This is crucial to the decoding of BCH codes.|$|E
5000|$|Notice that non-malleability is {{a weaker}} {{guarantee}} than error correction/detection; the latter ensure that {{any change in}} the code-word can be corrected or at least detected by the <b>decoding</b> <b>procedure,</b> whereas the former does allow the message to be modified, but only to an unrelated value. However, when studying error correction/detection we usually restrict ourselves to limited forms of tampering which preserve some notion of distance (e.g., usually hamming distance) between the original and tampered code-word. For example, it is already impossible to achieve error correction/detection for the simple family of functions [...] which, for every constant , includes a [...] "constant" [...] function [...] that maps all inputs to [...] There is always some function in [...] that maps everything to a valid code-word [...] In contrast, it is trivial to construct codes that are non-malleable w.r.t , as the output of a constant function is clearly independent of its input. The prior works on non-malleable codes show that one can construct non-malleable codes for highly complex tampering function families [...] for which error correction/detection can not be achievable.|$|E
5000|$|... {{described}} a theoretical decoder that corrected errors by finding {{the most popular}} message polynomial. The decoder only knows the set of values [...] to [...] and which encoding method was used to generate the codeword's sequence of values. The original message, the polynomial, and any errors are unknown. A <b>decoding</b> <b>procedure</b> could use a method like Lagrange interpolation on various subsets of n codeword values taken k {{at a time to}} repeatedly produce potential polynomials, until a sufficient number of matching polynomials are produced to reasonably eliminate any errors in the received codeword. Once a polynomial is determined, then any errors in the codeword can be corrected, by recalculating the corresponding codeword values. Unfortunately, in all but the simplest of cases, there are too many subsets, so the algorithm is impractical. The number of subsets is the binomial coefficient, , and the number of subsets is infeasible for even modest codes. For a [...] code that can correct 3 errors, the naive theoretical decoder would examine 359 billion subsets. Practical decoding involved changing the view of codewords to be a sequence of coefficients as explained in the next section.|$|E
30|$|To further {{improve the}} PAPR {{reduction}} {{performance of the}} ECM method, {{it is possible to}} increase the value of the SLM parameter U since the computational complexity of ECM-based data <b>decoding</b> <b>procedures</b> is independent of the value of U.|$|R
40|$|In this paper, a VLSI Reed-Solomon (RS) decoder {{architecture}} for concatenate-coded {{space and}} spread spectrum communications, is presented. The known <b>decoding</b> <b>procedures</b> for RS codes are exploited and modified {{to obtain a}} repetitive and recursive decoding technique which is suitable for VLSI implementation and pipeline processing...|$|R
40|$|Results are {{presented}} on families of balanced binary error-correcting codes that extend {{those in the}} literature. The idea is to consider balanced blocks as symbols over an alphabet and to construct error-correcting codes over that alphabet. Encoding and <b>decoding</b> <b>procedures</b> {{are presented}}. Several improvements to the general construction are discussed...|$|R
5000|$|Flowers' {{first contact}} with the wartime codebreaking effort came in February 1941 when his director, W. Gordon Radley was asked for help by Alan Turing, who was then working at the government's Bletchley Park codebreaking {{establishment}} 50 miles north west of London in Buckinghamshire. Turing wanted Flowers to build a decoder for the relay-based Bombe machine, which Turing had developed to help decrypt the Germans' Enigma codes. Although the decoder project was abandoned, Turing was impressed with Flowers's work, and in February 1943 introduced him to Max Newman who was leading the effort to automate part of the cryptanalysis of the Lorenz cipher. This was a high-level German cipher generated by a teletypewriter in-line cipher machine, the SZ40/42, one of their [...] "Geheimschreiber" [...] (secret writer) systems, that was called [...] "Tunny" [...] (tunafish) by the British. It was a much more complex system than Enigma; the <b>decoding</b> <b>procedure</b> involved trying so many possibilities that it was impractical to do by hand. Flowers and Frank Morrell (also at Dollis Hill) designed the Heath Robinson, the first machine developed {{in an attempt to}} automate the cryptanalysis of the Lorenz SZ-40/42 cipher machine, called Tunny (for tunafish) by those working at Bletchley Park.|$|E
30|$|Next, we give {{an example}} on this four-step <b>decoding</b> <b>procedure.</b>|$|E
40|$|This paper {{presents}} a tunable <b>decoding</b> <b>procedure</b> for Genetic Algorithm (GA) based schedule optimization. Tuning the <b>decoding</b> <b>procedure</b> of the algorithm scales {{the size of}} the search space of the underlying optimization problem. We showby experiment, that a tradeo exists between the advantage of searching a small space and the impacts of excluding near-optimal solutions from being searched at the same time. Therebywereveal the weakness of GAs when coping with large and di cult search spaces. Finally we discuss the de ciencies observed by tuning the <b>decoding</b> <b>procedure</b> auto-adaptively and leave this matter as an open issue. ...|$|E
30|$|Simulations {{were carried}} out to {{evaluate}} and compare the PAPR reduction capability and BER performance in a fading channel between the conventional SLM-OFDM system and the ECM method. Simulations also evaluate and compare the computational complexity between ECM data <b>decoding</b> <b>procedures</b> and the conventional SLM-OFDM receiver that uses the FDC-based SI estimation scheme.|$|R
40|$|An {{algorithm}} for cryptanalysis {{of certain}} keystream generators is proposed. The developed algorithm has {{the following two}} advantages over other reported ones: (i) it is more powerful and (ii) it provides a high-speed software implementation, {{as well as a}} simple hardware one, suitable for high parallel architectures. The novel algorithm is a method for the fast correlation attack with significantly better performance than other reported methods, assuming a lower complexity and the same inputs. The algorithm is based on <b>decoding</b> <b>procedures</b> of the corresponding binary block code with novel constructions of the paritychecks, and the following two decoding approaches are employed: the a posterior probability based threshold decoding and the belief propagation based bit-flipping iterative <b>decoding.</b> These <b>decoding</b> <b>procedures</b> offer good trade-offs between the required sample length, overall complexity and performance. The novel algorithm is compared with recently proposed impro [...] ...|$|R
40|$|Recursive <b>decoding</b> <b>procedures</b> {{minimizing}} the bit error probability for binary block and convolutional codes, {{which have the}} same complexity as the maximum likelihood decoding, are derived. The key idea of the approach to decoding problem is to represent the codewords as boundary points of the space consisting of the real-valued vectors, whose components are probabilities...|$|R
30|$|The {{encoding}} and <b>decoding</b> <b>procedure</b> {{is based}} on block Markov techniques.|$|E
3000|$|... [...]. We use the <b>decoding</b> <b>procedure</b> {{which follows}} the same {{principle}} as of the primitive binary BCH code.|$|E
30|$|Decoding: The <b>decoding</b> <b>procedure</b> at Rx 1 {{is similar}} to Theorem 1 and the error in this {{receiver}} can be bounded if (3)-(6) hold.|$|E
40|$|A {{method for}} {{encoding}} and decoding spectrum shaped binary run-length constrained sequences is described. The binary sequences with predefined range of exponential sums are introduced. On {{the base of}} Cover's enumerative scheme, recurrence relations for calculating the number of these sequences are derived. Implementation of encoding and <b>decoding</b> <b>procedures</b> is also shown. Comment: 5 pages, 1 figure, 2 tables, submitted to ISIT 201...|$|R
30|$|Chroma {{components}} coding In {{all three}} architectures including their enhancements, {{there is no}} mentioning of encoding and <b>decoding</b> <b>procedures</b> for Chroma component. Moreover, the performance comparisons have been done only for Luma components. Considering the significance of Luma component, more details on Luma coding in terms of compression mechanism {{as well as its}} impact on overall performance, should be provided.|$|R
3000|$|... [k] of source B decoded {{first at}} the destination. The {{analysis}} of the remaining three cases (obtained respectively from case 2, case 3, and case 4 by swapping {{the roles of the}} sources) can be obtained straightforwardly by symmetry. For each of the four cases that will be analyzed, we first describe the <b>decoding</b> <b>procedures</b> at the relay and the destination and then analyze the achievable sum-rate.|$|R
3000|$|... 1 Here we ambiguously use the {{terminology}} partition, since every two adjacent cells overlap with their common endpoints. But {{this does not}} harm the <b>decoding</b> <b>procedure.</b>|$|E
3000|$|... [...]. The entire <b>decoding</b> <b>procedure</b> {{is listed}} in Algorithm 1. In Algorithm 1, Dist{·} is the Hamming {{distance}} and s(i:j) means from the i th sample to the j th sample in s.|$|E
30|$|Watermarked {{image is}} divided into the cross set and the dot set. <b>Decoding</b> <b>procedure</b> {{proceeds}} in the inverse order of the embedding procedure. In other words, dot set decoding proceeds first and cross set second.|$|E
40|$|Minimum risk {{estimation}} and decoding strategies based on lattice segmentation techniques {{can be used}} to refine large vocabulary continuous speech recognition systems through the estimation of the parameters of the underlying hidden Mark models and through the identification of smaller recognition tasks which provides the opportunity to incorporate novel modeling and <b>decoding</b> <b>procedures</b> in LVCSR. These techniques are discussed in the context of going ‘beyond HMMs’...|$|R
40|$|Codes with decomposable {{structure}} {{allow the}} use of multistage <b>decoding</b> <b>procedures</b> to achieve suboptimum bounded-distance error performance with reduced decoding complexity. This correspondence presents some new decomposable codes, including a class of distance- 8 codes, that are constructed based on the|a + x|b + x|a + b + x| construction method. Some existing best codes are shown to be decomposable and hence can be decoded with multistage decoding...|$|R
40|$|Abstract. An {{algorithm}} for cryptanalysis {{of certain}} keystream generators is proposed. The developed algorithm has {{the following two}} advantages over other reported ones: (i) it is more powerful and (ii) it provides a high-speed software implementation, {{as well as a}} simple hardware one, suitable for high parallel architectures. The novel algorithm is a method for the fast correlation attack with significantly better performance than other reported methods, assuming a lower complexity and the same inputs. The algorithm is based on <b>decoding</b> <b>procedures</b> of the corresponding binary block code with novel constructions of the paritychecks, and the following two decoding approaches are employed: the a posterior probability based threshold decoding and the belief propagation based bit-flipping iterative <b>decoding.</b> These <b>decoding</b> <b>procedures</b> offer good trade-offs between the required sample length, overall complexity and performance. The novel algorithm is compared with recently proposed improved fast correlation attacks based on convolutional codes and turbo decoding. The underlying principles, performance and complexity are compared, and the gain obtained with the novel approach is pointed out...|$|R
3000|$|We are {{now ready}} {{to prove that the}} {{destinations}} can correctly decode source symbols. We present the <b>decoding</b> <b>procedure</b> for nodes on the right border of the network, that is, for nodes of type [...]...|$|E
30|$|Fitness {{calculation}} All genotypes {{should be}} decoded as phenotypes {{according to the}} <b>decoding</b> <b>procedure</b> before fitness calculation. The fitness of each individual means the completion time that is calculated according to the fitness function (1).|$|E
3000|$|Although the {{proposed}} JID scheme {{has the advantage}} of enhanced reliability/security performances, this is achieved from an extra complexity/decoding delay. Basically, extra decoding complexity is needed since the FF <b>decoding</b> <b>procedure</b> is performed l [...]...|$|E
30|$|The {{organization}} of the paper is as follows. In Section 2, the considered error-correcting codes are described. In Section 3, the various <b>decoding</b> <b>procedures</b> are presented and expressions for computing their complexities are provided. The performance of these schemes is then assessed in Section 4, also {{taking into account the}} quantization issues. Section 5 is devoted to the analysis of the impact of limited memory and to the latency evaluation. Concluding remarks are given in Section 6.|$|R
40|$|International Telemetering Conference Proceedings / October 29 -November 02, 1990 / Riviera Hotel and Convention Center, Las Vegas, NevadaThe paper {{analyses}} {{the properties}} of m-sequence error-correcting codes when adapting the correlation detection decoding method, deduces the error-tolerant number formula of binary sequence with a good auto-correlation property being used as error-correcting codes, provides with a method {{to increase the efficiency}} of the m-sequence error-correcting codes and make its coding and <b>decoding</b> <b>procedures</b> in the form of framed figures...|$|R
40|$|In this paper, we {{introduce}} a simple technique for incorporating domain information into a {{statistical machine translation}} system that significantly improves translation quality when test data comes from multiple domains. Our approach augments (conjoins) standard translation model and language model features with domain indicator features and requires only minimal modifications to the optimization and <b>decoding</b> <b>procedures.</b> We evaluate our method on two language pairs with varying numbers of domains, and observe significant improvements of up to 1. 0 BLE...|$|R
