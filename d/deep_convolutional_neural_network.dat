504|10000|Public
50|$|Competing {{with top}} human {{players in the}} ancient game of Go has been a {{long-term}} goal of artificial intelligence. Go’s high branching factor makes traditional search techniques ineffective, even on cutting-edge hardware, and Go’s evaluation function could change drastically with one stone change. However, by using a <b>Deep</b> <b>Convolutional</b> <b>Neural</b> <b>Network</b> designed for long-term predictions, Darkforest {{has been able to}} substantially improve the win rate for bots over more traditional Monte Carlo Tree Search based approaches.|$|E
50|$|Related to {{multi-task}} {{learning is}} the concept of knowledge transfer. Whereas traditional multi-task learning implies that a shared representation is developed concurrently across tasks, transfer of knowledge implies a sequentially shared representation. Large scale machine learning projects such as the <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network</b> GoogLeNet, an image-based object classifier, can develop robust representations which may be useful to further algorithms learning related tasks. For example, the pre-trained model {{can be used as a}} feature extractor to perform pre-processing for another learning algorithm. Or the pre-trained model can be used to initialize a model with similar architecture which is then fine-tuned to learn a different classification task.|$|E
50|$|Since around 2013, as mentioned, Google DeepMind showed very {{impressive}} learning results {{in learning to}} play TV games and game of Go (AlphaGo). They used a <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network</b> that has shown superior results in image recognition. They also used 4 frames of almost raw RGB pixels (84x84) as inputs of the network, and the network was trained based on reinforcement learning with the reward representing {{the sign of the}} change in the game score. All the 49 games could be learned using the same network architecture and Q-learning with the minimal prior knowledge, and it outperformed competing methods in almost all the games and performed at a level that is broadly comparable with or superior to a professional human game tester in the majority of games. It is sometimes called DQN (Deep-Q network). In AlphaGo, deep neural networks are trained not only by reinforcement learning, but also by supervised learning. It was also combined with Monte Carlo tree search.|$|E
5000|$|Waifu2x {{is officially}} {{described}} as [...] "Image Super-Resolution for Anime-styled art using <b>Deep</b> <b>Convolutional</b> <b>Neural</b> <b>Networks.</b> It also supports photos."The demo application {{can be found}} at http://waifu2x.udp.jp/ ...|$|R
3000|$|... where z and a are {{the input}} and output of {{activation}} function, respectively. Experiments in [41] show that <b>deep</b> <b>convolutional</b> <b>neural</b> <b>networks</b> with ReLUs train several times faster than their equivalents with tanh units.|$|R
40|$|This paper {{addresses}} {{the challenge of}} establishing a bridge between <b>deep</b> <b>convolutional</b> <b>neural</b> <b>networks</b> and conventional object detection frameworks for accurate and efficient generic object detection. We introduce Dense Neural Patterns, short for DNPs, which are dense local features derived from discriminatively trained <b>deep</b> <b>convolutional</b> <b>neural</b> <b>networks.</b> DNPs can be easily plugged into conventional detection frameworks {{in the same way}} as other dense local features(like HOG or LBP). The effectiveness of the proposed approach is demonstrated with the Regionlets object detection framework. It achieved 46. 1 % mean average precision on the PASCAL VOC 2007 dataset, and 44. 1 % on the PASCAL VOC 2010 dataset, which dramatically improves the original Regionlets approach without DNPs...|$|R
5000|$|WaveNet {{is a type}} of {{feed-forward}} artificial {{neural network}} known as a <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network.</b> These consist of layers of interconnected nodes similar to the brain’s neurons. The CNN takes a raw signal as an input and synthesises an output one sample at a time. [...] In the 2016 paper, the network was fed real waveforms of English and Mandarin speech. As these pass through the network, it learns a set of rules to describe how the audio waveform evolves over time. The trained network can then be used to create new speech-like waveforms from scratch at 16,000 samples per second. These waveforms include realistic breaths and lip smacks - but do not conform to any language. [...] WaveNet is able to accurately model different voices, with the accent and tone of the input correlating with the output. For example, if it is trained on with German, it produces German speech. This ability to clone voices has raised ethical concerns about WaveNets ability to mimic anyone’s voice. [...] The capability also means that if the WaveNet is fed other inputs - such as music - its output will be musical. At the time of its release, DeepMind showed that WaveNet could produce classical sounding music.|$|E
30|$|We {{propose a}} <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network</b> that can {{identify}} Alzheimer’s disease and classify the current disease stage.|$|E
40|$|In this paper, {{we propose}} a {{deep neural network}} {{architecture}} for object recognition based on recurrent neural networks. The proposed network, called ReNet, replaces the ubiquitous convolution+pooling layer of the <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network</b> with four recurrent neural networks that sweep horizontally and vertically in both directions across the image. We evaluate the proposed ReNet on three widely-used benchmark datasets; MNIST, CIFAR- 10 and SVHN. The result suggests that ReNet is {{a viable alternative to}} the <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network,</b> and that further investigation is needed...|$|E
40|$|Computer aided {{detection}} (CAD) {{systems can}} assist radiologists {{by offering a}} second opinion on early diagnosis of lung cancer. Classification and feature representation play critical roles in false-positive reduction (FPR) in lung nodule CAD. We design a <b>deep</b> <b>convolutional</b> <b>neural</b> <b>networks</b> method for nodule classification, which has an advantage of autolearning representation and strong generalization ability. A specified network structure for nodule images is proposed to solve the recognition of three types of nodules, that is, solid, semisolid, and ground glass opacity (GGO). <b>Deep</b> <b>convolutional</b> <b>neural</b> <b>networks</b> are trained by 62, 492 regions-of-interest (ROIs) samples including 40, 772 nodules and 21, 720 nonnodules from the Lung Image Database Consortium (LIDC) database. Experimental results demonstrate {{the effectiveness of the}} proposed method in terms of sensitivity and overall accuracy and that it consistently outperforms the competing methods...|$|R
40|$|Content-based image {{retrieval}} and similarity search {{has been}} investigated for several decades with many different approaches proposed. This thesis fo- cuses on a comparison of two orthogonal similarity models on two different im- age retrieval tasks. More specifically, traditional image representation models based on feature signatures are compared with models based on state-of-the-art <b>deep</b> <b>convolutional</b> <b>neural</b> <b>networks.</b> Query-by-example benchmarking and tar- get browsing tasks were selected for the comparison. In a thorough experimental evaluation, we confirm that models based on <b>deep</b> <b>convolutional</b> <b>neural</b> <b>networks</b> outperform the traditional models. However, in the target browsing scenario, we show that the traditional models could still represent an effective option. We have also implemented a feature signature extractor into the OpenCV library {{in order to make}} the source codes available for the image retrieval and computer vision community. ...|$|R
3000|$|<b>Deep</b> <b>convolutional</b> <b>neural</b> <b>networks</b> <b>Deep</b> CNNs are {{architectures}} {{that try}} to exploit the spatial structure of input information [12]. They have been used with great success in various applications, including image analysis, vision, object and emotion recognition. The most successful CNN was used for classifying millions of images in 1000 classes [21].|$|R
40|$|By {{the reason}} of the {{variability}} of light and pedestrians’ appearance, {{it is hard for}} a camera to obtain a clear human figure. Person re-identification with different cameras is a difficult visual recognition task. In this paper, a novel approach called attribute learning based on distributed <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network</b> model is proposed to address person re-identification task. It shows how attributes, namely the mid-level medium between classes and features, are obtained automatically and how they are employed to re-identify person with semantics when an author-topic model is used to mapping category. Besides, considering the ability to operate on raw pixel input without the need to design special features, <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network</b> is employed to generate features without supervision for attributes learning model. To overcome the model’s weakness in computing speed, parallelized implementations such as distributed parameter manipulation and attributes learning are employed in attribute learning based on distributed <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network</b> model. Experiments show that the proposed approach achieves state-of-the-art recognition performance in the VIPeR data set and is with a good semantic explanation...|$|E
40|$|A {{multiscale}} input {{strategy for}} multiview deep learning is proposed for supervised multispectral land-use classification {{and it is}} validated on a well-known dataset. The hypothesis that simultaneous multiscale views can improve compositionbased inference of classes containing size-varying objects compared to single-scale multiview is investigated. The end-to-end learning system learns a hierarchical feature representation {{with the aid of}} convolutional layers to shift the burden of feature determination from hand-engineering to a <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network.</b> This allows the classifier to obtain problemspecific features that are optimal for minimizing the multinomial logistic regression objective, as opposed to user-defined features which trades optimality for generality. A heuristic approach to the optimization of the <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network</b> hyperparameters is used, based on empirical performance evidence. It is shown that a single <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network</b> can be trained simultaneously with multiscale views to improve prediction accuracy over multiple single-scale views. Competitive performance is achieved for the UC Merced dataset where the 93. 48 % accuracy of multiview deep learning outperforms the 85. 37 % accuracy of SIFT-based methods and the 90. 26 % accuracy of unsupervised feature learning. National Research Foundation (NRF) of South Africa[URL]...|$|E
30|$|Facial image {{recognition}} {{is one of}} the most challenging tasks in surveillance systems due to problems such as low quality of images and significant variance in pose, expression, illumination, and resolution. Although a number of face recognition algorithms have been proposed in the literature, face recognition in an unconstrained environment still presents low accuracy. Recently, <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network</b> (DCNN)-based techniques have shown excellent results in face recognition by discovering intricate features in large data-sets. However, DCNN-based models struggle to suggest uncertainty in the prediction of the output class which can be useful to reduce false positives. In this study, Bayesian <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network</b> (B-DCNN) is employed to represent model uncertainty to improve the accuracy of facial {{image recognition}}.|$|E
40|$|Live-cell imaging {{has opened}} an {{exciting}} {{window into the}} role cellular heterogeneity plays in dynamic, living systems. A major critical challenge for this class of experiments {{is the problem of}} image segmentation, or determining which parts of a microscope image correspond to which individual cells. Current approaches require many hours of manual curation and depend on approaches that are difficult to share between labs. They are also unable to robustly segment the cytoplasms of mammalian cells. Here, we show that <b>deep</b> <b>convolutional</b> <b>neural</b> <b>networks,</b> a supervised machine learning method, can solve this challenge for multiple cell types across the domains of life. We demonstrate that this approach can robustly segment fluorescent images of cell nuclei as well as phase images of the cytoplasms of individual bacterial and mammalian cells from phase contrast images without the need for a fluorescent cytoplasmic marker. These networks also enable the simultaneous segmentation and identification of different mammalian cell types grown in co-culture. A quantitative comparison with prior methods demonstrates that <b>convolutional</b> <b>neural</b> <b>networks</b> have improved accuracy and lead to a significant reduction in curation time. We relay our experience in designing and optimizing <b>deep</b> <b>convolutional</b> <b>neural</b> <b>networks</b> for this task and outline several design rules that we found led to robust performance. We conclude that <b>deep</b> <b>convolutional</b> <b>neural</b> <b>networks</b> are an accurate method that require less curation time, are generalizable to a multiplicity of cell types, from bacteria to mammalian cells, and expand live-cell imaging capabilities to include multi-cell type systems...|$|R
40|$|This thesis tackles {{fine-grained}} image recognition, {{the task}} of sub-category or species classification. It explores general methods to improve fine-grained image classification {{including the use of}} generative models and <b>deep</b> <b>convolutional</b> <b>neural</b> <b>networks</b> leading to novel models such as a Mixture of deep convolution <b>neural</b> <b>networks.</b> This work led to 9 peer reviewed publications and a Best Paper Award...|$|R
40|$|Convolutional kernels {{are basic}} and vital {{components}} of <b>deep</b> <b>Convolutional</b> <b>Neural</b> <b>Networks</b> (CNN). In this paper, we equip convolutional kernels with shape attributes {{to generate the}} <b>deep</b> Irregular <b>Convolutional</b> <b>Neural</b> <b>Networks</b> (ICNN). Compared to traditional CNN applying regular convolutional kernels like 3 × 3, our approach trains irregular kernel shapes to better fit the geometric variations of input features. In other words, shapes are learnable parameters in addition to weights. The kernel shapes and weights are learned simultaneously during end-to-end training with the standard back-propagation algorithm. Experiments for semantic segmentation are implemented to validate the effectiveness of our proposed ICNN. Comment: 7 pages, 5 figures, 3 table...|$|R
30|$|Our {{proposed}} model learns low-level {{features in}} a small local area, and use the simple DSIFT-BoW to express the high-level scene concept, which restricts the expression ability of the model. Embedding the proposed method with the <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network,</b> Bengio et al. (2013) might enable higher expression ability.|$|E
40|$|Aiming at low {{precision}} of {{remote sensing image}} scene classification owing to small sample sizes, a new classification approach is proposed based on multi-scale <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network</b> (MS-DCNN), which is composed of nonsubsampled Contourlet transform (NSCT), <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network</b> (DCNN), and multiple-kernel support vector machine (MKSVM). Firstly, remote sensing image multi-scale decomposition is conducted via NSCT. Secondly, the decomposing high frequency and low frequency subbands are trained by DCNN to obtain image features in different scales. Finally, MKSVM is adopted to integrate multi-scale image features and implement remote sensing image scene classification. The experiment results in the standard image classification data sets indicate that the proposed approach obtains great classification effect due to combining the recognition superiority to different scenes of low frequency and high frequency subbands...|$|E
40|$|In this work, we {{investigated}} {{the feasibility of}} applying deep learning techniques to solve Poisson's equation. A <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network</b> is set up to predict the distribution of electric potential in 2 D or 3 D cases. With proper training data generated from a finite difference solver, the strong approximation capability of the <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network</b> allows it to make correct prediction given information of the source and distribution of permittivity. With applications of L 2 regularization, numerical experiments show that the predication error of 2 D cases can reach below 1. 5 % and the predication of 3 D cases can reach below 3 %, with {{a significant reduction in}} CPU time compared with the traditional solver based on finite difference methods. Comment: 7 pages, 10 figure...|$|E
40|$|Compact keyframe-based video {{summaries}} are {{a popular}} way of generating viewership on video sharing platforms. Yet, creating relevant and compelling summaries for arbitrarily long videos {{with a small}} number of keyframes is a challenging task. We propose a comprehensive keyframe-based summarization framework combining <b>deep</b> <b>convolutional</b> <b>neural</b> <b>networks</b> and restricted Boltzmann machines. An original co-regularization scheme is used to discover meaningful subject-scene associations. The resulting multimodal representations are then used to select highly-relevant keyframes. A comprehensive user study is conducted comparing our proposed method to a variety of schemes, including the summarization currently in use by {{one of the most popular}} video sharing websites. The results show that our method consistently outperforms the baseline schemes for any given amount of keyframes both in terms of attractiveness and informativeness. The lead is even more significant for smaller summaries. Comment: Video summarization, <b>deep</b> <b>convolutional</b> <b>neural</b> <b>networks,</b> co-regularized restricted Boltzmann machine...|$|R
40|$|International audienceWe {{present a}} Multiscale <b>Convolutional</b> <b>Neural</b> <b>Network</b> (MCNN) {{approach}} for vision-based classification of cells. Based on several <b>deep</b> <b>Convolutional</b> <b>Neural</b> <b>Networks</b> (CNN) acting at different resolutions, the proposed architecture avoid the classical handcrafted features extraction step, by processing features extraction and classification as a whole. The proposed approach gives better classification rates than classical state-of-the-art methods allowing a safer Computer-Aided Diagnosis of pleural cancer...|$|R
40|$|Most {{existing}} star-galaxy classifiers use {{the reduced}} summary information from catalogs, requiring careful feature extraction and selection. The latest advances in machine learning that use <b>deep</b> <b>convolutional</b> <b>neural</b> <b>networks</b> allow a machine to automatically {{learn the features}} directly from data, minimizing the need for input from human experts. We present a star-galaxy classification framework that uses <b>deep</b> <b>convolutional</b> <b>neural</b> <b>networks</b> (ConvNets) directly on the reduced, calibrated pixel values. Using data from the Sloan Digital Sky Survey (SDSS) and the Canada-France-Hawaii Telescope Lensing Survey (CFHTLenS), we demonstrate that ConvNets are able to produce accurate and well-calibrated probabilistic classifications that are competitive with conventional machine learning techniques. Future advances in deep learning may bring more success with current and forthcoming photometric surveys, such as the Dark Energy Survey (DES) and the Large Synoptic Survey Telescope (LSST), because deep <b>neural</b> <b>networks</b> require very little, manual feature engineering. Comment: 13 page, 13 figures. Accepted for publication in the MNRAS. Code available at [URL]...|$|R
40|$|The fully {{connected}} layers of a <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network</b> typically contain over 90 % {{of the network}} parameters, and consume {{the majority of the}} memory required to store the network parameters. Reducing the number of parameters while preserving essentially the same predictive performance is critically important for operating deep neural networks in memory constrained environments such as GPUs or embedded devices. In this paper we show how kernel methods, in particular a single Fastfood layer, can be used to replace all {{fully connected}} layers in a <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network.</b> This novel Fastfood layer is also end-to-end trainable in conjunction with convolutional layers, allowing us to combine them into a new architecture, named deep fried convolutional networks, which substantially reduces the memory footprint of convolutional networks trained on MNIST and ImageNet with no drop in predictive performance. Comment: svd experiments include...|$|E
40|$|Robust face {{representation}} {{is imperative to}} highly accurate face recognition. In this work, we propose an open source face recognition method with deep representation named as VIPLFaceNet, which is a 10 -layer <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network</b> with 7 convolutional layers and 3 fully-connected layers. Compared with the well-known AlexNet, our VIPLFaceNet takes only 20...|$|E
30|$|Regarding {{convolutional}} {{network for}} NLP tasks, Collobert et al. [15] for semantic role labeling task avoid excessive feature engineering {{by using the}} convolutional neural network. In 2011 Collobert used a similar network architecture for syntactic parsing. In [43] a <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network</b> is proposed that exploits the character-to sentence-level information to perform sentiment analysis of short texts.|$|E
3000|$|The {{back-end}} processing modules {{vary from}} different applications. For audio scene classification task, {{they will be}} <b>deep</b> <b>convolutional</b> <b>neural</b> <b>networks</b> followed by a softmax layer to convert the feature maps to the corresponding categories. However, for audio source separation task, the modules will be composed by a binary gating layer and some spectrogram reconstruction layers. We define them as nonlinear functions f [...]...|$|R
40|$|We {{present an}} {{approach}} to automatically classify clinical text at a sentence level. We are using <b>deep</b> <b>convolutional</b> <b>neural</b> <b>networks</b> to represent complex features. We train the network on a dataset providing a broad categorization of health information. Through a detailed evaluation, we demonstrate that our method outperforms several approaches widely used in natural language processing tasks by about 15 %...|$|R
40|$|Recently, the {{end-to-end}} {{approach that}} learns hierarchical representations from raw data using <b>deep</b> <b>convolutional</b> <b>neural</b> <b>networks</b> {{has been successfully}} explored in the image, text and speech domains. This approach was applied to musical signals as well but has been not fully explored yet. To this end, we propose sample-level <b>deep</b> <b>convolutional</b> <b>neural</b> <b>networks</b> which learn representations from very small grains of waveforms (e. g. 2 or 3 samples) beyond typical frame-level input representations. Our experiments show how deep architectures with sample-level filters improve the accuracy in music auto-tagging and they provide results comparable to previous state-of-the-art performances for the Magnatagatune dataset and Million Song Dataset. In addition, we visualize filters learned in a sample-level DCNN in each layer to identify hierarchically learned features and {{show that they are}} sensitive to log-scaled frequency along layer, such as mel-frequency spectrogram that is widely used in music classification systems. Comment: 7 pages, Sound and Music Computing Conference (SMC), 201...|$|R
40|$|Model based {{iterative}} reconstruction (MBIR) algorithms for low-dose X-ray CT are computationally expensive. To {{address this}} problem, we recently proposed the world-first <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network</b> (CNN) for low-dose X-ray CT {{and won the}} second place in 2016 AAPM Low-Dose CT Grand Challenge. However, some of the texture were not fully recovered. To cope with this problem, here we propose a deep residual learning approach in directional wavelet domain. The proposed method is motivated by an observation that a <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network</b> {{can be interpreted as}} a multilayer convolutional framelets expansion using non-local basis convolved with data-driven local basis. We further extend the idea to derive a deep convolutional framelet expansion by combining global redundant transforms and signal boosting from multiple signal representations. Extensive experimental results confirm that the proposed network has significantly improved performance and preserves the detail texture of the original image...|$|E
40|$|Abstract. We {{investigate}} if a <b>deep</b> <b>Convolutional</b> <b>Neural</b> <b>Network</b> {{can learn}} representations of local image patches that are usable in the impor-tant task of keypoint matching. We examine several possible loss func-tions for this correspondance task and show emprically that a newly suggested loss formulation allows a Convolutional Neural Network to find compact local image descriptors that perform comparably to state-of-the-art approaches...|$|E
40|$|The goal of {{this thesis}} is have a robot that learn how to grasp an unknown object with an {{underactuated}} end-effector. We chose to implement regression via <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network</b> in a supervised learning scenario. We present a 3 D and 2 D <b>deep</b> <b>convolutional</b> <b>neural</b> <b>network</b> with as input, respectively a voxel and images takes from vary {{point of view of}} the object. We create a database that match the image with one feasible pose. The pose is make using Minimum Volume Bounding Box, minimizing the volume of the boxes which fit partial point clouds. In these way we can focus on outermost boxes and we can choose a desired pose that grasping the object in a successful manner. The simulation is made using Klamp't simulator, and the neural network is implement using Theano library. We compare the results of our neural network trying different loss function and two structure of network: one with regularization and the other without it...|$|E
40|$|<b>Deep</b> <b>convolutional</b> <b>neural</b> <b>networks</b> have {{recently}} proven extremely competitive in challenging image recognition tasks. This paper proposes the epitomic convolution {{as a new}} building block for deep <b>neural</b> <b>networks.</b> An epitomic convolution layer replaces a pair of consecutive convolution and max-pooling layers found in standard <b>deep</b> <b>convolutional</b> <b>neural</b> <b>networks.</b> The main version of the proposed model uses mini-epitomes in place of filters and computes responses invariant to small translations by epitomic search instead of max-pooling over image positions. The topographic version of the proposed model uses large epitomes to learn filter maps organized in translational topographies. We show that error back-propagation can successfully learn multiple epitomic layers in a supervised fashion. The effectiveness of the proposed method is assessed in image classification tasks on standard benchmarks. Our experiments on Imagenet indicate improved recognition performance compared to standard <b>convolutional</b> <b>neural</b> <b>networks</b> of similar architecture. Our models pre-trained on Imagenet perform excellently on Caltech- 101. We also obtain competitive image classification results on the small-image MNIST and CIFAR- 10 datasets. Comment: 9 page...|$|R
40|$|Social {{behavior}} and many cultural etiquettes {{are influenced by}} gender. There are numerous potential applications of automatic face gender recognition such as human-computer interaction systems, content based image search, video surveillance and more. The immense increase of images that are uploaded online has fostered the construction of large labeled datasets. Recently, impressive progress has been demonstrated in the closely related task of face verification using <b>deep</b> <b>convolutional</b> <b>neural</b> <b>networks.</b> In this paper we explore the applicability of <b>deep</b> <b>convolutional</b> <b>neural</b> <b>networks</b> on gender classification by fine-tuning a pretrained <b>neural</b> <b>network.</b> In addition, we explore the performance of dropout support vector machines by training them on the deep features of the pretrained network {{as well as on}} the deep features of the fine-tuned network. We evaluate our methods on the color FERET data collection and the recently constructed Adience data collection. We report crossvalidated performance rates on each dataset. We further explore generalization capabilities of our approach by conducting crossdataset tests. It is demonstrated that our fine-tuning method exhibits state-of-the-art performance on both datasets...|$|R
40|$|Music tag {{words that}} {{describe}} music audio by text have {{different levels of}} abstraction. Taking this issue into account, we propose a music classification approach that aggregates multi-level and multi-scale features using pre-trained feature extractors. In particular, the feature extractors are trained in sample-level <b>deep</b> <b>convolutional</b> <b>neural</b> <b>networks</b> using raw waveforms. We show that this approach achieves state-of-the-art results on several music classification datasets. Comment: ICML Music Discovery Workshop 201...|$|R
