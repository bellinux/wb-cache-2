5177|5190|Public
25|$|Chaplot et al. was {{the first}} to use Discrete Wavelet Transform (DWT) {{coefficients}} to detect pathological brains. Maitra and Chatterjee employed the Slantlet transform, which is an improved version of DWT. Their <b>feature</b> <b>vector</b> of each image is created by considering the magnitudes of Slantlet transform outputs corresponding to six spatial positions chosen according to a specific logic.|$|E
25|$|Jobs and Wozniak co-founded Apple in 1976 to sell Wozniak's Apple I {{personal}} computer. The visionaries gained {{fame and}} wealth {{a year later}} for the Apple II, {{one of the first}} highly successful mass-produced personal computers. In 1979, after a tour of PARC, Jobs saw the commercial potential of the Xerox Alto, which was mouse-driven and had a graphical user interface (GUI). This led to development of the unsuccessful Apple Lisa in 1983, followed by the breakthrough Macintosh in 1984. In addition to being the first mass-produced computer with a GUI, the Macintosh introduced the sudden rise of the desktop publishing industry in 1985 with the addition of the Apple LaserWriter, the first laser printer to <b>feature</b> <b>vector</b> graphics. Following a long power struggle, Jobs was forced out of Apple in 1985.|$|E
5000|$|With {{each time}} step t a data vector {{randomly}} chosen from P is presented. Subsequently, the distance {{order of the}} feature vectors to the given data vector x is determined. i0 denotes the index of the closest <b>feature</b> <b>vector,</b> i1 the index of the second closest <b>feature</b> <b>vector</b> etc. and iN-1 the index of the <b>feature</b> <b>vector</b> most distant to x. Then each <b>feature</b> <b>vector</b> (k=0,...,N-1) is adapted according to ...|$|E
30|$|Once all the <b>feature</b> <b>vectors</b> in a {{test signal}} are labeled, the <b>feature</b> <b>vectors</b> that are {{assigned}} to common clusters are excluded and the labeling of the remaining <b>feature</b> <b>vectors</b> are used to classify the signal. A test signal is classified as a class e signal, if {{the majority of its}} test <b>feature</b> <b>vectors</b> (i.e., excluding the <b>feature</b> <b>vectors</b> assigned to common clusters) belong to class e.|$|R
30|$|Subtract {{the mean}} from each audio <b>feature</b> <b>vectors.</b> It {{produces}} audio <b>feature</b> <b>vectors</b> whose mean is zero.|$|R
40|$|In this paper, {{the use of}} a {{specific}} metric as a feature selection step is investigated. The feature selection step tries to model the correlation among adjacent <b>feature</b> <b>vectors</b> and the variability of the speech. We propose a new procedure which performs the feature selection in two steps. The first step takes into account the temporal correlation among the N <b>feature</b> <b>vectors</b> of a template in order to obtain a new set of <b>feature</b> <b>vectors</b> which are uncorrelated. This step gives a new template of M <b>feature</b> <b>vectors,</b> with M « N. The second step defines {{a specific}} distance among <b>feature</b> <b>vectors</b> {{to take into account the}} frequency discrimination features which discriminate each word of the vocabulary from the others or a set of them. Thus, the new <b>feature</b> <b>vectors</b> are uncorrelated in time and discriminant in frequency. Peer ReviewedPostprint (published version...|$|R
5000|$|Where [...] is the {{original}} <b>feature</b> <b>vector,</b> [...] is the mean of that <b>feature</b> <b>vector,</b> and [...] is its standard deviation.|$|E
50|$|A set of numeric {{features}} can be conveniently {{described by}} a <b>feature</b> <b>vector.</b> An example of reaching a two way classification from a <b>feature</b> <b>vector</b> (related to the perceptron) consists ofcalculating the scalar product between the <b>feature</b> <b>vector</b> and a vector of weights,comparing the result with a threshold, and deciding the class based on the comparison. Algorithms for classification from a <b>feature</b> <b>vector</b> include nearest neighbor classification, neural networks, and statistical techniques such as Bayesian approaches.|$|E
5000|$|... “Connected {{components}} analysis derives one <b>feature</b> <b>vector</b> of each {{connected component}} in a binary 2-D input image. A <b>feature</b> <b>vector</b> of a connected component is an n-tuple composed of {{functions of the}} component’s pattern.” ...|$|E
40|$|In this study, a {{new feature}} {{selection}} algorithm, the neighborhood-relationship feature selection (NRFS) algorithm, is proposed for identifying rat electroencephalogram signals and recognizing Chinese characters. In these two applications, dependent relationships exist among the <b>feature</b> <b>vectors</b> and their neighboring <b>feature</b> <b>vectors.</b> Therefore, the proposed NRFS algorithm {{was designed for}} solving this problem. By applying the NRFS algorithm, unselected <b>feature</b> <b>vectors</b> have a high priority of being added into the feature subset if the neighboring <b>feature</b> <b>vectors</b> have been selected. In addition, selected <b>feature</b> <b>vectors</b> have a high priority of being eliminated if the neighboring <b>feature</b> <b>vectors</b> are not selected. In the experiments conducted in this study, the NRFS algorithm was compared with two feature algorithms. The experimental {{results indicated that the}} NRFS algorithm can extract the crucial frequency bands for identifying rat vigilance states and identifying crucial character regions for recognizing Chinese characters...|$|R
3000|$|After all the <b>feature</b> <b>vectors</b> are clustered, {{clusters}} {{with large}} overlapping (i.e., containing more than 30 % overlapping <b>feature</b> <b>vectors)</b> are associated as common clusters (i.e., K [...]...|$|R
30|$|After {{obtaining the}} <b>feature</b> <b>vectors</b> from the image, the image can be {{described}} as a vector of fixed length, and then a classifier is needed to classify the <b>feature</b> <b>vectors.</b>|$|R
5000|$|For each pixel, {{label the}} pixel {{and form a}} new <b>feature</b> <b>vector</b> for it.|$|E
5000|$|The LBP <b>feature</b> <b>vector,</b> in its {{simplest}} form, {{is created}} in the following manner: ...|$|E
5000|$|Concatenate (normalized) histograms of all cells. This gives a <b>feature</b> <b>vector</b> for {{the entire}} window.|$|E
30|$|At first, the {{extracted}} {{static and}} dynamic string features will be embedded into <b>vector</b> space, generating <b>feature</b> <b>vectors.</b> Then, a Chi 2 -based feature selection method {{is applied to}} these <b>feature</b> <b>vectors.</b>|$|R
40|$|The {{selection}} of the <b>features</b> <b>vector</b> is crucial, taking direct effect on the accu-racy of target recognition. Considering that the airplane has smooth surface and regular geometric shape, this paper chooses the geometric shape feature to describe the target of air-plane. These geometric <b>features</b> <b>vector</b> include the center point, the characteris-tics of size and shape, the ration of air-frame-length to wingspan etc. Finally, this paper applies Support Vector Machine to validate the effective-ness of geometric <b>features</b> <b>vector.</b> From the result of experiment, it can be con-cluded that these <b>features</b> <b>vector</b> can sat-isfy the requirement of target recognition...|$|R
40|$|In {{supervised}} classification of image database, <b>feature</b> <b>vectors</b> of images with known classes, {{are used for}} training purpose. <b>Feature</b> <b>vectors</b> are extracted {{in such a way}} that it will represent maximum information in minimum elements. Accuracy of classification highly depends on the content of training <b>feature</b> <b>vectors</b> and number of training <b>feature</b> <b>vectors.</b> If the number of training images increases then the performance of classification also improves. But it also leads to more storage space and computation time. The main aim of this research is to reduce the number of <b>feature</b> <b>vectors</b> in an effective way so as to reduce memory space required and computation time as well as to increase an accuracy. This paper proposes three major steps for automatic classification of image database. First step is the generation of featur...|$|R
5000|$|If {{the input}} <b>feature</b> <b>vector</b> to the {{classifier}} {{is a real}} vector , then the output score is ...|$|E
5000|$|The contextual {{classification}} rule {{is described as}} below, it uses the <b>feature</b> <b>vector</b> [...] rather than [...]|$|E
5000|$|Use the new <b>feature</b> <b>vector</b> {{and combine}} the contextual {{information}} to assign the final label to the ...|$|E
30|$|For the {{standard}} AISs and mAISs, a sixfold cross-validation was used where the training set consisted of 38 <b>feature</b> <b>vectors</b> associated with 20 benign and 18 malicious apps, the tuning set consisted of 10 <b>feature</b> <b>vectors</b> associated with 5 benign and 5 malicious apps {{and the test}} set also consisted of 10 <b>feature</b> <b>vectors</b> associated with 5 benign and 5 malicious apps similar to the setup used in [27].|$|R
40|$|This paper {{purposes}} a new, {{simple and}} lightweight approach of previously studied algorithms {{that can be}} used for extracting of <b>feature</b> <b>vectors</b> that in turn enables one to classify a vehicle based on its magnetic signature shape. This algorithm is called ASWA that stands for Adaptive Spectral and Wavelet Analysis and it is a combination of features of a signal extracted by both of the spectral and wavelet analysis algorithms. The performance of classifiers using this <b>feature</b> <b>vectors</b> is compared to another <b>feature</b> <b>vectors</b> consisting of <b>features</b> extracted by Fourier transform and pattern information of the signal extracted by Hill-Pattern algorithm (CFTHP). By using ASWA-based <b>feature</b> <b>vectors,</b> there have been improvements in all of classification algorithms results such as K-Nearest Neighbors (KNN), Support Vector Machine (SVM) and Probabilistic Neural Networks (PNN). However, the best improvement rate achieved using an ASWA-Based <b>feature</b> <b>vectors</b> in K-NN algorithm. The correct rate of the classifier using CFTHP-based <b>feature</b> <b>vectors</b> was 39. 82 %, which have improved to 69. 93 % by using ASWA. This is corresponding an overall improvement by 76 % in correct classification rates...|$|R
40|$|In {{this paper}} we {{investigate}} {{the use of}} a feature selection step in a isolated word recognition system. The feature selection step tries lo modal the correlalion among dajacent <b>feature</b> <b>vectors</b> and the variabilily of the speech. Thus, the feature selection is performed in two steps. The first step takes into account the temporal correlation among <b>feature</b> <b>vectors</b> in order to obtain a new set of <b>feature</b> <b>vectors</b> which are uncorrelated. This step gives a new template of M <b>feature</b> <b>vectors,</b> being M<<N. The second step takes into account the frequency discrimination features which discriminate each word of the vocabulary from the others or a set of them. Thus, the new <b>feature</b> <b>vectors</b> are uncorrelated in time and discriminant in frecuency. The result obtained in the recognition of the digit data base are reported. Peer ReviewedPostprint (published version...|$|R
5000|$|Like {{most other}} {{techniques}} for training linear classifiers, the perceptron generalizes naturally to multiclass classification. Here, the input [...] and the output [...] {{are drawn from}} arbitrary sets. A feature representation function [...] maps each possible input/output pair to a finite-dimensional real-valued <b>feature</b> <b>vector.</b> As before, the <b>feature</b> <b>vector</b> is multiplied by a weight vector , but now the resulting score is used to choose among many possible outputs: ...|$|E
5000|$|After {{transforming}} original <b>feature</b> <b>vector</b> [...] to the ordinal descriptor , {{the difference}} between two ordinal descriptors can be evaluated in the following two measurements.|$|E
50|$|Note: An n-tuple of clause {{attributes}} {{is similar}} (but not the same) to the <b>feature</b> <b>vector</b> named by Stephan Schulz, PhD (see E equational theorem prover).|$|E
40|$|Abstract. A gray-image face {{detection}} algorithm was proposed, {{in which}} <b>feature</b> <b>vectors</b> were Gabor wavelet coefficients at 4 scales and 6 orientations. Firstly, <b>feature</b> <b>vectors</b> of training samples and negative samples were separately clustered. Secondly, these clusters were reduced by discriminate analysis. Finally, distribution model of <b>feature</b> <b>vectors</b> was built and average probability was calculated {{to determine if}} the testing image was similar to face samples. The experimental results show the algorithm is effective...|$|R
30|$|Wavelet {{coefficients}} {{related to}} different emotions were obtained using DWT method. These wavelet coefficients were evaluated as <b>feature</b> <b>vectors.</b> The size of <b>feature</b> <b>vectors</b> was reduced by using five statistical parameters {{to get rid}} of the computing load.|$|R
40|$|Nearest {{neighbor}} {{search of}} <b>feature</b> <b>vectors</b> representing local <b>features</b> is often employed for specific object recognition. In such a method, it {{is required to}} store many <b>feature</b> <b>vectors</b> to match them by distance calculation. The number of <b>feature</b> <b>vectors</b> is, in general, so large that {{a huge amount of}} memory is needed for their storage. A way to solve this problem is to skip the distance calculation because no <b>feature</b> <b>vectors</b> need to be stored if {{there is no need to}} calculate the distance. In this paper, we propose a method of object recognition without distance calculation. The characteristic point of the proposed method is to use a Bloomier filter, which is far memory efficient than hash tables, for storage and matching of <b>feature</b> <b>vectors.</b> From experiments of planar and 3 D specific object recognition, the proposed method is evaluated in comparison to a method with a hash table. 1...|$|R
5000|$|Given [...] "n" [...] {{dimensional}} points, let Ci be {{a cluster}} of data points. Let Xj be an [...] "n"-dimensional <b>feature</b> <b>vector</b> assigned to cluster Ci.|$|E
5000|$|Here [...] is a [...] matrix {{in which}} every row {{contains}} a <b>feature</b> <b>vector</b> for corresponding row, [...] is a weight vector with n parameters and usually [...]|$|E
5000|$|Suppose [...] is the <b>feature</b> <b>vector</b> of a {{key point}} and the {{elements}} of [...] is the corresponding rank of [...] in [...] [...] is defined as follows: ...|$|E
30|$|Indeed, most {{features}} are L_ 1 normalized {{to produce a}} relative histogram. The manifold of these <b>feature</b> <b>vectors</b> is thus a high-dimensional diamond shape. The SHOT <b>feature</b> <b>vectors</b> are L_ 2 normalized, and thus lie on a unit hypersphere.|$|R
30|$|During {{signature}} detection, the <b>feature</b> <b>vectors</b> of malicious and benign apps will {{be generated}} first, as stated above. Then a linearSVC classifier model is trained {{based on these}} train <b>feature</b> <b>vectors</b> which consist of known malware and benign apps.|$|R
30|$|The K-Nearest Neighbor (K-NN) {{algorithm}} {{is used to}} automatically classify {{a large amount of}} textures. In general the selection of <b>feature</b> <b>vectors</b> greatly affects the accuracy of the classification. Here we introduce two <b>feature</b> <b>vectors</b> based on colors and edges.|$|R
