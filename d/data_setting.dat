402|10000|Public
5000|$|For {{the case}} of discrete-time {{duration}} <b>data,</b> <b>setting</b> up the likelihood function when the regressors vary over time takes more work than in the case with time-invariant covariates, but one can assume conditional independence such that ...|$|E
5000|$|Data input: specifying {{physical}} {{properties of the}} already modeled building envelope using the default values set for different type of building parts or entering custom <b>data,</b> <b>setting</b> up location, weather data, building type and HVAC systems for the project.|$|E
50|$|Shadow IT adds hidden {{costs to}} organizations, {{consisting}} largely of non-IT workers in finance, marketing, HR, etc., who spend {{a significant amount}} of time discussing and re-checking the validity of certain <b>data,</b> <b>setting</b> up and managing systems and software without experience.|$|E
40|$|Results {{from using}} {{different}} breast cancer <b>data</b> <b>set</b> as the training <b>data</b> <b>set</b> in the MCL+superpc approach In order {{to check the}} robustness of our MCL+superpc approach, we used each of four validation <b>data</b> <b>sets</b> as the training <b>data</b> <b>set,</b> and the remaining four <b>data</b> <b>sets</b> as validation <b>data</b> <b>sets.</b> The following tables show these results. Table S 1. Superpc continuous prediction results from breast cancer data analysis. The results were generated by using the GSE 4922 <b>data</b> <b>set</b> as the training <b>data</b> <b>set</b> and four independent <b>data</b> <b>sets</b> as validation <b>data</b> with a threshold value of 1. 10 and 9 selected MCL modules. The training <b>data</b> <b>set</b> is highlighted in red. P-values less than 0. 05 are highlighted in yellow...|$|R
30|$|<b>Data</b> <b>set</b> feature {{processing}} scheme includes feature collection, feature {{conversion and}} reservation. The filter feature of <b>data</b> <b>set</b> would be pre-fetched. The classification characteristics of <b>data</b> <b>set</b> {{could be obtained}} based on the classification accuracy of <b>data</b> <b>set.</b> According to {{the characteristics of the}} <b>data</b> <b>set,</b> the crowd filter would select the appropriate crowd incentive strategy. Based on the complexity characteristics of the <b>data</b> <b>set</b> classification, the transformation of the subset of the <b>data</b> <b>set</b> would be completed.|$|R
30|$|<b>Data</b> <b>set</b> 4 : Reduction, transformation, and {{clustering}} <b>data</b> <b>sets</b> (14 <b>data</b> <b>sets,</b> 2 variables).|$|R
5000|$|This VPN {{was used}} to {{transmit}} the votes of over 44 million citizens from 36.805 polling centres. On election night, 4 hours after the polls closed, 80% vote counting machines had transmitted the election <b>data,</b> <b>setting</b> a new record for the Philippines.|$|E
5000|$|In {{a linear}} panel <b>data</b> <b>setting,</b> {{it can be}} {{desirable}} to estimate {{the magnitude of the}} Fixed Effects, as they provide measures of the unobserved components. For instance, in wage equation regressions, Fixed Effects capture ability measures that are constant over time, such as motivation. Chamberlain's approach to unobserved effects models is a way of estimating the linear unobserved effects, under Fixed Effect (rather than Random Effects) assumptions, in the following unobserved effects model ...|$|E
50|$|Partial (pooled) {{likelihood}} estimation for panel data {{assumes that}} density of yit given xit is correctly specified for each time period but {{it allows for}} misspecification in the conditional density of yi≔(yi1,…,yiT) given xi≔(xi1,…,xiT). Concretely, partial likelihood estimation uses the product of conditional densities as {{the density of the}} joint conditional distribution. This generality facilitates maximum likelihood methods in panel <b>data</b> <b>setting</b> because fully specifying conditional distribution of yi can be computationally demanding. On the other hand, allowing for misspecification generally results in violation of information equality and thus requires robust standard error estimator for inference.|$|E
3000|$|... (a) to {{represent}} target <b>data</b> <b>set</b> and auxiliary <b>data</b> <b>set,</b> respectively. Denote the feature matrix of target <b>data</b> <b>set</b> as X^(t)∈R^k× n^(t), the feature matrix of spectral information of auxiliary <b>data</b> <b>set</b> as X^(a)∈R^k× n^(a), and the texture feature information matrix in auxiliary <b>data</b> <b>set</b> as T^(a)∈R^m× n^(a). For target <b>data</b> <b>set,</b> {{we assume that}} each sample corresponds to particular auxiliary information. We use S [...]...|$|R
40|$|A pre-coding {{method and}} device for {{improving}} data compression performance by removing correlation between a first original <b>data</b> <b>set</b> {{and a second}} original <b>data</b> <b>set,</b> each having M members, respectively. The pre-coding method produces a compression-efficiency-enhancing double-difference <b>data</b> <b>set.</b> The method and device produce a double-difference <b>data</b> <b>set,</b> i. e., an adjacent-delta calculation performed on a cross-delta <b>data</b> <b>set</b> or a cross-delta calculation performed on two adjacent-delta <b>data</b> <b>sets,</b> from either one of (1) two adjacent spectral bands coming from two discrete sources, respectively, or (2) two time-shifted <b>data</b> <b>sets</b> coming from a single source. The resulting double-difference <b>data</b> <b>set</b> is then coded using either a distortionless data encoding scheme (entropy encoding) or a lossy data compression scheme. Also, a post-decoding method and device for recovering a second original <b>data</b> <b>set</b> having been represented by such a double-difference <b>data</b> <b>set...</b>|$|R
40|$|Several <b>data</b> <b>sets</b> {{have been}} {{proposed}} for benchmarking in time series prediction. A popular one is <b>Data</b> <b>Set</b> A from the Santa Fe Competition. This <b>data</b> <b>set</b> {{was the subject of}} analysis in many papers. In this note, it is shown that predicting the continuation of <b>Data</b> <b>Set</b> A is nothing else than a pattern matching problem. Looking at studies of this <b>data</b> <b>set,</b> it is remarkable that most of the very good forecasts of <b>Data</b> <b>Set</b> A used upsampled training data. We explain why upsampling is crucial for this <b>data</b> <b>set.</b> Finally, it is demonstrated that simple pattern matching performs as good as sophisticated prediction methods on <b>Data</b> <b>Set</b> A...|$|R
50|$|World Moon Bounce Day, June 29, 2009, {{was created}} by Echoes of Apollo and celebrated {{worldwide}} as an event preceding the 40th anniversary of the Apollo 11 Moon landing. A highlight of the celebrations was an interview via the Moon with Apollo 8 astronaut Bill Anders. He was {{also part of the}} backup crew for Apollo 11. The University of Tasmania in Australia with their 26m dish was able to bounce a data signal off the surface of the Moon which was received by a large dish in the Netherlands - Dwingeloo Radio Observatory. The data signal was successfully resolved back to <b>data</b> <b>setting</b> a world record for the lowest power data signal returned from the Moon with a transmit power of 3 milliwatts - about 1,000th of the power of a strong flashlight filament globe. The second World Moon Bounce Day was April 17, 2010 coinciding with the 40th anniversary of the conclusion of the Apollo 13 mission.|$|E
50|$|The {{organization}} is {{the creator of}} World Moon Bounce Day. A global event where both commercial and amateur radio operators bounce signals off {{the surface of the}} Moon and back to Earth. World Moon Bounce Day was first celebrated on June 27, 2009 (GMT) with some events starting some hours earlier. Countries around the world participate as the Earth turns and during a 24-hour event the Moon is visible to all countries. In amateur circles, the activities are also known as EME (communications) (Earth-Moon-Earth). The first World Moon Bounce Day had Apollo astronaut Bill Anders as a guest. His interview was bounced off the Moon as part of the activities. The University of Tasmania in Australia with their 26m dish was able to bounce a data signal off the surface of the Moon which was received by a large dish in the Netherlands. The data signal was successfully resolved back to <b>data</b> <b>setting</b> a world record for the lowest power data signal returned from the Moon with a transmit power of 3mW - about 1,000th of the power of a strong flashlight filament globe.|$|E
30|$|This {{subsection}} {{consists of}} three parts. Part one contains the comparisons of Algorithm 3, DADM, and NESTA for <b>data</b> <b>setting</b> with partial DCT measurement matrices, part two contains that for <b>data</b> <b>setting</b> with partial DWHT measurement matrices, and part three contains results on random matrices with orthonormalized rows.|$|E
40|$|This paper {{describes}} a prototype system for registering geologic <b>data</b> <b>sets</b> through ontologies {{to assist in}} integrating and querying heterogeneous geologic <b>data</b> <b>sets.</b> The system consists of three components: an ontology repository, the <b>data</b> <b>set</b> registration, and ontology-aware applications. User-defined ontologies in OWL are saved and used by the system. Each <b>data</b> <b>set</b> must be registered before it becomes available, and the registration semi-automatically generates a mapping from <b>data</b> <b>sets</b> to ontologies. The mapping between <b>data</b> <b>sets</b> and ontologies are used by applications to explore and extract information from the <b>data</b> <b>set...</b>|$|R
30|$|The {{experiments}} adopts {{the classic}} <b>data</b> <b>set</b> DataSet 1 provided by KDD Cup’ 99 {{to test the}} correctness of the proposed parallel spectral clustering algorithm; we use respectively 10000 (<b>Data</b> <b>Set</b> DS 1), 50000 (<b>Data</b> <b>Set</b> DS 2), 100000 (<b>Data</b> <b>Set</b> DS 3), 1000000 (<b>Data</b> <b>Set</b> DS 4), 5000000 (<b>Data</b> <b>Set</b> DS 5) to verify {{the superiority of the}} proposed parallel algorithm, and data samples is the multidimensional data listed in literature [20, 21].|$|R
50|$|Wylbur {{provides}} a line editor {{that works with}} temporary <b>data</b> <b>sets,</b> similar to buffers in other editors. At any point in time one of the temporary <b>data</b> <b>sets</b> is designated as default. Wylbur maintains a current line pointer for each temporary <b>data</b> <b>set.</b> The user may specify an explicit working <b>data</b> <b>set</b> on a command; if he omits it, then the default temporary <b>data</b> <b>set</b> is used as the working <b>data</b> <b>set.</b>|$|R
40|$|The {{purpose of}} this article is to {{introduce}} a kind of <b>data</b> <b>setting</b> to handle radial basis functions. Traditionally the meshless method RBF uses scattered <b>data</b> <b>setting</b> to do interpolations. This approach faces two hard problems. First, the optimal choice of the shape parameters contained in smooth radial functions are not easy to find. Second, the crucial constant ω in the exponential-type error bound, which is O(ω 1 d), is too large, making this error bound meaningful only when the fill distance d is extremely small. However, in the evenly spaced <b>data</b> <b>setting,</b> an error bound of the form O(d...|$|E
40|$|Unlike the {{previous}} papers of the author, {{which are in}} an evenly spaced <b>data</b> <b>setting,</b> we present an approach which predicts the optimal value of the shape parameter contained in the muiltiquadrics and inverse multiquadrics in a purely scattered <b>data</b> <b>setting.</b> For the purpose of practical application, this approach {{is expected to be}} more useful. Comment: 17 pages, 8 figure...|$|E
40|$|We {{consider}} a conceptual {{correspondence between the}} missing <b>data</b> <b>setting,</b> and joint modeling of longitudinal and time-to-event outcomes. Based on this, we formulate an extended shared random effects joint model. Based on this, we provide a characterization of missing at random, which {{is in line with}} that in the missing <b>data</b> <b>setting.</b> The ideas are illustrated using data from a study on liver cirrhosis, contrasting the new framework with conventional joint models. status: publishe...|$|E
30|$|To {{evaluate}} our approach, we use two <b>data</b> <b>sets,</b> the TEE 2014 <b>data</b> <b>set</b> and the CLEF Replab 2014 <b>data</b> <b>set.</b>|$|R
40|$|The <b>data</b> <b>set</b> {{specifications}} for the NASA Aerospace Safety Information System (NASIS) are presented. The <b>data</b> <b>set</b> specifications describe the content, format, and medium of communication of every <b>data</b> <b>set</b> {{required by the}} system. All relevant information pertinent to a particular <b>data</b> <b>set</b> is prepared in a standard form and centralized in a single document. The format for the <b>data</b> <b>set</b> is provided...|$|R
40|$|Abstract Background The {{information}} from different <b>data</b> <b>sets</b> experimented under different conditions may be inconsistent {{even though they}} are performed with the same research objectives. More than that, even when the <b>data</b> <b>sets</b> were generated from the same platform, the data agreement may be affected by the technical variation among the laboratories. In this case, it is necessary to use the combined <b>data</b> <b>set</b> after adjusting the differences between such <b>data</b> <b>sets,</b> for detecting the more reliable information. Results The proposed method combines <b>data</b> <b>sets</b> posterior to the discretization of <b>data</b> <b>sets</b> based on the ranks of the gene expression ratios, and the statistical method is applied to the combined <b>data</b> <b>set</b> for predictive gene selection. The efficiency of the proposed method was evaluated using five colon cancer related <b>data</b> <b>sets,</b> which were experimented using cDNA microarrays with different RNA sources, and one experiment utilized oligonucleotide arrays. NCI- 60 cell lines <b>data</b> <b>sets</b> were used, which were performed with two different platforms of cDNA microarrays and Affymetrix HU 6800 oligonucleotide arrays. The combined <b>data</b> <b>set</b> by the proposed method predicted the test <b>data</b> <b>sets</b> more accurately than the separated <b>data</b> <b>sets</b> did. The biological significant genes were detected from the combined <b>data</b> <b>set,</b> which were missed on the separated <b>data</b> <b>sets.</b> Conclusion By transforming gene expressions using ranks, the proposed method is not influenced by systematic bias among chips and normalization method. The method may be especially more useful to find predictive genes from <b>data</b> <b>sets</b> which have different scale in gene expressions. </p...|$|R
40|$|In practice, data often contain {{discrete}} variables. But most of {{the popular}} nonparametric estimation methods have been developed in a purely continuous framework. A common trick among practitioners is to make discrete variables continuous by adding {{a small amount of}} noise. We show that this approach is justified if the noise distribution belongs to a certain class. In this case, any estimator developed in a purely continuous framework extends naturally to the mixed <b>data</b> <b>setting.</b> Estimators defined that way will be called continuous convolution estimators. They are extremely easy to implement and their asymptotic properties transfer directly from the continuous to the mixed <b>data</b> <b>setting...</b>|$|E
30|$|At f^r_t = 0 it {{is still}} not cost-optimal to open a {{remanufacturing}} centre. Hence, decreasing the cost parameter f^r_t, ∀ r ∈ R, t ∈ T has no impact on the decision to open a remanufacturing centre under this <b>data</b> <b>setting.</b>|$|E
30|$|The FLCAPPR {{problem with}} the {{described}} <b>data</b> <b>setting</b> can be solved by the optimization software Gurobi 6.5. 0 on a two 3.10 GHz Intel Xeon Processor E 5 - 2687 W and 128 GB RAM computer in 197.71 seconds. The total discounted costs are 19, 534, 389.05 MU.|$|E
50|$|Documents and <b>data</b> <b>sets</b> sectionAnother {{section of}} the Facility Information Model {{consists}} of documents and <b>data</b> <b>sets</b> in various formats. Each of those documents and <b>data</b> <b>sets</b> {{is related to the}} element in the facility model about which the document or <b>data</b> <b>set</b> contains information.|$|R
30|$|Hypotheses {{related to}} {{operating}} core (innovation process, cross-functional organisation, {{and implementation of}} tools/technology) and competition-informed pricing The result of H 4 is supported in the full <b>data</b> <b>set</b> (β =  0.170, p <  0.05) and the Malaysian <b>data</b> <b>set</b> (β =  0.255, p <  0.05), while in the Bangladeshi data it was not supported. The result of H 5 was supported in the full <b>data</b> <b>set</b> (β =  0.266, p <  0.05) and the Bangladeshi <b>data</b> <b>set</b> (β =  0.275, p <  0.05), while in the Malaysian <b>data</b> <b>set</b> it was not supported. The result of H 6 was supported in the full <b>data</b> <b>set</b> (β =  0.295, p <  0.01) and the Bangladeshi <b>data</b> <b>set</b> (β =  0.536, p <  0.01), while in the Malaysian <b>data</b> <b>set,</b> H 6 was not supported.|$|R
50|$|In the {{geospatial}} (GIS) domain, {{data fusion}} is often synonymous with data integration. In these applications, {{there is often}} a need to combine diverse <b>data</b> <b>sets</b> into a unified (fused) <b>data</b> <b>set</b> which includes all of the data points and time steps from the input <b>data</b> <b>sets.</b> The fused <b>data</b> <b>set</b> is different from a simple combined superset in that the points in the fused <b>data</b> <b>set</b> contain attributes and metadata which might not have been included for these points in the original <b>data</b> <b>set.</b>|$|R
40|$|For {{estimating}} distributions from grouped <b>data,</b> <b>setting</b> up moment {{conditions in}} terms of group shares and group means leads to an optimal weight matrix and a GMM objective function that are considerably simpler than those from a previous specification. Minimization is more efficient and convergence is more reliable...|$|E
40|$|The {{main purpose}} of this study is to {{investigate}} the country-specific determinants of horizontal and vertical intra-industry trade (IIT) in South Africa using the gravity model of trade in a panel <b>data</b> <b>setting.</b> It empirically tests new and existing country-specific hypotheses of intra-industry trade. Copyright 2005 Economic Society of South Africa. ...|$|E
30|$|We {{have chosen}} 2  m as the σ _gps value to {{compensate}} for the RTK-Fix-GPS multipath error when moving near the building area. Lee et. al. have evaluated the RTK-Fix-GPS horizontal multipath error, which is 1.069  m [27]. To prevent overconfidence on the RTK-Fix-GPS data, we carefully fused GPS <b>data</b> <b>setting</b> a value of 2  m for σ _gps.|$|E
40|$|This paper {{describes}} {{how to build}} an audit and tracking system using SAS/AF and SCL. There will be a master <b>data</b> <b>set</b> which contains the original information and several transaction <b>data</b> <b>sets</b> which contain the new information. The audit system will take information from the transaction <b>data</b> <b>sets</b> and apply changes to the master <b>data</b> <b>set.</b> An audit <b>data</b> <b>set</b> {{will be used to}} keep track of all changes. The tracking system will display the master <b>data</b> <b>set</b> in the first data table on {{the top half of the}} screen and the audit <b>data</b> <b>set</b> in the second data table on the bottom half...|$|R
40|$|Abstract. This paper {{reviews the}} {{appropriateness}} for application to large <b>data</b> <b>sets</b> of standard machine learning algorithms, which were mainly {{developed in the}} context of small <b>data</b> <b>sets.</b> Sampling and parallelisation have proved useful means for reducing computation time when learning from large <b>data</b> <b>sets.</b> However, such methods assume that algorithms that were designed for use with what are now considered small <b>data</b> <b>sets</b> are also fundamentally suitable for large <b>data</b> <b>sets.</b> It is plausible that optimal learning from large <b>data</b> <b>sets</b> requires a different type of algorithm to optimal learning from small <b>data</b> <b>sets.</b> This paper investigates one respect in which <b>data</b> <b>set</b> size may affect the requirements of a learning algorithm – the bias plus variance decomposition of classification error. Experiments show that learning from large <b>data</b> <b>sets</b> may be more effective when using an algorithm that places greater emphasis on bias management, rather than variance management. ...|$|R
40|$|Geographic <b>data</b> <b>set</b> {{integration}} {{is particularly important}} for update propagation, i. e. the reuse of updates from one <b>data</b> <b>set</b> in another <b>data</b> <b>set.</b> In this thesis geographic <b>data</b> <b>set</b> integration (also known as map integration) between two topographic <b>data</b> <b>sets,</b> GBKN and TOP 10 vector, is described. GBKN is a large-scale topographic <b>data</b> <b>set</b> and TOP 10 vector is a medium-scale topographic <b>data</b> <b>set.</b> Geographic <b>data</b> <b>set</b> integration (or map integration) is defined as ‘the process of establishing links between corresponding object instances in different, autonomously produced, geographic <b>data</b> <b>sets</b> of the same geographic space’. Corresponding object instances are semantically similar. Semantically similar means that corresponding object instances refer to the same terrain situation. In {{the first part of}} this thesis a general introduction to geographic <b>data</b> <b>set</b> {{integration is}} given. Relevant literature is reviewed. In the second part a conceptual framework for geographic <b>data</b> <b>set</b> integration is developed. Two important components of this framework are a domain ontology and a set of surveying rules. A domain ontology is important because it contains a set of shared concepts. It is this set of shared concepts of terrain situations that makes it possible to detect corresponding object instances...|$|R
