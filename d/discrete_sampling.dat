335|1094|Public
25|$|Surface pattern {{analysis}} deals with spatially continuous phenomena. After the {{spatial distribution of}} the variables is determined through <b>discrete</b> <b>sampling,</b> statistical methods are used to quantify the magnitude, intensity, and extent of spatial autocorrelation present in the data (such as correlograms, variograms, and peridograms), {{as well as to}} map the amount of spatial variation.|$|E
2500|$|A final {{source of}} {{distortion}} (or perhaps illusion) is the DFT itself, {{because it is}} just a <b>discrete</b> <b>sampling</b> of the DTFT, which is a function of a continuous frequency domain. [...] That can be mitigated by increasing the resolution of the DFT. [...] That procedure is illustrated at Sampling the DTFT.|$|E
50|$|An {{imaging system}} running at 24 {{frames per second}} is {{essentially}} a <b>discrete</b> <b>sampling</b> system that samples a 2D area. The same limitations described by Nyquist apply to this system as to any signal sampling system.|$|E
30|$|Br counts {{measured}} on {{exactly the same}} stratigraphic intervals as the <b>discrete</b> <b>samples</b> used for the TOC, TN, and δ 13 C measurements (Table 1) were used for comparison with the TOC, TN, and δ 13 C results. Since <b>discrete</b> <b>samples</b> represent 1 -cm intervals and the step size of the ITRAX measurement was 2  mm, the ITRAX results from five adjacent measurements for the same stratigraphic intervals as the <b>discrete</b> <b>samples</b> were averaged (Table 1).|$|R
40|$|PROBLEM TO BE SOLVED To {{provide a}} method and a device capable of holding the {{appearance}} of a clear line and text with optional re-sampling ratio. SOLUTION Access is made to plural <b>discrete</b> <b>sample</b> values of image data and kernel values of each <b>discrete</b> <b>sample</b> value are calculated by using a scaled kernel by this method. The scaled kernel is constructed by converting the kernel from a first range into a second range. The kernel value is integrated by the <b>discrete</b> <b>sample</b> value to express the image data...|$|R
5000|$|In 1959 a {{competitive}} system of analysis {{was introduced by}} Hans Baruch of Research Specialties Company. That system became known as <b>Discrete</b> <b>Sample</b> Analysis and was represented by an instrument known as the [...] "Robot Chemist." [...] Over the years the <b>Discrete</b> <b>Sample</b> Analysis method slowly replaced the Continuous Flow system in the clinical laboratory.|$|R
50|$|A final {{source of}} {{distortion}} (or perhaps illusion) is the DFT itself, {{because it is}} just a <b>discrete</b> <b>sampling</b> of the DTFT, which is a function of a continuous frequency domain. That can be mitigated by increasing the resolution of the DFT. That procedure is illustrated at Sampling the DTFT.|$|E
50|$|Surface pattern {{analysis}} deals with spatially continuous phenomena. After the {{spatial distribution of}} the variables is determined through <b>discrete</b> <b>sampling,</b> statistical methods are used to quantify the magnitude, intensity, and extent of spatial autocorrelation present in the data (such as correlograms, variograms, and peridograms), {{as well as to}} map the amount of spatial variation.|$|E
5000|$|In 2008, Kline and Eagleman {{demonstrated}} that illusory reversals of two spatially overlapping motions could be perceived separately, providing {{further evidence that}} illusory motion reversal is not caused by temporal sampling. They also showed that illusory motion reversal occurs with non-uniform and non-periodic stimuli (for example, a spinning belt of sandpaper), which also cannot be compatible with <b>discrete</b> <b>sampling.</b> Kline and Eagleman proposed instead that the effect results from a [...] "motion during-effect", meaning that a motion after-effect becomes superimposed on the real motion.|$|E
5000|$|... 13 The Ergodic Theory of <b>Discrete</b> <b>Sample</b> Paths, Paul C. Shields (1996, [...] ) ...|$|R
5000|$|By 1954 {{the concept}} of an {{automatic}} <b>discrete</b> <b>sample</b> analyzer had become sufficiently concrete in his mind that Baruch began work on actual design. Construction of prototypes began in 1956 and design, development, and engineering continued until 1959 when the first units of the Robot Chemist were sold commercially. It was at this juncture that a theoretical competition began between the continuous flow concept of analysis (CFA) and the <b>discrete</b> <b>sample</b> analysis concept of the Robot Chemist. At first, it appeared that CFA would prevail, but it turned out that [...] "it was the <b>discrete</b> <b>sample</b> concept of the Robot Chemist that eventually was adopted by instrument designers and manufacturers and achieved the dominant position in the clinical laboratory." ...|$|R
3000|$|This {{shows that}} the {{directional}} components are obtained by simply translating the window function V. The <b>discrete</b> <b>samples</b> g [...]...|$|R
5000|$|Because of aliasing, {{one only}} computes the {{coefficients}} [...] up to k=N/2, since <b>discrete</b> <b>sampling</b> {{of the function}} makes the frequency of 2k indistinguishable from that of N-2k. Equivalently, the [...] are the amplitudes of the unique bandlimited trigonometric interpolation polynomial passing through the N+1 points where f(cos &theta;) is evaluated, and we approximate the integral by the integral of this interpolation polynomial. There is some subtlety in how one treats the [...] coefficient in the integral, however—to avoid double-counting with its alias it is included with weight 1/2 in the final approximate integral (as {{can also be seen}} by examining the interpolating polynomial): ...|$|E
50|$|Neutron {{activation}} analysis (NAA) is a nuclear process used {{for determining the}} concentrations of elements in {{a vast amount of}} materials. NAA allows <b>discrete</b> <b>sampling</b> of elements as it disregards the chemical form of a sample, and focuses solely on its nucleus. The method is based on neutron activation and therefore requires a source of neutrons. The sample is bombarded with neutrons, causing the elements to form radioactive isotopes. The radioactive emissions and radioactive decay paths for each element are well known. Using this information, it is possible to study spectra of the emissions of the radioactive sample, and determine the concentrations of the elements within it. A particular advantage of this technique {{is that it does not}} destroy the sample, and thus has been used for analysis of works of art and historical artifacts. NAA can also be used to determine the activity of a radioactive sample.|$|E
50|$|In {{the case}} where the child nodes of the {{collapsed}} nodes themselves have children, the conditional distribution {{of one of these}} child nodes given all other nodes in the graph will {{have to take into account}} the distribution of these second-level children. In particular, the resulting conditional distribution will be proportional to a product of the compound distribution as defined above, and the conditional distributions of all of the child nodes given their parents (but not given their own children). This follows from the fact that the full conditional distribution is proportional to the joint distribution. If the child nodes of the collapsed nodes are continuous, this distribution will generally not be of a known form, and may well be difficult to sample from despite the fact that a closed form can be written, for the same reasons as described above for non-well-known compound distributions. However, in the particular case that the child nodes are <b>discrete,</b> <b>sampling</b> is feasible, regardless of whether the children of these child nodes are continuous or discrete. In fact, the principle involved here is described in fair detail in the article on the Dirichlet-multinomial distribution.|$|E
3000|$|... is the {{one-dimensional}} discrete Fourier transform. Thus, (6) {{gives the}} algorithmic implementation for computing the <b>discrete</b> <b>samples</b> of g [...]...|$|R
50|$|In this method, a {{sequence}} of dirac impulses, xs(t), representing the <b>discrete</b> <b>samples,</b> xn, is low-pass filtered to recover a continuous-time signal, x(t).|$|R
3000|$|Now {{we move on}} to {{the case}} of one {{dimension}} higher, i.e., a set of <b>discrete</b> <b>samples</b> of a 3 D scalar field [...]...|$|R
5000|$|Many domains contain {{multiple}} displays, {{and require}} {{more than a simple}} discrete yes/no response time measurement. A critical question for these situations may be [...] "How much time will operators spend looking at X relative to Y?" [...] or [...] "What is the likelihood that the operator will completely miss seeing a critical event?" [...] Visual sampling is the primary means of obtaining information from the world. An early model in this domain is Sender's (1964, 1983) based upon operators monitoring of multiple dials, each with different rates of change. Operators try, as best as they can, to reconstruct the original set of dials based on <b>discrete</b> <b>sampling.</b> This relies on the mathematical Nyquist theorem stating that a signal at W Hz can be reconstructed by sampling every 1/W seconds. This was combined with a measure of the information generation rate for each signal, to predict the optimal sampling rate and dwell time for each dial. Human limitations prevent human performance from matching optimal performance, but the predictive power of the model influenced future work in this area, such as Sheridan's (1970) extension of the model with considerations of access cost and information sample value.|$|E
30|$|Algorithm 1. <b>Discrete</b> <b>sampling</b> theorem {{imputation}} algorithm.|$|E
30|$|Next we {{outline the}} <b>discrete</b> <b>sampling</b> theorem, which {{constitute}} the data imputation scheme presented here.|$|E
5000|$|Statistical errors - These {{errors are}} caused by the averages in traffic {{measurements}} and {{by the fact that}} measurements are made from <b>discrete</b> <b>samples</b> ...|$|R
5000|$|... #Caption: Signal {{sampling}} representation. The continuous {{signal is}} represented {{with a green}} colored line while the <b>discrete</b> <b>samples</b> are indicated by the blue vertical lines.|$|R
50|$|In mathematics, the Poisson {{summation}} {{formula is}} an equation that relates the Fourier series coefficients of the periodic summation of a function to {{values of the}} function's continuous Fourier transform. Consequently, the periodic summation of a function is completely defined by <b>discrete</b> <b>samples</b> of the original function's Fourier transform. And conversely, the periodic summation of a function's Fourier transform is completely defined by <b>discrete</b> <b>samples</b> of the original function. The Poisson summation formula was discovered by Siméon Denis Poisson and is sometimes called Poisson resummation.|$|R
40|$|Abstract—Following {{the pricing}} {{approach}} proposed by Zhu & Lian [19], we present an exact solution for pricing variance swaps with the realized {{variance in the}} payoff function being a logarithmic return of the underlying asset at some pre-specified <b>discrete</b> <b>sampling</b> points. Our newly-found pricing formula {{is based on the}} Heston’s [8] two-factor stochastic volatility model. The discovery of this exact and closed-form solution has significantly improved the computational efficiency involved in computing the value of variance swaps with <b>discrete</b> <b>sampling</b> points...|$|E
30|$|Air {{quality is}} well {{recognized}} as a contributing factor for various physical phenomena and {{as a public health}} risk factor. Consequently, {{there is a need for}} an accurate way to measure the level of exposure to various pollutants. Longitudinal continuous monitoring however, is often incomplete due to measurement errors, hardware problems or insufficient sampling frequency. In this paper we introduce the <b>discrete</b> <b>sampling</b> theorem for the task of imputing missing data in longitudinal air-quality time series. Within the context of the <b>discrete</b> <b>sampling</b> theorem, two spectral schemes for filling missing values are presented—a Discrete Cosine Transform (DCT) and Clustering Single Variable Decomposition (K-SVD) based methods.|$|E
40|$|Drawing {{a sample}} from a {{discrete}} distribution {{is one of}} the building components for Monte Carlo methods. Like other sampling algorithms, <b>discrete</b> <b>sampling</b> suffers from the high computational burden in large-scale inference problems. We study the problem of sampling a discrete random variable {{with a high degree of}} dependency that is typical in large-scale Bayesian inference and graphical models, and propose an efficient approximate solution with a subsampling approach. We make a novel connection between the <b>discrete</b> <b>sampling</b> and Multi-Armed Bandits problems with a finite reward population and provide three algorithms with theoretical guarantees. Empirical evaluations show the robustness and efficiency of the approximate algorithms in both synthetic and real-world large-scale problems...|$|E
40|$|We {{report the}} paleo{{magnetic}} and rock magnetic results from <b>discrete</b> <b>sample</b> analysis of sediments from Walvis Ridge, Leg 208 of the Ocean Drilling Program. In {{an effort to}} refine the shipboard magnetostratigraphy, alternating field and thermal demagnetization of <b>discrete</b> <b>samples</b> were carried out, predominantly on samples from Sites 1262 and 1267. Results are generally consistent with the shipboard pass-through cryomagnetometer data, though {{in some cases the}} <b>discrete</b> <b>samples</b> resolved ambiguities in the reversal record. Significantly, the C 24 r/C 24 n reversal boundary was identified at Sites 1262 and 1267, and most boundaries in the Paleocene and Upper Cretaceous sections are now identified to within 10 - 30 cm. Magnetic mineralogy results show that prior to the late Miocene, the predominant detrital magnetic component was coarse-grained magnetite and that after the late Miocene, titanomagnetite has also been present. This suggests a possible change in detrital source at that time...|$|R
3000|$|Notation: The {{continuous}} {{time domain}} signal is denoted by x(t); x[k] is the associated <b>sampled</b> <b>discrete</b> time domain signal; {{and the frequency}} domain representation of the <b>discrete</b> <b>sample</b> x[k] is X[k]. A single lower case bold letter x represents a column vector with a given dimension. By x [...]...|$|R
5000|$|... the Euler {{approximation}} {{of the usual}} derivative with a <b>discrete</b> <b>sample</b> time [...] The delta-formulation obtains {{a significant number of}} numerical advantages compared to the shift-operator at fast sampling.|$|R
40|$|Ideal state reconstructor is {{conceptual}} subsystem of deterministic {{digital control}} system estimating state of controlled plant periodically at <b>discrete</b> <b>sampling</b> intervals of duration T. Does not add new states, eigenvalues, or dynamics. Based on concepts described in "State-Variable Representations For Moving-Average Sampling" (MFS- 28405) ...|$|E
40|$|This paper {{deals with}} {{an effect of}} the {{modified}} <b>discrete</b> <b>sampling</b> air at a non-standard measurement of aerosol particles, when it is used for measurement of suspended particulate matter by Czech Hydrometeorological Institute. In the paper is an explaination of very non-standard measurements and practical measurement...|$|E
30|$|A formal {{mathematical}} {{framework for}} recovering missing signal’s samples {{in the frequency}} domain, the <b>discrete</b> <b>sampling</b> theorem, was presented by  Yaroslavsky et. al (2009). The <b>discrete</b> <b>sampling</b> theorem states the terms and conditions a band-limited frequency representation of a signal with missing samples must fulfill so the signal can be fully recovered, given it is narrow banded in any spectral domain. The theorem constitutes the new data imputation scheme presented here. Within its context two spectral signal representations are considered: The DCT (Rao et al. 1990; Yaroslavsky et al. 2009) and the sparse coding K-Cluster Single Variable Decomposition (K-SVD) (Aharon et al. 2006). The application of the suggested methods {{show that they are}} comparable to the state-of-the-art when imputing short missing sequences and do hold the upper hand when larger chunks of subsequent data are missing.|$|E
50|$|Discrete wavelet {{transform}} theory (continuous in the variable(s)) offers an approximation to transform <b>discrete</b> (<b>sampled)</b> signals. In contrast, the discrete subband transform theory provides a perfect representation of discrete signals.|$|R
50|$|In mathematics, the discrete-time Fourier {{transform}} (DTFT) {{is a form}} of Fourier {{analysis that}} is applicable to the uniformly-spaced samples of a continuous function. The term discrete-time refers {{to the fact that the}} transform operates on <b>discrete</b> data (<b>samples)</b> whose interval often has units of time. From only the samples, it produces a function of frequency that is a periodic summation of the continuous Fourier transform of the original continuous function. Under certain theoretical conditions, described by the sampling theorem, the original continuous function can be recovered perfectly from the DTFT and thus from the original <b>discrete</b> <b>samples.</b> The DTFT itself is a continuous function of frequency, but <b>discrete</b> <b>samples</b> of it can be readily calculated via the discrete Fourier transform (DFT) (see Sampling the DTFT), which is by far the most common method of modern Fourier analysis.|$|R
50|$|Note {{that the}} {{oscillation}} data, {{rather than being}} continuous functions, are actually <b>discrete</b> <b>samples</b> in space and time, and are subject to observational error. When computing transforms, interpolation is implied, a process which inevitably introduces further errors.|$|R
