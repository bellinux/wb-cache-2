712|15|Public
25|$|The {{application}} supports redundancy via multiple displays {{as well as}} <b>failover.</b> Network elements can {{be tested}} from multiple locations and users can write custom tests.|$|E
25|$|Most {{switches}} {{have some}} rate-limiting and ACL capability. Some switches provide automatic and/or system-wide rate limiting, traffic shaping, delayed binding (TCP splicing), deep packet inspection and Bogon filtering (bogus IP filtering) {{to detect and}} remediate DoS attacks through automatic rate filtering and WAN Link <b>failover</b> and balancing.|$|E
25|$|On 24 March, the European Wikipedia servers went offline {{due to an}} {{overheating}} problem. <b>Failover</b> to servers in Florida {{turned out}} to be broken, causing DNS resolution for Wikipedia to fail across the world. The problem was resolved quickly, but due to DNS caching effects, some areas were slower to regain access to Wikipedia than others.|$|E
5000|$|Watchdog for {{all other}} {{processes}} and responsible for initiating <b>failovers</b> if an error is detected ...|$|R
30|$|<b>Failovers</b> are {{participants}} {{which serve}} as hot standbys should the leader fail or elect to stand down.|$|R
50|$|With {{appropriately}} set-up Data Guard operations, DBAs {{can facilitate}} <b>failovers</b> or switchovers to alternative hosts {{in the same}} or alternative locations.|$|R
25|$|These schemes {{will work}} {{as long as}} the DoS attacks can be {{prevented}} by using them. For example, SYN flood can be prevented using delayed binding or TCP splicing. Similarly content based DoS may be prevented using deep packet inspection. Attacks originating from dark addresses or going to dark addresses can be prevented using bogon filtering. Automatic rate filtering can work as long as set rate-thresholds have been set correctly. Wan-link <b>failover</b> will work as long as both links have DoS/DDoS prevention mechanism.|$|E
500|$|Windows Server 2012, {{along with}} Windows 8, {{includes}} {{a new version}} of Hyper-V, as presented at the Microsoft BUILD event. Many new features have been added to Hyper-V, including network virtualization, multi-tenancy, storage resource pools, cross-premises connectivity, and cloud backup. Additionally, many of the former restrictions on resource consumption have been greatly lifted. Each virtual machine in this version of Hyper-V can access up to 64 virtual processors, up to 1 terabyte of memory, and up to 64 terabytes of virtual disk space per virtual hard disk (using a new [...]vhdx format). Up to 1024 virtual machines can be active per host, and up to 8000 can be active per <b>failover</b> cluster. SLAT is a required processor feature for Hyper-V on Windows 8, while for Windows Server 2012 it is only required for the supplementary RemoteFX role.|$|E
2500|$|However, {{with the}} release of Windows Server 2008 R2, live {{migration}} is supported with the use of Cluster Shared Volumes (CSVs). This allows for <b>failover</b> of an individual VM as opposed to the entire host having to <b>failover</b> (it seems that when a node (Hyper-V server, not a VM) fails then each [...] "VM running on the failed node" [...] may migrate to other live nodes independently of [...] "other VMs on the same LUN running on other nodes that share the LUN with the failed node". In Hyper-V we are clustering the Hyper-V nodes not the VMs.). See also Cluster Shared Volumes.|$|E
40|$|SCTP’s {{multihoming}} failure {{detection time}} depends on three tunable parameters: RTO. min (minimum retransmission timeout), RTO. max (maximum retransmission timeout), and Path. Max. Retrans (threshold number of consecutive timeouts {{that must be}} exceeded to detect failure). RFC 2960 recommends Path. Max. Retrans, which translates to a failure detection time of at least 63 seconds – unacceptable to many applications. This research investigates the tradeoff between a more aggressive (i. e., lower) threshold, and spurious <b>failovers</b> {{for the application of}} bulk file transfer. We surprisingly find that spurious <b>failovers</b> do not degrade overall performance, and sometimes actually improve goodput performance. ...|$|R
5000|$|The zSeries, zEnterprise, [...] "System z and [...] "IBM Z" [...] {{families}} were named for their availability [...] - [...] z stands for zero downtime. The systems are built with spare components capable of hot <b>failovers</b> to ensure continuous operations.|$|R
50|$|In {{a network}} utilising SMLT, {{it is often}} no longer {{necessary}} to run a spanning tree protocol of any kind {{since there are no}} logical bridging loops introduced by the presence of the IST. This eliminates the need for spanning tree reconvergence or root-bridge <b>failovers</b> in failure scenarios which causes interruptions in network traffic longer than time-sensitive applications are able to cater for.|$|R
2500|$|The DoD NetOps Community {{strives to}} obtain common {{visibility}} of network resources {{so that these}} can be manage, anticipate and mitigate problems, ensuring uninterrupted availability and protection of the GIG and provide for graceful degradation, self-healing, <b>failover,</b> diversity, and elimination of critical failure points. [...] Through effective visibility, the NetOps community endeavors to attain the three goals of NetOps: Assured System and Network Availability, Assured Information Protection and Assured Information Delivery.|$|E
50|$|These {{features}} help {{minimize the}} chances that the clustering <b>failover</b> between systems will be required. In such a <b>failover,</b> the service provided is unavailable {{for at least a}} little while, so measures to avoid <b>failover</b> are preferred.|$|E
5000|$|<b>Failover</b> servers {{more complex}} - <b>Failover</b> servers must {{themselves}} have {{copies of the}} fleets of database shards.|$|E
40|$|In {{this paper}} we present {{the design of}} IRL (Interoperable Replication Logic), a FT-CORBA {{compliant}} platform that provides transparent client-server interactions and server <b>failovers</b> to application clients by using a set of replicated CORBA objects (IRL components). As cooperation among IRL components is carried out using standard CORBA invocations, IRL allows deployments of CORBA server objects and IRL components over ORBs from distinct vendors satisfying the interoperability property...|$|R
40|$|This paper {{describes}} a field demonstration and presents the network performance of an 802. 11 ground-UAV network composed of 11 ground stations, a mobile vehicle and two fixed wing UAVs, connected by two routing gateways to a legacy wired network. The network effects demonstrated include mobility, network partitions, network merges and gateway <b>failovers.</b> The paper presents experimental results for recorded data traffic {{and for the}} state of the routing protocols, with the mobile node...|$|R
50|$|The Network Emulator Add on {{allows users}} to link NetSim to live {{applications}} running on real devices. This allows for real traffic to flow via the emulator and experience network effects. In this virtual network, numerous test scenarios, involving real devices and application, can be constructed and executed repetitively for normal operation as well as perturbed operation. Impairment scenarios can studied which included escalating latency, bandwidth constriction at various points, jitter tolerance, packet loss, packet reordering, route loss, <b>failovers</b> and {{single point of failure}} identification.|$|R
50|$|CUBRID's 3-tier {{architecture}} allows native {{support for}} High-Availability with two-level auto failover: the broker <b>failover</b> and server <b>failover.</b>|$|E
5000|$|<b>Failover</b> support: As Cassandra is a {{distributed}} {{data store}} where hosts (nodes) may go down. Hector {{has its own}} <b>failover</b> policy.|$|E
50|$|Service Pack 1 (SP1) of SQL Server 2005 {{introduced}} Database Mirroring, a {{high availability}} option that provides redundancy and <b>failover</b> capabilities at the database level. <b>Failover</b> {{can be performed}} manually or can be configured for automatic <b>failover.</b> Automatic <b>failover</b> requires a witness partner and an operating mode of synchronous (also known as high-safety or full safety). Database Mirroring {{was included in the}} first release of SQL Server 2005 for evaluation purposes only. Prior to SP1, it was not enabled by default, and was not supported by Microsoft.|$|E
40|$|In {{this paper}} we present the failure {{management}} system of IRL (Interoperable Replication Logic) {{which is a}} FT-CORBA platform that provides transparent/server interactions and server <b>failovers</b> to application clients by using a set of replicated CORBA objects (IRL components). The IRL failure management system includes two basic mechanisms, failure detection and recovery. Each of these mechanisms follows a service stack approach (i) to reduce {{as many as possible}} the impact of the services in terms of network traffic and (ii) to use the same services both for application objects and for IRL components. Index Terms [...] High Availability, Fault Tolerance, Object Replication, Fault Tolerant CORBA. 1...|$|R
40|$|Abstract. As a {{transport}} layer protocol SCTP uses {{end to end}} metrics, such as Retransmission Time Out (RTO), to manage mobility handover. Our investigation illustrates that Wireless LAN (WLAN) mobility causes continuously increased Round Trip Times (RTT) resulting from 802. 11 MAC retransmissions, regardless of the service specified by upper layers. We present scenarios where the current understanding of SCTP switchover aggressiveness is invalid; spurious <b>failovers</b> together with excessive RTO result in new forms of receiver buffer blocking communication failure. Given wireless mobility performance issues, together with the ambiguity of end to end metrics, we propose an Adaptive Optimized RTO algorithm for wireless Access Networks (AORAN) which uses local as well as end to end metrics to manage mobility. AORAN measures RTT between the mobile node and Access Point (AP) to calculate wireless and Internet RTO subcomponents. We also show binary exponential backoff has negative effects on SCTP with increased wireless RTT; AORAN introduces a decision mechanism which implements backoff on RTO subcomponents only when appropriate...|$|R
40|$|In early 1970 s, Ethernet was {{developed}} as a single coaxial cable network. This work led to the original IEEE 802. 3 standard in the early 1980 s and Ethernet now enjoys being the most popular physical layer LAN technology in use today. Early Ethernet LANs were first wired using coaxial cables, with each station tapping into the cable. As a shared single collision domain (a single cable shared by all devices on the network), this created non-determinism and performance and fault-isolation problems, {{as well as the}} lack of network redundancy. To overcome these original Ethernet shortcomings, industrial networks typically deploy multiple networks. This use of multiple networks is hard to deploy and manage, and led to proprietary solutions. As the popularity of Ethernet LANs continued to grow, a more structured approach, called star (or hub-and-spoke) topology, was used where all attaching devices were linked to a repeater. This helped with fault isolation and provided a more organized methodology for expanding LANs, however it did not help with network redundancy. With advent of state-of-the-art standard network technologies, such as Rapid Spanning Tree (IEEE 802. 1 w), a single Ethernet LAN can now be deployed as a highly reliable, deterministic and self-healing mesh network. The mesh network provides the redundancy needed for industrial networks. It also provides industrial applications with a fault tolerant network with redundant data path <b>failovers</b> in the order of hundreds of milliseconds...|$|R
50|$|In {{a serial}} {{connection}} based <b>failover</b> configuration two ADN devices communicate via a standard RS232 connection {{instead of the}} network, and all sharing of session information and status is exchanged over this connection. <b>Failover</b> is nearly instantaneous, though it suffers from the same constraints regarding sessions initiated while the primary device is failing as network based <b>failover.</b>|$|E
5000|$|Certain systems, intentionally, do not <b>failover</b> entirely automatically, but require human intervention. This [...] "automated with manual approval" [...] {{configuration}} runs automatically once a human {{has approved}} the <b>failover.</b>|$|E
50|$|In {{computing}} {{and related}} {{technologies such as}} networking, <b>failover</b> is switching to a redundant or standby computer server, system, hardware component or network upon the failure or abnormal termination of the previously active application, server, system, hardware component, or network. <b>Failover</b> and switchover are essentially the same operation, except that <b>failover</b> is automatic and usually operates without warning, while switchover requires human intervention.|$|E
40|$|Voice over Internet Protocol (VOIP) is {{increasingly}} gaining popularity over the Public Switched Telephone Network (PSTN) {{because of its}} advanced features and flexibility. A crucial factor {{in the success of}} a VOIP system is its reliability. VOIP systems must attain reliability levels comparable to the PSTN, which has five nines availability. This thesis describes the design and implementation of a fully functional VOIP system that is well suited for a high availability design. For a comprehensive understanding of the IP based PBX, the working of traditional PBXs and their evolution has been described. A discussion on high availability and how the existing systems achieve it has also been given. The protocols and individual components of the designed system have also been described in detail. The fully functional VOIP system was successfully designed and implemented from basic elements. and is completely developed from open source software. It uses Kamailio, a Session Initiation Protocol (SIP) router for the setting up and routing of calls. It also contains FreeSWITCH, which acts as a SIP accessory to provide extra features such as music-on-hold, voice-mail, automated attendant and conference calls. Calls to the PSTN are supported with the help of FreeSWITCH, which also functions as the Session Border Controller (SBC). Alpine Linux is the operating source used as it is highly conducive for running VOIP systems. The system implemented is flexible and can even be accessed through analog legacy phones with the help of Analog Telephone Adapters (ATAs). The system can be scaled to provide thousands of call setups per second. It uses a modular design and DNS based telephone number mapping. This makes it highly favorable to achieve high availability by adding redundant components and managing <b>failovers</b> with the help of DNS Service Resource (SRV) records...|$|R
40|$|The {{current work}} aims to the {{development}} of the Business Continuity Testing Points method which can help both IT as well as business managers define an efficient business continuity strategy. The BCTP method stems from the UML Use Case Points theory which is a practically tested and accepted approach to SW complexity estimation. The Use Case Points methodology was selected as the theory behind the construction of the BCTP model, due to the fact that firstly, both theories share the requirement analysis task and secondly because complexity of information systems is strongly related to their recovery in cases of their unexpected <b>failovers.</b> In the Use Case Points theory IT analysts perform software requirement analysis by executing various business scenarios. The BCTP theory, on the other hand, is constructed to support the analysis of IT system recovery requirements, by executing multiple efficient recovery scenarios. The method is a new approach to the objective determination of the Recovery Time Effort of a business function in comparison to the Rational Time Objective and the Maximum Acceptable Outage, which are defined with regard to the Impact Value Level of the function. The most critical functions of the enterprise should be included in the Minimum Business Continuity Objective (MBCO) concept. MBCO refers to vital business functions without which the enterprise is not able to perform its basic operations. The Recovery Time Effort of a given business function is affected by multiple Technical, Environmental and Unexpected factors with precise weights and assessment values. Recovery exercises should be based on scenarios which include the unexpected factors that may delay the recovery process. The derived exercise results are proposed as drivers for the reassessment of the criticality of a business function...|$|R
40|$|The NASA Exoplanet Science Institute (NExScI) {{hosts the}} annual Sagan Workshops, {{thematic}} meetings aimed at introducing researchers {{to the latest}} tools and methodologies in exoplanet research. The theme of the Summer 2012 workshop, held from July 23 to July 27 at Caltech, was to explore the use of exoplanet light curves to study planetary system architectures and atmospheres. A {{major part of the}} workshop was to use hands-on sessions to instruct attendees in the use of three open source tools for the analysis of light curves, especially from the Kepler mission. Each hands-on session involved the 160 attendees using their laptops to follow step-by-step tutorials given by experts. One of the applications, PyKE, is a suite of Python tools designed to reduce and analyze Kepler light curves; these tools can be invoked from the Unix command line or a GUI in PyRAF. The Transit Analysis Package (TAP) uses Markov Chain Monte Carlo (MCMC) techniques to fit light curves under the Interactive Data Language (IDL) environment, and Transit Timing Variations (TTV) uses IDL tools and Java-based GUIs to confirm and detect exoplanets from timing variations in light curve fitting. Rather than attempt to run these diverse applications on the inevitable wide range of environments on attendees laptops, they were run instead on the Amazon Elastic Cloud 2 (EC 2). The cloud offers features ideal for this type of short term need: computing and storage services are made available on demand for as long as needed, and a processing environment can be customized and replicated as needed. The cloud environment included an NFS file server virtual machine (VM), 20 client VMs for use by attendees, and a VM to enable ftp downloads of the attendees' results. The file server was configured with a 1 TB Elastic Block Storage (EBS) volume (network-attached storage mounted as a device) containing the application software and attendees home directories. The clients were configured to mount the applications and home directories from the server via NFS. All VMs were built with CentOS version 5. 8. Attendees connected their laptops to one of the client VMs using the Virtual Network Computing (VNC) protocol, which enabled them to interact with a remote desktop GUI during the hands-on sessions. We will describe the mechanisms for handling security, <b>failovers,</b> and licensing of commercial software. In particular, IDL licenses were managed through a server at Caltech, connected to the IDL instances running on Amazon EC 2 via a Secure Shell (ssh) tunnel. The system operated flawlessly during the workshop...|$|R
50|$|Wireless <b>failover</b> is a {{business}} continuity function. That is, it allows businesses to continue operations even {{in the event of}} a network failure. In retail, wireless <b>failover</b> is typically used when a standard connection for a point of sale credit card machine fails. In this instance, the wireless <b>failover</b> allows business transactions to continue to be processed, ensuring business continuity.|$|E
50|$|Both server and {{application}} <b>failover</b> is supported for the Microsoft Windows. Application <b>Failover</b> supports Microsoft Exchange, BlackBerry Enterprise Server, Microsoft SQL Server, File Servers, Microsoft Sharepoint, Oracle, MySQL among others.|$|E
5000|$|... 1+1 {{redundancy}} typically {{offers the}} advantage of additional <b>failover</b> transparency {{in the event of}} component failure. The level of resilience is referred to as active/active or hot as backup components actively participate with the system during normal operation. <b>Failover</b> is generally transparent (disruption to system availability) as <b>failover</b> does not actually occur (just degradation to system resilience) as the backup components were already active within the system.|$|E
50|$|All {{models of}} the Hitachi Adaptable Modular Storage 2000 family support host path <b>failover,</b> fully redundant, hot swappable components, online {{microcode}} updates and mirrored cache with battery backup. The active-active controllers include automatic <b>failover.</b>|$|E
5000|$|This {{function}} {{provides a}} means of detecting {{the loss of a}} data resource attached to a JCA connection factory and automatically failing over to a defined alternate JNDI. Detection of primary data resource recovery and failback is also an element of this functional design. The resource <b>failover</b> design is present in WebSphere Application Server Version 8 across all platforms for JDBC and JCA. WAS z/OS Version 8 provides support for WOLA resource <b>failover</b> as part of the general support for JCA resource <b>failover.</b> Invocation of the <b>failover</b> occurs when a configurable threshold number of getConnection (...) failures occur. After <b>failover</b> is invoked, all new getConnection (...) requests are routed to the alternate connection factory connection pool. Failback occurs when WAS z/OS determines the failed primary data resource has returned. New getConnection (...) requests are processed against the primary connection factory.|$|E
50|$|Fault Tolerant Messaging or <b>Failover</b> Abstraction is {{the ability}} to transparently “failover” a call or request from one service {{transport}} protocol to another upon failure with no changes to the functional code or business logic implementation. In elemenope, this ability to “failover” is achieved via Dispatcher <b>Failover</b> DFo configuration. The elemenope framework has the ability to configure multiple nested <b>failover</b> chains. A typical use of the DFo functionality is the <b>failover</b> from a synchronous service transport protocol to an asynchronous service transport protocol. For instance, when an XML-RPC service is down, the messages may be failed over to an asynchronous JMS queue implementation for processing when the service is available.|$|E
