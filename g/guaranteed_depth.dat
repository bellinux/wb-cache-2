6|26|Public
50|$|Another upgrade of {{the canal}} {{took place in the}} 1970s. During the reconstruction, the <b>guaranteed</b> <b>depth</b> of the fairway was {{increased}} to 4 meters, and the channel became part of the Unified Deep Water System of European Russia.|$|E
50|$|The Gellenstrom has a <b>guaranteed</b> <b>depth</b> of 3.70 metres and is {{controlled}} by the Gellen light which marks its northern approach. The maximum speed limit is 10 kn. To the south is the port of Barhöft, which is also accessible from the Gellenstrom via the Barhöft Creek (Barhöft -Rinne). To the west is the island of Bock, which was formed from sand dredged from the channel.|$|E
5000|$|Along {{with the}} Volga-Don Canal, the {{reservoir}} forms {{part of a}} waterway for the shipping of raw materials from the upper Don and Volga-Caspian basins to the lower Don River - Sea of Azov basin and vice versa. According to the federal agency responsible {{for the maintenance of}} this waterway (Федеральное Государственное Учреждение [...] "Волго-Донское ГБУВПиС"), <b>guaranteed</b> <b>depth</b> of the main navigable waterway through the reservoir in the navigation season of 2007 was to be maintained at no less than 3.6 meters, with the width of the navigable waterway no less than 50 meters. It was to be available for navigation for 233 days, from April 5 to November 23, 2007.|$|E
50|$|At the {{beginning}} of the 1960s, Krokhino, Karlugino and other settlements of Belozersky District got into the inundation zone at the time of construction of deep Volga-Baltic Waterway. The level of water in Lake Beloye has risen to <b>guarantee</b> <b>depth</b> for ships, and ancient villages and their whole history have gone underwater. The inhabitants of the inundated areas disassembled their houses and moved away. Only the churches remained on the deserted land: the Nativity Church in Krokhino, more famous for its location, and the Church of Candlemas built {{at the end of the}} 18th century in Kovzha (at the mouth of the Kovzha River).|$|R
50|$|The Sea-Dweller was {{superseded}} by the DeepSea Sea-Dweller in late 2008, {{with the last}} 16600 Sea-Dwellers produced running into the V-series (late 2008). The DeepSea features a 44 mm case that <b>guarantees</b> a <b>depth</b> of 3900 m (ref. 116660).|$|R
50|$|The Rolex Sea-Dweller, {{introduced}} in 1971, is a heavier-duty steel {{version of the}} Submariner, with a thicker case and crystal, {{as well as a}} date feature, sans cyclops magnifier. The Sea-Dweller incorporates a helium escape valve for use when decompressing and helium is in the gas mixture of a pressurized habitat; this model (ref 16600) has a <b>guaranteed</b> waterproof <b>depth</b> of 1220 m.|$|R
40|$|Over {{the last}} centuries, Dutch rivers {{such as the}} Rhine have been heavily trained {{for the purpose of}} the safe {{discharge}} of water, sediment and ice, and navigability. After the notorious flood events of 1993 and 1995 along the Rhine, new large-scale river works were initiated, such as the Room for the River (RfR) programme, to increase flood conveyance capacity. For a better navigation, in 2006 the minimum <b>guaranteed</b> <b>depth</b> on the Waal has been raised from 2. 50 m to 2. 80 m (relative to the Agreed Low Water level). It is inevitable that the measures of the large-scale works (depending on the type and magnitude of the measure) and changes in the minimum <b>guaranteed</b> <b>depth</b> will influence the morphology of the river and the dredging effort. Three schematisations are distinguished in this research, namely maintenance dredging in the situation with a minimum <b>guaranteed</b> <b>depth</b> of 2. 5 m (‘Ref – 2. 5 m’), the situation with a minimum <b>guaranteed</b> <b>depth</b> of 2. 8 m (‘Ref – 2. 8 m’), and the situation after the implementation of the Room for the River programme (‘RfR’, with a minimum <b>guaranteed</b> <b>depth</b> of 2. 8 m). The aim of this research is to determine the impacts of the increase in the minimum <b>guaranteed</b> <b>depth</b> and the Room for the River programme on the maintenance dredging in the river Waal using the deterministic approach (traditional) and a stochastic approach, and to determine the potential of a stochastic approach with respect to the deterministic approach in river management practice. Currently, morphological calculations are being executed using a deterministic approach. The deterministic approach appears to be an effective tool to provide a quick expression of the physical morphodynamic processes. However, to fully acknowledge these morphodynamic processes and to derive a precise illustration using a deterministic model is very complex. By ignoring the complexity of the morphodynamic processes, the involved uncertainties are not made explicit. Identifying the uncertainty in morphodynamic predictions is necessary in order {{to come to grips with}} system behaviour of the Waal. Therefore, it is important to not only look at the deterministic calculation, but also to perform stochastic calculations. In this study, numerical calculations with a 2 D depth averaged Delft 3 D model are performed using a deterministic and stochastic approach to determine the bed level changes, navigability, and dredging effort. For the deterministic approach a representative discharge hydrograph is used and for the stochastic approach 75 different discharge time series. From the present research it follows that the maintenance dredging volume in the ‘Ref – 2. 8 m’ situation is twice as much as in the ‘Ref – 2. 5 m’ situation. It increases drastically with 196 %. The increase in the maintenance dredging volume in the ‘RfR’ situation compared to the ‘Ref – 2. 8 m’ situation is approximately 10 times lower than the increase in the maintenance dredging volume in the ‘Ref – 2. 8 m’ situation (which is related to the dredging effort in the ‘Ref – 2. 5 m’ situation). This concludes that the increase of the minimum <b>guaranteed</b> <b>depth</b> has a bigger impact on the maintenance dredging than the impact of the Room for the River measures. According to the simulations it also follows that the dredging volume in all sharp bends (Millingen (rkm 869 - 870), Erlecom (rkm 875 - 876) and Nijmegen (rkm 883 - 885)) in total is more than the half of the total dredging volume on the Waal. Nijmegen (rkm 883 - 886) only covers more than one-third of the total maintenance dredging volume on the Waal. As regards to the deterministic and the stochastic approach, the differences between the mean value of the stochastic approach and the deterministic approach are rather high in the entire Waal (which lies between 30 and 40 %). In the Nijmegen area this difference is negligible. Generally speaking, the mean value of the stochastic approach is not underestimated by the deterministic calculations. The large difference does not imply that the stochastic approach is more promising than the deterministic approach or vice versa. The uncertainty range (90 %-confidence band) in the stochastic approach helps the river manager to decide where and how to interfere in the river system and it helps in drawing up performance based contracts with dredging companies. The stochastic approach gives more insight in the range or likelihood of predictions if it comes to dredging. If the river manager wants to employ a dredging company (contractor) for maintenance dredging, he can sign the contract for a lower amount of money, since the mean value of the stochastic approach is lower than the results of the deterministic approach. Hydraulic structures & Flood risksHydraulic EngineeringCivil Engineering and Geoscience...|$|E
40|$|For {{any given}} partial order in a d-dimensional {{euclidean}} space, under mild regularity assumptions, {{we show that}} the intersection of closed (generalized) intervals containing more than 1 / 2 of the probability mass, is a non-empty compact interval. This property is shared with common intervals on real line, where the intersection is the median set of the underlying probability distribution. So obtained multivariate medians {{with respect to a}} partial order, can be observed as special cases of centers of distribution in the sense of type D depth functions introduced by Y. Zuo and R. Serfling, Ann. Stat., 28 (2000), 461 - 482. We show that the halfspace depth function can be realized via compact convex sets, or, for example, closed balls, in place of halfspaces, and discuss structural properties of halfspace and related depth functions and their centers. Among other things, we prove that, in general, the maximal <b>guaranteed</b> <b>depth</b> is 1 /d+ 1. As an application of these results, we provide a Jensen's type inequality for functions of several variables, with medians in place of expectations, which is an extension of the previous work by M. Merkle, Stat. Prob. Letters, 71 (2005), 277 [...] 281. Comment: 26 pages, 0 figure; Related to the Version 2, some errors are corrected, some formulations are improved; all numbered items remained, no items were added. One figure is delete...|$|E
40|$|This MSc thesis report {{discusses}} a greenfield deep-sea port {{development in}} a developing country along a shallow oceanic coast. A port layout is developed based on a fictional cargo forecast. In the interest of port adaptivity, this layout is kept flexible as possible. The design is subsequently checked with regard to wave penetration and morphology. The wave modelling shows {{that it is possible}} to achieve a sufficiently calm wave climate inside the port, by making use of the interaction between waves and the port’s approach channel. Morphological analysis indicates that a large amount of maintenance dredging will be required to keep the channel at guaranteed depth; this negatively influences the feasibility of the port development. The port layout is based on guidelines and design rules. It features a container terminal, a multi-purpose terminal, three dry bulk terminals and a liquid bulk terminal. The bulk terminals are located downwind of the other terminals. Due to long cargo dwell times, large terminal areas are required. The layout can be expanded step-wise and most terminals can be converted or re-purposed. The breakwater layout is optimised; it extends into the sea beyond the closure depth, preventing immediate bypassing of sediment. The approach channel is designed as a one-way system with a <b>guaranteed</b> <b>depth</b> of - 17 m MSL. This depth necessitates a very long dredged channel; the channel length is more than sixteen kilometres. The channel is designed completely according to PIANC guidelines. The main part of the channel is orientated in the dominant wave direction. Near the port entrance it bends and then enters the port at an angle of 55 ° with the dominant wave direction. The channel shows unexpected interactions with the long swell waves: the waves attune on the channel edge and are subsequently focused on the port entrance, enhancing wave penetration. This interaction is related to wave refraction. The refraction modes depend strongly on the critical wave angle; which is related to changes in water depth. The wave-channel interaction is thoroughly investigated. This investigation leads to measures which can reduce wave penetration. Civil Engineering and GeosciencesHydraulic Engineerin...|$|E
60|$|I {{will grant}} Gallipolis as to her costume, but firmly to Seville or Valladolid I am held by her eyes; castanets, balconies, mantillas, serenades, ambuscades, escapades--all these their dark <b>depths</b> <b>guaranteed.</b>|$|R
40|$|Enumeration of bounded size cuts is an {{important}} step in several logic synthesis algorithms such as technology mapping and re-writing. Since the standard algorithm enumerates all cuts, it does not scale beyond 6 or 7 input cuts (because there are many such cuts). We address the enumeration problem by introducing the notion of factor cuts. The set of factor cuts of a network is a small and efficiently enumerable subset of the set of all cuts that has the property that any cut of the network can be obtained easily from. In this paper we present the theory behind factor cuts and consider their use in depth-oriented FPGA mapping for large (upto 12 -input) LUTs and macrocells. In LUT mapping, we find that considering only factor cuts <b>guarantees</b> <b>depth</b> optimality for most nodes in the network. For the remaining, other cuts need to be generated from factor cuts and examined. In macrocell mapping, we focus on a particular 9 -input macrocell, and use factor cuts as a heuristic method to improve depth by reducing structural bias. Factor cuts are used to map the macrocell as a whole whenever possible instead of mapping its parts separately. In this context factor cuts enable a new quality [...] run-time tradeoff between mapping parts of the macrocell separately (poor quality), and mapping using all 9 -input cuts (long run-time) ...|$|R
40|$|Enumeration of bounded size cuts is an {{important}} step in several logic synthesis algorithms such as technology mapping and re-writing. The standard algorithm does not scale beyond 6 or 7 inputs because it enumerates all cuts and there are too many of them. We address the enumeration problem by introducing the notion of cut factorization. In cut factorization, one enumerates global and local cuts (collectively called the factor cuts) of the network, and uses these to generate other cuts. Depending on how global and local cuts are defined, one obtains different factorization schemes. In the first scheme, complete factorization, it is possible to generate any cut from factor cuts. However, complete factorization is expensive though less expensive than exhaustive enumeration. In the second scheme, partial factorization, there is no guarantee of generating all cuts from factor cuts. However, it is much faster, and produces good results. In this paper we also present two applications of factor cuts: LUT mapping and macrocell mapping. In LUT mapping, we find that considering only factor cuts <b>guarantees</b> <b>depth</b> optimality for most nodes in the network. For the remaining nodes, other cuts need to be generated from factor cuts and examined. In macrocell mapping, we focus on a particular 9 -input macrocell, and use factor cuts as a heuristic method to improve depth by reducing structural bias. Factor cuts are used to map the macrocell as a whole whenever possible instead of mapping its parts separately. In this context factor cuts enable a new quality–run-time tradeoff between mapping parts of the macrocell separately (poor quality), and mapping using all 9 -input cuts (long run-time). 1...|$|R
40|$|This thesis {{investigates the}} most cost {{effective}} dredging {{program for the}} Outer Harbour of Great Yarmouth. The dredging program consists {{of the type of}} dredger and the frequency of dredging required to maintain the safe navigable depth of - 10 mCD. In order to calculate the sedimentation volumes and analyse the scouring, a multibeam survey was carried out using RTK tides in the Outer Harbour Great Yarmouth. Volume calculations from the survey were compared with previous monthly surveys in order to produce volume calculations over a six month period. Five profile zones were identified in the Outer Harbour to assess critical shallow areas. The analysis indicated there was a significant build up of sediment over the six months and this was reflected in the profile zones and by the fifth month there were critical shallow areas. The shallow areas showed sedimentation levels above the <b>guaranteed</b> <b>depths</b> of - 10 mCD. This established that a five month dredging frequency was recommended. Analysing the method and effectiveness of different types of dredgers suggested that a trailing suction hopper dredger was found to be the most cost effective. The scour results showed a large scour pit located in front of the north arm of the breakwater and was possibly caused by the standing wave theory although further surveys and sampling will give a clearer indication. It was concluded that there is significant sediment movement in and around the outer harbour of Great Yarmouth which had caused significant scouring in front of the breakwater which could have significant implications for the structural stability of the breakwater if not managed in the future. Based on an analysis of volume calculations, it was determined the most cost effective dredging program for the Outer Harbour of Great Yarmouth would be to dredge every five months using a trailing suction hopper dredger. Faculty of Scienc...|$|R
40|$|In his {{magnificent}} book on {{the language}} relations across Bering Strait (1998), Michael Fortescue does not consider Nivkh (Gilyak) to be a Uralo-Siberian language. Elsewhere I {{have argued that the}} Indo-European verbal system can be understood in terms of its Indo-Uralic origins (2001). All of these languages belong to Joseph Greenberg’s Eurasiatic macro-family (2000). In the following I intend to reconsider the grammatical evidence for including Nivkh into the Uralo-Siberian language family. The Indo-Uralic evidence is of particular importance because it <b>guarantees</b> a time <b>depth</b> which cannot otherwise be attained...|$|R
40|$|Abstract — We apply recent {{results in}} {{queueing}} theory {{to propose a}} methodology for bounding the buffer depth and packet delay in high radix interconnection networks. While most work in interconnection networks {{has been focused on}} the throughput and average latency in such systems, few studies have been done providing statistical <b>guarantees</b> for buffer <b>depth</b> and packet delays. These parameters are key in the design and performance of a network. We present a methodology for calculating such bounds for a practical high radix network and through extensive simulations show its effectiveness for both bursty and non-bursty injection traffic. Our results suggest that modest speedups and buffer depths enable reliable networks without flow control to be constructed...|$|R
40|$|The depth-bounded {{fragment}} of the pi-calculus is an expressive class of systems enjoying decidability of some important verification problems. Unfortunately {{membership of the}} fragment is undecidable. We propose a novel type system, parameterised over a finite forest, that formalises name usage by pi-terms {{in a manner that}} respects the forest. Type checking is decidable and type inference is computable; furthermore typable pi-terms are <b>guaranteed</b> to be <b>depth</b> bounded. The second contribution of the paper is a proof of equivalence between the semantics of typable terms and nested data class memory automata, a class of automata over data words. We believe this connection can help to establish new links between the rich theory of infinite-alphabet automata and nominal calculi. Comment: 19 page...|$|R
40|$|Floristic and phytosociological studies {{undertaken}} in six {{areas of the}} state of Pernambuco were selected with the aim of analyzing the Relative Importance of the woody medicinal plant species of Pernambuco's caatinga from an ethnobotanical perspective. For the data analysis, only those identified up to the species level were selected and information on medicinal properties was obtained for each one from the literature. The Relative Importance was calculated for each species. From the 57 woody species, 22 had therapeutic indications; from these, Anacardium occidentale L., Tabebuia impetiginosa (Mart. ex DC) Standley, Schinopsis brasiliensis Engl., and Myracrodruon urundeuva (Engl.) Fr. All. had the greatest values of Relative Importance. The correlation analysis made clear that the Relative Importance of the species is negatively correlated with the Density and Relative Frequency (p< 0. 05). The most important species, in the ethnobotanical point view, are the most vulnerable, possible due to the systematic exploration they have been suffering. Schinopsis brasiliensis and Myracrodruon urundeuva are listed as endangered species and deserve special attention in the development of techniques of sustainable management, where both economic return and species conservation must be <b>guaranteed.</b> In <b>depth</b> studies that take into consideration each region's characteristics are necessary both from a floristic perspective and considering medicinal aspects, since each area seems to have its own woody medicinal flora, as suggested in the cluster analysis...|$|R
40|$|The {{significant}} {{growth in}} offshore wind activities in boulder clay areas requires suitable cable protection solutions. It is desirable {{to create a}} trench prior installation of the cable in order to <b>guarantee</b> the target <b>depth</b> of burial. Modern equipment of available on the market can be refined {{in order to create}} a reduced footprint, both economical and environmental. For this thesis there is looked into an innovative solution to create a pre-trench by utilization of a classic dredging vessel. Several alternatives are investigated. However, eventually two different mechanical cutting methods were selected for detailed investigation as a consequence of a multi criterion analysis. One uses counter rotating shredders and the other concept consists of an inclined screw conveyor. After physical considerations it turns out the screw conveyor has the best potential and is therefore subjected to a detailed design. Mechanical, Maritime and Materials EngineeringMarine and Transport TechnologyOffshore and Dredging Engineerin...|$|R
40|$|A query {{result of}} an XML keyword search is usually defined {{as a set of}} the most {{specific}} elements containing all query keywords. Search systems find the query result by considering the combinations of all elements in the inverted indexes of the query keywords. However, we conclude that {{it is not necessary to}} consider the combinations of all the elements, when an “effective result depth” (which represents how deeply nested elements are eligible for the query result) is given. This paper describes a way to construct partitioned indexes on the effective result <b>depth,</b> <b>guaranteeing</b> that the combinations of elements in different partitions never produce result elements. Therefore, search systems can find query results by considering only combinations of elements in the same partitions. Partitioned indexes are adaptable; when an effective result depth is changed, partitioned indexes constructed on the original depth can be used efficiently without being reconstructed physically on the changed depth. The experimental results show that our approach worked quite well in most cases...|$|R
40|$|Figure 1 : A {{collection}} of appearance-mimicking surfaces generated with our algorithm. We consider {{the problem of}} reproducing the look and {{the details of a}} 3 D object on a surface that is confined to a given volume. Classic examples of such “appearance-mimicking ” surfaces are bas-reliefs: decorations and artwork depicting recognizable 3 D scenes using only a thin volumetric space. The design of bas-reliefs has fascinated humankind for millennia and it is extensively used on coins, medals, pottery and other art forms. We propose a unified framework to cre-ate surfaces that depict certain shapes from prescribed viewpoints, as a generalization of bas-reliefs. Given target shapes, viewpoints and space restrictions, our method finds a globally optimal surface that delivers the desired appearance when observed from the designated viewpoints, while <b>guaranteeing</b> exact, per-vertex <b>depth</b> bounds. We use 3 D printing to validate our approach and demonstrate our results in a variety of applications, ranging from standard bas-reliefs to optical illusions and carving of complex geometries...|$|R
40|$|In thispaper {{we study}} the {{technology}} mappingproblem for FPGA architectures to minimize chip area, or {{the total number}} of lookup tables (LlJTs) ofthe mapped design, under the chip performance constraint. This is a well-studied topic and a very diflcult task (NP-hard) The contributions of this paper are as Jollaws: (i) we consider the potential node duplications during the CUI enumeration/generotion procedure so the mapping costs encoded in the cuts drive the area-optimization objective more effectively: (iij affer the timing constraint is determined, we will relax the non-critical paths by searching the solution space considering both local andglobal optimality information to minimize mapping area; (iiij an iterative cut selection procedure is carried out that further explores and perturbs the solution space to improve solution quality. We <b>guarantee</b> optimal mapping <b>depth</b> under the unit delay model. Experimental results show that our mapping algorithm, named DAOmap, produces significant quality and run-time improvements. Compared to the stote-o/-the-art depth-optimal, area minimization mapping algorithm CutMap [21], DAOmap is 16. 02 % better on area and runs 24. 2 X faster on average when both algorithms are mapping to FPGAs using LWs oJfive inputs. LUTs of other inputs are also used for comparisons...|$|R
40|$|The {{research}} conducted sheds {{a light on}} the question why robust in-process monitoring and adaptive control are not fully implemented in the welding industry. In the research project FaRoMonitA, the possibilities to monitor the weld quality during welding have been investigated. Research conducted in this area has merely focused on technical issues investigated in a laboratory environment. To advance the research front and release some barriers related to industrial acceptance, the studies conducted in this paper have been both quantitative and qualitative in form of experiments combined with an interview study. The quality property weld penetration depth was chosen for in-process monitoring to evaluate the industrial relevance and applicability. A <b>guaranteed</b> weld penetration <b>depth</b> is critical for companies producing parts influenced by fatigue. The parts studied were fillet welds produced by gas metal arc welding. The experiments {{show that there is}} a relationship between final penetration depth and monitored arc voltage signals and images captured by CMOS vision and infrared cameras during welding. There are still technical issues to be solved to reach a robust solution. The interview study indicates that soft issues, like competence and financial aspects, are just as critical...|$|R
40|$|The {{optimization}} of the bathymetric resurvey {{policy of}} the Netherlands Hydrographic Service requires insight into sea floor dynamics in the Southern North Sea. To study the spatial variations in sea floor dynamics, the bathymetric archives of the Netherlands Hydrographic Service are analyzed using deformation analysis, a statistical and innovative approach for bathymetric data. Based on {{the uncertainty of the}} data, our implementation of deformation analysis selects the significant spatial and temporal parameters, and provides estimates and their uncertainties for those parameters. We focus on sand wave areas in the regions of Rotterdam and of Amsterdam. In those areas, dredging takes place to <b>guarantee</b> a minimum <b>depth.</b> The results reveal a difference in sand wave migration between the two regions, over the past two decades. The dominant wavelengths of the sand waves vary within the regions, but we find a similar wavelength distribution for the two regions. We compare our results to earlier studies of the same sand wave areas in the Rotterdam region, showing similar migration rates, but different wavelengths. It is concluded, based on sand wave dynamics alone, that the Amsterdam region should be assigned a higher resurvey frequency than the Rotterdam region. ...|$|R
40|$|In {{this paper}} we study the {{technology}} mapping problem for FPGA architectures to minimize chip area, or {{the total number}} of lookup tables (LUTs) of the mapped design, under the chip performance constraint. This is a well-studied topic and a very difficult task (NP-hard). The contributions of this paper are as follows: (i) we consider the potential node duplications during the cut enumeration/generation procedure so the mapping costs encoded in the cuts drive the area-optimization objective more effectively; (ii) after the timing constraint is determined, we will relax the non-critical paths by searching the solution space considering both local and global optimality information to minimize mapping area; (iii) an iterative cut selection procedure is carried out that further explores and perturbs the solution space to improve solution quality. We <b>guarantee</b> optimal mapping <b>depth</b> under the unit delay model. Experimental results show that our mapping algorithm, named DAOmap, produces significant quality and runtime improvements. Compared to the state-of-the-art depthoptimal, area minimization mapping algorithm CutMap [21], DAOmap is 16. 02 % better on area and runs 24. 2 X faster on average when both algorithms are mapping to FPGAs using LUTs of five inputs. LUTs of other inputs are also used for comparisons. 1...|$|R
40|$|In {{programming}} languages with dynamic use of memory, such as Java, {{knowing that}} a reference variable x {{points to an}} acyclic data structure is valuable {{for the analysis of}} termination and resource usage (e. g., execution time or memory consumption). For instance, this information <b>guarantees</b> that the <b>depth</b> of the data structure to which x points is greater than the depth of the data structure pointed to by x. f for any field f of x. This, in turn, allows bounding the number of iterations of a loop which traverses the structure by its depth, which is essential in order to prove the termination or infer the resource usage of the loop. The present paper provides an Abstract-Interpretation-based formalization of a static analysis for inferring acyclicity, which works on the reduced product of two abstract domains: reachability, which models the property that the location pointed to by a variable w can be reached by dereferencing another variable v (in this case, v is said to reach w); and cyclicity, modeling the property that v can point to a cyclic data structure. The analysis is proven to be sound and optimal with respect to the chosen abstraction. Comment: 38 pages (included proofs...|$|R
40|$|Immersive media content {{provides}} a more natural representation of real world information compared to conventional two-dimensional video. In the near future, video applications {{will be replaced by}} immersive media with the technological advancements of three-dimensional video capture technologies, efficient multi-view compression algorithms, ever increasing bandwidth of communication links and three-dimensional video display techniques. This paper envisages an application scenario of stereoscopic TV over IP networks. The colour and depth/disparity videos are considered as the main source of stereoscopic video material. Furthermore, this paper proposes a methodology to encode colour and depth video asymmetrically using the layered architecture of the scalable extension of H. 264 /AVC. The proposed asymmetric coding method achieves high image quality for the rendered left and right videos at low overall bitrates compared to the bitrate requirements of existing IP TV applications. The proposed method can be used to scale existing IP TV applications into stereoscopic TV with a minimum overhead increase while <b>guaranteeing</b> high quality <b>depth</b> perception. Finally the performance of stereoscopic TV content over IP networks is analyzed. This concludes that the image quality of the rendered stereoscopic video over IP can be vastly improved by prioritizing the colour video packets ahead of depth video packets. ...|$|R
40|$|Abstract—Matching cost {{aggregation}} {{is one of}} {{the oldest}} and still pop-ular methods for stereo correspondence. While effective and efficient, cost aggregation methods typically aggregate the matching cost by summing/averaging over a user-specified, local support region. This is obviously only locally-optimal, and the computational complexity of the full-kernel implementation usually depends on the region size. In this paper, the cost aggregation problem is re-examined and a non-local solution is proposed. The matching cost values are aggregated adaptively based on pixel similarity on a tree structure derived from the stereo image pair to preserve depth edges. The nodes of this tree are all the image pixels, and the edges are all the edges between the nearest neighboring pixels. The similarity between any two pixels is decided by their shortest distance on the tree. The proposed method is non-local as every node receives supports from all other nodes on the tree. The proposed method can be naturally extended to the time domain for enforcing temporal coherence. Unlike previous methods, the non-local property <b>guarantees</b> that the <b>depth</b> edges will be preserved when the temporal coherency between all the video frames are considered. A non-local weighted median filter is also proposed based on the non-local cost aggregation algorithm. It has been demonstrated to outperform all local weighted median filters on disparity/depth upsampling and refinement...|$|R
40|$|In heap-based languages, {{knowing that}} a {{variable}} x {{points to an}} acyclic data structure is useful for analyzing termination. This information <b>guarantees</b> that the <b>depth</b> of the data structure to which x points {{is greater than the}} depth of the structure pointed to by x. fld, and allows bounding the number of iterations of a loop that traverses the data structure on fld. In general, proving termination needs acyclicity, unless program-specific or nonautomated reasoning is performed. However, recent work could prove that certain loops terminate even without inferring acyclicity, because they traverse data structures “acyclically. ” Consider a double-linked list: if it is possible to demon-strate that every cycle involves both the “next ” and the “prev ” field, then a traversal on “next ” terminates since no cycle will be traversed completely. This article develops a static analysis inferring field-sensitive reachability and cyclicity information, which is more general than existing approaches. Propositional formulæ are computed, which describe which fields {{may or may not be}} traversed by paths in the heap. Consider a tree with edges “left ” and “right ” to the left and right subtrees, and “parent ” to the parent node: termination of a loop traversing leaf-up cannot be guaranteed by state-of-the-art analyses. Instead, propositional formulæ computed by this analysis indicate that cycles must traverse “parent ” and at least one between “left ” and “right”: termination is guaranteed, a...|$|R
40|$|Figure 1 : We {{present a}} novel {{technique}} for seamlessly cloning content from one stereoscopic image pair to another. Given a synthetic 3 D SIGGRAPH Asia 2012 logo {{as the source}} image pair and a target stereoscopic image pair of a bumpy wall (a), we use perspective-aware warping to adjust {{the structure of the}} logo and paste it on to the bumpy wall (c). The perceived depth and projection of the pasted logo (b) are adjusted locally and adaptively to fit onto the bumpy surface. (Note that the resultant left and right images are included in the supplemental materials. It is recommended to watch them with stereoscopic displays for better visual effects.) This paper presents a novel technique for seamless stereoscopic image cloning, which performs both shape adjustment and color blending such that the stereoscopic composite is seamless in both the perceived depth and color appearance. The core of the proposed method is an iterative disparity adaptation process which alternates between two steps: disparity estimation, which re-estimates the disparities in the gradient domain so that the disparities are continuous across the boundary of the cloned region; and perspectiveaware warping, which locally re-adjusts the shape and size of the cloned region according to the estimated disparities. This process <b>guarantees</b> not only <b>depth</b> continuity across the boundary but also models local perspective projection in accordance with the disparities, leading to more natural stereoscopic composites. The proposed method allows for easy cloning of objects with intricate silhouettes and vague boundaries because it does not require precise segmentation of the objects. Several challenging cases are demonstrated to show that our method generates more compelling results compared to methods with only global shape adjustment...|$|R
40|$|Safe {{nautical}} charts {{require a}} carefully designed bathymetric survey policy, especially in shallow sandy seas that potentially have dynamic sea floor patterns. Bathymetric resurveying at sea is a costly process with limited resources, though. A {{pattern on the}} sea floor known as tidal sand waves is clearly present in bathymetric surveys, endangering navigation in the Southern North Sea {{because of the potential}} dynamics of this pattern. An important factor in an efficient resurvey policy is the type and size of sea floor dynamics. The uncertainties of measurement and interpolation associated with the depth values enable the statistical processing of a time series of surveys, using deformation analysis. Currently, there is no procedure available that satisfies the Royal Netherlands Navy requirements. Therefore, a deformation analysis procedure is designed, implemented and tested {{in such a way that}} the procedure works on bathymetric data and satisfies the Royal Netherlands Navy requirements. Also, it is necessary to develop a procedure that translates the results into changes of the resurvey policy, taking into account their confidence intervals. To describe the sea floor statistically, we assume the sea floor to consist of a spatial trend function (or characterization) and a residual function (or dispersion). Such a description is called a representation. The covariances between positions are expressed in a covariance function, based on the residual function. The covariance function is used by Kriging, an interpolation procedure that propagates the variances and covariances of the data points to variances of the interpolated values. This approach is used widely for spatial analyses, like the interpolation of a bathymetric data set. The method that we propose uses Kriging to produce a time series of grids of depth values and their variances. Subsequently, it uses deformation analysis, a statistical procedure based on testing theory. Our application of deformation analysis is particularly aimed at the detection of dynamics in areas with tidal sand waves, resulting in parameter estimates for the sea floor dynamics, and their uncertainty. We apply the method to sea floor representations both with and without a sand wave pattern. A test scenario is set up, consisting of a survey of an existing area in the Southern North Sea, for which dynamics are simulated. The results show that the proposed method detects different types of sea floor dynamics well, leading to satisfactory estimates of the corresponding parameters. We show results for the anchorage area Maas West near the Port of Rotterdam, the Netherlands first. The area is divided into 18 subareas. The results show that a sand wave pattern is detected for most of the subareas, and a shore ward migration is detected for a majority of them. The estimated migration rates of the sand waves are up to 7. 5 m/yr, with a 95 % confidence interval that depends on the regularity of the pattern. The results are in confirmation with previously observed migration rates for the Southern North Sea, and with an idealized process-based model. Thereafter, we analyze several other areas for which a time series of surveys is available in the bathymetric archives of the Netherlands Hydrographic Service, to study the spatial variations in sea floor dynamics. We present results for several sand wave areas and a single flat area. In some of those areas, dredging takes place, to <b>guarantee</b> minimum <b>depths.</b> The results indicate sand wave migration in areas close to the coast, and bed level changes of the order of decimeters. The dominant wavelength of the sand waves varies. We compare our results to literature of the same sand wave areas, in which we find similar migration rates, and different wavelengths. By formulating four indicators, recommendations are made for the resurvey policy on the Belgian and Netherlands Continental Shelf. These indicators follow from the estimates for sea floor dynamics. We present a concept for the shallowest likely depth surface, on which we base two of the indicators. The other two indicators act as a warning: they quantify the potentially missed dynamics, which makes the procedure more robust in case of complicated morphology. We show clear differences in recommended resurvey frequency between the five analyzed regions. We conclude that the designed method is able to use a time series of bathymetric surveys for the estimation of sea floor dynamics in a satisfactory way. Those dynamics may be present on the scale of the sea floor, it may be a local effect, or it may be due to a tidal sand wave pattern. Also, the results are successfully reduced to a set of four indicators, used to improve a resurvey policy. Based on these conclusions, we formulate recommendations on the extrapolation of the results in space and time, on potential adaptations to the designed procedure, and on implementation of the procedure...|$|R
40|$|Siltation of harbour basins and {{navigation}} channels {{is a serious}} problem in the port of Rotterdam as well in many other harbours all over the world. Due to siltation, basins and channels require frequent maintenance dredging to <b>guarantee</b> safe navigational <b>depths.</b> The costs associated with these dredging activities are quite high. To keep the channels and harbours in Rotterdam navigable, Rijkswaterstaat and the Port of Rotterdam are dredging approximately 15 million m 3 of sediment a year. The dredging cost of the Botlek Harbour only is already about 3 million Euros a year. It is a task to keep the costs in the Port of Rotterdam as low as possible to compete with other ports. Reducing maintenance dredging costs is in line with the goal of the Port of Rotterdam to be the most competitive, innovative and sustainable port in the world. Most sedimentation within the maintenance area of the Port of Rotterdam occurs in the Botlek. According data, between 1. 5 and 3 million m 3 /year is dredged in the Botlek Harbour. Although the current dredging philosophy more or less works, the question arises whether there are solutions that are more cost-effective. However, the problem is so complex that it narrowed down for the sake of research quality. The main causes of siltation in general and specifically in the Botlek Area form {{an important part of the}} study. Hydrodymical models (SIMONA & Delft 3 D), were used to gain insight in the sedimentation problem. The focus in this thesis was more on the hydrodynamics. The exchange mechanisms between the river and Botlek Harbour were investigated, which were needed to examine the effectiveness of certain solutions. In practice a lot of solutions are proposed in literature, however in this study only a couple of ‘hard’ measures are investigated. The first possible solution that was examined was the use of a Current Deflecting Wall. It turned out that the hydrodynamics were very sensitive to the configuration of the CDW. While sometimes it would lower the exchange flow, at other cases it would even make the problem worse. The second solution was to make a gap in the Geulhaven dam. However this was not a good solution as high exchange flows occurred. The last proposed solution, the filling of the underwater dam, seemed more feasible as it would decrease the exchange flow according the numerical models. The research has first order results which can be used in further studies. According to this results, certain solutions will decrease the exchange flows. On turn it would very likely result in lower sedimentation rates in the Botlek Area. It is expected that some CDW configurations and the filling of the underwater dam would have a positive effect when it comes to sedimentation. However, this research is the first step of an extensive study that must made to deal with the problem. First of all many things can be done to improve the models, for example by using a higher spatial resolution. Secondly, other sets of conditions must be modelled to see what kind of effect this has on exchange flows. In addition, sediment must be included in the models to have more insight on the sedimentation itself. The next step would be a feasibility study, including a cost benefit analysis. It would be wise to improve the models further and to make a scale model for the most feasible solution. In the ideal case, were all steps are positive and hard conclusion can be made, it would be a good idea for the Port of Rotterdam to start a pilot. Hydraulic EngineeringHydraulic EngineeringCivil Engineering and Geoscience...|$|R

