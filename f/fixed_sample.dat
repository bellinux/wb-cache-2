420|1258|Public
5000|$|With insufficiently large samples, the approach: <b>fixed</b> <b>sample</b> - random {{properties}} suggests inference {{procedures in}} three steps: ...|$|E
5000|$|Instruments are {{not limited}} to a <b>fixed</b> <b>sample</b> rate for a given note. The format stores the instrument's sample rate at middle C.|$|E
50|$|EastClinical trial {{statistical}} {{software for the}} design, simulation and monitoring of adaptive, group sequential and <b>fixed</b> <b>sample</b> size trials. As of 2013, East 6.4 is in use at over 140 pharmaceutical and biotechnology companies, research centers and regulatory agencies including the FDA’s Center for Drug Evaluation and Research, Center for Biologics Evaluation and Research and Center for Devices and Radiological Health divisions.|$|E
40|$|International audienceThe {{purpose of}} this study is to make a {{comparison}} between the fluorescence emissions of fresh extracted human biopsies and fixed human biopsies, in order to evaluate the impact of fixation on autofluoresence signal. Our group is developing an endo-microscope to image brain tissues in-vivo, however to date, in order to validate our technology the easiest type of samples we can access are <b>fixed</b> <b>samples.</b> However, the fixation is still challenging. For that, we aim through this study to determine whether we should pursue to work on <b>fixed</b> <b>samples</b> or we should shift to work on fresh biopsies. Data were collected on spectroscopic, lifetime measurement and fluorescence imaging setups with visible and two-photon excitations wavelengths. Five fresh and five <b>fixed</b> <b>samples</b> are involved in the experiment. Endogenous fluorescence of fixed biopsies were calculated. Experimental results reveal that at 405 nm and 810 nm, the fresh samples have an intensity of fluorescence two times higher than that of <b>fixed</b> <b>samples.</b> However, for each fluorophore and each excitation wavelength, the lifetime for fresh samples is shorter than that for <b>fixed</b> <b>samples.</b> Still, further studies and investigations involving the comparison between different samples are required to strengthen our findings...|$|R
40|$|MRI {{studies of}} chemically-fixed {{biological}} samples {{have become increasingly}} common. The fixatives typically used for MRI investigations, paraformaldehyde and gluteraldehyde, achieve fixation by cross-linking protein amino groups with methylene bridges to render tissues metabolically inactive and structurally stable [1]. These sample properties permit long scan times and thus, <b>fixed</b> <b>samples</b> are well-suited to high-resolution, multidimensional acquisition schemes with MR images that are devoid of motion or flow artifacts. Further, with <b>fixed</b> <b>samples,</b> it is often possible t...|$|R
40|$|In recent years, the {{increased}} sensitivity of electron detectors {{and the availability}} of low-vacuum or variable-pressure systems have allowed imaging of fresh tissue samples without the need for fixation, drying, and coating. This obviously saves a lot of time, although the image quality may not be as good as that obtained from <b>fixed</b> <b>samples.</b> However, for most applications that tend to be at a relatively low magnification, the quality can be as good as that obtained from <b>fixed</b> <b>samples...</b>|$|R
50|$|Sieve estimators {{have been}} used {{extensively}} for estimating density functions in high-dimensional spaces such as in Positron emission tomography(PET). The first exploitation of Sieves in PET for solving the maximum-likelihood Positron emission tomography#Image reconstruction problem was by Donald Snyder and Michael Miller, where they stabilized the time-of-flight PET problem originally solved by Shepp and Vardi. Shepp and Vardi's introduction of Maximum-likelihood estimators in emission tomography exploited {{the use of the}} Expectation-Maximization algorithm, which as it ascended towards the maximum-likelihood estimator developed a series of artifacts associated {{to the fact that the}} underlying emission density was of too high a dimension for any <b>fixed</b> <b>sample</b> size of Poisson measured counts. Grenander's method of sieves was used to stabilize the estimator, so that for any <b>fixed</b> <b>sample</b> size a resolution could be set which was consistent for the number of counts. As the observe PET imaging time would go to infinity, the dimension of the sieve would increase as well in such a manner that the density was appropriate for each sample size.|$|E
50|$|Cryo-scanning {{electron}} microscopy (CSEM) {{is a form}} of {{electron microscopy}} where a hydrated but cryogenically <b>fixed</b> <b>sample</b> is imaged on a scanning electron microscope's cold stage in a cryogenic chamber. The cooling is usually achieved with liquid nitrogen. Cryo-SEM of biological samples with a high moisture content can be done faster with fewer sample preparation steps than conventional SEM. In addition, the dehydration processes needed to prepare a biological sample for a conventional SEM chamber create numerous distortions in the tissue leading to structural artifacts during imaging.|$|E
5000|$|The c-chart {{differs from}} the p-chart in that it {{accounts}} {{for the possibility of}} more than one nonconformity per inspection unit, and that (unlike the p-chart and u-chart) it requires a <b>fixed</b> <b>sample</b> size. The p-chart models [...] "pass"/"fail"-type inspection only, while the c-chart (and u-chart) give the ability to distinguish between (for example) 2 items which fail inspection because of one fault each and the same two items failing inspection with 5 faults each; in the former case, the p-chart will show two non-conformant items, while the c-chart will show 10 faults.|$|E
40|$|In {{this paper}} we {{investigate}} {{the effects of}} different ranked set sampling protocols on the sign test statistic. Sampling protocols considered include sequential, mid-range and <b>fixed</b> <b>sampling</b> designs. We show that {{in all of these}} sampling protocols the introduction of any correlation structure on the quantified observations leads to a reduction in the Pitman efficiency of the sign test. In the <b>fixed</b> <b>sampling</b> protocol, design optimality is achieved when only the middle observation is quantified from each set. Nonparametric test Sampling design Pitman efficiency Optimal allocation Sequential sampling...|$|R
40|$|Methods for {{measuring}} concentrations and emission rates of particulate matter (PM) from mechanically ventilated livestock buildings were evaluated {{in a laboratory}} facility and in a swine-finishing barn. Concentrations of PM were measured inside the room (room sam-pling) and at the exhaust duct (exhaust sam-pling). Concentrations at the exhaust duct were determined using high-volume traverse downstream of the exhaust fan, low-volume traverse downstream of the fan, and <b>fixed</b> <b>sampling</b> upstream and downstream of the fan. The traverse methods, which served as the reference, were conducted under isokinetic conditions; <b>fixed</b> <b>sampling</b> was done under both isokinetic and sub-isokinetic conditions. Compared to the traverse method, both room sampling and exhaust sampling under sub-isokinetic conditions overestimated PM con-centrations. <b>Fixed</b> <b>sampling</b> under isokinetic conditions, on the other hand, {{did not differ significantly}} (P> 0. 05) from the high-volume traverse method. Thus, isokinetic fixed sam-pling can be an alternative to the more expen-sive and time-consuming high-volume PM traverse method to measure PM concentra-tions and emission rates at the exhaust...|$|R
30|$|For <b>fixed</b> <b>sampling</b> scheme, n, m and {{dependence}} structure θ_ 0, the ACIs are stable than the Boot-P CIs, they can maintain their coverage percentages at the pre-fixed normal level.|$|R
50|$|The {{operating}} characteristic (OC) curve is {{the probability that}} the null hypothesis is accepted when it is true. The OC curve characterizes the probabilities of both type I and II errors. Risk curves for model builder's risk and model user's can be developed from the OC curves. Comparing curves with <b>fixed</b> <b>sample</b> size tradeoffs between model builder's risk and model user's risk can be seen easily in the risk curves. If model builder's risk, model user's risk, and the upper and lower limits for the range of accuracy are all specified then the sample size needed can be calculated.|$|E
50|$|GarageBand is {{a digital}} audio {{workstation}} (DAW) and music sequencer that can record and play back multiple tracks of audio. Built-in audio filters {{that use the}} AU (audio unit) standard allow the user to enhance the audio track with various effects, including reverb, echo, and distortion amongst others. GarageBand also offers the ability to record at both 16-bit and 24-bit Audio Resolution, but at a <b>fixed</b> <b>sample</b> rate of 44.1 kHz. An included tuning system helps with pitch correction and can effectively imitate the Auto-Tune effect when tuned to the maximum level. It also has a large array of preset effects to choose from, with an option {{to create your own}} effects.|$|E
5000|$|Electron {{microscopy}} observations do {{confirm the}} model prediction of discrete cone angles. Two experimental artifacts {{must be considered}} though: (i) charging of the poorly conducting carbon samples under electron beam, which blurs the images and (ii) that electron microscopy observations at a <b>fixed</b> <b>sample</b> tilt only yield a two-dimensional projection whereas a 3D shape is required. The first obstacle is overcome by coating the cones with a metal layer of a few nanometers thickness. The second problem is solved through a geometrical shape analysis. Combined with significant statistics {{on the number of}} cones, it yields semi-discrete apex angles. Their values deviate from prediction by about 10% due to the limited measurement accuracy and slight variation of the cone thickness along its length.|$|E
40|$|Includes bibliographical {{references}} (pages 45 - 48) Proposed {{standards in}} the October 17, 1975 Federal Register (OSHA) require quarterly or monthly monitoring of workers' exposure levels to airborne beryllium contaminants. The standard does not specify the technique {{to be used for}} sample collection, therefore allowing industrial hygienists to use their discretion in choosing the best method for the particular situation. It is assumed that personal breathing zone samples provide the most accurate measurement of worker exposure, whereas <b>fixed</b> <b>sampling</b> techniques are used to indicate the average room concentration. In multiple machine operations, the use of personal sampling units to monitor worker exposure levels would take considerable time and money if performed on a monthly basis. In this situation it would be valuable to utilize <b>fixed</b> head <b>sampling</b> techniques which would monitor all machining processes simultaneously. An experiment was designed to determine if {{there was a significant difference}} between sampling techniques applied to a particular beryllium operation. Simultaneous samples were collected using both a personal sampling unit and a <b>fixed</b> <b>sampling</b> system. Prior testing and sample collection were used to determine the location for the <b>fixed</b> <b>sampling</b> head. A personal sampler was used to monitor the operator's breathing zone as specified in the NIOSH Analytical Methods. The mean concentrations obtained by both sampling techniques were then subjected to statistical analysis for determination of significant difference. The results of the comparison indicated that the mean concentration obtained using the <b>fixed</b> <b>sampling</b> system was significantly different from the mean concentration obtained using the personal sampling unit. Variables considered responsible for the difference in results include placement of the fixed head, worker mobility, other sources of contaminant generation within the room, type of fixed system, filter media and flowrate. These findings led to the conclusion that further research on <b>fixed</b> <b>sampling</b> techniques and their ability to measure actual worker exposure levels was necessary before relying upon them as a substitute for personal sampling...|$|R
30|$|From Tables  1, 2 and 3, the {{observations}} can be made. For <b>fixed</b> <b>sampling</b> scheme, {{sample size n}} and dependence structure θ_ 0, the MSEs and RABias decrease as the effective sample size m increase.|$|R
40|$|Rapid on-site {{evaluation}} (ROSE) {{can improve}} adequacy rates of fine-needle aspiration biopsy (FNAB) but increases operational costs. The performance of ROSE relative to <b>fixed</b> <b>sampling</b> depends on many factors. It {{is not clear}} when ROSE is less costly than <b>sampling</b> with a <b>fixed</b> number of needle passes. The objective {{of this study was}} to determine the conditions under which ROSE is less costly than <b>fixed</b> <b>sampling.</b> Cost comparison of sampling with and without ROSE using mathematical modeling. Models were based on a societal perspective and used a mechanistic, micro-costing approach. <b>Sampling</b> policies (ROSE, <b>fixed)</b> were compared using the difference in total expected costs per case. Scenarios were based on procedure complexity (palpation-guided or image-guided), adequacy rates (low, high) and sampling protocols (stopping criteria for ROSE and <b>fixed</b> <b>sampling).</b> One-way, probabilistic, and scenario-based sensitivity analysis was performed to determine which variables had the greatest influence on the cost difference. ROSE is favored relative to <b>fixed</b> <b>sampling</b> under the following conditions: (1) the cytologist is accurate, (2) the total variable cost ($/hr) is low, (3) fixed costs ($/procedure) are high, (4) the setup time is long, (5) the time between needle passes for ROSE is low, (6) when the per-pass adequacy rate is low, and (7) ROSE stops after observing one adequate sample. The model is most sensitive to variation in the fixed cost, the per-pass adequacy rate, and the time per needle pass with ROSE. Mathematical modeling can be used to predict the difference in cost between sampling with and without ROSE...|$|R
5000|$|In {{probability}} theory and statistics, the negative hypergeometric distribution describes probabilities for when sampling from a finite population without replacement {{in which each}} sample can be classified into two mutually exclusive categories like Pass/Fail, Male/Female or Employed/Unemployed. As random selections are made from the population, each subsequent draw decreases the population causing the probability of success to change with each draw. Unlike the standard hypergeometric distribution, which describes {{the number of successes}} in a <b>fixed</b> <b>sample</b> size, in the negative hypergeometric distribution, samples are drawn until [...] failures have been found, and the distribution describes the probability of finding [...] successes in such a sample. In other words, the negative hypergeometric distribution describes the likelihood of [...] successes in a sample with exactly [...] failures.|$|E
50|$|Likelihood is {{a synonym}} for {{probability}} in common usage. In statistics it is reserved for probabilities that fail to meet the frequentist definition. A probability refers to variable data for a fixed hypothesis while a likelihood refers to variable hypotheses for a fixed set of data. Repeated measurements of a fixed length with a ruler generate a set of observations. Each fixed set of observational conditions {{is associated with a}} probability distribution and each set of observations can be interpreted as a sample from that distribution - the frequentist view of probability. Alternatively a set of observations may result from sampling any of a number of distributions (each resulting from a set of observational conditions). The probabilistic relationship between a <b>fixed</b> <b>sample</b> and a variable distribution (resulting from a variable hypothesis) is termed likelihood - a Bayesian view of probability. A set of length measurements may imply readings taken by careful, sober, rested, motivated observers in good lighting.|$|E
5000|$|Cluster {{sampling}} is {{a sampling}} plan used when mutually homogeneous yet internally heterogeneous groupings are evident in a statistical population. It is often used in marketing research. In this sampling plan, the total population is divided into these groups (known as clusters) and a simple random sample of the groups is selected. The elements in each cluster are then sampled. If all elements in each sampled cluster are sampled, then this {{is referred to as}} a [...] "one-stage" [...] cluster sampling plan. If a simple random subsample of elements is selected within each of these groups, this is referred to as a [...] "two-stage" [...] cluster sampling plan. A common motivation for cluster sampling is to reduce the total number of interviews and costs given the desired accuracy. For a <b>fixed</b> <b>sample</b> size, the expected random error is smaller when most of the variation in the population is present internally within the groups, and not between the groups.|$|E
40|$|Frequency {{deviation}} is {{a common}} problem for power system signal processing. Many power system measurements are carried out in a <b>fixed</b> <b>sampling</b> rate assuming the system operates in its nominal frequency (50 or 60 Hz). However, the actual frequency may deviate from the normal value {{from time to time}} due to various reasons such as disturbances and subsequent system transients. Measurement of signals based on a <b>fixed</b> <b>sampling</b> rate may introduce errors under such situations. In order to achieve high precision signal measurement appropriate algorithms need to be employed to reduce the impact from frequency deviation in the power system data acquisition process. This paper proposes an advanced algorithm to enhance Fourier transform for power system signal processing. The algorithm is able to effectively correct frequency deviation under <b>fixed</b> <b>sampling</b> rate. Accurate measurement of power system signals is essential for the secure and reliable operation of power systems. The algorithm is readily applicable to such occasions where signal processing is affected by frequency deviation. Both mathematical proof and numerical simulation are given in this paper to illustrate robustness and effectiveness of the proposed algorithm. Crown Copyright (C) 2003 Published by Elsevier Science B. V. All rights reserved...|$|R
30|$|Abundances of Synechococcus and picoeukaryotes were {{determined}} in glutaraldehyde (1 % final concentration) <b>fixed</b> <b>samples.</b> All samples were frozen instantly in liquid nitrogen. Population was identified on FACSCalibur (Becton-Dickinson Biosciences, Franklin Lakes, NJ, USA) flow cytometer according to population fluorescence and light scatter characteristics (Vaulot et al. 1990).|$|R
40|$|In modern {{digital audio}} applications, a {{continuous}} audio signal stream is <b>sampled</b> at a <b>fixed</b> <b>sampling</b> rate, {{which is always}} greater than twice the highest frequency of the input signal, to prevent aliasing. A more energy efficient approach is to dynamically change the sampling rate based on the input signal. In the dynamic sampling rate technique, fewer samples are processed when there is little frequency content in the samples. The perceived quality of the signal is unchanged in this technique. Processing fewer samples involves less computation work; therefore processor speed and voltage can be reduced. This reduction in processor speed and voltage {{has been shown to}} reduce power consumption by up to 40 % less than if the audio stream had been run at a <b>fixed</b> <b>sampling</b> rate...|$|R
50|$|The Sound Blaster Live! {{featured}} higher {{audio quality}} than previous Sound Blasters, as it processed the sound digitally at every stage, {{and because of}} its greater chip integration that reduced the analog signal losses of older, larger cards. Unfortunately, digital processing brought some limitations. The DSP had an internal <b>fixed</b> <b>sample</b> rate of 48 kHz, a standard AC'97 clock, meaning that the EMU10K1 always captured external audio-sources at the 48 kHz, then performed a sample-rate conversion on the 48 kHz waveform to the output the requested target rate (such as 44.1 kHz or 32 kHz). This rate-conversion step introduced intermodulation distortion into the downsampled output. However, the rate-conversion was only applied when the audio signal was passed through the effects engine. The SB/Live had great difficulty with resampling audio-CD source material (44.1 kHz) without introducing audible distortion. Creative addressed this concern by recommending audio-recording be performed exclusively at 48 kHz, and use third-party software to handle the desired sample-rate conversion, to avoid using the EMU10K1's sample-rate conversion.|$|E
5000|$|The {{requirement}} for multiple cycles of activation, excitation and de-activation/bleaching would typically imply {{extended periods of}} time to form a PALM/STORM image, and therefore operation on a <b>fixed</b> <b>sample.</b> A number of works have been published as early as 2007 [...] performing PALM/STORM on live cells.The ability to perform live super-resolution imaging using these techniques ultimately depends on the technical limitations of collecting enough photons from a single emitter in a very short time. This depends both on the photophysical limitations of the probe {{as well as on the}} sensitivity of the detector employed. Relatively slow (seconds to tens of seconds) processes such as modification in the organization of focal adhesions have been investigated by means of PALM, whereas STORM has allowed imaging of faster processes such as membrane diffusion of clathrin coated pits or mitochondrial fission/fusion processes.A promising application of live cell PALM is the use of photoactivation to perform high-density single-particle tracking (sptPALM [...] ), overcoming the traditional limitation of single particle tracking to work with systems displaying a very low concentration of fluorophores.|$|E
40|$|Questions {{pertaining}} to the admissibility of <b>fixed</b> <b>sample</b> size tests of hypotheses, when sequential tests are available, are considered. For the normal case with unknown mean, suppose the risk function is a linear combination of probability of error and expected sample size. Then any <b>fixed</b> <b>sample</b> size test, with sample size n ⩾ 2, is inadmissible. On the other hand, suppose the risk function consists of the pair of components, probability of error and expected sample size. Then any optimal <b>fixed</b> <b>sample</b> size test for the one sided hypothesis is admissible. When the variance of the normal distribution is unknown, t-tests are studied. For one-sided hypotheses and componentwise risk functions the <b>fixed</b> <b>sample</b> size t-test is inadmissible {{if and only if}} the absolute value of the critical value of the test is {{greater than or equal to}} one. This implies that for the most commonly used sizes, the <b>fixed</b> <b>sample</b> size t-test is inadmissible. Other loss functions are discussed. Also an example for a normal mean problem is given where a nonmonotone test cannot be improved on by a monotone test when the risk is componentwise...|$|E
40|$|This study {{investigates the}} {{potential}} of microscopic Magnetic Resonance Imaging to obtain information for 3 D digital atlases of mouse development using <b>fixed</b> <b>samples.</b> <b>Fixed</b> <b>samples</b> allow direct comparison with already published atlases and provide a testing ground for future in vivo efforts. 3 D MR images of mouse embryos (dpc 6. 5 - 16) illustrate that the necessary contrast and level of detail is available with this technique. Diffusion weighted imaging, diffusion tensor imaging, and multi-valued data sets are presented as examples of uniquely MR methods of obtaining anatomical information. MRI is performed non-invasively on the intact sample, leaving open the possibility of other manipulations (e. g. classical histology, immunohistochemistry, in situ hybridization, and in vitro growth for unfixed samples) after conducting the MRI experiment # 1999 Elsevier Science Ltd. All rights reserved...|$|R
40|$|Consideration of {{the problem}} of {{selecting}} a low-pass sampling bandwidth for the digital mechanization of a matched-filter bit-synchronizer combination. An attempt is made to determine how large a data rate can be provided for a <b>fixed</b> <b>sampling</b> rate (limited by hardware considerations on the sampling device) without paying an excessive penalty in SNR performance...|$|R
5000|$|AC'97 1.x {{compliant}} indicates <b>fixed</b> 48K <b>sampling</b> rate operation (non-extended feature set) ...|$|R
30|$|The next theorem {{treats the}} moments of R_nij with <b>fixed</b> <b>sample</b> size m_n=m.|$|E
30|$|Miao et al. [2] {{obtained}} the density function for R_n 2 j for <b>fixed</b> <b>sample</b> size m_n=m, they also {{proved that the}} expectation of R_n 2 j is finite and the truncated second moment is slowly varying at ∞. Adler [1] also claimed that all the R_n 1 j have infinite expectations for <b>fixed</b> <b>sample</b> size, so our theorems extended their results.|$|E
40|$|AbstractUnder certain conditions, it {{is shown}} in this paper that a minimax {{sequential}} solution for a decision problem is a <b>fixed</b> <b>sample</b> size solution. This investigation is initiated by and applied to a specific problem, namely, {{the search for a}} maximum of a unimodal function. For this problem an ϵ-minimax sequential solution for a <b>fixed</b> <b>sample</b> size has already been given by J. Kiefer [1]...|$|E
40|$|Introduction In {{situations}} where data are collected over time, adaptive sampling methods {{often lead to}} more efficient results than do <b>fixed</b> <b>sampling</b> techniques. When sampling or "allocating" adaptively, sampling decisions are based on accruing data. In contrast, when using <b>fixed</b> <b>sampling</b> procedures, the sample sizes taken from different populations are specified in advance and {{are not subject to}} change. Using adaptive techniques can reduce costs, time and improve the precision of the results for a given sample size. Fully sequential adaptive procedures, in which one adjusts after each observation, are the most powerful. Such procedures are rarely used, however, due to difficulties related to generating and implementing good procedures as well as to complications associated with analyzing the resulting data. Our goal is to help researchers utilize adaptive allocation by creating a collection of algorithms to optimize and analyze a variety of sequential procedures. The techniqu...|$|R
40|$|This paper {{presents}} a predictive current control (PCC) strategy for doubly fed induction generators (DFIGs). The method predicts the DFIG's rotor current {{variations in the}} synchronous reference frame fixed to the stator flux within a <b>fixed</b> <b>sampling</b> period. This is then used to directly calculate the required rotor voltage to eliminate the current errors {{at the end of}} the following sampling period. Space vector modulation is used to generate the required switching pulses within the <b>fixed</b> <b>sampling</b> period. The impact of sampling delay on the accuracy of the sampled rotor current is analyzed and detailed compensation methods are proposed to improve the current control accuracy and system stability. Experimental results for a 1. 5 -kW DFIG system illustrate the effectiveness and robustness of the proposed control strategy during rotor current steps and rotating speed variation. Tests during negative-sequence current injection further demonstrate the excellent dynamic performance of the proposed PCC method...|$|R
30|$|The wood blocks, with {{sizes of}} 3 [*]×[*] 4 [*]×[*] 5  mm 3 were fixed as {{described}} by Daniel and Nilsson [7]. <b>Fixed</b> <b>samples</b> were washed with 0.1 % PBS several times, gradient dehydrated with ethanol, displaced with butanol [27], and 4  h after being freeze–dried with JFD- 320 freeze–drier (Japanese Electronics Company, Japan), the samples were mounted and sprayed gold.|$|R
