6|38|Public
50|$|Cab {{signals are}} {{presented}} to the locomotive {{by means of a}} cab signal display unit. The earliest CDUs consisted of miniature signals of the type visible along the track, back lit by light bulbs. These could be found in both color light and position light varieties depending on the railroad's native signaling system. Modern CDUs on passenger trains are often integrated with the speedometer, as cab signals now serve a speed control function. On trains equipped with automatic train control <b>functionality</b> <b>failure</b> to properly acknowledge a restrictive cab signal change results in a 'penalty brake application', as does failure to observe the cab signal speed limit.|$|E
40|$|Failure {{analysis}} {{is the process}} of identifying the causes and factors leading to undesired loss of <b>functionality.</b> <b>Failure</b> investigators use several kinds of notions to explain this loss. An important one is that of a root cause failure analysis. This paper presents a root cause failure analysis and methodology followed in post weld failure investigation of blanking device which fails during post weld heat treatment. After performing fault tree analysis it is found that the failure may happen due to material defect or generation of residual stress during post weld heat treatment. Keywords-Residual stress, Blanking device,Gr: 21 CrMoNiV 4 - 7, ANSYS, FT...|$|E
40|$|A very {{effective}} method for modelling and simulation of large complex dynamic systems {{is represented by}} distributed modelling using transmission line elements (or bi-lateral delay lines). This method evolves naturally for calculation of pressures when hydraulic pipelines are modelled with distributed parameters, {{and it can be}} used to effectively partition the model to use local solvers for the differential equations in each component or subsystem. It is also applicable to other physical systems, such as mechanical, electrical, gas etc. One interesting application for distributed solvers using bi-lateral delay lines is in real time simulation, since they are very robust and usually quite large simulation times steps can be used. Modelling for real-time applications puts special requirements on robustness in the numerical methods used. In real-time applications there is no room for decreasing time step in numerically critical stages. Furthermore, if a system is relaying on a real-time simulation for its <b>functionality,</b> <b>failure</b> in the numerical properties is unacceptable. It is also in many applications possible to simulate the system faster than real time, which means that high fidelity system simulation can be used to plan ahead in control applications, and for simulation based optimisation. Since solvers can be embedded in components or subsystems, it is very straightforward to implement parallel processing using the multi-core processors which now is the standard for desk top computers. There is also an increasing need in many situations to resolve the time scale to finer details, where the effect of wave propagation needs to be modelled. Invited conference contributio...|$|E
3000|$|To {{model the}} system {{lifetime}} and {{to quantify the}} degradation of <b>functionality</b> or <b>failure</b> probability, [...]...|$|R
40|$|Abstract — We {{characterize}} the TID resilience and annealing response of high-density SRAMs, fabricated in 90 nm commercial processes. Results demonstrate intrinsic SRAM hardness at 300 krad(Si), but also reveal several {{orders of magnitude}} increase in leakage current at 1 Mrad(Si) of exposure, with recorded critical <b>functionality</b> <b>failures.</b> However, the technology is shown to be very responsive to temperature treatments as all chips recover to pre-rad leakage levels after 5 hours at 150 o C with fully regained functionality. Finally, consecutive exposure/anneal cycles reveal a drastic improvement in TID resilience as SRAMs that did undergo at least one postrad anneal stayed fully functional up to the strategic TID level of 2 Mrad(Si) once re-exposed. The gained resilience is attributed to negative interface trapped charge build-up along the STI walls: it progressively turns OFF the channel of parasitic transistors responsible for TID-induced leakage paths creation. Index Terms — Leakage current, SRAM, temperature annealing, total ionizing dose, 90 nm commercial process, interface traps. T I...|$|R
40|$|This paper {{presents}} a hierarchical modelingapproach aimed at reliability assessment over a mission {{period of the}} software fault tolerance technique based on N version programming. The model is constructed in three layers wherein submodels represent different parts of an object system and time scale distinctions. Our modeling approach is systematic {{as opposed to the}} ad hoc methods used in related works. Moreover, it permits modifications to be flexibly made at a specific level or in a specific submodel. Thus, the work presented here generalizes our previous work as it allows to consider general distributions of the versions time to failure and execution time at the first level. Also, at the third level, instead of the performability model in our previous work, we develop a new model aimed at reliability assessment over a mission period which supports the evaluation of a reliability over mission period in terms of the functional and timeliness requirements. Keywords: Software fault tolerance, operational environment, <b>failures</b> of <b>functionality,</b> <b>failures</b> of performance, correlation, mission reliability. 1...|$|R
40|$|Proteins are {{essential}} biomolecules for living organism to regulate cellular growth and their sustainability {{in the crowded}} biological environments. Evolving proteins need to be correctly folded to embrace their appropriate structure to maintain proper <b>functionality.</b> <b>Failure</b> to do so or due to accumulation of misfolded proteins {{may contribute to the}} development of many neurodegenerative diseases like amyloidosis, diabetes and Alzheimer’s. Therefore, understating the specific adsorption behavior of proteins on gold nanoantennas is crucial towards the development of advanced biocompatible devices for improved medical diagnosis. Metallic-nanoantennas are subwavelength structures that interact with light and confine the optical intensity within a tiny volume. This research identifies the specific deposition nature of selective proteins on gold-nanoantenna surfaces using high resolution imaging tool - atomic force microscope (AFM) together with their molecular vibrational detection with FTIR spectroscopy. The specific nanoantennas are asymmetric-split ring resonators (A SRRs) based on a circular geometry and are fabricated using electron-beam lithography. The plasmonic resonance of A-SRRs were tuned to match the vibrational resonance of proteins molecules in the mid-infrared region, thereby enhancing the sensitivity of detection down to nanogram (10 - 9) per millilitre range. For higher concentrations - accumulated selective deposition of proteins were imaged on the functionalised nanoantenna surfaces and are shown in Figure 1. The specific nature of folded proteins may be significant because folded proteins are associated with the development of degenerative diseases, such as Alzheimer’s - but further experiments are required to draw firmer conclusions. The outcomes from this research will benefit significantly to the wider audience in understanding and development of new medical diagnostic devices with a faster response and enhanced sensitivity...|$|E
40|$|Purpose: To {{describe}} {{our experience}} with the technique of transhepatic venous access for hemodialysis and to evaluate its functionality and complications. Patients and methods: From March 2012 till October 2012, 23 patients with age ranging from 12 to 71  years old having end-stage renal disease (ESRD) were included in our study and were subjected to transhepatic venous catheter insertion. In 21 patients there were not any remaining patent peripheral venous accesses. In 2 patients {{there were only a}} last one venous access needed to be preserved. Thus, it was decided to make THVA. In all the 23 patients the indication was palliative due to inoperability which was because of inability to insert an arterio-venous graft or making another arterio-venous fistula. Complications were evaluated and calculated in terms of number of procedures, infection, dislodgement and outcome; in terms of disfunctionality of the catheter. Follow-up was performed by monitoring the catheter dialysis rate in each session, abdominal ultrasonography, fluoroscopy or CT. Mean survival time and median survival time from the start of treatment were calculated using Kaplan–Meier method. Results: Twenty-three patients required a single transhepatic access procedure. Because of catheter dislodgment, two patients required a second access placement procedure, which resulted in a total of 25 separate transhepatic access sites in 23 patients. Technical success was achieved in 22 procedures. Functionality success was achieved in 20 patients. <b>Functionality</b> <b>failure</b> occurred in 3 patients. The trans-hepatic catheters stayed in place between 90 and 300  days. Complications occurred in 14 patients. Conclusion: Based on our findings, transhepatic hemodialysis catheters have proven to achieve good long-term functionality. A high level of maintenance is required to preserve patency, although this approach provides remarkably durable access for patients who have otherwise exhausted access options...|$|E
40|$|In this paper, Hierarchical Time-Extended Petri Nets (H-EPNs) {{are used}} for the {{modeling}} and simulation of complex automated manufacturing system <b>functionalities</b> including <b>failures</b> and complex assembly operations. H-EPNs allow for integrating the top down and bottom up modeling techniques to effectively construct models of automated manufacturing systems {{through the use of}} activator arc extensions [3]. This flexibility allows for the reuse of Petri Net models of independent subsystems to be easily integrated into any top-down system decomposition...|$|R
40|$|International audienceInternet of Mobile Things (IoMT) {{is a new}} {{paradigm}} of the Internet of Things (IoT) where devices such as sensors, robots, unmanned aerial vehicles (UAV) and cars, are inherently mobile. While mobility enables innovative applications and allows new services, it remains a challenging issue as it causes disconnection of nodes and intermittent connectivity, which negatively impact the network performance; namely data loss, large handover delay and application <b>functionality</b> <b>failures.</b> In this paper, we propose a new energy efficient and mobility aware routing protocol named EC-MRPL based on the well-known Routing Protocol for Low power and Lossy Networks (RPL standard). Unlike RPL which is designed for low resources networks with basically static devices, the proposed protocol enables to better conserve the energy and sustain the connectivity of mobile nodes. EC-MRPL integrates an enhanced mobility detection method and a novel point of attachment prediction and replacement strategy aware of the resources constraints. As such, EC-MRPL overcomes and mitigates problems caused by mobility. Obtained simulation results using Cooja/Contiki show that ECMRPL outperforms both the RPL and the MRPL protocols in terms of handover delay, data loss rate, signaling cost and energy consumption...|$|R
40|$|The {{operation}} and maintenance phase is the longest and most expensive life-cycle period of buildings and facilities. Operators need to carry out activities to maintain equipment to prevent <b>functionality</b> <b>failures.</b> Although some software tools have already been introduced, research studies have concluded that (1) facility handover data is still predominantly dispersed, unformatted and paper-based and (2) hence operators still spend 50 % of their on-site work on target localization and navigation. To improve these procedures, the authors previously presented a natural marker-based Augmented Reality (AR) framework that digitally supports facility maintenance operators when navigating indoors. Although previous results showed the practical potential, this framework fails if no visual marker is available, if identical markers are at multiple locations, and if markers are light emitting signs. To overcome these shortcomings, this paper presents an improved method that combines an Inertial Measurement Unit (IMU) based step counter and visual live video feed for AR based indoor navigation support. In addition, the AR based marker detection procedure is improved by learning camera exposure times in case of light emitting markers. A case study and experimental results in a controlled environment reveal the improvements and advantages of the enhanced framework...|$|R
40|$|The {{operation}} and maintenance phase is the longest and most expensive life-cycle period of building facilities. Operators need to perform activities to provide a comfortable living and working environment and to upkeep equipment to prevent <b>functionality</b> <b>failures.</b> For that purpose they manually browse, sort and select dispersed and unformatted facility information before actually going on the site. Although some software tools have been introduced, they still spent 50 % of the on-site work on inspection target localization and navigation. To improve these manual, time consuming and tedious procedures, the authors previously presented a framework that uses BIM-based Augmented Reality (AR) to support facility maintenance tasks. The proposed workflow contains AR supported activities, namely AR-based indoor navigation and AR-based maintenance instructions. An inherent problem of AR is marker definition and detection. As introduced, indoor natural markers such as exit signs, fire extinguisher location signs, and appliances’ labels were identified to be suitable for both navigation and maintenance instructions. However, small markers, changing lighting conditions, low detection frame rates and accuracies might prevent the proposed approach from being practical. In this paper the performance of natural marker detection will be evaluated under different configurations, varying marker types, marker sizes, camera resolutions and lighting conditions. The detection performance will be measured using pre-defined metrics incorporating detection accuracy, tracking quality, frame rates, and robustness. The result will be a set of recommendations on what configurations are most suitable and practical within the given framework...|$|R
40|$|Genetic Programs {{that have}} phenotypes {{created by the}} {{application}} of genotypes comprising rules are robust and highly scalable. Such encodings are useful for complex applications such as controller design. This paper outlines an evolutionary algorithm capable of creating a controller for 2 DOF, path following robot. The controllers are embodied by Artificial Neural Networks capable of full <b>functionality</b> despite multiple <b>failures...</b>|$|R
40|$|Accident Warning Systems (AWSs) use Vehicular Ad-hoc Networks to help avoid {{potential}} collisions {{and spread}} safety notifications amongst nearby vehicles. The development of such systems often {{assumes that the}} users are honest and therefore will participate as expected to maximize the intended benefit. In practice, however, attacks are a real possibility and require appropriate counter-measures to avoid a range of undesirable outcomes from reduced <b>functionality</b> to <b>failure</b> to maliciously induced traffic incidents. This paper examines the subject and identifies potential adversaries and the attacks that they might use. Finally, it proposes a threat model that presents a comprehensive picture of how security, privacy and trust issues in AWSs will be targeted {{and how they can}} be protected...|$|R
40|$|Due to the {{increasing}} susceptibility of electronic device, the importance of electromagnetic immunity to interference also increasing. The basic task of testing the electromagnetic susceptibility of electronic device on the harmful interference is comparison of {{the data obtained by}} testing with data which are required by the relevant standards for the device. When the electronic device does not have a sufficient immunity can occur a deterioration of its <b>functionality,</b> temporary <b>failure</b> or even a permanent damage to the device. The aim of this paper is perform the test of electromagnetic susceptibility for the electrostatic discharge at the keyboard of the alarm system. Immunity test will be performed according to instructions in the standards of electromagnetic compatibility. LO 1303, Ministry of Educatio...|$|R
40|$|This report {{describes}} an approach enabling automatic structural reconfigurations of distributed applications based on configuration management {{in order to}} compensate for node and network failures. The major goal of the approach is to maintain the relevant application <b>functionality</b> after <b>failures</b> automatically. This goal is achieved by a dedicated system model and by a decentralized reconfiguration algorithm based on it. The system model provides support for redundant application object storage and for application-level consistency based on distributed checkpoints. The reconfiguration algorithm detects failures, computes a compensating configuration, and realizes this new configuration. The report emphasizes flexibility {{in the sense of}} adaptable levels of fault tolerance, as well as transparency in the sense of fully-automatic reaction to failures. Key words: Distributed applications, object-oriented systems, dynamic reconfiguration, fault tolerance, dependability, configuration manage [...] ...|$|R
40|$|Since {{its first}} {{prototype}} presented in Deliverable D 1. 5, some major enhancements {{have been added}} to the Network Service Plane implementation. These include possibilities of distributing the interdomain <b>functionality</b> to improve <b>failure</b> resilience and scalability, integration of AAA concepts developed within WP 4, multi-constraint support, malleable reservations for data transfers, a notification framework to eliminate the need for polling status changes in regular intervals, and some other minor enhancements. This deliverable describes the new prototype. Project...|$|R
40|$|A {{case study}} of {{performance}} and dependability evaluation of fault-tolerant multiprocessors is presented. Two specific architectures are analyzed taking into account system <b>functionality,</b> actual workloads, <b>failures</b> of system components {{as well as the}} inter-component dependencies. Since the evaluation of such complex systems has to be performed already during the design phase, simulation models are developed and used to provide insight into the system behavior and to uncover weak points and bottlenecks. Object-oriented software design and process-oriented simulation techniques are used for model construction allowing sophisticated performance and dependability analysis of massively parallel systems. ...|$|R
40|$|This report {{describes}} an implementation of Transaction Logging and Recovery using Unix Copy-On-Write on spawned processes. The {{purpose of the}} work is to extend WS-Iris, a research project on Object Oriented Main Memory Databases, with <b>functionality</b> for <b>failure</b> recovery. The presented work is a Master Thesis for a student of Master of Science in Computer Science and Technology. The work has been commissioned by Tore Risch, Professor of Engineering Databases at Computer Aided Engineering laboratory (CAElab), Linköping University (LiU/LiTH), Sweden. Keywords Transaction logging, logical logging, recovery, main memory database, copyon -write, process forking Jonas S Karlsson CAElab, IDA, Linköping University Transaction Logging and Recovery for a Main Memory OODB 3 CHAPTER 1 Introduction 5 1. 1 WS-Iris 5 1. 2 Reason & Goal 5 1. 3 Contents 6 CHAPTER 2 WS-Iris 7 2. 1 Internal Workings 7 2. 1. 1 The self-contained Lisp 8 2. 1. 2 Foreign functions 8 2. 2 Image 8 2. 3 Logging (Histories) 8 CH [...] ...|$|R
40|$|The paper {{presents}} a hierarchical modeling {{approach of the}} N version programming in a real – time environment. The model is constructed in three layers. At the first layer we distinguish the NVP structure from its operational environment. The NVP structure submodel considers both <b>failures</b> of <b>functionality</b> and <b>failures</b> of performance. The operational environment submodel {{is based on the}} concept of the operational profile. The second layer consists of a per run reliability and performance submodels. The first considers per run failure probabilities, while the second is responsible for modeling the series of successive runs over a mission. The information contributed by the second layer constitutes third layer models which support the evaluation of a performability and reliability over mission. The work presented here generalizes our previous work as it considers general distributions of the versions time to failure and execution time. Also, in addition to the performability model, the third layer includes a model aimed at reliability assessment over a mission period. 1...|$|R
40|$|I {{declare that}} this thesis is my own, unaided work. It is being {{submitted}} for the Degree of Doctor of Philosophy in the University of the Witwatersrand, Johannesburg. It {{has not been}} submitted before for any degree or examination in any other University. (Signature of candidate) day of 2014 i Industrial units cannot operate without failure forever. When the operation of a unit deviates from industrial standards, it is considered to have failed. The time from the moment a unit enters service until it fails is its lifetime. Within reliability and often in life data analysis in general, lifetime is the event of interest. For highly reliable units, accelerated life testing is required to obtain lifetime data quickly. Accelerated tests where failure is not instantaneous, but the end point of an underlying degradation process are considered. Failure during testing occurs when {{the performance of the}} unit falls to some specified threshold value such that the unit fails to meet industrial specifications though it has some residual <b>functionality</b> (degraded <b>failure)</b> or decrease...|$|R
40|$|Stochastic {{behaviors}} of resistive {{random access memory}} (RRAM) {{play an important role in}} the design of cross-point memory arrays. A Monte Carlo compact model of oxide RRAM is developed and calibrated with experiments on various device stack configurations. With Monte Carlo SPICE simulations, we show that an increase in array size and interconnect wire resistance will statistically deteriorate write <b>functionality.</b> Write <b>failure</b> probability (WFP) has an exponential dependency on device uniformity and supply voltage (VDD), and the array bias scheme is a key knob. Lowering array VDD leads to higher effective energy consumption (EEC) due to the increase in WFP when the variation statistics are included in the analysis. Random-access simulations indicate that data sparsity statistically benefits write functionality and energy consumption. Finally, we show that a pseudo-sub-array topology with uniformly distributed pre-forming cells in the pristine high resistance state is able to reduce both WFP and EEC, enabling higher net capacity for memory circuits due to improved variation tolerance. Comment: Accepted by IEEE Transactions on Electron Device...|$|R
40|$|Abstract—In resource-flow systems, e. g. {{production}} lines, {{agents are}} processing resources by applying capabilities {{to them in}} a given order. Such systems profit from self-organization as they become easier to manage and more robust against failures. This paper proposes a decentralized coordination process that restores a system’s <b>functionality</b> after a <b>failure</b> by propagating information about the error through the system until a fitting agent is found that is able to perform the required function. The mechanism has been designed by combining a top-down design approach for self-organizing resource-flow system and a systemic modeling approach for the design of decentralized, distributed coordination mechanisms. The systematic conception of the interagent process is demonstrated. Evaluations of convergence as well as performance are performed by simulations. I...|$|R
40|$|We {{present a}} case study of {{performance}} and dependability evaluation of fault-tolerant multiprocessors. A distributed shared-memory multiprocessor architecture providing faulttolerance is analyzed taking into account system <b>functionalities,</b> actual workloads, <b>failures</b> of system components as well as the inter-component dependences. Since the evaluation of such a complex system has to be performed already during the design phase, a simulation model has been developed and is being used to gain insight into the system behavior and to uncover weak points and bottlenecks. Object-oriented software design and process-oriented simulation techniques are used {{for the construction of a}} sophisticated simulation model. INTRODUCTION One of the design goals of architectures for massively parallel systems is scalability: It is desirable to adapt the size of a parallel machine to the size of the problems that have to be calculated. However, for most current parallel architectures, there are constraints [...] ...|$|R
40|$|The {{increasing}} {{complexity and}} interdependency of today's networks highlight {{the importance of}} studying network robustness to failure and attacks. Many large-scale networks are prone to cascading effects where {{a limited number of}} initial failures (due to attacks, natural hazards or resource depletion) propagate through a dependent mechanism, ultimately leading to a global failure scenario where a substantial fraction of the network loses its <b>functionality.</b> These cascading <b>failure</b> scenarios often take place in networks which are embedded in space and constrained by geometry. Building on previous results on cascading failure in random geometric networks, we introduce and analyze a continuous cascading failure model where a node has an initial continuously-valued state, and fails if the aggregate state of its neighbors fall below a threshold. Within this model, we derive analytical conditions for the occurrence and non-occurrence of cascading node failure, respectively...|$|R
30|$|This work {{is focused}} on the {{self-healing}} functionalities for ultra-dense small cell networks. Self-healing is key for network OAM automation and performance, since network failures can lead to service degradations that might highly impact the brand image and the long-term revenue of operators. Failure management is divided in four main subtasks [5]. Firstly, detection consists in the discovery of network problems, i.e., identifying cells with degradations in the provided service. Secondly, diagnosis, also called root cause analysis, aims at identifying the specific cause or fault producing the degradation. Finally, once a problem has been detected, different actions can take place to compensate its effects until recovery actions restore the network to its full <b>functionality.</b> In classic <b>failure</b> management approaches, these are very time consuming and signaling generating tasks. This makes the automation of these functions a field attracting an increasing attention, where the implications of its application for 5 G scenarios have been only scarcely considered.|$|R
40|$|Continuous {{renewal of}} {{intracellular}} components {{is required to}} preserve cellular <b>functionality.</b> In fact, <b>failure</b> to timely turnover proteins and organelles leads often to cell death and disease. Different pathways contribute to the degradation of intracellular components in lysosomes or autophagy. In this review, we focus on chaperone-mediated autophagy (CMA), a selective form of autophagy that modulates the turnover of a specific pool of soluble cytosolic proteins. Selectivity in CMA is conferred {{by the presence of}} a targeting motif in the cytosolic substrates that, upon recognition by a cytosolic chaperone, determines delivery to the lysosomal surface. Substrate proteins undergo unfolding and translocation across the lysosomal membrane before reaching the lumen, where they are rapidly degraded. Better molecular characterization of the different components of this pathway in recent years, along with the development of transgenic models with modified CMA activity and the identification of CMA dysfunction in different severe human pathologies and in aging, are all behind the recent regained interest in this catabolic pathway...|$|R
40|$|Conferencia del Programa "RETOS" del CENIMWith {{the growing}} use of dental implants, so grows the {{incidence}} of implants’ failures. Late treatment complications, after reaching full osseointegration and <b>functionality,</b> include mechanical <b>failures,</b> such as fracture of the implant and its components. Those complications are deemed severe in dentistry, albeit usually considered as rare, and therefore seldom addressed in the clinical literature. The introduction of dental implants into clinical practice fostered a wealth of research on their biological aspects. By contrast, mechanical strength and reliability issues were seldom investigated in the open literature, so {{that most of the}} information to date remains essentially with the manufacturers. Over the years, dental implants have gone through major changes regarding the material, the design, and the surface characteristics aimed at improving osseointegration. Did those changes improve the implants’ mechanical performance? This talk will present our results on various aspects of the mechanical integrity and failure of dental implantsN...|$|R
40|$|This paper proposes {{the use of}} metrics {{to refine}} system design for soft errors {{protection}} in system on chip architectures. Specifically this research shows the use of metrics in design space exploration that highlight where {{in the structure of}} the model and at what point in the behaviour, protection is needed against soft errors. As these metrics improve the ability of the system to provide functionality, they are referred to here as reliability metrics. Previous approaches to prevent soft errors focused on recovery after detection. Almost no research has been directed towards preventive measures. But in real-time systems, deadlines are performance requirements that absolutely must be met and a missed deadline constitutes an erroneous action and a possible system failure. This paper focuses on a preventive approach as a solution rather than recovery after detection. The intention of this research is to prevent serious loss of system <b>functionality</b> or system <b>failure</b> though it may not be able to eliminate the impact of soft errors completely...|$|R
40|$|OBJECTIVES/HYPOTHESIS: Reconstruction of the {{mandible}} {{and oral}} cavity after segmental resection is a challenging surgical problem. Although osteocutaneous free flaps are generally accepted to be optimal for reconstruction of anterior defects, {{the need for}} bony reconstruction for a pure lateral mandibular defect remains controversial. STUDY DESIGN: A retrospective study. METHODS: A retrospective comparative study of short- and long-term outcomes of three different reconstruction techniques for lateral defects was performed. In total, 57 patients were included, of whom 27 had a plate and pedicled pectoralis major myocutaneous flap (PMMF group), 16 had a plate and free radial forearm flap (FRFF group), and 14 had an osteocutaneous free flap. <b>Functionality,</b> flap <b>failure,</b> and complications were scored. RESULTS: Plates {{had to be removed}} in 7 of the 27 patients in the PMMF group and 2 of the 16 in the FRFF group; none of the 14 osteocutaneous free flaps failed. The difference was of borderline statistical significance (P =. 055). Longterm functional outcome revealed no statistically significant difference in oral deglutition (P =. 76) or in facial contour (P =. 36). Oral continence was significantly better in patients in the FRFF group (88 %) as compared with the PMMF group (52 %) or the osteocutaneous free flap group (43 %) (P =. 02). On the other hand, the results for speech favored the osteocutaneous free flap group; 13 of 14 patients (92. 9 %) had a normal score compared with 12 of 16 patients (75 %) in the FRFF group and 17 of 27 (63 %) in the PMMF group. However, this represented a borderline statistically significant result (P =. 06). CONCLUSIONS: For lateral mandibular defects, the osteocutaneous free flap is reliable and durable in the long term. However, in a selected group of patients either of the two flap-plate options is a viable reconstructive optio...|$|R
40|$|With {{the growing}} use of dental implants, the {{incidence}} of implants’ failures grows. Late treatment complications, after reaching full osseointegration and <b>functionality,</b> include mechanical <b>failures,</b> such as fracture of the implant and its components. Those complications are deemed severe in dentistry, albeit being usually considered as rare, and therefore seldom addressed in the clinical literature. The introduction of dental implants into clinical practice fostered a wealth of research on their biological aspects. By contrast, mechanical strength and reliability issues were seldom investigated in the open literature, so {{that most of the}} information to date remains essentially with the manufacturers. Over the years, implants have gone through major changes regarding the material, the design, and the surface characteristics aimed at improving osseointegration. Did those changes improve the implants’ mechanical performance? This review article surveys the state-of-the-art literature about implants’ mechanical reliability, identifying the known causes for fracture, while outlining the current knowledge-gaps. Recent results on various aspects of the mechanical integrity and failure of implants are presented and discussed next. The paper ends by a general discussion and suggestions for future research, outlining the importance of mechanical considerations for the improvement of their future performance...|$|R
40|$|With {{very large}} scale {{integrated}} (VLSI) circuit fabrication entering the deep sub-micron era, devices are scaled down to finer geometries, clocks are run at higher frequencies, and more functionality is integrated into one chip. All these bring a great promise of “system-on-a-chip”, but also introduce challenging new {{issues in the}} design process. As {{a result of the}} increasing frequency and density, coupling effects or crosstalk between neighboring wires are increased. These effects can cause <b>functionality</b> and timing <b>failures</b> in a circuit. The dynamic power consumption in charging or discharging coupling capacitances is timing dependent, and contributes significantly to a circuit’s power consumption. In addition, manufacturing process variations (e. g. VT, Le), and environmental variations (e. g. Vdd, Temperature) contribute to uncertainties that deeply impact the timing characteristics of a circuit. This variability makes timing verification, and consequently, timing driven circuit optimization extremely difficult. Although worst case analyses for circuit optimization are simpler, they are not desirable since they severely over-constrain the optimization problem, and result in designs that have excessive penalties in terms of area or power consumption. In this research, we investigate the essential problems of timing verification, power estimation...|$|R
40|$|AbstractEnterprises {{implement}} {{their business}} process as web services {{in order to}} meet the market demand. Typically composite business process requires collection of services under composition along with real time access control to provide the integrated functionalities to the consumer by orchestrating the service functionalities in a meaningful manner. Composite services often need to change their execution order under various circumstances based on the business requirements. In Enterprise view order of execution of rules associated with the single service or composite services can be altered dynamically that in turn leads to the many issues such as <b>functionality</b> variation, authentication <b>failure,</b> loss of composition factors, work flow inconsistency and SLA violation. The proposed ac ess control mechanism built over the dynamic work flow management of web service business logic overcomes the issues and provide the secure environment for the enterprises. This fast and efficient access control mechanism is implemented using Finite State Machine, in which the states of the FSM stores the additional attribute called Access Point through that the transition from one state to another state determines the required access control is set to true or false...|$|R
40|$|A thesis {{submitted}} to the Faculty of Science, University of the Witwatersrand, Johannesburg, in fulfilment of the requirements for the degree of Doctor of Philosophy. Johannesburg, December 2014. Industrial units cannot operate without failure forever. When the operation of a unit deviates from industrial standards, it is considered to have failed. The time from the moment a unit enters service until it fails is its lifetime. Within reliability and often in life data analysis in general, lifetime is the event of interest. For highly reliable units, accelerated life testing is required to obtain lifetime data quickly. Accelerated tests where failure is not instantaneous, but the end point of an underlying degradation process are considered. Failure during testing occurs when {{the performance of the}} unit falls to some specified threshold value such that the unit fails to meet industrial specifications though it has some residual <b>functionality</b> (degraded <b>failure)</b> or decreases to a critical failure level so that the unit cannot perform its function to any degree (critical failure). This problem formulation satisfies the random signs property, a notable competing risks formulation originally developed in maintenance studies but extended to accelerated testing here. Since degraded and critical failures are linked through the degradation process, the open problem of modelling dependent competing risks is discussed. A copula model is assumed and expert opinion is used to estimate the copula. Observed occurrences of degraded and critical failure times are interpreted as times when the degradation process first crosses failure thresholds and are therefore postulated to be distributed as inverse Gaussian. Based on the estimated copula, a use-level unit lifetime distribution is extrapolated from test data. Reliability metrics from the extrapolated use-level unit lifetime distribution are found to differ slightly with respect to different degrees of stochastic dependence between the risks. Consequently, a degree of dependence between the risks that is believed to be realistic to admit is considered an important factor when estimating the use-level unit lifetime distribution from test data. Keywords: Lifetime; Accelerated testing; Competing risks; Copula; First passage time...|$|R
40|$|Contemporary {{complex systems}} {{generally}} have multiple sensors embedded {{at various levels}} within their structure. Sensors are data gathering mechanisms that measure a systemic quantity (such as <b>functionality</b> or <b>failure)</b> providing the engineer with a multitude of reliability information. Data sets {{are said to be}} overlapping when drawn simultaneously from multiple sensors in a system. Current methodologies focus on system reliability analysis of non-overlapping data sets. We introduce a Bayesian methodology that allows analysis of overlapping data sets, exploiting their inherent inter-dependence to yield significant additional information. Data gathered from any sub-system or component contextualizes data gathered from a sensor placed at the `top' of the system (i. e. systemic functionality) through dependence. A system that is functional in spite of a non-functional sub-system infers information about the reliability characteristics of the clearly functional remainder of the system. The same principle extends to any other sensor that has subordinate sensors upon which it is observationally dependent. We apply overlapping Bayesian analysis on several example systems to highlight the information inherent in overlapping data sets and compare these results to those obtained by constraining the data to be analysed as if it were non-overlapping. The Bayesian methodology we introduce deals with on-demand and continuous life metric systems. The likelihood function for on-demand systems accommodates multiple degraded states and relies on an algorithm we introduce that rapidly generates combinations of disjoint cut-sets that imply the evidence. The likelihood function for continuous life-metric systems (such as those whose failure probability is time based) examines each sensor data when contextualised through all other data sets. We generalise these likelihood functions for uncertain data, allowing simplification through real-life measuring inaccuracies. Finally, we use the methodologies developed above to assess probable information gain for various sensor placement permutations. We embed this process into a Bayesian experimental design framework to optimise sensor placement. This can then be fed into any multi-objective optimization framework, or used in isolation to allow informed sensor placement...|$|R
40|$|The {{potential}} risk factors in CRM implementation can cause serious failures either in project phase or in go-live phase. To eliminate, prevent or control these risk factors, {{they have to}} be identified and classified. The readiness dimensions for CRM are the categorized dimensions that their definitions are based on the problematic nature of the several risk factors. In this research we classified and explained the most important aspects and factors which affect CRM readiness for B 2 B markets, according to their complicated network of relationships. Then we suggested the hierarchical model for assessing CRM readiness in organizations based on six main dimensions, seventeen sub-dimensions and forty eight indicators. Furthermore some new aspects of readiness such as cross <b>functionality,</b> competition environment, <b>failure</b> legitimacy and failure learning organization, continuous improvement procedures, and customer adoption have also been considered {{in order to create a}} more completed assessment model. The condition of business network market was also considered in both indicators and assessment criterions. Finally we assessed the business cases of the research based on this model and its assessment criterions, and analyzed the data using analytical hierarchy process (AHP) method to gain an overall indication of the readiness in dimensions and in total model. ...|$|R
