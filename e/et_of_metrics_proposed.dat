0|10000|Public
30|$|The set <b>of</b> <b>metrics</b> <b>proposed</b> in Section 2.2 will be used, as they {{actually}} aim at assessing {{the usefulness of}} the interactive schemes in terms of the user effort.|$|R
30|$|Metrics {{are used}} to ensure control over {{software}} projects, products, and processes [23]. More specifically, they allow the assessment of attributes, features, or characteristics of software entities, {{making it possible to}} characterize, monitor, and control them. In the context of object-oriented programming, the set <b>of</b> <b>metrics</b> <b>proposed</b> by Chidamber and Kemerer [24] makes it possible to characterize, for example, the size, complexity, and coupling of the code.|$|R
40|$|This paper reports an {{evaluation}} of a number <b>of</b> <b>metrics</b> <b>proposed</b> to measure user productivity and product quality in a usability laboratory setting. The examined metrics {{were found to be}} unsuitable indicators of productivity and quality {{in the context of the}} experimental study. Many insights were obtained, however, into what will be required to make future experimentation successful. A series of recommendations are made. [URL]...|$|R
40|$|International audienceIn {{this paper}} {{we focus on}} vertex-cut graph {{partitioning}} and we investigate {{how it is possible}} to evaluate the quality of a partition before running the computation. To this purpose we scrutinize a set <b>of</b> <b>metrics</b> <b>proposed</b> in literature. We carry experiments with the widely-used framework for graph processing Apache GraphX and we perform an accurate statistical analysis. Our preliminary experimental results show that communication metrics like vertex-cut and communication cost are effective predictors on most of the cases...|$|R
30|$|Readability {{research}} largely traces {{its origins}} to an initial study by Kitson [33] that demonstrates tangible differences in sentence lengths and word lengths, measured in syllables, between two newspapers and two magazines (see also [55] for an historical perspective of readability). The majority <b>of</b> <b>metrics</b> <b>proposed</b> for readability {{are based on}} factors that represent two broad aspects of comprehension difficulty: (i) lexical or semantic features and (ii) sentence or syntactic complexity. According to [7], formulas that depend on these variables are popular because they are easily associated with text simplification.|$|R
40|$|In {{this paper}} we propose a unified {{framework}} for automatic evaluation of NLP applications using N-gram co-occurrence statistics. The automatic evaluation <b>metrics</b> <b>proposed</b> to date for Machine Translation and Automatic Summarization are particular instances from the family <b>of</b> <b>metrics</b> we <b>propose.</b> We show that different members of the same family <b>of</b> <b>metrics</b> explain best the variations obtained with human evaluations, according to the application being evaluated (Machine Translation, Automatic Summarization, and Automatic Question Answering) and th...|$|R
40|$|Evaluation is {{a central}} issue in the design, implementation, and {{performance}} assessment of all systems. Recently, a number <b>of</b> <b>metrics</b> have been <b>proposed</b> to assess the performance of segmentation algorithms for image and video data. This paper {{provides an overview of}} state <b>of</b> the art <b>metrics</b> <b>proposed</b> so-far, and introduces a new and efficient such metric. Doing so, subjective experiments are carried out to derive a perceptual metric. As a result, it also provides a comparison of performance <b>of</b> segmentation assessment <b>metrics</b> for different video object segmentation techniques. 1...|$|R
30|$|Creating {{more value}} via {{business}} and manufacturing {{process is a}} competitive advantage for engineering managers in today’s market. There are many metrics, suggested in the past literatures, to evaluate supply chain performance. Evaluating all these metrics is difficult for engineering managers and they miss the monitoring <b>of</b> effective <b>metrics</b> as they are engaged with all <b>metrics.</b> Some <b>of</b> <b>metrics</b> <b>proposed</b> in the literature were fitted into more than one perspective of BSC. Some of them contradict other <b>metrics</b> and some <b>of</b> them may compromise others. This study proposes {{the use of a}} developed BSC framework using effective metrics to align companies’ strategies and supply chain performance for creating more value.|$|R
40|$|We also {{investigate}} mesh quality metricsbased on weighted Jacobian matrices. We {{suggest that}} element skew, shape, size, and orien-tation form a mutually-exclusive set <b>of</b> <b>metrics.</b> We <b>propose</b> abstract (algebraic) defini-tions <b>of</b> these <b>metrics</b> in terms <b>of</b> largely noncontroversial sets of requirements. The defini-tions enable one {{to determine whether}} or not particular existing and <b>proposed</b> <b>metrics</b> sat-isfy the requirements necessary to constitute a proper metric...|$|R
40|$|Because {{there is}} no {{generally}} accepted metric for measuring the performance of anaphora resolution systems, a combination <b>of</b> <b>metrics</b> was <b>proposed</b> to evaluate submissions to the 2011 CONLL Shared Task (Pradhan et al., 2011). We investigate therefore Multiobjective function Optimization (MOO) techniques based on Genetic Algorithms to optimize models according to multiple metrics simultaneously. ...|$|R
40|$|Component-Based Software Engineering (CBSE) {{has shown}} {{significant}} prospects in rapid pro-duction of large software systems with enhanced quality, and emphasis on decomposition of the engineered systems into functional or logical components with well-defined interfaces used for communication across the components. In this paper, a series <b>of</b> <b>metrics</b> <b>proposed</b> by various re-searchers have been analyzed, evaluated and benchmarked using several large-scale publicly available software systems. A systematic {{analysis of the}} values for various metrics has been car-ried out and several key inferences have been drawn from them. A number of useful conclusions have been drawn from various metrics evaluations, which include inferences on complexity, reus-ability, testability, modularity and stability of the underlying components. The inferences are ar-gued to be beneficial for CBSE-based software development, integration and maintenance...|$|R
40|$|Many {{learning}} algorithms rely on distance metrics {{to receive}} their input data. Research has shown that these metrics can improve the performance of these algorithms. Over the years an often popular function is the Euclidean function. In this paper, we investigate a number <b>of</b> different <b>metrics</b> <b>proposed</b> by different communities, including Mahalanobis, Euclidean, Kullback-Leibler and Hamming distance. Overall, the best-performing method is the Mahalanobis distance metric...|$|R
40|$|Screenshot of the multiagent {{exploration}} simulator. The thick darker {{area is the}} explored area, the cones {{represents the}} field of perception of the enemies and the agents of the exploration team. When a group of characters in a game aim to efficiently explore an environment, {{it is important that}} they coordinate their actions to cooperatively discover new areas. This paper tackles the exploration task as a multiagent problem in the context of computer games. Four simple strategies and an auction-based negotiation strategy were implemented and evaluated. Their performance was compared in different scenarios according to a set <b>of</b> <b>metrics</b> <b>proposed</b> in the paper. Then, it was possible to figure out an efficient strategy for random scenarios. A simulator has also been developed in order to perform the necessary tests...|$|R
40|$|Tools are the {{fundamental}} requirement for acceptability <b>of</b> any <b>metrics</b> programme {{in the software}} industry. It is observed that majority <b>of</b> the <b>metrics</b> <b>proposed</b> and {{are available in the}} literature lack tool support. This {{is one of the reasons}} why they are not widely accepted by the practitioners. In order to improve the acceptability <b>of</b> <b>proposed</b> <b>metrics</b> among software engineers that develop Web applications, there is need to automate the process. In this paper, we have developed a tool for computing metrics for Cascading Style Sheets (CSS) and named it as CSS Analyzer (CSSA). The tool is capable <b>of</b> measuring different <b>metrics,</b> which are the representation of different quality attributes: which include understandability, reliability and maintainability based on some previously <b>proposed</b> <b>metrics.</b> The tool was evaluated by comparing its result on 40 cascading style sheets with results gotten by the manual process of computing the complexities. The results show that the tool computes in far less time when compared to the manual process and is 51. 25 % accurate...|$|R
40|$|We {{propose a}} {{methodology}} {{for the early}} estimation of communication implementation choice effects, starting from an abstract transaction-level system model (TLM). The reference version of the TLM considered is the Open SystemC Initiative library. The methodology {{is based on the}} computation <b>of</b> <b>metrics</b> that abstract useful information from the initial system model. The metrics are precisely defined upon a general formal model of transaction-level system descriptions. A set of design problems of relevant interest, such as shared communication resource assign- ment, pipelining partitioning, bandwidth, and latency constraint estimation, is considered to show some potential applications <b>of</b> the <b>metrics</b> <b>proposed...</b>|$|R
40|$|Using an open-source, Java toolkit of name-matching methods, we {{experimentally}} compare string distance metrics on {{the task}} of matching entity names. We investigate a number <b>of</b> different <b>metrics</b> <b>proposed</b> by different communities, including edit-distance metrics, fast heuristic string comparators, token-based distance metrics, and hybrid methods. Overall, the best-performing method is a hybrid scheme combining a TFIDF weighting scheme, which is widely used in information retrieval, with the Jaro-Winkler string-distance scheme, which was developed in the probabilistic record linkage community...|$|R
40|$|A {{large number}} <b>of</b> <b>metrics</b> for {{evaluating}} the quality of software have been proposed in the literature. However, there is no standard terminology or formalism for defining metrics and consequently many <b>of</b> the <b>metrics</b> <b>proposed</b> have some ambiguity in their definitions. This hampers the empirical validation <b>of</b> these <b>metrics.</b> To address this problem, we generalise an existing approach to defining metrics that is based on the Object Constraint Language and the Unified Modelling Language metamodel. We have developed a prototype tool called DMML (Defining Metrics at the Meta Level) that supports this approach and we present details of this tool. To illustrate the approach, we present formal definitions for the Chidamber and Kemerer metrics suite...|$|R
40|$|In {{this paper}} Knockout Refinement Algorithm (KRA) is {{proposed}} to refine original clusters obtained by applying SOM and K-Means clustering algorithms. KRA Algorithm {{is based on}} Contingency Table concepts. Metrics are computed for the Original and Refined Clusters. Quality of Original and Refined Clusters are compared in terms <b>of</b> <b>metrics.</b> The <b>proposed</b> algorithm (KRA) is tested in the educational domain and results show that it generates better quality clusters in terms of improved metric values...|$|R
40|$|In this paper, we <b>propose</b> a new <b>metrics</b> for {{evaluating}} network security. The <b>proposed</b> <b>metrics</b> {{are based on}} network vulnerabilities in the evaluated network. The <b>proposed</b> <b>metrics</b> are Non Exploited Vulnerability Percentage (NEVP) metric, Non Vulnerable Host Percentage (NVHP) metric and Coefficient of Network Security Level (CNSL) metric. These metrics {{can be used to}} evaluate the security of one or more networks. Background <b>of</b> the <b>proposed</b> <b>metrics</b> is explained on the chapter about background and previous works. Formulation <b>of</b> the <b>proposed</b> <b>metrics</b> also explained and equipped with the formal proof. In the simulation chapter we provide a simulation results in the form of data and two dimension graphs. The presented graphs are two dimension graphs consisted <b>of</b> <b>metrics</b> variable and network size as x-axis and y-axis respectively. Analysis of simulation results and future works are also provided at the end part of this paper...|$|R
40|$|Abstract This paper {{presents}} an ant-inspired method for clustering semantic Web services. The method considers {{the degree of}} semantic similarity between services as the main clustering criterion. To measure the semantic similarity between two services we propose a matching method and a set <b>of</b> <b>metrics.</b> The <b>proposed</b> <b>metrics</b> evaluate the degree of match between the ontology concepts describing two services. We have tested the ant-inspired clustering method on the SAWSDL-TC benchmark and we have evaluated its performance using the Dunn Index, the Intra-Cluster Vari-ance metric and an original metric we introduce in this paper. ...|$|R
40|$|With the {{increasing}} use of object-oriented methods in new software development there is a growing need to both document and improve current practice in object-oriented design and development. In response to this need, a number of researchers have developed various metrics for object-oriented systems as proposed aids to the management of these systems. In this research an analysis of a set <b>of</b> <b>metrics</b> <b>proposed</b> by Chidamber and Kemerer [10] is performed in order to assess their usefulness for practicing managers. First, an informal introduction to the metrics is provided by way of an extended example of their managerial use. Second, exploratory analyses of empirical data relating the metrics to productivity, rework effort, and design effort on three commercial object-oriented systems are provided. The empirical results suggest that the metrics provide significant explanatory power for variations in these economic variables, over and above that provided by traditional measures, such as size in lines of code, and after controlling for the effects of individual developers...|$|R
3000|$|... [...]. Namely, {{the power}} of channel taps is {{normalized}} to one. In each run, 20 OFDM symbols are tested. <b>Metrics</b> <b>of</b> the <b>proposed</b> and compared estimators are averaged over simulated symbols.|$|R
40|$|Component-Based Development(CBD) is an {{emerging}} discipline for promoting practical reuse of software. In CBD, by building new software with independently developed components, we can gain the benefits {{promised by the}} software reuse such as quality improvement and rapid development. Accordingly, to improve quality of components, we <b>propose</b> the component-based <b>metrics</b> applying the strength of dependency between classes to measure precisely. In addition, we prove the theoretical soundness <b>of</b> the <b>proposed</b> <b>metrics</b> by the axioms <b>of</b> Briand <b>et</b> al. and suggest the accuracy and practicality <b>of</b> the <b>proposed</b> <b>metrics</b> through a comparison with the conventional metrics in component development phase...|$|R
40|$|We <b>propose</b> a set <b>of</b> <b>metrics</b> that {{evaluate}} the uniformity, sharpness, continuity, noise, stroke width variance,pulse width ratio, transient pixels density, entropy and variance of components {{to quantify the}} quality of a document image. The measures are intended to be used in any optical character recognition (OCR) engine to a priori estimate the expected performance of the OCR. The suggested measures have been evaluated on many document images, which have different scripts. The quality of a document image is manually annotated by users to create a ground truth. The idea is to correlate the values of the measures with the user annotated data. If the measure calculated matches the annotated description,then the metric is accepted; else it is rejected. In the set <b>of</b> <b>metrics</b> <b>proposed,</b> some <b>of</b> them are accepted and the rest are rejected. We have defined metrics that are easily estimatable. The <b>metrics</b> <b>proposed</b> in this paper are based on the feedback of homely grown OCR engines for Indic (Tamil and Kannada) languages. The <b>metrics</b> are independent <b>of</b> the scripts, and depend only on the quality and age of the paper and the printing. Experiments and results for each proposed metric are discussed. Actual recognition of the printed text is not performed to {{evaluate the}} <b>proposed</b> <b>metrics.</b> Sometimes, a document image containing broken characters results in good document image as per the evaluated metrics, {{which is part of the}} unsolved challenges. The proposed measures work on gray scale document images and fail to provide reliable information on binarized document image...|$|R
40|$|Ontology {{languages}} such as OWL {{are being}} widely {{used as the}} Semantic Web movement gains momentum. With the proliferation of the Semantic Web, more and more large-scale ontologies are being developed in real-world applications to represent and integrate knowledge and data. There is an increasing need for measuring the complexity of these ontologies in order for people to better understand, maintain, reuse and integrate them. In this paper, inspired by the concept <b>of</b> software <b>metrics,</b> we <b>propose</b> a suite <b>of</b> ontology <b>metrics,</b> at both the ontology-level and class-level, to measure the design complexity <b>of</b> ontologies. The <b>proposed</b> <b>metrics</b> are analytically evaluated against Weyuker 2 ̆ 7 s criteria. We have also performed empirical analysis on public domain ontologies to show the characteristics and usefulness <b>of</b> the <b>metrics.</b> We point out possible applications <b>of</b> the <b>proposed</b> <b>metrics</b> to ontology quality control. We believe that the proposed metric suite is useful for managing ontology development projects...|$|R
40|$|A {{large number}} <b>of</b> <b>metrics</b> have been <b>proposed</b> {{for the quality}} of object {{oriented}} software. Many <b>of</b> these <b>metrics</b> have not been properly validated due to poor methods of validation and non acceptance <b>of</b> <b>metrics</b> on scientific grounds. In the literature, two types of validations namely internal (theoretical) and external (empirical) are recommended. In this study, the authors have used both theoretical as well as empirical validation for validating already <b>proposed</b> set <b>of</b> <b>metrics</b> for the five quality factors. These <b>metrics</b> were <b>proposed</b> by Kumar and Soni. Comment: 7 pages IEEE format, International Journal of Computer Science and Information Security, IJCSIS December 2009, ISSN 1947 5500, [URL]...|$|R
40|$|This paper {{describes}} the empirical {{evaluation of a}} set <b>of</b> <b>proposed</b> <b>metrics</b> for {{evaluating the quality of}} data models. A total of twenty nine candidate <b>metrics</b> were originally <b>proposed,</b> each of which measured a different aspect of quality of a data model. Action research was used to evaluate the usefulness <b>of</b> the <b>metrics</b> in five application development projects in two private sector organisations. <b>Of</b> the <b>metrics</b> originally <b>proposed,</b> only three “survived ” the empirical validation process, and two new metrics were discovered. The result was a set <b>of</b> five <b>metrics</b> which participants felt were manageable to apply in practice. An unexpected finding was that subjective ratings of quality and qualitative descriptions of quality issues were perceived to be much more useful than the metrics. While the idea <b>of</b> using <b>metrics</b> to quantify the quality of data models seems good in theory, {{the results of this study}} seem to indicate that it is not quite so useful in practice. The conclusion is that using a combination of “hard ” and “soft ” information (metrics, subjective ratings, qualitative description o...|$|R
40|$|Software {{engineering}} {{activities in the}} Industry {{has come a long}} way with various improve- ments brought in various stages of the software development life cycle. The complexity of modern software, the commercial constraints and the expectation for high quality products demand the accurate fault prediction based on OO design metrics in the class level in the early stages of software development. The object oriented class metrics are used as quality predictors in the entire OO software development life cycle even when a highly iterative, incremental model or agile software process is employed. Recent research has shown some of the OO design metrics are useful for predicting fault-proneness of classes. In this paper the empirical validation of a set <b>of</b> <b>metrics</b> <b>proposed</b> by Chidamber and Kemerer is performed to assess their ability in predicting the software quality in terms of fault proneness and degradation. We have also proposed the design complexity of object-oriented software with Weighted Methods per Class metric (WMC-CK metric) expressed in terms of Shannon entropy, and error proneness. Comment: 9 pages, 2 figure...|$|R
40|$|Abstract. Software {{engineering}} {{activities in the}} Industry {{has come a long}} way with various improvements brought in various stages of the software development life cycle. The complexity of modern software, the commercial constraints and the expectation for high quality products demand the accurate fault prediction based on OO design metrics in the class level in the early stages of software development. The object oriented class metrics are used as quality predictors in the entire OO software development life cycle even when a highly iterative, incremental model or agile software process is employed. Recent research has shown some of the OO design metrics are useful for predicting fault-proneness of classes. In this paper the empirical validation of a set <b>of</b> <b>metrics</b> <b>proposed</b> by Chidamber and Kemerer is performed to assess their ability in predicting the software quality in terms of fault proneness and degradation. We have also proposed the design complexity of object-oriented software with Weighted Methods per Class metric (WMC-CK metric) expressed in terms of Shannon entropy, and error proneness...|$|R
40|$|Abstract: Although some {{important}} technological developments {{have been achieved}} during last decade, information systems still do not answer efficiently enough to the continuous demands that organisations are facing – causing a non-alignment between business and information technologies (IT) and therefore reducing organisation competitive abilities. This paper <b>proposes</b> sixteen <b>metrics</b> for the Information System Architecture (ISA) evaluation, supported in an ISA modelling framework. The major goal <b>of</b> the <b>metrics</b> <b>proposed</b> is to assist the architect previewing the impact of his/her ISA design choices on the non-functional qualities of the Enterprise Information System (EIS), ensuring EIS better align with business needs. The <b>metrics</b> <b>proposed</b> {{are based on the}} research accomplished by other authors, from the knowledge in other more mature areas and on the authors experience on real world ISA evaluation projects. The <b>metrics</b> <b>proposed</b> are applied to an e-government project in order to support the definition of a suitable ISA for a set of business and technological requirements...|$|R
40|$|Instance based {{learning}} and clustering are popular methods in propositional machine learning. Both methods use {{a notion of}} similarity between objects. This dissertation investigates these methods in a relational setting. First, a number <b>of</b> new <b>metrics</b> are <b>proposed.</b> Next, these <b>metrics</b> are used to upgrade clustering and instance based learning to first order logic. status: publishe...|$|R
40|$|As {{object-oriented}} software development methods come into more widespread use, basic questions of {{software quality assurance}} must be reconsidered. We will highlight efforts now underway at NASA's Jet Propulsion Laboratory to both assess the quality of software systems developed using object oriented technology and develop guidelines for future development of such systems. The current focus is on design and code reusability., and system size estimation. A number <b>of</b> <b>metrics</b> are <b>proposed</b> and two software systems measured and analyzed. The preliminary results reported here should be useful to software development and quality assurance personnel working in C++ implementation environment...|$|R
40|$|In {{the pursuit}} of ever {{increasing}} productivity, {{the need to be}} able to measure specific aspects of software is generally agreed upon. As object oriented programming languages are becoming more and more widely used, metrics specifically designed for object oriented software are required. In recent years there has been an explosion of new, object oriented software <b>metrics</b> <b>proposed</b> in the literature. Unfortunately, many or most <b>of</b> these <b>proposed</b> <b>metrics</b> have not been validated to measure what they claim to measure. In fact, an analysis of many <b>of</b> these <b>metrics</b> shows that they do not satisfy basic properties of measurement theory, and thus their application has to be suspect. In this paper ten improved <b>metrics</b> are <b>proposed</b> and are validated using measurement theory...|$|R
40|$|In {{this work}} we study {{different}} classes <b>of</b> effective composite <b>metrics</b> <b>proposed</b> {{in the context}} of one-loop quantum corrections in bimetric gravity. For this purpose we consider contributions of the matter loops in form of cosmological constants and potential terms yielding two types <b>of</b> effective composite <b>metrics.</b> This guarantees a nice behaviour at the quantum level. However, the theoretical consistency at the classical level needs to be ensured additionally. It turns out that among all these possible couplings only one unique effective metric survives this criteria at the classical level. Comment: 5 page...|$|R
40|$|Abstract: In {{order to}} improve the quality of an {{application}} during the development process, developers use several metrics. These metrics measure the different software attributes such as cohesion, coupling and complexity. To measure the class cohesion several class cohesion metrics have been introduced till date. Cohesion is measured during the design phase to predict software quality. A high cohesive module is easier to understand, modify and maintain in comparison to a less cohesive module. Class cohesion metrics measure the relatedness of the methods and attributes within a class. Several <b>metrics</b> have been <b>proposed</b> in the literature to evaluate class cohesion based on the information that is available during high or low level design phases. This review paper discusses some <b>of</b> the <b>metrics</b> <b>proposed</b> till date...|$|R
40|$|Software {{developers}} {{develop their}} software with some standard specification, but important {{issue is how}} to measure the quality of software modularization. In this paper advanced set <b>of</b> <b>metrics</b> are <b>proposed</b> which measure quality of modularization of an Object-Oriented Software (OOS) System. Metrics are designed with universal standard principles to measure the quality of Object-Oriented software which has been developed. <b>Proposed</b> <b>metrics</b> are coupling based structural metrics, these metrics measures the function-call traffic through the API’s of the modules {{in relation to the}} overall function-call traffic. It is universally accepted that quality of the software modularization is improved when interaction between modules of system is through published APIs only. The metrics can be validated from the results obtained on human-modularized versions of the software...|$|R
