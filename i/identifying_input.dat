22|855|Public
30|$|This {{study showed}} that the {{indicator}} chemical/indicator bioassay approach is suitable for <b>identifying</b> <b>input</b> sources of a mixture of organic micropollutants and to trace changes in the water quality along small rivers. This method forms the necessary basis for evaluating the natural attenuation processes of organic micropollutants on a catchment scale, especially when combined with enhanced sampling strategies in future studies.|$|E
40|$|Boundary value {{analysis}} {{is a typical}} conventional testing technique. However, manually <b>identifying</b> <b>input</b> regions and writing test cases are labor-intensive and time-consuming. In this paper, we propose a search-based random testing approach, which automatically generates test data along the boundaries of semantic regions of the input domain. The experiments on mutated programs confirm the effectiveness and efficiency of the proposed approach. Furthermore, our approach significantly outperforms the conventional ART (Adaptive Random Testing) methods, which sample test cases evenly across the input regions. Our approach also outperforms EvoSuite, a state-of-the-art tool that generates test cases satisfying certain coverage criterion...|$|E
40|$|This paper {{deals with}} the {{optimization}} of data routing processes and with optimization of deployed quality of service mechanisms in computer networks. The paper addresses the problems related to requirement of monitoring and managing of network infrastructures with attention given to data routing mechanisms in network and often used QoS mechanisms. This paper also presents the concept of tool for automated network traffic management in order to network traffic optimization by <b>identifying</b> <b>input</b> curve α(t) and service curve β(t) with application of mechanisms for traffic shaping and adaptive elimination of aggressive data flows. Proposed methods are experimentaly verified and compared with conventional methods...|$|E
2500|$|Evaluating the {{potential}} impacts associated with <b>identified</b> <b>inputs</b> and releases; ...|$|R
5000|$|Weaver's Triangle (simply asks organisations to <b>identify</b> <b>inputs,</b> {{outcomes}} and outputs).|$|R
5000|$|... <b>identifying</b> <b>inputs</b> which accept only {{values of}} a {{specific}} type (e.g., date), and ...|$|R
40|$|Construction {{is a major}} {{industry}} in fast growing countries and plays {{a leading role in}} the process of economic development. Using input-output tables, the performance of the construction sector in six emerging countries (Brazil, Russia, India, Indonesia, China and South Africa) is compared from 1995 to 2005. First, the construction performance in these developing economies, by using standard indicators based on value added, gross output, final demand and intermediate inputs, is investigated. Then, the similarity cosine index is introduced to assess structural change and differences in input expenditures between countries. This index is a useful tool for <b>identifying</b> <b>input</b> bundles that require a probing international comparison of construction performance...|$|E
40|$|Variable selection, or {{the process}} of <b>identifying</b> <b>input</b> {{variables}} {{that are relevant to}} a particular learning problem, has recently received much attention in the learning community. Methods that employ the learning algorithm {{as a part of the}} selection process (wrappers) have been shown to outperform methods that select variables independent of the learning algorithm (filters), but only at great computational expense. We present a randomized wrapper algorithm for variable elimination that runs in time only a constant factor greater than that of simply learning in the presence of all input variables, provided that the cost of learning grows at least polynomially with the number of inputs...|$|E
40|$|Variable selection, {{the process}} of <b>identifying</b> <b>input</b> {{variables}} {{that are relevant to}} a particular learning problem, has received much attention in the learning community. Methods that employ a learning algorithm {{as a part of the}} selection process (wrappers) have been shown to outperform methods that select variables independently from the learning algorithm (filters), but only at great computational expense. We present a randomized wrapper algorithm whose computational requirements are within a constant factor of simply learning in the presence of all input variables, provided that the number of relevant variables is small and known in advance. We then show how to remove the latter assumption, and demonstrate performance on several problems. 1...|$|E
3000|$|Developers first analyse {{the input}} flows of the nodes that contain the automata {{modelling}} the devices. The {{aim is to}} <b>identify</b> the <b>input</b> flows that are controllable (their values are not given by the monitoring). Then, developers declare the <b>identified</b> <b>input</b> flows, in the contract part of the main H/BZR node, as controllable variables. For instance, in the lamp automaton, presented in Fig.  5 [...]...|$|R
40|$|This report {{describes}} and <b>identifies</b> <b>input</b> parameter values {{used for}} the hypothetical inadvertent intruder analyses for Radiological PAs at SRS. It is intended to describe and document the input parameters residing in the input file Intruder input of the intruder analysis application developed by Koffman (2004) ...|$|R
40|$|The {{transport}} interchange functioning {{has been}} presented as a gray box cybernetic model. The <b>identified</b> <b>input</b> data characterizes the transport interchange as an infrastructure object and influence the processes both for public transport and passengers. An integrated indicator has been chosen as the efficiency criterion, combining economic, environmental and social costs...|$|R
40|$|The {{problem of}} {{tracking}} {{small changes in}} the output wavelength of laser diodes is addressed using a dual slab waveguide interferometer fabricated from silicon oxynitride. Waveguide mode dispersion differences between the waveguide modes provide a mechanism for <b>identifying</b> <b>input</b> wavelength shifts that are measured as shifts in the output far-field diffraction image. At visible wavelengths the device can transduce input wavelength changes into phase responses with a sensitivity of + 0. 9 rad/nm. The lower threshold limit of detection for laser output frequency shifts, is 2. 2 GHz or 6 pm at a center wavelength of 635 run. The TE and TM sensitivities to wavelength are approximately equivalent in the device described...|$|E
40|$|Separability {{filter method}} is a {{reliable}} method for pupil detection. However, {{so far this}} method is implemented for detecting pupil of normal eye, while for abnormal eye such as cataract and glaucoma patients; they have different characteristics of pupil such as color, shape and radius size of pupil. In this paper we propose to use separability filter for detecting pupil of abnormal patients with different characteristics. We faced a problem about radius size, shape and color of pupil; therefore we implemented Hough Transform, Blob area and Brightness for <b>identifying</b> <b>input</b> images before applying separability filter. The experiment results show that we can increase performance of pupil detection for abnormal eye to be 95. 65 %...|$|E
40|$|A {{system for}} {{developing}} distributed robot application-level software includes a robot having an associated control module which controls {{motion of the}} robot {{in response to a}} commanded task, and a robot task commander (RTC) in networked communication with the control module over a network transport layer (NTL). The RTC includes a script engine(s) and a GUI, with a processor and a centralized library of library blocks constructed from an interpretive computer programming code and having input and output connections. The GUI provides access to a Visual Programming Language (VPL) environment and a text editor. In executing a method, the VPL is opened, a task for the robot is built from the code library blocks, and data is assigned to input and output connections <b>identifying</b> <b>input</b> and output data for each block. A task sequence(s) is sent to the control module(s) over the NTL to command execution of the task...|$|E
40|$|A {{new method}} for {{analyzing}} 3 D goal-directed movements provides more insights than existing approaches by dividing them into meaningful phases. An experiment applying a simple 3 D task, resembling a standardized 2 D multidirectional pointing task, yielded insights {{that can help}} researchers better <b>identify</b> <b>input</b> devices' and interaction techniques' strengths and weaknesses...|$|R
50|$|QTAM {{consists}} of a Message Control Program (MCP) and zero or more Message Processing Programs (MPP). The MCP handles communications with the terminals, <b>identifies</b> <b>input</b> messages and starts MPPs to process them as required. This is similar in concept to the much later internet service daemon (inetd) in unix and other systems.|$|R
40|$|The {{study results}} {{presented}} in this WSU fact sheet {{can serve as a}} general guide for evaluating the feasibility of establishing and producing cider apples in western Wash¬ington as of 2013. This fact sheet {{can also be used to}} <b>identify</b> <b>inputs,</b> costs, and yields that are considered typical of a well-managed cider apple orchard in western Washington...|$|R
40|$|An {{underlying}} assumption for N-version programming {{technique is}} that independently developed versions would fail in a statistically independent manner. However, empirical {{studies have demonstrated}} that common mode failures can occur even for independently developed versions, and that common mode failures degrade system reliability. In this paper, we demonstrate that the weakest precondition analysis is effective in determining input spaces leading to common mode failures. We applied the weakest precondition to the Launch Interceptor Programs which were used in several other experiments related to the N-version programming technique. We detected 13 out of 18 fault pairs which have been known to cause common mode failure. Thesefaults were due to logical fl aws in program design. Although the weakest precondition analysis may be labor-intensive since they are applied manually, our results convincingly demonstrate that it is effective for <b>identifying</b> <b>input</b> spaces causing common mode failures and further improving the reliability of N-version software...|$|E
40|$|In this article, we {{describe}} a business modeling exercise that helps {{students understand the}} complex relationship between demand and price. The exercise seeks to determine the optimum pricing, in view of anticipated occupancy response that maximises profit for a hotel. Through the exercise, students are introduced to advanced Excel operations such as Goal Seek and Solver. This exercise goes through a systematic series of basic modeling steps, starting from <b>identifying</b> <b>input</b> variables and performance measures, and building from a basic model to a final model with sufficient complexity to represent reality. A problem commonly encountered when modeling real-world problems {{is the lack of}} complete information; often, information has to be inferred from what little is available from the past. This is demonstrated in developing the hotel occupancy and rate relationship. To ensure the model is robust, we show how trade-off and sensitivity analyses can be conducted...|$|E
40|$|A {{statistical}} {{sensitivity analysis}} procedure {{has been developed}} for ranking the input data of large computer codes {{in the order of}} sensitivity-importance. The method is economical for large codes with many input variables, since it uses {{a relatively small number of}} computer runs. No prior judgmental elimination of input variables is needed. The screening method is based on stage-wise correlation and extensive regression analysis of output values calculated with selected input value combinations. The regression process deals with multivariate nonlinear functions, and statistical tests are also available for <b>identifying</b> <b>input</b> variables that contribute to threshold effects, i. e., discontinuities in the output variables. A computer code SCREEN has been developed for implementing the screening techniques. The efficiency has been demonstrated by several examples and applied to a fast reactor safety analysis code (Venus-II). However, the methods and the coding are general and not limited to such applications...|$|E
40|$|Before using {{a trained}} {{artificial}} neural network (ANN) in an application {{it is important to}} <b>identify</b> <b>inputs</b> which cause incorrect behaviours. We therefore propose the use of an evolutionary algorithm (EA) to invert the mappings of ANNs. The EA is used to search for input patterns which produce strong (distinct) classifications into one of the classes. Since the input space is typically very large, multimodal, and poorly understood, EAs {{are likely to be more}} robust than gradient methods, with a lower probability of getting stuck on local optima. Analysis of our results supports this hypothesis. Our evolutionary algorithm also involves the use of niching, which allows it to simultaneously explore multiple regions of the search space. The resulting population of input patterns therefore typically represents a set of distinctly different instances. This property is important, since the aim is to <b>identify</b> <b>inputs</b> which are erroneously classified. We show how analysis of the set of inputs fou [...] ...|$|R
50|$|TCAM {{consists}} of a Message Control Program (MCP) and zero or more application programs. The MCP handles communications with the terminals, <b>identifies</b> <b>input</b> messages and starts application programs to process them as required. This is similar in concept to the much later internet service daemon (inetd) in unix and other systems. It is also similar to QTAM, where the application programs are called Message Processing Programs (MPP).|$|R
50|$|In {{terms of}} encoding, a {{dataflow}} program might be implemented as a hash table, with uniquely <b>identified</b> <b>inputs</b> as the keys, {{used to look}} up pointers to the instructions. When any operation completes, the program scans {{down the list of}} operations until it finds the first operation where all inputs are currently valid, and runs it. When that operation finishes, it will typically output data, thereby making another operation become valid.|$|R
40|$|Penetration {{testing is}} widely used to help ensure the {{security}} of web applications. It discovers vulnerabilities by simulating attacks from malicious users on a target application. Identifying the input vectors of a web application and checking {{the results of an}} attack are important parts of penetration testing, as they indicate where an attack could be introduced and whether an attempted attack was successful. Current techniques for <b>identifying</b> <b>input</b> vectors and checking attack results are typically ad-hoc and incomplete, which can cause parts of an application to be untested and leave vulnerabilities undiscovered. In this paper, we propose a new approach to penetration testing that addresses these limitations by leveraging two recentlydeveloped analysis techniques. The first is used to identify a web application’s possible input vectors, and the second is used to automatically check whether an attack resulted in an injection. To empirically evaluate our approach, we compare it against a state-of-the-art, alternative technique. Our results show that our approach performs a more thorough penetration testing and leads to the discovery of more vulnerabilities...|$|E
40|$|Automatic {{test data}} {{generation}} usually concerns <b>identifying</b> <b>input</b> values that cause a selected path to execute. If a given path involves pointers, then input values may {{be represented in}} terms of 2 -dimensional dynamic data structures such as lists or trees. When testing is conducted for programs {{in the presence of}} pointers, {{it is very important to}} identify a shape of the input data structure describing how many nodes are required and how nodes are connected each other. The approach presented in this paper makes use of the points-to information for each statement in the selected path that will be used to represent the shape of an input data structure. It also converts each statement into static single assignment (SSA) form without pointer dereferences. This allows the approach to consider each statement in the selected path as a constraint involving equality or inequality to make use of current constraint solving systems without significant effort. The SSA form serves as a system of constraints to be solved to yield input values for non-pointer types. An empirical evaluation shows that shape generation can be achieved in linear time in terms of the number of pointer dereference operations. Keywords: Program Testing, Shape Generation, Automated Test Data Generation. ...|$|E
40|$|A {{research}} project continuing at RMIT University is exploring {{the resilience of}} port structures in a changing climate. Research completed to date comprises of identifying types of port infrastructure vulnerable to climate change, establishing materials and exposure conditions, developing deterioration models based on current knowledge to simulate the effect of climate change on key port infrastructure and modeling the selected elements of infrastructure to derive outcomes which will aid in decision making in port infrastructure management. A considerable effort has been concentrated on <b>identifying</b> <b>input</b> climate data most appropriate for the models developed. The modeling approach is {{presented in this paper}} for quantitative projections of damage probability on port infrastructure taking into account the variability of material type, design considerations and environmental exposures with a changing climate. This paper provides a summary of the research undertaken in the development of material deterioration models and their responses to a changing climate load. Using climate information drawn from historical weather records and future climate projections, existing deterioration models were refined to include climate data into modeling runs in order to analyze changes to deterioration rates of different materials when impacted by a change in climate variables. Outputs from this modeling process will assist port authorities in making informed decisions on maintenance and capital budget planning allowing for impacts of climate change...|$|E
5000|$|While genetic {{algorithms}} are very powerful tools {{to identify the}} fuzzy membership functions of a pre-defined rule base, they have their limitation especially when it also comes to <b>identify</b> the <b>input</b> and output variables of a fuzzy system from a given set of data. Genetic programming {{has been used to}} <b>identify</b> the <b>input</b> variables, the rule base as well as the involved membership functions of a fuzzy model (Bastian, 2000) ...|$|R
40|$|Sophisticated {{computer}} {{codes that}} implement mathematical models of physical processes can involve {{large numbers of}} inputs, and screening to determine the most active inputs is critical for understanding the input- output relationship. This article presents a new two-stage group screening methodology for <b>identifying</b> active <b>inputs.</b> In Stage 1, groups of inputs showing low activity are screened out; in Stage 2, individual inputs from the active groups are <b>identified.</b> <b>Inputs</b> are evaluated through their estimated total (effect) sensitivity indices (TSIs), which are compared with a benchmark null TSI distribution created from added low noise inputs. Examples show that, compared with other procedures, the proposed method provides more consistent and accurate results for high-dimensional screening. Additional details and computer code are provided in supplementary materials available online...|$|R
40|$|This Comment {{considers}} the current regulatory environments for fixed network telecommunications and cable {{television in the}} European Community to determine the implications for future broadband networks. Corresponding regulatory environments of the USA and Japan are also reviewed to <b>identify</b> <b>input</b> into future regulatory developments. The regulatory environments for telecommunications and cable television are compared and contrasted to highlight the potential hurdles in supporting both telecommunications and broadcast services over a single broadband network. ...|$|R
40|$|Abstract Background Over {{the past}} decade {{there has been a}} growing body of {{literature}} on how the Systematised Nomenclature of Medicine Clinical Terms (SNOMED CT) can be implemented and used in different clinical settings. Yet, for those charged with incorporating SNOMED CT into their organisation's clinical applications and vocabulary systems, there are few detailed encoding instructions and examples available to show how this can be done and the issues involved. This paper describes a heuristic method {{that can be used to}} encode clinical terms in SNOMED CT and an illustration of how it was applied to encode an existing palliative care dataset. Methods The encoding process involves: <b>identifying</b> <b>input</b> data items; cleaning the data items; encoding the cleaned data items; and exporting the encoded terms as output term sets. Four outputs are produced: the SNOMED CT reference set; interface terminology set; SNOMED CT extension set and unencodeable term set. Results The original palliative care database contained 211 data elements, 145 coded values and 37, 248 free text values. We were able to encode ~ 84 % of the terms, another ~ 8 % require further encoding and verification while terms that had a frequency of fewer than five were not encoded (~ 7 %). Conclusions From the pilot, it would seem our SNOMED CT encoding method has the potential to become a general purpose terminology encoding approach that can be used in different clinical systems. </p...|$|E
40|$|Increased {{utilization}} of low-quality hardwoods would {{aid in the}} management options of hardwood stands. With market development, materials of poor quality could be moved to productive uses while allowing higher quality stock more growing room. A description of a concentration and marketing center is offered {{that is designed to}} provide a simple selling opportunity for land owners and timber improvement contractors;This study develops a financial spreadsheet template to aid in feasibility analysis and business planning for potential marketing center scenarios. The spreadsheet and its accompanying documentation identify input data requirements. By <b>identifying</b> <b>input</b> needs for the spreadsheet, this study covers elements of market research that are required to support financial projections. The quality of the input data is critical to the usefulness of the output reports;The financial spreadsheet provides essential statements: (a) an income statement showing profit or loss; (b) a cash-flow forecast showing the timing of receipts and disbursements; and (c) a balance sheet showing assets and liabilities. The spreadsheet also provides support schedules: (a) sales projections; (b) direct costs statement; (c) indirect operations costs; (d) marketing and administrative costs; and (e) capital purchases schedule;To help analyze a wide range of possible functional forms that a marketing center could take, the spreadsheet template is flexible in application. 2 ̆ 2 What if 2 ̆ 2 analysis is easily performed through alteration of input values. Financial ratios for liquidity, profitability, and debt management are provided for comparison with industry averages...|$|E
40|$|Background: Over {{the past}} decade {{there has been a}} growing body of {{literature}} on how the Systematised Nomenclature of Medicine Clinical Terms (SNOMED CT) can be implemented and used in different clinical settings. Yet, for those charged with incorporating SNOMED CT into their organisation’s clinical applications and vocabulary systems, there are few detailed encoding instructions and examples available to show how this can be done and the issues involved. This paper describes a heuristic method {{that can be used to}} encode clinical terms in SNOMED CT and an illustration of how it was applied to encode an existing palliative care dataset. Methods: The encoding process involves: <b>identifying</b> <b>input</b> data items; cleaning the data items; encoding the cleaned data items; and exporting the encoded terms as output term sets. Four outputs are produced: the SNOMED CT reference set; interface terminology set; SNOMED CT extension set and unencodeable term set. Results: The original palliative care database contained 211 data elements, 145 coded values and 37, 248 free text values. We were able to encode ~ 84 % of the terms, another ~ 8 % require further encoding and verification while terms that had a frequency of fewer than five were not encoded (~ 7 %). Conclusions: From the pilot, it would seem our SNOMED CT encoding method has the potential to become a general purpose terminology encoding approach that can be used in different clinical systems...|$|E
30|$|A linear {{design of}} {{experiment}} was performed to <b>identify</b> key <b>input</b> parameters for the proposed quadratic surrogate model implemented on MATLAB.|$|R
30|$|As {{environmental}} awareness increases, industries and businesses {{have started to}} assess how their activities affect the environment. Life cycle assessment (LCA) is the best Environmental System Analysis tool to evaluate the environmental burdens associated with a product, process, or activity by: compiling an inventory of relevant energy and material inputs and environmental releases; evaluating the potential environmental impacts associated with <b>identified</b> <b>inputs</b> and releases; and, interpreting the results to help make a more informed decision [8].|$|R
40|$|An {{estimation}} of costs for maintenance and rehabilitation {{is subject to}} variation due to the uncertainties of input parameters. This paper {{presents the results of}} an analysis to <b>identify</b> <b>input</b> parameters that affect the prediction of variation in road deterioration. Road data obtained from 1688 km of a national highway located in the tropical northeast of Queensland in Australia were used in the analysis. Data were analysed using a probability-based method, the Monte Carlo simulation technique and HDM- 4 ’s roughness prediction model. The results of the analysis indicated that among the input parameters the variability of pavement strength, rut depth, annual equivalent axle load and initial roughness affected the variability of the predicted roughness. The second part of the paper presents an analysis to assess the variation in cost estimates due to the variability of the overall <b>identified</b> critical <b>input</b> parameters...|$|R
