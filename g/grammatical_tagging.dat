19|44|Public
50|$|<b>Grammatical</b> <b>tagging</b> of {{each word}} {{was added to}} the text of the SEC by an {{automatic}} process; the fact that this tagging was in machine-readable form made it possible to relate grammatical and prosodic information in the texts. Subsequent work used probabilistic models to develop further the <b>grammatical</b> <b>tagging</b> and to produce automatic parsing techniques.|$|E
50|$|In corpus linguistics, {{part-of-speech tagging}} (POS tagging or PoS tagging or POST), also called <b>grammatical</b> <b>tagging</b> or word-category disambiguation, {{is the process}} of marking up a word in a text (corpus) as {{corresponding}} to a particular part of speech, based on both its definition and its context - i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph.A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.|$|E
5000|$|Correct <b>grammatical</b> <b>tagging</b> {{will reflect}} that [...] "dogs" [...] is here {{used as a}} verb, not as the more common plural noun. Grammatical context {{is one way to}} {{determine}} this; semantic analysis {{can also be used to}} infer that [...] "sailor" [...] and [...] "hatch" [...] implicate [...] "dogs" [...] as 1) in the nautical context and 2) an action applied to the object [...] "hatch" [...] (in this context, [...] "dogs" [...] is a nautical term meaning [...] "fastens (a watertight door) securely").|$|E
5000|$|ABC gives {{grammatical}} {{information with}} around 30 tags, including both {{parts of speech}} ("V." [...] for verb) and other tags ("ID." [...] for idiom). CED provides just 11 <b>grammatical</b> <b>tags,</b> while CCD only marks numerals and classifiers. The comprehensive <b>grammatical</b> <b>tags</b> in ABC are [...] "a most valuable feature", especially for learners of Chinese, and many words {{have more than one}} grammatical function (e.g., [...] xuéxí 学习 學習 [...] study; learn; emulate ◆ [...] learning"). Another useful ABC <b>grammatical</b> <b>tag</b> is B.F. for [...] "bound form" [...] (as opposed to free form words, mentioned above), but this distinction is not usually indicated in PRC dictionaries such as CED and CCD (Sawer 2004: 311).|$|R
5000|$|... be {{the set of}} <b>grammatical</b> <b>tags</b> of the application, that is, the set of all {{possible}} tags which may be assigned to a word, and let ...|$|R
50|$|Language {{can also}} be a factor that typographers would take into {{consideration}} for word spacing. For a language like Latin, “most boundaries are marked by <b>grammatical</b> <b>tags,</b> and a smaller space is therefore sufficient”. In English, the ability to read a line easily, instead of needing {{to make sense of it}} first, is also attributed by good word spacing.|$|R
50|$|Gross's work, {{and that}} of the LADL, gives {{priority}} to the principles of methodological rigor, respect for data, empirical observation, comprehensive coverage of a language, and reproducibility of experiments. A systematic description of simple sentences of French yielded a dictionary based on the syntax identifying properties of words salient for parsing and <b>grammatical</b> <b>tagging,</b> and providing a reasoned and detailed classification of most of the elements of the French language. Indeed, before generative grammar adopted the Projection Principle or the Theta criterion, Gross had undertaken the systematic investigation of the interdependence of lexical entries and grammatical rules. It was for this reason that his methods and results were given the name lexicon-grammar. His students have verified this working hypothesis in many typologically diverse languages, including not only Romance languages and German, but also Modern Greek, Korean, Arabic, Malagasy, and other languages.|$|E
40|$|This paper {{illustrates}} {{an application}} of <b>grammatical</b> <b>tagging</b> as a methodological tool for the investigation of small specialized corpora. A contrastive analysis was performed on two tagged corpora that represent genres used {{for the purpose of}} financial disclosure: spoken earnings presentations and written earnings releases. The analysis focused on two key features that could be studied systematically and comprehensively thanks to grammatical tagging: lexical density and evaluative adjectives. The results revealed interesting differences between the two corpora that appeared to be influenced by mode, interactional setting, and role/status of speakers and writers. The study shows how <b>grammatical</b> <b>tagging</b> offers new ways to integrate quantitative and qualitative methods in order to better understand discourse used in specific communicative contexts...|$|E
40|$|The {{paper by}} Leech et al. {{describes}} {{the aims of}} the LOB Corpus <b>Grammatical</b> <b>Tagging</b> project, and explains the suite of programs we are using to achieve these aims. In this paper, I would like to look in greater detail at the theoretical basis of these programs; I shall attempt to explain exactly what constituent-likelihood grammar involves, and suggest some other applications of this probabilistic approach to natural language syntax analysis...|$|E
40|$|International audienceWe present SEJF, a lexical {{resource}} of Polish nominal, adjectival and adverbial multi-word expressions. It {{consists of an}} intensional module with about 4, 700 multi-word lemmas assigned to 160 inflection graphs, and an extensional module with 88, 000 automatically generated inflected forms annotated with <b>grammatical</b> <b>tags.</b> We show the results of its coverage evaluation against an annotated corpus. The resource is freely available under the Creative Commons BY-SA license...|$|R
50|$|Constraint Grammar (CG) is a methodological {{paradigm}} for {{natural language processing}} (NLP). Linguist-written, context dependent rules are compiled into a grammar that assigns <b>grammatical</b> <b>tags</b> ("readings") to words or other tokens in running text. Typical tags address lemmatisation (lexeme or base form), inflexion, derivation, syntactic function, dependency, valency, case roles, semantic type etc. Each rule either adds, removes, selects or replaces a tag or a set of <b>grammatical</b> <b>tags</b> in a given sentence context. Context conditions {{can be linked to}} any tag or tag set of any word anywhere in the sentence, either locally (defined distances) or globally (undefined distances). Context conditions in the same rule may be linked, i.e. conditioned upon each other, negated, or blocked by interfering words or tags. Typical CGs consist of thousands of rules, that are applied set-wise in progressive steps, covering ever more advanced levels of analysis. Within each level, safe rules are used before heuristic rules, and no rule is allowed to remove the last reading of a given kind, thus providing a high degree of robustness.|$|R
40|$|In this paper, {{we present}} a hybrid {{approach}} to align single words, compound words and idiomatic expressions from bilingual parallel corpora. The objective is to develop, improve and maintain automatically translation lexicons. This approach combines linguistic and statistical information {{in order to improve}} word alignment results. The linguistic improvements taken into account refer to the use of an existing bilingual lexicon, named entities recognition, <b>grammatical</b> <b>tags</b> matching and detection of syntactic dependency relations between words. Statistical information refer to the number of occurrences of repeated words, their positions in the parallel corpus and their lengths in terms of number of characters. Single-word alignment uses an existing bilingual lexicon, named entities and cognates detection and <b>grammatical</b> <b>tags</b> matching. Compound-word alignment consists in establishing correspondences between the compound words of the source sentence and the compound words of the target sentences. A syntactic analysis is applied on the source and target sentences in order to extract dependency relations between words and to recognize compound words. Idiomatic expressions alignment starts with a monolingual term extractio...|$|R
40|$|Abstract—Corpora {{present a}} basic informational source for varied NLP applications. Their {{construction}} becomes now days necessary. This paper presents a tool created to help syntactic tagging an Arabic Treebank. It {{is based on}} an already constructed grammar called ArabTAG which constitutes a representational method of Arabic grammatical rules using TAG formalism. This tagging tool is a node among a set of other pretreatment steps as: morpho-syntactic analysis, <b>grammatical</b> <b>tagging</b> and sentence segmentation. It gets as input a sentence and helps to give it the appropriate syntactic tree in an incremental manner. U I...|$|E
40|$|In {{collaboration}} with the English Department, University of Oslo, and the Nowegian Computing Centre for the Humanities, Bergen we {{have been engaged in}} the automatic <b>grammatical</b> <b>tagging</b> of the LOB (LancasterOslo/Bergen) Corpus of British English. The computer programs for this task are running at a success rate of approximately 96. 7 % and a substantial part of the 1, 000, 000 -word corpus has already been tagged. The {{purpose of this paper is}} to give an account of the project, with special reference to the methods of tagging we have adopted...|$|E
40|$|This study {{focuses on}} {{prosodic}} differences between speaking styles, and their automatic distinction. We aim at characterizing speaking styles {{with the purpose}} of distinguishing them from each other, modeling them and eventually adding expressivity to text-to-speech systems. This can be done with a multi-level annotation of varied corpora, based on automatic processes (like phonetic segmentation, <b>grammatical</b> <b>tagging)</b> and manual annotations (of perceived syllabic prominences and of delivery speech objects). Quantitative comparisons of various prosodic parameters are conducted through the acoustic and linguistic dimension to catch differences between speaking genre...|$|E
40|$|Abstract. We {{explore the}} use of a {{partially}} annotated corpus to build a dependency parser for Japanese. We examine two types of partially annotated corpora. It is found that a parser trained with a corpus that does not have any <b>grammatical</b> <b>tags</b> for words can demonstrate an accuracy of 87. 38 %, which is comparable to the current state-of-the-art accuracy on the Kyoto University Corpus. In contrast, a parser trained with a corpus that has only dependency annotations for each two adjacent bunsetsus (chunks) shows moderate performance. Nonetheless, it is notable that features based on character n-grams are found very useful for a dependency parser for Japanese. ...|$|R
40|$|In {{this paper}} {{we present a}} lexicon-based {{approach}} {{to the problem of}} morphological processing. Full-form words, lemmas and <b>grammatical</b> <b>tags</b> are interconnected in a DAWG. Thus, the process of analysis/synthesis is reduced to a search in the graph, which is very fast and can be performed even if several pieces of information are missing from the input. The contents of the DAWG are updated using an on-line incremental process. The proposed approach is language independent and it does not utilize any morphophonetic rules or any other special linguistic information. Comment: 4 (+ 2) pages, 3 figures, 1 table, 10 references. Keywords: Morphology, Directed Acyclic Word Graphs, Lexicon Structures. For related work, see also [URL]...|$|R
40|$|We {{describe}} {{a case study}} in which a memory-based learning algorithm is trained to simultaneously chunk sentences and assign <b>grammatical</b> function <b>tags</b> to these chunks. We compare the algorithm 's performance on this parsing task with varying training set sizes (yielding learning curves) and different input representations...|$|R
40|$|This paper {{describes}} {{work on the}} <b>grammatical</b> <b>tagging</b> of a {{newly created}} Norwegian speech corpus: the first corpus of modern Norwegian speech. We use an iterative procedure to perform computer-aided manual tagging of {{a part of the}} corpus. This material is then used to train the final taggers, which are applied {{to the rest of the}} corpus. We experiment with taggers that are based on three different data-driven methods: memory-based learning, decision trees, and hidden Markov models, and find that the decision tree tagger performs best. We also test the effects of removing pauses and/or hesitations from the material before training and applying the taggers. We conclude that these attempts at cleaning up hurt the performance of the taggers, indicating that such material, rather than functioning as noise, actually contributes important information about the grammatical function of the words in their nearest context. ...|$|E
40|$|The paper  deals  with the Old Russian  subcorpus of the Russian National Corpus that {{comprises}} original Old Russian texts (chronicles, hagiography etc.) and Old Russian translations from Greek. The subcorpus contains approximately 500 000 words. The paper {{describes the}} <b>grammatical</b> <b>tagging</b> {{used in the}} subcorpus and possible ways of constructing complex queries taking into account specific features of Old Russian texts and with special emphasis on possibility to construct syntactic queries. The results of some queries are presented and analyzed in the article: 1) the usage of the construction “finite form of the verb byti + participle” (e. g. bě lovy děja), 2) the lack of active praesens participle and imperfect forms of the perfective verbs dati and pustiti except the contexts with negation, 3) word orders in the group “finite verb + direct object + indirect object represented by the pronoun и (ємѹ, єи, имъ) ”...|$|E
40|$|Abstract — Parts of Speech Tagger (POS) also called, as <b>grammatical</b> <b>tagging</b> or word {{category}} disambiguation, is {{the task}} of assigning to each word of a text the proper POS tag in its context of appearance in sentences. The importance of the problem focuses {{from the fact that}} the POS is one of the first stages in the process performed by various natural language related process. There are different approaches to the problem of assigning a part of speech (POS) tag to each word of a natural language sentence. Here we present a tag set for the Indian language Malayalam, a relatively free word order morphologically productive and agglutinative language. The paper is about performing Part of speech tagging for the Indian language Malayalam using SVM Tool, which was implemented using support vector machines and TnT tagger, which was built using Hidden Markov Model. The SVMTool is giving accuracy of 87. 5 % and TnT tagger is giving an accuracy of about 80 %...|$|E
40|$|In this paper, we {{introduce}} {{our recent}} work on re-annotating the deep information, which includes both the <b>grammatical</b> functional <b>tags</b> and the traces, in a Chinese scientific tree-bank. The issues {{with regard to}} re-annotation and its corresponding solutions are discussed. Furthermore, {{the process of the}} re-annotation work is described...|$|R
40|$|Subsymbolic {{systems have}} been {{successfully}} used to model several aspects of human language processing. Such parsers are appealing because they allow revising the interpretation as words are incrementally processed. Yet, {{it has been very}} hard to scale them up to realistic language due to training time, limited memory, and the difficulty of representing linguistic structure. In this study, we show {{that it is possible to}} keep track of long-distance dependencies and to parse into deeper structures than before based on two techniques: a localist encoding of the input sequence and a dynamic unrolling of the network according to the parse tree. With these techniques, the system can nonmonotonically parse a corpus of realistic sentences into parse trees labelled with <b>grammatical</b> <b>tags</b> from a broadcoverage Head-driven Phrase Structure Grammar of English...|$|R
5000|$|The Bulgarian Part of Speech-{{annotated}} Corpus (BulPosCor) (in Bulgarian: Български Пос анотиран корпус (БулПосКор)) is a morphologically annotated general monolingual {{corpus of}} written language where each item {{in a text}} is assigned a <b>grammatical</b> <b>tag.</b> BulPosCor is created by the Department of Computational Linguistics at the Institute for Bulgarian Language of the Bulgarian Academy of Sciences and consists of 174 697 lexical items.BulPosCor has been compiled from the Structured [...] "Brown" [...] Corpus of Bulgarian by sampling 300+ word-excerpts (expanded to sentence boundary) from the original BCB files {{in such a way}} as to preserve the BCB overall structure. The annotation process consists of a primary stage of automatically assigning tags from the Bulgarian Grammar Dictionary and a stage of manual resolving of morphological ambiguities.The disambiguated corpus consists of 174,697 lexical units.|$|R
40|$|Abstract. This {{research}} {{represents the}} first attempt to produce a working system for the automatic processing of texts of Bahasa Melayu ‘Malay’. At {{the heart of the}} system is an integrated relational lexical database called MALEX, which draws on the experience of working on English and other languages, but which is specifically tailored to the conditions of Malay. The development of the database is from the beginning entirely data driven, and is based on the analysis of a corpus of naturally produced Malay texts. In designing procedures which access the database, properties of the text are consistently and rigorously distinguished from properties of the lexicon and of the grammar. The system is currently used to provide information for a range of applications, for <b>grammatical</b> <b>tagging,</b> stemming and lemmatisation, parsing, and for generating phonological representations. It is hoped and intended that the design features of MALEX will be transferable, and provide a model for the development of working systems for other Asian languages...|$|E
40|$|The thesis was {{supervised}} by Jane Simpson, with Vrasidas Karalis (Department of Modern Greek). This thesis traces {{the history of}} usage {{of a group of}} words (‘P-words’) that were adverbial particles in Proto-Indo-European and became in Greek, as in many other IE languages, both prepositions and verbal prefixes. It adopts a corpus-linguistic approach which, when allied with a suitable statistical method and a theoretical framework for analysis of syntactic change (grammaticalisation), allows for the detection and sometimes the explanation of trends in usage which may be invisible to a general reader. However, this method relies on the availability of suitably tagged texts for analysis; such tagging exists for the New Testament, the basis of the statistical analyses of this study, and a few other documents of roughly the same period of Greek, but not for large portions of text from other periods. The finding of this paper is that the method is reliable and likely to produce interesting results once diachronic comparisons and same-period genre and register comparisons become possible with the production of standardised <b>grammatical</b> <b>tagging</b> of texts, a program that is being pursued in New Testament studies and has potential for much wider use in Greek linguistics...|$|E
40|$|The Newcastle Electronic Corpus of Tyneside English (NECTE) {{presented}} {{a number of}} problems not encountered by those producing corpora of standard varieties. The primary material consisted of audio recordings which needed to be orthographically transcribed and grammatically tagged. Preston (1985), (2000), Macaulay (1991), Kirk (1997), Cameron (2001) and Beal (2005) all note that representing vernacular Englishes orthographically, e. g. by using "eye dialect", can be problematic on various levels. Apart from unwelcome associations with negative political, racial or social connotations, there are theoretical objections to devising non-standard spellings which represent certain groups of vernacular speakers, thus making their speech appear more differentiated from mainstream colloquial varieties than is warranted. In {{the first half of this}} paper, we outline the principles and methods adopted in devising an Orthographic Transcription Protocol (OTP) for such a vernacular corpus, and the challenges faced by the NECTE team in practice. Protocols for <b>grammatical</b> <b>tagging</b> have likewise been devised with standard varieties in mind. In the second half, we relate how existing part-of-speech (POS) -tagging software (CLAWS 4, cf. Garside & Smith 1997; and Template Tagger, cf. Fligelstone et al. 1997) had to be adapted to take account of the non-standard grammar of Tyneside English...|$|E
40|$|In {{this paper}} we explore the use of text-mining methods for the {{identification}} of the author of a text. For the first time we apply the support vector machine (SVM) to this problem. As it is able to cope with half a million of inputs it requires no feature selection and can process the frequency vector of all words of a text. We performed a number of experiments with texts from a German newspaper. With nearly perfect reliability the SVM was able to reject other authors and detected the target author in 60 - 80 % of the cases. In a second experiment we ignored nouns, verbs and adjectives and replaced them by <b>grammatical</b> <b>tags</b> and bigrams. This resulted in slightly reduced performance. Author detection with SVM on full word forms was remarkably robust even if the author wrote about different topics...|$|R
40|$|International audienceIn this paper, {{we present}} a hybrid {{approach}} to align single words, compound words and idiomatic expressions from bilingual parallel corpora. The objective is to develop, improve and maintain automatically translation lexicons. This approach combines linguistic and statistical information {{in order to improve}} word alignment results. The linguistic improvements taken into account refer to the use of an existing bilingual lexicon, named entities recognition, <b>grammatical</b> <b>tags</b> matching and detection of syntactic dependency relations between words. Statistical information refer to the number of occurrences of repeated words, their positions in the parallel corpus and their lengths in terms of number of characters. Single-word alignment uses an existing bilingual lexicon, named entities and cognates detection and <b>grammatical</b> <b>tags</b> matching. Compound-word alignment consists in establishing correspondences between the compound words of the source sentence and the compound words of the target sentences. A syntactic analysis is applied on the source and target sentences in order to extract dependency relations between words and to recognize compound words. Idiomatic expressions alignment starts with a monolingual term extraction for each of the source and target languages, which provides a list of sequences of repeated words and a list of potential translations. These sequences are represented with vectors which indicate their numbers of occurrences and the numbers of segments in which they appear. Then, the translation relation between the source and target expressions are evaluated with a distance metric. The single and compound word aligners have been evaluated on a subset of 1103 sentences in English and French of the JOC (Official Journal of the European Community) corpus. The obtained results showed that these aligners generate a translation lexicon with 90 % of precision for single words and 84 % of precision for compound words. We evaluated the idiomatic expressions aligner on a subset of the Canadian Parliament Hansard corpus and we obtained a precision of 81 %...|$|R
40|$|Abstract. Part-of-speech <b>tagging</b> (POS) assigns <b>grammatical</b> <b>tags</b> (like noun, verb, etc.) to a word {{depending}} on its definition and its context. This {{is a first}} step before parsing may be applied. POS tagging and more generically word tagging, {{plays an important role in}} computational linguistics and in many information retrieval and text mining tasks. Neither pure rule based, nor machine learning approaches give satisfying results: rule based systems can not adapt well to existing samples; machine learning systems ignore available background knowledge. A combination of both is needed. In particular, we show some cases where the initialization of a neural network based tagger with background knowledge obtains better results. In this preliminary work we study some implications of embedding background knowledge for POS and word tagging problems. Preliminary results show that the combined system outperforms a purely machine learning system when only limited samples are available. ...|$|R
40|$|The {{ambiguity}} of words {{is a crucial}} question when dealing with an automatic analysis of documentary data bases. In Text Mining, Word Sense Disambiguation is the task of giving a particular sense to a term with different meanings both {{in the case of}} coincidental and polysemous homographs. In literature, the proposed solutions are mainly based on two elements: some knowledge related to the term and {{the context in which the}} term appears. Limiting the related knowledge to <b>grammatical</b> <b>tagging,</b> and to an analysis of collocations, here we focus our attention on identifying the context, in a data driven approach. Our framework is based on Textual Data Analysis and we assume that language and knowledge can be modeled as networks of words and the relations between them. The aim of this paper is to propose an extension of the strategy for building lexical sources in Balbi et al. (2012), in order to deal with ambiguous words. The methodological basis is given by the joint use of lexical Correspondence Analysis and Network Analysis. Our idea is investigating the neighborhood of ambiguous terms, with respect to the different latent semantic components, emerging thanks to Correspondence Analysis of a training set of documents, in order to build some rules, useful in solving WSD problems in the entire corpus...|$|E
40|$|Abstract: In this work, {{we present}} a tagging system which classifies the words in a non-vocalized Arabic text to their tags. The {{proposed}} tagging system passes through three levels of analysis. The first level is a lexical analyzer that composed of a lexicon containing all fixed words and particles such as prepositions and pronouns. The second level is a morphological analyzer which relies on word structure using patterns and affixes to determine word class. The third level is a syntax analyzer or a <b>grammatical</b> <b>tagging</b> which relies {{on the process of}} assigning grammatical tags to words based on their context or the position of the word in the sentence. The syntax analyzer level consists of two stages: the first stage depends on specific keywords that inform the tag of the successive word, the second stage is the reversed parsing technique which scans the available grammars of Arabic language to get the class of a single ambiguity word in the sentence. We have tested the proposed system on a corpus consists of 2355 words. Experimental results showed that the proposed system achieved a rate of success approaching 94 % {{of the total number of}} words in the sample used in the study. Keywords: Part-of-speech tagging, lexical analyzer, morphological analyzer, Arabic language processing...|$|E
40|$|Corpus {{linguistics}} studies {{take advantage}} of the existence of large collections of language production (written or spoken language) in order to investigate a language. Foreign language pedagogy is now beginning to see new possibilities for recent advances in corpus linguistics to improve language teaching and learning. Readily available Korean-language corpora and easy-to-use tools can now be used on the spot in a language teaching context, by teachers and learners without extensive training in computational linguistics, and studies of linguistic features can be tailored to specific pedagogic context and learning requirements. The NFLRC Summer Institute workshop Corpus Linguistics for Korean Language Learning and Teaching will acquaint participants with the basic concepts of corpus linguistics, including corpus construction and annotation, concordancing, frequency counts and ranks, <b>grammatical</b> <b>tagging,</b> and related concepts. Participants will learn how to access Korean corpora and how to use the available computer programs for Korean corpus analysis. The workshop will outline techniques for the teacher to use in materials preparation and curriculum design and for the individual learner to use in exploring meaning, structure, and use in Korean. In {{the second part of the}} workshop, participants will engage in individual projects chosen with their own learning goals in mind...|$|E
50|$|In 2014, the {{morphological}} marking of the Tatar Corpus {{was carried}} out. The meta-language of grammatical labels {{is based on}} the system of tags for Turkic languages developed by the international project Apertium. This project is aimed to develop automatic translating system for a big variety of languages. The main arguments in favor of choosing Apertium's morphological tagger for marking the Corpus are:- high quality of morphological annotation;- its being an Open Source project: all the source code and data are publicly available for all for free.The Complex Morphological Search system developed by us in 2015-2016 allows to perform searches in the Corpus by different combinations of such parameters as word form, lemma, morphological (<b>grammatical)</b> <b>tags</b> set, beginning of the word, middle part, end of the word, and the distance between searched words. The maximum length of the search query is five tokens + accordingly four distances between them.|$|R
40|$|International audienceIn this article, {{we present}} an {{experiment}} of linguistic parameter tuning in {{the representation of}} the semantic space of polysemous words. We evaluate quantitatively the influence of some basic linguistic knowledge (lemmas, multi-word expressions, <b>grammatical</b> <b>tags</b> and syntactic relations) on the performances of a similarity-based Word-Sense disambiguation method. The question we try to answer, by this experiment, is which kinds of linguistic knowledge are most useful for the semantic disambiguation of polysemous words, in a multilingual framework. The experiment is about 20 French polysemous words (16 nouns and 4 verbs) and we make use of the French-English part of the sentence-aligned EuroParl Corpus for training and testing. Our results show a strong correlation between the system accuracy and the degree of precision of the linguistic features used, particularly the syntactic dependency relations. Furthermore, the lemma-based approach absolutely outperforms the word form-based approach. The best accuracy achieved by our system amounts to 90 %...|$|R
40|$|Among tagged {{language}} {{resources for}} Arabic {{there is a}} high density for Modern Standard Arabic. Nonetheless, the tagged corpora for Classical Arabic are of very low density. Moreover, such corpora are normally developed applying software that are of serious shortcomings. This paper is elaborating on the tagging approach of the Islamic corpora which are being tagged at Noorsoft, Qom, Iran, exploiting Mobin Expert System of Mahmoud Shokrollahi-Far at University College of Nabi-Akram, Tabriz, Iran. The system relying on the traditional grammar of Arabic where there are just three parts-of-speech, after tokenizing the phrasal words, bootstraps the <b>grammatical</b> <b>tags</b> in the corpora employing the vocalism in the vowelized texts. This gives the opportunity for the system to incorporate a tagset which is morpho-syntactically as diverse as possible. The prepared corpora to be tagged, being in a variety of Islamic genre, consist of 1 G of phrasal words, whose tagged output is in xml format...|$|R
