52|111|Public
25|$|In September 2007, NetApp sued Sun {{claiming}} that ZFS infringed some of NetApp's patents on Write Anywhere <b>File</b> <b>Layout.</b> Sun counter-sued in October {{the same year}} claiming the opposite. The lawsuits were ended in 2010 with an undisclosed settlement.|$|E
5000|$|Write Anywhere <b>File</b> <b>Layout</b> (WAFL) NetApp's {{proprietary}} <b>file</b> <b>layout</b> ...|$|E
50|$|The Write Anywhere <b>File</b> <b>Layout</b> (WAFL) is a <b>file</b> <b>layout</b> that {{supports}} large, high-performance RAID arrays, quick restarts without lengthy consistency checks {{in the event}} of a crash or power failure, and growing the filesystems size quickly. It was designed by NetApp for use in its storage appliances.|$|E
50|$|The Apple III used a keyboard-pattern file loaded from {{a floppy}} disk: the {{standard}} system-software package included QWERTY and Dvorak <b>layout</b> <b>files.</b> Changing <b>layouts</b> required restarting the machine.|$|R
40|$|This paper {{describes}} transformation {{techniques for}} out-of-core pro-grams (i. e., those {{that deal with}} very large quantities of data) based on exploiting locality {{using a combination of}} loop and data trans-formations. Writing efficient out-of-core program is an arduous task. As a result, compiler optimizations directed at improving I/O performance are becoming increasingly important. We de-scribe how a compiler can improve the performance of the code by determining appropriate <b>file</b> <b>layouts</b> for out-of-core arrays and finding suitable loop transformations. In addition to optimizing a single loop nest, our solution can handle a sequence of loop nests. We also show how to generate code when the <b>file</b> <b>layouts</b> are optimized. Experimental results obtained on an Intel Paragon distributed-memory message-passing multiprocessor demonstrate marked improvements in performance due to the optimizations de-scribed in this paper. ...|$|R
50|$|Open report {{definition}} <b>files,</b> report <b>layout</b> {{is stored}} in XML.|$|R
50|$|Camera file {{systems can}} usually be {{accessed}} by directly mounting them via the USB mass storage device class protocol, which exposes the <b>file</b> <b>layout,</b> whether DCF compliant or otherwise. Alternatively, and independent of DCF, files may be accessed via the Picture Transfer Protocol, which provides an object-oriented view and need not expose the <b>file</b> <b>layout.</b>|$|E
5000|$|TB-2014-006 - Informative Notes on SMPTE ST 2065-4: ACES Image Container <b>File</b> <b>Layout</b> ...|$|E
5000|$|Write Anywhere <b>File</b> <b>Layout</b> - NetApp's storage {{solutions}} {{implement a}} file system called WAFL, which uses snapshot technology to keep {{different versions of}} all files in a volume around.|$|E
50|$|He {{documented}} the <b>file</b> system <b>layout</b> {{as part of}} the process.|$|R
5000|$|Mount {{namespace}} allows {{creating a}} different <b>file</b> system <b>layout,</b> or making certain mount points read-only.|$|R
40|$|With the {{increasing}} number of scientific applications manipulating huge amounts of data, effective high-level data management is an increasingly important problem. Unfortunately, so far the solutions to the high-level data management problem either require deep understanding of specific storage architectures and <b>file</b> <b>layouts</b> (as in high-performance file storage systems) or produce unsatisfactory I/O performance in exchange for ease-of-use and portability (as in relational DBMSs) ...|$|R
50|$|The {{hardware}} NetCache appliance {{included the}} NetApp Data ONTAP microkernel, with its Write Anywhere <b>File</b> <b>Layout</b> file system, achieving {{four times the}} throughput as software equivalents available at time, according to NetApp comparisons.|$|E
50|$|The {{most common}} special file is the {{directory}}. The layout of a directory file {{is defined by}} the filesystem used. As several filesystems, both native and non-native, are available under Unix, there is not one directory <b>file</b> <b>layout.</b>|$|E
50|$|In September 2007, NetApp sued Sun {{claiming}} that ZFS infringed some of NetApp's patents on Write Anywhere <b>File</b> <b>Layout.</b> Sun counter-sued in October {{the same year}} claiming the opposite. The lawsuits were ended in 2010 with an undisclosed settlement.|$|E
40|$|The Board of Equalization (BOE) has {{completed}} its January 2010 revision Motor Fuels Electronic Filing Handbook (eFiling Guide). The eFiling Guide has been updated to include <b>file</b> <b>layouts</b> for the Aircraft Jet Fuel Dealer Return, to reflect current operating practices, and provide expanded reporting examples. This notice highlights the major {{changes to the}} eFiling Guide and the electronic file maps. • Added disclaimer; page 1 - 2...|$|R
5000|$|Metadata server (MDS) — {{manages the}} {{location}} (<b>layout)</b> of <b>files,</b> file access and namespace hierarchy. The {{current version of}} MooseFS does support multiple metadata servers and automatic failover. Clients only talk to the MDS to retrieve/update a <b>file's</b> <b>layout</b> and attributes; the data itself is transferred directly between clients and chunk servers. The Metadata server is a user-space daemon; the metadata is kept in memory and lazily stored on local disk.|$|R
5000|$|... //NEWFILE DD DSN=MYFILE01,UNIT=DISK,SPACE=(TRK,80,10),// DCB=(LRECL=100,BLKSIZE=1000),// DISP=(NEW,CATLG,DELETE)All of {{the major}} {{parameters}} of OS JCL statements are identified by keywords and can be presented in any order. A few of these contain two or more sub-parameters, such as [...] (how much disk space to allocate to a new file) and [...] (detailed specification of a <b>file's</b> <b>layout)</b> in the example above. Sub-parameters are sometimes positional, as in , but the most complex parameters, such as , have keyword sub-parameters.|$|R
5000|$|Autopackage {{packages}} are {{indicated by the}} [...]package extension. They are executable bash scripts, and can be installed by running them. Files in an Autopackage archive are not easily extracted by anything other than Autopackage itself as the internal format must be parsed {{in order to determine}} <b>file</b> <b>layout</b> and other issues.|$|E
50|$|As {{the name}} {{suggests}} Write Anywhere <b>File</b> <b>Layout</b> does not store data or metadata in pre-determined locations on disk, instead it automatically places data using temporal locality to write metadata alongside user {{data in a}} way designed to minimize the number of disk operations required to commit data to stable disk storage using single and dual parity based RAID.|$|E
5000|$|If the {{referring}} pages {{must also}} be updated via shadow paging, this procedure may [...] recurse many times, becoming quite costly. One solution, employed by the WAFL file system (Write Anywhere <b>File</b> <b>Layout)</b> is to be lazy about making pages durable (i.e. write-behind caching). This increases performance significantly by avoiding many writes on hotspots {{high up in the}} referential hierarchy (e.g.: a file system superblock) at the cost of high commit latency.|$|E
50|$|Visual Components files use a {{proprietary}} structured file format. This {{means that there}} are various files embedded within each VCM (component model & <b>layout</b> <b>file)</b> and VCP (<b>layout</b> package) <b>file</b> including preview bitmaps, geometry information and metadata sub-files.|$|R
50|$|The <b>file</b> system <b>layout</b> {{is often}} opaque to users, as images are copied onto a {{computer}} or printer {{and the application}} deals with layout.|$|R
50|$|Version 3.0 {{was also}} sold by Sun Microsystems as Solstice FireWall-1. This was {{essentially}} the same product, but with slightly different packaging and <b>file</b> system <b>layout.</b>|$|R
50|$|After {{the initial}} lookup of the <b>file</b> <b>layout,</b> the MDS is not {{involved}} in file IO operations since all block allocation is managed internally by the OST. Clients do not directly modify the objects on the OST filesystems, but instead delegate this task to OSS nodes. This approach ensures scalability for large-scale clusters and supercomputers, as well as improved security and reliability. In contrast, shared block-based filesystems such as Global File System and OCFS must allow direct access to all of the underlying storage by all of the clients in the filesystem, which requires a large back-end SAN attached to all clients, and increases the risk of filesystem corruption from misbehaving/defective clients.|$|E
5000|$|A Tux3 inode is a {{variable}} sized object {{consisting of a}} list of attributes, each labeled by the version at which the attribute was added, changed or removed. The B-tree index by which inodes are referenced is not versioned. The B-tree index of a large file is also not versioned, except for {{the leaves of the}} index, which contain lists of extents, each labeled by the version at which the referenced file data was added or changed. This style of versioning is essentially the inverse of tree versioning methods used by Write Anywhere <b>File</b> <b>Layout</b> (WAFL), ZFS and Btrfs, where multiple tree roots of an entire filesystem are created in order to express differences between versions of the filesystem.|$|E
5000|$|One or more {{metadata}} servers (MDS) nodes {{that has}} {{one or more}} metadata target (MDT) devices per Lustre filesystem that stores namespace metadata, such as filenames, directories, access permissions, and <b>file</b> <b>layout.</b> The MDT data is stored in a local disk filesystem. However, unlike block-based distributed filesystems, such as GPFS and PanFS, where the metadata server controls all of the block allocation, the Lustre metadata server is only involved in pathname and permission checks, and {{is not involved in}} any file I/O operations, avoiding I/O scalability bottlenecks on the metadata server. The ability to have multiple MDTs in a single filesystem is a new feature in Lustre 2.4, and allows directory subtrees to reside on the secondary MDTs, while 2.7 and later allow large single directories to be distributed across multiple MDTs as well.|$|E
40|$|Effective {{high-level}} {{data management}} {{is becoming an}} important issue {{with more and more}} scientific applications manipulating huge amounts of secondary-storage and tertiary-storage data using parallel processors. A major problem facing the current solutions to this data management problem is that these solutions either require a deep understanding of specific data storage architectures and <b>file</b> <b>layouts</b> to obtain the best performance (as in high-performance storage management systems and parallel file systems) or they sacrifice significant performance in exchange for ease-of-use and portability (as in traditional database management systems). In thi...|$|R
40|$|Parallel NFS (pNFS) Flexible <b>Files</b> <b>Layout</b> draft-bhalevy-nfsv 4 -flex-files- 00 Parallel NFS (pNFS) extends Network File System version 4 (NFSv 4) {{to allow}} clients to {{directly}} access file {{data on the}} storage used by the NFSv 4 server. This ability to bypass the server for data access can increase both performance and parallelism, but requires additional client functionality for data access, some of which {{is dependent on the}} class of storage used, a. k. a. the Layout Type. The main pNFS operations and data types in NFSv 4 Minor version 1 specify a layouttype-independent layer; layout-type-specific information is conveyed using opaque data structures whose internal structure is further defined by the particular layout type specification. This document specifies the NFSv 4. 1 Flexible <b>Files</b> pNFS <b>Layout</b> as a companion to the main NFSv 4 Minor version 1 specification for use of pNFS with Data Servers over NFSv 4 or higher minor versions using flexible, perfile striping topology...|$|R
40|$|This paper {{presents}} Clusterfile, {{a parallel}} file system that provides parallel file access on {{a cluster of}} computers. Existing parallel file systems offer little control over matching the I/O access patterns and <b>file</b> data <b>layout.</b> Without this matching the applications may face the following problems: contention at I/O nodes, fragmentation of file data, false sharing, small network messages, high overhead of scattering /gathering the data. Clusterfile addresses some of these inefficiencies. Parallel applications can physically partition a file in arbitrary patterns. They can also set arbitrary views on a file. Views hide the parallel structure of the file and ease the programmer's burden of computing complex access indices. The intersections between views and layouts are computed by a memory redistribution algorithm. Read and write operations are optimized by pre-computing the direct mapping between access patterns and disks. Clusterfile uses the same data representation for <b>file</b> <b>layouts,</b> access patterns, and the mappings between each other...|$|R
50|$|When {{a client}} accesses a file, it {{performs}} a filename lookup on the MDS. When the MDS filename lookup is complete, either {{the layout of}} an existing file is returned to the client or a new file is created {{on behalf of the}} client, if requested. For read or write operations, the client then interprets the <b>file</b> <b>layout</b> in the logical object volume (LOV) layer, which maps the file offset and size to one or more objects, each residing on a separate OST. The client then locks the file range being operated on and executes one or more parallel read or write operations directly to the OSS nodes. With this approach, bottlenecks for client-to-OSS communications are eliminated, so the total bandwidth available for the clients to read and write data scales almost linearly with the number of OSTs in the filesystem.|$|E
5000|$|Lustre 2.9 was {{released}} in December 2016and included a number of features related to security and performance. The Shared Secret Key security flavour uses the same GSSAPI mechanism as Kerberos to provide client and server node authentication, and RPC message integrity and security (encryption). The Nodemap feature allows categorizing client nodes into groups and then mapping the UID/GID for those clients, allowing remotely-administered clients to transparently use a shared filesystem without having a single set of UID/GIDs for all client nodes. The subdirectory mount feature allows clients to mount {{a subset of the}} filesystem namespace from the MDS. This release also added support for up to 16MiB RPCs for more efficient I/O submission to disk, and added the [...] interface to allow clients to provide I/O hints to the servers to prefetch file data into server cache or flush file data from server cache. There was improved support for specifying filesystem-wide default OST pools, and improved inheritance of OST pools in conjunction with other <b>file</b> <b>layout</b> parameters.|$|E
40|$|<b>File</b> <b>layout</b> of {{array data}} is a {{critical}} factor that effects the behavior of storage caches, and has so far taken not much attention {{in the context of}} hierarchical storage systems. The main contribution of this paper is a compiler-driven <b>file</b> <b>layout</b> optimization scheme for hierarchical storage caches. This approach, fully automated within an optimizing compiler, analyzes a multi-threaded application code and determines a <b>file</b> <b>layout</b> for each disk-resident array referenced by the code, such that the performance of the target storage cache hierarchy is maximized. We tested our approach using 16 I/O intensive application programs and compared its performance against two previously proposed approaches under different cache space management schemes. Our experimental results show that the proposed approach improves the execution time of these parallel applications by 23. 7 % on average...|$|E
40|$|Striping is a {{technique}} that distributes file content over multiple storage servers and thereby enables parallel ac-cess. In {{order to be able}} to provide a consistent view across file data and metadata operations, the file system has to track the <b>layout</b> of the <b>file</b> and know where the file ends and where it contains gaps. In this paper, we present a light-weight protocol for maintaining a consis-tent notion of a <b>file’s</b> <b>layout</b> that provides POSIX seman-tics without restricting concurrent access to the file. In an evaluation, we show that the protocol scales and elicit its corner cases. ...|$|R
50|$|By contrast, if {{a camera}} is mounted via USB MSC, the {{physical}} <b>file</b> system and <b>layout</b> {{are exposed to}} the user.|$|R
40|$|With the {{increasing}} number of scientific applications manipulating huge amounts of data, effective data management is an increasingly important problem. Unfortunately, so far the solutions to this data management problem either require deep understanding of specific storage architectures and <b>file</b> <b>layouts</b> (as in high-performance file systems) or produce unsatisfactory I/O performance in exchange for ease-of-use and portability (as in relational DBMSs). In this paper we present a new environment which is built around an active meta-data management system (MDMS). The key components of our three-tiered architecture are user application, the MDMS, and a hierarchical storage system (HSS). Our environment overcomes the performance problems of pure database-oriented solutions, while maintaining their advantages in terms of ease-of-use and portability. The high levels of performance are achieved by the MDMS, with the aid of user-specified directives. Our environment supports a simple, easy-to- [...] ...|$|R
