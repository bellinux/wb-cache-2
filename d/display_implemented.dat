9|172|Public
40|$|This paper {{reports on}} work in {{progress}} to develop a computational auditory perspective taking system for a robot. Auditory perspective taking is construed {{as the ability to}} reason about inferred or posited factors that affect an addressee's perspective as a listener for the purpose of presenting auditory information in an appropriate and effective manner. High-level aspects of this aural interaction skill are discussed, and a prototype adaptive auditory <b>display,</b> <b>implemented</b> {{in the context of a}} robotic information kiosk, is described and critiqued. Additionally, a sketch of the design and goals of a user study planned for later this year is given. A demonstration of the prototype system will accompany the presentation of this research in the poster session. 1...|$|E
40|$|Working on the ACLO (Autonomous Cryogenics Loading Operations) project I {{have had}} the {{opportunity}} to add functionality to the physics simulation software known as KATE (Knowledgebase Autonomous Test Engineer), create a new application allowing WYSIWYG (what-you-see-is-what-you-get) creation of KATE schematic files and begin a preliminary design and implementation of a new subsystem that will provide vision services on the IHM (Integrated Health Management) bus. The functionality I added to KATE over the past few months includes a dynamic visual representation of the fluid height in a pipe based on number of gallons of fluid in the pipe and implementing the IHM bus connection within KATE. I also fixed a broken feature in the system called the Browser <b>Display,</b> <b>implemented</b> many bug fixes and made changes to the GUI (Graphical User Interface) ...|$|E
40|$|International audienceMany {{pictures}} circulating online inform industrial legacies by mapping, aestheticizing, or documenting contemporary ruin landscapes, such as brownfields {{and other}} remnants of industry. This phenomenon seems to mark {{a turning point}} in the history of heritage, for marginal interests spread and tend to become popular hobbies. Nevertheless, the practices of <b>display</b> <b>implemented</b> by Internet users show various perspectives connected with offline experiences. They may reveal conflicts of perception and narrative but also alternative ways of seeing how to preserve precarious places without following the path of museumification. From an inductive approach, this article explores how sharing digital photographs leads to recognition and increases the value of (post) industrial inheritance, engendering innovative tactics of preservation through networking. The method used is based on a “multimodal ethnography,” taking into account the diversity and the intertwining of modes expressed through a range of different media...|$|E
5000|$|Modern day LCD <b>displays</b> <b>implement</b> FSC {{by using}} several colors of LED backlight, by cycling the backlights, and gain several {{advantages}} such as brighter colors, darker blacks, and lower cost. [...] These displays {{are used in}} LCD Camera viewfinders and other industrial applications.|$|R
50|$|Very few <b>display</b> devices <b>implemented</b> this protocol.|$|R
50|$|Other {{artifacts}} showcased by the Lamar County Historical Museum include early 20th century furniture, antique washing machines, a loom, {{and various}} tools. The museum also <b>displays</b> <b>implements</b> used for cotton and hay farming, relics from the Buckner Orphanage, which was demolished in 2000, the Judge Jim Noble Thompson Portico facade, an iron lung, and a gallery documenting Lamar County's smaller communities.|$|R
40|$|Presented at the 12 th International Conference on Auditory Display (ICAD), London, UK, June 20 - 23, 2006. This paper {{reports on}} work in {{progress}} to develop a computational auditory perspective taking system for a robot. Auditory perspective taking is construed {{as the ability to}} reason about inferred or posited factors that affect an addressee's perspective as a listener for the purpose of presenting auditory information in an appropriate and effective manner. High-level aspects of this aural interaction skill are discussed, and a prototype adaptive auditory <b>display,</b> <b>implemented</b> {{in the context of a}} robotic information kiosk, is described and critiqued. Additionally, a sketch of the design and goals of a user study planned for later this year is given. A demonstration of the prototype system will accompany the presentation of this research in the poster session...|$|E
40|$|Recently, {{there has}} been a growing trend towards the use of threedimensional {{displays}} and virtual reality (VR) {{in a wide variety of}} applications. Without an understanding of the human factors involved, there is a danger that such displays will be misapplied, leading to worse performance than the equivalent two-dimensional display. This research examines 3 D displays and VR for the particularly demanding application of dynamic command/control tasks in 3 D space, using air traffic control (ATC) as a basis. It is proposed to compare four display types: (i) two-dimensional synthetic radar plan position indicator (the reference), (ii) three-dimensional "through-the-window" non-stereoscopic <b>display,</b> <b>implemented</b> using a conventional workstation and video monitor, (iii) 3 D TTW stereoscopic on a workstation using LCD shutter glasses to implement stereopsis and (iv) immersive (VR). This document forms an interim report detailing progress on the author 's Ph. D. programme to date. Prototypical imple [...] ...|$|E
40|$|This article {{presents}} flight tests {{of a newly}} developed Helmet-Mounted Flight Parameters Display System (SWPL). Presented system is designed to illustrate full piloting and navigational information, warning about emergency on the helicopter’s board and signaling on-board helicopter systems failures on the translucent display. The system is designed to work in day and night time conditions. In night time conditions, the system cooperates with night vision goggles applicable in Polish Air Force. The {{article presents}} {{the main components of}} the tested system, along with their purpose and function. It also describes in detail the methods of <b>display</b> <b>implemented</b> in the system and the amount of displayed information. The article discusses the required range and the actual course of the flight tests of that system. Tests were conducted on a Mi- 17 - 1 V helicopter. The results of the flight tests of the Helmet-Mounted Display System are given in conclusion, in particular regarding meeting the tactical and technical requirements of the system...|$|E
50|$|The Museum of Rural Life {{is located}} in an 18th Century granary building, with <b>displays</b> of <b>implements</b> and tools.|$|R
5|$|HDMI 1.4a was {{released}} on March 4, 2010, and adds two additional mandatory 3D formats for broadcast content, which was deferred with HDMI 1.4 {{in order to see}} the direction of the 3D broadcast market. HDMI 1.4a has defined mandatory 3D formats for broadcast, game, and movie content. HDMI 1.4a requires that 3D <b>displays</b> <b>implement</b> the frame packing 3D format at either 720p50 and 1080p24 or 720p60 and 1080p24, side-by-side horizontal at either 1080i50 or 1080i60, and top-and-bottom at either 720p50 and 1080p24 or 720p60 and 1080p24.|$|R
5000|$|... 2012 - The first {{holographic}} <b>display</b> is <b>implemented</b> in a car's interactive {{navigation display}} system. The technology was showcased through the exclusive luxury car, the Lykan HyperSport.|$|R
40|$|This paper {{presents}} the preliminary performance {{results of the}} artificial intelligence monitoring system in full operational mode using near real time acceleration data downlinked from the International Space Station. Preliminary microgravity environment characterization analysis result for the International Space Station (Increment- 2), using the monitoring system is presented. Also, comparison between the system predicted performance based on ground test data for the US laboratory "Destiny" module and actual on-orbit performance, using measured acceleration data from the U. S. laboratory module of the International Space Station is presented. Finally, preliminary on-orbit disturbance magnitude levels are presented for the Experiment of Physics of Colloids in Space, which are compared with on ground test data. The ground test data for the Experiment of Physics of Colloids in Space were acquired from the Microgravity Emission Laboratory, located at the NASA Glenn Research Center, Cleveland, Ohio. The artificial intelligence {{was developed by the}} NASA Glenn Principal Investigator Microgravity Services Project to help the principal investigator teams identify the primary vibratory disturbance sources that are active, at any moment of time, on-board the International Space Station, which might impact the microgravity environment their experiments are exposed to. From the Principal Investigator Microgravity Services' web site, the principal investigator teams can monitor via a dynamic graphical <b>display,</b> <b>implemented</b> in Java, in near real time, which event(s) is/are on, such as crew activities, pumps, fans, centrifuges, compressor, crew exercise, structural modes, etc., and {{decide whether or not to}} run their experiments, whenever that is an option, based on the acceleration magnitude and frequency sensitivity associated with that experiment. This monitoring system detects primarily the vibratory disturbance sources. The system has built-in capability to detect both known and unknown vibratory disturbance sources. Several soft computing techniques such as Kohonen's Self-Organizing Feature Map, Learning Vector Quantization, Back-Propagation Neural Networks, and Fuzzy Logic were used to design the system...|$|E
40|$|This thesis {{develops}} a novel method of decomposing a 3 D phase space description of light into multiple partially coherent modes, and applies this decomposition {{to the creation}} of a more flexible 3 D display format. Any type of light, whether it is completely coherent, partially coherent or incoherent, can be modeled either as a sum of coherent waves or as rays. A set of functions, known as phase space functions, provide an intuitive model for these waves or rays as they pass through a 3 D volume to a display viewer's eyes. First, this thesis uses phase space functions to mathematically demonstrate the limitations of two popular 3 D display setups: parallax barriers and coherent holograms. Second, this thesis {{develops a}} 3 D image design algorithm based in phase space. The "mode-selection" algorithm can find an optimal holographic display setup to create any desired 3 D image. It is based on an iterative algebraic-rank restriction process, and can be extended to model light with an arbitrary degree of partial coherence. Third, insights gained from partially coherent phase space representations lead to the suggestion of a new form of 3 D <b>display,</b> <b>implemented</b> with multiple time-sequential diffracting screens. The mode-selection algorithm determines an optimal set of diffracting screens to display within the flicker-fusion rate of a viewer's eye. It is demonstrated both through simulation and experiment that this time-sequential display offers improved performance over a fixed holographic display, creating 3 D images with increased intensity variation along depth. Finally, this thesis investigates the tradeoffs involved with multiplexing a holographic display over time with well-known strategies of multiplexing over space, illumination angle and wavelength. The examination of multiplexing tradeoffs is extended into the incoherent realm, where comparisons to ray-based 3 D displays can hopefully offer a more unified summary of the limitations of controlling light within a volume. by Roarke Horstmeyer. Thesis (S. M.) [...] Massachusetts Institute of Technology, School of Architecture and Planning, Program in Media Arts and Sciences, 2011. Cataloged from PDF version of thesis. Includes bibliographical references (p. 111 - 120) ...|$|E
40|$|This {{dissertation}} {{describes the}} development of an assist-device aimed to deliver 3 D graphic information to the visually impaired people. A human-in-loop approach was used to analyze whether a virtual 3 D shape can be transferred correctly to the human users. The proposed device in this dissertation consists of two major parts: (a) A system of position sensors for real time localization based on magnetization, and (b) A single vibratory actuator working at varied frequencies based on its real time location. The error bound of the position measurement was tested to be 2 mm, which defined the machine resolution of the shape display. In order to realize the refresh rate of the localization that can follow user's scanning speed, the parallel data processing sequences for computer and microcontroller were designed. Additionally, vibratory electromagnetic (EM) actuators were discussed based on eddy current and permanent magnet methods. The simulation study showed that eddy current method was not applicable for millimeter size coil. Accordingly, the permanent magnet method was developed and the force detection threshold of human tactile perceptions was studied. Virtual shape perception experiments were made with participation of 3 volunteers who were not aware of the 3 D shape information prior to the tests. Based on the four sets of shape tests, we conclude {{that the majority of the}} shape information is able to be delivered to users by using the proposed device. Difficulties for perceiving the local sharp profile e. g. thin plates and large curvature in small shapes may be better addressed by multiple actuators simultaneously providing shape information in the local boundary detection. The major contribution of this dissertation is the 3 D shape <b>display</b> <b>implemented</b> by a miniature and low cost device. The developed device utilizes both passive stimulation and active search so that a commonly used large scale actuators matrix based on mere active touch method is avoided. The studies on the required force/energy input from the actuator showed that EM actuators can be miniaturized to millimeter scale without sacrificing the ability to induce tactile stimulation. Additional uniqueness of the proposed system is the ability to present hollow features, which is impossible to display by the existing devices...|$|E
5000|$|... #Caption: The basic {{components}} of a GUI: The <b>display</b> server <b>implements</b> the windowing system. A simple window manager merely draws the window decorations, but compositing window managers do more.|$|R
50|$|Like other web map libraries, {{the basic}} <b>display</b> model <b>implemented</b> by Leaflet is one basemap, plus zero or more {{translucent}} overlays, with zero or more vector objects displayed on top.|$|R
50|$|Notable {{examples}} of <b>display</b> servers <b>implementing</b> the X11 <b>display</b> server protocol are X.Org Server, XFree86, XQuartz and Cygwin/X, while client libraries <b>implementing</b> the X11 <b>display</b> server protocol are Xlib and XCB.|$|R
40|$|A common {{observation}} {{when working}} on physically large displays, such as wall-sized projection, {{is that a}} certain amount of information privacy is lost. A common explanation for this loss in privacy is the higher legibility of information presented on large displays. In this paper, we present a novel paradigm for measuring whether or not a user has read certain content. We show that, even with constant visual angles and legibility, visitors are still more likely to glance over a user's shoulder to read information on a large wall-projected display than on a smaller traditional desktop monitor. We assert that, in addition to legibility, there are more subtle social factors that may contribute to the loss of privacy on physically large <b>displays.</b> <b>Implementing</b> hardware and software ideas for mitigating this loss of privacy remains future research...|$|R
5000|$|More complex was {{a system}} {{built into the}} CH <b>displays,</b> <b>implemented</b> in order to remove {{spurious}} signals from unsynchronized jamming pulses. It consisted of two layers of phosphor in the CRT screen, a quick-reacting layer of zinc sulphide below, and a slower [...] "afterglow" [...] layer of zinc cadmium sulphide on top. During normal operation the bright blue signal from the zinc sulphide was visible, and its signal would activate the yellow zinc cadmium sulphide layer, causing an [...] "averaged" [...] signal to be displayed in yellow. To filter out jamming pulses, a yellow plastic sheet was {{placed in front of}} the display, rendering the blue display invisible and revealing the dimmer yellow averaged signal. This is the reason many radars from the War through to the 1960s have yellow displays.|$|R
50|$|<b>Display</b> servers that <b>implement</b> the Wayland <b>display</b> server {{protocol}} {{are also}} called Wayland compositors because they additionally perform {{the task of}} a compositing window manager.|$|R
5000|$|The Hildesheim Reliquary of Mary, the Crosses of Bernward {{and further}} {{magnificent}} reliquaries and liturgical <b>implements</b> <b>displayed</b> in the Hildesheim Cathedral Museum (...) {{in the south}} transept ...|$|R
5000|$|... eXodus is a <b>display</b> server <b>implementing</b> the X11 <b>display</b> server {{protocol}} {{that was}} originally developed by White Pine Software. White Pine founders began {{development in the}} fall of 1987, and released Version 1.0 in April 1989. Version 1.0 was only built for the Macintosh platform, but versions for Windows, DOS, and NeXT would follow in the coming years.|$|R
40|$|The STAR system {{developed}} by NASA enables any user with a logic diagram {{to design a}} semicustom digital MOS integrated circuit. The system is comprised of a library of standard logic cells and computer programs to place, route, and <b>display</b> designs <b>implemented</b> with cells from the library. Library cells of the CMOS metal gate and CMOS silicon gate technologies were simulated using SPICE, {{and the results are}} shown and compared...|$|R
40|$|This note {{wants to}} explain how to obtain {{meaningful}} pictures of (possibly high-dimensional) convex polytopes, triangulated manifolds, and other objects {{from the realm of}} geometric combinatorics such as tight spans of finite metric spaces and tropical polytopes. In all our cases we arrive at specific, geometrically motivated, graph drawing problems. The methods <b>displayed</b> are <b>implemented</b> in the software system polymake. Comment: 18 pages, 17 examples, 13 figures, 0 theorem...|$|R
40|$|We {{describe}} a virtual sound display {{built around a}} 12 MHz 80286 microcomputer and special purpose analog hardware. The <b>display</b> <b>implements</b> most of the primary cues for sound localization in the ear-level plane. Static information about direction is conveyed by interaural time differences and, for frequencies above 1800 Hz, by head sound shadow (interaural intensity differences) and pinna sound shadow. Static information about distance is conveyed by variation in sound pressure (first power law) for all frequencies, by additional attenuation in the higher frequencies (simulating atmospheric absorption), and by the proportion of direct to reverberant sound. When the user actively locomotes, the changing angular position of the source occasioned by head rotations provides further information about direction and the changing angular velocity produced by head translations (motion parallax) provides further information about distance. Judging both from informal observations by users and from objective data obtained in an experiment on homing to virtual and real sounds, we conclude that simple displays such as this are effective in creating the perception of external sounds to which subjects can home with accuracy and ease...|$|R
30|$|The {{testing of}} {{equipment}} should address the various critical {{stages of the}} imaging chain (acquisition, processing and <b>display)</b> and be <b>implemented</b> in a multidisciplinary team approach by trained staff (radiographer, medical physicist, radiologist) [3, 5].|$|R
50|$|The GNOME Display Manager (GDM) is a <b>display</b> manager that <b>implements</b> all {{significant}} features {{required for}} managing attached and remote displays. GDM was written from scratch {{and does not}} contain any XDM or X Consortium code.|$|R
40|$|This study {{evaluated}} {{the performance of}} four prototype unmanned aircraft detect-and-avoid (DAA) display configurations, each with different informational elements driven by alerting and guidance algorithms. Sixteen unmanned aircraft pilots flew each combination of the display configurations, with half being given zero DAA surveillance sensor uncertainty {{and the other half}} experiencing errors that were comparable, and in some cases slightly better than, errors that were measured in DAA system flight tests. The displays that showed intruder alert information in altitude and heading bands had significantly fewer losses of well clear compared with alternative displays that lacked that information. This difference was significant from a statistical and practical perspective: those losses that did occur lasted for shorter periods and did not penetrate as far into the geometric "separation cylinder" as those in the non-banded displays. A modest level of DAA surveillance sensor uncertainty did not affect the proportion of losses of well clear or their severity. It is recommended that DAA traffic <b>displays</b> <b>implement</b> a band-type <b>display</b> in order to improve the safety of UAS operations in the National Airspace System. Finally, this report provides pilot response time distributions for responding to DAA alerts...|$|R
40|$|Flight {{test results}} have been {{obtained}} which demonstrate the feasibility and desirability of using knowledge-based systems architectures for flight test investigations of primary flight display information management-related issues. LISP-based software was used for real-time operation of the primary flight display. The two integrated knowledge-based systems designed to control the primary flight <b>displays</b> were <b>implemented</b> aboard a NASA-Langley B- 737. The programmer is noted {{to be capable of}} more easily developing initial systems via the present method than with more conventional techniques...|$|R
40|$|A visual {{processing}} system for bionic eye {{with a focus}} on 12 objects: different shape, texture, color, and physical properties. obstacle avoidance. Placed on the textured carpets. Extracting salient information in an unknown environment for assisting patients in daily tasks. 10 videos, approx 600 images per video. Runtime: 0. 2 s / frame Designed a fully portable system (a camera, a laptop, and a head mounted <b>display</b> (HMD)). <b>Implemented</b> a state-of-the-art saliency detection algorithm. Efficient image processing, effective obstacle identification, and eventually providing useful information for users...|$|R
5|$|HDMI 1.4 was {{released}} on May 28, 2009, and the first HDMI 1.4 products were available {{in the second half}} of 2009. HDMI 1.4 added support for 4K×2K, i.e. 4096×2160 at 24Hz (which is a resolution used with digital theaters), 3840×2160 (Ultra HD) at 24Hz/25Hz/30Hz, and support for 1080p at 120Hz. It also added an HDMI Ethernet Channel (HEC) which allows for a 100Mbit/s Ethernet connection between the two HDMI connected devices so they can share an Internet connection, introduced an audio return channel (ARC), 3D Over HDMI, a new Micro HDMI Connector, an expanded set of color spaces with the addition of sYCC601, Adobe RGB and Adobe YCC601, and an Automotive Connection System. HDMI 1.4 defined several stereoscopic 3D formats including field alternative (interlaced), frame packing (a full resolution top-bottom format), line alternative full, side-by-side half, side-by-side full, 2D + depth, and 2D + depth + graphics + graphics depth (WOWvx). HDMI 1.4 requires that 3D <b>displays</b> <b>implement</b> the frame packing 3D format at either 720p50 and 1080p24 or 720p60 and 1080p24. High Speed HDMI cables as defined in HDMI 1.3 work with all HDMI 1.4 features except for the HDMI Ethernet Channel, which requires the new High Speed HDMI Cable with Ethernet defined in HDMI 1.4.|$|R
50|$|<b>Display</b> servers that <b>implement</b> the Wayland <b>display</b> server protocol, {{are called}} Wayland {{compositor}}s. Like any display server, a Wayland compositor {{is responsible for}} handling input and output for its clients and - in contrast to X11 - additionally for the compositing. Examples are Weston, Mutter, KWin or Enlightenment.|$|R
40|$|Abstract. The key {{techniques}} of intelligent analysis instrument developed are proposed. Based on {{the technique of}} virtual instrumentation, the intelligent PID control algorithm to control the temperature of thermal analysis instrument is described. The dynamic character and the robust performance of traditional PID controls are improved through the dynamic gain factor, temperature rate change factor, the forecast factor, and the temperature correction factor is introduced. Using the graphic development environment of LabVIEW, the design of system modularization and the graphic <b>display</b> are <b>implemented.</b> By means of multiple mathematical modules, intelligent data processing is realized. 1...|$|R
50|$|The <b>display</b> {{is usually}} <b>implemented</b> with a liquid crystal display, {{and it may}} show one or more values at once. Many current models display one value, such as current speed, with large numbers, and another number that the user may select, such as time, distance, average speed, etc., with small numbers.|$|R
50|$|HDMI 1.4 was {{released}} on May 28, 2009, and the first HDMI 1.4 products were available {{in the second half}} of 2009. HDMI 1.4 added support for 4K × 2K, i.e. 4096×2160 at 24 Hz (which is a resolution used with digital theaters), 3840×2160 (Ultra HD) at 24 Hz/25 Hz/30 Hz, and support for 1080p at 120 Hz. It also added an HDMI Ethernet Channel (HEC) which allows for a 100 Mbit/s Ethernet connection between the two HDMI connected devices so they can share an Internet connection, introduced an audio return channel (ARC), 3D Over HDMI, a new Micro HDMI Connector, an expanded set of color spaces with the addition of sYCC601, Adobe RGB and Adobe YCC601, and an Automotive Connection System. HDMI 1.4 defined several stereoscopic 3D formats including field alternative (interlaced), frame packing (a full resolution top-bottom format), line alternative full, side-by-side half, side-by-side full, 2D + depth, and 2D + depth + graphics + graphics depth (WOWvx). HDMI 1.4 requires that 3D <b>displays</b> <b>implement</b> the frame packing 3D format at either 720p50 and 1080p24 or 720p60 and 1080p24. High Speed HDMI cables as defined in HDMI 1.3 work with all HDMI 1.4 features except for the HDMI Ethernet Channel, which requires the new High Speed HDMI Cable with Ethernet defined in HDMI 1.4.|$|R
