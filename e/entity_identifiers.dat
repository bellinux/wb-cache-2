25|52|Public
40|$|Running a {{business}} involves much administration {{in terms of}} setting the business up, and successfully running it. A number of <b>entity</b> <b>identifiers</b> and common rules apply for administration purposes to structures used for business. These <b>entity</b> <b>identifiers</b> play a fundamental role as government authorities, including the Australian Taxation Office (ATO), conduct data matching for various compliance purposes...|$|E
40|$|Abstract. The {{generation}} of <b>entity</b> <b>identifiers</b> {{is a key}} issue {{in the context of}} semantic web technologies. <b>Entity</b> <b>identifiers</b> are needed when we perform any kind of operation (e. g., reference, match, disambiguation) on entities. In this paper, we present a methodology for generating <b>entity</b> <b>identifiers</b> which can be well integrated in the Entity Name System of the OKKAM infrastructure: a system for managing <b>entity</b> <b>identifiers.</b> Our approach is based on a known labelling scheme for directed acyclic graphs. Unlike other related approaches, it allows {{to keep track of the}} lineage of an entity over the operations of creation, merge of entities, and split of a single entity. This feature is important in a identifier generation framework because it can allow both local deprecation detection between two entities, as well as the reconstruction of the origin of an entity partially described in different sources (i. e., knowing which entity attributes are coming from which sources). ...|$|E
40|$|Entities {{have been}} {{deserved}} special {{attention in the}} latest years, however their identification is still troublesome. Existing approaches exploit ad hoc services or centralized architectures. In this paper we present a novel approach to recognize naturally emerging <b>entity</b> <b>identifiers</b> built on top of Linked Data concepts and protocols...|$|E
50|$|In {{computer}} science, identifiers (IDs) are lexical tokens {{that name}} <b>entities.</b> <b>Identifiers</b> are used extensively {{in virtually all}} information processing systems. Identifying entities {{makes it possible to}} refer to them, which is essential for any kind of symbolic processing.|$|R
50|$|A key {{political}} {{achievement was}} also successfully securing a legislative requirement for a Legal <b>Entity</b> <b>Identifier</b> for Financial Transactions {{which is now}} being processed in a facility in North Wales, thereby linking her work on financial services back to her constituency.|$|R
50|$|A Legal <b>Entity</b> <b>Identifier</b> (or LEI), is a 20-character {{identifier}} {{that identifies}} distinct legal entities that engage in financial transactions. It {{is defined by}} ISO 17442. The LEI is a global standard, designed to be non-proprietary data that is freely accessible to all. Over 435,000 legal entities from more than 195 countries have now been issued with LEIs.|$|R
40|$|Recognizing that {{information}} from different sources {{refers to the}} same (real world) entity is a crucial challenge in instance level information integration, {{as it is a}} pre-requisite for combining the information about one entity from different sources. The required entity matching is time consuming and thus imposes a crucial limit for largescale, dynamic information integration. An increased re-use of <b>entity</b> <b>identifiers</b> (or names) across different information collections such as RDF repositories, databases and document collections, eases this situation. In the ideal case, entity matching can be reduced to the trivial problem of spotting the same entity identifier in different information collections. In this paper we propose the use of a Entity Name System (ENS) – as it is currently under development in the EUfunded project OKKAM – for systematically supporting the re-use of <b>entity</b> <b>identifiers.</b> The main purpose of the ENS is to provide unique and uniform names for entities for the use in information collections, so that the same name is used for an entity, even when it is referenced in different contexts. Of course the creation of an ENS that can efficiently deal with entities on the Web scale raises scalability issues of its own. However, this paper focuses on the role of an ENS in contributing to the scalability of ad-hoc and on demand information integration tasks. ...|$|E
40|$|The {{analysis}} of causal relations among {{events in a}} distributed computation plays {{a central role in}} distributed systems modeling. Existing models for causal time-stamping are based on a known set of entities, either processes or data repositories, where events can occur. The advent of mobile computing settings that stimulate cooperation among arbitrary groups of nodes, possibly in isolation, precludes the use of a pre-established set of <b>entity</b> <b>identifiers.</b> Addressing this problem, the article proposes a causality definition and a time-stamping model that allows the {{analysis of}} those environments, while retaining compatibility with the classic causality model...|$|E
40|$|Financial {{entities}} {{are often}} referred to with ambiguous descriptions and identifiers. To tackle this issue, the Financial Entity Identification and Information Integration (FEIII) Challenge requires participants to automatically reconcile financial entities among three datasets: the Federal Financial Institution Examination Council (FFIEC), the Legal <b>Entity</b> <b>Identifiers</b> (LEI) and the Security and Exchange Commission (SEC). Our approach {{is based on the}} combination of different Naive Bayes classifiers through an ensemble approach. The evaluation on the Gold Standard developed by the challenge organizers shows F 1 -scores that are above the average of the other participants for the two proposed tasks...|$|E
50|$|A {{surrogate}} key (or synthetic key, <b>entity</b> <b>identifier,</b> system-generated key, database sequence number, factless key, technical key, or arbitrary unique identifier) in {{a database}} {{is a unique}} identifier for either an entity in the modeled world or an object in the database. The surrogate key is not derived from application data, unlike a natural (or business) key which is derived from application data.|$|R
40|$|A {{significant}} number of neural architectures for reading comprehension have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of "predication structure" in the hidden state vectors of these readers. More specifically, we provide evidence that the hidden state vectors represent atomic formulas Φ[c] where Φ is a semantic property (predicate) and c is a constant symbol <b>entity</b> <b>identifier.</b> Comment: Accepted for Repl 4 NLP: 2 nd Workshop on Representation Learning for NL...|$|R
25|$|The ISE is {{endorsed}} by the Regulatory Oversight Committee (ROC) and sponsored by the Central Bank of Ireland as a Local Operating Unit (LOU) for the processing of Legal <b>Entity</b> <b>Identifier</b> (LEI) services for Ireland. LEIs are codes designed to create a global reference data system that uniquely identifies every legal entity or structure, in any jurisdiction, that is party to a financial transaction. Businesses may apply for an LEI code through ISEdirect. LEIs were brought in by global regulators {{as part of the}} response to the global financial crisis.|$|R
40|$|We {{examine the}} {{financial}} stability implications of covered bonds. Banks issue covered bonds by encumbering assets on their balance sheet and placing them within a dynamic ring fence. As more assets are encumbered, jittery unsecured creditors may run, {{leading to a}} banking crisis. We provide conditions for such a crisis to occur. We examine how different over-the-counter market network structures influence the liquidity of secured funding markets and crisis dynamics. We draw on the framework to consider several policy measures aimed at mitigating systemic risk, including caps on asset encumbrance, global legal <b>entity</b> <b>identifiers,</b> and swaps of good for bad collateral by central banks...|$|E
40|$|This workshop, co-organized by BioMedBridges WP 3 and 12, {{was held}} on 24 - 25 June and hosted by BioMedBridges {{partners}} at VUMC in Amsterdam. Attendees included BioMedBridges personnel, members from the ESFRI BMS research infrastructures and invited external experts from existing standards organisations. The following aspects of data standardisation were explored: 	Defining <b>entity</b> <b>identifiers</b> and identifiers best practice 	Development of a Meta models and Mappings Registry for biomedical standards Data integration occurs when a query proceeds through multiple data sets, thereby relating diverse data extracted from different data sources. Standards and data harmonisation is a prerequisite to data integration, a primary aim of the BioMedBridges project. For the various ESFRI projects to exchange and integrate data {{in a meaningful way}} there needs to be agreement on how biomedical data is annotated, formatted and organised. Whilst an ultimate aim would be a single database, with a standardised user interface (UI), this is unlikely and a system of federated databases each specialising in a particular data type is more achievable. Provided that standardisation is implemented across the databases, a common UI could be deployed to allow integrated and simultaneous searching of data. The use of <b>entity</b> <b>identifiers</b> and common standards is essential to achieve the above vision, but many challenges exist. The complex and varied nature of the biomedical sciences, and biology in general, means that each researcher can have their own requirements and reasons for structuring their data in a particular format. To capture the possible extent of these variables, standards would require a high degree of granularity. Therefore organisation is required within the community and compromise needs to be sought...|$|E
40|$|Propensity score {{matching}} (PSM) is {{a widely}} used method for performing causal inference with observational data. PSM requires fully specifying the set of confounding variables of treatment and outcome. In the case of relational data, this set may include non-intuitive relational variables, i. e., variables derived from the relational structure of the data. In this work, we provide an automated method to derive these relational variables based on the relational structure {{and a set of}} naive confounders. This automatic construc-tion includes two unusual classes of variables: relational degree and <b>entity</b> <b>identifiers.</b> We provide experimental evidence that demon-strates the utility of these variables in ac-counting for certain latent confounders. Fi-nally, through a set of synthetic experiments, we show that our method improves the per-formance of PSM for causal inference with relational data. ...|$|E
50|$|NSD {{acts as the}} National Numbering Agency (NNA) for Russia and the Substitute Numbering Agency (SNA) for the CIS {{countries}} and as such assigns ISINs and CFIs to securities or other financial instruments issued or registered in Russia or other CIS countries. In addition, NSD is a pre-Local Operating Unit (pre-LOU). Pre-LEI codes assigned by NSD are published {{on the web site}} of the Global Legal <b>Entity</b> <b>Identifier</b> Foundation authorized by the Regulatory Oversight Committee (ROC) to maintain the global LEI database, and are globally accepted by regulators, market participants, and pre-LOUs of other countries.|$|R
50|$|Marcelle {{has been}} {{an active member of}} the Private Sector Preparatory Group (LEI-PSPG) for the Legal <b>Entity</b> <b>Identifier</b> set up and run by the Financial Stability Board from 2011 onwards. Marcelle {{has been an}} elected and served as member of the Governing Council of the Royal Institute of Navigation (RIN) for two {{consecutive}} terms first from August 2010 to July 2012 and then from August 2012 to July 2015. She has also served on the RIN's Finance committee from 2007 to 2013 and the RIN Audit & Risk committee since 2013.|$|R
50|$|The ISE is {{endorsed}} by the Regulatory Oversight Committee (ROC) and sponsored by the Central Bank of Ireland as a Local Operating Unit (LOU) for the processing of Legal <b>Entity</b> <b>Identifier</b> (LEI) services for Ireland. LEIs are codes designed to create a global reference data system that uniquely identifies every legal entity or structure, in any jurisdiction, that is party to a financial transaction. Businesses may apply for an LEI code through ISEdirect. LEIs were brought in by global regulators {{as part of the}} response to the global financial crisis.|$|R
40|$|Abstract—Real-word {{entities}} can be {{mapped to}} unique <b>entity</b> <b>identifiers</b> through an Entity Name System (ENS), to systematically support the re-use of these identifiers and disambiguate references to real world entities in the Web. An entity subscription service informs subscribed users {{of changes in}} the descriptive data of an entity, which is a set of attribute name-value pairs. We study the design, implementation and application of an adaptable push-policy subscription service, within a large-scale ENS. The subscription system aims to deliver ranked descriptions of the changes on entities, following user preferences through a feedback-driven adaptation process. The adaptation is based on both the content and the type of each entity change. We evaluate the learning curve of the system and the utility of the content-type discrimination. The experiments demonstrate good results, especially in the system’s content-aware adaptation aspect. Keywords-Adaptive systems; Artificial intelligence; User modelin...|$|E
40|$|This {{document}} {{contains the}} UDF Document Change Notices (DCNs) that were approved as UDF 2. 50 errata by the OSTA UDF Committee. Important notice: UDF 2. 50 rules identical to UDF 2. 60 for non-POW. On {{issue of the}} UDF 2. 60 specification, some minor modifications {{were made to the}} UDF 2. 50 errata, see DCN- 5122. The effect of these modifications is that for non-POW media, the rules are identical for UDF 2. 50 plus 2. 50 errata and UDF 2. 60 plus 2. 60 errata. POW stands for Pseudo OverWrite. A Major benefit from this is that an implementer can use the UDF 2. 60 documents to implement both UDF 2. 50 and 2. 60, instead of going through the UDF 2. 50 spec and all 2. 50 errata DCNs for implementing UDF 2. 50. The only thing that a UDF 2. 50 implementer needs to know when implementing UDF 2. 50 from the UDF 2. 60 documents is that Pseudo OverWrite and pseudo-overwritable partitions are not allowed for UDF 2. 50 and that the UDF Revision field in Domain <b>Entity</b> <b>Identifiers</b> and UDF <b>Entity</b> <b>Identifiers</b> must have a value # 0250 instead of # 0260 (see 2. 1. 5. 3). Note: This assumes that non-POW DCNs that are approved as UDF 2. 60 errata are also approved as UDF 2. 50 errata and that POW and non-POW issues are dealt with in separate DCNs. UDF 2. 50 approved errata 1 July 21, 2006 UDF 2. 50 approved errata History of this document: 30 - 04 - 2003 : Release of the approved UDF revision 2. 50 document. 22 - 01 - 2004 : Added DCN- 5101 till DCN- 5109 incl. as approved on December 8, 2003. 25 - 06 - 2004 : Replaced DCN- 5101 by modified version as approved on June 14, 2004. 26 - 08 - 2004 : Editorial addition in description of DCN- 5107 about section numbering. 20 - 09 - 2004 : Replaced DCN- 5104 and DCN- 5108 by modified versions as approved o...|$|E
40|$|In {{this paper}} we study the {{prevalence}} of unique <b>entity</b> <b>identifiers</b> on the Web. These are, e. g., ISBNs (for books), GTINs (for com-mercial products), DOIs (for documents), email addresses, and oth-ers. We show how these identifiers can be harvested systematically from Web pages, {{and how they can}} be associated with human-readable names for the entities at large scale. Starting with a simple extraction of identifiers and names from Web pages, we show how we can use the properties of unique iden-tifiers to filter out noise and clean up the extraction result on the entire corpus. The end result is a database of millions of uniquely identified entities of different types, with an accuracy of 73 – 96 % and a very high coverage compared to existing knowledge bases. We use this database to compute novel statistics on the presence of products, people, and other entities on the Web. 1...|$|E
50|$|In {{computer}} languages, identifiers are tokens (also called symbols) which name language entities. Some of {{the kinds}} of <b>entities</b> an <b>identifier</b> might denote include variables, types, labels, subroutines, and packages.|$|R
50|$|EMIR {{requires}} that all entities entering into derivative contracts must submit reports to their corresponding trade repositories, outlining each over-the-counter trade. These mandatory reports must {{also include a}} Unique Transaction <b>Identifier</b> (UTI), legal <b>entity</b> <b>identifier</b> (LEI), information on the trading capacity of the counterparty, and the marked-to-market valuation of the position. The counterparty data in a report includes 26 fields for data and the common data includes 59 fields of data. These fields include an LEI, or a unique 20 digit alphanumeric code {{that may be used}} for eight of the 26 counterparty data fields, and the unique trade identifier, which are generated based on the report's LEI.|$|R
40|$|The {{passage of}} the Dodd-Frank Wall Street Reform and Consumer Protection Act sparked {{discussion}} of creating a systematic code that uniquely identifies an entity. This code is {{commonly referred to as}} a legal <b>entity</b> <b>identifier</b> (LEI). The information that is collected to accompany and describe the LEI will {{play an important role in}} enhancing the usefulness of the LEI. This paper explores the information (referred to as reference data) commonly used in datasets that describe entities and evaluates the usefulness of reference data elements for uniquely identifying an entity and for monitoring systemic risk in the financial industry. Financial institutions; Information resources management; Financial services industry...|$|R
40|$|Large {{amounts of}} data are being {{collected}} both by organisations in {{the private and public}} sectors, as well as by individuals Much of these data are about people, or they are generated by people Financial, shopping, and travel transactions Electronic health and financial records Tax, social security, and census records Emails, tweets, SMSs, blog posts, etc. Analysing (mining) such data can provide huge benefits to businesses and governments May 2012 – p. 2 / 96 Motivation (continued) Often data from different sources need to be integrated and linked Improve data quality Enrich data Allow analyses that are impossible on individual databases Lack of unique <b>entity</b> <b>identifiers</b> means that linking is often based on personal information When databases are linked across organisations, maintaining privacy and confidentiality is vital This is where privacy-preserving record linkage (PPRL) can help May 2012 – p. 3 / 96 Motivating example: Health surveillance (1...|$|E
40|$|International {{audience}} In {{this paper}} we study {{the prevalence of}} unique <b>entity</b> <b>identifiers</b> on the Web. These are, e. g., ISBNs (for books), GTINs (for commercial products), DOIs (for documents), email addresses, and others. We show how these identifiers can be harvested systematically from Web pages, {{and how they can}} be associated with humanreadable names for the entities at large scale. Starting with a simple extraction of identifiers and names from Web pages, we show how we can use the properties of unique identifiers to filter out noise and clean up the extraction result on the entire corpus. The end result is a database of millions of uniquely identified entities of different types, with an accuracy of 73 [...] 96 % and a very high coverage compared to existing knowledge bases. We use this database to compute novel statistics on the presence of products, people, and other entities on the Web. </p...|$|E
40|$|Abstract. In many {{data mining}} {{projects}} {{the data to}} be analysed contains personal information, like names and addresses. Cleaning and preprocessing of such data likely involves deduplication or linkage with other data, which is often challenged {{by a lack of}} unique <b>entity</b> <b>identifiers.</b> In recent years there has been an increased research effort in data linkage and deduplication, mainly in the machine learning and database communities. Publicly available test data with known deduplication or linkage status is needed so that new linkage algorithms and techniques can be tested, evaluated and compared. However, publication of data containing personal information is normally impossible due to privacy and confidentiality issues. An alternative is to use artificially created data, which has the advantages that content and error rates can be controlled, and the deduplication or linkage status is known. Controlled experiments can be performed and replicated easily. In this paper we present a freely available data set generator capable of creating data sets containing names, addresses and other personal information. ...|$|E
5000|$|The November 2014 G-20 {{summit in}} Brisbane, Australia, {{demonstrated}} that the chances are slim for schemes to make any additional multinational companies' tax information public. Thus any proposals more ambitious than the OECD's BEPS appear impractical for now, like the Global Legal <b>Entity</b> <b>Identifier</b> (LEI) for financial markets that was established after recommendations by the international Financial Stability Board and on which the G-20 has been working since 2012, or Gabriel Zucman's idea for a world financial registry. Zucman gave the keynote address at the two-day event on [...] "Human Rights and Tax in an Unequal World" [...] at NYU's CHRGJ from September 22 to 23, 2016.|$|R
50|$|The AIFMD's {{reporting}} regulations require each AIF to {{be named}} using a series of codes, including its national identification code, the Bank Identifier Code (BIC) and the Legal <b>Entity</b> <b>Identifier</b> code. AIFMs must select only brokers and counterparties that are subject to regulatory supervision, that are financially sound, and that have the necessary organizational structure to provide services to the AIFM or the AIF. All AIFMs must submit quarterly, semi-annual, or annual reports to their respective member state regulator with information about the AIFM and its AIFs {{as well as an}} annual report with information such as the fund's financial statements, activities, and information about the total amount of remuneration paid by the AIFM to its staff.|$|R
5000|$|The [...] "left" [...] (alpha) {{side of the}} node graph forms a {{discrimination}} network {{responsible for}} selecting individual WMEs based on simple conditional tests which match WME attributes against constant values. Nodes in the discrimination network may also perform tests that compare two or more attributes of the same WME. If a WME is successfully matched against the conditions represented by one node, it is passed to the next node. In most engines, the immediate child nodes of the root node are {{used to test the}} <b>entity</b> <b>identifier</b> or fact type of each WME. Hence, all the WMEs which represent the same entity type typically traverse a given branch of nodes in the discrimination network.|$|R
40|$|We have {{developed}} an intelligent knowledge management environment called EER-ConcepTool, which analyses conceptual schemas and ontologies. In this paper, we show how analysis supports knowledge modelling and validation at the conceptual level {{through a combination of}} different logicbased and heuristic reasoning services. We highlight how EER-ConcepTool overcomes some typical drawbacks of current intelligent analysers, outlining how {{it can be used to}} provide enhanced support to knowledge modelling. Knowledge in EER-ConcepTool is represented using a formal, expressive entity-relationship model. This knowledge model includes <b>entity</b> <b>identifiers,</b> relationship attributes and a full range of participatory cardinality constraints. It also allows multiple inheritance, disjointness and full coverage assertions for entities and relationships. EER-ConcepTool provides a complete set of automated services (e. g. consistency, classification, explicitation and propagation of constraints, linguistic correlation) to analyse schemas and application ontologies based on the above knowledge model. During analysis, EER-ConcepTool can ignore selected portions of the expressive power of this model in order to increase the number and the generality of its deductions. We show how this feature can be used to provide a simple mechanism which explains schema inconsistencies...|$|E
40|$|Abstract. Entity {{resolution}} {{is the process}} of matching records that refer to the same entities from one or several databases in situations where the records to be matched do not include unique <b>entity</b> <b>identifiers.</b> Matching therefore has to rely upon partially identifying information, such as names and addresses. Traditionally, entity resolution has been applied in batch-mode and on static databases. However, increasingly organisations are challenged by the task of having a stream of query records that need to be matched to a database of known entities. As these query records are matched, they are inserted into the database as either representing a new entity, or as the latest embodiment of an existing entity. We investigate how temporal and dynamic aspects, such as time differences between query and database records and changes in database content, affect matching quality. We propose an approach that adaptively adjusts similarities between records depending upon the values of the records’ attributes and the time differences between records. We evaluate our approach on synthetic data and a large real US voter database, with results showing that our approach can outperform static matching approaches. ...|$|E
40|$|Abstract. We {{show that}} the YAK {{protocol}} does not provide the joint key control attribute, and is vulnerable to some attacks. We also propose some improvements. The YAK protocol [1, 2] is {{a variant of the}} two-pass HMQV protocol [3], but uses zero-knowledge proofs for proving knowledge of ephemeral private keys. It uses public keys that are certified by certificate authorities. Although the YAK protocol is claimed to be an authenticated key agreement protocol [1, 2], the authentication is just zero-knowledge verification of a random number, generated by the other party. There is no binding between <b>entity</b> <b>identifiers</b> and the session key derivation function. Any authenticated key agreement protocol should provide several security attributes, and it should withstand well-known attacks [4]. There are claims for security and efficiency of the YAK protocol [1, 2]. The protocol is called a key agreement protocol, but we show that it lacks the joint key control attribute which is a requirement for any key agreement protocol. Furthermore, we {{show that the}} YAK protocol is vulnerable to an unknown key-share attack and a key-replication attack. The key confirmation is left optional in the YAK protocol, bu...|$|E
5000|$|ISO 9362 {{defines a}} {{standard}} format of Business Identifier Codes (also known as SWIFT-BIC, BIC, SWIFT ID or SWIFT code) {{approved by the}} International Organization for Standardization (ISO). It is a unique identification code for both financial and non-financial institutions. [...] The acronym SWIFT stands for the Society for Worldwide Interbank Financial Telecommunication. The ISO has designated SWIFT as the BIC registration authority. When assigned to a non-financial institution, the code may also {{be known as a}} Business <b>Entity</b> <b>Identifier</b> or BEI. These codes are used when transferring money between banks, particularly for international wire transfers, and also for the exchange of other messages between banks. The codes can sometimes be found on account statements.|$|R
40|$|SAML {{profiles}} require {{agreements between}} system <b>entities</b> regarding <b>identifiers,</b> binding support and endpoints, certificates and keys, and so forth. A metadata specification {{is useful for}} describing this information in a standardized way. This document defines an extensible metadata format for SAML system entities, organized by roles that reflect SAML profiles. Such roles includ...|$|R
40|$|Data Type Definitions The {{interaction}} service behaviour is only completely defined {{when the}} data sorts and the function Allowed are defined. However {{the definition of}} these sorts and function Allowed implies a lot of design decisions, for example on the data structures of the interaction structure, interaction offers, parameters, conditions on parameters and matching of interaction requests. This section defines these data structures in an abstract way using first-order logic and elementary set theory. 224 Chapter 8 : Design of an Interaction Server Interaction Structure Let A be a set of interaction identifiers, each interaction identifier identifying an unique interaction, and E be a set of i-entity identifier sets, where each <b>entity</b> <b>identifier</b> represents an i-entity, and consequently its ISAP. An IS S (interaction structure) of a certain specification S is a relation A E between interaction and i-entity identifier sets, such that IS S means that i-entities E i [...] ...|$|R
