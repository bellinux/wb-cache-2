1|17|Public
5000|$|<b>Graphic</b> <b>Noise</b> Poster Show - Museum of Design (Atlanta, GA) - Summer 2005 ...|$|E
50|$|Some {{of their}} boats have custom <b>graphics.</b> <b>Noise</b> {{complaints}} {{have been an}} issue for the company and industry.|$|R
50|$|Perlin {{noise is}} the {{earliest}} form of lattice noise, {{which has become}} very popular in computer <b>graphics.</b> Perlin <b>Noise</b> is not suited for simulation {{because it is not}} divergence-free.|$|R
50|$|Simplex {{noise is}} useful for {{computer}} <b>graphics</b> applications, where <b>noise</b> is usually computed over 2, 3, 4 or possibly 5 dimensions. For higher dimensions, n-spheres around n-simplex corners are not densely enough packed, reducing {{the support of the}} function and making it zero in large portions of space.|$|R
40|$|We {{present a}} system that classifies pixels in a {{document}} image according to marking type such as machine print, handwriting, and noise. A segmenter module first splits an input image into fragments, sometimes breaking connected components. Each fragment is then classified by an automatically trained multi-stage classifier that is fast and considers features of the fragment, {{as well as its}} neighborhood. Features relevant for discrimination are picked out automatically from among hundreds of measurements. Our system is trainable from example images in which each foreground pixel has a “ground-truth ” label. The main distinction of our system is the level of accuracy achieved in classifying fragments at sub-connected component level, rather than larger aggregate groups such as words or text-lines. We have trained this system to detect handwriting, machine print text, machine print <b>graphics,</b> and <b>noise.</b> ...|$|R
40|$|Digital {{images can}} be {{captured}} or generated {{by a variety of}} sources including digital cameras, scanners and computer graphics softwares. In many cases {{it is important to be}} able to determine the source of a digital image such as for criminal and forensic investigation. This paper presents methods for distinguishing between an image captured using a digital camera, a computer generated image and an image captured using a scanner. The method proposed here is based on the differences in the image generation processes used in these devices and is independent of the image content. The method is based on using features of the residual pattern noise that exist in images obtained from digital cameras and scanners. The residual noise present in computer generated images does not have structures similar to the pattern noise of cameras and scanners. The experiments show that a feature based approach using an SVM classifier gives high accuracy. Index Terms — image forensics, digital camera, scanners, computer <b>graphics,</b> pattern <b>noise...</b>|$|R
40|$|Noise {{functions}} are an essential building block for writing procedural shaders in 3 D computer <b>graphics.</b> The original <b>noise</b> function introduced by Ken Perlin {{is still the}} most popular because it is simple and fast, and many spectacular images have been made with it. Nevertheless, it is prone to problems with aliasing and detail loss. In this paper we analyze these problems and show that they are particularly severe when 3 D noise is used to texture a 2 D surface. We use the theory of wavelets to create a new class of simple and fast noise functions that avoid these problems...|$|R
50|$|More {{sophisticated}} models may feature dual cassette decks (often featuring high-speed dubbing, {{or sometimes}} even digitally controlled servo cassette mechanics), separate bass and treble level controls, five- or ten-band <b>graphic</b> equalizers, Dolby <b>noise</b> reduction, analog or LED sound level (VU) meters or even VFD, larger speakers, 'soft-touch' tape deck controls, multiple shortwave (SW) band reception with fine tuning, digital tuner with PLL, automatic song search functions for cassettes, line and/or phono inputs and outputs, microphone inputs, loudness switches, and detachable speakers, full function infrared remote control. A handful of models even featured an integrated vinyl record player, an 8-track tape player or a (typically black-and-white) television screen, although the basic radio/cassette models {{have historically been}} {{by far the most}} popular.|$|R
40|$|International audienceThe aim of {{the paper}} is to {{separate}} handwritten and printed text from a real document embedded with <b>noise,</b> <b>graphics</b> including annotations. Relying on run-length smoothing algorithm (RLSA), the extracted pseudo-lines and pseudo-words are used as basic blocks for classification. To handle this, a multi-class support vector machine (SVM) with Gaussian kernel performs a first labelling of each pseudo-word including the study of local neighbourhood. It then propagates the context between neighbours {{so that we can}} correct possible labelling errors. Considering running time complexity issue, we propose linear complexity methods where we use k-NN with constraint. When using a kd-tree, it is almost linearly proportional to the number of pseudo-words. The performance of our system is close to 90 %, even when very small learning dataset are used, where samples are basically composed of complex administrative documents...|$|R
40|$|Abstract. In this paper, {{we propose}} a novel type of {{explicit}} image fil-ter- guided filter. Derived {{from a local}} linear model, the guided filter generates the filtering output by considering the content of a guidance image, which can be the input image itself or another different image. The guided filter can perform as an edge-preserving smoothing operator like the popular bilateral filter [1], but has better behavior near the edges. It also has a theoretical connection with the matting Laplacian matrix [2], so is a more generic concept than a smoothing operator and can better utilize the structures in the guidance image. Moreover, the guid-ed filter has a fast and non-approximate linear-time algorithm, whose computational complexity is independent of the filtering kernel size. We demonstrate that the guided filter is both effective and efficient in {{a great variety of}} computer vision and computer <b>graphics</b> applications including <b>noise</b> reduction, detail smoothing/enhancement, HDR compression, im-age matting/feathering, haze removal, and joint upsampling. ...|$|R
40|$|Human facial {{gestures}} often exhibit such natural stochastic variations as {{how often}} the eyes blink, {{how often the}} eyebrows and the nose twitch, and how the head moves while speaking. The stochastic movements of facial features are key ingredients for generating convincing facial expressions. Although such small variations have been simulated using noise functions in many <b>graphics</b> applications, modulating <b>noise</b> functions to match natural variations induced from the affective states and the personality of characters is difficult and not intuitive. We present a technique for generating subtle expressive facial gestures (facial expressions and head motion) semiautomatically from motion capture data. Our approach is based on Markov random fields that are simulated in two levels. In the lower level, the coordinated movements of facial features are captured, parameterized, and transferred to synthetic faces using basis shapes. The upper level represents independent stochastic behavior of facial features. The experimental results show that our system generates expressive facial gestures synchronized with input speech...|$|R
40|$|International audienceProcedural {{noise is}} a {{fundamental}} tool in Computer <b>Graphics.</b> However, designing <b>noise</b> patterns is hard. In this paper, we present Gabor noise by example, a method to estimate the parameters of bandwidth-quantized Gabor noise, a procedural noise function that can generate noise with an arbitrary power spectrum, from exemplar Gaussian textures, a class of textures that is completely characterized by their power spectrum. More specifically, we introduce (i) bandwidth-quantized Gabor noise, a generalization of Gabor noise to arbitrary power spectra that enables robust parameter estimation and efficient procedural evaluation; (ii) a robust parameter estimation technique for quantized-bandwidth Gabor noise, that automatically decomposes the noisy power spectrum estimate of an exemplar into a sparse sum of Gaussians using non-negative basis pursuit denoising; and (iii) an efficient procedural evaluation scheme for bandwidth-quantized Gabor noise, that uses multi-grid evaluation and importance sampling of the kernel parameters. Gabor noise by example preserves the traditional advantages of procedural noise, including a compact representation and a fast on-the-fly evaluation, and is mathematically well-founded. See project page at : [URL]...|$|R
40|$|Procedural {{noise is}} a {{fundamental}} tool in Computer <b>Graphics.</b> However, designing <b>noise</b> patterns is hard. In this paper, we present Gabor noise by example, a method to estimate the parameters of bandwidth-quantized Gabor noise, a procedural noise function that can generate noise with an arbitrary power spectrum, from exemplar Gaussian textures, a class of textures that is completely characterized by their power spectrum. More specifically, we introduce (i) bandwidth-quantized Gabor noise, a generalization of Gabor noise to arbitrary power spectra that enables robust parameter estimation and efficient procedural evaluation; (ii) a robust parameter estimation technique for quantized-bandwidth Gabor noise, that automatically decomposes the noisy power spectrum estimate of an exemplar into a sparse sum of Gaussians using non-negative basis pursuit denoising; and (iii) an efficient procedural evaluation scheme for bandwidth-quantized Gabor noise, that uses multi-grid evaluation and importance sampling of the kernel parameters. Gabor noise by example preserves the traditional advantages of procedural noise, including a compact representation and a fast on-the-fly evaluation, and is mathematically well-founded. status: publishe...|$|R
40|$|Images {{exist in}} {{different}} formats {{in real time}} applications. There is no prescribed format in which an image should be presented as input to any image processing algorithm. This article experiments a neural network approach to classify the noises present in an image given in BMP (Bitmap), JPG/JPEG(Joint Photographic Experts Group), TIF/TIFF(Tagged Image File Format), GIF(Graphics Interchange Format) and PNG(Portable Network <b>Graphics)</b> format. The <b>noises</b> in the image are classified by extracting the statistical features like skewness and kurtosis, which is then applied to the Back Propagation Network (BPN) and Multi Layer Perceptron (MLP). This is done for images of all the formats. MLP is superior in classifying salt and pepper noise in images stored in PNG format. BPN is performing well in classifying Gaussian white noise in images stored in BMP format. The study throws light {{on the type of}} neural network to be employed for classifying the different noises present in images of different formats, which will prove to be useful in enhancing the image for further processing...|$|R
40|$|Because of its {{computational}} simplicity, {{the graphic}} method introduced by Logan et al. is frequently {{used to analyze}} time– activity curves of reversible radiotracers measured in brain regions with PET. The graphic method uses a nonlinear transformation of data to variables that have an asymptotically linear relationship. Compared with compartmental analysis of untransformed data, the graphic method enables derivation of regional distribution volumes that are free from assumptions about the underlying compartmental configuration. In this article, we describe statistical bias associated with this nonlinear transformation method. Methods: Theoretic analysis, Monte Carlo simulation, and statistical analysis of PET data were {{used to test the}} graphic method for bias. Results: Mean zero noise is associated with underestimation of distribution volumes when data are analyzed with graphic analysis, whereas this effect does not occur when the same data are analyzed by nonlinear regression and compartmental analysis. Moreover, this effect depends on the magnitude of the distribution volume, so that the bias is more pronounced in regions with high receptor density than regions with low receptor density or no receptors (region of reference). Conclusion: These results indicate that conventional kinetic analysis of untransformed data is less sensitive to mean zero <b>noise</b> than is <b>graphic</b> analysis of nonlinearly transformed data. Key Words: PET; kinetic modeling; <b>graphic</b> analysis; <b>noise</b> J Nucl Med 2000; 41 : 2083 – 2088 Graphic analysis of PET data acquired with reversible (1) or irreversible (2) radiotracers is commonly used for quantification. The method of Logan et al. (1) for analysis of reversible radiotracers comprises a nonlinear change of variables applied to region-of-interest (ROI) activities and to the arterial plasma input function (C a), in which the transformed variables have an asymptotically linear relationship. The linear part of the graph can be written {{in the form of an}} equation for a line: t � 0...|$|R
40|$|Self-assessment {{measures}} of competency are blends of an authentic self-assessment signal that researchers seek {{to measure and}} random disorder or "noise" that accompanies that signal. In this study, we use random number simulations to explore how random noise affects critical aspects of self-assessment investigations: reliability, correlation, critical sample size, and the graphical representations of self-assessment data. We show that graphical conventions common in the self-assessment literature introduce artifacts that invite misinterpretation. Troublesome conventions include: (y minus x) vs. (x) scatterplots; (y minus x) vs. (x) column graphs aggregated as quantiles; line charts that display data aggregated as quantiles; and some histograms. Graphical conventions that generate minimal artifacts include scatterplots with a best-fit line that depict (y) vs. (x) measures (self-assessed competence vs. measured competence) plotted by individual participant scores, and (y) vs. (x) scatterplots of collective average {{measures of}} all participants plotted item-by-item. This last <b>graphic</b> convention attenuates <b>noise</b> and improves {{the definition of the}} signal. To provide relevant comparisons across varied graphical conventions, we use a single dataset derived from paired measures of 1154 participants' self-assessed competence and demonstrated competence in science literacy. Our results show that different numerical approaches employed in investigating and describing self-assessment accuracy are not equally valid. By modeling this dataset with random numbers, we show how recognizing the varied expressions of randomness in self-assessment data can improve the validity of numeracy-based descriptions of self-assessment. ...|$|R
40|$|Noise is {{of vital}} {{interest}} {{in many parts}} of computer sciene, especially in the computer <b>graphics</b> eld where <b>noise</b> is used to create nature-like e ects. Perlin’s 1985 algorithm to generate noise remains the most pop- ular in spite of many alternatives having been presented over the years. In this report we have examined the execution time impact of two new gradient table data structures and a new hash method for this algo- rithm, suggested by Perlin in 2002 and Olano in 2005 respectively. Our implementation simulated turbulence and ran in parallel on a modern GPU using the OpenCL framework. We also examined if the turbulence method’s octave summation could bene t from parallelization. Results suggest that Olano’s hash method performs signi cantly faster, while Perlin’s original gradient table data structure performs slightly faster than the suggested improvements. We also found that a paral- lelization of the octave summation in the turbulence method performs signi cantly faster.  Brusalstring är av stort intresse i många datavetenskapliga områden, speciellt i datorgra ksfältet där det används för att simulera naturliga fenomen. Perlins algoritm från 1985 fortsätter vara den mest populära, trots att åtskilliga alternativ har presenterats genom åren. I den här rap- porten har vi undersökt hur exekveringstiden av denna algoritm påverkas av två nya gradientliststrukturer och en ny hashmetod som föreslagits av Perlin i 2002 respektive Olano i 2005. Vår implementation simulerade virvelströmning och kördes parallelt på en modern GPU via ramverket OpenCL. Vi undersökte också om virvelströmningens oktavsummering kunde förbättras med parallelisering. Resultaten var att Olanos föreslagna hashmetod förbättrade körtiden betydligt, medan gradientliststrukturen i Perlins ursprungliga algoritm resulterade in en snabbare körtid än de två föreslagna förbättringarna. Vi fann också att en parallelisering av virvelströmningens oktavsummation förbättrade körtiden betydligt. ...|$|R

