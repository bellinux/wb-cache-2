62|334|Public
25|$|For ZFS, data {{integrity}} {{is achieved by}} using a Fletcher-based checksum or a SHA-256 hash throughout the file system tree. Each block of data is checksummed and the checksum value is then saved in the pointer to that block—rather than at the actual block itself. Next, the block pointer is checksummed, with the value being saved at its pointer. This checksumming continues {{all the way up}} the file system's <b>data</b> <b>hierarchy</b> to the root node, which is also checksummed, thus creating a Merkle tree. In-flight data corruption or phantom reads/writes (the data written/read checksums correctly but is actually wrong) are undetectable by most filesystems as they store the checksum with the data. ZFS stores the checksum of each block in its parent block pointer so the entire pool self-validates.|$|E
50|$|<b>Data</b> <b>hierarchy</b> {{refers to}} the {{systematic}} organization of data, often in a hierarchical form. Data organization involves characters, fields, records, files and so on.|$|E
5000|$|Traceable - Mapped to a {{specific}} data element. In business, a term may be traced to an entity (for example, a customer) or an attribute (such as a customer's name). A term may be a value in a data set (such as gender), or designate the data set itself. Traceability indicates relationships in the <b>data</b> <b>hierarchy.</b>|$|E
50|$|Goalscape {{represents}} <b>data</b> <b>hierarchies</b> as a multi-level {{pie chart}} to improve user {{recognition of the}} structures. A multi-level pie chart {{can also be considered}} a Mandala Chart.|$|R
50|$|<b>Data</b> {{structure}} <b>hierarchy</b> {{is maintained}} by outline indentation.|$|R
40|$|Special Report Writer (SRR) accepts input {{submitted}} by user, accesses sequential data base, and produces desired special report. Program is batch-oriented information-retrieval system that performs multiple correlations on files containing several <b>data</b> <b>hierarchies.</b> Report requests are specified in simple notation, readily learned by people without extensive backgrounds in data processing...|$|R
50|$|It {{supports}} iPhone, Android and Windows 8 mobile platforms, however, {{its website}} {{has been designed}} to be accessible in most mobile web browsers. An iPad application was released on March 12, 2013. It added support for an unlimited number of tags, in the form of colored labels that can be renamed and new ones created, starting November 21, 2014. Cards accept comments, attachments, votes, due dates and checklists. Trello has an API. Users may organize projects through the utilization of boards, lists and cards, which form a bespoke <b>data</b> <b>hierarchy</b> that facilitates effective management of projects, jobs and tasks.|$|E
5000|$|In 1953, IBM {{recognized}} the immediate application {{for what it}} termed a [...] "Random Access File" [...] having high capacity and rapid random access at a relatively low cost. After considering technologies such as wire matrices, rod arrays, drums, drum arrays, etc., the engineers at IBM's San Jose California laboratory invented the hard disk drive. The disk drive created a new level in the computer <b>data</b> <b>hierarchy,</b> then termed Random Access Storage but today known as secondary storage, less expensive and slower than main memory (then typically drums) but faster and more expensive than tape drives.|$|E
50|$|For ZFS, data {{integrity}} {{is achieved by}} using a Fletcher-based checksum or a SHA-256 hash throughout the file system tree. Each block of data is checksummed and the checksum value is then saved in the pointer to that block—rather than at the actual block itself. Next, the block pointer is checksummed, with the value being saved at its pointer. This checksumming continues {{all the way up}} the file system's <b>data</b> <b>hierarchy</b> to the root node, which is also checksummed, thus creating a Merkle tree. In-flight data corruption or phantom reads/writes (the data written/read checksums correctly but is actually wrong) are undetectable by most filesystems as they store the checksum with the data. ZFS stores the checksum of each block in its parent block pointer so the entire pool self-validates.|$|E
5000|$|Exception {{handling}} - translating {{data access}} related exception to a Spring <b>data</b> access <b>hierarchy</b> ...|$|R
40|$|Algorithmic {{methods are}} {{demonstrated}} for information extraction from table header elements, including data categories and <b>data</b> <b>hierarchies.</b> The table headers are found with the Minimum Index Point Search algorithm. The header-path alignment and header completion algorithms yield database-ready table content and configuration statistics on {{a random sample}} of 400 diverse tables with ground truth and 1120 tables without ground truth from international statistical data sites...|$|R
50|$|Web {{applications}} {{often include}} lengthy descriptive attributes in their URLs which represent <b>data</b> <b>hierarchies,</b> command structures, transaction paths and session information. This practice {{results in a}} URL that is aesthetically unpleasant and difficult to remember, and which may not fit within the size limitations of microblogging sites. URL shortening services provide {{a solution to this}} problem by redirecting a user to a longer URL from a shorter one.|$|R
40|$|Contents The MPEG- 1 Standard 4 Introduction 4 MPEG Video Compression Techniques 5 Stream Structure 5 Video Stream <b>Data</b> <b>Hierarchy</b> 5 Group of Pictures (GOP) 5 Picture 6 Macroblock 6 Block 6 Audio Stream <b>Data</b> <b>Hierarchy</b> 6 Intra-Picture Coding 7 Picture Types 7 Intra Pictures 7 Predicted Pictures 7 Bidirectional Pictures 7 Video Stream Composition 8 Motion Compensation 8 Intra-picture (Transform) Coding 9 Synchronization 10 System Clock Reference 10 MPEG- 1 Audio Standard 10 Analog-to-Digital 11 Audio Encoding 11 MPEG- 1 Audio Encoding Diagram 12 Operating Mode 13 References 14 The MPEG- 2 Standard 15 Introduction 15 MPEG Standards 15 The {{following}} MPEG standards exist: 16 3 Overview of MPEG- 2 16 MPEG- 2 Video Compression 17 Video Stream <b>Data</b> <b>Hierarchy</b> 17 Picture Types 19 Profile and Levels 21 Details of profiles 21 MPEG- 2 Video profiles 22 Details {{of levels}} 23 Interla...|$|E
40|$|International audienceThe {{production}} of legislative acts {{is affected by}} multiple sources of latent hetero-geneity, due to multilevel and multivariate unobserved factors that operate in conjunction with observed covariates at all the levels of the <b>data</b> <b>hierarchy.</b> We account for these factors by estimating a multilevel Poisson regression model for repeated measurements of bivariate counts of executive and ordinary legislative acts, enacted under multiple Italian governments, nested within legislatures. The model integrates discrete bivariate random effects at the legislature level and Markovian sequences of discrete bivariate random effects at the government level. It can be estimated by a computationally feasible expectation–maximization algorithm. It naturally extends a traditional Poisson regression model to allow for multiple outcomes, longitudinal dependence and multilevel <b>data</b> <b>hierarchy.</b> The model is exploited to detect multiple cycles of legislative supply that arise at multiple timescales in a case-study of Italian legislative production...|$|E
40|$|We {{present a}} new {{visualization}} approach based on procedural grid generation for scattered data sets. Instead of pre-computing triangulations {{for a given}} set of levels in a <b>data</b> <b>hierarchy,</b> a triangulation within a specified region of interest (ROI) is computed by considering scattered data of a pre-computed scattered <b>data</b> <b>hierarchy.</b> By choosing an appropriate level in the hierarchy the number of samples representing the data set in the ROI can be kept sufficiently small to interactively generate a triangulation and a visualization of the data within the ROI. This allows one to generate the triangulation on-the-fly during the visualization process. 1 Introduction Visualization of large-scale scientific data sets is a field of increasing importance. Often, data are given as scattered data, i. e., data samples consisting of a position and a data value without connectivity. A commonly used solution {{to deal with the}} visualization of large-scale data sets is to use a hierarchical representat [...] ...|$|E
40|$|This paper {{demonstrates}} a general framework to restyle UI widgets, {{in order to}} adapt them to the user behavior. Differ-ent implementation examples illustrate its feasibility. The value of this methodology {{comes from the fact}} that it is suited to any application language or toolkit supporting struc-tured <b>data</b> <b>hierarchies</b> and style sheets; e. g., interfaces cre-ated in HTML, XUL, Flex/AIR (ActionScript), or Java. As described in the paper, an explicit end user intervention is not required, and changes are gradually applied so that they are not intrusive for the user...|$|R
5000|$|Data {{structures}} that aggregate objects {{are the focus}} of the [...] package. Included in the package is the Collections API, an organized <b>data</b> structure <b>hierarchy</b> influenced heavily by the design patterns considerations.|$|R
40|$|A {{special report}} writer (SSR) was {{developed}} which performs multiple correlations on files containing several <b>data</b> <b>hierarchies.</b> Output reports are specified {{in a simple}} notation, readily learned by persons having limited familarity with ADP. The SRR system can be adopted by other NASA installations while the basic techniques themselves are compatible with the information management needs {{of a wide range}} of organizations. Specifically, the program lends itself to generalization and can be readily adapted for other file management purposes. Extensive details on the characteristics of the SRR program are presented along with a full explanation of the system for those contemplating its application to other data bases. The complete COBOL program and documentation are available. "NASA TM X- 3346. ""February 1976. "Cover title. A special report writer (SSR) was developed which performs multiple correlations on files containing several <b>data</b> <b>hierarchies.</b> Output reports are specified in a simple notation, readily learned by persons having limited familarity with ADP. The SRR system can be adopted by other NASA installations while the basic techniques themselves are compatible with the information management needs {{of a wide range of}} organizations. Specifically, the program lends itself to generalization and can be readily adapted for other file management purposes. Extensive details on the characteristics of the SRR program are presented along with a full explanation of the system for those contemplating its application to other data bases. The complete COBOL program and documentation are available. Mode of access: Internet...|$|R
30|$|Mars is {{an ideal}} place to test generic {{planetary}} stereo analysis system as pyramidal <b>data</b> <b>hierarchy</b> in in-orbital imagery (from coarse- to high-resolution) is well established. Also, from the user’s perspective, the demand of high-resolution topographic datasets for analyses of geological, climatic and potentially exobiological evolution of the Mars has been rapidly increasing. Therefore, the prototype of stereo processing system was tested with Martian imagery in this study.|$|E
40|$|We {{present the}} design and {{evaluation}} of the query-routing protocol of the TerraDir distributed directory. TerraDir is a wide-area distributed directory designed for hierarchical namespaces, and provides a lookup service for mapping keys to objects. We introduce distributed lookup and caching algorithms that leverage the underlying <b>data</b> <b>hierarchy.</b> Our algorithms provide e#cient lookups while avoiding the load imbalances often associated with hierarchical systems. The TerraDir load balancing scheme also incorporates a node replication algorithm that provides configurable failure resilience with provably low overheads...|$|E
40|$|Abstract. Multiresolution {{representation}} of high-dimensional scattered data {{is a fundamental}} problem in scientific visualization. This paper introduces a <b>data</b> <b>hierarchy</b> of Voronoi diagrams as a versatile solution. Given an arbitrary set of points in the plane, our goal is {{the construction of an}} approximation hierarchy using the Voronoi diagram as the essential building block. We have implemented two Voronoi diagram-based algorithms to demonstrate their usefulness for hierarchical scattered data approximation. The first algorithm uses a constant function to approximate the data within each Voronoi cell, and the second algorithm uses the Sibson interpolant [14]. ...|$|E
40|$|We {{present a}} new {{hierarchical}} navigation interface for level-of-detail selection and rendering of multiresolution volumetric data. The interface consists of multiple coordi-nated views based on concepts from information visualiza-tion {{as well as}} scientific visualization literature. With key features such as brushing and linking, and focus and con-text, it gives the users full control over the level-of-detail se-lection when navigating through large multiresolution <b>data</b> <b>hierarchies.</b> The navigation interface can also be integrated with traditional level-of-detail selection methods for more effective visual data exploration. We test the utility and ef-fectiveness of this hierarchical navigation interface {{on a couple of}} large-scale three-dimensional steady and time-varying data sets. ...|$|R
40|$|We {{present a}} {{framework}} for visualizing large tabular data that combines two views: the table view and the treemap view. The table view extends the known table lens as follows: We cluster related elements to reduce subsampling artifacts and achieve table size independent rendering time; we use multiple-column sorting to create scenario-specific <b>data</b> <b>hierarchies</b> on the fly; and we use shaded cushions to show data structure and variation. Hierarchies built in the table view are shown in a customizable treemap view. One can choose both layout and rendering by a few clicks, effectively creating visual scenarios on-the-fly. We illustrate our framework on real-life stock data. ...|$|R
5000|$|... {{support for}} a {{dedicated}} <b>hierarchy</b> <b>data</b> type, such as in SQL's hierarchical query facility; ...|$|R
40|$|Mixture models {{implemented}} via the expectationmaximization (EM) algorithm {{are being}} increasingly {{used in a}} wide range of problems in statistical pattern recognition. For many applied problems in medical and health research, the data collected may exhibit a hierarchical structure. The independence assumption in the maximum likelihood (ML) learning of mixture models is no longer valid. Ignoring the correlation between hierarchically structured data can lead to misleading pattern recognition. In this paper, we consider the extension of Gaussian mixtures to incorporate data hierarchies via the linear mixed-effects model (LMM). Clustered and longitudinal <b>data</b> <b>hierarchy</b> settings in medical and biological research are considered. 1...|$|E
30|$|From the {{description}} of the workflow and DTM products demonstrated in Figs. 2 and 3, it was realized that no manual DTM editing was necessary and obvious matching blunders were not observed in the final DTM. The reliability and effectiveness of the model-based matching strategy exploiting <b>data</b> <b>hierarchy</b> was proved. Figure 3 (a) and 3 (b) demonstrated a DTM products created with the maximum grid spacing in test areas with HiRISE. The maximum HiRISE DTM grid spacing depends on the stereo image quality but normally 0.5 – 0.7 m was achievable with the developed matching scheme. Clearly the hierarchical approach for the model-based matching in the stereo implementation is capable of producing sub-meter topography with HiRISE image pair.|$|E
40|$|With the advancements of {{both the}} {{computational}} power of machine learning models and the complexity of data being generated for analysis, the process of identifying the integration flow of information before analysis is becoming more challenging. Implemented models require information such as source <b>data</b> <b>hierarchy</b> and cardinality information and correlation of subject matter as used in ontology. This research project focuses on the integration phase of database elements to its matching component in the data warehouse using only the source data elements and its content information. It questions the capability of these elements and the profile information of itself and its contents in automating the matching process to its designated target element in a data warehouse...|$|E
40|$|Time-course {{experiments}} with microarrays {{are often used}} to study dynamic biological systems and genetic regulatory networks (GRNs) that model how genes inﬂuence each other in cell-level development of organisms. The inference for GRNs provides important insights into the fundamental biological processes such as growth and is useful in disease diagnosis and genomic drug design. Due to the experimental design, multilevel <b>data</b> <b>hierarchies</b> are often present in time-course gene expression data. Most existing methods, however, ignore the dependency of the expression measurements over time and the correlation among gene expression proﬁles. Such independence assumptions violate regulatory interactions and can result in overlooking certain important subject eﬀects and lead to spurious inference for regulatory networks or mechanisms. In this paper, a multilevel mixed-eﬀects model is adopted to incorporate <b>data</b> <b>hierarchies</b> {{in the analysis of}} time-course data, where temporal and subject eﬀects are both assumed to be random. The method starts with the clustering of genes by ﬁtting the mixture model within the multilevel random-eﬀects model framework using the expectation-maximization (EM) algorithm. The network of regulatory interactions is then determined by searching for regulatory control elements (activators and inhibitors) shared by the clusters of co-expressed genes, based on a time-lagged correlation coeﬃcients measurement. The method is applied to two real time-course datasets from the budding yeast (Saccharomyces cerevisiae) genome. It is shown that the proposed method provides clusters of cell-cycle regulated genes that are supported by existing gene function annotations, and hence enables inference on regulatory interactions for the genetic network...|$|R
40|$|We {{propose the}} use of {{competitive}} learning in deep networks for understanding sequential <b>data.</b> <b>Hierarchies</b> of competitive learning algorithms {{have been found in}} the brain [1] and their use in deep vision networks has been validated [2]. The algorithm is simple to comprehend and yet provides fast, sparse learning. To understand temporal patterns we use the depth of the network and delay blocks to encode time. The delayed feedback from higher layers provides meaningful predictions to lower layers. We evaluate a multi-factor network design by using it to predict frames in movies it has never seen before. At this task our system outperforms the prediction of the Recurrent Temporal Restricted Boltzmann Machine [3] on novel frame changes. ...|$|R
40|$|Information Maps can be sourced from {{relational}} database tables or views, SAS datasets, SAS views, SAS OLAP cubes, or SAS stored processes, {{or a combination}} of the above. This “Stack ” of source options has varying pros and cons for development and ongoing maintenance costs. Depending upon any required data manipulation and/or the need to build <b>data</b> <b>hierarchies,</b> filtering requirements, and considering the need to return data results quickly in an interactive environment, some choices for Information Map sources may be better than others. In this paper, we will share our lessons learned in source selection and Information Map design, some realistic performance testing guidelines, and recommendations for improving the user experience using SAS 9. 1. 3 Intelligence Platform...|$|R
40|$|The {{underlying}} {{framework of}} the GRIDGEN multiple block grid generation system has been refined so that grid components are now stored within a hierarchical data structure. This restructuring has enhanced the usability of the software by allowing grids to be generated on a more intuitive level. This new framework also provides {{a means by which}} the multiple block system can be edited at most any level in the grid generation process. Editing tools are currently being added to GRIDGEN so that a change to the grid can be propagated backward and forward in the <b>data</b> <b>hierarchy.</b> The new data structure, the editing tools, and other recent GRIDGEN improvements are described in this paper...|$|E
40|$|ABSTRACT. A {{statistical}} model is proposed {{that describes the}} determination of an educational outcome variable as a nonlinear function of explanatory variables defined {{at different levels of}} a survey <b>data</b> <b>hierarchy,</b> say students and classes. The model hypothesizes that the student-level explanatory variables form a composite such that the intercept and slope in the regression of the outcome on the composite vary across classes systematically as functions of class-level variables and aggregates. A method is described for estimating the parameters of the model using robust techniques. The theoretical and practical derivation of the model is discussed, and an example is given. Since 1960, policymakers have made many efforts to use large-scale surveys to obtain evidence about the quality of education and educational production...|$|E
40|$|Abstract. For {{interactive}} systems, recognition, reproduction, and {{generalization of}} observed motion data are crucial for successful interaction. In this paper, {{we present a}} novel method for analysis of motion data that we refer to as K-OMM-trees. K-OMM-trees combine Ordered Means Models (OMMs) a model-based machine learning approach for time series with an hierarchical analysis technique for very large data sets, the K-tree algorithm. The proposed K-OMM-trees enable unsupervised prototype extraction of motion time series data with hierarchical data representation. After introducing the algorithmic details, we apply the proposed method to a gesture data set that includes substantial inter-class variations. Results from our studies show that K-OMM-trees are able to substantially increase the recognition performance and to learn an inherent <b>data</b> <b>hierarchy</b> with meaningful gesture abstractions...|$|E
40|$|Many {{data are}} {{naturally}} modeled by an unobserved hierarchical structure. In this {{paper we propose}} a flexible nonparametric prior over unknown <b>data</b> <b>hierarchies.</b> The approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are infinitely exchangeable. One can view our model as providing infinite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree. By using a stick-breaking approach, we can apply Markov chain Monte Carlo methods based on slice sampling to perform Bayesian inference and simulate from the posterior distribution on trees. We apply our method to hierarchical clustering of images and topic modeling of text data. ...|$|R
40|$|Abstract. Dimension schemas are {{abstract}} {{models of}} the <b>data</b> <b>hierarchies</b> that populate OLAP warehouses. Although there is abundant work on schema equivalence {{in a variety of}} data models, these works do not cover dimension schemas. In this paper we propose a notion of equivalence that allows to compare dimension schemas with respect to their information capacity. The proposed notion is intended to capture dimension schema equivalence in the context of OLAP schema restructuring. We offer characterizations of schema equivalence in terms of graph and schema isomorphisms, and present algorithms for testing it in well known classes of OLAP dimension schemas. Our results also permit to compare the expressiveness of different known classes of dimension schemas. ...|$|R
50|$|There {{are several}} reasons to use URL shortening. Often regular unshortened links may be aesthetically unpleasing. Many web {{developers}} pass descriptive attributes in the URL to represent <b>data</b> <b>hierarchies,</b> command structures, transaction paths or session information. This can result in URLs that are hundreds of characters long and that contain complex character patterns. Such URLs are difficult to memorize, type-out or distribute. As a result, long URLs must be copied-and-pasted for reliability. Thus, short URLs may be more convenient for websites or hard copy publications (e.g. a printed magazine or a book), the latter often requiring that very long strings be broken into multiple lines (as {{is the case with}} some e-mail software or internet forums) or truncated.|$|R
