9|362|Public
30|$|With {{the model}} {{parameters}} for distance loss, cumulated wall loss, and interaction loss now <b>fixed,</b> <b>measurements</b> {{on the third}} floor of the Zuiderpoort building were performed as a first validation without any further tuning (see Table 1), albeit in a very similar environment.|$|E
30|$|A {{number of}} {{experimental}} procedures were {{organized in the}} study area. The field surveys involved microclimatic monitoring by <b>fixed</b> <b>measurements</b> {{and a set of}} portable equipment. Note that, this was a measurement campaign that did not include a fully equipped monitoring station such the stations in the entrance and away from the city.|$|E
40|$|A {{number of}} {{diffusive}} samplers {{that are used}} for longer-term monitoring of nitrogen dioxide in the European Union were subjected to a review of their use and performance characteristics. The information collected {{was used to evaluate}} the capacity of diffusive samplers for monitoring the European Union annual limit value of 40 µg. m- 3 and to draft a proposal method for monitoring nitrogen dioxide in ambient air using diffusive samplers for later standardisation by the CEN Technical Committee 264 ¿Air Quality¿ Working Group 11 ¿Diffusive Samplers¿. A bibliographic review was carried out to determine the availability of validation data for these samplers, their capacity to meet the data quality objectives for indicative and/or <b>fixed</b> <b>measurements</b> of the European Directive, the possibility for users to analyse the samplers and their current use throughout the European Union for measuring ambient air quality. Two tubes-type samplers and a radial sampler were found to be used throughout the European Union. Based on the findings of the review it was concluded only for tube-type samplers based on the application of triethanolamine as the sorbent sufficient information was available to underpin its potential for meeting European Union data quality objectives, at the least, for indicative measurements of nitrogen dioxide in ambient air. JRC. H. 4 -Transport and air qualit...|$|E
40|$|This paper {{presents}} a Distributed compressive video sensing scheme with Adaptive measurements (DCVS-AM). In this approach, the key frame in each Group of pictures (GOP) is coded by Compressive sensing (CS) with a <b>fixed</b> <b>measurement</b> rate; whereas other frames {{in the same}} GOP are compressed by an adaptive random projection in two stages, yielding the Adaptive compressive sensing (ACS) frames. The first stage uses a small and <b>fixed</b> <b>measurement</b> rate and recovers a coarse version. In the second stage, each coarse-version ACS-frame together with its proceeding and following key frames will go through a joint analysis at the decoder side and the analysis result - Structural similarity (SSIM) {{that is based on}} a motion-guided interpolation and calculated in a multilevel discrete wavelet transform domain - is sent back to the encoder side to facilitate a re-sampling of the ACS-frame with an adaptive measurement rate. Experimental results show that our proposed DCVS-AM consistently outperforms the state-of-the-art DCVS with a <b>fixed</b> <b>measurement...</b>|$|R
3000|$|... ∗, {{probability}} values P become smaller for a <b>fixed</b> <b>measurement</b> scenario. This can {{be explained}} by looking at the dashed and dotted lines in the figures, which specify the border of D [...]...|$|R
50|$|A {{snap gage}} {{is a form}} of go/no go gauge. It is a limit gage with {{permanently}} or temporarily <b>fixed</b> <b>measurement</b> aperture(s) (gaps) which is used to quickly verify whether an outside dimension of a part matches a preset dimension or falls within predefined tolerances.|$|R
40|$|Studies in {{international}} relations frequently use <b>fixed</b> <b>measurements</b> {{over the course of}} observations that are too aggregated for appropriately testing our theories. Such is the case with testing how the balance of power affects conflict escalation. Although power balances have been examined as a cause of war {{perhaps more than any other}} factor {{in international}} relations, studies of the balance of power all use static measurements of power which remain fixed over the course of a conflict (typically, the militarized dispute). Since we do not measure the dynamics of the balance of power, we still do not understand how the balance of power affects dispute escalation. To properly evaluate how the balance and changes in it lead to dispute escalation or de-escalation, we must apply a statistical model which allows time-varying covariates over the course of a dispute. Once measured properly and assessed in a competing risks hazard model, the balance of power does appear to affect dispute escalation, with a preponderance of power in favor of one side leading to longer disputes, but also disputes that are ultimately more likely to end in peace than war. The method applied here to balance of power can be applied more broadly to a class of similar problems where variable values change over time within what is nominally our unit of analysis...|$|E
40|$|Problem statement: Several {{methods can}} be adopted {{to study the}} {{variations}} in urban climate. The mobile measurement method is one of them, involving information provided by moving measurements of air temperature, that are taken in points defined along pre-established routes and also data from fixed-point temperature recording stations. Because moving measurements are made in different times along the measurement process, adjustments must be made in order to adequately analyze the air temperature measurements. Approach: Mobile measurements were taken in an urban area and contextualized {{in the domain of}} some fixed-point temperature recording stations. Therefore, a linear model proposed to investigate and represent the variables that influence moving measurements estimation in the urban context. Results: All proposed variables in the linear model were considered relevant, because all coefficients of the determined model were non null. Also, the identified model presents a good fit to the field data, as indicated by the resulting coefficient of determination (R 2) that is 90. 3 %. Conclusion/Recommendations: The linear model described in this work is easy to apply, requiring few input variables. It is important to emphasize that the model was developed to estimate moving measurements as a function of <b>fixed</b> <b>measurements</b> and presents the potential to identify new input variables based on moving measurements, as shown by the fit among fixed and moving temperature measurements, in order to provide insight about other possible models of late time adjustment. © 2011 Science Publications...|$|E
40|$|A {{method is}} {{proposed}} for the moisture analysis of buildings, based on the combination of complementing monitoring techniques. A periodic JR scanning of the whole surface is combined with a visual recording, plus an automatic data logging of the environmental conditions. Thermography allows imaging the temperature pattern, while {{a long history of}} thermal hygrometric parameters come available for the <b>fixed</b> <b>measurements</b> at selected points. The used equipment is relatively simple and competitive. A very few thermographic surveys are integrated by several periodic scanning, using an JR pyrometer. The visual monitoring is of great help in rendering results and documenting surface appearance at different seasons. About 20 probes are sufficient to measure thermal hygrometric parameters and evaluate the condensation risk. A review of algorithms for the moisture testing by thermography and guidelines for the identification of the moisture sources in thick historical buildings is presented. A numerical model has been adopted to simulate the internal thermo-hygrometrical conditions. In such a way, data acquired could be significantly extended and gaps occurred during the data logging has been filled. As case study was identified a North-East Italian ancient church (Duomo of Venzone) destroyed by an earthquake in 1976 and fully restored under the patrol of the cultural heritage authority. A one year round monitoring of the church and tour, according to the proposed procedure, allowed to control the moisture levels both in time and space. This endorsed to identify the sourèes of the water flux and therefore to suggest the repairing guidelines. Experimental results are reported...|$|E
40|$|The paper {{analyses}} {{the expected}} value of OD volumes from probe with fixed error, error that {{is proportional to}} zone size and inversely proportional to zone size. To add realism to the analysis, real trip ODs in the Tokyo Metropolitan Region are synthesised. The results show that for small zone coding with average radius of 1. 1 km, and <b>fixed</b> <b>measurement</b> error of 100 m, an accuracy of 70...|$|R
30|$|We have {{developed}} a trace-driven simulator to evaluate the improvement provided by SILVIO. Real traffic traces, collected at a <b>fixed</b> <b>measurement</b> point {{in the city of}} Madrid, are used to feed the simulator in which large vehicles are considered as obstacles, as well. Simulation results show that using SILVIO, the time elapsed under multi-hop wireless connectivity can increase up to 80 %, introducing a control overhead on the 3 G network absolutely affordable.|$|R
40|$|Field {{observations}} of new particle formation {{and the subsequent}} particle growth are typically only possible at a <b>fixed</b> <b>measurement</b> location, and hence do not follow the temporal evolution of an air parcel in a Lagrangian sense. Standard analysis for determining formation and growth rates requires that the time-dependent formation rate and growth rate of the particles are spatially invariant; air parcel advection means that the observed temporal evolution of the particle size distribution at a <b>fixed</b> <b>measurement</b> location may not represent the true evolution if there are spatial variations in the formation and growth rates. Here we present a zero-dimensional aerosol box model coupled with one-dimensional atmospheric flow to describe the impact of advection {{on the evolution of}} simulated new particle formation events. Wind speed, particle formation rates and growth rates are input parameters that can vary as a function of time and location, using wind speed to connect location to time. The output simulates <b>measurements</b> at a <b>fixed</b> location; formation and growth rates of the particle mode can then be calculated from the simulated observations at a stationary point for different scenarios and be compared with the ‘true’ input parameters. Hence, we can investigate how spatial variations in the formation and growth rates of new particles would appear in {{observations of}} particle number size distributions at a <b>fixed</b> <b>measurement</b> site. We show that the particle size distribution and growth rate at a fixed location is dependent on the formation and growth parameters upwind, even if local conditions do not vary. We also show that different input parameters used may result in very similar simulated measurements. Erroneous interpretation of observations in terms of particle formation and growth rates, and the time span and areal extent of new particle formation, is possible if the spatial effects are not accounted for...|$|R
40|$|The goal of {{this thesis}} was to examine {{possibilities}} to improve energy efficiency of data center cooling. Data centers form the ground for the telecommunication business. Energy consumption of the data centers is already globally significant because data centers consumed 1. 1 - 1. 5 % of world’s electricity use in 2010, and about 40 % of that is used for cooling. Enhancement of the data center cooling has a good potential to reduce energy consumption, greenhouse gases, and costs of telecommunication. This thesis {{is divided into two}} parts. The first part will offer a basic theoretical background for the data centers and their cooling systems as well as issues related to thermal management of data center. The second part consists of case study made for Finnish data centers locating part of the office building. Power usage effectiveness (PUE) were inspected and evaluated for the data centers of the building. The close liquid cooled system were compared to the traditionally air cooled system using measurements. Air flow problems at the data centers were discussed. The effects of free cooling and district cooling to the cooling system and energy consumption were discussed. PUE-value of the examined building varied between 1. 2 and 1. 8. Comprehensive, real-time and partial monitoring of PUE is demanding, but inserting a few <b>fixed</b> <b>measurements,</b> PUE-value can be defined. Using close liquid cooling, the power density of IT equipment can be higher than with traditional air cooling. Close liquid cooling had minor effect on the PUE-value. The traditional vapor compression cooling system is energy intensive, but it can be replaced, entirely or partly, by free cooling which utilizes coldness of outdoor air, water or ground. District cooling would decrease PUE-value and energy consumption of cooling, especially in summer, when air cooling based free cooling is not in use...|$|E
40|$|Reliable {{ambiguity}} resolution (AR) {{is essential}} to Real-Time Kinematic (RTK) positioning and its applications, since incorrect ambiguity fixing can lead to largely biased positioning solutions. A partial ambiguity fixing technique is developed to improve the reliability of AR, involving partial ambiguity decorrelation (PAD) and partial ambiguity resolution (PAR). Decorrelation transformation could substantially amplify the biases in the phase measurements. The purpose of PAD {{is to find the}} optimum trade-off between decorrelation and worst-case bias amplification. The concept of PAR refers to the case where only a subset of the ambiguities can be fixed correctly to their integers in the integer least-squares (ILS) estimation system at high success rates. As a result, RTK solutions can be derived from these integer-fixed phase measurements. This is meaningful provided that the number of reliably resolved phase measurements is sufficiently large for least-square estimation of RTK solutions as well. Considering the GPS constellation alone, partially <b>fixed</b> <b>measurements</b> are often insufficient for positioning. The AR reliability is usually characterised by the AR success rate. In this contribution an AR validation decision matrix is firstly introduced to understand the impact of success rate. Moreover the AR risk probability is included into a more complete evaluation of the AR reliability. We use 16 ambiguity variance-covariance matrices with different levels of success rate to analyse the relation between success rate and AR risk probability. Next, the paper examines during the PAD process, how a bias in one measurement is propagated and amplified onto many others, leading to more than one wrong integer and to affect the success probability. Furthermore, the paper proposes a partial ambiguity fixing procedure with a predefined success rate criterion and ratio-test in the ambiguity validation process. In this paper, the Galileo constellation data is tested with simulated observations. Numerical results from our experiment clearly demonstrate that only when the computed success rate is very high, the AR validation can provide decisions about the correctness of AR which are close to real world, with both low AR risk and false alarm probabilities. The results also indicate that the PAR procedure can automatically chose adequate number of ambiguities to fix at given high-success rate from the multiple constellations instead of fixing all the ambiguities. This is a benefit that multiple GNSS constellations can offer. ...|$|E
40|$|The EU {{directives}} on {{ambient air}} quality define a series of reference methods, which have {{to be used by}} the Member States to monitor {{ambient air quality}}. These methods have been standarised by CEN and their analytical uncertainties have been determined to be in line with the data quality objectives of the directives. The use of equivalence methods for reporting ambient air monitoring data to the 	 Commission should be guaranteed by a robust procedure, which is capable of evaluating the uncertainty associated with the candidate method under similar operational field conditions. 	 The grade of complexity of the procedure for demonstration of equivalence depends on how much the candidate method differs from the reference method. Such a variation ranges from a modification of the minor part of the standardised method to a method that could work with a completely different principle. The "Guideline on Demonstration of Equivalence" deals with these matters, first by defining 	 equivalence as " [...] . a method meeting the Data Quality Objectives for continuous or <b>fixed</b> <b>measurements</b> specified in the relevant air quality directive" and, secondly, by establishing a procedure for demonstration of equivalence based on specific programmes for laboratory and field tests with the ultimate goal of determining the uncertainty of the candidate method in comparison to the reference method. 	 This publication collates the proceedings of the Workshop on Demonstration of Equivalence between Ambient Air Monitoring Methods, which was held at the JRC in Ispra (VA) from 2 - 4 May 2007 and the major discussions vis-à-vis the last version of the "Guideline on Demonstration of Equivalence" (included as an annex). 	 During the workshop in Ispra, a series of relevant items were discussed including the same approaching principle, practical discussions on experimental design, QAQC requirements and other statistical approaches. All this information has been collected in the current proceedings and the different presentations slides {{can be found on the}} following website of the Joint Research Centre: 	 [URL] should be understood that the articles included in these proceedings do not represent any official position of the European Commission, rather the point of view of the corresponding authors. Theses proceedings provide an extensive discussion on the current limitations of the guideline on Demonstration of Equivalence and, consequently, give some space for possible improvements. The Guideline is reinforced through extensive application and use by the reference laboratories from Member States. JRC. H. 4 -Transport and air qualit...|$|E
40|$|The {{measurement}} of environmental data in city areas {{has become an}} important issue to municipalities due to several national, European and even international climate directives. However, <b>fixed</b> <b>measurement</b> stations are inflexible, cost-intensive and limited to monitoring environmental data in distinct key areas only. Large-scale data collection with a high spatial resolution requires mobile measurements, which are an active area of research but still offer many challenges. In this paper we give an overview of current approaches and challenges...|$|R
40|$|What's new? Minor bug-fix release: Fix runtime {{error in}} bsearch_py() when no burst are present (d 9 efddcd 5 d 51843 e 3 f 9 bab 1 e 7 cec 3 aa 61 af 3097 d). Fix runtime error in select_bursts. time() (953 e 96 f 70 f 4 b 078210 d 234 b 4 d 112234 fb 3 fbfc 9 c). <b>Fix</b> <b>measurement</b> slicing of us-ALEX data with particles. Docs cleanups and updates. Added tests for most {{selection}} functions. Plots: updates for matplotlib 1. 5, cleanups and fixes to the timetrace scrolling GU...|$|R
40|$|We {{consider}} {{the problem of}} correcting the errors incurred from sending quantum information through a noisy quantum environment by using classical information obtained from a measurement on the environment. For discrete time Markovian evolutions, {{in the case of}} <b>fixed</b> <b>measurement</b> on the environment, we give criteria for quantum information to be perfectly corrigible and characterize the related feedback. Then we analyze the case when perfect correction is not possible and, in the qubit case, we find optimal feedback maximizing the channel fidelity. Comment: 11 pages, 1 figure, revtex...|$|R
40|$|The {{purpose of}} this paper is to extend a result by Donoho and Huo, Elad and Bruckstein, Gribnoval and Nielsen on sparse {{representations}} of signals in dictionaries to general matrices. We consider a general <b>fixed</b> <b>measurement</b> matrix, not necessarily a dictionary, and derive sufficient condition for having unique sparse representation of signals in this matrix. Currently, to the best of our knowledge, no such method exists. In particular, if matrix is a dictionary, our method is at least as good as the method proposed by Gribnoval and Nielsen...|$|R
40|$|We derive {{stochastic}} representations for {{the finite}} dimensional distributions of a multidimensional diffusion {{on a fixed}} time interval,conditioned on the terminal state. The conditioning can be {{with respect to a}} <b>fixed</b> <b>measurement</b> point or more generally with respect to some subset. The representations rely on a reverse process connected with the given (forward) diffusion as introduced by Milstein, Schoenmakers and Spokoiny in the context of density estimation. The corresponding Monte Carlo estimators have essentially root-N accuracy, and hence they do not suffer from the curse of dimensionality. We also present an application in statistics, {{in the context of the}} EM algorithm...|$|R
40|$|<b>Fixing</b> <b>measurement</b> {{errors in}} the Consumer Price Index is a small idea that offers big payoffs to Canadians and the government. In this paper, the author says if the {{upcoming}} federal budget devoted the resources needed to improve Statistics Canada’s measurement of the Consumer Price Index, Canadians would have a truer sense {{of changes in the}} cost of living, monetary policy would be guided by a more accurate measure of inflation, and Minister Flaherty would more easily achieve the government’s commitment to balance the federal budget by 2015 / 16. Monetary Policy, Consumer Price Index (CPI), Statistics Canada, inflation rate...|$|R
5000|$|... #Caption: King Manuel I, who <b>fixed</b> the country's <b>measurement</b> standards, in 1495.|$|R
40|$|Recent {{experiments}} have reached detection efficiencies sufficient {{to close the}} detection loophole with photons. Both experiments ran multiple successive trials in <b>fixed</b> <b>measurement</b> configurations, rather than randomly re-setting the measurement configurations before each measurement trial. This opens a new potential loophole for a local hidden variable theory. The loophole invalidates one proposed method of statistical analysis of the experimental results, as demonstrated in this note. Therefore a different analysis {{will be necessary to}} definitively assert that these experiments are subject only to the locality loophole. Comment: 5 pages. The single change in this version is the addition of a footnote to the abstract, which highlights some recent work on the proble...|$|R
40|$|Abstract: European egalitarianism is {{confirmed}} by a propensity to agree with government intervention to reduce income inequality. This propensity is driven by lower educational level, societal dissatisfaction, liberal partisanship, and economic anxiety. These findings are realized with data from the fifth round of the European Social Survey. The regression of agreement propensities on true explanatory values is made possible by correcting for measurement error in explanatory scores. This resolves two major problems in propensity regression, i. e. errors in variables and imputation errors. These resolutions are attained by a pure randomization theory that places <b>fixed</b> <b>measurement</b> error in design-based regression. This type of regression (versus model-based regression) is used by statistical agencies and polling organizations for sampling large populations...|$|R
40|$|The Holevo bound {{provides}} {{a limit to}} the amount of information which can be obtained by an observer for a fixed encoding. Here we point out that there exists a complementary bound, very similar in form, which {{provides a}} limit {{to the amount of}} information that can be obtained by an observer for a <b>fixed</b> <b>measurement.</b> Both of these bounds are corollaries of the Schumacher-Westmoreland-Wootters theorem [Phys. Rev. Lett. 76, 3452 (1996) ]. We discuss this theorem and another bound involving the effect of the incompleteness of a measurement on information gathering. We also show that, given the Holevo bound, the measurement bound may be proved in a special case by exploiting the partial duality between measurements and encodings...|$|R
40|$|Since {{entanglement}} {{is not an}} observable per se, measuring {{its value}} in practice is a difficult task. Here we propose a protocol for quantifying a particular entanglement measure, namely concurrence, of an arbitrary two-qubit pure state via a single <b>fixed</b> <b>measurement</b> set-up by exploiting so-called weak measurements and the associated weak values together with {{the properties of the}} Laguerre-Gaussian modes. The virtue of our technique is that it is generally applicable for all two-qubit systems and does not involve simultaneous copies of the entangled state. We also propose an explicit optical implementation of the protocol. Comment: 7 pages, 2 figures. Final ver 3 : An appendix explaining the robustness of our method was addde...|$|R
40|$|Summary: A fast spin-echo inversion-recovery (FSE-IR) {{sequence}} is described for its utility regarding surgical planning {{for patients with}} Parkinson’s disease (PD) who are undergoing microelectrode-guided internal globus pallidus (GPi) ablation. Images from thirty-seven adult patients with PD were reviewed and visualization of the GPi, globus pallidus externa (GPe), and the intervening lamina was noted. High-resolution images were acquired from all patients despite the external hardware and the patients’ movement disorder. In all cases, the conventional surgical trajectory, determined indirectly by a <b>fixed</b> <b>measurement</b> from the anteroposterior commissure line, was modified by the ability to visualize the GPi and optic tract directly. This sequence facilitated accurate stereotactic targeting. Surgical ablation of the internal globus pallidu...|$|R
40|$|INTRODUCTION: Inspiratory {{capacity}} (IC) assessments {{have been}} performed mainly in laboratory settings, because of <b>fixed</b> <b>measurement</b> devices. Oxycon Mobile (OM) is the mobile and wireless version of Oxycon Pro (OP), a commonly used <b>fixed</b> <b>measurement</b> device. The {{purpose of this study}} was to examine IC agreement between OM and OP at rest and during steady-state exercise. Also, the within- and between-days variability of IC's were determined. METHODS: Thirty-five healthy subjects were recruited. Twenty-five subjects were included for determining validity of the OM and ten subjects for the variability study. For validation of OM, resting and exercise IC's (IC(rest) and IC(exercise) respectively) were measured consecutively by OM and OP, in random order. Exercise consisted of cycle ergometry at 50 % of subject's predicted maximal exercise capacity. RESULTS: The mean difference between OM and OP regarding IC(rest) was - 0. 05 L, with limits of agreement of - 0. 47 to 0. 37 L (or - 1. 2 % with limits of agreement of - 11. 6 to 9. 3 %) (P> 0. 05). The mean difference of IC(exercise) was - 0. 06 L, and the limits of agreement were - 0. 48 to 0. 35 L (or - 1. 4 % with limits of agreement of - 11. 8 to 9. 0 %) (P> 0. 05). No significant differences in IC's within- or between-days were found. DISCUSSION: The limits of agreement of the IC measured by OM and OP were +/- 10 %, which is recommended for interdevice reproducibility. We conclude that OM and OP can be used interchangeably for measuring IC at rest and during steady-state exercise...|$|R
40|$|This paper reports an {{adaptation}} of the GM-PHD filter for ground moving target tracking. In particular, a technique for modeling the measurement covariance matrix adaptively based on the signal power of each detection is investigated. This technique is compared to the standard method of using a fixed covariance matrix both by simulation and experimental data. Notably, {{the analysis of the}} error distribution within the experimental data set shows the improvement of the measurement model due to the introduced technique. Simulation and results with experimental data also show that tracking with adaptive measurement covariance matrices yields results that are at least equal to tracking with the best choice of a <b>fixed</b> <b>measurement</b> covariance matrix. Finally, figures with tracks of ground truth targets and targets of opportunity are presented...|$|R
40|$|We {{consider}} a subclass of bipartite CHSH-type Bell inequalities. We investigate operations, which leave their Tsirelson bound invariant, but change their classical bound. The optimal observables are unaffected {{except for a}} relative rotation of the two laboratories. We illustrate the utility of these operations by giving explicit examples: We prove that for a fixed quantum state and <b>fixed</b> <b>measurement</b> setup except for a relative rotation of the two laboratories, there is a Bell inequality that is maximally violated for this rotation, and we optimise some Bell inequalities {{with respect to the}} maximal violation. Finally we optimise the qutrit to qubit ratio of some dimension witnessing Bell inequalities. Comment: 12 pages, 2 figures; contribution to special issue of JPA: 50 years of Bell's theore...|$|R
50|$|Measurements most {{commonly}} use the International System of Units (SI) as a comparison framework. The system defines seven fundamental units: kilogram, metre, candela, second, ampere, kelvin, and mole. Six of these units are defined {{without reference to}} a particular physical object {{which serves as a}} standard (artifact-free), while the kilogram is still embodied in an artifact which rests at the headquarters of the International Bureau of Weights and Measures in Sèvres near Paris. Artifact-free definitions <b>fix</b> <b>measurements</b> at an exact value related to a physical constant or other invariable phenomena in nature, in contrast to standard artifacts which are subject to deterioration or destruction. Instead, the measurement unit can only ever change through increased accuracy in determining the value of the constant it is tied to.|$|R
40|$|This paper proposes an {{adaptive}} block-based compressed sensing (ABCS) technique {{to build a}} new progressive image coding scheme, in which both image acquisition and reconstruction are carried out in two layers. At the base layer, an original image is sampled and restored by the block-based compressed sensing (BCS) method with a low and <b>fixed</b> <b>measurement</b> rate. Second, all blocks in the enhancement layer are re-sampled with different rates according to a block classification. The final reconstruction of a block at the enhancement layer is performed in multiple stages where each stage only knows a part of sampled coefficients. We present some experimental results to show that our proposed ABCS method outperforms the BCS method; in particular, it produces a better visual quality in regions that contain edges, patterns, and textures...|$|R
40|$|This thesis {{presents}} estimation {{and control}} algorithms used to coordinate a multi-vehicle testbed. A total sensor package using the Global Positioning System {{as the primary}} sensor with secondary inertial sensors for when GPS is not available and a single point position <b>fix</b> <b>measurement</b> is designed. A new method of integrated velocity estimation is presented using only the Doppler measurements provided by the GPS NAVSTAR constellation. This shows significant improvement on previously used velocity integration methods by elminating unmodeled sensor biases. Furthermore, the algorithm includes the ability to calibrate inertial sensor biases in real-time, which are then used when observability to the constellation is blocked. The single point position fix is able to determine the initial position of the estimation or reset position estimation error drift. Methods are also presented for doing the low-level velocit...|$|R
40|$|Since the GEOS- 3 and SEASAT- 1 radar {{altimeters}} measured altitude over {{a narrow}} swath along the satellite subtrack, ocean current and mesoscale feature maps {{could only be}} generated after {{a large number of}} satellite revolutions. The present paper analyzes a new multibeam altimeter technique that has the potential to cover wide swaths. The multibeam altimeter uses two antenna elements, simple parabolic dishes with offset feeds, deployed cross-track on singly hinged booms into a <b>fixed</b> <b>measurement</b> geometry to generate an interferometer pattern over the desired swath extent. Range gating allows the isolation of a single interferometeric lobe, and the desired altitude measurements are extracted by using a modified altitude tracker design. Implementing this sensor on future altimetry missions would allow the timely generation of ocean current and mesoscale feature maps for the first time...|$|R
40|$|Monitoring of {{acoustical}} data {{is often}} done at <b>fixed</b> <b>measurement</b> stations {{to verify the}} compliance with environmental noise or work safety regulations. In this work, a different method is explored for monitoring acoustical conditions in indoor environments {{with the assistance of}} location information. Such a method exploits a wideband localization system to determine the listener position with high accuracy, which leads, together with prior knowledge of the sound field, to assess the behaviour of the local acoustical data in the indoor environment. An application of this method is developed, considering as case study a long partitioned virtual room, where an ultra- wideband (UWB) localization system is used and the subject is moving along a trajectory characterized by abrupt acoustical changes. First, the behaviour of various acoustical metrics is assessed along the trajectory. Then, the perspectives of this monitoring method are outlined and discussed...|$|R
30|$|We {{report the}} strain effect of {{suspended}} graphene prepared by micromechanical method. Under a <b>fixed</b> <b>measurement</b> orientation of scattered light, {{the position of}} the 2 D peaks changes with incident polarization directions. This phenomenon is explained by a proposed mode in which the peak is effectively contributed by an unstrained and two uniaxial-strained sub-areas. The two axes are tensile strain. Compared to the unstrained sub-mode frequency of 2, 672 cm− 1, the tension causes a red shift. The 2 D peak variation originates in that the three effective sub-modes correlate with the light polarization through different relations. We develop a method to quantitatively analyze the positions, intensities, and polarization dependences of the three sub-peaks. The analysis reflects the local strain, which changes with detected area of the graphene film. The measurement can be extended to detect the strain distribution of the film and, thus, is a promising technology on graphene characterization.|$|R
