10|10000|Public
5000|$|Sometimes a [...] "dummy task" [...] is added, to {{represent}} a <b>dependency</b> <b>between</b> <b>tasks,</b> which does not represent any actual activity. The dummy task is added to indicate precedence that can't be expressed using only the actual activities. Such a dummy task often has a completion time of 0.|$|E
40|$|The present paper aims at {{presenting}} some of {{the real}} problems that the Constructions Industry in Romania faces, along with a method that can improve {{the performance of the}} projects facing the already named problems. Thus, the Critical Chain Project Management permits higher gains without significant investment. Based on the study of the relationship system of precedence among tasks, and also on the <b>dependency</b> <b>between</b> <b>tasks</b> and resources, the Critical Chain method changes {{the way in which we}} must look at project management, taking into account the fact that the resources are never available in unlimited quantities. Aiming at a moderate loading of all resources involved in the project, this theory manages to keep the project on schedule. management; project; flexibility; critical chain; planning. ...|$|E
40|$|This paper {{considers}} {{the relation between}} item recognition and cued recall [...] two standard measures of episodic memory. Going beyond measures of performance on each task, we examine {{the degree to which}} correlations between successful recognition and successful recall of a single studied episode reflect the commonality of memory processes underlying the two tasks. Specifically, we consider whether four computational memory models (local and global match versions of both matrix and convolution-correlation models) can account for the relatively invariant correlation (# 0. 5) between successive recognition and recall tests. Whereas basic versions of each model cannot account for the correlation, versions that take into account variability in goodness-of-encoding and in response criteria, as well as output encoding, are able to account for the level of <b>dependency</b> <b>between</b> <b>tasks.</b> These elaborated models also succeeded in fitting data from two new experiments that manipulated the level of variability in goodness-of-encoding across conditions. This model...|$|E
50|$|The {{tasks are}} also prioritized, <b>dependencies</b> <b>between</b> <b>tasks</b> are identified, and this {{information}} is documented in a project schedule. The <b>dependencies</b> <b>between</b> the <b>tasks</b> can affect {{the length of the}} overall project (dependency constrained), as can the availability of resources (resource constrained). Time is different from all other resources and cost categories.|$|R
50|$|Projects module is {{developed}} for managing project stages: planning, team management and task delegation, monitoring and reporting. Module also includes Gantt Chart for illustrating the projects stages and <b>dependencies</b> <b>between</b> <b>tasks.</b>|$|R
5000|$|... psake executes the Task {{functions}} and enforces the <b>dependencies</b> <b>between</b> <b>tasks.</b> Since psake {{is written in}} a real programming language and not XML, {{you have a lot}} more freedom and flexibility in your build scripts. You can use all the features of PowerShell and the [...]NET Framework within your build.|$|R
40|$|International audienceMinimally Invasive Surgery (MIS) has {{definitively}} {{changed the}} procedures performed in operating rooms. In many cases, MIS {{has become the}} recommended standard technique, replacing the traditional open surgery. Effective training and objective assessment of surgeons in these techniques become a major concern in hospitals in recent years, encouraged primarily by patients and a society that demands safer surgical procedures, which is associated with better surgeons training. In the framework of surgery, the difficulty of defining objective metrics for performance evaluation lies in the strict <b>dependency</b> <b>between</b> <b>tasks</b> {{and the difficulty of}} defining the meaning of optimal performance, related to the characterization of gestures made by the experts. An objective method to compare 3 D gestures between an expert and novice surgeons through multidimensional data analysis is proposed in this paper. A survey of different algorithms for surgical gestures analysis in time domain is carried out. These ones include the Multi-dimensional Dynamic Time Warping (MD-DTW) and Multi-Dimensional Derivative Dynamic Time Warping (MD-DDTW). Simulation and experimental results are given with this different techniques...|$|E
40|$|Dynamic {{scheduling}} techniques, and EDF (Earliest Deadline First) in particular, {{have demonstrated}} {{their ability to}} increase the schedulability of real time systems compared to fixed-priority scheduling. In distributed systems, the scheduling policies of the processing nodes {{tend to be the}} same as in stand-alone systems and, although few EDF networks exist, it is foreseen that dynamic scheduling will gradually develop into real-time networks. There are some response time analysis techniques for EDF scheduled distributed systems, mostly derived from the holistic analysis developed by Spuri. The convergence of the holistic analysis in context of EDF distributed systems with shared resources had not been studied until now. There is a circular <b>dependency</b> <b>between</b> <b>tasks</b> ’ release jitter values, response times and preemption level ceilings of shared resources. In this paper we present an extension of Spuri’s algorithm and we demonstrate that its iterative formulas are non-decreasing, even in the presence of shared resources. This result enables us to assert that the new algorithm converges towards a solution for the response times of the tasks and messages in a distributed system...|$|E
40|$|High-performance {{reconfigurable}} computing involves {{acceleration of}} significant portions of an ap-plication using reconfigurable hardware. When the hardware tasks of an application cannot si-multaneously fit in an FPGA, the task graph {{needs to be}} partitioned and scheduled into multiple FPGA configurations, {{in a way that}} minimizes the total execution time. This article proposes the Reduced Data Movement Scheduling (RDMS) algorithm that aims to improve the overall per-formance of hardware tasks by taking into account the reconfiguration time, data <b>dependency</b> <b>between</b> <b>tasks,</b> intertask communication as well as task resource utilization. The proposed algo-rithm uses the dynamic programming method. A mathematical analysis of the algorithm shows that the execution time would at most exceed the optimal solution by a factor of around 1. 6, in the worst-case. Simulations on randomly generated task graphs indicate that RDMS algorithm can reduce interconfiguration communication time by 11 % and 44 % respectively, compared with two other approaches that consider data dependency and hardware resource utilization only. The prac-ticality, as well as efficiency of the proposed algorithm over other approaches, is demonstrated by simulating a task graph from a real-life application- N-body simulation- along with constraints for bandwidth and FPGA parameters from existing high-performance reconfigurable computers...|$|E
40|$|<b>Dependencies</b> <b>between</b> <b>tasks</b> in {{clinical}} processes are often complex and error-prone. Our {{aim is to}} describe a new approach for the automatic derivation of clinical events identified via the behaviour of IT systems using Complex Event Processing. Furthermore we map these events on transition systems to monitor crucial clinical processes in real-time for preventing and detecting erroneous situations...|$|R
25|$|Generic Task Type: This layer {{identifies}} the constituent tasks of generic, repetitive processes and the logical <b>dependencies</b> <b>between</b> these <b>tasks.</b>|$|R
40|$|In many task-planning domains, {{assemblies}} {{of autonomous}} agents need to construct plans and schedules for executing their assigned sets of tasks {{in order to}} complete them. Often, there will be <b>dependencies</b> <b>between</b> <b>tasks</b> to be executed by different agents. In such a situation, a plan-coordination problem arises when the joint plan is required to be feasible, whatever locally-feasible plans the individual agents come up with. This problem can be solved by plan decoupling, which is to add a minimum number of constraints such that each agent can make a plan for its set of tasks independently of the others while still joint-plan feasibility is guaranteed. Previous work on plan de-coupling concentrated on a coordination framework where the only <b>dependencies</b> <b>between</b> <b>tasks</b> are precedence constraints. In this paper {{an extension of the}} framework is discussed where not only precedence constraints, but also synchronisation constraints can be used in order to express qualitative temporal constraints <b>between</b> <b>tasks.</b> It is shown that adding synchronisation constraints does not add any complexity to the plan-decoupling problem, and that a previously-developed ap-proximation algorithm for plan decoupling can be extended to cope with synchronisation constraints as well. 1...|$|R
40|$|AbstractToday central {{topic in}} science and {{engineering}} is parallel and distributed computing, research performing in the area of development of new approaches for the modeling, design, analysis, evaluation, and programming of future parallel and distributed computing systems and its applications. To detect the presence of <b>dependency</b> <b>between</b> <b>tasks,</b> analytic methodology is required. This review theoretically analyzes what is dependency, different types of dependencies, various applications which require dependency analysis and different approaches towards dependency analysis. This research classifies dependency analysis approaches on the basis of input source; for different input source different methodology is available, this review considers an input source as a component base system with dependent components. This research gives review of dependency analysis solutions for various areas like parallel processing, high performance computing, security and vulnerability in software's where various approaches are used for dependency analysis. This review concludes that there are various methods available for dependency analysis. Most of the methods use graph base approach, conceptual graph approach and matrix based approach, out of which this review puts forward the best possible approach. This research proposes a new approach for dependency analysis which results in the reduced dependent analysis time, low memory consumption and less cost sensitive...|$|E
40|$|International audienceFactorizing sparse {{matrices}} using direct multi-frontal methods generates directed tree-shaped task graphs, where edges represent data <b>dependency</b> <b>between</b> <b>tasks.</b> This paper revisits {{the execution}} of tree-shaped task graphs using multiple processors that share a bounded memory. A task can only be executed if all its input and output data can fit into the memory. The key difficulty is to manage {{the order of the}} task executions so that we can achieve high parallelism while staying below the memory bound. In particular, because input data of unprocessed tasks must be kept in memory, a bad scheduling strategy might compromise the termination of the algorithm. In the single processor case, solutions that are guaranteed to be below a memory bound are known. The multi-processor case (when one tries to minimize the total completion time) {{has been shown to be}} NP-complete. We present in this paper a novel heuristic solution that has a low complexity and is guaranteed to complete the tree within a given memory bound. We compare our algorithm to state of the art strategies, and observe that on both actual execution trees and synthetic trees, we always perform better than these solutions, with average speedups between 1. 25 and 1. 45 on actual assembly trees. Moreover, we show that the overhead of our algorithm is negligible even on deep trees, and would allow its runtime execution...|$|E
40|$|This thesis {{describes}} {{a project that}} applies the Design Structure Matrix (DSM) {{in support of the}} Manufacturing Excellence (MX) program at Cisco Systems, Inc to reduce the cycle time of new product development initiatives (NPI). Because they are inherently iterative with interdependent tasks, NPIs are difficult to manage. Two case studies applying the DSM were performed and used to study the inputs and outputs of the process as well as the dependencies between the process steps. Both case studies indicated that defining product requirements and needs upfront helped to eliminate rework later on in the process. The DSMs also showed that cycle time and standard deviation of cycle time were especially sensitive to interactions between changes in the Bill of Materials (BOM) and other tasks. In fact there was a "tipping point" where reducing the <b>dependency</b> <b>between</b> <b>tasks</b> could yield significant reductions in cycle time and standard deviation of cycle time. More significantly, the case studies highlighted the large number of stakeholders involved in the process and revealed the degree to which engineering and manufacturing must work together to reduce NPI cycle times. (cont.) In fact, the name "Manufacturing Excellence Initiative in NPI" is a misnomer. New Product Introduction is not just the job of manufacturing but is highly integrated between such groups as marketing, design, and engineering. If the Mx Initiative in NPI is to fully meet its potential, all of these groups must fully realize this. In addition, {{there is a need for}} process infrastructure, data infrastructure, and close examination of incentives. This thesis thus shows that in order for Cisco's process improvement initiatives to succeed, buy-in from all relevant stakeholders must be won. by Julie W. Go. Thesis (M. B. A.) [...] Massachusetts Institute of Technology, Sloan School of Management; and, (S. M.) [...] Massachusetts Institute of Technology, Engineering Systems Division; in conjunction with the Leaders for Manufacturing Program at MIT, 2007. Includes bibliographical references (p. 63) ...|$|E
30|$|We can notice {{also that}} our {{scheduler}} supports the dynamic creation and deletion of tasks. These online services are only possible when keeping a fixed {{structure of the}} DFG along the execution. In that case the <b>dependencies</b> <b>between</b> <b>tasks</b> are known a priori. Dynamic deletion is then possible by assigning a null execution time to the tasks which are not active. and dynamic creation by assigning their execution time when they become active.|$|R
40|$|Abstract — A {{schedule}} is said robust {{if it is}} able to absorb some degree of uncertainty in tasks duration while maintaining a stable solution. This intuitive notion of robustness has led to a lot of different interpretations and metrics. However, no comparison of these different metrics have ever been preformed. In this paper, we perform an experimental study of these different metrics and show how they are correlated {{to each other in the}} case of <b>task</b> scheduling, with <b>dependencies</b> <b>between</b> <b>tasks.</b> I...|$|R
40|$|AbstractThe {{current work}} {{introduces}} {{a model that}} integrates organizational constraints and policies when converting software specifications into a Gantt chart of the development process planned for that software specification. Typically, a system analyst and a software development project manager interact through software specifications, which are usually involved using a CASE tool in modeling the system. The project manager re-analyzes the model, extracts {{a work breakdown structure}} (WBS), and edits it to obtain a new Gantt chart incorporating resource allocations, cost considerations concerning outsourcing, matching each task to an appropriate qualification, setting priorities for each task, assigning safety buffers for each resource or <b>task,</b> setting <b>dependencies</b> <b>between</b> <b>tasks,</b> considering task risk assessments, identifying tasks that can be broken down into parallel subtasks, and determining organizational constraints that have to be taken into consideration...|$|R
40|$|A Problem Solving Agent (PSA) has {{the ability}} to execute a finite set of tasks in an {{application}} domain. The Capability-based activity specification and decomposition (CapBasED), composed of an execution and monitoring module, is presented. Each of the system's PSA has its competence defined by a set of capabilities to execute tasks, with each task requiring a certain competence from the PSAs for its execution. The <b>dependencies</b> <b>between</b> <b>tasks</b> can either be of data or control dependency, temporal dependency, or external dependency. These dependents are expressed by events (system, external, temporal). The novel features of the CapBasED are discussed...|$|R
50|$|Each of the {{workflow}} models has <b>tasks</b> (nodes) and <b>dependencies</b> <b>between</b> the nodes. <b>Tasks</b> are activated {{when the}} dependency conditions are fulfilled.|$|R
40|$|It {{remains a}} {{challenging}} problem to tightly estimate {{the worst case}} response time of an application in a distributed embedded system, especially when there are <b>dependencies</b> <b>between</b> <b>tasks.</b> We discovered that the state-of-the art techniques considering task dependencies either fail to obtain a conservative bound or produce a loose upper bound. We propose a novel conservative performance analysis, called hybrid performance analysis, combining the response time analysis technique and the scheduling time bound analysis technique to compute a tighter bound fast. Through extensive experiments with randomly generated graphs, superior performance of our proposed approach compared with previous methods is confirmed. Comment: 29 pages (25 page paper, 4 page appendix), 16 figure...|$|R
40|$|We {{present a}} {{methodology}} to visually model intensive signal processing applications for embedded systems. This methodology {{is based on}} the Array-OL language. The idea is to represent an application as a graph of <b>dependencies</b> <b>between</b> <b>tasks</b> and arrays. It differs from the classical reactive programming or message passing paradigm. A task may iterate the same code on different patterns tilling its depending arrays. In this case, visual specifications of <b>dependencies</b> <b>between</b> the pattern elements are enough to define an application. The visual notation we propose uses the UML standard. This allows usage of existing UML tools to model an application. More, the application's model can be exported and saved in a standardized XMI format. The resulting application's model can then be imported by others tools performing automatic exploitation, like validation, transformation or code generation...|$|R
40|$|International audienceLearning {{multiple}} related tasks {{from data}} simultaneously can improve predictive performance relative to learning these tasks independently. In this {{paper we propose}} a novel multi-task learning algorithm called MT-Adaboost: it extends Adaboost algorithm to the multi-task setting; it uses as multi-task weak classifier a multi-task decision stump. This allows to learn different <b>dependencies</b> <b>between</b> <b>tasks</b> for different regions of the learning space. Thus, we relax the conventional hypothesis that tasks behave similarly in the whole learning space. Moreover, MT-Adaboost can learn multiple tasks without imposing the constraint of sharing the same label set and/or examples <b>between</b> <b>tasks.</b> A theoretical analysis is derived from the analysis of the original Adaboost. Experiments for multiple tasks over large scale textual data sets with social context (Enron and Tobacco) give rise to very promising results...|$|R
40|$|International audienceTo {{efficiently}} exploit {{high performance}} computing platforms, applications currently have to express {{more and more}} finer-grain parallelism. The OpenMP standard allows programmers to do so since version 3. 0 {{and the introduction of}} task parallelism. Even if this evolution stands as a necessary step towards scalability over shared memory machines holding hundreds of cores, the current specification of OpenMP lacks ways of expressing <b>dependencies</b> <b>between</b> <b>tasks,</b> forcing programmers to make unnecessary use of synchronization degrading overall performance. This paper introduces libKOMP, an OpenMP runtime system based on the X-Kaapi library that outperforms popular OpenMP implementations on current task-based OpenMP benchmarks, but also provides OpenMP programmers with new ways of expressing data-flow parallelism...|$|R
40|$|AbstractCOMPSs is a {{programming}} {{framework that}} aims to facilitate the parallelization of existing applications written in Java, C/C++ and Python scripts. For that purpose, it offers a simple programming model based on sequential development in which the user is mainly responsible for (i)  identifying the functions to be executed as asynchronous parallel tasks and (ii) annotating them with annotations or standard Python decorators. A runtime system {{is in charge of}} exploiting the inherent concurrency of the code, automatically detecting and enforcing the data <b>dependencies</b> <b>between</b> <b>tasks</b> and spawning these tasks to the available resources, which can be nodes in a cluster, clouds or grids. In cloud environments, COMPSs provides scalability and elasticity features allowing the dynamic provision of resources...|$|R
40|$|In {{conjunction}} with [Cluster 2007] The 2007 IEEE International Conference on Cluster Computing. The workshop proceedings {{will be published}} through the IEEE Computer Society Press {{as part of the}} Cluster 2007 proceedings. International audienceA schedule is said robust if it is able to absorb some degree of uncertainty in tasks duration while maintaining a stable solution. This intuitive notion of robustness has led to a lot of different interpretations and metrics. However, no comparison of these different metrics have ever been preformed. In this paper, we perform an experimental study of these different metrics and show how they are correlated {{to each other in the}} case of <b>task</b> scheduling, with <b>dependencies</b> <b>between</b> <b>tasks...</b>|$|R
40|$|Thesis (Master's) [...] University of Washington, 2015 The {{increasing}} need {{of large-scale}} data centers has brought new {{challenges to the}} development of energy-efficiency techniques. Although many optimization techniques have been proposed, most of them neglect the characteristics of tasks and fail to consider the <b>dependencies</b> <b>between</b> <b>tasks.</b> Therefore, more comprehensive optimization is needed. The goal of this thesis is to combine capacity provisioning with scientific workflow scheduling to generate optimization in these two ways. Capacity provisioning is built upon a fuzzy logical mechanism to accomplish constructing computing resources proactively, and workflow scheduling is designed with the purpose of achieving energy efficiency. We aim at achieving optimization both in energy consumption and completion time...|$|R
40|$|This paper {{presents}} spade, {{an integrated}} partitioning and register transfer level (rtl) design space exploration system for multi-fpga reconfigurable architectures. The input to spade is an acyclic task graph, TG = (T; E), where T is {{the set of}} tasks and E is the set of edges. Tasks represent behavioral or algorithmic code segments. Edges denote data <b>dependencies</b> <b>between</b> <b>tasks.</b> Each task is pre-synthesized to obtain a set of equivalent rtl implementations that correspond to various area-time trade-off points in the design space of the task. spade partitions the tasks across fpgas and also selects an rtl implementation for each task such that the throughput of the task graph is maximized without violating the architectural constraints posed by the multi-fpga board. spade consists of an iterative partitioning engine, an architectural constraint evaluator, and a throughput optimization heuristic. The paper provides detailed description of all components in spade. We propose a novel geneti [...] ...|$|R
40|$|Rate Monotonic Analysis (RMA) is a {{well-established}} technique for assessing schedulability of periodic and sporadic tasks which share a processor resource using fixed priority scheduling. An alternative approach to analysing such systems {{is to build}} a model which represents the behaviour of the system more dynamically, taking into account the <b>dependency</b> <b>between</b> the <b>tasks...</b>|$|R
40|$|The {{present study}} {{examined}} whether the central bottleneck, {{assumed to be}} primarily responsible for the psychological refractory period (PRP) effect, is intact, bypassed, or shifted in locus with ideomotor (IM) -compatible tasks. In 4 experiments, factorial combinations of IM- and non-IM-compatible tasks were used for Task 1 and Task 2. All experiments showed substantial PRP effects, with a strong <b>dependency</b> <b>between</b> <b>Task</b> 1 and Task 2 response times. These findings, along with model-based simulations, indicate that the processing bottleneck was not bypassed, even with two IM-compatible tasks. Nevertheless, systematic changes in the PRP and correspondence effects across experiments suggest that IM compatibility shifted the locus of the bottleneck. The findings favor an engage-bottleneck-later hypothesis, whereby parallelism <b>between</b> <b>tasks</b> occurs deeper into the processing stream for IM- than for non-IM-compatible tasks, without the bottleneck being actually eliminated...|$|R
40|$|International audienceReal-time {{systems with}} {{functional}} <b>dependencies</b> <b>between</b> <b>tasks</b> often require end-to-end (as opposed to task-level) guarantees. For {{many of these}} systems, it is even possible to accept the possibility of longer end-to-end delays if one can bound their frequency. Such systems are called weakly-hard. In this paper we provide end-to-end deadline miss models for systems with task chains using Typical Worst-Case Analysis (TWCA). This bounds the number of potential deadline misses in a given sequence of activations of a task chain. To achieve this we exploit task chain properties which arise from the priority assignment of tasks in static-priority preemptive systems. This work is motivated by and validated on a realistic case study inspired by industrial practice and derived synthetic test cases...|$|R
40|$|We {{have started}} the {{extension}} of the electromagnetic software CAPITOLE developed at ENTARES Engineering to HPC machines. This has been possible with the aid of the SHAPE pilot project, which has provided us with some expertise and computation hours at Marenostrum III. Two numerical methods have been addressed to solve the resulting dense MoM linear system, MSCBD and MLACA. A new implementation based on asynchronous tasks has been performed for the direct method MSCBD. <b>Dependencies</b> <b>between</b> <b>tasks</b> need to be well defined before moving to a runtime scheduling such as STARPU. As for the iterative method MLACA, a hybrid MPI-OpenMP parallelization has been done with excellent results. So far, electromagnetic models up to 6 Million unknowns have been properly solved. ...|$|R
40|$|Separation of {{duty and}} binding {{of duty in}} {{workflow}} systems is an important area of current research in computer security. We introduce a formal model for constrained workflow systems that incorporate constraints for implementing such policies. We define an entailment constraint, which is defined {{on a pair of}} tasks in a work flow, and show that such constraints can be used to model many familiar authorization policies. We show that a set of entailment constraints can be manipulated algebraically in order to compute all possible <b>dependencies</b> <b>between</b> <b>tasks</b> in the workflow. The resulting set of constraints form the basis for an analysis of the satisfiability of a workflow. We briefly consider how this analysis can be used to implement a reference monitor for workflow systems...|$|R
40|$|A {{version of}} the H-LU {{factorization}} is introduced, based on the individual compu-tational tasks occurring during the block-wise H-LU factorization. The <b>dependencies</b> <b>between</b> these <b>tasks</b> form a directed acylic graph, which is used for efficient scheduling on parallel systems. The algorithm is especially suited for many-core processors and shows a much improved parallel scaling behavior compared to previous H-LU factorization algorithms...|$|R
40|$|Abstract — SuperMatrix out-of-order {{scheduling}} leverages high-level abstractions {{and straightforward}} data dependency analysis {{to provide a}} general-purpose mechanism for obtaining parallelism {{from a wide range}} of linear algebra operations. Viewing submatrices as the fundamental unit of data allows us to decompose operations into component tasks that operate upon these submatrices. Data <b>dependencies</b> <b>between</b> <b>tasks</b> are determined by observing the submatrix blocks read from and written to by each task. We employ the same dynamic outof-order execution techniques traditionally exploited by modern superscalar micro-architectures to execute tasks in parallel according to data dependencies within linear algebra operations. This paper provides a general explanation of the SuperMatrix implementation followed by empirical evidence of its broad applicability through performance results of several standard linear algebra operations on a wide range of computer architectures. I...|$|R
40|$|Predictive {{modelling}} in {{drug discovery}} is challenging to automate as it often contains multiple analysis steps and might involve cross-validation and parameter tuning that create complex <b>dependencies</b> <b>between</b> <b>tasks.</b> With large-scale data or when using computationally demanding modelling methods, e-infrastructures such as high-performance or cloud computing are required, {{adding to the}} existing challenges of fault-tolerant automation. Workflow management systems can aid {{in many of these}} challenges, but the currently available systems are lacking in the functionality needed to enable agile and flexible predictive modelling. We here present an approach inspired by elements of the flow-based programming paradigm, implemented {{as an extension of the}} Luigi system which we name SciLuigi. We also discuss the experiences from using the approach when modelling a large set of biochemical interactions using a shared computer cluster...|$|R
