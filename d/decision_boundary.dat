596|532|Public
50|$|Neural {{networks}} try {{to learn}} the <b>decision</b> <b>boundary</b> which minimizes the empirical error, while support vector machines {{try to learn}} the <b>decision</b> <b>boundary</b> which maximizes the empirical margin between the <b>decision</b> <b>boundary</b> and data points.|$|E
50|$|Nearest {{neighbor}} {{rules in}} effect implicitly compute the <b>decision</b> <b>boundary.</b> It {{is also possible}} to compute the <b>decision</b> <b>boundary</b> explicitly, {{and to do so}} efficiently, so that the computational complexity {{is a function of the}} boundary complexity.|$|E
50|$|The cluster {{assumption}} {{is equivalent to}} the Low density separation assumption which states that the <b>decision</b> <b>boundary</b> should lie on a low-density region. To prove this, suppose the <b>decision</b> <b>boundary</b> crosses one of the clusters. Then this cluster will contain points from two different classes, therefore it is violated on this cluster.|$|E
5000|$|A key {{observation}} is that rate [...] {{depends on the}} <b>decision</b> <b>boundaries</b> [...] and the codeword lengths , whereas the distortion [...] depends on the <b>decision</b> <b>boundaries</b> [...] and the reconstruction levels [...]|$|R
40|$|The {{hidden layer}} neurons in a multi-layered {{feed-forward}} neural network serve a critical role. From one perspective, the hidden layer neurons establish (linear) <b>decision</b> <b>boundaries</b> in the feature space. These linear <b>decision</b> <b>boundaries</b> are then combined by succeeding layers leading to convex-open and thereafter arbitrarily shaped <b>decision</b> <b>boundaries.</b> In this paper we show {{that the use of}} unidirectional Gaussian lateral connections from a hidden layer neuron to an adjacent hidden layer leads to a much richer class of <b>decision</b> <b>boundaries.</b> In particular the proposed class of networks has the advantage of sigmoidal feed-forward networks (global characteristics) but with the added flexibility of being able to represent local structure. An algorithm to train the proposed network is presented and its training and validation performance shown using a simple classification problem. Keywords: Lateral Connections, Feed-Forward Neural Networks, Multi-Layered Perceptrons, Radial Basis Function Net [...] ...|$|R
40|$|The {{exponential}} bound parameter is numerically {{evaluated for}} 16 -zone quantized 8 -PSK for signal vectors bisecting decision regions and for signal vectors lying on <b>decision</b> <b>boundaries.</b> For <b>decision</b> regions of equal span, signal vectors lying on <b>decision</b> <b>boundaries</b> are superior, confirming {{the result of}} Parsons and Wilson (1990). Signal vectors bisecting decision regions are superior if span of decision regions not containing signal vectors is optimized. Using the partial derivative, it is shown that the optimal configuration depends on SNR. Furthermore, the optimal configuration is not necessarily {{one in which the}} <b>decision</b> <b>boundaries</b> are straight lines...|$|R
5000|$|... #Caption: An {{example of}} the {{influence}} of unlabeled data in semi-supervised learning. The top panel shows a <b>decision</b> <b>boundary</b> we might adopt after seeing only one positive (white circle) and one negative (black circle) example. The bottom panel shows a <b>decision</b> <b>boundary</b> we might adopt if, in addition to the two labeled examples, we were given a collection of unlabeled data (gray circles). This could be viewed as performing clustering and then labeling the clusters with the labeled data, pushing the <b>decision</b> <b>boundary</b> away from high-density regions, or learning an underlying one-dimensional manifold where the data reside.|$|E
50|$|In a statistical-classification {{problem with}} two classes, a <b>{{decision}}</b> <b>boundary</b> or decision surface is a hypersurface that partitions the underlying vector space into two sets, {{one for each}} class. The classifier will classify all the points {{on one side of}} the <b>decision</b> <b>boundary</b> as belonging to one class and all those on the other side as belonging to the other class.|$|E
50|$|A <b>decision</b> <b>boundary</b> is {{the region}} of a problem space in which the output label of a {{classifier}} is ambiguous.|$|E
40|$|A useful {{method for}} {{representing}} Bayesian classifiers is through discriminant functions. Here, using copula functions, we propose {{a new model}} for discriminants. This model provides a rich and generalized class of <b>decision</b> <b>boundaries.</b> These <b>decision</b> <b>boundaries</b> significantly boost the classification accuracy especially for high dimensional feature spaces. We strengthen our analysis through simulation results. ...|$|R
40|$|The main {{aim of this}} master's {{thesis is}} to {{describe}} {{the subject of the}} implementation of <b>decision</b> <b>boundaries</b> with the help of artificial neural networks. The objective is to present theoretical knowledge concerning this field and on practical examples prove these statements. The work contains basic theoretical description of the field of pattern recognition and the field of feature based representation of objects. A classificator working on the basis of Bayes decision is presented in this part, and other types of classificators are named as well. The work then deals with artificial neural networks in more detail; it contains a theoretical description of their function and their abilities in the creation of <b>decision</b> <b>boundaries</b> in the feature plane. Examples are shown from literature for the use of neural networks in corresponding problems. As part of this work, the program ANN-DeBC was created using Matlab, for the generation of practical results about the usage of feed-forward neural networks for the implementation of <b>decision</b> <b>boundaries.</b> The work contains a detailed description of this program, and the achieved results are presented and analyzed. It is shown as well, how artificial neural networks are creating <b>decision</b> <b>boundaries</b> in the form of geometrical shapes. The effects of the chosen topology of the neural network and the number of training samples on the success of the classification are observed, and the minimal values of these parameters are determined for the successful creation of <b>decision</b> <b>boundaries</b> at the individual examples. Furthermore, it's presented how the neural networks behave at the classification of realistically distributed training samples, and what methods can affect the shape of the created <b>decision</b> <b>boundaries...</b>|$|R
50|$|Affine hyperplanes {{are used}} to define <b>decision</b> <b>boundaries</b> in many machine {{learning}} algorithms such as linear-combination (oblique) decision trees, and perceptrons.|$|R
50|$|In {{the case}} of {{backpropagation}} based artificial neural networks or perceptrons, the type of <b>decision</b> <b>boundary</b> that the network can learn {{is determined by the}} number of hidden layers the network has. If it has no hidden layers, then it can only learn linear problems. If it has one hidden layer, then it can learn any continuous function on compact subsets of Rn as shown by the Universal approximation theorem, thus it can have an arbitrary <b>decision</b> <b>boundary.</b>|$|E
5000|$|... #Caption: Manifold regularization can {{classify}} data when labeled data (black {{and white}} circles) are sparse, by {{taking advantage of}} unlabeled data (gray circles). Without many labeled data points, supervised learning algorithms can only learn very simple decision boundaries (top panel). Manifold learning can draw a <b>decision</b> <b>boundary</b> between the natural classes of the unlabeled data, under the assumption that close-together points probably belong to the same class, and so the <b>decision</b> <b>boundary</b> should avoid areas with many unlabeled points. This is one version of semi-supervised learning.|$|E
50|$|The {{algorithm}} can {{be understood}} as selecting samples that surprises the pilot model. Intuitively these samples are closer to the <b>decision</b> <b>boundary</b> of the classifier and is thus more informative.|$|E
50|$|Points {{which are}} {{close to each other}} are more likely to share a label. (More accurately, this is a {{continuity}} assumption rather than a smoothness assumption.) This is also generally assumed in supervised learning and yields a preference for geometrically simple <b>decision</b> <b>boundaries.</b> In the case of semi-supervised learning, the smoothness assumption additionally yields a preference for <b>decision</b> <b>boundaries</b> in low-density regions, so that there are fewer points close to each other but in different classes.|$|R
40|$|In this paper, we {{investigate}} the dimension expansion property of 3 layer feedforward neural networks {{and provide a}} helpful insight into how neural networks define complex <b>decision</b> <b>boundaries.</b> First, we note that adding a hidden neuron is equivalent to expanding the dimension of the space defined by the outputs of the hidden neurons. Thus, {{if the number of}} hidden neurons is larger than the number of inputs, the input data will be warped into a higher dimensional space. Second, we will show that the weights between the hidden neurons and the output neurons always define linear boundaries in the hidden neuron space. Consequently, the input data is first mapped non-linearly into a higher dimensional space and divided by linear planes. Then the linear <b>decision</b> <b>boundaries</b> in the hidden neuron space will be warped into complex <b>decision</b> <b>boundaries</b> in the input space...|$|R
40|$|It {{is common}} in {{classification}} methods to first place data in a vector space and then learn <b>decision</b> <b>boundaries.</b> We propose reversing that process: for fixed <b>decision</b> <b>boundaries,</b> we ``learnamp;amp;lsquo;amp;amp;lsquo; {{the location of the}} data. This way we (i) do not need a metric (or even stronger structure) [...] pairwise dissimilarities suffice; and additionally (ii) produce low-dimensional embeddings that can be analyzed visually. We achieve this by combining an entropy-based embedding method with an entropy-based version of semi-supervised logistic regression. We present results for clustering and semi-supervised classification...|$|R
5000|$|In {{the case}} of Gaussian-distributed data and {{unbiased}} class distributions, this statistic {{can be related to}} classification accuracy given an ideal linear discrimination, and a <b>decision</b> <b>boundary</b> can be derived.|$|E
5000|$|... where [...] is a vector of real-valued weights, [...] is the {{dot product}} , where m {{is the number}} of inputs to the {{perceptron}} and [...] is the bias. The bias shifts the <b>decision</b> <b>boundary</b> away from the origin and does not depend on any input value.|$|E
5000|$|Another major {{class of}} methods {{attempts}} to place boundaries in regions {{where there are}} few data points (labeled or unlabeled). One {{of the most commonly}} used algorithms is the transductive support vector machine, or TSVM (which, despite its name, may be used for inductive learning as well). Whereas support vector machines for supervised learning seek a <b>decision</b> <b>boundary</b> with maximal margin over the labeled data, the goal of TSVM is a labeling of the unlabeled data such that the <b>decision</b> <b>boundary</b> has maximal margin over all of the data. In addition to the standard hinge loss [...] for labeled data, a loss function [...] is introduced over the unlabeled data by letting [...] TSVM then selects [...] from a reproducing kernel Hilbert space [...] by minimizing the regularized empirical risk: ...|$|E
40|$|Neutral zone {{classifiers}} include 'no-decision' as {{a classification}} outcome. This paper extends neutral zone classifiers to sequential contexts for analyzing longitudinal data. Applications could include medical diagnosis where a decision variable is repeatedly measured on each subject {{with the expectation}} {{of being able to}} ultimately identify a patient disease status. Sequential classifiers monitor the sequence of measurements and decide when to stop sampling and how to classify the subject. Bayesian sequential classification rules make classifications {{on the basis of the}} minimum expected loss. This approach is a challenge due to computational complexity associated with evaluating the expected future costs. We consider Gaussian contexts. In the homogeneous case we demonstrate the equivalence between sequential Bayesian classifier and a simpler boundary-based framework. A solution for the heterogeneous Gaussian case is presented using the boundary-based framework. A recursive algorithm is developed to efficiently determine <b>decision</b> <b>boundaries</b> that minimize the overall expected cost. Alternative <b>decision</b> <b>boundaries</b> which are competitive with the optimal <b>decision</b> <b>boundaries</b> are studied. Misclassification rates and expected sample size are investigated and the results are compared with non-sequential classifiers...|$|R
40|$|This thesis {{comprises}} three nearly {{self contained}} parts. First we examine a few types of multi-class Support Vector Machine (SVM) classifiers that are typically used in applied machine learning. Unlike the original binary SVM formulation, in these classifiers the margins {{which are being}} maximized in the optimization problem do not represent distances to the <b>decision</b> <b>boundaries</b> of the final classifier. We investigate whether improvement {{can be obtained by}} employing classifiers which maximiz margins with respect to the classifier’s actual <b>decision</b> <b>boundaries.</b> Perhaps surprisingly, we will prove a theorem that negates that theory- the optimization problem solved by the unified versions (Crammer & Singer, 2001), (Weston & Watkins, 1998), obtains a solution that is identical to that of the optimization problem that maximizes margins with respect to the actual <b>decision</b> <b>boundaries.</b> In addition, we present a connection between this version and the 1 -vs- 1 SVM multiclass classifier. Later, we explore the use of descriptors extracted from pre-trained CNNs for image classification of new classes; in our work we addressed the sparsity o...|$|R
3000|$|..., and Gray-coded bits. For Gray-coded M-QAM, the <b>decision</b> <b>boundaries</b> for {{horizontal}} {{signals are}} independent of the vertical signal levels, and vice versa[20]. Thus for perfect channel information it is sufficient to calculate the BER for horizontal [...]...|$|R
5000|$|The G.729 {{standard}} calculates {{the following}} features for its VAD: line spectral frequencies, full-band energy, low-band energy (<1 kHz), and zero-crossing rate. It applies a simple classification using a fixed <b>decision</b> <b>boundary</b> {{in the space}} defined by these features, and then applies smoothing and adaptive correction to improve the estimate.|$|E
50|$|In machine learning, {{a margin}} {{classifier}} is a classifier which {{is able to}} give an associated distance from the <b>decision</b> <b>boundary</b> for each example. For instance, if a linear classifier (e.g. perceptron or linear discriminant analysis) is used, the distance (typically euclidean distance, though others may be used) of an example from the separating hyperplane is the margin of that example.|$|E
50|$|Neural data {{recorded}} from LIP neurons in rhesus monkeys {{supports the}} DDM, as firing {{rates for the}} direction selective neuronal populations sensitive to the two directions used in the 2AFC task increase firing rates at stimulus onset, and average activity in the neuronal populations is biased {{in the direction of}} the correct response. In addition, it appears that a fixed threshold of neuronal spiking rate is used as the <b>decision</b> <b>boundary</b> for each 2AFC task.|$|E
40|$|After errors, <b>decision</b> <b>boundaries</b> change, {{which results}} in post-error slowing of decisions. Purcell and Kiani (2016) report {{simultaneously}} decreased sensitivity to sensory information counteracts post-error increases in accuracy. Early post-error adjustments reflect a general orienting reflex rather than goal-directed adaptation...|$|R
25|$|Chen, C-H., Parekh, R., Yang, J., Balakrishnan, K. and Honavar, V. (1995). Analysis of <b>Decision</b> <b>Boundaries</b> Generated by Constructive Neural Network Learning Algorithms. In: Proceedings of the World Congress on Neural Networks (WCNN'95). Washington, D.C. July 17–21, 1995. pp.628–635.|$|R
30|$|However, the DNN is more {{powerful}} in noise pattern learning than the GMM. Due to its discriminative nature, the DNN model focuses on phone/state boundaries, and the boundaries it learns might be highly complex. Therefore, {{it is capable of}} addressing more severe noises and dealing with heterogeneous noise patterns. For example, a DNN may obtain a reasonable phone classification accuracy in a very noisy condition, if the noise does not drastically change the <b>decision</b> <b>boundaries</b> (e.g., with car noise). In addition, noises of different types and at different magnitude levels can be learned simultaneously, as the complex <b>decision</b> <b>boundaries</b> that the DNN classifier may learn provide sufficient freedom to address complicated decisions in heterogeneous acoustic conditions.|$|R
5000|$|The kernel trick {{avoids the}} {{explicit}} mapping {{that is needed}} to get linear learning algorithms to learn a nonlinear function or <b>decision</b> <b>boundary.</b> For all [...] and [...] in the input space , certain functions [...] can be expressed as an inner product in another space [...] The function [...] {{is often referred to as}} a kernel or a kernel function. The word [...] "kernel" [...] is used in mathematics to denote a weighting function for a weighted sum or integral.|$|E
50|$|In machine {{learning}} {{the margin of}} a single data point is defined to be {{the distance from the}} data point to a <b>decision</b> <b>boundary.</b> Note that there are many distances and decision boundaries that may be appropriate for certain datasets and goals. A margin classifier is a classifier that explicitly utilizes the margin of each example while learning a classifier. There are theoretical justifications (based on the VC dimension) as to why maximizing the margin (under some suitable constraints) may be beneficial for {{machine learning}} and statistical inferences algorithms.|$|E
5000|$|Support vector {{machines}} (SVMs) {{can effectively}} utilize {{a class of}} activation functions that includes both sigmoids and RBFs. In this case, the input is transformed to reflect a <b>decision</b> <b>boundary</b> hyperplane based on a few training inputs called support vectors [...] The activation function for the hidden layer of these machines {{is referred to as}} the inner product kernel, [...] The support vectors are represented as the centers in RBFs with the kernel equal to the activation function, but they take a unique form in the perceptron as ...|$|E
40|$|Abstract — Linear {{discriminant}} analysis by R. A. Fisher {{is based on}} the linear projection of the entire data set. Although the linear discriminant function is easy to comprehend, local nonlinearity can not be taken into consideration. In this paper a new approach to {{discriminant analysis}} is proposed, in which Fuzzy c-Means (FCM) clustering algorithm is simultaneously applied. The data set is fuzzily partitioned in order for good use of the linear discriminant, that is, many cluster centers are placed close to <b>decision</b> <b>boundaries.</b> Minimization of the within-group sum-ofsquared-error and maximization of the canonical correlation coefficients yield memberships to fuzzy clusters whose centers are close to the <b>decision</b> <b>boundaries.</b> The linear discriminant analysis is implemented in each obtained cluster. ...|$|R
40|$|Key Words: Amplitude Phase Shift Keying (APSK), {{iterative}} decoding, {{log likelihood}} ratio (LLR) This paper proposes an approximated soft decision demapping algorithm with low computational complexity for coded 4 + 12 + 16 amplitude {{phase shift keying}} (APSK) in an additive white Gaussian noise (AWGN) channel. To derive the proposed algorithm, we approximate the <b>decision</b> <b>boundaries</b> for 4 + 12 + 16 APSK symbols, and then decide the log likelihood ratio (LLR) value for each bit from the approximated <b>decision</b> <b>boundaries.</b> Although the proposed algorithm shows about 0. 6 ∼ 1. 1 dB degradation on the error performance compared with the conventional max-log algorithm, it gives a significant result {{in terms of the}} computational complexity...|$|R
40|$|Binary {{classifiers}} (dichotomizers) {{are combined}} for multi-class classification. Each region {{formed by the}} pairwise <b>decision</b> <b>boundaries</b> is assigned to the class with the highest frequency of training samples in that region. With more samples and classifiers, the frequencies converge to increasingly accurate non-parametric estimates of the posterior class probabilities {{in the vicinity of}} the <b>decision</b> <b>boundaries.</b> The method is applicable to non-parametric discrete or continuous class distributions dichotomized by either linear or non-linear classifiers (like support vector machines). We present a formal description of the method and place it in context with related methods. We present experimental results on machine-printed digits that demonstrate the viability of frequency coding in a classification task...|$|R
