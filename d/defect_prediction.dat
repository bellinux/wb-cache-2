279|77|Public
5000|$|The Fenton and Neil paper [...] "A {{critique}} of software <b>defect</b> <b>prediction</b> models" [...] placed in top 1% most influential papers in its field based on number of citations (according to Essential Science Indicators).|$|E
40|$|Software <b>defect</b> <b>prediction</b> {{is one of}} {{the most}} active {{research}} areas in software engineering. <b>Defect</b> <b>prediction</b> results provide the list of defect-prone source code artifacts so that quality assurance teams can effectively allocate limited resources for validating software products by putting more effort on the defect-prone source code. As the size of software projects becomes larger, <b>defect</b> <b>prediction</b> techniques will play an important role to support developers as well as to speed up time to market with more reliable software products. In this survey, we first introduce the common <b>defect</b> <b>prediction</b> process used in the literature and how to evaluate <b>defect</b> <b>prediction</b> performance. Second, we compare different <b>defect</b> <b>prediction</b> techniques such as metrics, models, and algorithms. Third, we discuss various approaches for cross-project <b>defect</b> <b>prediction</b> that is an actively studied topic in recent years. We then discuss applications on <b>defect</b> <b>prediction</b> and other emerging topics. Finally, based on this survey, we identify challenging issues for the next step of the software <b>defect</b> <b>prediction.</b> I...|$|E
40|$|Academia and {{industry}} expend much effort to predict software defects. Researchers proposed many <b>defect</b> <b>prediction</b> algorithms and metrics. While previous <b>defect</b> <b>prediction</b> techniques often take {{the author of}} the code into consideration, none of these techniques build a separate prediction model for each developer. Different developers have different coding styles, commit frequencies, and experience levels, which would result in different defect patterns. When the defects of different developers are combined, such differences are obscured, hurting the prediction performance. This thesis proposes two techniques to improve <b>defect</b> <b>prediction</b> performance: personalized <b>defect</b> <b>prediction</b> and confidence-based hybrid <b>defect</b> <b>prediction.</b> Personalized <b>defect</b> <b>prediction</b> builds a separate prediction model for each developer to predict software defects. Confidence-based hybrid <b>defect</b> <b>prediction</b> combines different models by picking the prediction from the model with the highest confidence. As a proof of concept, we apply the two techniques to classify defects at the file change level. We implement the state-of-the-art change classification as the baseline and compare with the personalized <b>defect</b> <b>prediction</b> approach. Confidence-based <b>defect</b> <b>prediction</b> combines these two models. We evaluate on six large and popular software projects written in C and Java—the Linux kernel, PostgreSQL, Xorg, Eclipse, Lucene and Jackrabbit...|$|E
40|$|An {{important}} goal during {{the cycle of}} software development is to find and fix existing defects as early as possible. This {{has much to do}} with software <b>defects</b> <b>prediction</b> and management. Nowadays,many  big software development companies have their own development repository, which typically includes a version control system and a bug tracking system. This has no doubt proved useful for software <b>defects</b> <b>prediction.</b> Since the 1990 s researchers have been mining software repository to get a deeper understanding of the data. As a result they have come up with some software <b>defects</b> <b>prediction</b> models the past few years. There are basically two categories among these prediction models. One category is to predict how many defects still exist according to the already captured defects data in the earlier stage of the software life-cycle. The other category is to predict how many defects there will be in the newer version software according to the earlier version of the software defects data. The complexities of software development bring a lot of issues which are related with software defects. We have to consider these issues as much as possible to get precise prediction results, which makes the modeling more complex. This thesis presents the current research status on software <b>defects</b> classification <b>prediction</b> and the key techniques in this area, including: software metrics, classifiers, data pre-processing and the evaluation of the prediction results. We then propose a way to predict software defects classification based on mining software repository. A way to collect all the defects during the development of software from the Eclipse version control systems and map these defects with the defects information containing in software defects tracking system to get the statistical information of software defects, is described. Then the Eclipse metrics plug-in is used to get the software metrics of files and packages which contain defects. After analyzing and preprocessing the dataset, the tool(R) is used to build a prediction models on the training dataset, in order to predict software defects classification on different levels on the testing dataset, evaluate the performance of the model and comparedifferent models’ performance...|$|R
30|$|In another study, Menzies et al. [20] {{reassured}} {{the usefulness}} of static code attributes to learn defect predictors. They showed that naive Bayes machine learning methods outperform rule-based or decision tree learning methods and they showed, on the other hand, that the choice of learning methods used for <b>defect</b> <b>predictions</b> can be {{much more important than}} used attributes. Unlike this previous work, we use static code attributes to predict issue fix time and we use neural networks as additional method for prediction.|$|R
40|$|This project {{reviews the}} use of Bays Networks (BNs) in {{software}} <b>defects</b> <b>Prediction.</b> The idea allows us to incorporate causal pro-cess factors. It does the combination of qualitative and quanti-tative software measures. It stops to play some well-known tra-ditional software metrics methods limitations. Decision support tools for this have been built using causal models represented by Bays Networks (BNs), incorporate empirical data and judg-ment of experts. Previously, this required a custom BN for each development lifecycle phase. We described a more general idea that allows causal models {{to be applied to}} any lifecycle phases. The approach is evolved through collaborative projects and cap-tures significant commercial input. For software projects within the range of the models, accuracy of <b>defect</b> <b>predictions</b> are very good. The main functions provided to the end-user is observa-tions and can be entered using a questionnaire interface, where questions are concerned to Bays Network variables. The model predicts the defects likely to be left in software after testing. The model uses the results of statistical analysis on the Previ-ous software projects. It can be combined with other defect pre-diction models to predict the number of residual defects of dif-ferent categories. The Bayesian network structure is, here, a set of project domain conditional independence relation. BN learn-ing structure which represents a domain. This domain can light on its underlying causal structure. This results in significantly im-proved accuracy for <b>defects</b> and reliability <b>prediction</b> type models...|$|R
40|$|In the {{software}} development process, {{how to develop}} better soft-ware at lower cost {{has been a major}} issue of concern. One way that helps is to find more defects as early as possible, on which <b>defect</b> <b>prediction</b> can provide effective guidance. The most popular <b>defect</b> <b>prediction</b> technique is to build <b>defect</b> <b>prediction</b> models based on machine learning. To improve the performance of <b>defect</b> <b>prediction</b> model, selecting appropriate features is critical. On the other hand, static analysis is usually used in defect detection. As static defect analyzers detects defects by matching some well-defined “defect patterns”, its result is useful for locating defects. However, <b>defect</b> <b>prediction</b> and static defect analysis are supposed to be two paral-lel areas due to the differences in research motivation, solution and granularity. In this paper, we present a possible approach to improve the per-formance of <b>defect</b> <b>prediction</b> with the help of static analysis tech-niques. Specifically, we present to extract features based on defect patterns from static defect analyzers to improve the performance of <b>defect</b> <b>prediction</b> models. Based on this approach, we implemented a <b>defect</b> <b>prediction</b> tool and set up experiments to measure the effect of the features...|$|E
40|$|Ideally, {{software}} <b>defect</b> <b>prediction</b> models {{should help}} organize {{software quality assurance}} (SQA) resources and reduce cost of finding defects by allowing the modules most likely to contain defects to be inspected first. In this paper, we study the cost-effectiveness of applying <b>defect</b> <b>prediction</b> models in SQA and propose a basic cost-effectiveness criterion. The criterion implies that <b>defect</b> <b>prediction</b> models should be applied with caution. We also propose a new metric FN/(FN+TN) to measure the cost-effectiveness of a <b>defect</b> <b>prediction</b> model. Copyright 2013 ACM...|$|E
40|$|Abstract Background: Cross-project <b>defect</b> <b>prediction,</b> which {{provides}} feasibility to build <b>defect</b> <b>prediction</b> {{models in the}} case of lack of local data repositories, have been gaining attention within research community recently. Many studies have pursued improving predictive performance of cross-project <b>defect</b> <b>prediction</b> models by mitigating challenges related to cross-project <b>defect</b> <b>prediction.</b> However there has been no attempt to analyse the empirical evidence on cross-project <b>defect</b> <b>prediction</b> models in a systematic way. Objective: The objective {{of this study is to}} summarise and synthesise the existing cross-project <b>defect</b> <b>prediction</b> studies in order to identify what kind of independent variables, modelling techniques, performance evaluation criteria and different approaches are used in building cross-project <b>defect</b> <b>prediction</b> models. Further, this study aims to explore the predictive performance of cross-project <b>defect</b> <b>prediction</b> models compared to within-project <b>defect</b> <b>prediction</b> models. Method: A systematic literature review was conducted to identify 30 relevant primary studies. Then qualitative and quantitative results of those studies were synthesized to answer defined research questions. Results: The majority of the Cross Project <b>Defect</b> <b>Prediction</b> (CPDP) models have been constructed using combinations of different types of independent variables. The models that perform well tend to be using combinations of different types of independent variables. Models based on Nearest Neighbour (NN) and Decision Tree (DTree) appear to perform well in CPDP context. Most commonly used Naive Bayes (NB) seemed to having average performance among other modelling techniques. Recall, precision, F-measure, probability of false alarm (pf) and Area Under Curve (AUC) are the commonly used performance metrics in cross-project context. Filtering and data transformation are also frequently used approaches in the cross-project context. The majority of the CPDP approaches address one or more data related issues using various row and column processing methods. Models appear to be performing well when filtering approach is used and model is built based on NB. Further, models perform well with data transformation approach is used and model is built based on Support Vector Machine (SVM). There is no significant difference in performance of CPDP models compared with Within Project <b>Defect</b> <b>Prediction</b> (WPDP) model performance. CPDP model perform well in majority cases in terms of recall. Conclusion: There are various types of independent variables, modelling techniques, performance evaluation criteria that have been used in cross-project <b>defect</b> <b>prediction</b> context. Cross-project <b>defect</b> <b>prediction</b> model performance is influenced by the way it is being built. Cross-project <b>defect</b> <b>prediction</b> still remains as a challenge, but they can achieve comparative predictive performance as within-project <b>defect</b> <b>prediction</b> models when the factors influencing the performance are optimized...|$|E
40|$|Most {{software}} metrics activities {{are carried out}} {{for the purposes of}} risk analysis of some form or another. Yet traditional metrics approaches, such as regression-based models for cost estimation and <b>defects</b> <b>prediction,</b> provide little support for managers wishing to use measurement to analyse and minimise risk. Many traditional approaches are not only insufficient in this respect but also fundamentally flawed. Significant improvements can be achieved by using causal models that require no new metrics. The new approach, using Bayesian nets, provides true decision-support and risk analysis potential. ...|$|R
40|$|Early {{estimation}} of defect density {{of a product}} {{is an important step}} towards the remediation of the problem associated with affordably guiding corrective actions in the software development process. This paper presents a suite of in-process metrics that leverages the software testing effort to create a <b>defect</b> density <b>prediction</b> model for use throughout the software development process. A case study conducted with Galois Connections, Inc. in a Haskell programming environment indicates that the resulting <b>defect</b> density <b>prediction</b> is indicative of the actual system defect density...|$|R
40|$|An {{important}} decision in software projects is {{when to stop}} testing. Decision support tools for this have been built using causal models represented by Bayesian Networks (BNs), incorporating empirical data and expert judgement. Previously, this required a custom BN for each development lifecycle. We describe a more general approach that allows causal models {{to be applied to}} any lifecycle. The approach evolved through collaborative projects and captures significant commercial input. For projects within the range of the models, <b>defect</b> <b>predictions</b> are very accurate. This approach enables decision-makers to reason {{in a way that is}} not possible with regression-based models...|$|R
40|$|Case studies {{focused on}} {{software}} <b>defect</b> <b>prediction</b> in real, industrial software development projects are extremely rare. We report on dedicated R&D project established in cooperation between Wroclaw University of Technology {{and one of}} the leading automotive software development companies to research possibilities of introduction of software <b>defect</b> <b>prediction</b> using an open source, extensible software measurement and <b>defect</b> <b>prediction</b> framework called DePress (<b>Defect</b> <b>Prediction</b> in Software Systems) the authors are involved in. In the first stage of the R&D project, we verified what kind of problems can be encountered. This work summarizes results of that phase...|$|E
40|$|<b>Defect</b> <b>prediction</b> is {{relatively}} a new research area of software quality assurance. A project team always aims {{to produce a}} quality product with zero or few defects. Quality of a product is correlated {{with the number of}} defects as well as it is limited by time and by money. So, <b>defect</b> <b>prediction</b> is very important in the field of software quality and software reliability. This paper gives you a vivid description about software <b>defect</b> <b>prediction.</b> It describes the key areas of software <b>defect</b> <b>prediction</b> practice, and highlights some key open issues for the future...|$|E
40|$|Abstract. The {{development}} of new product with low cost and reliable quality is one of important means to improve customer satisfaction and increase manufactures ’ profits. It is necessary to identify the key factors affecting product defects and control them early in the new product development (NPD) process with <b>defect</b> <b>prediction</b> methods, because <b>defect</b> <b>prediction</b> can effectively avoid or lower testing and unnecessary rework costs. The author proposes a new product <b>defect</b> <b>prediction</b> approach {{on the basis of}} Bayesian Network theory for decision-making in the NPD process. The proposed approach makes use of Bayesian Network to simulate defects ’ formation process, and it has a strong learning ability without requiring much data at the beginning of <b>defect</b> <b>prediction.</b> Product developers can easily predict the probability of defect occurrence of new products with this practical approach. The proposed product <b>defect</b> <b>prediction</b> approach can also help to focus on key factors influencing defects most. An example of turbine valve development is used to illustrate the proposed <b>defect</b> <b>prediction</b> approach. Also, recommendations for future research have been suggested...|$|E
30|$|It {{is worth}} {{mentioning}} that we previously performed an extensive study {{to evaluate the}} application of Granger Causality Test on software <b>defects</b> <b>prediction</b> (Couto et al. 2014). Basically, we focus on answering questions such as: (a) How many time series pass the preconditions related to defects (preconditions P 1, P 2, P 3)? (b) How many time series pass the preconditions related to source code metrics (preconditions P 4 and P 5)? (c) How many classes present positive results on the Granger Test? (d) What {{is the number of}} defects potentially covered by Granger? To answer these questions, we used a dataset including time series of source code metrics and defects for four real-world systems (Eclipse JDT Core, Eclipse PDE UI, Equinox Framework, and Lucene) (Couto et al. 2013 b).|$|R
40|$|Software metrics {{should provide}} {{information}} to support quantitative managerial decisionmaking during the software lifecycle. Good support for decision-making implies support for risk assessment and reduction. Traditional metrics approaches, often driven by regression-based models for cost estimation and <b>defects</b> <b>prediction,</b> provide little support for managers wishing to use measurement to analyse and minimise risk. The future for software metrics lies in using relatively simple existing metrics to build management decision-support tools that combine {{different aspects of}} software development and testing and enable managers to make many kinds of predictions, assessments and trade-offs during the software life-cycle. Our recommended approach is to handle the key factors largel ~ missing from the usual metrics approaches (causality, uncertainty, and combining different, often subjective, evidence) using causal modelling (we propose using Bayesian nets), empirical software engineering, and multi-criteria decision aids...|$|R
40|$|Background: Previous {{research}} {{has provided evidence}} {{that a combination of}} static code metrics and software history metrics can be used to predict with surprising success which files in the next release of a large system will have the largest numbers of defects. In contrast, very little research exists to indicate whether information about individual developers can profitably be used to improve predictions. Aims: We investigate whether files in a large system that are modified by an individual developer consistently contain either more or fewer faults than the average of all files in the system. The goal of the investigation is to determine whether information about which particular developer modified a file is able to improve <b>defect</b> <b>predictions.</b> We also continue an earlier study to evaluate the use of counts of th...|$|R
40|$|Part 8 : QualityInternational audienceSoftware <b>defect</b> <b>prediction</b> {{has drawn}} the {{attention}} of many researchers in empirical software engineering and software maintenance due to its importance in providing quality estimates and to identify the needs for improvement from project management perspective. However, most <b>defect</b> <b>prediction</b> studies seem valid primarily in a particular context and little concern is given on how {{to find out which}} prediction model is well suited for a given project context. In this paper we present a framework for conducting software <b>defect</b> <b>prediction</b> as aid for the project manager {{in the context of a}} particular project or organization. The framework has been aligned with practitioners’ requirements and is supported by our findings from a systematical literature review on software <b>defect</b> <b>prediction.</b> We provide a guide to the body of existing studies on <b>defect</b> <b>prediction</b> by mapping the results of the systematic literature review to the framework...|$|E
40|$|Abstract—Software <b>defect</b> <b>prediction</b> {{can help}} to {{allocate}} testing resources efficiently through ranking software modules according to their defects. Existing software <b>defect</b> <b>prediction</b> models that are optimized to predict explicitly the number of defects in a software module might fail to give an accurate order because it is very dif-ficult to predict {{the exact number of}} defects in a software module due to noisy data. This paper introduces a learning-to-rank ap-proach to construct software <b>defect</b> <b>prediction</b> models by directly optimizing the ranking performance. In this paper, we build on our previous work, and further study whether the idea of directly optimizing the model performance measure can benefit software <b>defect</b> <b>prediction</b> model construction. The work includes two aspects: one is a novel application of the learning-to-rank approach to real-world data sets for software <b>defect</b> <b>prediction,</b> and the other is a comprehensive evaluation and comparison of th...|$|E
40|$|Application of defect {{predictors}} {{in software}} development helps the managers to allocate their {{resources such as}} time and effort more efficiently and cost effectively to test certain sections of the code. In this research, we have used naive Bayes classifier (NBC) to construct our <b>defect</b> <b>prediction</b> framework. Our proposed framework uses the hierarchical structure information about the source code of the software product, to perform <b>defect</b> <b>prediction</b> at a functional method level and source file level. We have applied our model on SoftLAB and Eclipse datasets. We have measured the performance of our proposed model and applied cost benefit analysis. Our results reveal that source file level <b>defect</b> <b>prediction</b> improves the verification effort, while decreasing the <b>defect</b> <b>prediction</b> performance in all datasets...|$|E
40|$|Molecular Dynamics (MD) {{is a very}} {{powerful}} tool for studying displacement cascades initiated by the neutrons when they interact with matter and thus evaluate the primary damage. The mean number of point defects created can be obtained with a fair standard error with a reasonable number of cascade simulations (10 to 20 [1]), however other cascades characteristics (spatial distribution, size and amount of defect clusters) display a huge variability. Therefore, they may need to be studied using faster methods such as the Binary Collision Approximation (BCA) which is several order of magnitude less time consuming. We have investigated the point defect distributions subsequent to atomic collision cascades by both MD (using EAM potentials for Fe) and its BCA. MD and its BCA lead to comparable point <b>defect</b> <b>predictions.</b> The significant similarities and differences are discussed. © 2001 Materials Research Society. R 4. 3. 1 SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
40|$|Software <b>defects</b> <b>prediction</b> was {{introduced}} to support development and maintenance activities and improve the software quality. Reliable defect predictors can significantly optimize the utilization of software projects resources and increase customers confidence in the developed software products. In this paper, three different classifiers (LMT, SMO and J 48) are used to study the relations between dependency collected metrics and bugs collected for the software under study. ANT open source software is used in this case study. The selection of this open source was relation to the availability of source code and bug reports. Results varied between the three classifiers and showed J 48 {{to be the best}} classifier in terms of predicating such correlation between dependency metrics and defects. In general the three classifiers showed that there is a high significant correlation between proposed and evaluated dependency metrics and software defects which showed that they can be used as important early predictors for the software quality in general...|$|R
40|$|Abstract Standard {{practice}} in building models in software engineering normally involves three steps: collecting domain knowledge (previous results, expert knowledge); building a skeleton {{of the model}} based on step 1 including as yet unknown parameters; estimating the model parameters using historical data. Our experience shows that {{it is extremely difficult}} to obtain reliable data of the required granularity, or of the required volume with which we could later generalize our conclusions. Therefore, in searching for a method for building a model we cannot consider methods requiring large volumes of data. This paper discusses an experiment to develop a causal model (Bayesian net) for predicting the number of residual defects that are likely to be found during independent testing or operational usage. The approach supports (1) and (2), does not require (3), yet still makes accurate <b>defect</b> <b>predictions</b> (an R 2 of 0. 93 between predicted and actual defects). Since our method does no...|$|R
40|$|Software {{development}} involves {{plenty of}} risks, and errors exist in software modules represent a major kind of risk. Software <b>defect</b> <b>prediction</b> techniques and tools that identify software errors {{play a crucial}} role in software risk management. Among software <b>defect</b> <b>prediction</b> techniques, classification is a commonly used approach. Various types of classifiers have been applied to software <b>defect</b> <b>prediction</b> in recent years. How to select an adequate classifier (or set of classifiers) to identify error prone software modules is an important task for software development organizations. There are many different measures for classifiers and each measure is intended for assessing different aspect of a classifier. This paper developed a performance metric that combines various measures to evaluate the quality of classifiers for software <b>defect</b> <b>prediction.</b> The performance metric is analyzed experimentally using 13 classifiers on 11 public domain software defect datasets. The results of the experiment indicate that support vector machines (SVM), C 4. 5 algorithm, and K-nearest-neighbor algorithm ranked the top three classifiers. Classification, software risk management, software <b>defect</b> <b>prediction,</b> performance metric...|$|E
40|$|Abstract—Many {{empirical}} {{studies have shown}} that <b>defect</b> <b>prediction</b> models built on product metrics can be used to assess the quality of software modules. So far, most methods proposed in this direction predict defects by class or file. In this paper, we propose a novel software <b>defect</b> <b>prediction</b> method based on functional clusters of programs to improve the performance, especially the effort-aware performance, of <b>defect</b> <b>prediction.</b> In the method, we use proper-grained and problem-oriented pro-gram clusters as the basic units of <b>defect</b> <b>prediction.</b> To evaluate the effectiveness of the method, we conducted an experimental study on Eclipse 3. 0. We found that, comparing with class-based models, cluster-based prediction models can significantly improve the recall (from 31. 6 % to 99. 2 %) and precision (from 73. 8 % to 91. 6 %) of <b>defect</b> <b>prediction.</b> According to the effort-aware evaluation, the effort needed to review code to find half of the total defects can be reduced by 6 % if using cluster-based prediction models. Keywords-software quality; program clustering; defect predic-tion; I...|$|E
40|$|Abstract Software <b>defect</b> <b>prediction</b> {{can help}} us better {{understand}} and control software quality. Current <b>defect</b> <b>prediction</b> techniques are mainly based on a sufficient amount of historical project data. However, historical data is often not available for new projects and for many organizations. In this case, effective <b>defect</b> <b>prediction</b> is difficult to achieve. To address this problem, we propose sample-based methods for software <b>defect</b> <b>prediction.</b> For a large software system, we can select and test {{a small percentage of}} modules, and then build a <b>defect</b> <b>prediction</b> model to predict defectproneness {{of the rest of the}} modules. In this paper, we describe three methods for selecting a sample: random sampling with conventional machine learners, random sampling with a semi-supervised learner and active sampling with active semi-supervised learner. To facilitate the active sampling, we propose a novel active semi-supervised learning method ACoForest which is able to sample the modules that are most helpful for learning a good prediction model. Our experiments on PROMISE datasets show that the proposed methods are effective and have potential to be applied to industrial practice...|$|E
40|$|Abstract—Data miners can infer rules {{showing how}} to improve either (a) the effort {{estimates}} of a project or (b) the <b>defect</b> <b>predictions</b> of a software module. Such studies often exhibit conclusion instability regarding {{what is the most}} effective action for different projects or modules. This instability can be explained by data heterogeneity. We show that effort and defect data contain many local regions with markedly different properties to the global space. In other words, what appears to be useful in a global context is often irrelevant for particular local contexts. This result raises questions about the generality of conclusions from empirical SE. At the very least, SE researchers should test if their supposedly general conclusions are valid within subsets of their data. At the very most, empirical SE should become a search for local regions with similar properties (and conclusions should be constrained to just those regions). Index Terms—Data mining, defect/effort estimation, validation, empirical SE. I...|$|R
40|$|For {{some years}} {{software}} engineers have been attempting to develop useful prediction systems to estimate such attributes as {{the effort to}} develop a piece of software and the likely number of <b>defects.</b> Typically, <b>prediction</b> systems are proposed and then subjected to empirical evaluation. Claims are then made {{with regard to the}} quality of the prediction systems. A wide variety of prediction quality indicators have been suggested in the literature. Unfortuunately...|$|R
40|$|Software metrics {{are used}} as {{indicators}} {{of the quality of}} the developed software. Metrics can be collected from any software part such as: code, design, or requirements. In this paper, we evaluated several examples of design coupling metrics. Analysis and experiments follow hereinafter to demonstrate the use and value of those metrics. This is the second part for a paper we published in Computer Science Journal of Moldova (CSJM), V. 21, N. 2 (62), 2013 [19]. We proposed and evaluated several design and code coupling metrics. In this part, we collected source code from Scarab open source project. This open source is selected due to the availability of bug reports. We used bug reports for further analysis and association where bugs are used to form a class for classification and prediction purposes. Metrics are collected and analyzed automatically through the developed tool. Statistical and data mining methods are then used to generalize some findings related to the collected metrics. In addition classification and prediction algorithms are used to correlate collected metrics with high level quality attributes such as maintainability and <b>defects</b> <b>prediction...</b>|$|R
40|$|Abstract—Many {{software}} <b>defect</b> <b>prediction</b> {{approaches have}} been proposed and most are effective in within-project prediction settings. However, for new projects or projects with limited training data, it is desirable to learn a prediction model by using sufficient training data from existing source projects and then apply the model to some target projects (cross-project <b>defect</b> <b>prediction).</b> Unfortunately, the performance of crossproject <b>defect</b> <b>prediction</b> is generally poor, largely because of feature distribution differences between the source and target projects. In this paper, we apply a state-of-the-art transfer learning approach, TCA, to make feature distributions in source and target projects similar. In addition, we propose a novel transfer defect learning approach, TCA+, by extending TCA. Our experimental results for eight open-source projects show that TCA+ significantly improves cross-project prediction performance. Index Terms—cross-project <b>defect</b> <b>prediction,</b> transfer learning, empirical software engineering I...|$|E
40|$|<b>Defect</b> <b>prediction</b> is an {{important}} task in the mining of software repositories, but the quality of predictions varies strongly within and across software projects. In this paper we investigate {{the reasons why the}} prediction quality is so fluctuating due to the altering nature of the bug (or defect) fixing process. Therefore, we adopt the notion of a concept drift, which denotes that the <b>defect</b> <b>prediction</b> model has become unsuitable as set of influencing features has changed – usually due to a change in the underlying bug generation process (i. e., the concept). We explore four open source projects (Eclipse, OpenOffice, Netbeans and Mozilla) and construct file-level and project-level features for each of them from their respective CVS and Bugzilla repositories. We then use this data to build <b>defect</b> <b>prediction</b> models and visualize the prediction quality along the time axis. These visualizations allow us to identify concept drifts and – as a consequence – phases of stability and instability expressed in the level of <b>defect</b> <b>prediction</b> quality. Further, we identify those project features, which are influencing the <b>defect</b> <b>prediction</b> quality using both a tree induction-algorithm and a linear regression model. Our experiments uncover that software systems are subject to considerable concept drifts in their evolution history. Specifically, we observe that the change in number of authors editing a file and the number of defects fixed by them contribute to a project’s concept drift and therefore influence the <b>defect</b> <b>prediction</b> quality. Our findings suggest that project managers using <b>defect</b> <b>prediction</b> models for decision making {{should be aware of the}} actual phase of stability or instability due to a potential concept drift. 1...|$|E
40|$|We {{present an}} {{integrated}} measurement and <b>defect</b> <b>prediction</b> tool: Dione. Our tool enables organizations to measure, monitor, and control product quality through learning based <b>defect</b> <b>prediction.</b> Similar existing tools either provide {{data collection and}} analytics, or work just as a prediction engine. Therefore, companies {{need to deal with}} multiple tools with incompatible interfaces in order to deploy a complete measurement and prediction solution. Dione provides a fully integrated solution where data extraction, <b>defect</b> <b>prediction</b> and reporting steps fit seamlessly. In this paper, we present the major functionality and architectural elements of Dione followed by an overview of our demonstration...|$|E
40|$|Software metrics as {{a subject}} area is over 30 years old, but it has barely {{penetrated}} into mainstream software engineering. A key {{reason for this is}} that most software metrics activities have not addressed their most important requirement: to provide information to support quantitative managerial decision-making during the software lifecycle. Good support for decision-making implies support for risk assessment and reduction. Yet traditional metrics approaches, often driven by regression-based models for cost estimation and <b>defects</b> <b>prediction,</b> provide little support for managers wishing to use measurement to analyse and minimise risk. The future for software metrics lies in using relatively simple existing metrics to build management decision-support tools that combine different aspects of software development and testing and enable managers to make many kinds of predictions, assessments and trade-offs during the software life-cycle. Our recommended approach is to handle the key factors largely missing from the usual metrics approaches, namely: causality, uncertainty, and combining different (often subjective) evidence. Thus the way forward for software metrics research lies in causal modelling (we propose using Bayesian nets), empirical software engineering, and multi-criteria decision aids...|$|R
40|$|Quantitatively-based risk {{management}} {{can reduce the}} risks associated with field defects for both software producers and software consumers. In this paper, we report experiences and results from initiating risk-management activities at a large systems development organization. The initiated activities aim to improve product testing (system/integration testing), to improve maintenance resource allocation, and to plan for future process improvements. The experiences we report address practical issues not commonly addressed in research studies: how to select an appropriate modeling method for product testing prioritization and process improvement planning, how to evaluate accuracy of predictions across multiple releases in time, and how to conduct analysis with incomplete information. In addition, we report initial empirical results for two systems with 13 and 15 releases. We present prioritization of configurations to guide product testing, field <b>defect</b> <b>predictions</b> within the first year of deployment to aid maintenance resource allocation, and important predictors across both systems to guide process improvement planning. Our results and experiences are steps towards quantitatively-based {{risk management}}. Categories and Subject Descriptors D. 2. 8 [Software Engineering]: Metrics – Complexity metrics, Process metrics, Product metric...|$|R
40|$|International audienceThe paper {{deals with}} {{flatness}} <b>defects</b> <b>prediction</b> in thin plates which appear during rolling. Their origin is the roll stack thermo-elastic deformation. The {{combination of the}} elastic deflection, the thermal crown and the roll grinding crown results in a non-parallel bite. If the transverse roll profile is not an affinity of the incoming strip profile, differential elongation results and induces high stresses in the outgoing strip. The latter combine with the imposed strip tension force, resulting in a net post-bite stress field which may be sufficiently compressive locally to promote buckling. A variety of non-developable shapes may result, generally occurring as waviness, and classified as flatness defects (center waves, wavy edges, quarterbuckles [...] .). The {{purpose of the present}} paper is to present a coupled approach, following [1]: a simple buckling criterion is introduced in the FEM model of strip and roll deformation, LAM 3 /TEC 3 [2]. The post-bite stress field is in much better agreement with experiments if this treatment is used, as will be demonstrated...|$|R
