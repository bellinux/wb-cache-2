3|586|Public
40|$|This paper {{describes}} a prototype grid infrastructure, called the “eMinerals minigrid”, for molecular simulation scientists. {{which is based}} on an integration of shared compute and data resources. We describe the key components, namely the use of Condor pools, Linux/Unix clusters with PBS and IBM’s LoadLeveller job handling tools, the use of Globus for security handling, the use of Condor-G tools for wrapping globus <b>job</b> <b>submit</b> commands, Condor’s DAGman tool for handling workflow, the Storage Resource Broker for handling data, and the CCLRC dataportal and associated tools for both archiving data with metadata and making data available to other workers...|$|E
40|$|Experiments like ATLAS at LHC {{involve a}} scale of {{computing}} and data management that greatly exceeds the capability of existing systems, making it necessary to resort to Grid-based Parallel Event Processing Systems (GEPS). Traditional Grid systems concentrate the data in central data servers which have to be accessed by many nodes each time an analysis or processing job starts. These systems require very powerful central data servers and make little use of the distributed disk space that is available in commodity computers. The Grid-Brick system, which is described in this paper, follows a different approach. The data storage is split among all grid nodes having each one {{a piece of the}} whole information. Users submit queries and the system will distribute the tasks through all the nodes and retrieve the result, merging them together in the <b>Job</b> <b>Submit</b> Server. The main advantage of using this system is the huge scalability it provides, while its biggest disadvantage appears in the case of failure of one of the nodes. A workaround for this problem involves data replication or backup. 1...|$|E
5000|$|... • Download Manager: this {{component}} {{is responsible for}} fetching the input files belonging to <b>jobs</b> <b>submitted</b> in a remote manner.|$|R
3000|$|If T is {{the total}} {{available}} time, t is the execution time of each job and N {{is the total}} number of <b>jobs</b> <b>submitted</b> for execution, then [...]...|$|R
50|$|A run is a {{specific}} instantiation of a <b>Job</b> description <b>submitted</b> to the Sun Cloud ComputeUtility. Runs occur when the <b>job</b> is <b>submitted</b> to the Compute Utility for execution.|$|R
50|$|The <b>jobs</b> <b>submitted</b> to GRAM are {{targeted}} {{at a single}} computation resource, and consist of an optional input file staging phase, job execution, and an optional output file staging and cleanup stage.|$|R
40|$|The {{workload}} of the CERN central {{computer system}} is dominated, {{in terms of}} number of jobs, by short <b>jobs</b> <b>submitted</b> from, terminals and remote batch stations. The arrival of the CDC 7600, which {{will be even more}} easily accessible and will give the user his results more quickly, is likely to accentuate this domination...|$|R
50|$|Oneflare is an Australian online {{marketplace}} that connects customers and businesses. It has over 150 service categories from plumbers and electricians to pet groomers and interior designers {{to service the}} desired <b>job.</b> <b>Submitting</b> a <b>job</b> request is free, and customers can expect to receive up to 3 competitive quotes from nearby businesses. Billy Tucker joined Oneflare as CEO in February 2017.|$|R
50|$|Despite {{the growing}} {{popularity}} of Unicode in computing, the VNI Encoding (see below) is still in wide use by Vietnamese speakers both in Vietnam and abroad. All professional printing facilities in the Little Saigon neighborhood of Orange County, California continue to use the VNI Encoding when processing Vietnamese text. For this reason, print <b>jobs</b> <b>submitted</b> using the VNI Character Set are compatible with local printers.|$|R
30|$|The <b>job</b> script <b>submitted</b> to the Stallo job {{scheduler}} describes the resources requested on a node (scale up) {{and the number}} of nodes requested for the job (scale out). Both Galaxy and the {{job scheduler}} allow multiple job submissions from multiple users at the same time, but whether the jobs run simultaneously depends on the load of the cluster. HPC clusters are typically run with a high utilization, so jobs are often queued {{for a long time and}} therefore <b>jobs</b> <b>submitted</b> at the same time may not run at the same time. HPC clusters are not designed for elastic resource provision, so it is difficult to efficiently scale the backend to support the resource requirement variations of multi-user workloads.|$|R
30|$|The {{application}} layer {{at the top}} represents various types of applications, web or application or database servers that perform transactions on data. In CESM experiments, each <b>job</b> <b>submitted</b> to underlying infrastructure can perform its operations on data by using the provided self-service interfaces via REST APIs and catalogs. For more flexible storage services, the applications or service users can be provided a form of tagging or policies {{for the type of}} storage and data services applied [27].|$|R
5000|$|... qsub is an IEEE Std 1003.1-2008 Unix command for <b>submitting</b> <b>jobs</b> {{to a job}} scheduler, {{usually in}} cluster or grid computing. The qsub command is used to <b>submit</b> <b>jobs</b> to Slurm Workload Manager, to TORQUE, and to Oracle Grid Engine; HTCondor calls it condor_qsub.|$|R
5000|$|... when a <b>job</b> is <b>submitted</b> for {{execution}} by a submitter, it's {{moved to}} preinput queue: while there, JCL is validated (JCL validation {{is done by}} a cluster's node) ...|$|R
50|$|These {{programs}} {{support the}} line printer daemon protocol, so that other machines {{on a network}} can <b>submit</b> <b>jobs</b> to a print queue on a machine running the Berkeley printing system, and so that the Berkeley printing system user commands can <b>submit</b> <b>jobs</b> to machines that support that protocol.|$|R
40|$|Job {{submission}} on the GRID {{is controlled}} within the ATLAS experiment with PanDA system. This {{system is a}} multi-user pilot based system, with jobs being controlled from all ATLAS users by pilots on GRID CE. These pilots are submitted on the GRID using GRID proxies with special permissions, and all <b>jobs</b> <b>submitted</b> by PanDA then run with this proxy. This presentation describes improvements to have jobs run using the submitting user's proxys. Changes to the system are described, and the effects on security of jobs on the GRID...|$|R
40|$|The program AECOPY {{can be used}} {{to inspect}} and copy IBM 360 / 370 {{magnetic}} tapes. The number and length of physical records on the tape and actual records may be printed, {{and a number of other}} typical magnetic tape operations performed. The record characteristics and format of input tapes need not be known beforehand, and the type of error handling required may be specified by the user. Input to AECOPY may also be given in free form English from a batch <b>job</b> <b>submitted</b> by the user or by the operator at the computer console...|$|R
50|$|In {{a campus}} setting, VSPC offered users {{the ability to}} create and submit {{programs}} to an IBM (or compatible) mainframe without using punched cards, though the programs were still submitted as card images, and programs so submitted needed all the usual IBM Job Control Language (JCL) statements to access the mainframe batch submission and resource allocation processes. Output from a <b>job</b> <b>submitted</b> through VSPC could be routed to a printer, or back to the user's VSPC account, though in general the output would be too wide to easily view on a VSPC terminal.|$|R
5000|$|In 2008, <b>Jobs</b> <b>submitted</b> {{a renewed}} permit {{application}} with updated estimates. The Woodside Town Council granted the permit a year later, in May 2009, {{with the condition}} that Jobs must allow the house to be disassembled and moved elsewhere. In February 2010, Magalli and Jason Yoho offered to move the mansion to their five-acre lot in Woodside. Magalli Yoho reported in March that the house resembled a Spanish Colonial Revival mansion she lived in {{as a child in}} Ica, Peru. She said, [...] "This house is just a good house for our family." ...|$|R
40|$|AbstractIn this paper, {{we present}} a {{resource}} broker architecture for a computational Grid which uses Genetic Algorithm (GA) for brokering. Resource brokering implies selection of appropriate resource providers for <b>jobs</b> <b>submitted</b> to the Grid. Resource brokering is normally done {{with the objective of}} optimizing some performance parameter such as minimizing the total cost of running the jobs or maximizing the utilization of Grid resources. It is a challenging task since the search space for the problem consists of all possible allocations of <b>submitted</b> <b>jobs</b> to available resource providers in a Grid and may be very large. GAs are found to be efficient for such optimization problems. Moreover, the configuration and workload of a Grid is dynamic in nature. Our GA based resource broker tries to address these issues so that jobs are scheduled efficiently...|$|R
30|$|Step 5 : Allocate <b>submitted</b> <b>jobs</b> to the VMs.|$|R
40|$|Abstract. Production {{parallel}} {{systems are}} space-shared and hence em-ploy batch queues {{in which the}} <b>jobs</b> <b>submitted</b> to the systems are made to wait before execution. Thus, <b>jobs</b> <b>submitted</b> to parallel batch systems incur queue waiting times {{in addition to the}} execution times. Prediction of these queue waiting times is important to provide overall estimates to the users and can also help metaschedulers make scheduling decisions. Analyses of the job traces of supercomputers reveal that about 56 to 99 % of the jobs incur queue waiting times of less than an hour. Hence, identifying these quick starters or jobs with short queue waiting times is essential for overall improvement on queue waiting time predictions. Existing strategies provide high overestimates of upper bounds of queue waiting times rendering the bounds less useful for jobs with short queue waiting times. In this work, we have developed an integrated framework that uses the job characteristics, and states of the queue and processor occupancy to identify and predict quick starters, and use the existing strategies to predict jobs with long queue waiting times. Our experiments with different production supercomputer job traces show that our pre-diction strategies can lead to correct identification of up to 20 times more quick starters and provide tighter bounds for these jobs, and thus result in up to 64 % higher overall prediction accuracy than existing methods...|$|R
3000|$|... φ is the concession, {{per unit}} cost invested, that the cell owner {{is willing to}} allow and is {{inversely}} related to the budget. S is the <b>job</b> size <b>submitted</b> for analysis.|$|R
40|$|A {{classical}} {{problem in}} grammatical inference {{is to identify}} a deterministic finite automaton (DFA) from a set {{of positive and negative}} examples. In this paper, we address the related - yet seemingly novel - problem of identifying a set of DFAs from examples that belong to different unknown simple regular languages. We propose two methods based on compression for clustering the observed positive examples. We apply our methods to a set of print <b>jobs</b> <b>submitted</b> to large industrial printers. Comment: This paper has been accepted at the Learning and Automata (LearnAut) Workshop, LICS 2017 (Reykjavik, Iceland...|$|R
40|$|Predicting queue {{times on}} space-sharing {{parallel}} computers We present statistical techniques for predicting the queue times experienced by <b>jobs</b> <b>submitted</b> to a space-sharing parallel machine with first-come-first-served (FCFS) scheduling. We apply these techniques to trace {{data from the}} Intel Paragon at the San Diego Supercomputer Center and the IBM SP 2 at the Cornell Theory Center. We show {{that it is possible}} to predict queue times with accuracy that is acceptable for several intended applications. The coefficient of correlation between our predicted queue times and the actual queue times from simulated schedules is between 0 : 65 and 0 : 72. ...|$|R
40|$|To {{support the}} highly {{concurrent}} {{processing of transactions}} with low price at parallel and distributed database systems, Network Of Workstations(NOW) is utilized. All workstations cooperate to perform the <b>jobs</b> <b>submitted</b> by database applications. Each job consists of several transactions and these transactions are executed on NOW. Each transaction sent to NOW is allocated to a certain workstation by the coordinator running at a workstation. In this paper, we present a Distributed-Transaction Coordinator (DTC). In DTC, the cost to finish transactions is collected automatically and each transaction is assigned to an appropriate site based on the collected information...|$|R
40|$|The arrival {{process of}} <b>jobs</b> <b>submitted</b> to a {{parallel}} system is bursty, leading to {{fluctuations in the}} load at many time scales. In particular, rare events of extreme load may occur. Such events {{lead to an increase}} in the standard deviation of performance metrics, and thus delay the convergence of simulations used to evaluate the scheduling. Different performance metrics have been proposed in an effort to reduce this variability, and indeed display different rates of convergence. However, there is no single metric that outperforms the others under all conditions. Rather, the convergence of different metrics depends on the system being studied. ...|$|R
40|$|The {{problem of}} {{minimizing}} mean response time of generic <b>jobs</b> <b>submitted</b> to a heterogenous distributed computer systems is considered in this paper. A static load balancing strategy, in which decision of redistribution of loads {{does not depend}} {{on the state of the}} system, is used for this purpose. The article is closely related to a previous article on the same topic. The present article points out number of inconsistencies in the previous article, provides a new formulation, and discusses the impact of new findings, based on the improved formulation, on the results of the previous article. Comment: 18 page...|$|R
5000|$|A {{web portal}} which {{provided}} a graphical method for <b>submitting</b> <b>jobs.</b>|$|R
30|$|In {{this section}} we {{present the results}} of our data {{intensive}} experiment on a private cloud. Our work investigates the impact of adopting cohesive operational behaviours among the VMs to exploit the differences in data access times and implement effective resource provisioning and job scheduling. Each virtual machine requests bandwidth of a certain size to satisfy the <b>job</b> <b>submitted.</b> After the duration of data access, the virtual machines bandwidth is reduced to the basic bandwidth. This is achieved by inserting new action table entry in the software-defined enabled virtual switch introduced in Section ‘Container-based cloud framework’. The residual bandwidth from the reconfiguration is made available for other VMs.|$|R
50|$|A special {{configuration}} of the G-20, a dual processor G-21, was used to support campus computing at Carnegie Institute of Technology in the 1960s. Usually the two processors ran independently, one CPU handling card-based input, and the other handling <b>jobs</b> <b>submitted</b> through one of 16 AT&T Dataphones connected to telephone lines, usually via Teletype Model 35 KSR, Model 35 ASR and Teletype Model 33 ASR teleprinters. The G-21 had 32k words of memory for each processor, but could be reconfigured for 64k mode for large programs, usually as a single processor. A true dual processor operating system was developed late {{in the life of}} the G-21, but never reached production status.|$|R
40|$|Analyzing Diffusion Tensor Image data of {{the human}} brain of large study groups is complex and demands new, {{sophisticated}} and computationally intensive pipelines that can efficiently be executed. We present our progress {{over the past five years}} in the development and porting of the DTI analysis pipeline to a grid infrastructure. Starting with simple <b>jobs</b> <b>submitted</b> from the command-line, we moved towards a workflow-based implementation and finally into the e-BioInfra Gateway, which offers a web interface for the execution of selected biomedical data analysis software on the Dutch Grid. This gateway is currently being actively used by neuroscientists and for educational purposes. (C) 2012 Elsevier B. V. All rights reserve...|$|R
40|$|This paper {{introduces}} a new preemptive algorithm {{that is well}} suited for fair on-line scheduling of parallel jobs. Fairness is achieved by selecting job weights to be equal to the resource consumption of the job and by limiting the time span a job can be delayed by other <b>jobs</b> <b>submitted</b> after it. Further, the processing time of a job is not known when the job is released. It is proven that the algorithm achieves a constant competitive ratio for both the makespan and the weighted completion time for the given weight selection. Finally, the algorithm is also experimentally evaluated {{with the help of}} workload traces...|$|R
40|$|We present {{statistical}} {{techniques for}} predicting the queue times experienced by <b>jobs</b> <b>submitted</b> to a space-sharing parallel machine with first-come-first-served (FCFS) scheduling. We apply these techniques to trace {{data from the}} Intel Paragon at the San Diego Supercomputer Center and the IBM SP 2 at the Cornell Theory Center. We show {{that it is possible}} to predict queue times with accuracy that is acceptable for several intended applications. The coefficient of correlation between our predicted queue times and the actual queue times from simulated schedules is between 0 : 65 and 0 : 72. 1 Introduction On space-sharing parallel computers, it is useful to be able to predict how long a <b>submitted</b> <b>job</b> will be queued before processors are allocated to it. Some of the applications of these predictions are: Load metrics: They provide a measure of load that is more concrete than abstractions such as load average, allowing users to make decisions about what jobs to run, where to run them or what si [...] ...|$|R
40|$|The goal of {{the work}} {{described}} in {{this paper is to}} design and build a scalable infrastructure for executing grid applications on a widely distributed set of resources. Such grid infrastructure must be decentralized, robust, highly available, and scalable, while efficiently mapping application instances to available resources in the system. However, current desktop grid computing platforms are typically based on a client-server architecture, which has inherent shortcomings with respect to robustness, reliability and scalability. Fortunately, these problems can be addressed through the capabilities promised by new techniques and approaches in Peer-to-Peer (P 2 P) systems. By employing P 2 P services, our system allows users to <b>submit</b> <b>jobs</b> to be run in the system and to run <b>jobs</b> <b>submitted</b> by other users on any resources available in the system, essentially allowing a group of users to form an ad-hoc set of shared resources. The initial target application areas for the desktop grid system are in astronomy and space science simulation and data analysis. ...|$|R
40|$|This paper {{designed}} and built a distributed hash table based computing resource sharing platform named OE-P 2 RSP. By employing Peer-to-Peer services, OE-P 2 RSP allowed users to <b>submit</b> <b>jobs</b> to be run {{in the system and}} to run <b>jobs</b> <b>submitted</b> by other users on any resources available over the Internet, essentially allowing a group of users to form an Ad hoc set of shared resources. OE-P 2 RSP is based on structured peer on network share, improves the communication mode of system by encapsulated object, object group, and physical proximity principle, and the use of physical proximity principle, and gathers computing resources better in the same research institution or enterprise within local area network. The experimental results obtained via simulations show that the system can reliably execute scientific applications on a widely distributed set of resources with good load balancing and low matchmaking cost and that OE-P 2 RSP has good efficiency, load balancing and scalability. </p...|$|R
50|$|A {{property}} list (plist) {{is a type}} of file that launchd uses for program configuration. When launchd scans a folder, or a <b>job</b> is <b>submitted</b> with launchctl, it reads a plist file that describes how the program is to be run.|$|R
