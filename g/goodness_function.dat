16|20|Public
5000|$|Let [...] be {{a global}} {{optimization}} problem, where [...] {{is a state}} in the problem space [...] In SCO, each state is called a knowledge point, and the function [...] is the <b>goodness</b> <b>function.</b>|$|E
40|$|Abstract: Now a day’s most of {{the people}} are using {{scheduling}} algorithms in {{most of the}} resources where in this paper we are highlighting the concept of process scheduling used in Linux operating system, with the use of <b>goodness</b> <b>function.</b> This function is used to reduce the CPU time scheduling and effects and also find the task with relative desirability. Comparative to the other scheduling algorithms <b>goodness</b> <b>function</b> works well in all circumstances like giving IO-bound processes with a good response and efficiently. It took less time with good response, the same we explained and shown in this paper. Key terms: Scheduling, Process scheduling algorithms, priorities, goodnes...|$|E
40|$|Linear Relational Embedding is {{a method}} of {{learning}} a distributed representation of concepts from data consisting of binary relations between concepts. Concepts are represented as vectors, binary relations as matrices, and the operation of applying a relation to a concept as a matrix-vector multiplication that produces an approximation to the related concept. A representation for concepts and relations is learned by maximizing an appropriate discriminative <b>goodness</b> <b>function</b> using gradient ascent. On a task involving family relationships, learning is fast and leads to good generalization...|$|E
40|$|This {{dissertation}} {{analyzes the}} elusive morality in {{the fiction of}} acclaimed American author Flannery O’Connor (1925 - 1964) and attempts to determine what her work {{has to say about}} essential human goodness. The range of research includes three of O’Connor’s most famous short stories: “A Good Man Is Hard to Find”, “Good Country People”, “Revelation”, and her enigmatic debut novel Wise Blood. British Philosopher Philippa Foot’s (1920 - 2010) acclaimed book Natural Goodness (2001), in which she argues for a naturalistic objective view of human <b>goodness,</b> <b>functions</b> as the primary theoretical backbone to this research. ...|$|R
40|$|Cross-language {{research}} on adult speech perception demonstrates a strong effect of linguistic experience on consonant perception {{but not on}} vowel perception. Our paper re-examines the effect of linguistic experience on adults' vowel perception. First, identification and <b>goodness</b> <b>functions</b> for the high front quadrant of the vowel space were mapped for speakers of Swedish, English, and Spanish. Second, speakers performed a discrimination task for one vector in this vowel space. Stimuli along this vector were identified by Swedish speakers as belonging to the Swedish front rounded vowel series / :/ - /:/. However, English and Spanish speakers reported that the stimuli were not in their language...|$|R
40|$|Abstract This paper applies a {{simulated}} evolution (SE) {{approach to the}} problem of matching and scheduling dependent tasks in a heterogeneous suite of computers interconnected via a high-speed network. The various steps of the SE approach are discussed in details. <b>Goodness</b> <b>functions</b> required by SE are designed and explained. Experimental results applied on various types of workloads are analyzed. Workloads are characterized according to the connectivity, heterogeneity, and communication-to-cost ratio of the task graphs representing the application tasks. The performance of SE is compared with a genetic algorithm approach for the same problem with respect to the quality of solutions generated, and timing requirements of the algorithms. r 2003 Elsevier Science Ltd. All rights reserved. Keywords: Scheduling; Optimization; Heuristics; Heterogeneous system...|$|R
40|$|ABSTRACT This paper {{describes}} and {{analyzes the}} application of a simulated evolution (SE) approach to the problem of matching and scheduling of coarse-grained tasks in a heterogeneous suite of machines. The various steps of the SE algorithm are first discussed. <b>Goodness</b> <b>function</b> required by SE is designed and explained. Then experimental results applied on various types of workloads are analyzed. Workloads are characterized according to the connectivity, heterogeneity, and communication-to-cost ratio of the task graphs. The performance of SE is also compared with a genetic algorithm (GA) approach for the same problem with respect to the quality of solutions generated, and timing requirements of the algorithms...|$|E
40|$|AbstractÐIn this paper, we {{introduce}} Linear Relational Embedding as a {{means of}} learning a distributed representation of concepts from data consisting of binary relations between these concepts. The key idea is to represent concepts as vectors, binary relations as matrices, and the operation of applying a relation to a concept as a matrix-vector multiplication that produces an approximation to the related concept. A representation for concepts and relations is learned by maximizing an appropriate discriminative <b>goodness</b> <b>function</b> using gradient ascent. On a task involving family relationships, learning is fast and leads to good generalization. Index TermsÐ Distributed representations, feature learning, concept learning, learning structured data, generalization on relationa...|$|E
40|$|This paper {{describes}} and {{analyzes the}} application of a simulated evolution (SE) approach to the problem of matching and scheduling of coarse-grained tasks in a heterogeneous suite of machines. The various steps of the SE algorithm are first discussed. <b>Goodness</b> <b>function</b> required by SE is designed and explained. Then experimental results applied on various types of workloads are analyzed. Workloads are characterized according to the connectivity, heterogeneity, and communication-to-cost ratio of the task graphs. The performance of SE is also compared with a genetic algorithm (GA) approach for the same problem with respect to the quality of solutions generated, and timing requirements of the algorithms. 1...|$|E
40|$|In this paper, the Simulated Evolution {{algorithm}} (SimE) is en-gineered {{to solve}} the optimization problem of multi-objective VLSI netlist bi-partitioning. The multi-objective version of the prob-lem is addressed in which, power dissipation, timing performance, as well as cut-set are optimized while Balance is taken as a con-straint. Fuzzy rules are used in order to design the overall multi-objective cost function that integrates the costs of three objectives in a single overall cost value. Fuzzy <b>goodness</b> <b>functions</b> are de-signed for delay and power, and proved efficient. A series of exper-iments {{are performed to evaluate}} the efficiency of the algorithm. ISCAS- 85 / 89 benchmark circuits are used and experimental re-sults are reported and compared to earlier algorithms like GA and TS...|$|R
40|$|Abstract—We {{consider}} {{the problem of}} construction of decision trees in cases when data is non-categorical and is inherently high-dimensional. Using conventional tree growing algorithms that either rely on univariate splits or employ direct search methods for determin-ing multivariate splitting conditions is computationally prohibitive. On the other hand application of standard optimization methods for finding locally optimal splitting conditions is obstructed by abundance of local minima and discontinuities of classical <b>goodness</b> <b>functions</b> such as e. g. information gain or Gini impurity. In order to avoid this limitation a method to generate smoothed replacement for measuring impurity of splits is proposed. This enables to use vast number of efficient optimization techniques for finding locally optimal splits and, at the same time, decreases the number of local minima. The approach is illustrated with examples. I...|$|R
40|$|Abstract. The {{problem of}} {{partitioning}} appears {{in several areas}} ranging from VLSI, parallel programming, to molecular biology. The interest in finding an optimal partition especially in VLSI has been a hot issue in recent years. In VLSI circuit partitioning, the problem of obtaining a minimum cut is of prime importance. With current trends, partitioning with multiple objectives which includes power, delay and area, in addition to minimum cut is in vogue. In this paper, we engineer three iterative heuristics for the optimization of VLSI netlist bi-Partitioning. These heuristics are based on Genetic Algorithms (GAs), Tabu Search (TS) and Simulated Evolution (SimE). Fuzzy rules are incorporated in order to handle the multiobjective cost function. For SimE, fuzzy <b>goodness</b> <b>functions</b> are designed for delay and power, and proved efficient. A series of experiments {{are performed to evaluate}} the efficiency of the algorithms. ISCAS- 85 / 89 benchmark circuits are used and experimental results are reported and analyzed to compare the performance of GA, TS and SimE. Further, we compared the results of the iterative heuristics with a modified FM algorithm, named PowerFM, which targets power optimization. PowerFM performs better in terms of power dissipation for smaller circuits. For larger sized circuits SimE outperforms PowerFM in terms of all three, delay, number of net cuts, and power dissipation. Keywords: Genetic Algorithms, Tabu Search, Simulated Evolution, multiobjective, Fuzzy Logic, Netlist partitioning...|$|R
40|$|Abstract. Setting weights for Open Shortest Path First (OSPF) rout-ing {{protocol}} is an NP-hard problem. Optimizing these weights {{leads to}} less congestion {{in the network}} while utilizing link capacities e±ciently. In this paper, Simulated Evolution (SimE), a non-deterministic iterative heuristic, is engineered to solve this problem. A cost function that de-pends on the utilization and the extra load caused by congested links in the network is used. A goodness measure which is a prerequisite of SimE is designed to solve this problem. The proposed SimE algorithm is compared with Simulated Annealing. Results show that SimE explores search space intelligently due to its <b>goodness</b> <b>function</b> feature and reaches near optimal solutions very quickly. ...|$|E
40|$|Conventional {{clustering}} algorithms utilize {{a single}} criterion {{that may not}} conform to the diverse shapes of the underlying clusters. We offer a new clustering approach that uses multiple clustering objective functions simultaneously. The proposed multiobjective clustering is a two-step process. It includes detection of clusters {{by a set of}} candidate objective functions as well as their integration into the target partition. A key ingredient of the approach is a cluster <b>goodness</b> <b>function</b> that evaluates the utility of multiple clusters using re-sampling techniques. Multiobjective data clustering is obtained as a solution to a discrete optimization problem in the space of clusters. At meta-level, our algorithm incorporates conflict resolution techniques along with the natural data constraints. An empirical study on a number of artificial and real-world data sets demonstrates that multiobjective data clustering leads to valid and robust data partitions. ...|$|E
40|$|In {{this paper}} we {{introduce}} Linear Relational Embedding {{as a means}} of learning a distributed representation of concepts from data consisting of binary relations between concepts. The key idea is to represent concepts as vectors, binary relations as matrices, and the operation of applying a relation to a concept as a matrix-vector multiplication that produces an approximation to the related concept. A representation for concepts and relations is learned by maximizing an appropriate discriminative <b>goodness</b> <b>function</b> using gradient ascent. On a task involving family relationships, learning is fast and leads to good generalization. 1 Introduction Given data which consists of concepts and relations among concepts, our goal is to correctly predict unobserved instances of relationships between concepts. We do this by representing each concept as a vector in a Euclidean space and the relationships between concepts as linear operations. To illustrate the approach, we start with a very si [...] ...|$|E
40|$|We {{introduce}} {{a method for}} segmenting a shape from an image and simultaneously determining its symmetry axis. The symmetry is used to help the segmentation and in turn the segmentation determines the symmetry. The problem is formulated as one of minimizing a <b>goodness</b> of fitness <b>function</b> and Dijkstra's algorithm is used to find the global minimum of the cost function. The results are illustrated on real images...|$|R
40|$|In {{this paper}} a {{comparative}} study {{is made for}} the efficiency of an empirical distribution <b>function</b> <b>goodness</b> of fit tests. The tests are used for the logistic distribution. A new method is introduced to improve the efficiency of these tests under ranked set sampling. Simulation is used {{to show that the}} tests are more efficient than their counterparts in simple random sampling. The percentage points are obtained under the null hypotheses...|$|R
40|$|The aim of {{this article}} is to show that metaphysical level is the core and {{condition}} of possibility of axiology. The paper focuses on methodological differentiation concerning two levels: metaphysical and axiological one. The first one is connected with Goodness, which is the essence of values and an atmosphere that precedes every evaluation, while the second one means discourse about values. When metaphysical level with its Goodness is forgotten, axiology changes itself into “game of values”, that arbitrarily builds hierarchy and tries to measure everything. This “game of values” discounts the level of <b>Goodness</b> and <b>functions</b> as a closed area – a system of rules, where there are only  pseudo-values (goods), but there is no reference to Goodness. Without taking into consideration the metaphysical (i. e. ethical in Levinasian terminology) level and personal thinking axiology shows itself as a totalitarian construct, which only seems to be interested in Goodness and in fact it is more focused on constructing values. The difference between metaphysics and axiology is described via phenomenological analysis of Enigma of Goodness (the order of metaphysics) and phenomenon of good (the order of axiology) ...|$|R
40|$|We {{address the}} {{question}} 'how to measure goodness of timeprocessor optimal PRAM simulations'. Instead of measuring only the asymptotic complexity of simulation time, we attempt {{to take into account}} all aspects of simulations exactly. We present a <b>goodness</b> <b>function</b> framework and propose a generic function for measuring the goodness. 1 Introduction A simulation of an N-processor PRAM on a P -processor distributed memory machine (DMM) is time-processor optimal, if simulation of a PRAM step succeeds in time O(N=P) (with high probability). The simulation time is lower bounded by the diameter OE of the routing machinery and the expected memory congestion fl. If the DMM is symmetric and memory requests can be satisfied by only one memory module (one hash function), expected length ffi of memory request route is ffi = Θ(OE). If the total routing capacity is /P (/ packets per physical processor per step), two necessary conditions for time-processor optimality are that the load ` = N=P ( [...] ...|$|E
40|$|Location-based {{services}} allow {{users to}} perform geo-spatial check-in actions, which facilitates the mining {{of the moving}} activities of human beings. This paper proposes to recommend time-sensitive trip routes, consisting of a sequence of locations with associated time stamps, based on knowledge extracted from large-scale check-in data. Given a query location with the starting time, {{our goal is to}} recommend a time-sensitive route. We argue a good route should consider (a) the popularity of places, (b) the visiting order of places, (c) the proper visiting time of each place, and (d) the proper transit time from one place to another. By devising a statistical model, we integrate these four factors into a <b>goodness</b> <b>function</b> which aims to measure the quality of a route. Equipped with the goodness measure, we propose a greedy method to construct the time-sensitive route for the query. Experiments on Gowalla datasets demonstrate the effectiveness of our model on detecting real routes and cloze test of routes, comparing with other baseline methods. We also develop a system TripRouter as a real-time demo platform...|$|E
40|$|Clustering is a {{prominent}} method {{in the data}} mining field. It is a discovery process that groups data such that intra cluster similarity is maximized and the inter cluster similarity is minimized. Clustering has been widely used {{in a variety of}} areas and many clustering algorithms have been developed in response. Almost every report emphasizes differences and ignores similarities among algorithms. This is true in general and specifically for the algorithms of central concern in this paper: agglomerative hierarchical ones. The principal view adopted here is that improved clustering quality can be achieved through exploiting commonalties among methods, e. g., considerations relating to merging clusters and criteria for it, e. g., single link merging (SLINK, OPTICS); edge cut merging (CHAMELEON, ROCK); and criteria based on the square of the adjacency matrix (OPTICS, ROCK). MABAC (matrix based clustering), a proposed algorithm, introduces a <b>goodness</b> <b>function</b> based on notions of link and inner link that in turn involve direct and indirect similarity measures. It provides good clustering quality for data with different shape, density and can be modified for some applications such as web mining, microarray data analysis and sequence alignment analysis. 1...|$|E
40|$|A {{method is}} {{presented}} for deterministic global optimization in {{the estimation of}} parameters in models of dynamic systems. The method can be implemented as an ɛ-global algorithm, or, by use of the interval-Newton method, as an exact algorithm. In the latter case, the method provides a mathematically guaranteed and computationally validated global optimum in the <b>goodness</b> of fit <b>function.</b> A key feature of the method {{is the use of}} a new validated solver for parametric ODEs, which is used to produce guaranteed bounds on the solutions of dynamic systems with intervalvalued parameters, {{as well as on the}} first- and second-order sensitivities of the state variables with respect to the parameters. The computational efficiency of the method is demonstrated using several benchmark problems...|$|R
40|$|Calibration of car-following models against {{trajectory}} {{data has}} been widely applied {{as the basis for}} several type of studies ranging from the investigation and benchmarking of models, to the study of parameters correlation, or other theoretical issues like the inter/intra driver heterogeneity or the multi-anticipative driving behaviour. However, very few of these studies attempted also to analyze and quantify the uncertainty entailed in the calibration process and its impacts on the accuracy and reliability of results. A thorough understanding of the whole calibration problem (against trajectory data), {{as well as of the}} mutual effect of the specific problems raised in the field literature, indeed, does not yet exist. In this view, a general methodology to assess a calibration procedure was proposed and applied to the calibration of the Gipps’ car-following model. Compact indicators were proposed to evaluate the capability of a calibration setting to find the “known” global solution, in terms of both the accuracy and the robustness as to the variation of the starting conditions of the optimisation algorithm. Then, a graphical inspection method, based on the so called cobweb plots, was proposed to explore the existence and the nature of the local minima found by the algorithms, as well as to give insights into the measure of performances and the <b>goodness</b> of fit <b>functions</b> used in the calibration experiments. The methodology has been applied to all the calibration settings (i. e. combinations of algorithms, measure of performances and <b>goodness</b> of fit <b>functions)</b> utilized in the field literature so far. The study allowed us to highlight and motivate, for the model under investigation, the limits of some of these calibration settings. Research lines towards the definition of robust settings for the problem of car-following models calibration based on real trajectory data, are finally outlined...|$|R
40|$|The {{purpose of}} this study was to {{investigate}} the structural relationships between variables related to the Suicidal Ideation of Adolescences. A total of 923 Middle school students residing in Daegu City completed questionnaires which assessed family adaptability, family cohesion, self-esteem, bullying victimhood, depression and suicidal ideation. The sample variance-covariance matrix was analyzed using AMOS 20. 0, and a maximum likelihood minimization <b>function.</b> <b>Goodness</b> of fit was evaluated using the SRMS, RMSEA, and its 90 % confidence interval, CFI, and TLI. The results were as follows： First, the variables of family adaptability and family cohesion did not have a statistically significant Korean J. of Child Studies Vol. 34, No. 3, 93 - 112, June 2013 DOI：dx. org/ 10. 5723 /KJCS. 2013. 34. 3. 93 www. childkorea. or. kr pISSN 1226 - 1688 eISSN 2234 - 408...|$|R
40|$|Swarm {{systems are}} {{products}} of natural evolution. The complex collective behavior can emerge from a society of N autonomous cognitive entities [2], called as agents [5]. Each agent acquires knowledge in socially biased individual learning [4]. For human, the extrasomatic arbitrary symbols that manipulated by language allows for cognition {{on a grand scale}} [3], since agent can acquire social information that is no longer lim-ited to direct observation to other agents. The individual learning then only plays secondary role due to the ubiquity and efficiency of social learning [1]. Social cognitive optimization (SCO) is based on human social cognition, which op-erates on a symbolic space (S) that encodes the problem. Each x S∈ G is a knowledge point that evaluated by the <b>goodness</b> <b>function</b> F (x G). The foundational entity of SCO is social cognitive (SC) agent, which includes a memory (MD) and a set of action rules (RA). Specially, the agent acquires social sharing information only from an external medium called library (L), which stores NL points. The cognition is realized by inter-playing between learning and memory, which the MD and the L are employed for storing knowledge for guiding future actions; and RA are executed for acquiring mem-ory. Each SC agent is worked in iterated learning cycles. Suppose T is the number of maximum learning cycles. At the tth (1,t T t ≤ ≤ ∈]) learning cycle, its MD stores the most recently point () tx G. The action rules (RA) include: a) Selects a better point Bκ G from L by a tournament size Bτ; b) Determines the model point MX G and the refer point RX G: if () () () tBF F xκ...|$|E
40|$|Aftercare of {{sanitary}} landfills {{represents a}} burden for future generations, for emission potential of leachate and gases remains for hundred of years. Treatment methods {{have to be}} developed in order to accelerate waste degradation and reduce emission potential preferably within the time-span of one generation. Aeration seems a promising treatment method but as yet has to be proven effective as a methodology to enhance waste degradation at full scale. Water content {{plays a crucial role}} in evaluating aeration, but the highly heterogeneous nature of a landfill body poses a big uncertainty in quantifying it and therefore also quantifying the effectiveness of aeration in reducing emission potential. To improve understanding of water within a waste body, Electrical Resistivity Tomography ERT is to be used to indirectly measure water content by obtaining electric resistivity information. However, full scale landfills have large areas and therefore a protocol needs to be developed for generating an optimum survey strategy, so that high resolution information is obtained while covering a large area. This thesis presents such a protocol consisting of four parts. First, optimum spread and spacing are defined by building a Pareto front with resolution and covered area as objective criteria. Second, array is designed in the previously defined grid, with standard and non-standard four-electrode configurations, by using a <b>goodness</b> <b>function</b> applied to multiple channel acquisition systems. Third, array design is tested with synthetic models showing that smooth resistivity models are well captured by data inversion, but array design performs poorly in a sharp resistivity model. Finally, practical aspects namely injection time, polarization effects and unstable configurations which are usually overlooked, are shown to have significant influence in data quality. This protocol is intended as a systematic approach to generate an optimum ERT survey strategy which could be extended to other geophysical methods...|$|E
40|$|The {{nowadays}} {{increasing number}} of fields where large quantities of data are collected generates an emergent demand for methods for extracting relevant information from huge databases. Amongst the various existing data mining models, decision trees are widely used since they represent a good trade-off between accuracy and interpretability. However, one of their main problems {{is that they are}} very instable, which complicates the process of the knowledge discovery because the users are disturbed by the different decision trees generated from almost the same input learning samples. In the current work, binary tree classifiers are analyzed and partially improved. The analysis of tree classifiers goes from their topology from the graph theory point of view {{to the creation of a}} new tree classification model by means of combining decision trees and soft comparison operators (Mlynski, 2003) with the purpose to not only overcome the well known instability problem of decision trees, but also in order to confer the ability of dealing with uncertainty. In order to study and compare the structural stability of tree classifiers, we propose an instability coefficient which is based on the notion of Lipschitz continuity and offer a metric to measure the proximity between decision trees. This thesis converges towards its main part with the presentation of our model "Soft Operators Decision Tree (SODT). Mainly, we describe its construction, application and the consistency of the mathematical formulation behind this. Finally we show the results of the implementation of SODT and compare numerically the stability and accuracy of a SODT and a crisp DT. The numerical simulations support the stability hypothesis and a smaller tendency to overfitting the training data with SODT than with crisp DT is observed. A further aspect of this inclusion of soft operators is that we choose them in a way so that the resulting <b>goodness</b> <b>function</b> (used by this method) is differentiable and thus allows to calculate the best split points by means of gradient descent methods. The main drawback of SODT is the incorporation of the unpreciseness factor, which increases the complexity of the algorithm...|$|E
40|$|We {{introduce}} {{a method for}} segmenting a shape from an image and simultaneously determining its symmetry axis. The symmetry is used to help the segmentation and in turn the segmentation determines the symmetry. The problem is formulated as one of minimizing a <b>goodness</b> of fitness <b>function</b> and Dijkstra's algorithm is used to find the global minimum of the cost function. The results are illustrated on real images. 1. Introduction The importance of symmetric shapes has long been realized in the vision community [2, 3, 4]. Such symmetries, when detected, contain useful information about the shape {{which can be used}} for shape description and for breaking shapes up into symmetric parts. In addition, it is clear that human observers are highly sensitive to symmetry and make use of it in their processing of images [21]. Most work on symmetry has assumed that the shape has first been segmented from the background. In practice, however, image segmentation is often very difficult to do correctly. It [...] ...|$|R
50|$|Much {{recent work}} in {{political}} theory addresses whether egalitarianism {{should be replaced}} by prioritarianism. Prioritarians hold that an outcome’s <b>goodness</b> is a <b>function</b> of overall wellbeing across all individuals, with extra weight given to worse off individuals. This view first appeared under the name “the priority view” in Derek Parfit’s renowned 1991 article “Equality or Priority.” But the idea dates back to Temkin’s 1983 Ph.D. thesis, where it was presented under the name “extended humanitarianism.” And the word “prioritarianism” first appears in Temkin’s “Equality, Priority, and the Levelling Down Objection”. Prioritarianism has great plausibility. Many {{are drawn to the}} idea that making improvements in the wellbeing of the badly off should take priority over making equal improvements in the wellbeing of the well off. Prioritarianism, moreover, avoids the leveling down objection. Temkin, however, argues {{that it would be a}} mistake to jettison egalitarianism altogether since only egalitarianism reflects a fundamental concern for comparative fairness. There is an important place for egalitarian considerations in our all things considered judgments, Temkin argues, in addition to considerations of priority.|$|R
40|$|Goel {{proposed}} {{generalization of}} the Goel-Okumoto (G-O) software reliability growth model (SRGM), {{in order to}} model the failure intensity function, i. e. the rate of occurrence of failures (ROCOF) that initially increases and then decreases (I/D), which occurs in many projects due to the learning phenomenon of the testing team {{and a few other}} causes. The ROCOF of the generalized non-homogenous poisson process (NHPP) model can be expressed in the same mathematical form as that of a two-parameter Weibull function. However, this SRGM is susceptible to wide fluctuations in time between failures and sometimes it seems unable to recognize the I/D pattern of ROCOF present in the datasets and hence does not adequately describe such data. The authors therefore propose a shifted Weibull function ROCOF instead for the generalized NHPP model. This modification to the Goel-generalized NHPP model results in an SRGM that seems to perform better consistently, as confirmed by the goodness of fit statistic and predictive validity metrics, when applied to failure datasets of 11 software projects with widely varying characteristics. A case study on software release time determination using the proposed SRGM is also given. failure intensity <b>function,</b> <b>goodness</b> of fit statistic, mean value function, NHPP model, predictive validity, SRGM,...|$|R
40|$|The {{notion of}} {{community}} of web services has been recently proposed and investigated to gather functionally similar web {{services in the}} same virtual space. This allows increasing the visibility of web services and their collaboration, which makes their discovery and composition easier. Using the community infrastructure, users are supposed to direct their requests to the community's manager (called master), that {{is in charge of}} selecting the appropriate web service. Because many communities providing the same functionality are available, selecting the best community to deal with, from the users and providers perspectives, is a key factor that still needs to be investigated. Another particularly challenging issue yet to be addressed is the selection by the master of the appropriate web service to be hosted in the community. Reputation has been proposed as a means to help users, providers, and masters evaluate and rank different candidates. However, reputation is mainly based on users feedback, which is not always accurate. Moreover, other performance parameters should be considered in the selection game. In this thesis, we propose a new assessment process that focuses on various performance aspects of the community rather than just its reputation. This assessment considers the performance parameters from the users, providers, and masters perspectives. In this approach, the communities performance rate is mainly based on the web services hosted by those communities. Such an assessment approach helps the master of the community differentiate between web services so that only the appropriate ones can be invited or accepted to join based on the communities requirements. It also helps the users and providers select the best available communities. The proposed method works on three steps. The first step focuses on defining and iv computing the evaluation metrics used in the assessment process while considering the requirements of all the stakeholders, namely users, providers, and communities. Thus, each community or web service is described by a vector of metrics. The second step includes the clustering of the evaluated communities and web services using the resulted vectors from the first step. During the third step, the resulting clusters are ranked using a function called <b>goodness</b> <b>function.</b> Web services and communities belonging to the best cluster are then selected. The effectiveness of the proposed assessment approach is tested by simulation and comparison to two other approaches in the literature...|$|E
40|$|In {{applications}} of Web data integration, we frequently {{need to identify}} whether data objects in different data sources represent the same entity in the real world. This problem is known as entity resolution. In this paper, we propose a generic framework for entity resolution for relational data sets, called BARM, consisting of the Blocker, Attribute matchers and the Record Matcher. BARM is convenient for different blocking and matching algorithms to fit into it. For the blocker, we apply the SPectrAl Neighborhood (SPAN), a state-of-the-art blocking algorithm, to our data sets and show that SPAN is effective and efficient. For attribute matchers, we propose the Context Sensitive Value Matching Library (CSVML) for matching attribute values and also an approach to evaluate the <b>goodness</b> of matching <b>functions.</b> CSVML takes the meaning and context of attribute values into consideration and therefore has good performance, as shown in experimental results. We adopt Bayesian network as the record matcher in the framework and propose a method of inference from Bayesian network based on Markov blanket of the network. As a comparison, we also apply three other classifiers, including Decision Tree, Support Vector Machines, and the Naive Bayes classifier to our data sets. Experiments show that Bayesian network is advantageous in the book domain. 1...|$|R
40|$|For coalbed methane (CBM) wells {{with rising}} gas {{production}}, {{time to reach}} peak of gas production and peak gas rate are two fundamental features of a production profile. This paper investigates the time of occurrence of peak of gas production in CBM wells using production data analysis (PDA). The methodology is simple and requires gas rate and flowing bottom-hole pressure (BHP) data. Signatures of time of peak gas production are obtained using time derivative of square of BHP over gas rate. Signatures are obtained for constant and variable bottom-hole pressure operational constraints. Derivative of square of BHP over gas rate with respect to time is calculated for observed production data. Then, plot of this derivative versus production time is constructed for the CBM well. Finally, the plot is extrapolated to a time at which the signature of peak production appears. This time {{is estimated to be}} the time of occurrence of peak of gas production. The production data for a CBM well in San Juan basin is analyzed using this technique to demonstrate the signature and the estimation of time of peak gas production. The accuracy and correctness of estimation depend upon quality of production data, <b>goodness</b> of fitted <b>functions</b> used in this approach, and time interval between time of peak gas rate and observed production data. The signatures of time of peak production introduced in this paper can contribute to production forecasting of the negative decline portion of the production profile of CBM wells. Alireza Salmachi, Jake Darby, Zahra Yarmohammadtoosk...|$|R
40|$|Forward stratigraphic {{modelling}} aims at {{representing the}} {{spatial distribution of}} lithology {{as a function of}} physical processes and environmental conditions at the time of deposition so as to integrate geological knowledge into the reservoir modelling workflow, thus increasing predictive capabilities of reservoir models and efficient exploitation of hydrocarbons. Application of process-based models in inverse mode is not yet well-established due to our limited insight into the information content of common subsurface data and the computational overhead involved. In this paper we examine inverse modelling of stratigraphy by using a typical dataset acquired in the hydrocarbon industry, which consists of seismic data and standard logs from a limited number of wells. The approach is based on the use of a forward model called SimClast, developed at Delft University of Technology, to generate facies distribution and architecture at the regional scale. Three different <b>goodness</b> of fit <b>functions</b> were proposed for model inversion, following an inference approach. A synthetic reservoir unit was used to investigate the impact of the uncertainty affecting the input parameters and the information content of seismic and well data. The case study showed that the model was more sensitive to the initial topography and to the location of the sediment entry point than to sea level. The depth of the seismic reflector corresponding to the top-reservoir surface was the most informative data source; the initial and boundary conditions of the simulation were constrained by evaluating the depth of this reflector across the whole basin area. In the reservoir area, where the seismic-to-well tie was established, the depth of the reservoir top does not give enough information for constraining the model parameters. Our results thus indicate that evaluation of basin-scale data permits reduction of uncertainty in (geostatistical) reservoir models relative to the current workflow, in which only local data are used. Effective use of well data to generate reservoir models conditioned to basin-scale scenarios requires post-processing methods to downscale the output of the forward model used in the experiment...|$|R

