0|10000|Public
40|$|Univariate {{spectral}} analysis {{is used to}} model seasonally unadjusted quarterly unemployment rate data for Australia, 1978 (2) to 2002 (3). Data are tested for three categories: persons, males and females. Dynamic out-of-sample forecasts are made for 8 quarters using {{spectral analysis}} models evaluated against ARIMA model counterparts. It is found that the spectral analysis models achieve <b>higher</b> <b>levels</b> of <b>forecasting</b> accuracy than ARIMA counterparts, including turning point forecast accuracy. These results emerge in spite of weaker in-sample explanatory power of the spectral models against the ARIMA models. It is concluded {{the results suggest that}} the spectral model is ultimately better attuned to the various cyclical forces of the past unfolding into the future. 1...|$|R
40|$|This paper {{introduces}} {{the concept of}} Temporal Hierarchies for time series forecasting. A temporal hierarchy can be constructed for any time series by means of non-overlapping temporal aggregation. Predictions constructed at all aggregation levels are combined with the proposed framework to result in temporally reconciled, accurate and robust forecasts. The implied combination mitigates modelling uncertainty, while the reconciled nature of the forecasts results in a unified prediction that supports aligned decisions at different planning horizons: from short-term operational up to long-term strategic planning. The proposed methodology is independent of forecasting models. It can embed <b>high</b> <b>level</b> managerial <b>forecasts</b> that incorporate complex and unstructured information with lower <b>level</b> statistical <b>forecasts.</b> Our results show that forecasting with temporal hierarchies increases accuracy over conventional forecasting, particularly under increased modelling uncertainty. We discuss organisational implications of the temporally reconciled forecasts using {{a case study of}} Accident & Emergency departments...|$|R
40|$|Introverts {{are less}} likely to emerge as leaders than extraverts, however the {{existing}} literature provides little explanation as to why. To investigate the potential cause of this trait-based difference in emergent leadership, we measured trait extraversion in a sample of 184 business students and studied their leadership-related behavior in an unstructured group task. We drew from a model of forecasted affect to hypothesize that introverts {{would be less likely to}} emerge as leaders based on their belief that engaging in the necessary extraverted behavior would be unpleasant/unenjoyable (i. e. they would <b>forecast</b> <b>higher</b> <b>levels</b> of negative affect compared to extraverts). Consistent with this, we found that introverts were less likely to emerge as leaders, and that forecasted negative affect fully accounted for the relationship between extraversion and peer-rated emergent leadership. We therefore argue that introverts fail to emerge as leaders as often as extraverts because they engage in <b>higher</b> <b>levels</b> of <b>forecasted</b> negative affect and that these forecasts impede their emergent leadership potential...|$|R
40|$|In {{this study}} data {{assimilation}} based on variational assimilation was implemented with the HBV hydrological model using the YAO platform of University Pierre and Marie Curie (France). The {{principle of the}} variational assimilation is to consider the model state variables as control variables and optimise them by minimizing a cost function measuring the disagreement between observations and model simulations. The variational assimilation {{is used for the}} hydrological forecasting. In this case four state variables of the rainfall–runoff model HBV (those related to soil water content in the water balance tank and to water contents in rooting tanks) are considered as control variables. They were updated through the 4 D-VAR procedure using daily discharge incoming information. The Serein basin in France was studied and a <b>high</b> <b>level</b> of <b>forecasting</b> accuracy was obtained with variational assimilation allowing flood anticipation...|$|R
40|$|Abstract: The {{shareholder}} value concept reflects {{the tenets of}} modern financial theory. It is more meaningful than conventional, accounting-based approaches to valuation, but also more challenging. Its particular focus on risk and future potential demands a <b>high</b> <b>level</b> of <b>forecasting</b> and planning skills, and its application therefore requires great expertise and experience. Each company must be analyzed on its own merits {{in order to identify}} its individual opportunities for improvement. The increasing popularity of the {{shareholder value}} concept testifies to its superiority in communicating as well as analyzing corporate strategy. Its transparent methodology and open presentation of assumptions are major advantages. The clear objective of a long-term increase in value is key in creating value for all of a company's stakeholders through factors such as: cost leadership and differentiation, organization, strategic innovation, compensation and incentives, optimizing capital structure, financial engineering, communications. Key words: shareholders value, firm, capital structure 1...|$|R
40|$|This paper {{develops}} {{a model to}} forecast container volumes of all Korean seaports using a Seasonal ARIMA (Autoregressive Integrated Moving Average) technique with the quarterly data from the year of 1994 to 2010. In order to verify forecasting accuracy of the SARIMA model, this paper compares the predicted volumes resulted from the SARIMA model with the actual volumes. Also, the forecasted volumes of the SARIMA model is compared to those of an ARIMA model to demonstrate the superiority as a forecasting model. The results showed the SARIMA Model has a <b>high</b> <b>level</b> of <b>forecasting</b> accuracy and is superior to the ARIMA model in terms of estimation accuracy. Most of the previous research regarding the container-volume forecasting of seaports have been focussed on long-term forecasting with mainly monthly and yearly volume data. Therefore, this paper suggests a new methodology that forecasts shot-term demand with quarterl...|$|R
40|$|Abstract. A {{method of}} {{industrial}} property item monetary evaluation {{within the framework}} of the cost approach and in view of the preparation of the item to further transfer is proposed. Unlike the existing ones, the proposed method permits to obtain a <b>higher</b> <b>level</b> of <b>forecasting</b> precision of economic cost elements as part of the first cost of an industrial property item, which allows using this method in long-term planning. This method provides an opportunity to account for the following: the existing adjusting economic and statistical indicators specially designed for such purposes (inflation indexes etc.); the state-approved values of specific economic cost elements (minimum wage, fuel and electric power etc.); other adjusting indicators subject to calculation, particularly the cost indexes (of proceeds and turnover) of an enterprise as well as the indexes of physical production volume, risk factors etc. It is advisable to apply the individual price indexes of the enterprise proposed in the article while using this method...|$|R
40|$|This paper {{investigates the}} {{behavior}} of the ratio of new orders to shipments of durable goods (NO/S). <b>High</b> <b>levels</b> of NO/S are associated with a business cycle peak. They predict a short-run increase in employment and fixed and inventory investment but a dramatic long-run decline in employment, fixed investment, inventories, and GDP as a whole. We also find that NO/S captures time-varying risk premia. <b>Higher</b> <b>levels</b> of NO/S <b>forecast</b> lower excess returns on a broad set of assets, including equities, long-and intermediate-term Treasury bonds, and high- and low-grade corporate bonds, at horizons from one month to one year. These effects are robust to the inclusion of all common return predictors. We then construct an equilibrium model of investment with time to plan and a countercyclical price of risk and find that the ratio of new orders to shipments is procyclical and predicts lower excess returns. JEL classification: G 12, E 32, E 4...|$|R
40|$|Long-term load {{forecasting}} {{is intended}} to estimate the electrical load on an annual time period. It has {{an important role in}} the real control and security functions of an energy management system. This study is focused on designing long-term load forecasting in Palu electrical systems by using mamdani fuzzy logic method. The process of long term load forecasting is done by providings inputs; the number of customers, PDRB, and the power used for residences, businesses and public load at the previous year into the fuzzy logic system so that it is produced an output: the power used for the next year. The shows that mamdani fuzzy logic provide <b>high</b> <b>level</b> accuracy of <b>forecasting</b> and very small value of MSE. Index Terms:- fuzzy logic, MSE value, number of consumers, PDRB, the power use...|$|R
40|$|A {{decision}} support system for planning marketing strategies and allocating resources for a multi-store retailer is described. This {{decision support}} system combines well-known model building and analysis methodology, sophisticated computer software, and attention to management's implementation needs in order to apply management science thinking to messy, <b>high</b> <b>level</b> strategy and <b>forecasting</b> problems. The system consists of a planning model, national campaign evaluation system, experimental analysis system, and an ongoing interactive data base and reporting system. The planning model was first implemented with subjective judgment as input. The rest of the system was then refined to aid management in improving their subjective judgments, and for tracking and control. The marketing mix for total and business entity (groups of departments) sales is presently being planned {{with the support of}} this system. Profit improvements have been identified and implemented. {{decision support system}}s, retailing, marketing planning...|$|R
40|$|This is {{the first}} of three papers {{describing}} NAOMI 1, a new quarterly forecasting model developed in the Economic Analysis and Forecasting Division at the Department of Finance. NAOMI’s intended purpose is twofold. First, it is capable of producing reliable, judgement-free macroeconomic forecasts on a timely basis. Second, it can accurately quantify the level of uncertainty associated with each forecast while ensuring this uncertainty is minimised. Jointly achieving these two objectives represents a formidable task and necessitates a somewhat unique approach to the model building exercise. This paper outlines the model building strategy used to create NAOMI. The proposed approach differs from existing methods in that its sole objective is to minimise forecast uncertainty. We demonstrate that the <b>high</b> <b>level</b> of <b>forecast</b> imprecision typically associated with multi-equation forecasting models such as vector autoregressions stems from the large number of free parameters embodied in such systems. Furthermore, it is shown that lag length selection procedures based on in-sample fit often produce sub-optimal forecasting models in small samples. This paper proposes a technique whereby the sample is divided into two distinct sub-samples, called the estimation and evaluation sets. We then suggest choosing the lag order that minimises the mean squared error over the evaluation sample conditional on the OL...|$|R
40|$|This study investigates whether {{auditors}} {{respond to}} earnings management pressure created by analyst forecasts. Analyst forecasts create an important earnings target for management, and professional standards direct auditors {{to consider how}} this pressure could affect their clients. Using annual analyst forecasts available during the planning phase of the audit, I examine whether this form of earnings management pressure affects clients? financial statement misstatements. Next, I investigate whether auditors respond to earnings forecast pressure through audit fees and reporting delay. I find that <b>higher</b> <b>levels</b> of analyst <b>forecast</b> pressure {{increase the likelihood of}} client restatement. I also find that auditors charge higher audit fees and delay the issuance of the audit report in response to pressure from analyst expectations. Finally, I find that when audit clients are subject to high analyst forecast pressure, a high audit fee response by auditors mitigates the likelihood of client misstatements...|$|R
40|$|The Amity School District (ASD) {{experienced}} enrollment growth {{during the}} late 1980 s and early 1990 s, but reached a peak of 945 K- 12 students in Fall 1994. In the 12 years that followed, enrollment fell by 143 students, or 15 percent. This year’s enrollment of 822 represents an increase of 20 students (2. 5 percent) from Fall 2006, the largest increase since 1994. The long period of decline initially had its greatest impact on elementary grades, followed by middle school and more recently high school. This report {{presents the results of}} a study conducted by the Portland State University Population Research Center (PRC) concluding that the Amity District should not expect enrollment declines to continue in the long run, although about three more years of small losses are forecast at the <b>high</b> school <b>level.</b> Two <b>forecast</b> scenarios are presented, a 2 ̆ 2 baseline 2 ̆ 2 forecast under which housing development and migration trends remain similar to their recent levels, and a 2 ̆ 2 high range 2 ̆ 2 forecast that anticipates <b>higher</b> migration <b>levels,</b> primarily due to an increase in housing development...|$|R
40|$|We {{investigate}} the asset pricing and macroeconomic {{implications of the}} ratio of new orders (NO) to shipments (S) of durable goods. NO/S measures investment commitments by firms, and high values of NO/S {{are associated with a}} business cycle peak. We find that NO/S proxies for a short-horizon component of risk premia not identified in prior work. <b>Higher</b> <b>levels</b> of NO/S <b>forecast</b> lower excess returns on equities and many types of bonds, at horizons from one month to one year. These effects are generally robust to the inclusion of common return predictors and are significant on an out-of-sample basis as well. We also address the term structure of risk premia by constructing a similar ratio to measure longer-term investment commitments, which predicts returns primarily at longer horizons. (JEL G 12, E 32, E 44) Durable goods spending represents physical capital investment by businesses and households. As such, standard theory predicts that the decision to undertake these investments will be based on the discounted value of the future profits or service flows provided by the durable good. As long as some of the variation in these discounted values is due to time variation in risk premia, the amount o...|$|R
40|$|Flood {{forecasting}} {{models are}} a necessity, as they help in planning for flood events, and thus help prevent loss of lives and minimize damage. Current {{studies have shown}} that artificial neural networks (ANN) which is a parallel computing model have been successfully applied in water <b>level</b> <b>forecasting</b> studies. (ANN) models require historical data of the subject being study. This data is normally separated into a training dataset and a validation dataset. Several performance measures such as Nash-Sutcliffe efficiency, root mean square error and error distribution are used to evaluate forecasting results. BASIC 256 software and Microsoft Excel are other way used to implement to ANN modelling technique. The daily water level data can be taken from the Department of Irrigation and Drainage (DID), Malaysia. Water <b>level</b> <b>forecasting</b> is important for environmental protection and flood control since, when flood events occur, reliable water <b>level</b> <b>forecasts</b> enable the early warning systems to mitigate the flood effects. Importantly, the forecasting model developed based on (ANN) successfully achieves high accuracy forecasting result and satisfactory performance result...|$|R
40|$|A novel {{approach}} for the nowcasting of clouds and direct normal irradiance (DNI) {{based on the}} Spinning Enhanced Visible and Infrared Imager (SEVIRI) aboard the geostationary Meteosat Second Generation (MSG) satellite is presented for a forecast horizon up to 120  min. The basis of the algorithm is an optical flow method to derive cloud motion vectors for all cloudy pixels. To facilitate forecasts over a relevant time period, a classification of clouds into objects and a weighted triangular interpolation of clear-sky regions are used. Low and <b>high</b> <b>level</b> clouds are <b>forecasted</b> separately because they show different velocities and motion directions. Additionally a distinction in advective and convective clouds together with an intensity correction for quickly thinning convective clouds is integrated. The DNI is calculated from the forecasted optical thickness of the low and <b>high</b> <b>level</b> clouds. In order to quantitatively assess {{the performance of the}} algorithm, a forecast validation against MSG/SEVIRI observations is performed for a period of 2 months. Error rates and Hanssen–Kuiper skill scores are derived for forecasted cloud masks. For a forecast of 5  min for most cloud situations more than 95  % of all pixels are predicted correctly cloudy or clear. This number decreases to 80 – 95  % for a forecast of 2  h depending on cloud type and vertical cloud level. Hanssen–Kuiper skill scores for cloud mask go down to 0. 6 – 0. 7 for a 2  h forecast. Compared to persistence an improvement of forecast horizon by a factor of 2 is reached for all forecasts up to 2  h. A comparison of forecasted optical thickness distributions and DNI against observations yields correlation coefficients larger than 0. 9 for 15  min forecasts and around 0. 65 for 2  h forecasts...|$|R
40|$|Demand {{forecasting}} {{performance is}} subject to the uncertainty underlying the time series an organisation is dealing with. There are many approaches that may be used to reduce demand uncertainty and consequently improve the forecasting (and inventory control) performance. An intuitively appealing such approach that is known to be effective is demand aggregation. One approach is to aggregate demand in lower-frequency ‘time buckets’. Such an approach is often referred to, in the academic literature, as temporal aggregation. Another approach discussed in the literature is that associated with cross-sectional aggregation, which involves aggregating different time series to obtain <b>higher</b> <b>level</b> <b>forecasts.</b> This research discusses whether it is appropriate to use the original (not aggregated) data to generate a forecast or one should rather aggregate data first and then generate a forecast. This Ph. D. thesis reveals the conditions under which each approach leads to a superior performance as judged based on forecast accuracy. Throughout this work, it is assumed that the underlying structure of the demand time series follows an AutoRegressive Integrated Moving Average (ARIMA) process. In the first part of our 1 research, the effect of temporal aggregation on demand forecasting is analysed. It is assumed that the non-aggregate demand follows an autoregressive moving average process of order one, ARMA(1, 1). Additionally, the associated special cases of a first-order autoregressive process, AR(1) and a moving average process of order one, MA(1) are also considered, and a Single Exponential Smoothing (SES) procedure is used to forecast demand. These demand processes are often encountered in practice and SES is one of the standard estimators used in industry. Theoretical Mean Squared Error expressions are derived for the aggregate and the non-aggregate demand in order to contrast the relevant forecasting performances. The theoretical analysis is validated by an extensive numerical investigation and experimentation with an empirical dataset. The results indicate that performance improvements achieved through the aggregation approach are a function of the aggregation level, the smoothing constant value used for SES and the process parameters. In the second part of our research, the effect of cross-sectional aggregation on demand forecasting is evaluated. More specifically, the relative effectiveness of top-down (TD) and bottom-up (BU) approaches are compared for forecasting the aggregate and sub-aggregate demands. It is assumed that that the sub-aggregate demand follows either a ARMA(1, 1) or a non-stationary Integrated Moving Average process of order one, IMA(1, 1) and a SES procedure is used to extrapolate future requirements. Such demand processes are often encountered in practice and, as discussed above, SES is one of the standard estimators used in industry (in addition to being the optimal estimator for an IMA(1) process). Theoretical Mean Squared Errors are derived for the BU and TD approach in order to contrast the relevant forecasting performances. The theoretical analysis is supported by an extensive numerical investigation at both the aggregate and sub-aggregate levels in addition to empirically validating our findings on a real dataset from a European superstore. The results show that the superiority of each approach {{is a function of the}} series autocorrelation, the cross-correlation between series and the comparison level. Finally, for both parts of the research, valuable insights are offered to practitioners and an agenda for further research in this area is provided. L'objectif principal de cette recherche est d'analyser les effets de l'agrégation sur la prévision de la demande. Cet effet est examiné par l'analyse mathématique et l’étude de simulation. L'analyse est complétée en examinant les résultats sur un ensemble de données réelles. Dans la première partie de cette étude, l'impact de l'agrégation temporelle sur la prévision de la demande a été évalué. En suite, Dans la deuxième partie de cette recherche, l'efficacité des approches BU(Bottom-Up) et TD (Top-Down) est analytiquement évaluée pour prévoir la demande au niveau agrégé et désagrégé. Nous supposons que la série désagrégée suit soit un processus moyenne mobile intégrée d’ordre un, ARIMA (0, 1, 1), soit un processus autoregressif moyenne mobile d’ordre un, ARIMA (1, 0, 1) avec leur cas spéciales...|$|R
5000|$|... #Subtitle <b>level</b> 2: <b>Forecasting,</b> {{budgeting}} {{and organizational}} dynamics ...|$|R
40|$|Design, development, {{implementation}} {{and operation of}} a short-term blood inventory <b>level</b> <b>forecast</b> system is described. The {{primary purpose of the}} forecast system is to alert blood center management to the short-term inventory level of blood supplies so that they can take corrective notion to either reduce or increase blood collections. A test is performed on the system, and it is demonstrated that, management took action to reduce collections in response to higher than necessary inventory levels and increased collections during a period when insufficient inventory <b>levels</b> were <b>forecast.</b> ...|$|R
5000|$|... #Subtitle <b>level</b> 2: <b>Forecasting</b> of Sufyani revolt and {{government}} ...|$|R
40|$|The {{purpose of}} this {{research}} was to provide a forecast of financial trends in major intercollegiate athletics over the next 15 years for strategic planning purposes. This study focused specifically on the trends of revenue generation and cost containment in the athletic departments of the public institutions in the ACC, Big 12 and SEC Conferences. Most of these large programs are expected to externally produce a majority of their fiscal resources and compete at a <b>high</b> <b>level.</b> This <b>forecast</b> is important because of administrator's increasing difficulty to find the fiscal resources to adequately subsidize their program. The mixed methods study uncovered the myth that intercollegiate athletic programs are in great fiscal health and outlined where leaders in intercollegiate athletics think the future will take us. Over 35 forecasts were identified through interviews with an intercollegiate athletic conference commissioner and an intercollegiate athletic consultant and were then rated by a panel of athletic directors from the aforementioned conferences based on their desirability, impact and likelihood of occurrence. After two rounds of a Delphi procedure, it was determined that over half of the issues, should they occur, would have a high impact. None of the 35 issues were rated as having no or low impact. One issue was rated as having the highest possible likelihood of occurrence. The issue was that employee compensation, utility bills, travel costs, and medical insurance will increase for institutions and athletic departments faster than the general, national rate of inflation. 32 issues were rated as having between a 21 - 80 % chance of occurring, while two issues were given only a 0 - 20 % chance of occurring within the next 10 - 15 years. The first forecasted that football scholarship limits will be lowered from 85 over the next 10 - 15 years. The second forecasted that an antitrust exemption will be implemented over the next 10 - 15 years for intercollegiate athletics by the federal government to cap wages. Over one-third of the issues obtained consensus in two of the three areas rated. Three of the issues obtained consensus in all areas rated...|$|R
5000|$|... #Subtitle <b>level</b> 3: <b>Forecasts</b> of {{the economy}} and public {{finances}} ...|$|R
50|$|The Stormvloedwaarschuwingsdienst (SVSD; Storm Surge Warning Service) makes a water <b>level</b> <b>forecast</b> {{in case of}} a {{storm surge}} and warns the {{responsible}} parties in the affected coastal districts. These can then take appropriate measures depending on the expected water levels, such as evacuating areas outside the dikes, closing barriers and in extreme cases patrolling the dikes during the storm.|$|R
40|$|The {{domain of}} multi <b>level</b> <b>forecast</b> {{combination}} is a challenging new domain containing a large potential for forecast improvements. This thesis presents a theoretical and experimental analysis {{of different types}} of forecast diversification on forecast error covariances and resulting combined forecast quality. Three types of diversification are used: (a) diversification concerning the level of learning (b) diversification of predefined parameter values and (c) the use of different forecast models. The diversification is carried out on forecasts of seasonal factor predictions in Revenue Management for Airlines. After decomposing the data and generating diversified forecasts a (multi step) combination procedure is applied. We provide theoretical evidence of why and under which conditions multi step multi <b>level</b> <b>forecast</b> combination can be a powerful approach in order to build a high quality and adaptive forecast system. We theoretically and experimentally compare models differing with respect to the used decomposition, diversification as well as the applied combination models and structures. After an introduction into the application of forecasting seasonal behaviour in Revenue Management, a literature review of the theory of forecast combination is provided. In order to get a clearer idea of under which condition combination works, we then investigate aspects of forecast diversity and forecast diversification. The diversity of forecast errors in terms of error covariances can be expressed in a decomposed manner in relation to different independent error components. This type of decomposed analysis has the advantage that it allows conclusions concerning the potential of the diversified forecasts for future combination. We carry out such an analysis of effects {{of different types of}} diversification on error components corresponding to the bias-variance-Bayes decomposition proposed by James and Hastie. Different approaches of how to include information from different <b>levels</b> into <b>forecasting</b> are also discussed in the thesis. The improvements achieved with multi <b>level</b> <b>forecast</b> combination prove that theoretical analysis is extremely important in this relatively new field. The bias-variance-Bayes decomposition is extended to the multi level case. An analysis of the effects of including forecasts with parameters learned at different levels on the bias and variance error components show that forecast combination is the best choice in comparison to some other discussed alternatives. The proposed approach represents a completely automatic procedure. It realises changes in the error components which are not only advantageous at the low level, but have also a stabilising effect on aggregates of low <b>level</b> <b>forecasts</b> to the <b>higher</b> <b>level.</b> We also identify cases in which multi <b>level</b> <b>forecast</b> combination should ideally be connected with the use of different function spaces and/or thick modelling related to certain parameter values or preprocessing procedures. In order to avoid problems occurring for large sets of highly correlated forecasts when considering covariance information, we investigated the potential of pooling and trimming for our case. We estimate the expected behaviour of our diversified forecasts in purely error variance based pooling represented by a common approach of Aiolfi and Timmermann and analyse effects of different kinds of covariances on the accuracy of the combined forecast. We show that a significant loss in the expected forecast accuracy may ensue because of typical inhomogeneities in the covariance matrix for the analysed case. If covariance information is available in a sufficiently high quality, it is possible to run a clustering directly based on covariance information. We discuss how to carry out a clustering in that case. We also consider a case (quite common in our application) when covariance information may not be available and propose a novel simplified representation of the covariance matrix which represents the distance in the forecast generation space and is only based on knowledge about the forecast generation process. A new pooling approach is proposed that avoids inhomogeneities in the covariance matrix by considering the information contained in the simplified covariance representation. One of the main advantages of the proposed approach is that the covariance matrix {{does not have to be}} calculated. We compared the results of our approach with the approach of Aiolfi and Timmermann and explained the reasons for significant improvement. Another advantage of our approach is that it leads to the generation of novel multi step, multi <b>level</b> <b>forecast</b> generation structures that carry out the combination in different steps of pooling. Finally, we describe different evolutionary approaches in order to generate combination structures automatically. We investigate very flexible approaches as well as approaches that avoid the expected inhomogeneities in the error covariance matrix based on our theoretical findings. The theoretical analysis is supported by experimental results. We could achieve an improvement of forecast quality up to 11 percent for the practical application of demand forecasting in Revenue Management compared to the current optimised forecasting system...|$|R
40|$|A Human-In-The-Loop {{air traffic}} control {{simulation}} investigated the impact of uncertainties in trajectory predictions on NextGen Trajectory-Based Operations concepts, seeking to understand when the automation would become unacceptable to controllers or when performance targets {{could no longer be}} met. Retired {{air traffic control}}lers staffed two en route transition sectors, delivering arrival traffic to the northwest corner-post of Atlanta approach control under time-based metering operations. Using trajectory-based decision-support tools, the participants worked the traffic under varying <b>levels</b> of wind <b>forecast</b> error and aircraft performance model error, impacting the ground automations ability to make accurate predictions. Results suggest that the controllers were able to maintain <b>high</b> <b>levels</b> of performance, despite even the <b>highest</b> <b>levels</b> of trajectory prediction errors...|$|R
40|$|L'objectif {{principal}} de cette recherche est d'analyser les effets de l'agrégation sur la prévision de la demande. Cet effet est examiné par l'analyse mathématique et l étude de simulation. L'analyse est complétée en examinant les résultats sur {{un ensemble}} de données réelles. Dans la première partie de cette étude, l'impact de l'agrégation temporelle sur la prévision de la demande a été évalué. En suite, Dans la deuxième partie de cette recherche, l'efficacité des approches BU(Bottom-Up) et TD (Top-Down) est analytiquement évaluée pour prévoir la demande au niveau agrégé et désagrégé. Nous supposons que la série désagrégée suit soit un processus moyenne mobile intégrée d ordre un, ARIMA (0, 1, 1), soit un processus autoregressif moyenne mobile d ordre un, ARIMA (1, 0, 1) avec leur cas spéciales. Demand forecasting performance {{is subject to}} the uncertainty underlying the time series an organisation is dealing with. There are many approaches that may be used to reduce demand uncertainty and consequently improve the forecasting (and inventory control) performance. An intuitively appealing such approach that is known to be effective is demand aggregation. One approach is to aggregate demand in lower-frequency time buckets. Such an approach is often referred to, in the academic literature, as temporal aggregation. Another approach discussed in the literature is that associated with cross-sectional aggregation, which involves aggregating different time series to obtain <b>higher</b> <b>level</b> <b>forecasts.</b> This research discusses whether it is appropriate to use the original (not aggregated) data to generate a forecast or one should rather aggregate data first and then generate a forecast. This Ph. D. thesis reveals the conditions under which each approach leads to a superior performance as judged based on forecast accuracy. Throughout this work, it is assumed that the underlying structure of the demand time series follows an AutoRegressive Integrated Moving Average (ARIMA) process. In the first part of our 1 research, the effect of temporal aggregation on demand forecasting is analysed. It is assumed that the non-aggregate demand follows an autoregressive moving average process of order one, ARMA(1, 1). Additionally, the associated special cases of a first-order autoregressive process, AR(1) and a moving average process of order one, MA(1) are also considered, and a Single Exponential Smoothing (SES) procedure is used to forecast demand. These demand processes are often encountered in practice and SES is one of the standard estimators used in industry. Theoretical Mean Squared Error expressions are derived for the aggregate and the non-aggregate demand in order to contrast the relevant forecasting performances. The theoretical analysis is validated by an extensive numerical investigation and experimentation with an empirical dataset. The results indicate that performance improvements achieved through the aggregation approach are a function of the aggregation level, the smoothing constant value used for SES and the process parameters. In the second part of our research, the effect of cross-sectional aggregation on demand forecasting is evaluated. More specifically, the relative effectiveness of top-down (TD) and bottom-up (BU) approaches are compared for forecasting the aggregate and sub-aggregate demands. It is assumed that that the sub-aggregate demand follows either a ARMA(1, 1) or a non-stationary Integrated Moving Average process of order one, IMA(1, 1) and a SES procedure is used to extrapolate future requirements. Such demand processes are often encountered in practice and, as discussed above, SES is one of the standard estimators used in industry (in addition to being the optimal estimator for an IMA(1) process). Theoretical Mean Squared Errors are derived for the BU and TD approach in order to contrast the relevant forecasting performances. The theoretical analysis is supported by an extensive numerical investigation at both the aggregate and sub-aggregate levels in addition to empirically validating our findings on a real dataset from a European superstore. The results show that the superiority of each approach {{is a function of the}} series autocorrelation, the cross-correlation between series and the comparison level. Finally, for both parts of the research, valuable insights are offered to practitioners and an agenda for further research in this area is provided. BORDEAUX 1 -Bib. electronique (335229901) / SudocSudocFranceF...|$|R
40|$|Flood {{forecasting}} {{models are}} a necessity, as they help in planning for flood events, and thus help prevent loss of lives and minimize damage. At present, artificial neural networks (ANN) {{have been successfully}} applied in river flow and water <b>level</b> <b>forecasting</b> studies. ANN requires historical data to develop a forecasting model. However, long-term historical water level data, such as hourly data, poses two crucial problems in data training. First is that the high volume of data slows the computation process. Second is that data training reaches its optimal performance within a few cycles of data training, due to there being a high volume of normal water level data in the data training, while the forecasting performance for <b>high</b> water <b>level</b> events is still poor. In this study, the zoning matching approach (ZMA) is used in ANN to accurately monitor flood events in real time by focusing {{the development of the}} forecasting model on <b>high</b> water <b>level</b> zones. ZMA is a trial and error approach, where several training datasets using <b>high</b> water <b>level</b> data are tested to find the best training dataset for <b>forecasting</b> <b>high</b> water <b>level</b> events. The advantage of ZMA is that relevant knowledge of water level patterns in historical records is used. Importantly, the forecasting model developed based on ZMA successfully achieves high accuracy forecasting results at 1 to 3 h ahead and satisfactory performance results at 6 h. Seven performance measures are adopted in this study to describe the accuracy and reliability of the forecasting model developed...|$|R
40|$|Abstract: Over recent years, the <b>high</b> <b>levels</b> of air {{pollution}} have become a quite im-portant problem due to their direct impact on human health. Due to these health risks, EU directive (2008 / 50 /EC) recommends member states to ensure that timely information about actual and <b>forecasted</b> <b>levels</b> of pollutants are provided to the public. In order to follow these guidelines, prevent critical episodes and inform the public, environmental authorities need fast and reliable forecasting systems. In literature, {{air pollution}} forecast...|$|R
40|$|<b>Forecasting</b> {{the water}} <b>level</b> at Venice lagoon has been object of {{extensive}} {{studies in the}} past. For example, the numerical model (MIKE 21) based on deterministic equations has been setup {{for the purposes of}} the operational water <b>level</b> <b>forecast.</b> The model includes all the fundamental modelling components necessary for use in operational mode and model has been tested against a number of historical storms...|$|R
40|$|The {{proposed}} {{method is}} based on calculation of a regular trend deviation. It {{can be applied to}} the forecast model accuracy enhancement in regular processes. The method consists in calculation of regular trend after real data sample and forecasting of the trend deviation by combinatorial Group Method of Data Handling algorithm. An example of blood glucose <b>level</b> <b>forecasting</b> in the task of diabetic treatment is show...|$|R
40|$|A Large Scale Urban Model of the Brisbane-South East Queensland region {{incorporates}} a predictive model to simulate future patterns of employment in industry sectors. Those patterns may be visualized in “real time” over five year intervals. A predictive sub-regional input-output model {{is used to}} generate aggregate <b>level</b> <b>forecasts</b> of employment growth and distribution across 14 industry sectors as the input data to the disaggregated spatial allocation simulation model...|$|R
40|$|Polynomial {{regression}} {{to forecast}} earth dam piezometer levels gives a quadratic {{expression of the}} piezometer data variation versus reservoir level, which indicates a hysteresis of the piezometer measurements at mid-reservoir levels for increasing reservoir levels compared to decreasing reservoir <b>levels.</b> <b>Forecasting</b> of the piezometer levels based on the calculated quadratic expressions gives expected piezometer values with mean square error around 10 % of the standard deviation of the polynomial regression curves...|$|R
40|$|A model-based {{approach}} to the forecasting of short-range product demand within the semiconductor industry is presented. Device-level forecast models are developed via a novel two-stage stochastic algorithm that permits leading indicators to be optimally blended with smoothed estimates of unit-level demand. Leading indicators include backlog, bookings, delinquencies, inventory positions, and distributor resales. Group <b>level</b> <b>forecasts</b> are easily obtained through upwards aggregation of the device <b>level</b> <b>forecasts.</b> The forecasting algorithm is demonstrated at two major US-based semiconductor manufacturers. The first application involves a product family consisting of 254 individual devices with a 26 -month training dataset and eight-month ex situ validation dataset. A subsequent demonstration refines the approach, and is demonstrated across a panel of six high volume devices with a 29 -month training dataset and a 13 -month ex situ validation dataset. In both implementations, significant improvement is realised versus legacy forecasting systems. [Received 11 May 2007; Revised 5 September 2007; Accepted 15 October 2007]demand forecasting; statistical forecasting; enterprise resource planning; ERP; supply chain management; SCM; operations modelling; semiconductor fabrication; demand signal modelling; panel forecasting algorithm; device-level demand. ...|$|R
40|$|Maran {{is located}} at {{district}} {{of the same name}} between Temerloh and Kuantan, Pahang which is surrounded by remote forest and palm oil plantations. In December 2013, a worst flood occurred in Maran and had cause loss of lives and massive damages to the area. In this study, the data from Lubok Paku Station was used for analyzing data for flood forecasting. Lubok Paku Station in district Maran was one of the stations that are located along Sungai Pahang. In order to reduce the possibility for flood events to occur again in Maran, a reliable water <b>level</b> <b>forecasting</b> models is extremely important. Developing water <b>level</b> <b>forecasting</b> models is essential in water resources management and flood prediction. Accurate water <b>level</b> <b>forecasting</b> aids in achieve proficient and ideal utilization of water resources and minimize flooding damages. Conventional linear modelling forecasting model approach such as regression mostly provided relatively poor accuracy for forecasting peak inflow events of floods or drought. An artificial neural network or known as ANN is a computing model with a nonlinear mathematical approach that has been proven in many forecasting studies. Improving the ANN computational approach could help produce accurate forecasting results. ANN also has the ability to map inputs and outputs pattern and copy different elements experienced in the data. In this study, ANN Modelling is used to analyze the water level data. Different model architectures are examined based on the length of the forecasting period, epochs and training data. The data training was conducted using six ANN architectures. The performance of data training and data validation were evaluated using the coefficient of efficiency (NSC) and root mean square (RMSE). The results showed a strong performance of forecasting accuracy which can provides a significant tool for the authorities to take proper actions to minimize the damage of flooding...|$|R
30|$|The {{purpose of}} the present {{research}} is to arrive at an LNG <b>forecast</b> at port <b>level.</b> However, the existing literature on LNG forecasting provides only aggregated forecasts, not port-level forecasts. Therefore, a more specific forecasting method needs to be developed that takes account of historical transitions in shipping fuels, aggregated LNG bunker development forecasts, and current port-specific bunker volumes per ship type. The next section explains in further detail the proposed methodology for port <b>level</b> <b>forecasting.</b>|$|R
40|$|In this paper, a is {{presented}} {{for assessing the}} predictive uncertainty of rainfall-runoff and hydraulic forecasts that conditions forecasts uncertainty on the forecasted value itself, based on retrospective quantile regression of hindcasted water <b>level</b> <b>forecasts</b> and forecast errors. To test the robustness of the method, a number of retrospective forecasts for different catchments across England and Wales having different size and hydrological characteristics {{have been used to}} derive in a probabilistic sense the relation between simulated values of discharges and water levels, and matching errors. From this study, we can conclude that using quantile regression for estimating forecast errors conditional on the <b>forecasted</b> water <b>levels</b> provides an extremely simple, efficient and robust means for uncertainty estimation of deterministic forecasts...|$|R
