181|1851|Public
5000|$|A {{number of}} {{variants}} of PLS exist for estimating the factor and loading matrices [...] and [...] Most of them construct {{estimates of the}} linear regression between [...] and [...] as [...] Some PLS algorithms are only appropriate for the case where [...] is a column vector, while others deal with the general case of a matrix [...] Algorithms also differ on whether they estimate the <b>factor</b> <b>matrix</b> [...] as an orthogonal, an orthonormal matrix or not. The final prediction {{will be the same}} for all these varieties of PLS, but the components will differ.|$|E
5000|$|... where [...] is an [...] {{matrix of}} predictors, [...] is an [...] matrix of responses; [...] and [...] are [...] {{matrices}} that are, respectively, projections of [...] (the X score, component or <b>factor</b> <b>matrix)</b> and projections of [...] (the Y scores); [...] and [...] are, respectively, [...] and [...] orthogonal loading matrices; and matrices [...] and [...] are the error terms, {{assumed to be}} independent and identically distributed random normal variables. The decompositions of [...] and [...] are made so as to maximise the covariance between [...] and [...]|$|E
50|$|Varimax {{rotation}} is an orthogonal {{rotation of}} the factor axes to maximize the variance of the squared loadings of a factor (column) on all the variables (rows) in a <b>factor</b> <b>matrix,</b> which {{has the effect of}} differentiating the original variables by extracted factor. Each factor will tend to have either large or small loadings of any particular variable. A varimax solution yields results which make it as easy as possible to identify each variable with a single factor. This is the most common rotation option. However, the orthogonality (i.e., independence) of factors is often an unrealistic assumption. Oblique rotations are inclusive of orthogonal rotation, and for that reason, oblique rotations are a preferred method.|$|E
3000|$|This {{equation}} {{leads to}} two different {{interpretations of the}} PARALIND model, as a constrained Tucker model whose core tensor admits a PARAFAC decomposition with <b>factor</b> <b>matrices</b> Φ(n), called ‘interaction matrices,’ and as a constrained PARAFAC model with constrained <b>factor</b> <b>matrices</b> [...]...|$|R
3000|$|From (37), we can deduce that a mode {{combination}} {{results in}} a Khatri‐Rao product of the corresponding <b>factor</b> <b>matrices.</b> Consequently, the tensor contraction (5) associated with the PARAFAC‐N model (35) gives a PARAFAC‐ N 1 model whose <b>factor</b> <b>matrices</b> are equal to [...]...|$|R
40|$|In this paper, r-permutation <b>factor</b> {{circulant}} <b>matrix</b> {{is defined}} {{based on the}} permutation <b>factor</b> circulant <b>matrix,</b> and a fast algorithm for conditions of solution and solution of r-permutation <b>factor</b> circulant <b>matrix</b> linear equations AX=b are presented. When r-permutation <b>factor</b> circulant <b>matrix</b> are nonsingular, this algorithm computes the single solution of r-permutation <b>factor</b> circulant <b>matrix</b> linear equations, that is, there exists a unique r-permutation factor circulant matrix*, which the only solution of AX=b is the first column of C; When r-permutation <b>factor</b> circulant <b>matrix</b> are singular, it computes the special solution and general of r-permutation <b>factor</b> circulant <b>matrix</b> linear equations, {{which there is a}} unique r-permutation <b>factor</b> circulant <b>matrix</b> *and * makes the first column X 1 of C is a special solution of AX=b, but also the * is the general solution of AX=b, here Z is an n arbitrary-dimensional column vectors. (* Indicates a formula, please see the full text...|$|R
40|$|Abstract. As a {{derivative}} parameter {{of the structural}} displacement mode, damage <b>factor</b> <b>matrix</b> was adopted to investigate the detection of structural crack location and damage. A methodological strategy by combination of the damage <b>factor</b> <b>matrix</b> and the wavelet multi-resolution analysis {{is presented in the}} paper, in order to identify structural damages of the cantilevered beam. The results show that damage can be identified by the damage <b>factor</b> <b>matrix</b> and the wavelet multi-resolution analysis, and the damage degree can be estimated by damage frequency band energy of the wavelet analysis. A reference to damage identification in current structure engineering is provided...|$|E
30|$|Here, {{the matrix}} G is the {{weighting}} <b>factor</b> <b>matrix.</b> OC is the control matrix.|$|E
40|$|We {{investigate}} the multiple-input multiple-output broadcast channel with channel distribution information {{available at the}} transmitter. The so-called fading-paper model is considered with Nt transmit antennas, and each user having Nr receive antennas. Near-optimal designs are proposed for the inflation <b>factor</b> <b>matrix</b> under general fading conditions, based on maximizing the approximation of linear assignment capacity. It is proved that for Nt ≤ Nr, this matrix has a similar structure and achieves a similar interference elimination effect as dirty-paper coding. Also, low complexity iterative algorithms are proposed for Nt > N r case, which yield {{a good choice for}} the inflation <b>factor</b> <b>matrix</b> numerically. Based on the obtained inflation <b>factor</b> <b>matrix,</b> we provide efficient approaches to evaluate the linear assignment achievable rate region for some popular statistical channel models. © 2011 IEEE...|$|E
5000|$|The core tensor [...] is {{then the}} {{projection}} of [...] onto the tensor basis formed by the <b>factor</b> <b>matrices</b> , i.e., ...|$|R
40|$|The {{concept of}} r-block {{permutation}} <b>factor</b> circulant <b>matrix</b> is presented. The characteristics of r-block permutation <b>factor</b> circulant <b>matrix</b> are discussed by Kronecker. The interchange ability of r-block permutation <b>factor</b> circulant <b>matrix</b> has been demonstrated, that is AB=BA. The calculation method of matrix determinant and the sufficient condition of nonsingular matrix {{based on the}} diagonalization of circulant matrices are given. Finally, the method of inverse matrix is given in r-block permutation <b>factor</b> circulant <b>matrix.</b> ...|$|R
3000|$|... [...]. Therefore, the PARAFAC model (17) {{is equally}} valid for the {{multiuser}} case by simply interpreting its <b>factor</b> <b>matrices</b> as block-matrices.|$|R
3000|$|... [...]), n= 1, 2,…,N. It is {{demonstrated}} that the total coherence depends merely on the <b>factor</b> <b>matrix</b> with the largest coherence coefficient for a Kronecker dictionary [39, 40].|$|E
40|$|In this article, we derive a Bayesian {{model to}} {{learning}} the sparse and low rank PARAFAC decomposition for the observed tensor with missing values via the elastic net, with property {{to find the}} true rank and sparse <b>factor</b> <b>matrix</b> which is robust to the noise. We formulate efficient block coordinate descent algorithm and admax stochastic block coordinate descent algorithm to solve it, {{which can be used}} to solve the large scale problem. To choose the appropriate rank and sparsity in PARAFAC decomposition, we will give a solution path by gradually increasing the regularization to increase the sparsity and decrease the rank. When we find the sparse structure of the <b>factor</b> <b>matrix,</b> we can fixed the sparse structure, using a small to regularization to decreasing the recovery error, and one can choose the proper decomposition from the solution path with sufficient sparse <b>factor</b> <b>matrix</b> with low recovery error. We test the power of our algorithm on the simulation data and real data, which show it is powerful...|$|E
40|$|This {{research}} {{presents the}} development of a critical success <b>factor</b> <b>matrix</b> for increasing positive user experience of hotel websites based upon user ratings. Firstly, a number of critical success factors for web usability have been identified through the initial literature review. Secondly, hotel websites were surveyed in terms of critical success factors identified through the literature review. Thirdly, Herzberg's motivation theory has been applied to the user rating and the critical success factors were categorized into two areas. Finally, the critical success <b>factor</b> <b>matrix</b> has been developed using the two main sets of data. Comment: Journal articl...|$|E
40|$|The Schatten quasi-norm {{can be used}} {{to bridge}} the gap between the nuclear norm and rank function, and is the tighter {{approximation}} to matrix rank. However, most existing Schatten quasi-norm minimization (SQNM) algorithms, as well as for nuclear norm minimization, are too slow or even impractical for large-scale problems, due to the SVD or EVD of the whole matrix in each iteration. In this paper, we rigorously prove that for any p, p 1, p 2 > 0 satisfying 1 /p= 1 /p 1 + 1 /p 2, the Schatten-p quasi-norm of any matrix is equivalent to minimizing the product of the Schatten-p 1 norm (or quasi-norm) and Schatten-p 2 norm (or quasi-norm) of its two <b>factor</b> <b>matrices.</b> Then we present and prove the equivalence relationship between the product formula of the Schatten quasi-norm and its weighted sum formula for the two cases of p 1 and p 2 : p 1 =p 2 and p 1 ≠ p 2. In particular, when p> 1 / 2, there is an equivalence between the Schatten-p quasi-norm of any matrix and the Schatten- 2 p norms of its two <b>factor</b> <b>matrices,</b> where the widely used equivalent formulation of the nuclear norm can be viewed as a special case. That is, various SQNM problems with p> 1 / 2 can be transformed into the one only involving smooth, convex norms of two <b>factor</b> <b>matrices,</b> which can lead to simpler and more efficient algorithms than conventional methods. We further extend the theoretical results of two <b>factor</b> <b>matrices</b> to the cases of three and more <b>factor</b> <b>matrices,</b> from which we can see that for any 0 <p< 1, the Schatten-p quasi-norm of any matrix is the minimization of the mean of the Schatten-(p 3 + 1) p norms of all <b>factor</b> <b>matrices,</b> where p 3 denotes the largest integer not exceeding 1 /p. In other words, for any 0 <p< 1, the SQNM problem can be transformed into an optimization problem only involving the smooth, convex norms of multiple <b>factor</b> <b>matrices.</b> Comment: 21 pages. CUHK Technical Report CSE-ShangLC 20160307, March 7, 201...|$|R
40|$|Abstract. In {{this paper}} we propose a new {{flexible}} group tensor analysis model called the linked CP tensor decomposition (LCPTD). The LCPTD method can decompose given multiple tensors into common <b>factor</b> <b>matrices,</b> individual <b>factor</b> <b>matrices,</b> and core tensors, simultaneously. We applied the Hierarchical Alternating Least Squares (HALS) algorithm to the LCPTD model; besides we impose additional constraints to obtain sparse and nonnegative factors. Furthermore, we conducted some experiments of this model to demonstrate its advantages over existing models...|$|R
3000|$|... [...]. So, P nested Tucker {{models can}} then be {{interpreted}} as a Tucker model for which the <b>factor</b> <b>matrices</b> are products of P matrices. When [...]...|$|R
3000|$|... where m is a speaker- and channel-independent supervector, whose {{value is}} often taken from UBM supervector. T {{is the total}} <b>factor</b> <b>matrix</b> with low rank, which expands a {{subspace}} containing speaker- and channel-dependent information. w is a vector with a prior of standard Gaussian distribution. Speaker frames of an utterance are represented by posterior estimation of w, the new feature vector w is named total factor, {{often referred to as}} identity vector or i-vector. The process of training the total <b>factor</b> <b>matrix</b> is detailedly explained in [2], which is similar as learning the eigenvoice matrix. In order to sufficiently estimate T-Matrix, large quantity of development corpus is necessary.|$|E
30|$|The {{eigenvalues}} of {{the state}} matrix A supply useful information about the small-signal stability of the system. The participation <b>factor</b> <b>matrix</b> obtained from {{the left and right}} eigenvectors gives insight into the relationship between the states and the modes.|$|E
3000|$|This {{expression}} evidences {{that the}} Tucker {{model can be}} viewed as the transformation of the core tensor resulting from its multiplication by the <b>factor</b> <b>matrix</b> A(n) along its mode‐n, which corresponds to a linear map applied to the mode‐n space of [...]...|$|E
3000|$|... {{gave rise}} to the name PARATUCK‐ 2. The {{constraint}} matrices (Φ(1),Φ(2)) define interactions between columns of the <b>factor</b> <b>matrices</b> (A(1),A(2)), along the mode‐ 3 of [...]...|$|R
3000|$|... where S are the {{prosodic}} values (m[*]×[*] 1); E is {{the emotional}} state (n[*]×[*] 1); Fʹ is the <b>factors</b> <b>matrix</b> (m[*]×[*]n); and Bʹ is the offset matrix (m[*]×[*] 1).|$|R
40|$|Tensor {{canonical}} polyadic decomposition (CPD), which recovers {{the latent}} <b>factor</b> <b>matrices</b> from multidimensional data, {{is an important}} tool in signal processing. In many applications, some of the <b>factor</b> <b>matrices</b> {{are known to have}} orthogonality structure, and this information can be exploited to improve the accuracy of latent factors recovery. However, existing methods for CPD with orthogonal factors all require the knowledge of tensor rank, which is difficult to acquire, and have no mechanism to handle outliers in measurements. To overcome these disadvantages, in this paper, a novel tensor CPD algorithm based on the probabilistic inference framework is devised. In particular, the problem of tensor CPD with orthogonal factors is interpreted using a probabilistic model, based on which an inference algorithm is proposed that alternatively estimates the <b>factor</b> <b>matrices,</b> recovers the tensor rank and mitigates the outliers. Simulation results using synthetic data and real-world applications are presented to illustrate the excellent performance of the proposed algorithm in terms of accuracy and robustness...|$|R
3000|$|... is {{the lower}} bound of the {{reduction}} <b>factor</b> <b>matrix.</b> As {{it can be}} seen, Eq. (16) is easily implementable in Algorithm  3. This condition implies that the length parameter (10) is only reduced until a certain positive value (which is defined by [...]...|$|E
40|$|Abstract. Quotients and {{factors are}} {{important}} notions {{in the design}} of various computational procedures for regular languages and for the analysis of their logical properties. We propose a new representation of regular languages, by linear systems of language equations, which is suitable for the following computations: language reversal, left quotients and factors, right quotients and factors, and <b>factor</b> <b>matrix.</b> We present algorithms for the computation of all these notions, and indicate an application of the <b>factor</b> <b>matrix</b> to the computation of solutions of a particular language reconstruction problem. The advantage of these algorithms is that they all operate only on linear systems of language equations, while the design of the same algorithms for other representations often require translations to other representations. ...|$|E
40|$|Canonical Polyadic Decomposition (CPD) of a higher-order tensor is {{decomposition}} in {{a minimal}} number of rank- 1 tensors. We give {{an overview of}} existing results concerning uniqueness. We present new, relaxed, conditions that guarantee uniqueness of one <b>factor</b> <b>matrix.</b> These conditions involve Khatri-Rao products of compound matrices. We make links with existing results involving ranks and k-ranks of factor matrices. We give a shorter proof, based on properties of second compound matrices, of existing results concerning overall CPD uniqueness in the case where one <b>factor</b> <b>matrix</b> has full column rank. We develop basic material involving $m$-th compound matrices that will be instrumental in Part II for establishing overall CPD uniqueness in cases where none of the factor matrices has full column rank. Comment: 28 page...|$|E
3000|$|... {{that are}} {{decomposed}} into a sum of P Tucker models of rank‐ (L,M,N), which {{corresponds to the}} particular case where all the <b>factor</b> <b>matrices</b> are full column rank, with [...]...|$|R
40|$|Abstract. This work {{addresses}} {{the concept of}} nonnegative matrix factorization (NMF). Some relevant issues for its formulation as as a nonlinear optimization problem will be discussed. The primary goal of NMF is that of obtaining good quality approximations, namely for video/image visualization. The importance of the rank of the <b>factor</b> <b>matrices</b> {{and the use of}} global optimization techniques is investigated. Some computational experience is reported indicating that, in general, the relation between the quality of the obtained local minima and the <b>factor</b> <b>matrices</b> dimensions has a strong impact {{on the quality of the}} solutions associated with the decomposition...|$|R
40|$|A {{computer}} program to compare <b>factor</b> <b>matrices</b> which {{have at least}} some variables in common is described. The program calculates both the salient variable similarity index and the congruence coeffi-cient. The PLI program compares all <b>factors</b> in one <b>matrix</b> with all <b>factors</b> in the other matrix. There are no limits {{for the number of}} variables or <b>factors</b> in either <b>matrix.</b> The order of variables in one matrix may be changed to match the other and variables not shared by both matrices may be deleted...|$|R
30|$|We used a male UBM {{containing}} 1024 Gaussians and {{the order}} of total <b>factor</b> <b>matrix</b> T is 400, the corpus from the 2005 1 conv 4 w transcription index and 2006 1 conv 4 w transcription index was used to train the UBM with a total length of 17 h of speech from about 550 speakers. The corpus from the 2005 8 conv 4 w and 2006 8 conv 4 w transcription index {{was used as the}} development datasets to train the total <b>factor</b> <b>matrix</b> because sessions for each speaker consists of recordings from eight different microphones which is capable of modeling the speaker and channel variability. The development set of i-vectors was also extracted from the same corpus set which was used to train the T-Matrix. All the decision scores were given without normalization.|$|E
3000|$|... mean weightings. The PCs were {{selected}} when its eigenvalue is 1 or higher (according to the Kaiser’s rule). Weightings, regarding respective variables, {{could be calculated}} by the reverse use of the <b>factor</b> <b>matrix</b> between PCs and variables. In this study, the composite technological level was analyzed through the weightings figured out with respect to each variable.|$|E
40|$|International audienceIn {{this paper}} {{we aim to}} blindly {{calibrate}} a mobile sensor network whose sensor outputs and the sensed phenomenon are linked by a polynomial relationship. The proposed approach {{is based on a}} novel informed semi-nonnegative matrix factorization with a Vandermonde <b>factor</b> <b>matrix.</b> The proposed approach outperforms a matrix-completion-based method in a crowdsensing-like simulation of particulate matter sensing...|$|E
3000|$|... via its {{multiplication}} by the <b>factor</b> <b>matrices</b> A(n),n= 1,…,N 1, {{along its}} first N 1 modes, {{combined with a}} mode‐n resource allocation (n= 1,…,N 1) relatively to the mode‐ (N 1 + 1) of the transformed tensor [...]...|$|R
30|$|The {{basic idea}} {{in this work}} is to revert this process. The unipartite network with time-weighted Katz matrix K as the {{adjacency}} matrix of Gkatz(N,Ekatz) is known while {{the goal is to}} find the <b>factor</b> <b>matrices</b> W and H that best summarise the relational patterns in K by assigning nodes to node classes. Similar to Yu et al. 2005, this can be modelled as a non-negative matrix factorisation (NMF) problem where K is approximated by the product of the <b>factor</b> <b>matrices</b> W and H as depicted in Fig.  2. Note that in many cases there is no unique solution for and the found solution for deriving W and H and the original network can only be approximated.|$|R
40|$|Bayesian {{treatment}} of matrix factorization {{has been successfully}} applied {{to the problem of}} collaborative prediction, where unknown ratings are determined by the predictive distribution, inferring posterior distributions over user and item <b>factor</b> <b>matrices</b> that are used to approximate the user-item matrix as their product. In practice, however, Bayesian matrix factorization suffers from cold-start problems, where inferences are required for users or items about which a sufficient number of ratings are not gathered. In this paper we present a method for Bayesian matrix factorization with side information, to handle cold-start problems. To this end, we place Gaussian-Wishart priors on mean vectors and precision matrices of Gaussian user and item <b>factor</b> <b>matrices,</b> such that mean of each prior distribution is regressed on corresponding side information. We develop variational inference algorithms to approximately compute posterior distributions over user and item <b>factor</b> <b>matrices.</b> In addition, we provide Bayesian Cramér-Rao Bound for our model, showing that the hierarchical Bayesian matrix factorization with side information improves the reconstruction over the standard Bayesian matrix factorization where the side information is not used. Experiments on MovieLens data demonstrate the useful behavior of our model in the case of cold-start problems. ...|$|R
