801|371|Public
25|$|Some MOLAP methodologies {{introduce}} <b>data</b> <b>redundancy.</b>|$|E
25|$|The {{capacity}} of a hard disk drive, as reported by an operating system to the end user, is smaller than the amount stated by the manufacturer, which has several reasons: the operating system using some space, use of some space for <b>data</b> <b>redundancy,</b> and space use for file system structures. The difference in capacity reported in SI decimal prefixed units vs. binary prefixes {{can lead to a}} false impression of missing capacity.|$|E
25|$|Each vdev {{that the}} user defines, is {{completely}} independent from every other vdev, so {{different types of}} vdev can be mixed arbitrarily in a single ZFS system. If <b>data</b> <b>redundancy</b> is required (so that data is protected against physical device failure), then this is ensured by the user when they organize devices into vdevs, either by using a mirrored vdev or a RaidZ vdev. Data on a single device vdev may be lost if the device develops a fault. Data on a mirrored or RaidZ vdev will only be lost if enough disks fail {{at the same time}} (or before the system has resilvered any replacements due to recent disk failures). A ZFS vdev will continue to function in service if it is capable of providing at least one copy of the data stored on it, although it may become slower due to error fixing and resilvering, as part of its self-repair and data integrity processes. However ZFS is designed to not become unreasonably slow due to self-repair (unless directed to do so by an administrator) since one of its goals is to be capable of uninterrupted continual use even during self checking and self repair.|$|E
40|$|As XML becomes widely used, {{dealing with}} <b>redundancies</b> in XML <b>data</b> {{has become an}} {{increasingly}} important issue. Redundantly stored information can lead not just to a higher data storage cost, but also to increased costs for data transfer and data manipulation. Furthermore, such <b>data</b> <b>redundancies</b> can lead to potential update anomalies, rendering the database inconsistent. One way to avoid <b>data</b> <b>redundancies</b> is to employ good schema design based on known functional dependencies. In fact, several recent {{studies have focused on}} defining the notion of XML Functional Dependencies (XML FDs) to capture XML <b>data</b> <b>redundancies.</b> We observe further that XML databases are often “casually designed ” and XML FDs may not be determined in advance. Under such circumstances, discovering XML <b>data</b> <b>redundancies</b> (in terms of FDs) from the data itself becomes necessary and {{is an integral part of}} the schema refinement process. In this paper, we present the design and implementation of the first system, DiscoverXFD, for efficient discovery of XML <b>data</b> <b>redundancies.</b> It employs a novel XML data structure and introduces a new class of partition based algorithms. DiscoverXFD can not only be used for the previous definitions of XML functional dependencies, but also for a more comprehensive notion we develop in this paper, capable of detecting redundancies involving set elements while maintaining clear semantics. Experimental evaluations using real life and benchmark datasets demonstrate that our system is practical and scales well with increasing data size. 1...|$|R
40|$|For {{to explain}} how to design and build a medical record {{information}} system at Graha Rap maternity hospital in Tanjung Balai Karimun. The problem is medical record system is still used manual which is written on the paper by the receptionist. Disanvantages of using the system is taking long enough time to find the data of patients, it often <b>data</b> <b>redundancies</b> and also patient card loss. Data required is collected by observasing and some data That is obtained from Graha Rap maternity hospital in tanjung Balai Karimun. FAST method is used to do system development. The result of this final task are “A MEDICAL RECORD INFORMATION SYSTEM AT GRAHA RAP IN TANJUNG BALAI KARIMUN” that is computerized with this new system will speed ip processing the whole patients documents at Graha Rap maternity hospital and also to overcome the <b>data</b> <b>redundancies,</b> medical record card loss that will reduce error. The result is also speed up processing the patients data...|$|R
40|$|This study aims {{to prove}} the {{usability}} of Rough Set approach in capturing {{the relationship between the}} technical indicators and the level of Kuala Lumpur Stock Exchange Composite Index (KLCI) over time. Stock markets are affected by many interrelated economic, political, and even psychological factors. Therefore, it is generally very difficult to predict its movements. There are extensive literatures available describing attempts to use artificial intelligence techniques; in particular neural networks and genetic algorithm for analyzing stock market variations. However, drawbacks are found where neural networks have great complexity in interpreting the results; genetic algorithms create large <b>data</b> <b>redundancies.</b> A relatively new approach, the rough sets are suggested for its simple knowledge representation, ability to deal with uncertainties and lowering <b>data</b> <b>redundancies.</b> In this study, a few different discretization algorithms were used at data preprocessing. From the simulations and result produced, the rough sets approach can be a promising alternative to the existing methods for stock market prediction...|$|R
2500|$|Heterogeneous {{clusters}} {{are fully}} supported, {{and there are}} several deployment options that are available, including some that provide very high levels of <b>data</b> <b>redundancy</b> and fault tolerance. [...] This feature is marketed by IBM as Informix Flexible Grid.|$|E
2500|$|When a {{block is}} accessed, {{regardless}} of whether it is data or meta-data, its checksum is calculated and compared with the stored checksum value of what it [...] "should" [...] be. If the checksums match, the data are passed up the programming stack to the process that asked for it; if the values do not match, then ZFS can heal the data if the storage pool provides <b>data</b> <b>redundancy</b> (such as with internal mirroring), assuming that the copy of data is undamaged and with matching checksums. It is optionally possible to provide additional in-pool redundancy by specifying [...] (or [...] or more), which means that data will be stored twice (or three times) on the disk, effectively halving (or, for , reducing to one third) the storage capacity of the disk. Additionally some kinds of data used by ZFS to manage the pool are stored multiple times by default for safety, even with the default copies=1 setting.|$|E
5000|$|... mirror maps a {{mirrored}} logical device, {{while providing}} <b>data</b> <b>redundancy</b> ...|$|E
40|$|We {{study the}} design issues of {{data-centric}} XML documents where (1) {{there are no}} mixed contents, i. e., each element may have some subelements and attributes, or it may have a single value {{in the form of}} a character string, but not a mixture of strings and subelements and/or attributes, (2) the ordering of subelements is of no significance. We provide a new definition of functional dependency (FD) for XML that generalizes those published previously. We also define equality-generating dependencies (EGDs) for XML, which, to our knowledge, have not been studied before. We show how to use EGDs and FDs to detect <b>data</b> <b>redundancies</b> in XML, and propose normal forms of DTDs with respect to these constraints. We show that our normal forms are necessary and su#cient to ensure all conforming XML documents have no redundancies. In passing, we define a normal form for relational databases based on EGDs in relational systems that can help remove <b>data</b> <b>redundancies</b> across multiple relations...|$|R
40|$|We define binary {{equality}} implication constraints (BEICs) in relational {{databases and}} study the implication problem of these constraints, in particular, we provide a sound and complete set of inference rules for a common subset of BEICs. Two normal forms with respect to BEICs are defined and shown to be necessary and sufficient to prevent different types of <b>data</b> <b>redundancies</b> that {{may be caused by}} these constraints...|$|R
40|$|This paper {{presents}} an efficient lossless compression method for 4 -D medical images {{based on the}} advanced video coding scheme (H. 264 /AVC). The proposed method efficiently reduces <b>data</b> <b>redundancies</b> in all four dimensions by recursively applying multiframe motion compensation. Performance evaluations on real 4 -D medical images of varying modalities including functional magnetic resonance show an improvement in compression efficiency of up to three times that of other state-of-the-art compression methods such as 3 D-JPEG 2000...|$|R
5000|$|Facilitates {{avoidance}} of <b>data</b> <b>redundancy</b> and thus prevent data & business transaction inconsistency ...|$|E
5000|$|Reduced <b>data</b> <b>redundancy.</b> Data need {{be stored}} in only one {{location}} in a network.|$|E
5000|$|A {{specific}} {{method of}} <b>data</b> <b>redundancy</b> is duplication, {{which can be}} applied in several ways, {{as described in the}} following: ...|$|E
50|$|While data {{analysis}} {{is a common}} term for data modeling, the activity actually has {{more in common with}} the ideas and methods of synthesis (inferring general concepts from particular instances) than it does with analysis (identifying component concepts from more general ones). {Presumably we call ourselves systems analysts because no one can say systems synthesists.} Data modeling strives to bring the data structures of interest together into a cohesive, inseparable, whole by eliminating unnecessary <b>data</b> <b>redundancies</b> and by relating data structures with relationships.|$|R
5000|$|Query performance. Dimensional {{models are}} more denormalized and {{optimized}} for data querying, while normalized models seek to eliminate <b>data</b> <b>redundancies</b> and are optimized for transaction loading and updating. The predictable {{framework of a}} dimensional model allows the database to make strong assumptions about the data which may {{have a positive impact}} on performance. Each dimension is an equivalent entry point into the fact table, and this symmetrical structure allows effective handling of complex queries. Query optimization for star-joined databases is simple, predictable, and controllable.|$|R
40|$|Abstract—This paper {{presents}} an efficient lossless compression method for 4 -D medical images {{based on the}} advanced video coding scheme (H. 264 /AVC). The proposed method efficiently reduces <b>data</b> <b>redundancies</b> in all four dimensions by recursively applying multiframe motion compensation. Performance evaluations on real 4 -D medical images of varying modalities including functional magnetic resonance show an improvement in compression efficiency of up to three times that of other state-of-the-art compression methods such as 3 D-JPEG 2000. Index Terms— 3 D-JPEG 2000, 4 -D medical images, H. 264 /AVC, lossless compression...|$|R
50|$|All {{implementations}} of RAID, redundant {{array of}} independent disks, except RAID 0, {{are examples of}} a fault-tolerant storage device that uses <b>data</b> <b>redundancy.</b>|$|E
5000|$|Optional <b>data</b> <b>redundancy</b> is {{provided}} {{in the form of}} Reed-Solomon recovery records and recovery volumes, allowing reconstruction of damaged archives (including reconstruction of entirely missed volumes) ...|$|E
50|$|The {{first one}} {{corresponds}} to duplicating {{some or all}} of the program variables to introduce <b>data</b> <b>redundancy,</b> and modifying all the operators to manage the introduced replica of the variables.|$|E
40|$|This paper tackles {{the problem}} of {{accelerating}} motion estimation for video processing. A novel architecture using binary data is proposed, which attempts to reduce power consumption. The solution exploits redundant operations in the sum of absolute differences (SAD) calculation, by a mechanism known as early termination. Further <b>data</b> <b>redundancies</b> are exploited by using a run length coding addressing scheme, where access to pixels which do {{not contribute to the}} final SAD value is minimised. By using these two techniques operations and memory accesses are reduced by 93. 29 % and 69. 17 % respectively relative to a systolic array implementation...|$|R
40|$|VoIP (Voice Over IP) is {{becoming}} an alternative way of voice communications over the Internet. To better utilize voice call bandwidth, some standard compression algorithms are applied in VoIP systems. However, these algorithms affect the voice quality with high compression ratios. This paper presents a lossless data reduction technique to improve VoIP data transfer rate over the IP network. The proposed algorithm exploits the <b>data</b> <b>redundancies</b> in digitized VFs (Voice Frames) generated by VoIP systems. Performance of proposed data reduction algorithm has been presented in terms of compression ratio. The proposed algorithm will help retain the voice quality along with the improvement in VoIP data transfer rates...|$|R
50|$|To {{cope with}} a massive BTF <b>data</b> with high <b>redundancy,</b> many {{compression}} methods were proposed.|$|R
50|$|For instance, when {{customer}} {{data are}} duplicated and attached with each product bought, then redundancy of data is a known source of inconsistency since customer might appear with different values for given attribute. <b>Data</b> <b>redundancy</b> leads to data anomalies and corruption and generally {{should be avoided}} by design; applying database normalization prevents redundancy and makes the best possible usage of storage. At the same time, proper use of foreign keys can minimize <b>data</b> <b>redundancy</b> and chance of destructive anomalies. However, concerns of efficiency and convenience can sometimes result in redundant data design despite the risk of corrupting the data.|$|E
5000|$|Since {{there is}} minimal to no <b>data</b> <b>redundancy,</b> if {{a set of}} data is {{unexpectedly}} lost {{it is very hard}} to retrieve it back, in most cases it would have to be done manually.|$|E
5000|$|RAID (redundant {{array of}} {{independent}} disks) is a data storage virtualization technology that combines multiple physical {{disk drive components}} into a single logical unit {{for the purposes of}} <b>data</b> <b>redundancy,</b> performance improvement, or both.|$|E
40|$|ASEE 2014 In WSNs, {{hundreds}} of sensors collect {{data from the}} environment but these sensors have limited energy. Therefore, energy consumption is a very challenging issue {{in the design of}} WSNs. Sometimes, sensors fail as they got affected by the pressure or temperature. Such failure can lead to misleading measurements which in turn are waste of energy. As a result, data fusion is needed to overcome such confusion where it assures data’s efficiency and eliminates <b>data’s</b> <b>redundancy.</b> This paper provides an analysis of the state-of-the-art data fusion models along with their architectures. It also presents a comparison between these models to highlight the main objectives of each. In addition, it analyzes the advantages and the limitation of these models...|$|R
40|$|Designing a well-structured XML {{document}} {{is important for}} the sake of readability and maint ainabilit y. More importantly, this will avoid <b>data</b> <b>redundancies</b> and update anomalies when maintaining a large quantity of XML based documents. In this paper, we propose a method to improve XML structural design by adopting graphical notations for Document Type Definitions (GN-DTD), which is used to describe the structure of an XML document at the schema level. Multiples levels of normal forms for GN-DTD are proposed on the basis of conceptual model approaches and theories of normalization. The normalization rules are applied to transform a poorly designed XML document into a well- designed based on normalized GN-DTD, which is illustrated through examples...|$|R
40|$|We {{identify}} {{a new type}} of data integration problem that arises in functional genomics research in the context of large-scale experiments involving arrays, 2 -dimensional protein gels and mass-spectrometry. We explore the current practice of data analysis that involves repeated web queries iterating over long lists of gene or protein names. We postulate a new approach to solve this problem, applicable to data sets stored in XML format. We propose to discover <b>data</b> <b>redundancies</b> using an XML index we construct and to remove them from the results returned by the query. We combine XML indexing with queries carried out on top of relational tables. We believe our approach could support semi-automated data integration such as that required in the interpretation of large-scale biological experiments...|$|R
50|$|In communications, {{sources of}} {{interference}} are usually present, and noise is frequently a significant problem. The effects of interference are typically minimized by filtering off interfering signals {{as much as}} possible and by using <b>data</b> <b>redundancy.</b>|$|E
50|$|Heterogeneous {{clusters}} {{are fully}} supported, {{and there are}} several deployment options that are available, including some that provide very high levels of <b>data</b> <b>redundancy</b> and fault tolerance. This feature is marketed by IBM as Informix Flexible Grid.|$|E
50|$|In 2009, {{technology}} {{services company}} Unisys gave a presentation about using cryptographic splitting with storage area networks. By splitting the data into {{different parts of}} the storage area network, this technique provided <b>data</b> <b>redundancy</b> in addition to security.|$|E
40|$|Accurate {{prediction}} of suicide risk {{in mental health}} patients remains an open problem. Existing methods including clinician judgments have acceptable sensitivity, but yield many false positives. Exploiting administrative data has a great potential, but the data has high dimensionality and redundancies in the recording processes. We investigate the efficacy of three most effective randomized machine learning techniques random forests, gradient boosting machines, and deep neural nets with dropout in predicting suicide risk. Using a cohort of mental health patients from a regional Australian hospital, we compare the predictive performance with popular traditional approaches clinician judgments based on a checklist, sparse logistic regression and decision trees. The randomized methods demonstrated robustness against <b>data</b> <b>redundancies</b> and superior predictive performance on AUC and F-measure...|$|R
50|$|Numerous {{point of}} care {{documentation}} systems produce <b>data</b> <b>redundancies,</b> inconsistencies and irregularities of charting. Some electronic formats are repetitious and time-consuming. Moreover, some {{point of care}} documentation from one setting to another without a standardized pattern, {{and there are no}} guidelines for standards to documenting. Inaccessibility also causes time to be lost in searching for charts. These issues all lead to wasted time, increasing costs and uncomfortable charting. A study adopted both qualitative and quantitative methods have confirmed complexities in point of care documentation. The study has also categorized these complexities into three themes: disruption of documentation; incompleteness in charting; and inappropriate charting. As a result, these barriers limit nurses competence, motivation and confidence; ineffective nursing procedures; and inadequate nursing auditing, supervision and staff development.|$|R
40|$|The Social Semantic Web is {{the data}} {{space on the}} Web where human {{produced}} information are enriched and modeled using Semantic Web standards. This vision is enabled by the convergence between Web 2. 0 and Web 3. 0 but still, it's far from be put into practice. Moreover, this miss causes drawbacks such as <b>data</b> <b>redundancies</b> and overhead {{in the development of}} social applications. In this paper we discuss how, in pursuing the Social Semantic Web vision, a different approach for opening social data to the Semantic Web should be adopted. Furthermore, without reinventing the wheel, we will use existing technologies to define a framework for integrating Social Web and Semantic Web. This would help to reengineer or extend the Social Network infrastructures in order to fully benefit from a social Semantic Web...|$|R
