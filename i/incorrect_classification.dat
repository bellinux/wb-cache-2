112|69|Public
25|$|The {{confusion}} over their exact taxonomy and distribution {{led to an}} incomplete assessment of their conservation status by the IUCN. In 1991, with only two species recognized, Cyclemys was given the Action Plan Rating of 3 - in need of some conservation action. In the last assessment in 2000 by the Asian Turtle Trade Working Group of IUCN, the different species currently recognized under the genus Cyclemys were treated as all belonging to the species C. dentata. This resulted in the current <b>incorrect</b> <b>classification</b> {{of all of the}} species as Lower Risk/Near Threatened. Until now, the true conservation status, the actual effects of wildlife trade, deforestation, and habitat loss on individual Cyclemys species, have yet to be studied.|$|E
50|$|In its {{index for}} kanji, pages where a user might erroneously expect a kanji {{to be located}} (either because of <b>{{incorrect}}</b> <b>classification</b> or incorrect stroke count) are cross-referenced with their correct SKIP number.|$|E
50|$|In many jurisdictions, traders alone {{bear the}} legal {{responsibility}} to accurately classify their goods. Depending on {{the severity of}} the infraction, <b>incorrect</b> <b>classification</b> can result in the imposition of non-compliance penalties, border delays or seizures, or denial of import privileges.|$|E
40|$|The {{microbial}} community compositions of surface and subsurface marine sediments and sediments lining burrows of marine polychaetes and hemichordates from the North Inlet estuary (near Georgetown, S. C.) {{were analyzed by}} comparing ester-linked phospholipid fatty acid (PLFA) profiles with a back-propagating neural network (NN). The NNs were trained to relate PLFA inputs to sediment type outputs (e. g., surface, subsurface, and burrow lining) and worm species (e. g., Notomastus lobatus, Balanoglossus aurantiacus, and Branchyoasychus americana). Sensitivity {{analysis was used to}} determine which of the 60 PLFAs significantly contributed to training the NN. The NN architecture was optimized by changing the number of hidden neurons and calculating the cross-validation error between predicted and actual outputs of training and test data. The optimal NN architecture was found to be four hidden neurons with 60 -input neurons representing the 60 PLFAs, and four output neurons coding for both sediment types and worm species. Comparison of cross-validation results using NNs and linear discriminant analysis (LDA) revealed that NNs had significantly fewer <b>incorrect</b> <b>classifications</b> (2. 7 %) than LDA (8. 4 %). For the NN cross-validation, both sediment type and worm species had 3 <b>incorrect</b> <b>classifications</b> out of 112. For the LDA cross-validation, sediment type and worm species had 7 and 12 <b>incorrect</b> <b>classifications</b> out of 112, respectively. Sensitivity analysis of the trained NNs revealed that 17 fatty acids explained 50 % of variability in the data set. These PLFAs were highly different among sediments an...|$|R
40|$|When {{approximating}} a concept, probabilistic {{rough set}} models use probabilistic positive, boundary and neg-ative regions. Rules {{obtained from the}} three regions are recently interpreted as making three-way decisions, consisting of acceptance, deferment, and rejection. A particular decision is made by minimizing the cost of correct and <b>incorrect</b> <b>classifications.</b> This framework is further extended into sequential three-way decision-making, in which the cost of obtaining required evi-dence or information is also considered. 1...|$|R
50|$|There {{are also}} {{two types of}} <b>incorrect</b> <b>classifications</b> and they are {{represented}} by 01 and 10. They are called false positives (FP) when the actual value is 0 and the model predicts a 1; and false negatives (FN) when the target is 1 and the model predicts a 0. The counts of TP, TN, FP, and FN are usually kept on a table known as the confusion matrix.|$|R
5000|$|Early paleontologists {{originally}} classified many burrow fossils as {{the remains}} of marine algae, as is apparent in ichnogenera named with the -phycus suffix. Alfred Gabriel Nathorst and Joseph F. James both controversially challenged this <b>incorrect</b> <b>classification,</b> suggesting the reinterpretation of many [...] "algae" [...] as marine invertebrate trace fossils.|$|E
50|$|If viewers {{believe a}} network has {{breached}} the TV Code of Practice (an <b>incorrect</b> <b>classification</b> have been given, for example), viewers can submit a complaint to Free TV Australia, who then submit that complaint to the network. If viewers are {{dissatisfied with the}} result, they may then refer their complaint to the ACMA for an investigation.|$|E
50|$|On August 1, the TSB said it {{has taken}} samples of the oil for analysis. Both Canadian and US {{investigators}} have found the Bakken crude was not identified correctly in shipping documents, and the <b>incorrect</b> <b>classification</b> led to its volatility being underestimated. The following month, the TSB identified a defective piston in the head engine {{as the cause of}} the original fire in Nantes.|$|E
50|$|One way {{to improve}} this type of hits-based fitness {{function}} consists of expanding the notion of correct and <b>incorrect</b> <b>classifications.</b> In a binary classification task, correct classifications can be 00 or 11. The “00” representation means that a negative case (represented by “0”) was correctly classified, whereas the “11” means that a positive case (represented by “1”) was correctly classified. Classifications of the type “00” are called true negatives (TN) and “11” true positives (TP).|$|R
50|$|Margin-infused relaxed {{algorithm}} (MIRA) is {{a machine}} learning algorithm, an online algorithm for multiclass classification problems. It {{is designed to}} learn a set of parameters (vector or matrix) by processing all the given training examples one-by-one and updating the parameters according to each training example, so that the current training example is classified correctly with a margin against <b>incorrect</b> <b>classifications</b> at least as large as their loss. The change of the parameters is kept as small as possible.|$|R
40|$|Pacemaker {{diagnostics}} {{are important}} tools for patient follow-up and device programming[1]. Due to limited memory size earlier pacemakers provided diagnostic data such as histograms {{and later as}} marker chains, but no stored electrograms. These data have already improved pacemaker follow-up and trouble-shooting[2, 3]. Stored electrograms are well-known diagnostic tools in implantable cardioverter defibril-lators[4 – 6]. They are now available in new generation pacemakers. Stored EGMs allow a verification of diagnostic data, thereby elucidating possible detec-tion problems or <b>incorrect</b> <b>classifications</b> of sense...|$|R
5000|$|Chapter 11 cases {{dropped by}} 60% from 1991 to 2003. One 2007 study found {{this was because}} {{businesses}} were turning to bankruptcy-like proceedings under state law, rather than the federal bankruptcy proceedings, including those under chapter 11. Insolvency proceedings under state law, the study stated, are currently faster, less expensive, and more private, with some states not even requiring court filings. However, a 2005 study claimed the drop {{may have been due}} to an increase in the <b>incorrect</b> <b>classification</b> of many bankruptcies as [...] "consumer cases" [...] rather than [...] "business cases".|$|E
5000|$|The {{perceptron}} algorithm {{is an old}} but popular online learning algorithm that operates by a principle called [...] "error-driven learning". It iteratively improves a model by running it on training samples, then updating the model whenever it finds it has made an <b>incorrect</b> <b>classification</b> {{with respect to a}} supervised signal. The model learned by the standard {{perceptron algorithm}} is a linear binary classifier: a vector of weights [...] (and optionally an intercept term , omitted here for simplicity) that is used to classify a sample vector [...] as class [...] "one" [...] or class [...] "minus one" [...] according to ...|$|E
5000|$|Misclassification errors: {{special case}} {{used for the}} dummy regressors. If [...] is an {{indicator}} of a certain event or condition (such as person is male/female, some medical treatment given/not, etc.), then the measurement error in such regressor will correspond to the <b>incorrect</b> <b>classification</b> similar to type I and type II errors in statistical testing. In this case the error [...] may take only 3 possible values, and its distribution conditional on [...] is modeled with two parameters: , and [...] The necessary condition for identification is that , that is misclassification should not happen [...] "too often". (This idea can be generalized to discrete variables with more than two possible values.) ...|$|E
40|$|The {{author has}} {{identified}} the following significant results. The Minnesota Iron Range area {{was selected as}} one of the land use areas to be evaluated. Six classes were selected: (1) hardwood; (2) conifer; (3) water (including in mines); (4) mines, tailings and wet areas; (5) open area; and (6) urban. Initial classification results show a correct classification of 70. 1 to 95. 4 % for the six classes. This is extremely good. It can be further improved since there were some <b>incorrect</b> <b>classifications</b> in the ground truth...|$|R
40|$|In {{a global}} {{knowledge}} economy both {{business leaders and}} government policy makers have identified service exports as key outcomes of innovation based strategies and areas in which to achieve sustainable competitive advantage. This Australian based study of service exporters identified key barriers to success and potential role for government agencies. Key findings included misunderstandings over defining service exports, <b>incorrect</b> <b>classifications</b> of exporters, the need for appropriate and timely planning, challenges in obtaining financial support, need for qualified and appropriate human resources, {{and the need for}} government incentives and support aimed at increasing innovation and export orientation amongst service exporters...|$|R
2500|$|Traditionally, the {{instrument}} was simply {{referred to as the}} [...] "qin" [...] (琴) but by the twentieth century the term had come to be applied to many other musical instruments as well: the yangqin hammered dulcimer, the huqin family of bowed string instruments, and the Western piano are examples of this usage. The prefix [...] "gu-" [...] (古; meaning [...] "ancient") was later added for clarification. Thus, {{the instrument}} is called [...] "guqin" [...] today. [...] It can also be called qixian-qin (七絃琴; lit. [...] "seven-stringed zither"). Because Robert Hans van Gulik's book about the qin is called The Lore of the Chinese Lute, the guqin is sometimes inaccurately called a lute. Other <b>incorrect</b> <b>classifications,</b> mainly from music compact discs, include [...] "harp" [...] or [...] "table-harp".|$|R
50|$|The {{confusion}} over their exact taxonomy and distribution {{led to an}} incomplete assessment of their conservation status by the IUCN. In 1991, with only two species recognized, Cyclemys was given the Action Plan Rating of 3 - in need of some conservation action. In the last assessment in 2000 by the Asian Turtle Trade Working Group of IUCN, the different species currently recognized under the genus Cyclemys were treated as all belonging to the species C. dentata. This resulted in the current <b>incorrect</b> <b>classification</b> {{of all of the}} species as Lower Risk/Near Threatened. Until now, the true conservation status, the actual effects of wildlife trade, deforestation, and habitat loss on individual Cyclemys species, have yet to be studied.|$|E
5000|$|In statistics, {{typically}} a loss function {{is used for}} parameter estimation, and the event in question is some function {{of the difference between}} estimated and true values for an instance of data. The concept, as old as Laplace, was reintroduced in statistics by Abraham Wald {{in the middle of the}} 20th century. [...] In the context of economics, for example, this is usually economic cost or regret. In classification, it is the penalty for an <b>incorrect</b> <b>classification</b> of an example. In actuarial science, it is used in an insurance context to model benefits paid over premiums, particularly since the works of Harald Cramér in the 1920s. In optimal control the loss is the penalty for failing to achieve a desired value. In financial risk management the function is mapped to a monetary loss.|$|E
50|$|The {{first known}} {{reference}} to common scab of potatoes, {{dates back to}} 1825, {{but it was not}} initially thought to have a biological cause. Isolates of an organism that causes common scab of potato were first isolated by Roland Thaxter in Connecticut in 1890 and in 1892 he described the primary strain as Oospora scabies. The original culture was not maintained. In 1914 H. T. Gussow renamed the species Actinomyces scabies, noting that Oospora was an <b>incorrect</b> <b>classification</b> since the disease was not caused by a fungus. The Streptomyces genus was first proposed by Waksman and Henrici in 1943, meaning 'pliant or bent fungus'. Most species of Streptomyces are saprotrophic feeding off dead matter with relatively few being causing disease. In 1948 Waksman and Henrici used the name Streptomyces scabies to describe the species and this name was revived in 1989 by Lambert and Loria, who bought together 12 different strains that formed one homogeneous group. In 1997 the name was changed to Streptomyces scabiei following a grammatical convention as set out in Rule 12c of the International Code of Nomenclature of Bacteria. In 2007 Lambert and Loria recommended that the original name of Streptomyces scabies be kept due to its long-established use and it continues to be used today.|$|E
40|$|Machine {{learning}} models, especially {{based on}} deep architectures {{are used in}} everyday applications ranging from self driving cars to medical diagnostics. It {{has been shown that}} such models are dangerously susceptible to adversarial samples, indistinguishable from real samples to human eye, adversarial samples lead to <b>incorrect</b> <b>classifications</b> with high confidence. Impact of adversarial samples is far-reaching and their efficient detection remains an open problem. We propose to use direct density ratio estimation as an efficient model agnostic measure to detect adversarial samples. Our proposed method works equally well with single and multi-channel samples, and with different adversarial sample generation methods. We also propose a method to use density ratio estimates for generating adversarial samples with an added constraint of preserving density ratio. Comment: Update...|$|R
40|$|Abstract—When {{detecting}} {{and tracking}} vehicles in LIDAR data {{it is important}} to differentiate between the measurements from vehicles and the measurements from fixed objects such as road boundaries. In this paper we present our approach to classify the LIDAR targets as vehicles and non-vehicle objects, and detect GPS positioning errors from a probe vehicle equipped with LIDAR and DGPS. The classification approach is developed based on a world space density image generated by averaging over historical LIDAR measurements. A correlation approach is used to detect GPS positioning errors and prevent <b>incorrect</b> <b>classifications</b> of the targets caused by erroneous projections between the vehicle coordinate system and the world coordinate system. The classification approach is then modified accordingly and becomes robust to these GPS errors...|$|R
40|$|Abstract. Artificial Intelligence (AI) {{methods are}} used to build {{classifiers}} that give different levels of accuracy and solution explication. The intent {{of this paper is}} to provide a way of building a hierarchical classifier composed of several artificial neural networks (ANN’s) organised in a tree-like fashion. Such method of construction allows for partition of the original problem into several sub-problems which can be solved with simpler ANN’s, and be built quicker than a single ANN. As the subproblems extracted start to be independent of one another, this paves a way to realise the solutions for the individual sub-problems in a parallel fashion. It is observed that <b>incorrect</b> <b>classifications</b> are not random and can be therefore used to find clusters defining sub-problems. ...|$|R
40|$|Abstract—This paper {{explains}} {{the implementation of}} Receiver Operating Characterictic (ROC) graph addressing the <b>incorrect</b> <b>classification</b> of images for stegogramme and non-stegogramme classes using Pairs Analysis detection technique. The threshold value to discriminate between the two classes is identified, to reduce the rate of False Negative (FN). Index Terms—non-stegogramme, pairs analysis, ROC, stegogramme, threshold...|$|E
40|$|Clinical, histological, {{and genetic}} studies of two cases of {{isolated}} fibro-osseous lesions of the femur in adults show the overlap between monostotic fibrous dysplasia (MFD) of the proximal femur {{and the so-called}} liposclerosing myxofibrous tumor. The two cases highlight how the incomplete understanding of {{the natural history of}} MFD may result in diagnostic pitfalls or <b>incorrect</b> <b>classification</b> of individual lesions...|$|E
40|$|Aims To further {{evaluate}} mephenytoin as a probe for CYP 2 C 19 phenotyping. Methods Healthy subjects (n= 2638) were phenotyped {{using the}} urinary (S) -mephenytoin to (R) -mephenytoin ratio. This method was evaluated for (a) {{the stability of}} the S/R-ratio following sample storage, (b) the intraindividual reproducibility of the ratio, and (c) the occurrence of adverse events. Results After prolonged storage, the S/R-ratio of samples from extensive metabolisers (EM) increased up to 85 %. In 1. 5 % of the cases (1 out 66), this led to <b>incorrect</b> <b>classification</b> of phenotype. In EMs, but not in poor metabolisers (PMs), the S/R-ratio increased after acid treatment. The intraindividual reproducibility of the mephenytoin phenotyping procedure was 28 %. No major side-effects were observed and there was no relationship between the incidence of side-effects and the phenotype of the subject. Conclusions After prolonged storage the S/R-ratio significantly increased in EMs and, although low, the risk of <b>incorrect</b> <b>classification</b> should not be ignored. Our data support the use of mephenytoin as a safe drug for CYP 2 C 19 phenotyping...|$|E
5000|$|Traditionally, the {{instrument}} was simply {{referred to as the}} [...] "qin" [...] (琴) but by the twentieth century the term had come to be applied to many other musical instruments as well: the yangqin hammered dulcimer, the huqin family of bowed string instruments, and the Western piano are examples of this usage. The prefix [...] "gu-" [...] (古; meaning [...] "ancient") was later added for clarification. Thus, {{the instrument}} is called [...] "guqin" [...] today. It can also be called qixian-qin (七絃琴; lit. [...] "seven-stringed zither"). Because Robert Hans van Gulik's book about the qin is called The Lore of the Chinese Lute, the guqin is sometimes inaccurately called a lute. Other <b>incorrect</b> <b>classifications,</b> mainly from music compact discs, include [...] "harp" [...] or [...] "table-harp".|$|R
40|$|In fringe-projection and deflectometric {{measurements}} {{that use}} structured illumination, measurement artefacts {{can be observed}} around abrupt changes of surface reflectivity. This can lead to <b>incorrect</b> error <b>classification.</b> We explore {{the origin of the}} phenomenon by simulation and modelling, and offer a first attempt at correcting these errors...|$|R
40|$|Dental {{maturity}} was studied from dental panoramic radiographs of 2523 Belgian children (1255 {{girls and}} 1268 boys) aged 2 to 18 years. The {{aim was to}} compare the efficiency of two methods of age prediction: Demirjian's method, using differently weighted scores, and polynomial functions. The two methods present some differences: Demirjian is {{used to determine the}} maturity score as a function of age and polynomial functions are used to determine age {{as a function of the}} maturity score. We present, for each method, gender-specific dental maturity tables and curves for Belgian children. Girls always present advanced dental maturity compared with boys. The polynomial functions are highly reliable (0. 21 % of <b>incorrect</b> <b>classifications)</b> and the percentile method, using Belgian weighted scores, is very accurate (+/- 2. 08 years on average, between 2 and 16 years of age). status: publishe...|$|R
30|$|The names, {{data and}} test files {{required}} for the execution of C 4.5 algorithm were constructed. Then, the decision trees were generated through several commands. On the other hand, in order to evaluate {{the performance of the}} generated classifiers, we used the accuracy metric which is the rate of <b>incorrect</b> <b>classification</b> (Esfandiari et al. 2014). For this reason, the error rate was calculated for each classifier. This metric measures the proportion of errors made over the whole set of instances.|$|E
40|$|I {{consider}} {{the effect of}} repeating gamma ray burst sources on the statistic. I find that the treatment of repeating events, if applied consistently, will not affect the effectiveness of {{as a test of}} burst homogeneity. The calculation of for apparent repeating and nonrepeating source populations will be biased by the <b>incorrect</b> <b>classification</b> of faint bursts. The current practice of calculating using all bursts is valid and consistent. Comment: 9 pages, LaTeX (version 2. 09) with AAS preprint substyle (v 3. 0), SP- 93 - 7...|$|E
40|$|Part 7 : AlgorithmsInternational audienceThis paper {{presents}} {{the problem of}} building the decision scheme in the multistage pattern recognition task. This task can be presented using a decision tree. This decision tree is built in the learning phase of classification. This paper proposes a split criterion based on {{the analysis of the}} confusion matrix. Specifically, we propose the division associated with an <b>incorrect</b> <b>classification.</b> The obtained results were verified on the data sets form UCI Machine Learning Repository and one real-life data set of the computer-aided medical diagnosis...|$|E
40|$|Introduction: Detailed {{radiographic}} imaging {{seems to}} be necessary to classify acetabular fractures. The limits of Judet and Letournel classification for acetabular fractures have been shown in several studies. The use of standard pelvic radiography alone led to less intra- and interobserver agreement. The object {{of this investigation was}} to develop and to evaluate a new CT-based classification guide so that acetabular fractures could be classified without using standard Judet views. Materials and Methods: Twelve cases with acetabular fractures being picked from a data pool of the years 2005 / 2006 have been examined. Eight characteristic CT scans (5 axial scans, 2 coronar scans and 1 sagittal scan) and the standard pelvic radiography have been chosen for each case. Fourteen members of the study group pelvic from the German section of AO classified these cases according to Judet and Letournel classification for acetabular fractures. The results were compared to reference classification. It was differentiated between agreement to classification and differing classification depending from operative access to the acetabular. Training level also was considered. Results: In 90 of 167 classifications (54 %) there was a totally agreement to the reference classification – 168 classifications were possible. 69 cases (41 %) showed a different type of fracture but there had been no change of therapy or operative access to the fracture. All in all 159 (95 %) good to excellent classification results were achieved. Totally <b>incorrect</b> <b>classifications</b> being described by a changing in treatment the fracture were counted 8 times (5 %). According to the training level deputies agreed in 54 %, residents in 53 %. <b>Incorrect</b> <b>classifications</b> by deputies were counted in 7. 5 %. 93 % of the interviewees asked for more CT scans for the classification. Conclusion: The demonstrated CT-based classification guide presents an adaptation to the common standard of diagnostic radiographic imaging for acetabular fractures. It implies a step to an easier classification. It is useful to evaluate the extent of injury and fractures’ characteristics. For a detailed classification more scans and reconstructions seem to be necessary...|$|R
40|$|This thesis {{will address}} {{classification}} problems with two sources of cost: {{the cost of}} acquiring feature values {{and the cost of}} <b>incorrect</b> <b>classifications.</b> In particu- lar, I address problems with feature costs and instance-dependent misclassification costs. Many real-world applications, such as medical diagnosis, contain both feature acquisition costs and instance-dependent misclassification costs. The goal of my re- search is to minimize the total cost of classifying an unknown instance. This goal is accomplished with a new approach: Simultaneous Feature Acquisition and Cost Estimation (SFACE), which combines feature acquisition methods with a regression algorithm that estimates misclassification costs. The estimated cost values are used to estimate the expected cost reduction for the acquisition of each feature. SFACE is evaluated by comparing the total cost of operation to the cost incurred by existing cost-insensitive, cost-sensitive, and feature acquisition algorithms. The results show that SFACE results in lower total cost for the tested datasets...|$|R
40|$|Foreground {{segmentation}} is {{a fundamental}} first processing stage for vision systems which monitor real-world activity. In this paper we consider the problem of achieving robust segmentation in scenes where {{the appearance of the}} background varies unpredictably over time. Variations may be caused by processes such as moving water, or foliage moved by wind, and typically degrade the performance of standard per-pixel background models. Our proposed approach addresses this problem by modeling homogeneous regions of scene pixels as an adaptive mixture of Gaussians in color and space. Model components are used to represent both the scene background and moving foreground objects. Newly observed pixel values are probabilistically classified, such that the spatial variance of the model components supports correct classification even when the background appearance is significantly distorted. We evaluate our method over several challenging video sequences, and compare our results with both per-pixel and Markov Random Field based models. Our results show the effectiveness of our approach in reducing <b>incorrect</b> <b>classifications...</b>|$|R
