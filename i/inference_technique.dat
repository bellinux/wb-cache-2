290|1037|Public
50|$|Variational {{message passing}} (VMP) is an {{approximate}} <b>inference</b> <b>technique</b> for continuous- or discrete-valued Bayesian networks, with conjugate-exponential parents, developed by John Winn. VMP was {{developed as a}} means of generalizing the approximate variational methods used by such techniques as Latent Dirichlet allocation and works by updating an approximate distribution at each node through messages in the node's Markov blanket.|$|E
5000|$|A {{vector space}} {{model can be}} used as a {{strategy}} for classifying the candidate answers. Check if the answer is of the correct type as determined in the question type analysis stage. <b>Inference</b> <b>technique</b> can also be used to validate the candidate answers. A score is then given to each of these candidates according to the number of question words it contains and how close these words are to the candidate, the more and the closer the better. The answer is then translated into a compact and meaningful representation by parsing. In the previous example, the expected output answer is [...] "1st Oct." ...|$|E
40|$|A new fuzzy <b>inference</b> <b>technique</b> is {{presented}} to replace the conventional fuzzy inference process of type- 2 fuzzy logic systems. Because conventional type- 2 fuzzy logic systems demand {{a large amount of}} memory, they cannot be used by most embedded systems, which do not have enough memory space. To overcome this problem, a new fuzzy <b>inference</b> <b>technique</b> for singleton type- 2 fuzzy logic systems {{is presented}} in this paper which designs mapping functions from input variables to firing sets and brings out the firing sets directly without using as much memory...|$|E
40|$|We {{argue that}} many {{problems}} in robotics {{arise from the}} difficulty of integrating multiple knowledge representation and <b>inference</b> <b>techniques.</b> We describe an architecture that integrates disparate reasoning, planning, sensation and mobility algorithms by composing them from strategies for managing mental simulations. Since simulations are conducted by modules that include high-level knowledge representation and <b>inference</b> <b>techniques</b> in addition to algorithms for sensation and reactive mobility, cognition, perception and action are continually integrated. An implemented robot using this framework in object-tacking and human–robot interaction tasks demonstrates that knowledge representation and <b>inference</b> <b>techniques</b> enable more complex and flexible robot behavior...|$|R
40|$|AbstractWe {{explore the}} role of {{lightweight}} <b>inference</b> <b>techniques</b> in creating highly automated engineering support environments {{for the development of}} computer-based systems. Lightweight <b>inference</b> <b>techniques</b> are scalable methods for automated reasoning. We outline the types of automation that would be enabled by effective lightweight inference capabilities and survey some promising approaches to realizing the needed capabilities...|$|R
3000|$|Also, {{future work}} may {{be carried out}} to test the system {{behaviour}} toward other fuzzy <b>inference</b> <b>techniques.</b> In our model, we have used Mamdani-type fuzzy inference system. However, there are other <b>inference</b> <b>techniques</b> that can be tested on the system and evaluate its performance. other popular common methods of deductive inference for fuzzy systems (Ross 2005) that can be tested on this model are: [...]...|$|R
40|$|In {{this paper}} {{we present a}} {{framework}} for the definition of generic and thus reusable tactics. We present {{an extension of the}} window <b>inference</b> <b>technique</b> which is the formal basis of a hierarchical, problem-reduction style of reasoning. The window <b>inference</b> <b>technique</b> is analyzed and general reasoning rules are separated from logic specific rules. The separation between logic specific and general rules is used to define a framework offering generic window reasoning rules to allow for the definition of generic tactics, where logic specific parts are separated from the tactic level...|$|E
40|$|Abstract- In {{this paper}} we {{investigate}} the fault diagnosis problem in IP networks. We provide a lower bound {{on the average}} number of probes per edge using variational <b>inference</b> <b>technique</b> proposed in the context of graphical models under noisy probe measurements. To obtain the bounds, we construct a graphical model using Bayesian networks. The advantages of the variational <b>inference</b> <b>technique</b> are the explicit choices of a simplifying conjugate function and a computationally tolerable approximation to address the intractable detection problem for large networks. We propose an entropy lower (EL) bound by drawing similarities between the coding problem over binary symmetric channel and the diagnosis problem and compare it against the variational lower bound. In addition, we discuss scalable and non-scalable scenarios in the presence of noise. Simulation results demonstrate that indeed the variational <b>inference</b> <b>technique</b> can provide a linear growth of {{the average number of}} probes per edge {{as a function of the}} network size. 1...|$|E
40|$|We {{suggest a}} fresh {{approach}} to the modeling of the human cardiovascular system. Taking advantage of a new Bayesian <b>inference</b> <b>technique,</b> {{able to deal with}} stochastic nonlinear systems, we show that one can estimate parameters for models of the cardiovascular system directly from measured time series. We present preliminary results of inference of parameters of a model of coupled oscillators from measured cardiovascular data addressing cardiorespiratory interaction. We argue that the <b>inference</b> <b>technique</b> offers a very promising tool for the modeling, able to contribute significantly towards the solution of a long standing challenge [...] development of new diagnostic techniques based on noninvasive measurements...|$|E
40|$|Component-based {{software}} development has increasingly gained popularity in industry. While correct component usage {{is critical to}} successful reuse of components, the expected component usage is rarely specified explicitly. To address this issue, one recent area of research has been to infer specifications of protocols or sequencing constraints using both static and dynamic techniques. This paper explores the research area of software component protocol inference {{with a focus on}} dynamic <b>inference</b> <b>techniques.</b> A framework is proposed to compare the existing dynamic <b>inference</b> <b>techniques.</b> Along with the framework, some static <b>inference</b> <b>techniques</b> are covered in brief discussions. In the end, directions for future work are suggested to push {{the state of the art}} forward...|$|R
40|$|Sudoku is a {{very simple}} and {{well-known}} puzzle that has achieved international popularity in the recent past. This paper addresses the problem of encoding Sudoku puzzles into conjunctive normal form (CNF), and subsequently solving them using polynomial-time propositional satisfiability (SAT) <b>inference</b> <b>techniques.</b> We introduce two straightforward SAT encodings for Sudoku: the minimal encoding and the extended encoding. The minimal encoding suffices to characterize Sudoku puzzles, whereas the extended encoding adds redundant clauses to the minimal encoding. Experimental results demonstrate that, for thousands of very hard puzzles, <b>inference</b> <b>techniques</b> struggle to solve these puzzles when using the minimal encoding. However, using the extended encoding, unit propagation is able to solve about half of our set of puzzles. Nonetheless, for some puzzles more sophisticated <b>inference</b> <b>techniques</b> are required. ...|$|R
40|$|Grammatical inference, an {{important}} field of syntactic pattern recognition, is finding wider acceptance in many practical applications like computation biology. In this work we show {{the use of}} grammatical <b>inference</b> <b>techniques</b> in identifying pseudoknots in the RNA secondary structures. Identification of RNA secondary structure is among the few structure identification problems that can be solved satisfactorily in polynomial time and data. We propose an Infer-Test model to identify the pseudoknots. This model uses the Terminal Distinguishable Even Linear Language <b>inferencing</b> <b>technique</b> to identify pseudoknots in the RNA secondary structures. ...|$|R
40|$|Identification of continuous-time systems {{typically}} present problems due to {{the facts}} that one cannot, in general, measure the time derivatives of the signals and, also, the sampled nature of the data. We utilise indirect inference as the underlying principle for continuous time system identification. Indirect inference has been widely used in the econometrics area for time series modeling. Here we adapt the indirect <b>inference</b> <b>technique</b> to include systems with an exogenous input {{and apply it to}} the problem of system identification. We use an example problem posed by Rao and Garnier to show the effectiveness of the indirect <b>inference</b> <b>technique</b> when contrasted to other continuous-time methods of identification...|$|E
40|$|The most {{effective}} complete method for testing propositional satisfiability (SAT) is backtracking search. Recent {{research suggests that}} adding more inference to SAT search procedures can improve their performance. This paper presents two ways to combine neighbour resolution (one such <b>inference</b> <b>technique)</b> with search...|$|E
40|$|A {{method for}} {{inferring}} causal directions based on errors-in-variables models where both the cause vari-able {{and the effect}} variable are observed with measure-ment errors is concerned in this paper. The <b>inference</b> <b>technique</b> and estimation algorithms are given. Some experiments are included to illustrate our method...|$|E
40|$|International audienceContext: Finite State Machine (FSM) {{inference}} from execution traces {{has received}} a lot of attention over the past few years. Various approaches have been explored, each holding different properties for the resulting models, but the lack of standard benchmarks limits the ability of comparing the proposed techniques. Evaluation is usually performed on a few case studies, which is useful for assessing the feasibility of the algorithm on particular cases, but fails to demonstrate effectiveness in a broad context. Consequently, understanding {{the strengths and weaknesses of}} <b>inference</b> <b>techniques</b> remains a challenging task. Objective: This paper proposes CARE, a general, approach-independent, platform for the intensive evaluation of FSM <b>inference</b> <b>techniques.</b> Method: Grounded in a program specification scheme that provides a good control on the expected program structures, it allows the production of large benchmarks with well identified properties. Results: The CARE platform demonstrates the following features: (1) providing a benchmarking mechanism for FSM <b>inference</b> <b>techniques,</b> (2) allowing analysis of existing techniques w. r. t. a class of programs and/or behaviors, and (3) helping users in choosing the best suited approach for their objective. Moreover, our extensive experiments on different FSM <b>inference</b> <b>techniques</b> highlight that they do not behave in the same manner on every class of program. Characterizing different classes of programs thus helps understanding the strengths and weaknesses of the studied techniques. Conclusion: Experiments reported in this paper show examples of use cases that demonstrate the ability of the platform to generate large and diverse sets of programs, which allows to carry out meaningful <b>inference</b> <b>techniques</b> analysis. The analysis strategies the CARE platform offers open new opportunities for program behavior learning, particularly in conjunction with model checking techniques. The CARE platform is available at [URL]...|$|R
40|$|The {{present study}} {{is an attempt to}} {{investigate}} the comparative effects of lexical translation and lexical <b>inferencing</b> <b>techniques</b> on Female intermediate EFL learners’ vocabulary retention. For this purpose, 90 female learners attending the Jahad Daneshgahi Center in Qom took a piloted sample KET test, 60 of whom were selected as homogenous learners. They were randomly divided into two experimental groups-one learning new vocabulary items through lexical translation technique and the other with the lexical <b>inferencing</b> <b>technique.</b> They were given a pre-test on vocabulary to ensure that the participants had no prior knowledge of the target words. Then all participants in both groups were taught using the same material and received the same amount of instruction. The only difference was for teaching of new lexical items. One experimental group was taught mainly through the lexical translation technique while the other experimental group learned by the lexical <b>inferencing</b> <b>technique.</b> After conducting the treatment, a post-test was administered to both groups in order to measure the students' ability in the retention of the lexical items taught through lexical translation and lexical <b>inferencing</b> <b>techniques</b> after a two-week interval. The analysis of the test scores using independent sample t-test revealed that the lexical inferencing group significantly outperformed the lexical translation group on the retention of the lexical items suggesting its benefits for teaching new words. Findings provide insights to teachers as well as students on how to best approach learning new lexical items...|$|R
40|$|Abstract. Planning is {{concerned}} with the development of solvers {{for a wide range of}} models where actions must be selected for achieving goals. In these models, actions may be deterministic or not, and full or partial sensing may be available. In the last few years, significant progress has been made, resulting in algorithms that can produce plans effectively in a variety of settings. These developments have to do with the formulation and use of general <b>inference</b> <b>techniques</b> and transformations. In this invited talk, I’ll review the <b>inference</b> <b>techniques</b> used for solving individual planning instances from scratch, and discuss the use of learning methods and transformations for obtaining more general solutions. ...|$|R
40|$|Many {{business}} {{elements are}} used to develop credit scorecards. Reject inference, related {{to the issue of}} sample bias, {{is one of the key}} processes required to build relevant application scorecards and is vital in creating successful scorecards. Reject inference is used to assign a target class (that is, a good or bad designation) to applications that were rejected by the financial institution and to applicants who refused the financial institution’s offer. This paper uses real-world data to present an example of using memorybased reasoning as a reject <b>inference</b> <b>technique.</b> SAS ® Enterprise Miner ™ software is used to perform the analysis. The paper discusses the technical concepts in reject inference and the methodology behind using memory-based reasoning as a reject <b>inference</b> <b>technique.</b> Several misclassification measures are reported to determine how well memory-based reasoning performs as a reject <b>inference</b> <b>technique.</b> In addition, a macro to determine how to pick the number of neighbors for the memory-based reasoning technique is given and discussed. This macro is implemented in a SAS ® Enterprise Miner™ code node. OVERVIEW OF SCORECARDS Credit scorecard development is a method of modeling potential risk of credit applicants. It involves using different statistical techniques and past historical data to create a scorecard that financia...|$|E
40|$|Inference in Markov Decision Processes has {{recently}} received interest {{as a means}} to infer goals of an observed action, policy recognition, and also as a tool to compute policies. A particularly interesting aspect of the approach is that any existing <b>inference</b> <b>technique</b> in DBNs now becomes available for answering behavioral questions–including those on continuous, factorial, or hierarchical state representations. Here we present an Expectation Maximization algorithm for computing optimal policies. Unlike previous approaches we can show that this actually optimizes the discounted expected future return for arbitrary reward functions and without assuming an ad hoc finite total time. The algorithm is generic in that any <b>inference</b> <b>technique</b> can be utilized in the E-step. We demonstrate this for exact inference on a discrete maze and Gaussian belief state propagation in continuous stochastic optimal control problems. 1...|$|E
3000|$|... |z 1 :t) be. This problem {{arises from}} {{the fact that some of}} the {{stochastic}} processes involved in the multiple object tracking model are nonlinear or/and non-Gaussian [29]. To overcome this problem, an approximate <b>inference</b> <b>technique</b> is introduced in the next section that allows to obtain an accurate suboptimal solution.|$|E
40|$|In this paper, {{we present}} an {{approach}} for approximate maximum likelihood parameter learning in discriminative field models, {{which is based}} on approximating true expectations with simple piecewise constant functions constructed using <b>inference</b> <b>techniques.</b> Gradient ascent with these updates shows interesting weak-convergence behavior which is tied closely to the number of errors made during inference. The performance of various approximations was evaluated with different <b>inference</b> <b>techniques</b> showing that the learned parameters lead to good classification performance so long as the method used for approximating the gradient is consistent with the inference mechanism. The proposed approach is general enough to be used for conditional training of conventional MRFs. ...|$|R
40|$|Modern {{reasoning}} {{is based on}} <b>inference</b> <b>techniques</b> such as induction, deduction, abduction, subsumption, classification and recognition. These <b>inference</b> <b>techniques</b> are very inefficient when applied to large amounts of knowledge such as ones employed by contemporary unmanned spacecraft. For efficient reasoning, we aim at knowledge representation based on special ambient trees determining special knowledge contexts to help such spacecraft retrieve context-relevant knowledge and perform deductive reasoning, which would not be otherwise highlighted. Contexts via their ambient trees provide {{a sort of a}} condensed and explicit symbolic representation of the world. This representation is cleaned from the overwhelming information that is non-relevant to the context and thus, it provides for efficient models of situations to reason about...|$|R
40|$|Recent {{advances}} in wearable sensing and computing devices and in fast probabilistic <b>inference</b> <b>techniques</b> make possible the fine-grained estimation of a person’s activities over {{extended periods of}} time [6]. Such technologies enable applications ranging from context aware computing to support for cognitivel...|$|R
40|$|The {{problem of}} the {{non-linear}} regression analysis is considered. The algorithm of the inductive model generation is described. The regression model is a superposition of given smooth functions. To estimate the model parameters two-level Bayesian <b>Inference</b> <b>technique</b> was used. It introduces hyperparameters, which describe the distribution function of the model parameters...|$|E
40|$|We find {{evidence}} for decaying magnons at strong magnetic {{field in the}} square lattice spin- 1 / 2 Heisenberg antiferromagnet. The results are obtained using Quantum Monte Carlo simulations combined with a Bayesian <b>inference</b> <b>technique</b> to obtain dynamics and are consistent with predictions from spin wave theory. Comment: 4 pages, 5 figure...|$|E
40|$|Abstract — Verifying the {{accuracy}} of a passive measurementsbased <b>inference</b> <b>technique</b> under all possible network scenarios is a difficult challenge- the measurement point has limited observability of events along the path, and monitored paths can exhibit {{a wide range of}} network properties (packet loss, reordering, end-end delay, route changes). In this paper, we propose and apply formal verification techniques to exhaustively verify the correctness of an <b>inference</b> <b>technique.</b> We apply this approach to the problem of inferring packet retransmissions and reorderings from passively observed packets at a single measurement point. We define classification rules for this inference problem and, through a combination of model-checking and formal reasoning, uncover all possible events in the network for which the rules produce incorrect inferences. Our work is novel in its use of formal verification tools for evaluating inference techniques in network measurements. I...|$|E
40|$|Many {{theories}} of human cognition postulate {{that people are}} equipped with a repertoire of strategies to solve the tasks they face. This theoretical framework of a cognitive toolbox provides a plausible account of intra- and interindividual differences in human behavior. Unfortunately, it is often unclear how to rigorously test the toolbox framework. How can a toolbox model be quantitatively specified? How can the number of toolbox strategies be limited to prevent uncontrolled strategy sprawl? How can a toolbox model be formally tested against alternative theories? The authors show how these challenges can be met by using Bayesian <b>inference</b> <b>techniques.</b> By means of parameter recovery simulations and the analysis of empirical data {{across a variety of}} domains (i. e., judgment and decision making, children's cognitive development, function learning, and perceptual categorization), the authors illustrate how Bayesian <b>inference</b> <b>techniques</b> allow toolbox models to be quantitatively specified, strategy sprawl to be contained, and toolbox models to be rigorously tested against competing theories. The authors demonstrate that their approach applies at the individual level but can also be generalized to the group level with hierarchical Bayesian procedures. The suggested Bayesian <b>inference</b> <b>techniques</b> represent a theoretical and methodological advancement for toolbox {{theories of}} cognition and behavior. (PsycINFO Database Record (c) 2012 APA, all rights reserved) ...|$|R
40|$|The {{objectives}} of the research are (1) to investigate whether there was difference of studentsâ€™ reading comprehension achievement between those taught through predictive technique and those taught through making <b>inferences</b> <b>technique,</b> and (2) to determine {{which one of the}} two techniques was more effective for teaching reading comprehension. The researcher applied pre-test and post-test control group design. This experimental method dealt with two groups: the experimental class and control class. The sample of the research was the second grade of SMAN 1 Kotagajah. The findings of the research revealed that there was difference of studentsâ€™ reading comprehension achievement between those taught through predictive technique and those taught through making <b>inferences</b> <b>technique.</b> In addition, predictive technique was more effective than making <b>inferences</b> <b>technique</b> to encourage the studentsâ€™ motivation to be more active during the process of learning reading. It {{can be seen from the}} results of post-test in the experimental class was 81. 50 which higher than the mean score of studentsâ€™ post-test in the control class which was 72. 44, with mean difference of score was 9. 06. The value of two tailed significant was 0. 000. It means that H 0 was rejected and H 1 was accepted since 0. 000 < 0. 05. ...|$|R
40|$|We {{propose a}} {{contribution}} to the PKDD- 2002 discovery challenge on the hepatitis dataset. This challenge aims at discovering regularities over patients strucked down by chronic hepatitis. Our approach addresses the problem of multi-relational Data Mining, extracting probabilistic tree patterns from a database using Grammatical <b>Inference</b> <b>techniques...</b>|$|R
40|$|The maximum entropy {{procedure}} is an axiomatically derived <b>inference</b> <b>technique,</b> producing density functions from moment constraints. In this paper {{we consider the}} implications of a hypothesized neuronal prediction based on the maximum entropy principle and investigate the computational and biological consequences of this hypothesis. The ultimate goal of the paper is to translate, as generally as possible, the maximum entropy <b>inference</b> <b>technique</b> into the context of neural computation. I. INTRODUCTION In 1989 and 1990 [2, 5], we introduced the hypothesis that certain neurons could perform probability inference via the maximum entropy (M. E.) technique, an axiomatically derived inference procedure [1, 6]. As introduced, this neural maximum entropy inference (N. M. E.) hypothesis concentrated its exposition on a specific example. This example was partially inspired by: (1) {{a limited number of}} physiological studies of long-term potentiation/depression from the hippocampus; (2) a specific, bu [...] ...|$|E
30|$|Casale et al. [34] {{presents}} an optimization-based <b>inference</b> <b>technique</b> that is formulated as a robust linear regression problem {{that can be}} used with both closed and open queueing network performance models. It uses aggregate measurements (i.e., system throughput and utilization of the servers), commonly retrieved from log files, in order to estimate service times.|$|E
40|$|Qualitative {{simulation}} {{is a key}} <b>inference</b> <b>technique</b> of model-based {{reasoning and}} is successfully demonstrated in areas like monitoring, fault-diagnosis and design. For industrial applications, embedded qualitative simulation is required, i. e., the qualitative simulator is coupled with the physical process {{by a set of}} sensors and actuators. Often the qualitative simulator must additionally satisfy real-time constraints...|$|E
40|$|This paper {{presents}} new <b>inference</b> <b>techniques</b> {{about language}} {{based on my}} theory of linguistic antinomies––pairs of apparently contradictory statements that complement each other. Since the cells, that is, the basic units of language are signs, the antinomies of language are the antinomies of signs and sign combinations...|$|R
30|$|The earlier {{analysis}} {{relied on}} aggregate data, albeit constructed from micro-level CPS data. 17 To assess the robustness {{of the earlier}} results, I additionally test for impacts on the fertility outcomes of Miami residents using a more traditional difference-in-differences estimator with <b>inference</b> <b>techniques</b> based on CPS micro data.|$|R
50|$|Inference in MLNs can be {{performed}} using standard Markov network <b>inference</b> <b>techniques</b> over the minimal subset of the relevant Markov network required for answering the query. These techniques include Gibbs sampling, which is effective but may be excessively slow for large networks, belief propagation, or approximation via pseudolikelihood.|$|R
