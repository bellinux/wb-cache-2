84|10000|Public
25|$|DRDO {{has worked}} {{extensively}} on <b>high</b> <b>speed</b> <b>computing</b> given its ramifications {{for most of}} its defence projects. These include supercomputers for computational flow dynamics, to dedicated microprocessor designs manufactured in India for flight controllers and the like, to <b>high</b> <b>speed</b> <b>computing</b> boards built around Commercial Off The Shelf (COTS) components, similar to the latest trends in the defence industry.|$|E
500|$|When Ulam {{returned}} to Los Alamos, his attention {{turned away from}} weapon design and toward {{the use of computers}} to investigate problems in physics and mathematics. [...] With John Pasta, who helped Metropolis to bring MANIAC on line in March 1952, he explored these ideas in a report [...] "Heuristic Studies in Problems of Mathematical Physics on <b>High</b> <b>Speed</b> <b>Computing</b> Machines", which was submitted on 9 June 1953. It treated several problems that cannot be addressed within the framework of traditional analytic methods: billowing of fluids, rotational motion in gravitating systems, magnetic lines of force, and hydrodynamic instabilities.|$|E
500|$|... as {{far back}} as the 1970s, Congressman Gore {{promoted}} the idea of high-speed telecommunications as an engine for both economic growth and the improvement of our educational system. He was the first elected official to grasp the potential of computer communications to have a broader impact than just improving the conduct of science and scholarship [...] the Internet, as we know it today, was not deployed until 1983. When the Internet was still in the early stages of its deployment, Congressman Gore provided intellectual leadership by helping create the vision of the potential benefits of <b>high</b> <b>speed</b> <b>computing</b> and communication.|$|E
5000|$|He {{has been}} {{instrumental}} in founding the field of Neuromorphic Photonics and developing the [...] "photonic neuron", a <b>high</b> <b>speed</b> optical <b>computing</b> device modeled on neural networks, as well as integrated optical circuits to improve wireless signal quality by cancelling radio interference.|$|R
3000|$|... [0,_e 11]. We {{have found}} it to be {{convenient}} to investigate our series only for finite sums. More powerful equipments with <b>higher</b> <b>speed</b> can easily <b>compute</b> the more complicated infinite series in a similar manner.|$|R
40|$|The Cell Broadband Enginee (Cell/B. E.) {{processor}} {{was developed}} by Sony, Toshiba, and IBM engineers to deliver a high-speed, high-performance, multicore processor that brings supercomputer performance via a custom system-on-a-chip (SoC) implementation. To achieve its goals, the Cell/B. E. processor uses an innovative architecture, new circuit design styles, and hierarchical integration and verification techniques. The Cell/B. E. processor design point was also targeted at high-volume manufacturing. To meet highvolume manufacturing requirements, the chip was designed {{so that it could}} be completely tested in less than 26 seconds. In addition to the above items, the Cell/B. E. processor was designed with the ‘‘triple design constraints’’ of maximizing performance while minimizing area and power consumed. The initial application was targeted at real-time systems that require high-speed data movement for both on-chip and off-chip transfers. This application also required very <b>high</b> <b>speed</b> <b>compute</b> and real-time response processes...|$|R
50|$|DRDO {{has worked}} {{extensively}} on <b>high</b> <b>speed</b> <b>computing</b> given its ramifications {{for most of}} its defence projects. These include supercomputers for computational flow dynamics, to dedicated microprocessor designs manufactured in India for flight controllers and the like, to <b>high</b> <b>speed</b> <b>computing</b> boards built around Commercial Off The Shelf (COTS) components, similar to the latest trends in the defence industry.|$|E
50|$|While {{the major}} focus of {{research}} is on Information Systems, the departmental research interests span the Computer Science spectrum, as well. Recent publications cover topics in <b>High</b> <b>Speed</b> <b>Computing,</b> Human-Computer Interaction, Information Security, Networked Embedded System, Natural Language Processing, Software Engineering, VLSI, etc.|$|E
50|$|Brobst {{authored}} {{journal and}} conference {{papers in the}} fields of data management and parallel computing environments. He was a contributing editor for Intelligent Enterprise Magazine and published technical articles in The International Journal of <b>High</b> <b>Speed</b> <b>Computing,</b> Communications of the ACM, The Journal of Data Warehousing, Enterprise Systems Journal, DM Review, Database Programming and Design, DBMS Tools & Techniques, DB2 Magazine, Oracle Magazine, Teradata Magazine and many others.|$|E
40|$|Thermocouple probe {{incorporates}} small jet of {{inert gas}} to cool thermocouple. To measure gas temperatures, cooling jet {{is turned off}} momentarily, allowing thermocouple to heat up to near its melting point, then cooling is reapplied. Heating curve is recorded by <b>high</b> <b>speed</b> digital system. <b>Computing</b> system extrapolates final temperature thermocouple would have attained...|$|R
40|$|This paper {{presents}} a fast algorithm for generating Hilbert address for hardware implementation with low storage requirement. This work avoids {{the use of}} recursive functions as compared with Quinqueton's work, and eliminates complicated bit manipulations as proposed by Butz, and does not use any look-up-tables as implemented by Kamata. Each address can be obtained in one clock cycle by one-to-one mapping using a simple incremental counter and cascading of multiplexers. The merit of our method is that it achieves very <b>high</b> <b>speed</b> when <b>computing</b> the Hilbert address which requires little memory storage...|$|R
40|$|This paper {{describes}} a computational architecture for an interconnected <b>high</b> <b>speed</b> distributed <b>computing</b> system for generalized bilateral control of robot arms. The key method {{of the architecture}} {{is the use of}} fully synchronized, interrupt driven software. Since an objective of the development is to utilize the processing resources efficiently, the synchronization is done in the hardware level to reduce system software overhead. The architecture also achieves a balaced load on the communication channel. The paper also describes some architectural relations to trading or sharing manual and automatic control...|$|R
50|$|He {{was awarded}} an UNESCO Special Fellowship {{on the study}} of <b>High</b> <b>Speed</b> <b>Computing</b> Machines in the United States of America and the United Kingdom during 1949-50 and worked at Harvard University, Institute of Advanced Studies, Princeton, USA and at the Mathematical Laboratory, University of Cambridge, U.K. During his time at the Institute of Advanced Studies at Princeton, he came in close contact with eminent {{physicists}} and mathematicians, such as Albert Einstein, Wolfgang Pauli, John Von Neumann. And, attended lectures of Niels Bohr and Robert Oppenheimer.|$|E
5000|$|After {{stepping}} down {{as director of}} the National Science Foundation, Bloch joined the Council on Competitiveness as its first distinguished fellow. The IEEE Computer Society awarded him the Computer Pioneer Award in 1993 for <b>high</b> <b>speed</b> <b>computing.</b> In 2002, the National Science Board honored Bloch with the Vannevar Bush Award. He was made a Fellow of the Computer History Museum in 2004 [...] "for engineering management of the IBM Stretch supercomputer, and of the Solid Logic Technology used in the IBM System/360, which revolutionized the computer industry." ...|$|E
5000|$|Reconfigurable {{computing}} is {{a computer}} architecture combining some of the flexibility of software with the high performance of hardware by processing with very flexible <b>high</b> <b>speed</b> <b>computing</b> fabrics like field-programmable gate arrays (FPGAs). The principal difference when compared to using ordinary microprocessors {{is the ability to}} make substantial changes to the datapath itself in addition to the control flow. On the other hand, the main difference with custom hardware, i.e. application-specific integrated circuits (ASICs) is the possibility to adapt the hardware during runtime by [...] "loading" [...] a new circuit on the reconfigurable fabric.|$|E
40|$|The AR-PDA project {{develops}} {{a framework that}} allows the use of mobile AR applications for the consumer market. Adapting existing technologies for PDAs (personal digital assistants), <b>high</b> <b>speed</b> wireless <b>computing,</b> computer graphics and computer vision, AR services will be provided at low costs. In this paper we discuss the suitability of mobile devices for AR, address problems of wireless network transmissions and latencies. We present a first running prototype of a wireless PDA with AR features demonstrating a proof-of-concept for a mobile AR system that addresses the consumer market...|$|R
5000|$|Von Neumann {{describes}} a detailed {{design of a}} [...] "very <b>high</b> <b>speed</b> automatic digital <b>computing</b> system." [...] He divides it into six major subdivisions: a central arithmetic part, CA, a central control part, CC, memory, M, input, I, output, O, and (slow) external memory, R, such as punched cards, Teletype tape, or magnetic wire or steel tape.|$|R
50|$|At Deeds' {{direction}} he conducted research to implement pioneering ideas {{regarding the use}} of tubes and circuitry in counting devices, with the idea of developing <b>high</b> <b>speed</b> mathematical <b>computing</b> machines to augment or replace the Company's mechanical machines. The idea of applying electronic counting to calculating mechanisms occurred to him when reading of a thyratron (gas-filled tube) counting ring of five places (5 digits, not five orders) developed by British scientist Dr C. E. Wynn-Williams. As a result Desch's lab received an introduction to and work with the MIT Electrical Engineering Department led by Vannevar Bush.|$|R
50|$|Novatium began {{operations}} in 2004, and has developed thin embedded devices capable of operating on Windows, Linux, Macintosh, Android and Solaris platform without {{any change in}} device. The company has partnered with Tata Teleservices Limited to launch Novatium Navigator with Tata Photon plus. Thus, Novatium is India's first cloud computing on wireless broadband service. The services include <b>high</b> <b>speed</b> <b>computing</b> experience {{with an emphasis on}} internet, gaming, digital entertainment, online education, telephony and business productivity. Bharti Airtel, an Asian integrated telecom service provider, also entered into a strategic partnership with Novatium to help expand the broadband market in India.|$|E
5000|$|When Ulam {{returned}} to Los Alamos, his attention {{turned away from}} weapon design and toward {{the use of computers}} to investigate problems in physics and mathematics. With John Pasta, who helped Metropolis to bring MANIAC on line in March 1952, he explored these ideas in a report [...] "Heuristic Studies in Problems of Mathematical Physics on <b>High</b> <b>Speed</b> <b>Computing</b> Machines", which was submitted on 9 June 1953. It treated several problems that cannot be addressed within the framework of traditional analytic methods: billowing of fluids, rotational motion in gravitating systems, magnetic lines of force, and hydrodynamic instabilities.|$|E
50|$|Wegman, a St. Louis, Missouri native, {{received}} a B.S. in mathematics from Saint Louis University in 1965. He {{then went to}} graduate school at the University of Iowa, where he earned an M.S. in 1967 and a Ph.D. in 1968, both in mathematical statistics. He held a faculty position at the University of North Carolina for ten years. In 1978, Wegman joined the Office of Naval Research, in which he headed the Mathematical Sciences Division. Later, Wegman served as the first program director of the Ultra <b>High</b> <b>Speed</b> <b>Computing</b> basic research program for the Strategic Defense Initiative's Innovative Science and Technology Office. He joined the faculty of George Mason University in 1986 and developed a master’s degree program in statistical science.|$|E
40|$|AbstractBiometrics is a {{field that}} navigates through a vast {{database}} and extracts only the qualifying data to accelerate the processes of biometric authentication/recognition. Image compression is {{a vital part of}} the process. Various Very Large Scale Integration (VLSI) architectures have emerged to satisfy the real time requirements of the online processing of the applications. This paper studies various techniques that help in realizing the fast operation of the transform stage of the image compression processes. Various parameters that may involve in optimizations for <b>high</b> <b>speed</b> like <b>computing</b> time, silicon area, memory size etc are considered in the survey...|$|R
40|$|We {{intend to}} {{describe}} here in full details our procedure for calculating the syn-thetical stellar spectrum. It features high accuracy and <b>high</b> <b>computing</b> <b>speed.</b> In combination with an interactive program for comparing synthetical spectrum with the observations this procedure {{proved to be}} a usefull and reliable tool for abundance analysis, spectral lines identification and other practical purposes. The compute...|$|R
40|$|Major {{technological}} advances have enabled {{the development of}} very <b>high</b> <b>speed</b> networks with data rates {{of the order of}} gigabits per second. In the future, wide area gigabit networks will interconnect database servers around the globe creating extremely powerful distributed information systems. In a <b>high</b> <b>speed</b> network, the size of the message is less of a concern than the number of sequential phases of message passing. In a previous paper, we have developed a lock-based concurrency control protocol for gigabit-networked databases (GNDB). In this paper, we expand on a log-based recovery protocol that provides efficient recovery in a GNDB with the above mentioned concurrency control scheme. 1 Introduction Several exciting advances are being made in the general area of <b>high</b> <b>speed</b> distributed <b>computing.</b> For instance, the rate at which information can be transmitted [12] and the rate at which information can be processed is increasing. Also, user desktops are being enhanced to the point tha [...] ...|$|R
5000|$|As {{far back}} as the 1970s Congressman Gore {{promoted}} the idea of high-speed telecommunications as an engine for both economic growth and the improvement of our educational system. He was the first elected official to grasp the potential of computer communications to have a broader impact than just improving the conduct of science and scholarship ... the Internet, as we know it today, was not deployed until 1993. When the Internet was still {{in the early stages of}} its deployment, Congressman Gore provided intellectual leadership by helping create the vision of the potential benefits of <b>high</b> <b>speed</b> <b>computing</b> and communication. As an example, he sponsored hearings on how advanced technologies might be put to use in areas like coordinating the response of government agencies to natural disasters and other crises.|$|E
5000|$|The ENIAC patent was invalid on {{the basis}} of the amount of time the inventors had {{permitted}} to elapse before filing the patent following the publication of its key features (Finding 7). The judge ruled that Herman Goldstine's June 30, 1945 dissemination of John von Neumann's First Draft of a Report on the EDVAC, a set of incomplete notes describing the logical design of the ENIAC's successor machine the EDVAC, which was being built simultaneously to the ENIAC's completion at the Moore School of Electrical Engineering at the University of Pennsylvania, constituted a publication under the law and an enabling disclosure of the ENIAC. Moreover, Eckert and Mauchly had published their own official report, Automatic <b>High</b> <b>Speed</b> <b>Computing,</b> A Progress Report on the EDVAC on September 30, 1945, which was, again, prior to the critical date of June 26, 1946.|$|E
50|$|With {{a view to}} {{promoting}} research, development and advanced training in Discrete Mathematics, Department of Science and Technology, Government of India, New Delhi has initiated steps towards setting up a National Centre for Advanced Research in Discrete Mathematics. Kalasalingam University is the first institution {{in the country to}} get n-CARDMATH. It was formally inaugurated by His Excellency Dr. A.P.J. Abdul Kalam on 5 January 2007. Under n-CARDMATH at Kalasalingam University ten sub-projects in the area of Graph Theory and Combinatorial Optimization have been approved by DST. The n-CARDMATH Core Group Research facility (CGRF) in the university include a library and a laboratory with <b>high</b> <b>speed</b> <b>computing</b> facilities. There is a provision for a Visitors' Programme. The funding for establishment of the Core facility and for individual projects in the first phase is around Rs.3,00,00,000. Dr. S. Arumugam, Senior Professor (Research), Kalasalingam University, is the coordinator of the project.|$|E
40|$|NASA-Lewis has {{committed}} to a long range goal of creating a numerical test cell for aeropropulsion research and development. Efforts are underway to develop a first generation Numerical Propulsion System Simulation (NPSS). The NPSS will provide a unique capability to numerically simulate advanced propulsion systems from nose to tail. Two essential ingredients to the NPSS are: (1) experimentally validated Computational Fluid Dynamics (CFD) codes; and (2) high performing computing systems (hardware and software) that will permit those codes to be used efficiently. To this end, NASA-Lewis is using <b>high</b> <b>speed,</b> interactive <b>computing</b> {{as a means for}} achieving Integrated CFD and Experiments (ICE). The development is described of a prototype ICE system for multistage compressor flow physics research...|$|R
40|$|The {{attainment}} of <b>high</b> <b>computing</b> <b>speed</b> {{as measured by}} the computational throughput is seen {{as one of the most}} challenging requirements. It is noted that <b>high</b> <b>speed</b> is cardinal in several distinct classes of applications. These classes are then discussed; they comprise (1) the real-time simulation of dynamic systems, (2) distributed parameter systems, and (3) mixed lumped and distributed systems. From the 1950 s on, the quest for <b>high</b> <b>speed</b> in digital simulators concentrated on overcoming the limitations imposed by the so-called von Neumann bottleneck. Two major architectural approaches have made ig possible to circumvent this bottleneck and attain <b>high</b> <b>speeds.</b> These are pipelining and parallelism. Supercomputers, peripheral array processors, and microcomputer networks are then discussed...|$|R
40|$|In the future, {{wide area}} gigabit {{networks}} will interconnect database servers {{around the globe}} creating extremely powerful distributed information systems. In a <b>high</b> <b>speed</b> network, {{the size of the}} message is less of a concern than the number of sequential phases of message passing. In a previous paper, we have developed a lock-based concurrency control protocol for gigabitnetworked databases (GNDB). In this paper, we expand on a log-based recovery protocol that provides efficient recovery in a GNDB with the above mentioned concurrency control scheme. Keywords: Concurrency control, log-based recovery, distributed transactions, distributed agreement protocol. forward recovery. 1 Introduction Several exciting advances are being made in the general area of <b>high</b> <b>speed</b> distributed <b>computing.</b> For instance, the rate at which information can be transmitted [12] and the rate at which information can be processed is increasing. Also, user desktops are being enhanced to the point that servers a [...] ...|$|R
5000|$|Gore {{was one of}} the Atari Democrats {{who were}} given this name due to their [...] "passion for {{technological}} issues, from biomedical research and genetic engineering to the environmental impact of the [...] "greenhouse effect." [...] On March 19, 1979, he became the first member of Congress to appear on C-SPAN. During this time, Gore co-chaired the Congressional Clearinghouse on the Future with Newt Gingrich. In addition, he has been described as having been a [...] "genuine nerd, with a geek reputation running back to his days as a futurist Atari Democrat in the House. Before computers were comprehensible, let alone sexy, the poker-faced Gore struggled to explain artificial intelligence and fiber-optic networks to sleepy colleagues." [...] Internet pioneers Vint Cerf and Bob Kahn noted that,as far back as the 1970s, Congressman Gore promoted the idea of high-speed telecommunications as an engine for both economic growth and the improvement of our educational system. He was the first elected official to grasp the potential of computer communications to have a broader impact than just improving the conduct of science and scholarship ... the Internet, as we know it today, was not deployed until 1983. When the Internet was still {{in the early stages of}} its deployment, Congressman Gore provided intellectual leadership by helping create the vision of the potential benefits of <b>high</b> <b>speed</b> <b>computing</b> and communication.|$|E
40|$|<b>High</b> <b>Speed</b> <b>computing</b> meets ever {{increasing}} real-time computational demands through the leveraging of flexibility and parallelism. The flexibility is achieved when computing platform designed with heterogeneous resources to support multifarious tasks of an application where as task scheduling brings parallel processing. The {{efficient task scheduling}} is critical to obtain optimized performance in heterogeneous computing Systems (HCS). In this paper, we brought a review of various application scheduling models which provide parallelism for homogeneous and heterogeneous computing systems. In this paper, we made a review of various scheduling methodologies targeted to <b>high</b> <b>speed</b> <b>computing</b> systems and also prepared summary chart. The comparative study of scheduling methodologies for <b>high</b> <b>speed</b> <b>computing</b> systems {{has been carried out}} based on the attributes of platform & application as well. The attributes are execution time, nature of task, task handling capability, type of host & computing platform. Finally a summary chart has been prepared and it demonstrates that the need of developing scheduling methodologies for Heterogeneous Reconfigurable Computing Systems (HRCS) which is an emerging <b>high</b> <b>speed</b> <b>computing</b> platform for real time applications. Comment: 12 page...|$|E
40|$|The {{characters}} of more <b>high</b> <b>speed</b> <b>computing</b> {{and much less}} low power dissipation are needed to settle for convolutional encodes. In this paper, we present a parallel method for convolutional encodes with SMIC 0. 35 μm CMOS technology; hardware design and VLSI implementation of this algorithm are also presented. Use this method, parallel circuits structure can be easily designed, which take on excellent {{characters of}} more <b>high</b> <b>speed</b> <b>computing</b> and low power dissipation compared with traditional serial shift register structure for convolutional encodes. © (2013) Trans Tech Publications, Switzerland. Korea Maritime University; Hong Kong Industrial Technology Research Centr...|$|E
40|$|A {{new trend}} in {{computing}} {{is the use}} of multi-core processors and the use of Graphics Processing Units (GPUs) for general purpose <b>high</b> <b>speed</b> parallel <b>computing.</b> Therefore TNO wants to respond to the current trend of parallel computing and wants to investigate on which architectures they want to apply and parallelize their applications on. This thesis discusses the implementation of two applications used by TNO onto a CUDA-enabled GPU and a multi-core processor. The applications chosen for the implementation are object detection and ultrasound simulation (UMASIS). the development time and performance gain were measured during the implementation of both case. With these measurements an estimation of future projects can be made. Microelectronics & Computer EngineeringElectrical Engineering, Mathematics and Computer Scienc...|$|R
40|$|Many {{software}} {{systems have}} been successfully implemented using an agent paradigm which employs a number of independent entities that {{communicate with one another}} to achieve a common goal. The distributed nature of such a paradigm makes it an excellent candidate for use in <b>high</b> <b>speed</b> reconfigurable <b>computing</b> hardware environments such as those present in modem FPGA's. In this paper, a distributed genetic algorithm that {{can be applied to the}} agent based reconfigurable hardware model is introduced. The effectiveness of this new algorithm is evaluated by comparing the quality of the solutions found by the new algorithm with those found by traditional genetic algorithms. The performance of a reconfigurable hardware implementation of the new algorithm on an FPGA is compared to traditional single processor implementations...|$|R
40|$|The {{performance}} of a massively parallel computing system is often limited by the speed of its interconnection network. One strategy that has been proposed for improving network efficiency {{is the use of}} adaptive routing, in which network state information can be used in determining message paths. The design of an adaptive routing system involves several parameters, and in order to build <b>high</b> <b>speed</b> scalable <b>computing</b> systems, {{it is important to understand}} the costs and performance benefits of these parameters. In this paper, we investigate the effect of buffer design on communication latency. Four message storage models and their related route selection algorithms are analyzed. A comparison of their performance is presented, and the features of buffer design which are found to significantly impact network efficiency are discussed...|$|R
