2|1|Public
40|$|The {{visualization}} of big-data represents a hard challenge {{due to the}} sheer amount of information contained in data warehouses. Thus, the accuracy on data relationships in a representation becomes {{one of the most}} crucial aspects to perform business knowledge discovery. A tool that allows to model and visualize information relationships between data is CoDe, which by processing several queries on a <b>data-mart,</b> generates a {{visualization of}} such data. However on a large data warehouse, the computation of these queries increases the response time by the query complexity. A common approach to speed up data warehousing is precompute a set of materialized views, store in the warehouse and use them to compute the workload queries. In this paper, we define a process exploiting the CoDe modeling to determine the minimal number of required OLAP queries and to mitigate the problem of view selection, i. e., select the optimal set of materialized views. The results of an experiment on a real data warehouse show an improvement in the range of 62 - 98 % with respect the approach that does not consider materialized views, and 5 % wrt. an approach that exploits them...|$|E
40|$|The HealthAgents pro ject aims {{to provide}} a {{decision}} support system for brain tumour diagnosis using a collaborative network of distributed agents. The goal is that through the aggregation of the small datasets available at individual hospitals much better decision support classifiers can be created and {{made available to the}} hospitals taking part. In this paper we describe the technicalities of the HealthAgents framework, in particular how the inter-operability of the various agents is managed using semantic web technologies. On the broad-scale the architecture is based around distributed <b>data-mart</b> agents that provide ontological access to hospitals’ underlying data that has been anonymised and processed from proprietary formats into a canonical format. Classifier producers have agents that gather the global data from participating hospitals such that classifiers can be created and deployed as agents. The design on a micro-scale has each agent built upon a generic layered-framework that provides the common agent program code, allowing rapid development of agents for the system. We believe our framework provides a well-engineered, agent-based approach to data-sharing in a medical context. It can provide a better basis on which to investigate the effectiveness of new classification techniques for brain tumour diagnosis...|$|E
40|$|We {{describe}} a sampling application that supports thirteen analytics <b>data-marts</b> by providing database samples {{and has been}} in daily operational use for over two years at one of the world’s top web portals. The main requirement for the application is the need to support ad-hoc aggregate queries against data that spans several years and which cannot be stored in its entirety at the low level needed by data-mining applications. Furthermore, since the query load is highly variable, we cannot choose a stratification of the data so we are led to use uniform sampling. Since the source data cannot be stored at the 100 % level, we have materialized our samples as substitutes for the source data. Another driving requirement is that the sampling process preserve the entire record of web activity for chosen users- if a chosen user appears in multiple populations (tables) she should appear in all the samples derived from those tables. This reduces variance of aggregate metrics based on joining samples and allows us to do longitudinal studies of the chosen users over long time periods. We use Bernoulli (“coin-flip”) sampling because it uses O(1) space whereas reservoir methods require holding a large in-memory array. However, Bernoulli sampling produces variable-sized samples and slightly wider confidence intervals than Simple Random Sampling. We also use Cluster Sampling since in our data, a user corresponds to a cluster of records, one record per web activity event. Our samples are sub-samples from a larger super-sample, therefore confidence intervals for aggregate metrics based on this scheme requires slight extensions to the theory of Horvitz-Thompson estimators, usually used for confidence intervals for count and sum statistics. We develop this new theory and provide some validation. This new theory complements the application which is used daily by business users to make important business decisions...|$|R

