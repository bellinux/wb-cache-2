29|87|Public
40|$|This paper {{discusses}} {{aspects of}} a computational model for the semantics of why-questions which {{are relevant to the}} implementation of an <b>explanation</b> <b>component</b> in a natural language dialogue system. After a brief survey of all of the explanation components which have been implemented to date, some of the distinguishing features of the <b>explanation</b> <b>component</b> designed and implemented by the author are listed. In {{the first part of the}} paper the major types of signals which, like the word why, can be used to set the <b>explanation</b> <b>component</b> into action are listed, and some ways of recognizing them automatically are considered. In addition to these linguistic signals, communicative and cognitive conditions which can have the same effect are discussed. In the second part the various schemata for argumentative dialogue sequences which can be handled by the <b>explanation</b> <b>component</b> in question are examined. Particular attention is paid to problems arising in connection with the iteration of why-questions and the verbalization of multiple justifications. Finally schemata for metacommunicative why-questions and for why-questions asked by the user are investigate...|$|E
40|$|Expert {{systems are}} {{typically}} {{expected to be}} able to justify their decisions to the user. This paper argues that help systems or tutoring systems based on a plan recognizer can equally benefit from an <b>explanation</b> <b>component.</b> To this end a plan recognition system equipped with a user model is presented and the techniques required to generate precise and useful justifications of its results are introduced. The system answers questions about various aspects of a decision and allows its user to check and adjust the user model if necessary. 1 Introduction Knowledge-based expert systems are almost expected to possess an <b>explanation</b> <b>component</b> that provides a justification of the results of their internal reasoning processes to their users (e. g. [10, 13]). This allows a "plausibility check" of the system's decision by comparing its inference steps with the user's own domain knowledge. Such an enhanced transparency of the system behavior is meant to increase the user's acceptance of the solut [...] ...|$|E
40|$|To some extent, {{explanations}} {{in computer}} science are answers to questions. Often an explanatory dialogue is necessary to satisfy needs of software users. In this paper, we introduce the concept of intuitive explanation representing the first explanations in an explanatory dialogue. This kind of explanation {{does not require a}} situational context to be established or that there is a user model. Depending on an abstract model of explanation generation we present the generic <b>explanation</b> <b>component</b> Kalliope applying Semantic Technologies to construct intuitive explanations. We illustrate our generation approach by means of the semantic search engine KOIOS++ enabling keyword-based search on medical articles. Since semantic search results are often hard to understand Kalliope was integrated into KOIOS++ in order to justify search results. In this work we describe in detail the construction of intuitive explanations for inexperienced users in the medical domain building on the concepts of Semantic Frequency Classes and Semantic Co-occurrence Classes. Various user experiments illustrate that these concepts enable the <b>explanation</b> <b>component</b> to rate the understandability of labels and of label connections. We show how Kalliope exploits these valuations to construct and select understandable explanations...|$|E
40|$|Abstract:- Humans {{seem to have}} {{a natural}} {{instinct}} for wanting to understand and make sense of their environment and things in it. In expert systems (ES), explanation can be used to clarify the reasoning process to users such that they can {{gain a better understanding of}} how the system functions. With the help of good explanation facilities, a user can know why an ES is asking a particular question, how the expert system will act if given a certain input, and how the ES reaches a particular conclusion. This is especially important when an ES application is used as a high level advisor to professionals who must retain responsibility for the decisions which are made. However, most ES <b>explanation</b> <b>components</b> require acquiring additional knowledge for explanation, thus increasing the effort of implementing an ES with explanation capabilities. The primary goal of this work is to present a methodology for automatically generating explanations during and at the end of the reasoning process. The developed <b>explanation</b> <b>components</b> can deal with different knowledge representation schemes that are used by problem solving methods namely, the “generate and confirm hypotheses ” that is based on the CommonKADS methodology[1], and the routine design generic task[2]. As a proof of concept the <b>explanation</b> <b>components</b> were developed and integrated into the agricultural expert system generic tool (AESGT) [3]. The developed <b>explanation</b> <b>components</b> can be easily reused with expert systems developed by the tool to automatically generate explanation for the reasoning process...|$|R
50|$|McGuinness {{is known}} for her work on {{description}} logics. particularly her work on the CLASSIC knowledge representation system, <b>explanation</b> <b>components</b> for description logics, {{and a number of}} long-lived applications of description logics such as the PROSE and QUESTAR configurators from AT&T and Lucent and the wines knowledge base for CLASSIC, DAML, OWL, and the KSL Wine Agent.|$|R
40|$|Objective - To {{develop a}} {{taxonomy}} of explanations {{for patients with}} persistent physical symptoms. Methods - We analysed doctors’ explanations from two studies of a moderately-intensive consultation intervention for patients with multiple, often “medically-unexplained,” physical symptoms. We used a constant comparative method to develop a taxonomy which was then applied to all verbatim explanations. Results - We analysed 138 explanations provided by five general practitioners to 38 patients. The taxonomy comprised explanation types and <b>explanation</b> <b>components.</b> Three <b>explanation</b> types described the overall structure of the explanations: Rational Adaptive, Automatic Adaptive, and Complex. These differed {{in terms of who}} or what was given agency within the <b>explanation.</b> Three <b>explanation</b> <b>components</b> described the content of the explanation: Facts – generic statements about normal or dysfunctional processes; Causes – person-specific statements about proximal or distal causes for symptoms; Mechanisms – processes by which symptoms arise or persist in the individual. Most explanations conformed to one type and contained several components. Conclusions - This novel taxonomy for classifying clinical explanations permits detailed classification of explanation types and content. Explanation types appear to carry different implications of agency. Practice implications - The taxonomy is suitable for examining explanations and developing prototype explanatory scripts in both training and research settings...|$|R
40|$|Explanation is an {{important}} function in symbolic artificial intelligence (AI). For instance, explanation is used in machine learning, in case-based reasoning and, most important, the explanation {{of the results of}} a reasoning process to a user must be a component of any inference system. Experience with expert systems has shown that the ability to generate explanations is absolutely crucial for the user acceptance of Al systems. In contrast to symbolic systems, neural networks have no explicit, declarative knowledge representation and therefore have considerable difficulties in generating explanation structures. In neural networks, knowledge is encoded in numeric parameters (weight) and distributed all over the system. It is the intention of this paper to discuss the ability of neural networks to generate explanations. It will be shown that connectionist systems benefit from the explicit coding of relations and the use of highly structured networks in order to allow explanation and explanation components (ECs). Connectionist semantic networks (CSNs), i. e. connectionist systems with an explicit conceptual hierarchy, belong to a class of artificial neural networks which can be extended by an <b>explanation</b> <b>component</b> which gives meaningful responses to a limited class of "How" questions. An <b>explanation</b> <b>component</b> of this kind is described in detail...|$|E
40|$|It {{is widely}} {{established}} debriefing in business games {{is important and}} influences the students' learning performance. Most games only support game statistics instead of explaining solution paths. We suggest the automatic generation of explanations for internet-mediated business games to improve the debriefing quality. As a proof of concept we developed a prototype of an internet-based auction game embedding an open simulation model and an automatic <b>explanation</b> <b>component</b> helping students and teachers to analyse the decision making process. This paper describes the usefulness of automated explanations and the underlying generic software architecture...|$|E
40|$|In this article, a way is {{proposed}} by modelling {{the design process}} as a search process {{that is based on}} construction principles. We introduce a new point of view into user modelling by analysing systematically the task of a database designer. The database designer accomplishes the design process using design steps which are comparable with operators in a search space. Design primitives are possible values of design steps. They are modelled by graph operations. The main idea of the article is to support an arbitrary user with an evaluation function and a finite set of application independent design primitives. A mechanism that is based on the belief function of the Dempster-Shafer calculus can be used to solve different problems: finding the most appropriate design strategy w. r. t. a specific designer and application, determining a plausible design primitive for the next design step and finally, an <b>Explanation</b> <b>Component</b> can use this mechanism to guide the designer and to explain possible design errors in an efficient way. An explicite user model supports the inference process. The User Guidance Tool is sketched, whereby some aspects of context-sensitivity of the <b>Explanation</b> <b>Component</b> are shown in more detail. The user interface design is presented by an overview of the functions which are included in the RADD system. (orig.) SIGLEAvailable from TIB Hannover: RR 8073 (1996, 8) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekDeutsche Forschungsgemeinschaft (DFG), Bonn (Germany) DEGerman...|$|E
40|$|An {{explanation}} {{capability is}} crucial in security-sensitive domains, such as medical applications. Although support vector machines (SVMs) have shown superior performance {{in a range of}} classification and regression tasks, SVMs, like artificial neural networks (ANNs), lack an explanatory capability. There is a significant literature on obtaining human-comprehensible rules from SVMs and ANNs in order to explain "how" a decision was made or "why" a certain result was achieved. This chapter proposes a novel approach to SVM classifiers. The experiments described in this chapter involve a first attempt to generate textual and visual explanations for classification results using multimedia content of various type: poems expressing positive or negative emotion, autism descriptions, and facial expressions, including those with medical relevance (facial palsy). Learned model parameters are analyzed to select important features, and filtering is applied to select feature subsets of explanatory value. The <b>explanation</b> <b>components</b> are used to generate textual summaries of classification results. We show that the explanations are consistent and that the accuracy of SVM models is bounded by the accuracy of <b>explanation</b> <b>components.</b> The results show that the generated explanations display a high level of fidelity and can generate textual summaries with an error rate of less than 35 %...|$|R
40|$|Objective To {{develop a}} {{taxonomy}} of explanations {{for patients with}} persistent physical symptoms. Methods We analysed doctors’ explanations from two studies of a moderately-intensive consultation intervention for patients with multiple, often “medically-unexplained,” physical symptoms. We used a constant comparative method to develop a taxonomy which was then applied to all verbatim explanations. Results We analysed 138 explanations provided by five general practitioners to 38 patients. The taxonomy comprised explanation types and <b>explanation</b> <b>components.</b> Three <b>explanation</b> types described the overall structure of the explanations: Rational Adaptive, Automatic Adaptive, and Complex. These differed {{in terms of who}} or what was given agency within the <b>explanation.</b> Three <b>explanation</b> <b>components</b> described the content of the explanation: Facts – generic statements about normal or dysfunctional processes; Causes – person-specific statements about proximal or distal causes for symptoms; Mechanisms – processes by which symptoms arise or persist in the individual. Most explanations conformed to one type and contained several components. Conclusions This novel taxonomy for classifying clinical explanations permits detailed classification of explanation types and content. Explanation types appear to carry different implications of agency. Practice implications The taxonomy is suitable for examining explanations and developing prototype explanatory scripts in both training and research settings. Abbreviations MUS, medically unexplained symptom(s); DSM, {{diagnostic and statistical manual of}} mental disorders; SCI, symptoms clinic intervention; GP, general practitioner; MSS 1, multiple symptoms study 1; MSS 2, multiple symptoms study 2; PHQ- 15, patient health questionnaire 1...|$|R
40|$|This {{bachelor}} {{thesis is}} specialized on characteristic and theoretical description of three principal parts of cutting machine with major effect for machining process – spindle, mandrel and cutting tool. These parts are characterized and described with schemes and examples. <b>Explanation</b> of <b>components</b> function and running...|$|R
40|$|Abstract being modified. In addition, these {{decisions}} interact with We {{address the problem}} of generating adjectives in a text generation system. We distinguish between usages of ad-jectives informing the hearer of a property of an object and usages expressing an intention of the speaker, or an ar-gumentative orientation. For such argumentative usages, we claim that a generator cannot simply map from infor-mation in the knowledge base to adjectives. Instead, we identify various knowledge sources necessary to decide whether to use an adjective, what adjective should be selected and what syntactic function it should have. We show how {{these decisions}} interact with lexical properties of adjectives and the syntax of the clause. We propose a mechanism for adjective selection and illustrate {{it in the context of}} the <b>explanation</b> <b>component</b> of the ADVISOR expert system. We describe an implementation of adjective selec-tion using a version of Functional Unification Grammars. the lexical properties of adjectives, the syntax of the clause and other factors like collocations. In this paper we therefore address the following two questions: What should be the input to a generator capable of producing argumentative usages of adjectives? And how should the generator combine the many interacting factors constraining the selection of an adjective? After reviewing previous work related to these ques-tions, we present the linguistic data upon which we base our approach and the conclusions we draw from its analysis. We then present and justify the input we require to properly select adjectives and discuss how adjective selection is constrained by the lexical properties of adjec-tives and interacts with other surface decisions. The paper illustrates the key features of our implementation of adjective selection in the context of the ADVISOR <b>explanation</b> <b>component...</b>|$|E
30|$|When {{students}} find and {{correct the errors}} in exercises, while justifying themselves, they are being encouraged to learn to construct viable arguments and critique the reasoning of others [19]. This study found that explaining why an exercise is correct or incorrect fostered transfer and led to better learning outcomes than explaining correct solutions only. However, some of the higher level students struggled with the <b>explanation</b> <b>component.</b> According to the teacher, many of these higher level students who typically do very well on the homework and quizzes scored lower on the unit quizzes and tests than the students expected due to the requirement of explaining the work. In the past, these students had not been justifying their thinking and always got correct answers. Therefore, providing reasons for erroneous examples and justifying their own process were difficult for them.|$|E
40|$|We {{address the}} problem of {{generating}} adjectives in a text generation system. We distinguish between usages of adjectives informing the hearer of a property of an object and usages expressing an intention of the speaker, or an argumentative orientation. For such argumentative usages, we claim that a generator cannot simply map from information in the knowledge base to adjectives. Instead, we identify various knowledge sources necessary to decide whether to use an adjective, what adjective should be selected and what syntactic function it should have. We show how these decisions interact with lexical properties of adjectives and the syntax of the clause. We propose a mechanism for adjective selection and illustrate {{it in the context of}} the <b>explanation</b> <b>component</b> of the ADVISOR expert system. We describe an implementation of adjective selection using a version of Functional Unification Grammars...|$|E
40|$|Abstract — Knowledge-based systems (KBS) {{should be}} able to explain their results to improve the {{understanding}} and credibility of their answers by users. However, most KBS <b>explanation</b> <b>components</b> cannot be easily reused by other applications, thus increasing the effort of implementing KBSs with explanation capabilities. In this paper we present WebExplain, an extension to Unified Problem-Solving Method Description Language (UPML), a KBS development framework. WebExplain is integrated with UPML generic components and can be easily reused during the development of other problem-solving methods and KBSs. WebExplain uses the Inference Web for enabling proof and explanation interoperability between distributed applications. We exemplify our approach by describing WebExplain’s use {{in the development of a}} problem-solving method and a KBS with explanation capabilities. 1...|$|R
5000|$|Following is an <b>explanation</b> of the <b>components,</b> {{where the}} {{coloring}} is for demonstration purposes {{and is not}} used in actual formatting: ...|$|R
40|$|Knowledge based systems (KBSs) {{should explain}} their answers if their users are {{expected}} to understand and thus trust the answers. Problem solvers, KBSs implementing problem solving methods (PSMs), should also explain their answers. Few KBS systems, however, are effective at explaining their answers either because they cannot systematically generate explanations or, when they can, their <b>explanation</b> <b>components</b> cannot easily be extended to new kinds of tasks. In this paper we present an approach enabling problem solvers to explain their answers in a systematic way. To generate proofs for their answers, the approach relies {{on the fact that}} problem solvers can retrieve and reuse their PSMs. To generate explanations automatically the approach relies on the Inference Web infrastructure. The approach is implemented for a deployed problem solver tool using explanations to train police teams to perform resource allocation for public safety. ...|$|R
40|$|The {{degree to}} which users {{understand}} and accept advice from Knowledge-Based Systems can be increased through explanation. However, different application tasks and different sets of users place diverse requirements on an <b>explanation</b> <b>component</b> of a KnowledgeBased System. Thus, the degree of portability of explanation components between applications is reduced. This paper discusses the aspects of explanation that change between application tasks {{and those that are}} required for any satisfactory explanation. The requirements placed on Knowledge-based Systems resulting from explanatory capabilities raises implications for the structure and contents of the knowledge-base and the visibility of the system. The discussion is illustrated by four Knowledge-Based System projects. 1. INTRODUCTION An important feature of knowledge-based systems compared to other information-providing systems is that the knowledge on which they are based is represented explicitly in the system rather than hidden in [...] ...|$|E
40|$|Machine {{learning}} {{is an area}} of computer science concerned with the study of algorithms that reveal patterns and rules from data sets. Genomic profiles describe alterations of a genome, like copy number variations. Cancer often originates from a combination of genomic alterations. In this thesis, I consider machine learning and its application to genomic profiles. The main aspects of this work can be summarised as follows: First, I discuss several machine learning methods, with particular regard to genomic profiles, and then develop a special loss function for survival data. Next, I introduce a framework to find aberration patterns associated with a particular tumour type or disease state. This workflow starts with pre-processing, feature selection and discretisation of genomic profiles, includes strategies to deal with missing values and provides a multi-resolutional ana-lysis. Then, training and analysis of a classifier is performed. Additionally, I introduce an <b>explanation</b> <b>component</b> that emphasizes impor...|$|E
40|$|Abstract. This paper {{reports on}} {{extensions}} to a decision-theoretic location-aware shopping guide {{and on the}} results of user studies that have accompanied its de-velopment. On the basis of the results of an earlier user study in a mock-up of a shopping mall, we implemented an improved version of the shopping guide. A new user study with the improved system in a real shopping mall conrms in a much more realistic setting the generally positive user attitudes found in the earlier study. The new study also sheds further light on the usability issues raised by the system, some of which can also arise with other mobile guides and rec-ommenders. One such issue concerns desire of users to be able to understand and second-guess the system’s recommendations. This requirement led to the devel-opment of an <b>explanation</b> <b>component</b> for the decision-theoretic guide, which was evaluated in a smaller follow-up study in the shopping mall...|$|E
30|$|Followings are {{to explain}} the eight {{principal}} components extracted. Since the paper employs an exploratory approach involving many attributes, the <b>explanation</b> of the <b>components</b> is challenging. Another challenge is that the combinations of variables that load high on a component have patterns {{that are difficult to}} interpret. Therefore, the explanation of factors needs a certain amount of imagination and ingenuity (Wai et al. 2013). In the following section, based on the content and relationships among the variables, the labelling and <b>explanation</b> of each <b>component</b> is given.|$|R
40|$|The Naval Research Laboratory has {{developed}} an oceanographic expert system that describes the evolution of mesoscale features in the Gulf Stream region of the northwest Atlantic Ocean. These features include the Gulf Stream current and the warm and cold core eddies associated with the Gulf Stream. An explanation capability {{was added to the}} eddy prediction component of the expert system in order to allow the system to justify the reasoning process it uses to make predictions. The eddy prediction and <b>explanation</b> <b>components</b> of the system have recently been redesigned and translated from OPS 83 to C and CLIPS and the new system is called WATE (Where Are Those Eddies). The new design has improved the system's readability, understandability and maintainability and will also allow the system to be incorporated into the Semi-Automated Mesoscale Analysis System which will eventually be embedded into the Navy's Tactical Environmental Support System, Third Generation, TESS(3) ...|$|R
30|$|As follows, the {{synthesized}} {{model is}} described in detail following an <b>explanation</b> of main <b>components</b> of the aggregated models and a comparison of different models of RMs.|$|R
40|$|Existing {{conflict}} detection methods for CSP's such as [de Kleer, 1989; Ginsberg, 1993] cannot {{make use of}} powerful propagation which makes them unusable for complex real-world problems. On the other hand, powerful constraint propagation methods lack the ability to extract dependencies or conflicts, which makes them unusable for many advanced AI reasoning methods that require conflicts, {{as well as for}} interactive applications that require explanations. In this paper, we present a non-intrusive {{conflict detection}} algorithm called QUICKXPLAIN that tackles those problems. It can be applied to any propagation or inference algorithm as powerful as it may be. Our algorithm improves the efficiency of direct non-intrusive conflict detectors by recursively partitioning the problem into subproblems of half the size and by immediately skipping those subproblems that do not contain an element of the conflict. QUICKXPLAIN is used as <b>explanation</b> <b>component</b> of an advanced industrial constraint-based configuration tool...|$|E
40|$|In the {{automotive}} industry, the compilation {{and maintenance of}} correct product configuration data is a complex task. Our work shows how formal methods {{can be applied to}} the validation of such business critical data. Our consistency support tool BIS works on an existing data base of Boolean constraints expressing valid configurations and their transformation into manufacturable products. Using a specially modified satisfiability checker with <b>explanation</b> <b>component,</b> BIS can detect inconsistencies in the constraints set and thus help increase the quality of the product data. BIS also supports manufacturing decisions by calculating the implications of product or production environment changes on the set of required parts. In this paper, we give a comprehensive account of BIS: the formalization of the business processes underlying its construction, the modifications of SAT-checking technology we found necessary in this context, and the software technology used to package the product as a client-server information system. ...|$|E
40|$|If user {{interfaces}} are {{to reap the}} benefits of natural language interaction, they must be endowed with the properties that make human natural language interaction so effective. Human-human explanation is an inherently incremental and interactive process. New information must be highlighted and related to what has already been presented. In this paper, we describe the <b>explanation</b> <b>component</b> of a medical information-giving system. We describe the architectural features that enable this component to generate subsequent explanations that take into account the context created by its prior utterances. GENERATING EXPLANATIONS IN CONTEXT Giuseppe Carenini and Johanna D. Moore University of Pittsburgh Department of Computer Science Pittsburgh, PA 15260 Phone: (412) 624 - 8408 fcarenini, jmooreg@cs. pitt. edu ABSTRACT If {{user interfaces}} are {{to reap the benefits}} of natural language interaction, they must be endowed with the properties that make human natural language interaction so effective. Hum [...] ...|$|E
30|$|Sections 2.2. 1, 2.2. 2, and 2.3 {{provide a}} brief <b>explanation</b> of the <b>components</b> {{which must be}} added to the multiradio node to make it {{possible}} to simulate the channel assignment mechanism.|$|R
30|$|Several cost {{components}} and assumptions {{are used in}} the economic calculation and cash flow calculations that are explained below. Items 1 – 5 give a detailed <b>explanation</b> of each <b>component</b> of the assumptions below.|$|R
40|$|Representational {{accounts}} for errors in reasoning about conjunctions and disjunctions of events were contrasted with accounts {{based on the}} algebraic combination of simple event likelihoods. In a series of two experiments, patterns in judgments about complex events were explored when the differing strategies {{were likely to be}} used. Evidence was gathered to determine when representations of complex events are likely to be constructed, and how judgments about complex events are derived when a representation is available. A particular representational account of judgment and decision making, explanation-based decision making, is explored as an account for errors in reasoning about conjunctions and disjunctions of events. As predicted, likelihood judgments for conjunctions of explanations consistently were above the estimated likelihoods of {{at least one of the}} <b>component</b> <b>explanations,</b> and likelihood judgments for disjunctions of explanations consistently fell below likelihoods of the more likely <b>component</b> <b>explanations</b> for common scenario problems. Furthermore, conjunction judgments were found to depend on the relationship between <b>component</b> <b>explanations,</b> a finding predicted by explanation-based decision making and consistent only with representational accounts of reasoning. However, disjunction judgments were not sensitive to the relationship between <b>component</b> <b>explanations,</b> suggesting that subjects may invoke a calculational strategy when assessing some types of disjunctive events. The findings of this research provide support for both representational and calculational accounts, and suggest that the representation of events plays in important role in the selection of strategies that are used to solve judgment problems...|$|R
30|$|It {{has to be}} kept in mind, {{that this}} {{approach}} shows a number of limitations and restrictions: The yielded result is an indication whether an influence can be assumed, not a classification model. The models themselves are only a means to get a result. The result does not provide any information an statistical significance of the discovered phenomena. The result does not provide any information on causalities and reasons for the discovered phenomena. This approach has yet {{to take into account the}} correlations among the input features. This approach has yet to provide any information whether a subset of the chosen features would have been sufficient to predict the class label. This approach does not provide any <b>explanation</b> <b>component</b> on how strong or in which way the features influence the target class. Nonetheless, it does yield an easy-to-use and easy-to-interpret indication on whether the assumed (even nonlinear) influence can be measured in the data.|$|E
40|$|This report {{describes}} the overall {{goals of the}} natural language system HAM-ANS (HAMburg Applicationoriented Natural Language System), which is currently being developed at the University of Hamburg. The project's more specific research topics are divided into application demands (e. g., optimization, portability) and basic research (e. g., <b>explanation</b> <b>component,</b> explicit partner model). HAM-ANS encompasses three different application classes: natural language access to a vision system (traffic at a street crossing), to a data base system (fishery data), and for guiding a competitive dialogue with a client (hotel reservation situation). We introduce the system's general architecture and knowledge sources, and describe two processing components in greater detail: the semantic-oriented ATN Parser and the strategy for evaluating the internal representation of a user's question. The final section contains implementation data and a description of modifications of the LISP/FUZZY programming environment. The appendix shows the processing of an example utterance throughout the system and excerpts from its various external knowledge source...|$|E
40|$|This paper {{describes}} RULEX, {{a technique}} for providing an <b>explanation</b> <b>component</b> for Local Cluster (LC) neural networks. RULEX extracts symbolic rules from the weights of a trained LC net. LC nets are a special class of multilayer perceptrons that use sigmoid functions to generate localised functions. LC nets are {{well suited to}} both function approximation and discrete classification tasks. The restricted LC net is constrained {{in such a way}} that the local functions are `axis parallel' thus facilitating rule extraction. This paper presents results for the LC net on a wide variety of benchmark problems and shows that RULEX produces comprehensible, accurate rules that exhibit a high degree of fidelity with the LC network from which they were extracted. Introduction In [1] Geva et. al. describe the Local Cluster (LC) network, a sigmoidal perceptron with 2 hidden layers where the connections are restricted {{in such a way that}} clusters of sigmoids form local response functions similar to Radial [...] ...|$|E
40|$|Unconstrained {{ordination}} of wing landmarks across sexes of each morphospecies: <b>Explanation</b> note: Principal <b>component</b> analysis (PCA) showing morphometric differences in wing landmarks between sexes of each morphospecies (Ceratitis anonae, Ceratitis fasciventris, Ceratitis rosa). (all 227 specimens included) ...|$|R
40|$|Following the {{availability}} of huge amounts of uncertain data, coming from diverse ranges of applications such as sensors, machine learning or mining approaches, information extraction and integration, etc. in recent years, {{we have seen a}} revival of interests in probabilistic databases. Queries over these databases result in probabilistic answers. As the process of arriving at these answers is based on the underlying stored uncertain data, we argue that from the standpoint of an end user, it is helpful for such a system to give an explanation on how it arrives at an answer and on which uncertainty assumptions the derived answer is based. In this way, the user with his/her own knowledge can decide how much confidence to place in this probabilistic answer. 	 The aim {{of this paper is to}} design such an answer explanation model for probabilistic database queries. We report our design principles and show the methods to compute the answer explanations. One of the main contributions of our model is that it fills the gap between giving only the answer probability, and giving the full derivation. Furthermore, we show how to balance verifiability and influence of <b>explanation</b> <b>components</b> through the concept of verifiable views. The behavior of the model and its computational efficiency are demonstrated through an extensive performance study...|$|R
40|$|Preliminary methodological experiment: {{unconstrained}} {{ordination of}} wing band areas: <b>Explanation</b> note: Principal <b>component</b> analysis (PCA) showing morphometric differences in wing band areas of 14 Ceratitis rosa specimens across sexes, wings (LW: left wing, RW: right wing), repeated {{images of the}} same wing (1, 2), repeated measures of the same image (A, B) ...|$|R
