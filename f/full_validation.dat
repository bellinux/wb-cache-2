161|136|Public
50|$|The {{required}} miracle - the {{prerequisite for}} beatification - {{was investigated in}} the diocese of its origin and received <b>full</b> <b>validation</b> from Roman officials on 12 November 1999. The pope approved it in 2002 and beatified Lluch on 23 March 2003.|$|E
50|$|The {{process for}} the miracle needed for Piamarta's sanctification {{was held in the}} place it {{originated}} in (spanning 15 June 2005 until 7 February 2006) and received <b>full</b> <b>validation</b> from the C.C.S. on 10 November 2006. The medical board granted approval to the healing on 20 December 2007 and theologians also came to the same assessment on 2 July 2011. The C.C.S. also granted assent on 18 October 2011 which paved a path for papal approval on 19 December 2011.|$|E
50|$|The miracle {{required}} for her beatification {{was investigated in}} Spain and received <b>full</b> <b>validation</b> on 12 May 1995 from the C.C.S. before a medical team examined the miracle in question and voiced their approval of it on 11 January 1996; theologians also assented on 29 March 1996 and the C.C.S. as well on 16 April 1996. The pope issued his final approval to this on 30 April 1996 and later beatified her in Saint Peter's Square on 12 May 1996.|$|E
50|$|The miracle {{needed for}} beatification was {{investigated}} in the diocese of its origin and received <b>full</b> C.C.S. <b>validation</b> on 26 September 1996 while a medical board based in Rome issued approval of the purported miracle on 16 December 1999.|$|R
50|$|Testing ARMulator {{was always}} a time {{consuming}} challenge, the <b>full</b> ARM architecture <b>validation</b> suites being employed. At over 1 million lines of C code it was a fairly hefty product.|$|R
40|$|Abstract. Though Web Services {{become more}} and more popular, not only inside closed intranets but also for {{inter-enterprise}} communications, few efforts have been made so far to secure a Web Service’s availability. Existing security standards like e. g. WS-Security only address message integrity and confidentiality, and user authentication and authorization. In this article we present a system for protecting Web Services from Denial-of-Service (DoS) attacks. DoS attacks often rely on misformed and/or overly long messages that engage a server in resource-consuming computations. Therefore, a suitable means to prevent such kinds of attacks is the <b>full</b> grammatical <b>validation</b> of messages by an application level gateway before forwarding them to the server. We discuss specific kinds of DoS attacks against Web Services, show how message grammars can automatically be derived from formal Web Service descriptions (written in the Web Service Description Language), and present an application level gateway solution called ”Checkway ” that uses these grammars to filter Web service messages. The paper closes by giving some performance figures for <b>full</b> grammatical <b>validation.</b> ...|$|R
50|$|Though the radios {{themselves}} are inexpensive, the zigbee Qualification Process involves a <b>full</b> <b>validation</b> {{of the requirements}} of the physical layer. All radios derived from the same validated semiconductor mask set would enjoy the same RF characteristics. An uncertified physical layer that malfunctions could cripple the battery lifespan of other devices on a zigbee network. Zigbee radios have very tight constraints on power and bandwidth. Thus, radios are tested with guidance given by Clause 6 of the 802.15.4-2006 Standard. Most vendors plan to integrate the radio and microcontroller onto a single chip getting smaller devices.|$|E
5000|$|There are {{two steps}} to validation, {{probable}} validation and known validation. [...] "Probable validation" [...] requires widespread {{agreement in the}} medical or scientific community as to its efficacy. [...] "Known validation" [...] requires a scientific framework or body of evidence that appears to elucidate the marker’s efficacy. For <b>full</b> <b>validation,</b> a biomarker must demonstrate that the treatment versus control differences {{are similar to the}} treatment versus control differences for clinical outcome. It is not sufficient to simply demonstrate that the biomarker responders survive longer than the biomarker non-responders.|$|E
5000|$|There is a {{hierarchy}} of queer validity and legitimacy in relation to representations of queer identities. For example, within queer cultures, marginalization and lack of <b>full</b> <b>validation</b> of bi-sexuality perpetuates an intra-queer discourse of queer embodiment. There is a separation of bi-sexual legitimacy because of one's possible position {{to be in a}} heterosexual relationship. Bisexuality can be understood as a privilege in order to be accepted within normative structures of sexuality and human relations. This privileged position, within a hetero-normative framework of gender and sexuality, allows for the misrepresentation and inclusion of these experiences. A woman who is feminine and bi-sexual can [...] "pass" [...] as straight if presently engaged in hetero-normative/straight sexual acts. In this case, femininity acts as a privilege within the queer context.|$|E
40|$|The {{operation}} of multi-domain and multi-vendor EONscan {{be achieved by}} interoperable Sliceable Bandwidth Variable Transponders, a GMPLS/ BGP-LS -based control plane and a planning tool. This paper reports the first <b>full</b> demonstration and <b>validation</b> this end-to-end architecturePeer ReviewedPostprint (published version...|$|R
5000|$|Therefore, {{the more}} time a Clarion Developer spends time {{defining}} the data-dictionary, {{the more time}} they will save later on when the 4GL application generator produces output, typically lists of data with <b>full</b> CRUD capabilities, <b>validation,</b> automatic lookup from [...] "child" [...] table capabilities etc.|$|R
40|$|A new {{empirical}} nonlinear {{model of}} GaN-based electron devices {{is presented in}} the paper. The model takes into account low-frequency dispersion due to self-heating and charge-trapping phenomena and provides accurate predictions at frequencies where nonquasi-static effects are important. The model is based on the application of a recently proposed equivalent-voltage approach and is identified by using pulsed measurements of drain current characteristics and pulsed S-parameter sets. <b>Full</b> experimental <b>validation</b> on a GaN on SiC PHEMT is provided at both small- and large-signal operating conditions...|$|R
50|$|For any system, one of {{the first}} tasks of {{reliability}} engineering is to adequately specify the reliability and maintainability requirements allocated from the overall availability needs and, more importantly, derived from proper design failure analysis or preliminary prototype test results. Clear requirements (able to designed to) should constrain the designers from designing particular unreliable items / constructions / interfaces / systems. Setting only availability, reliability, testability, or maintainability targets (e.g., max. failure rates) is not appropriate. This is a broad misunderstanding about Reliability Requirements Engineering. Reliability requirements address the system itself, including test and assessment requirements, and associated tasks and documentation. Reliability requirements are included in the appropriate system or subsystem requirements specifications, test plans, and contract statements. Creation of proper lower-level requirements is critical.Provision of only quantitative minimum targets (e.g., MTBF values or failure rates) is not sufficient for different reasons. One reason is that a <b>full</b> <b>validation</b> (related to correctness and verifiability in time) of a quantitative reliability allocation (requirement spec) on lower levels for complex systems can (often) not be made as a consequence of (1) the fact that the requirements are probabilistic, (2) the extremely high level of uncertainties involved for showing compliance with all these probabilistic requirements, and because (3) reliability is a function of time, and accurate estimates of a (probabilistic) reliability number per item are available only very late in the project, sometimes even after many years of in-service use. Compare this problem with the continuous (re-)balancing of, for example, lower-level-system mass requirements in the development of an aircraft, which is already often a big undertaking. Notice that in this case masses do only differ in terms of only some %, are not a function of time, the data is non-probabilistic and available already in CAD models. In case of reliability, the levels of unreliability (failure rates) may change with factors of decades (multiples of 10) as result of very minor deviations in design, process, or anything else. The information is often not available without huge uncertainties within the development phase. This makes this allocation problem almost impossible to do in a useful, practical, valid manner that does not result in massive over- or under-specification. A pragmatic approach is therefore needed—for example: the use of general levels / classes of quantitative requirements depending only on severity of failure effects. Also, the validation of results is a far more subjective task than for any other type of requirement. (Quantitative) reliability parameters—in terms of MTBF—are by far the most uncertain design parameters in any design.|$|E
40|$|Growth of {{companies}} through {{various stages of}} development is explored and validated by comparison with companies described by current company officers who were participating in a training program. Since data used were not representative of small business, <b>full</b> <b>validation</b> of the evolutionary model was not completed...|$|E
30|$|The {{model has}} been {{instantiated}} in a sensor network environment, and the algorithm has been validated in a smart home example scenario. Currently, the model is object of a thorough evaluation aimed at <b>full</b> <b>validation</b> and testing. In the near future, the proposed approach will be extended to the composition of software architectures addressing multiple goals and to different application domains.|$|E
40|$|Abstract. Automatic {{composition}} {{is not a}} trivial problem, especially {{when the number of}} services is high and there are different control struc-tures to handle the execution flow. In this paper, we present an A * algo-rithm for automatic service composition that obtains all valid composi-tions from a extended service dependency graph, using different control structures (sequence, split, choice). A <b>full</b> experimental <b>validation</b> with eight different public repositories has been done, showing a great perfor-mance as the algorithm found all valid compositions {{in a short period of}} time...|$|R
50|$|The miracle {{required}} {{for her to}} be beatified was investigated in the location that it had originated in and received <b>full</b> C.C.S. <b>validation</b> on 5 December 1987. This in turn allowed a medical board to discuss the healing and the latter voted in the affirmative for the healing as a miracle on 26 April 1989 while consulting theologians did so as well on 30 June 1989. The C.C.S. did so as well on 21 November 1989 and passed it to the pope who approved it on 21 December 1989.|$|R
40|$|Radar {{interferometry}} {{has evolved}} significantly {{during the late}} 80 's and in the 90 's. The applications of the technique have broadened, starting from topographic mapping using single-pass, dual antenna, airborne configurations, to deformation mapping and atmospheric water vapor mapping using multi-pass spaceborne configurations. The <b>full</b> geodetic <b>validation</b> of the technique, however, requires proper modeling of the dispersion of the data, i. e., the full variance covariance matrix. Although single-look and multi-look phase statistics have been discussed by many authors, parameter estimation using a Gauss-Markov model also requires the stochastic modeling of the covariance between resolution cells...|$|R
40|$|ABSTRACT: A {{validation}} {{based on}} solvation energies (vacuum to water transfer) {{is not sufficient}} to justify the use of approximated models of electrostatics to rank ligand/protein complexes. A <b>full</b> <b>validation</b> should be based on energies in solution, i. e., solvation plus vacuum Coulomb energies, because of the anticorrelation between solvation and vacuum energies. The energy in solution is the relevant quantity in simulations of biological macromolecule...|$|E
30|$|The {{main point}} in this {{resolution}} is that an analytical method that is not described in the official compendium recognized by ANVISA requires an analytical validation. A <b>full</b> <b>validation</b> should include accuracy, repeatability precision, intermediate precision, selectivity, limit of detection, limit of quantification, linearity, and interval. Compendial analytical methods shall have their suitability for the intended use shown by a partial validation study. A partial validation should include at least the parameters of precision, accuracy, and selectivity.|$|E
40|$|Abstract — We {{describe}} a distributed computing platform {{to carry out}} large scale dictionary attacks against cryptosystems compliant to the OpenPGP standard. Moreover, we {{describe a}} simplified mechanism to quickly test passphrases that might protect a specified private key ring. Only passphrases that pass this test complete the (much more time consuming) <b>full</b> <b>validation</b> procedure. This approach greatly reduces {{the time required to}} test a set of possible passphrases. Index Terms — OpenPGP, Volunteer computing, Middleware. I...|$|E
40|$|Improved {{agricultural}} practices {{have a significant}} potential to mitigate greenhouse gas (GHG) emissions. A key issue for implementing mitigation options is quantifying emissions practically and cost effectively. Web-based systems using process-based models provide a promising approach. COMET 2. 0 is a further development of the web-based COMET-VR system with an expanded set of crop management systems, inclusion of orchard and vineyards, new agroforestry options, and a nitrous oxide (N 2 O) emissions estimator, using the Century and DAYCENT dynamic ecosystem models. Compared to empirical emission factor models, COMET 2. 0 accounted {{for more of the}} between site variation in soil C changes following no-till adoption in Corn Belt and Great Plains experiment sites. Predicted N 2 O emission rates, as a function of application rate, timing (spring vs. fall), and use of nitrification inhibitors, were consistent with observations in the literature. Carbon dynamics for orchard and agroforestry compared well with field measurements but limited availability of data poses a challenge for a <b>fuller</b> <b>validation</b> of these systems. Advantages of a practiced-based approach, using dynamic process-based models include integration of interacting processes and local conditions for more accurate and complete GHG accounting. Web-based systems, designed for non-experts, allow land managers and others to evaluate trade-offs and select mitigation options for their particular conditions. Experimental networks such as GRACEnet will {{play an important role in}} improving decision support tools for implementation of agricultural GHG mitigation. Peer Reviewe...|$|R
50|$|At {{the launch}} of Google Public DNS, it did not {{directly}} support DNSSEC. Although RRSIG records could be queried, the AD (Authenticated Data) flag was not set in the launch version, meaning the server was unable to validate signatures {{for all of the}} data. This was upgraded on 28 January 2013, when Google's DNS servers silently started providing DNSSEC validation information, but only if the client explicitly set the DNSSEC OK (DO) flag on its query. This service requiring a client-side flag was replaced on 6 May 2013 with <b>full</b> DNSSEC <b>validation</b> by default, meaning all queries will be validated unless clients explicitly opt out.|$|R
40|$|During {{the past}} three years the Institute for Fluid Power Drives and Controls in Aachen has {{developed}} a new hydraulic system for mobile machinery called STEAM. The system represents a new step in excavator hydraulics, as it aims to reduce both the hydraulic system losses {{as well as those of}} the internal combustion engine by using a hybrid hydraulic architecture with accumulators. Starting with initial simulation studies the development has been followed by scaled test bench measurements and has progressed to a <b>full</b> scale <b>validation</b> using an 18 t excavator. The following publication aims to summarise the results obtained thus far with the aim of making them available to industry and encouraging their implementation in future applications...|$|R
40|$|This paper {{presents}} a test and deployment infrastructure {{that has been}} developed to validate the ICENI Grid Middleware. Services {{have been developed to}} monitor the different steps {{in the life of the}} middleware from its compilation to its execution. We will show how these services can be composed to provide a <b>full</b> <b>validation</b> of the architecture and the functionalities of ICENI. We also present a MDS to ICENI Gateway used to register machines currently available in the MDS Server as ICENI launcher services...|$|E
40|$|A {{new method}} for the {{reliable}} quantification of short-chain chlorinated paraffins (SCCPs) in sediments and soil, based on carbon skeleton gass chromatography {{is presented in}} this paper. The use of this procedure as a standard method coupled {{to the definition of}} a method-defined parameter overcomes some of the main difficulties encountered so far in the analysis of SCCPs. The uncertainty budget estimations show that with this approach the expanded uncertainty is reduced to acceptable values. In-house development and <b>full</b> <b>validation</b> of the method on sediment and soil samples, are presented in this paper. JRC. D. 2 -Reference material...|$|E
40|$|International audienceVehicles are {{nowadays}} increasingly software-intensive. Current practice {{shows that}} the whole software system is tested, validated and then uploaded in ECUs in a monolithic fashion, before a vehicle is put in operation. Yet, adding speci c features or updates without restarting the <b>full</b> <b>validation</b> and upload procedure could yield to signi cant gains. Unfortunately, the AUTOSAR (AUTomotive Open System ARchitec- ture) standard does not include native support for software updates. In this paper, we de ne a set of new concepts {{in order to support}} dynamic software updates for automotive systems, and present mechanisms that implement these concepts within AUTOSAR...|$|E
40|$|A {{reference}} {{setup for}} on-site calibration and validation of revenue metering systems is realized. The system {{consists of a}} three-phase power meter and high-accuracy current and voltage transformers (CTs and VTs). Measurements {{can be made up}} to 150 kV (system voltage) and line currents up to 5000 A. The power meter, the CTs, and the VTs were all calibrated. Based on the excellent results of the calibration of these components it is possible to predict the full system performance with an accuracy of 35 J. lWNA. This estimation was verified with a <b>full</b> system <b>validation,</b> showing that the system is much more accurate than required and that detrimental effects to the accuracy are well understood. Peer reviewed: YesNRC publication: Ye...|$|R
40|$|Pressure vessels, such as {{hydraulic}} cylinders, {{are one of}} {{the typical}} examples for the application of structural integrity concepts based on fracture mechanics. The mechanical and fracture mechanics properties affect the failure mode and safety in case of hydraulic cylinders operating in external adverse conditions. For this purpose an experimental plan was set up, the materials being characterized in terms of fracture mechanics properties at room and low temperature, mechanical properties and <b>full</b> scale <b>validation.</b> This latter was carried out on tubes containing defects with different depths. In this paper, an engineering critical assessment based on FITNET procedure is presented. In particular the behavior of different steel grades and load conditions has been investigated in the light of FITNET procedure...|$|R
40|$|AbstractFunctional imaging of {{the kidney}} using {{radiological}} techniques {{has a great}} potential of development because the functional parameters, which can be approached non-invasively, are multiple. CT can provide measurement of perfusion and glomerular filtration but has the inconvenient to deliver irradiation and potentially nephrotoxicity due to iodine agents in this context. Sonography is able to evaluate perfusion only but quantification remains problematic. Therefore, MR imaging shows the greatest flexibility measuring blood volume and perfusion as well as split renal function. The main applications of perfusion imaging of the kidney are vascular diseases, as renal artery stenosis, renal obstruction and follow-up of renal tumors under antiangiogenic therapy. However, <b>full</b> clinical <b>validation</b> of these methods and the evaluation of their clinical impact are still often worthwhile...|$|R
40|$|The aim of {{this study}} is to assess the {{features}} of a numerical tool able to predict in an efficient and accurate way the performance of a Ground Penetrating Radar (GPR) in many typical on-site Earth and planetary applications. Suitable implementation of a computer-aided-design (CAD) package is carried out by accounting for the most critical aspects of the GPR behavior (e. g., antenna elements, signal waveforms, physical characteristics of host media and scatterers). Representative examples of different application scenarios have been developed and tested. <b>Full</b> <b>validation</b> is achieved also by means of appropriate comparisons derived through ad-hoc experimental set-up. ...|$|E
40|$|Based on the ATM {{security}} requirements (T 4. 1), ATM security architecture (T 4. 2) and Scenarios, methods and means for validation exercises (T 5. 1), {{the high level}} system requirements for the validation environment, for the prototypes and for the validation platform subsets (wherever used in SMP stand alone, partial or <b>full</b> <b>validation</b> exercises) have to be fulfilled. In particular the system requirements for each single validation platform subset will address input / output protocols and interfaces towards any other possible prototype and/or validation platform subset interconnected. The definition of these requirements is done in Task 5. 2 and documented in this report...|$|E
30|$|It was {{noticeable}} {{that all}} tested signatures perform {{worse in the}} <b>full</b> <b>validation</b> set containing the influential observation (n[*]=[*] 13) than in the same validation set without it (n[*]=[*] 12). Therefore, it is not advisable to include such observations in future testing of the predictive signature. Further, a trend towards a degradation of performance is apparent in the tested signatures with {{an increasing number of}} probes in the <b>full</b> <b>validation</b> set. However, a clear picture emerges by seeing the performance RMSE results of Fig.  2 in the validation set when the influential observation is omitted (n[*]=[*] 12). The signatures containing exclusively probes with positive regression coefficients (the 149 and 249 probe signatures) perform worse than all the remainder (RMSE[*]=[*] 0.59 and 0.57 respectively). The signatures containing exclusively probes with negative regression coefficients (the 201 and 301 probes signatures) perform the best in the validation dataset (RMSE[*]=[*] 0.39 and 0.38 respectively) and perform better than the results obtained with signatures containing a mixture of probes with both positive and negative regression coefficients. Moreover, the performance of the original signature (909 probes) is not degraded by either increasing (the 0.8 and 0.9 threshold signatures) or reducing (the 1.1 and 1.2 threshold signatures) the number of probes. The RMSE for the original PLS- 3 signature with 909 probes (1.0 threshold) is approximately the same (between 0.45 and 0.46) as the 0.8 (2248 probes), 0.9 (1461 probes), 1.1 (547 probes), and 1.2 (323 probes) threshold signatures. Only the 1.3 threshold signature (174 probes) has a worse performance (RMSE[*]=[*] 0.52).|$|E
40|$|All {{students}} {{admitted to}} The Hong Kong Polytechnic University (PolyU) {{under the new}} 4 -year undergraduate degree structure starting in the 2012 - 13 school year {{are required to take}} at least one subject in “Service Learning”. Preparations started in 2010. Since then 24 subjects have been developed and approved, about half of them have been piloted – offered to the current students as elective subjects. These subjects have been taken by ~ 300 students, who have served in Hong Kong, mainland China, and overseas. At full scale, it is estimated that we have to offer 60 subjects to more than two thousand students each year. This paper discusses the challenges faced and the lessons learned from the preparation and piloting. The biggest and most fundamental challenge is perhaps the changing of the mindset: convincing the university community of the benefits and practicality of service learning as a core component of holistic education. This was overcome by open consultation, presentation of relevant research, reference to best practices both internal and external to the university, and the successful piloting. The second challenge that follows is the training of and provision of support for teachers, through seminars, workshops, discussions and e-Learning. The third and most demanding of creativity is the exploration and development of service learning projects with long-term sustainability, for the wide range of disciplines encompassed by a comprehensive university such as the Polytechnic University – ranging from information technology, to English, to Hotel and Tourism, to Building and Real Estate, to Rehabilitation Science and Engineering, to Design, to Textile and Fashion, to Finance and Accounting, … Finally, we have to evaluate the impact of service learning on the holistic development of students over time. Preliminary evaluation of the piloting subjects have been very encouraging. <b>Fuller</b> <b>validation,</b> however, must await full implementation and longer term studies...|$|R
40|$|QSPR {{studies for}} {{estimating}} the incorporation organic hazardous compounds in cationic surfactant (CTAB) {{were developed by}} application of the structural descriptors and MLR method. A simple model with low standard errors and high correlation coefficients was selected. It was also found that MLR method could model the relationship between solubility and structural descriptors perfectly. The proposed methodology was validated using <b>full</b> cross <b>validation</b> and external validation using division of the available data set into training and test sets. The proposed model can be used adequately for the prediction and description of the solubility of organic compounds in micellar solutions. The statistical parameters root mean squares error of prediction (RMSEP) relative error of prediction (REP) % and standard error of prediction (SEP) were 0. 169, 9. 561 and 0. 176 respectively for proposed MLR model...|$|R
40|$|Recent {{developments}} {{in the field of}} controls for asynchronous induction motors require a precise knowledge of the parameters of the machine. In particular, the estimation of the stator flux is a matter of highest importance, and many solutions can be found in literature, with different degrees of accuracy and complexity. The present paper deals with an industry-oriented estimation technique, performed at complete standstill, to let the estimation be performed even without disconnecting the motor from the load. In particular, the paper carries out a complete study on the influence of the squirrel cage rotor during the integration usually adopted for the flux linkage determination. A <b>full</b> experimental <b>validation</b> is included, to investigate a problem yet little known. In this regard, two twin machines have been designed and manufactured, with and without rotor cage die casting...|$|R
