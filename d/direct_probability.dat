29|74|Public
50|$|Some human males may excrete sperm {{carried out}} of the body by Cowper's gland {{secretions}} prior to ejaculation, at concentrations {{similar to those found}} in their semen. This could result in the possibility of conception from the introduction of pre-ejaculate alone to the vagina, though the <b>direct</b> <b>probability</b> of pregnancy has not been assessed.|$|E
5000|$|In modern terms, given a {{probability}} distribution p(x|θ) for an observable quantity x conditional on an unobserved variable θ, the [...] "inverse probability" [...] is the posterior distribution p(θ|x), which depends {{both on the}} likelihood function (the inversion of the probability distribution) and a prior distribution. The distribution p(x|θ) itself is called the <b>direct</b> <b>probability.</b>|$|E
40|$|Unwieldy {{probability}} entry interfaces are {{the norm}} for Bayesian Network knowledge engineers. This requires domain experts to provide unreasonably accurate probability estimates. To solve these problems, we have developed two applications: CPTable improves <b>direct</b> <b>probability</b> entry, and provides node customisation and sliding scale binary elicitation; Verbal Elicitor allows entry of probability values in ordinary English. The domain expert selects a verbal cue such as "unlikely" or "almost certain. " The probabilities are then set manually or optimised to minimise probabilistic incoherency...|$|E
3000|$|... which {{determines the}} {{proportion}} of undirected edges in the network which are selected to become <b>directed.</b> Another <b>probability</b> p [...]...|$|R
40|$|We {{propose a}} program which {{allows one to}} {{estimate}} significance, confidence intervals and limits, taking into account systematics and statistical uncertainties of variables described by Poisson distributions. The given program {{can be used for}} combining searches. The motivation of the <b>direct</b> <b>probabilities</b> calculations is determined by two reasons. Firstly, the tail of a Poisson distribution is heavier than that of a normal distribution. In the case of small probabilities the Gaussian approximation gives the wrong values of estimators. Secondly, the estimators which are constructed {{on the basis of the}} likelihood ratio often have poor statistical properties. 1...|$|R
40|$|This paper {{presents}} <b>direct</b> conditional imprecise <b>probabilities</b> for {{the number}} of successes in a finite number of future trials, given information about a finite number of past trials. A simple underlying process determining failures or successes is assumed, related to Bayes' postulate, and Hill's A(n) assumption is used. The results are related to the type of predictive inference known as low structure or black-box inference. A(n) Black-box inference <b>Direct</b> conditional <b>probabilities</b> Fundamental problem of practical statistics Fundamental theorem of probability Imprecise probabilities Low stochastic structure Predictive inference...|$|R
30|$|At {{the cost}} of dropping, the <b>direct</b> <b>probability</b> {{computation}} one can use log-odds-based approach of the Algorithm 2. In this case, the problem of division by zero is reduced to addition and subtraction of numbers {{in the same order}} of magnitude. The main disadvantage is requirement of additional memory storage—with Algorithm 1, only the vector of r probabilities is needed, while Algorithm 2 requires a r×r matrix of logarithmic odds. Additionally, the result in form of odds can be complicated to interpret if the resulting probability distribution of faults is multi-modal. Original probabilities can be reconstructed but also with additional computational cost.|$|E
40|$|Plant {{and system}} {{operators}} {{are looking for}} reliable products meeting high technical requirements while providing maximum economic benefit throughout the life cycle. Calculating life cycle costs different operating conditions {{have to be considered}} to provide reliable decision making assistance. If data for a <b>direct</b> <b>probability</b> measurement may not be available, expert knowledge needs to be considered. The model present enables a combination of expert knowledge and collected lifetime data to be used for estimating product operational costs. Modelling costs this way, the plausibility of a given statement can be updated in the light of new information to produce a more accurate cost estimation...|$|E
40|$|This paper {{discusses}} the analysis approach when using event trees and fault trees in a {{quantitative risk assessment}} context. The basic question raised is when to introduce probability models and frequentist probabilities (chances) instead of using <b>direct</b> <b>probability</b> assignments for {{the events of the}} trees. We argue that such models should only be used if the key quantities of interest of the risk assessment are frequentist probabilities and when systematic information updating is important for meeting the aim of the analysis. An example of an event tree related to the analysis of an LNG (Liquefied Natural Gas) plant illustrates the analysis and discussion...|$|E
40|$|We {{show the}} {{existence}} of independent random matching among a continuum of agents in discrete-time dynamic systems for which matching probabilities can be “directed. ” That is, the probability with which an agent is matched to agents of a given type need not be equal to the fraction of that type in the matched population. We prove {{the existence of}} a continuum of independent discrete-time Markov processes, one for each agent, that incorporates the effects of random mutation, random matching with <b>directed</b> <b>probabilities,</b> and match-induced random type changes. The empirical type evolution of such a discrete-time dynamic system is also determined via an exact law of large numbers. The results provide a mathematical foundation for many previously studied search-based models of labor markets, money, and financial markets...|$|R
40|$|In {{this paper}} we propose a new Bayesian {{estimation}} method to solve linear inverse problems in signal and image restoration and reconstruction problems which has the property to be scale invariant. In general, Bayesian estimators are nonlinear functions of the observed data. The only exception is the Gaussian case. When dealing with linear inverse problems the linearity is sometimes a too strong property, while scale invariance often remains a desirable property. As everybody knows one of the main difficulties with using the Bayesian approach in real applications is the assignment of the <b>direct</b> (prior) <b>probability</b> laws before applying the Bayes' rule. We discuss here how to choose prior laws to obtain scale invariant Bayesian estimators. In this paper we discuss and propose a familly of generalized exponential probability distributions functions for the <b>direct</b> <b>probabilities</b> (the prior p() and the likelihood p(|)), for which the posterior p(|), and, consequently, the main posterior estimators are scale invariant. Among many properties, generalized exponential can be considered as the maximum entropy probability distributions subject to the knowledge of a finite set of expectation values of some knwon functions. Comment: Presented at MaxEnt 93. Appeared in Maximum Entropy and Bayesian Methods, G. Heidbreder (Ed.), Kluwer Academic Publishers, pp: 121 - 134, ([URL]...|$|R
40|$|A novel Genetic Algorithm (GA) {{assisted}} <b>direct</b> error <b>probability</b> optimisation {{technique is}} proposed for adaptive beamforming, which reduces the achievable error probability by nearly two {{orders of magnitude}} {{at a signal-to-noise ratio}} of 10 dB in the investigated scenario in comparison to the minimum mean-squared error beamforming benchmarker...|$|R
40|$|A syntax-directed {{translator}} first parses the source-language {{input into}} a parsetree, and then recursively converts the tree into a string in the target-language. We model this conversion by an extended treeto-string transducer that have multi-level {{trees on the}} source-side, which gives our system more expressive power and flexibility. We also define a <b>direct</b> <b>probability</b> model and use a linear-time dynamic programming algorithm {{to search for the}} best derivation. The model is then extended to the general log-linear framework in order to rescore with other features like n-gram language models. We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring. Initial experimental results on English-to-Chinese translation are presented. ...|$|E
30|$|In {{the search}} for other risking methods that {{indicate}} one should invest in attractive projects (to the maximum extent concomitant on not jeopardizing a corporation’s financial health through gambler’s ruin), there has been progress made in utilizing methods based on <b>direct</b> <b>probability</b> without {{the appearance of the}} exponential behavior that is of fundamental importance in the Cozzolino procedure (see “Appendix 1 ”). Two such different risking methods are examined here: one usually named the Kelly method and the other related to corporate risk probability. As will be shown, both have their limitations and so {{the search for}} a stable and robust risk procedure is still a matter of extreme interest. Consider each of the two risking methods in turn.|$|E
40|$|AbstractThe paper {{presents}} a possibility theory based formulation of one-parameter estimation that unifies some usual <b>direct</b> <b>probability</b> formulations. Point and confidence interval estimation {{are expressed in}} a single theoretical formulation and incorporated into estimators of a generic form: a possibility distribution. New relationships between continuous possibility distribution and probability concepts are established. The notion of specificity ordering of a possibility distribution, corresponding to fuzzy subsets inclusion, is then used for comparing the efficiency of different estimators for the case of data points coming from a symmetric probability distribution. The usefulness of the approach is illustrated on common mean and median estimators from identical independent data sample of different size and of different common symmetric continuous probability distributions...|$|E
40|$|The {{motivation}} for {{this paper is}} the analysis of a cohort where not only the survival time of patients, or time to failure of industrial items, but also their level of degradation, classified into {{a finite number of}} states, is under study. The process is assumed to be semi-Markov in order to weaken the usual Markov assumption which is often too restrictive. The behavior of such a process is defined through the initial probabilities on the set of possible states, the <b>direct</b> transition <b>probabilities</b> between any two states and the sojourn times distributions in any state as functions of the actual state and the one reached from there {{at the end of the}} sojourn. Two models are considered: on one side the most usual model in this framework which is the independent competing risk (ICR) model and a general semi-Markov model (GSM) embedding (ICR). Based on an estimation of the <b>direct</b> transition <b>probabilities</b> in the general semi-Markov model (GSM) embedding (ICR), we derive a goodness of fit test for th...|$|R
40|$|An {{impact of}} ideas {{associated}} {{with the concept of}} a hypothetical quantum computer upon classical computing is analyzed. Two fundamental properties of quantum computing: <b>direct</b> simulations of <b>probabilities,</b> and influence between different branches of probabilistic scenarios, as well as their classical versions, are discussed...|$|R
40|$|Capturing {{word meaning}} {{is one of}} the {{challenges}} of natural language processing (NLP). Formal models of meaning, such as networks of words or concepts, are knowledge repositories used in a variety of applications. To be effectively used, these networks have to be large or, at least, adapted to specific domains. Learning word meaning from texts is then an active area of research. Lexico-syntactic pattern methods are one of the possible solutions. Yet, these models do not use structural properties of target semantic relations, e. g. transitivity, during learning. In this paper, we propose a novel lexico-syntactic pattern probabilistic method for learning taxonomies that explicitly models transitivity and naturally exploits vector space model techniques for reducing space dimensions. We define two probabilistic models: the direct probabilistic model and the induced probabilistic model. The first is directly estimated on observations over text collections. The second uses transitivity on the direct probabilistic model to induce probabilities of derived events. Within our probabilistic model, we also propose a novel way of using singular value decomposition as unsupervised method for feature selection in estimating <b>direct</b> <b>probabilities.</b> We empirically show that the induced probabilistic taxonomy learning model outperforms state-of-the-art probabilistic models and our unsupervised feature selection method improves performance...|$|R
40|$|Missing data {{is common}} in real world {{datasets}} and {{is a problem for}} many estimation techniques. We have developed a variational Bayesian method to perform Independent Component Analysis (ICA) on high-dimensional data containing missing entries. Missing data are handled naturally in the Bayesian framework by integrating the generative density model. Modeling the distributions of the independent sources with mixture of Gaussians allows sources to be estimated with different kurtosis and skewness. The variational Bayesian method automatically determines the dimensionality of the data and yields an accurate density model for the observed data without overfitting problems. This allows <b>direct</b> <b>probability</b> estimation of missing values in the high dimensional space and avoids dimension reduction preprocessing which is not feasible with missing data. 1...|$|E
40|$|In syntax-directed translation, the sourcelanguage input {{is first}} parsed into a parsetree, {{which is then}} recursively {{converted}} into a string in the target-language. We model this conversion by an extended treeto-string transducer that has multi-level trees on the source-side, which gives our system more expressive power and flexibility. We also define a <b>direct</b> <b>probability</b> model and use a linear-time dynamic programming algorithm {{to search for the}} best derivation. The model is then extended to the general log-linear framework in order to incorporate other features like n-gram language models. We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for ngram rescoring. Preliminary experiments on English-to-Chinese translation show a significant improvement in terms of translation quality compared to a state-of-theart phrase-based system. ...|$|E
40|$|The authors compare <b>direct</b> <b>probability</b> and non-probability {{parametric}} uncertainty {{analysis for}} {{decision making in}} early design phase of passive and active vibration isolation in suspension struts. A simple mathematical one degree of freedom linear model of an automobile's suspension leg with harmonic base point excitation and subject to passive and active vibration isolation is used as an example to answer three basic questions: which simple and direct yet adequate methodological tool to quantify data uncertainty analysis is reasonable for decision making in early design phase? What {{is the difference between}} uncertainty in the passive and in the active vibration isolation approach and what is the confidence level analyzing the difference? Is the data uncertainty quantification in early design sufficient and to what extend it is needed to quantify model uncertainty for decision making...|$|E
40|$|In this paper, several {{results on}} {{probability}} spaces have {{been extended to}} direct limits of such. The calculus includes convergence sequences, conditional expectations, completeness questions and extensions of probability spaces. Conditional expectations and direct limits completions and extensions and direct limits calculus of <b>direct</b> limits of <b>probabilities...</b>|$|R
5000|$|... that a {{form letter}} {{signed by the}} Army Deputy Chief of Staff for Personnel {{be placed in the}} Official Military Personnel File of those female enlisted service members {{reclassified}} as a result of <b>Direct</b> Combat <b>Probability</b> Coding (DCPC), stating that reclassification was directed by Department of the Army {{as a result of the}} Women In The Army Study and that the service member should not be penalized as a result of action taken by Headquarters Department of the Army and that Letters of Instruction to promotion boards address this issue.|$|R
40|$|We {{consider}} the usual Langevin equation depending on an internal time. This parameter is substituted by a {{first passage time}} of a self-similar Markov process. Then the Gaussian process is parent, and the hitting time process is <b>directing.</b> The <b>probability</b> to find the resulting process at the real time {{is defined by the}} integral relationship between the probability densities of the parent and directing processes. The corresponding master equation becomes the fractional Fokker-Planck equation. We show that the resulting process has non-Markovian properties, all its moments are finite, the fluctuation-dissipation relation and the H-theorem hold. Comment: 6 pages, 2 figure...|$|R
40|$|The {{computational}} method of parametric probability analysis is introduced. It is demonstrated how to embed logical formulas from the propositional calculus into parametric probability networks, thereby enabling sound reasoning about the probabilities of logical propositions. An alternative <b>direct</b> <b>probability</b> encoding scheme is presented, which allows statements of implication and quantification to be modeled directly as constraints on conditional probabilities. Several example problems are solved, from Johnson-Laird's aces to Smullyan's zombies. Many apparently challenging problems in logic {{turn out to}} be simple problems in algebra and computer science: systems of polynomial equations or linear optimization problems. This work extends the mathematical logic and parametric probability methods invented by George Boole. Comment: 39 pages including 4 page appendix; new title, additional clarifications, correction of bugs introduced into one example in the last revisio...|$|E
40|$|In {{order to}} {{evaluate}} the efficacy and safety of new antibiotics, Doxycycline (DOT C) and Rolitetracycline (Roli-TC) {{with the use of}} intravenous administration in urinary tract infections, a well-controlled open study was carried out, and the following results were obtained. 1) Any significant difference between the efficancy of these two drugs in urinary tract infections could not be found with the statistical analysis using x 2 -test, <b>direct</b> <b>probability,</b> multiple regression analysis, and discriminant analysis with calculated sample score by quantification theory. 2) A simple mathematical values calculated on the efficacy of these drugs with the incidence of the significant or efficient therapeutic results were 56. 5 % in DOTC-group and 42. 0 % in Roli-TC-group. And the incidence of the side-effects, including anorexia, flare, slight increase of serum GOT and GPT, and 1 case of allergic hives, and 3 cases of paralysed feeling of the tongue, were 3. 2 % in DOTC-group and 7. 8 % in Roli-TC-group. Four cases in Roli-TC-group dropped out due to lack of post-administrative clinical data, including one case of exanthema as the side-effect...|$|E
40|$|Fracture {{mechanics}} calculations {{are made}} {{to ensure the safety}} of the moderator vessel against failure by fracture. The 6061 -T 6 aluminum alloy is used for the moderator vessel structure. The fracture analysis of the moderator vessel consists of: (1) the probability of fracture calculations at the locations of the moderator where either the primary stress or the secondary stress assumes the highest value, (2) the vessel wall leak-before-break analysis by applying an edge crack solution, and (3) the crack penetration calculation as a result of radiation embrittlement by applying the flaw assessment diagram (FAD). The probability of fracture for the capsule is calculated by using a <b>direct</b> <b>probability</b> integration method instead of the Monte Carlo simulation method used by the PRAISE Code or the FAVOR Code developed in Oak Ridge. The probability of fracture as a function of radiation embrittlement is obtained. The leak before break analysis indicates that the vessel will fail by leak before fail by catastrophic fracture. A mass spectrometer will be installed to monitor the leak of hydrogen circulating within the moderator...|$|E
40|$|International audienceA multi {{detection}} system consisted of 90 3 He counters, gamma detector and beta detector has been installed at beamline of the accelerator complex ALTO. The system allows performing <b>direct</b> measurements of <b>probability</b> of delayed neutron emission from spontaneous fission fragments. A trial experiment {{was carried out}} to test the {{detection system}} created...|$|R
40|$|In this paper, we derive {{a second}} order mean field theory for <b>directed</b> {{graphical}} <b>probability</b> models. By using an information theoretic argu-ment it is shown how {{this can be}} done in the absense of a partition function. This method is a direct generalisation of the well-known TAP approximation for Boltzmann Machines. In a numerical example, it is shown that the method greatly improves the first order mean field ap-proximation. For a restricted class of graphical models, so-called single overlap graphs, the second order method has comparable complexity to the first order method. For sigmoid belief networks, the method is shown to be particularly fast and effective. ...|$|R
40|$|<b>Direct</b> K-shell {{ionization}} <b>probabilities</b> {{were measured}} in coincidence with elastically scattered particles in 30 -MeV/u Ne+Sn, Tb, Pb, Th and 8. 3 -MeV/u C+Zr, Ag, Sn, Sm, Au, Pb, Th reactions. Experimental data {{were compared with}} calculations in the semiclassical approximation. The transitional behavior at the reduced velocity xi(K) approximate to 1, where the projectile velocity approaches the velocity of the K-shell electrons, is discussed...|$|R
40|$|The problem {{motivating}} {{the paper}} is the quantification of students' preferences regarding teaching/coursework quality, under certain numerical restrictions, in order to build a model for identifying, assessing and monitoring the major components of the overall academic quality. After reviewing the strengths and limitations of conjoint analysis and of the random coefficient regression model used in similar problems in the past, we propose a Bayesian beta regression model with a Dirichlet prior on the model coefficients. This approach not only allows for the incorporation of informative prior when it is available but also provides user friendly interfaces and <b>direct</b> <b>probability</b> interpretations for all quantities. Furthermore, it is a natural way to implement the usual constraints for the model weights/coefficients. This model was applied to data collected in 2009 and 2013 from undergraduate students in Panteion University, Athens, Greece and besides the construction of an instrument for the assessment and monitoring of teaching quality, it gave some input for a preliminary discussion on the association of the differences in students preferences between the two time periods with the current Greek economic and financial crisis...|$|E
40|$|Future {{military}} {{communications systems}} {{are expected to}} exchange information in a seamless and reliable manner across heterogeneous networks composed of both jixed, wired nodes and mobile, wireless nodes. While commercial pro-tocols such as TCP/IP have improved ihe seamiessness of military communications systems, the use of TCP/IP in the mobile, wireless environment {{for which it was}} not designed can have a deleterious effect on reliability. Reliable perfor-mance of communication systems is ultimately limited by the occurrence of rare events, such as the probability of packet loss. Here we consider a model of the behavior of TCP/IP at a wired- wireless interface in the network which requires the use of Monte Carlo (MC) simulation to estimate the rare event probabilities. To achieve the large speedup required to make MC simulation feasible, we adapt our novel impor-tance sampling (IS) technique called <b>Direct</b> <b>Probability</b> Re-distribution (DPR) to the problem of estimating packet loss at the wired- wireless interface in TCP/IP based networks. We demonstrate that DPR is an eficient simulation tech-nique capable of working in the feedback environment result-ing from TCP/IP, and obtain orders-of-magnitude speedup of the Monte Carlo simulation used to estimate the packet loss rate in such systems 1...|$|E
40|$|In many {{fields of}} {{research}} null hypothesis significance tests and p values are the accepted way of assessing {{the degree of}} certainty with which research results can be extrapolated beyond the sample studied. However, there are very serious concerns about the suitability of p values for this purpose. An alternative approach is to cite confidence intervals for a statistic of interest, {{but this does not}} directly tell readers how certain a hypothesis is. Here, I suggest how the framework used for confidence intervals could easily be extended to derive confidence levels, or "tentative probabilities", for hypotheses. This allows researchers to state their confidence in a hypothesis as a <b>direct</b> <b>probability,</b> instead of circuitously by p values referring to an unstated, hypothetical null hypothesis. The inevitable difficulties of statistical inference mean that these probabilities can only be tentative, but probabilities are the natural way to express uncertainties, so, arguably, researchers using statistical methods have an obligation to estimate how probable their hypotheses are by the best available method. Otherwise misinterpretations will fill the void. Key words: Null hypothesis significance test, Confidence interval, Statistical inferenceComment: 7 pages, 1 figure, 3 table...|$|E
40|$|AbstractThe paper {{examined}} the catalysts of low-temperature ammonia synthesis Ru-Cs/Sibunit in which Sibunit, mesoporous carbon carrying agents, vary considerably in specific surface area and pore volume, but the pore sizes are similar. According to Raman spectroscopy, the Sibunit structures are same. Applied ruthenium dispersiveness, determined by CO chemisorption, are also close (∼ 13 - 16 %) {{for all the}} examined catalysts. The difference in {{the activity of the}} samples is due to the different <b>direct</b> contact <b>probability</b> of the particles of ruthenium and modifier (CsOx). This probability decreases with increasing specific surface area (SBEТ Sibunit) that are supposed to cause the decrease of catalyst activity on high-superficial Sibunit in the ammonia synthesis reaction...|$|R
50|$|Nomic Probability and the Foundations of Induction, Oxford, 1990 was Pollock's deep {{investigation}} {{of the relationship between}} defeasible reasoning and the estimation of <b>probability</b> from frequencies (<b>direct</b> inference of <b>probability).</b> It is a maturation of ideas originally found in a 1983 Theory and Decision paper. This work must be compared to Henry E. Kyburg's theories of probability, although Pollock believed that he was theorizing about a broader variety of statistical inferences.|$|R
40|$|The {{subject of}} the article {{concerns}} the issues of ancient Greek myth, its modifications and functions. The starting point for a such formulated problem is the Aristotle’s rules to which a myth is subject. The philosopher links these rules with the theory of tragedy’s poetics, that should be <b>directed</b> by <b>probability.</b> Following the standards, that were in force in Antiquity, a function, {{which had to be}} fulfilled by a myth, determined the way of using mythology by Greek dramatists. The modern culture reinterpret a Greek myth. A function, which a myth has to fulfil, as in ancient times, is its basis and determinant. The task of a myth is, first of all, indicated as an universalization of the values, that are inscribed in this myth...|$|R
