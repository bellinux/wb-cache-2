24|395|Public
40|$|Code {{obfuscation}} {{is intended}} to thwart reverse engineering by making programmes hard to understand. Call chains collected by stack tracing {{can be used to}} understand the behaviour of programmes. To hinder reverse analysis of stack tracing, a binary code obfuscation method based on random obfuscated table and <b>hash</b> <b>coding</b> is proposed. Random obfuscated table is used to map call addresses while call and ret instructions are executing. <b>Hash</b> <b>coding</b> and random value can be used to encode and decode the data of stack frames in the run-time programmes. Experiment and analysis show that the obfuscation can effectively impede stack trace analysis and increase the cost of reverse analysis for programmes. 	Code obfuscation {{is intended to}} thwart reverse engineering by making programmes hard to understand. Call chains collected by stack tracing can be used to understand the behaviour of programmes. To hinder reverse analysis of stack tracing, a binary code obfuscation method based on random obfuscated table and <b>hash</b> <b>coding</b> is proposed. Random obfuscated table is used to map call addresses while call and ret instructions are executing. <b>Hash</b> <b>coding</b> and random value can be used to encode and decode the data of stack frames in the run-time programmes. Experiment and analysis show that the obfuscation can effectively impede stack trace analysis and increase the cost of reverse analysis for programmes. </p...|$|E
40|$|A general {{method of}} <b>hash</b> <b>coding</b> is {{described}} {{together with an}} application for programs which play board games such as checkers, chess, and GO. An auxiliary method which detects retrieval errors is proposed. The error rate can be precisely controlled depending upon how much space in the hash table {{is devoted to the}} auxiliary method...|$|E
30|$|The {{proposed}} tracker, however, consumed {{much more}} time than the CAM-Shift, which was a common problem of PFs. Fortunately, the development of computer technology has provided many ways to solve the time consuming problem, such as to use parallel computing enabled by multi-core processors [25 â€“ 28] and to use <b>hash</b> <b>coding</b> techniques to improve the efficiency [29, 30].|$|E
40|$|Similarity search {{plays an}} {{important}} role in many applications involving high-dimensional data. Due to the known dimensionality curse, the performance of most existing indexing structures degrades quickly as the feature dimensionality increases. Hashing methods, such as locality sensitive hashing (LSH) and its variants, have been widely used to achieve fast approximate similarity search by trading search quality for efficiency. However, most existing hashing methods make use of randomized algorithms to generate <b>hash</b> <b>codes</b> without considering the specific structural information in the data. In this paper, we propose a novel hashing method, namely, robust hashing with local models (RHLM), which learns a set of robust hash functions to map the high-dimensional data points into binary <b>hash</b> <b>codes</b> by effectively utilizing local structural information. In RHLM, for each individual data point in the training dataset, a local hashing model is learned and used to predict the <b>hash</b> <b>codes</b> of its neighboring data points. The local models from all the data points are globally aligned so that an optimal <b>hash</b> <b>code</b> can be assigned to each data point. After obtaining the <b>hash</b> <b>codes</b> of all the training data points, we design a robust method by employing l(2),(1) -norm minimization on the loss function to learn effective hash functions, which are then used to map each database point into its <b>hash</b> <b>code.</b> Given a query data point, the search process first maps it into the query <b>hash</b> <b>code</b> by the <b>hash</b> functions and then explores the buckets, which have similar <b>hash</b> <b>codes</b> to the query <b>hash</b> <b>code.</b> Extensive experimental results conducted on real-life datasets show that the proposed RHLM outperforms the state-of-the-art methods in terms of search quality and efficiency...|$|R
40|$|We {{introduce}} an all-optical, format transparent <b>hash</b> <b>code</b> generator and a hash comparator {{for data}} packets verification with low latency at high baudrate. The device is reconfigurable {{and able to}} generate <b>hash</b> <b>codes</b> based on arbitrary functions and perform the comparison directly in the optical domain. <b>Hash</b> <b>codes</b> are calculated with custom interferometric circuits implemented with a Fourier domain optical processor. A novel nonlinear scheme featuring multiple four-wave mixing processes in a single waveguide is implemented for simultaneous phase and amplitude comparison of the <b>hash</b> <b>codes</b> before and after transmission. We demonstrate the technique with single polarisation BPSK and QPSK signals up to a data rate of 80 Gb/s...|$|R
40|$|For {{efficiently}} retrieving nearest neighbours from large-scale multi-view data, recently hashing {{methods are}} widely investigated, which can substantially improve query speeds. In this paper, we propose an effective probability-based Semantics-Preserving Hashing method {{to tackle the}} problem of cross-view retrieval, termed SePH. Considering the semantic consistency between views, SePH generates one unified <b>hash</b> <b>code</b> for all observed views of any instance. For training, SePH firstly transforms the given semantic affinities of training data into a probability distribution, and aims to approximate it with another one in Hamming space, via minimizing their Kullback-Leibler divergence. Specifically, the latter probability distribution is derived from all pair-wise Hamming distances between to be-learnt <b>hash</b> <b>codes</b> of the training data. Then with learnt <b>hash</b> <b>codes,</b> any kind of predictive models like linear ridge regression, logistic regression or kernel logistic regression, can be learnt as hash functions in each view for projecting the corresponding view-specific features into <b>hash</b> <b>codes.</b> As for out of-sample extension, given any unseen instance, the learnt hash functions in its observed views can predict view-specific <b>hash</b> <b>codes.</b> Then by deriving or estimating the corresponding output probabilities w. r. t the predicted view-specific <b>hash</b> <b>codes,</b> a novel probabilistic approach is further proposed to utilize them for determining a unified <b>hash</b> <b>code.</b> To evaluate the proposed SePH, we conduct extensive experiments on diverse benchmark datasets,and the experimental results demonstrate that SePH is reasonable and effective...|$|R
40|$|Hashing aims at {{generating}} highly compact similarity preserving {{code words}} which are {{well suited for}} large-scale image retrieval tasks. Most existing hashing methods first encode the images as a vector of hand-crafted features followed by a separate binarization step to generate hash codes. This two-stage process may produce sub-optimal encoding. In this paper, for the first time, we propose a deep architecture for supervised hashing through residual learning, termed Deep Residual Hashing (DRH), for an end-to-end simultaneous representation learning and <b>hash</b> <b>coding.</b> The DRH model constitutes four key elements: (1) a sub-network with multiple stacked residual blocks; (2) hashing layer for binarization; (3) supervised retrieval loss function based on neighbourhood component analysis for similarity preserving embedding; and (4) hashing related losses and regularisation to control the quantization error and {{improve the quality of}} <b>hash</b> <b>coding.</b> We present results of extensive experiments on a large public chest x-ray image database with co-morbidities and discuss the outcome showing substantial improvements over the latest state-of-the art methods. Comment: Submitted to Information Processing in Medical Imaging, 2017 (Under review...|$|E
40|$|Hashing {{is widely}} applied to {{approximate}} nearest neighbor search for large-scale multimodal retrieval with storage and computation efficiency. Cross-modal hashing improves {{the quality of}} <b>hash</b> <b>coding</b> by exploiting semantic correlations across different modalities. Existing cross-modal hashing methods first transform data into low-dimensional feature vectors, and then generate binary codes by another separate quantization step. However, suboptimal hash codes may be generated since the quantization error is not explicitly minimized and the feature representation is not jointly optimized with the binary codes. This paper presents a Correlation Hashing Network (CHN) approach to cross-modal hashing, which jointly learns good data representation tailored to <b>hash</b> <b>coding</b> and formally controls the quantization error. The proposed CHN is a hybrid deep architecture that constitutes a convolutional neural network for learning good image representations, a multilayer perception for learning good text representations, two hashing layers for generating compact binary codes, and a structured max-margin loss that integrates all things together to enable learning similarity-preserving and high-quality hash codes. Extensive empirical study shows that CHN yields {{state of the art}} cross-modal retrieval performance on standard benchmarks. Comment: 7 page...|$|E
40|$|Efficient C/C++ Programming {{describes}} a practical, real-world approach to efficient C/C++ programming. Topics covered range from {{how to save}} storage using a restricted character set and how to speed up access to records by employing <b>hash</b> <b>coding</b> and caching. A selective mailing list system is used to illustrate rapid access to and rearrangement of information selected by criteria specified at runtime. Comprised of eight chapters, this book begins by discussing factors to consider when deciding whether a program needs optimization. In the next chapter, a supermarket price lookup system is used t...|$|E
30|$|In Fixed length block-level {{deduplication}} technique any {{format of}} virtual disk image file {{is taken as}} input and the file is split into small chunks each of size 4 [*]KB. Using the SHA algorithm, the <b>hash</b> <b>code</b> of each chunk is calculated. The <b>hash</b> <b>code</b> and chunk name are stored in the hash table. If any further in coming chunk having the same <b>hash</b> <b>code,</b> the chunk is not stored in the hash table. Thus, the duplicates are eliminated. As the file size is small, {{there is no significant}} impact on live migration performance.|$|R
40|$|Visual search {{over large}} image {{repositories}} {{in real time}} {{is one of the}} key challenges for applications like mobile visual query by capture, augmented reality, biometrics based person identification, and effective management of billions of images being uploaded on the web. In the state-of-the-art mobile visual search scheme, images are represented as a binary <b>hash</b> <b>code.</b> With this <b>hash</b> <b>code</b> modeling, visual search is converted to comparing <b>hash</b> <b>code</b> of query input to database images. In this article, we present that the order statistics are much more reliable than weighed hamming distance when verifying two <b>hash</b> <b>codes.</b> As we will discuss further in this article, our simulation on the test model and the dataset provided by the MPEG Compact Descriptor for Visual Search (CDVS) group have favorably supported our claims...|$|R
30|$|Send the new two-terminal graphâ€™s <b>hash</b> <b>code</b> to <b>hash</b> table.|$|R
40|$|Thus even if hl is {{not already}} reduced modulo n, a sub-traction of n (rather than a more {{expensive}} division by n) will make it so. Since even this subtraction is needed only about Q/n of the time, a small Q (as in the last paragraph) saves subtract time. 6. Conclusion We have presented a new algorithm for <b>hash</b> <b>coding.</b> It {{has been shown to}} possess certain attributes that are de-sired in such algorithms. Specifically it is simple, efficient, exhaustive, needs little time per probe, and uses few probes per lookup...|$|E
40|$|Abstract: Recently, {{due to the}} {{existence}} of semantic gap between image visual features and human concepts, the seman-tic of image auto-annotation has become an important topic. Firstly, by extract low-level visual features of the image, and the corresponding Hash method, mapping the feature into the corresponding <b>Hash</b> <b>coding,</b> eventually, transformed that into a group of binary string and store it, image auto-annotation by search is a popular method, we can use it to design and implement a method of image semantic auto-annotation. Finally, Through the test based on the Corel image set, and the results show that, this method is effective...|$|E
40|$|This paper {{proposes to}} learn binary hash codes within a {{statistical}} learning framework, {{in which an}} upper bound of the probability of Bayes decision errors is derived for different forms of hash functions and a rigorous proof of the convergence of the upper bound is presented. Conse-quently, minimizing such an upper bound leads to consistent performance improvements of existing hash code learning algorithms, regardless of whether original algorithms are unsupervised or supervised. This paper also illustrates a fast <b>hash</b> <b>coding</b> method that exploits simple binary tests to achieve orders of magnitude improvement in coding speed as compared to projection based methods. 1...|$|E
40|$|An image hash is a {{short code}} that holds {{the content of the}} image. Image {{authentication}} using <b>hash</b> <b>code</b> is most commonly used technique. This image hash can be generated using many techniques. The major drawback of image hash is the possibility to have the same <b>hash</b> <b>code</b> for two different images. However, this vulnerability can be greatly reduced by generating <b>hash</b> <b>code</b> that uses both local and global features. In this paper, a survey on some feature extraction technique using local and global features is given. The advantages and disadvantages of each and every system along with future work is also given...|$|R
30|$|Select an {{adjacency}} matrix, send its <b>hash</b> <b>code</b> to <b>hash</b> table.|$|R
5000|$|... bits: Number of [...] "partial pre-image" [...] (zero) bits in the <b>hashed</b> <b>code.</b>|$|R
40|$|As {{the rapid}} growth of {{multi-modal}} data, hashing methods for cross-modal retrieval have received considerable attention. Deep-networks-based cross-modal hashing methods are appealing as they can integrate feature learning and <b>hash</b> <b>coding</b> into end-to-end trainable frameworks. However, it is still challenging to find content similarities between different modalities of data due to the heterogeneity gap. To further address this problem, we propose an adversarial hashing network with attention mechanism to enhance the measurement of content similarities by selectively focusing on informative parts of multi-modal data. The proposed new adversarial network, HashGAN, consists of three building blocks: 1) the feature learning module to obtain feature representations, 2) the generative attention module to generate an attention mask, which is used to obtain the attended (foreground) and the unattended (background) feature representations, 3) the discriminative <b>hash</b> <b>coding</b> module to learn hash functions that preserve the similarities between different modalities. In our framework, the generative module and the discriminative module are trained in an adversarial way: the generator is learned to make the discriminator cannot preserve the similarities of multi-modal data w. r. t. the background feature representations, while the discriminator aims to preserve the similarities of multi-modal data w. r. t. both the foreground and the background feature representations. Extensive evaluations on several benchmark datasets demonstrate that the proposed HashGAN brings substantial improvements over other state-of-the-art cross-modal hashing methods. Comment: 10 pages, 8 figures, 3 table...|$|E
40|$|In {{this paper}} {{trade-offs}} among certain computational factors in <b>hash</b> <b>coding</b> are analyzed. The paradigm problem considered {{is that of}} testing a series of messages one-by-one for membership in a given set of messages. Two new hashcoding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are {{the size of the}} hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency. The new methods are intended {{to reduce the amount of}} space required to contain the hash-coded information from that associated with conventional methods. The reduction in space is accomplished by exploiting the possibility that a small fraction of errors of commission may be tolerable in some applications, in particular, applications in which a large amount of data is involved and a core resident hash area is consequently not feasible using conventional methods. In such applications, it is envisaged that overall performance could be improved by using a smaller core resident hash area in conjunction with the new methods and, when necessary, by using some secondary and perhaps time-consuming test to "catch " the small fraction of errors associated with the new methods. An example is discussed which illustrates possible areas of application for the new methods. Analysis of the paradigm problem demonstrates that allowing a small number of test messages to be falsely identified as members of the given set will permit a much smaller hash area to be used without increasing reject time. KEY WORDS AND PHRASES: <b>hash</b> <b>coding,</b> hash addressing, scatter storage, searching, storage layout, retrieval trade-ofFs, retrieval efficiency, storag...|$|E
40|$|Similarity search (nearest {{neighbor}} search) is {{a problem}} of pursuing the data items whose distances to a query item are the smallest from a large database. Various methods have been developed to address this problem, and recently a lot of efforts have been devoted to approximate search. In this paper, we present a survey on one of the main solutions, hashing, which has been widely studied since the pioneering work locality sensitive hashing. We divide the hashing algorithms two main categories: locality sensitive hashing, which designs hash functions without exploring the data distribution and learning to hash, which learns hash functions according the data distribution, and review them from various aspects, including hash function design and distance measure and search scheme in the <b>hash</b> <b>coding</b> space...|$|E
40|$|Hashing {{has been}} widely {{deployed}} to perform the Approximate Nearest Neighbor (ANN) search for the large-scale image retrieval {{to solve the problem}} of storage and retrieval efficiency. Recently, deep hashing methods have been proposed to perform the simultaneous feature learning and the <b>hash</b> <b>code</b> learning with deep neural networks. Even though deep hashing has shown the better performance than traditional hashing methods with handcrafted features, the learned compact <b>hash</b> <b>code</b> from one deep hashing network may not provide the full representation of an image. In this paper, we propose a novel hashing indexing method, called the Deep Hashing based Fusing Index (DHFI), to generate a more compact <b>hash</b> <b>code</b> which has stronger expression ability and distinction capability. In our method, we train two different architectureâ€™s deep hashing subnetworks and fuse the <b>hash</b> <b>codes</b> generated by the two subnetworks together to unify images. Experiments on two real datasets show that our method can outperform state-of-the-art image retrieval applications...|$|R
40|$|Hashing {{is one of}} {{the most}} popular and {{powerful}} approximate nearest neighbor search techniques for large-scale image retrieval. Most traditional hashing methods first represent images as off-the-shelf visual features and then produce <b>hashing</b> <b>codes</b> in a separate stage. However, off-the-shelf visual features may not be optimally compatible with the <b>hash</b> <b>code</b> learning procedure, which may result in sub-optimal <b>hash</b> <b>codes.</b> Recently, deep <b>hashing</b> methods have been proposed to simultaneously learn image features and <b>hash</b> <b>codes</b> using deep neural networks and have shown superior performance over traditional hashing methods. Most deep hashing methods are given supervised information in the form of pairwise labels or triplet labels. The current state-of-the-art deep hashing method DPSH li 2015 feature, which is based on pairwise labels, performs image feature learning and <b>hash</b> <b>code</b> learning simultaneously by maximizing the likelihood of pairwise similarities. Inspired by DPSH li 2015 feature, we propose a triplet label based deep hashing method which aims to maximize the likelihood of the given triplet labels. Experimental results show that our method outperforms all the baselines on CIFAR- 10 and NUS-WIDE datasets, including the state-of-the-art method DPSH li 2015 feature and all the previous triplet label based deep hashing methods. Comment: Appear in ACCV 201...|$|R
30|$|If its <b>hash</b> <b>code</b> {{already exists}} in the hash table, go to 8). Otherwise, continue.|$|R
40|$|In {{this paper}} {{trade-offs}} among certain computational factors in <b>hash</b> <b>coding</b> are analyzed. The paradigm problem con-sidered {{is that of}} testing a series of messages one-by-one for membership in a given set of messages. Two new hash-coding methods are examined and compared with a par-ticular conventional hash-coding method. The computational factors considered are {{the size of the}} hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency. The new methods are intended {{to reduce the amount of}} space required to contain the hash-coded information from that associated with conventional methods. The reduction in space is accomplished by exploiting the possibility that a small fraction of errors of commission may be tolerable in some applications, in particular, applications in which a larg...|$|E
40|$|Using {{a simple}} {{multiprocessor}} scheduling problem as a vehicle, we explore {{the behavior of}} tabu search algorithms using different tabu, local search and list management strategies. We found that random blocking of {{the tail of the}} tabu list always improved performance; but that the use of frequency-based penalties to discourage frequently selected moves did not. <b>Hash</b> <b>coding</b> without conflict resolution was an effective way to represent solutions on the tabu list. We also found that the most effective length of the tabu list depended on features of the algorithm being used, but not on the size and complexity of the problem being solved. The best combination of features included random blocking of the tabu list, tasks as tabus and a greedy local search. An algorithm using these features was found to outperform a recently published algorithm solving a similar problem...|$|E
40|$|Attributed {{relational}} graphs {{have shown}} good adequacy for representation and analysis in computer vision. In some applications, pattern recognition requires that an attributed graph representation {{of an object}} is matched with several similar graphs stored in a database. However, {{there is no single}} algorithm that could be recommended for this problem. In this study, some well-known attributed graph matching algorithms were compared to find out the relevant properties and the types of problems they are best suited for. In addition, an algorithm based on <b>hash</b> <b>coding,</b> developed by the authors, was considered. Some test results are given on both the speed and classification accuracy of the algorithms for artificial and real images. 1. INTRODUCTION Attributed graphs {{have turned out to be}} very useful data structures for image representation and understanding in Computer Vision systems [10]. Examples of graph-based representations of patterns include the picture languages of Shaw [11], [...] ...|$|E
40|$|Finding {{visually}} identical {{images in}} large image collections {{is important for}} many applications such as intelligence propriety protection and search result presentation. Several algorithms {{have been reported in}} the literature, but they are not suitable for large image collections. In this paper, a novel algorithm is proposed to handle the situation, in which each image is compactly represented by a <b>hash</b> <b>code.</b> To detect duplicate images, only the <b>hash</b> <b>codes</b> are required. In addition, a very efficient search method is implemented to quickly group images with similar <b>hash</b> <b>codes</b> for fast detection. The experiments show that our algorithm can be both efficient and effective for duplicate detection in web image search. 1...|$|R
40|$|Many hashing {{techniques}} {{have been developed}} to secure data and store information like passwords, in the form of <b>hash</b> <b>codes,</b> which appear as a sequence of random characters. The hashing algorithms used have the pitfall that they can be reverse engineered using large tables that contain <b>hash</b> <b>codes</b> for frequently used passwords. In this paper a methodology is presented to protect against such attacks by using multiple hashing algorithms together...|$|R
40|$|Hashing {{has emerged}} as a popular {{technique}} for large-scale similarity search. Most learning-based hashing methods generate compact yet correlated <b>hash</b> <b>codes.</b> However, this redundancy is storage-inefficient. Hence we propose a lossless variable-length hashing (VLH) method that is both storage- and search-efficient. Storage efficiency is achieved by converting the fixed-length <b>hash</b> <b>code</b> into a variable-length code. Search efficiency is obtained by using a multiple hash table structure. With VLH, we are able to deliberately add redundancy into <b>hash</b> <b>codes</b> to improve retrieval performance with little sacrifice in storage efficiency or search complexity. In particular, we propose a block K-means hashing (B-KMH) method to obtain significantly improved retrieval performance with no increase in storage and marginal increase in computational cost. Comment: 10 pages, 6 figure...|$|R
40|$|In this paper, it {{is argued}} that single {{function}} dual process theory is a more credible psychological account of non-monotonicity in human conditional reasoning than recent attempts to apply logic programming (LP) approaches in artificial intelligence to these data. LP is introduced and among other critiques, {{it is argued}} that it is psychologically unrealistic in a similar way to <b>hash</b> <b>coding</b> in the classicism vs. connectionism debate. Second, {{it is argued that}} causal Bayes nets provide a framework for modelling probabilistic conditional inference in System 2 that can deal with patterns of inference LP cannot. Third, we offer some speculations on how the cognitive system may avoid problems for System 1 identified by Fodor in 1983. We conclude that while many problems remain, the probabilistic single function dual processing theory is to be preferred over LP as an account of the non-monotonicity of human reasoning...|$|E
40|$|Salton, ' {{published}} in 1968, {{deals with the}} computer processing of large information files and encompassss a large variety of useful techniques. Salton treats extensively such subjects as associations and relations among data items {{and the use of}} mathematical and statistical techniques for searching and retrieving information. The research of the problem indicates that relational analysis of the stored data items can provide useful information. Levien and Maron published a paper in 1967 in which they describe a system called the relational data file, which is used for the logical analysis of data. ' Childs, in a paper {{published in}} 1968, uses a set-theory approach for data structuring and relational analysis. &quot; Childs indicates that his set-theoretic data structure approach relies on set operations to do the work usually allocated to pointers or <b>hash</b> <b>coding</b> as in list structures, ring structures, associative structures, and relational files...|$|E
30|$|D-CNNs {{have a broad}} set of {{applications}} in image classification, object recognition, and detection. Glasssix [38] trained a CNN with an improved ResNet 34 layer and obtained 99.83 % accuracy on the famous LFW face recognition database. Considering the scale, context, sampling, and deep combined convolutional networks, the BDTA team won the championship of the ILSVRC 2017 object detection task. The Subbmission 4 model provided by BDTA can detect 85 object categories and achieved a 0.73 mean average precision on DET task 1 a (object detection with provided training data) [39]. Chen et al. provided an effective CNN named Dual Path Networks for object localization and object classification, which obtained a 6.2 % localization error rate and a 3.4 % classification error rate on the ILSVRC 2017 object localization task [40]. Yan et al. provided a supervised <b>hash</b> <b>coding</b> with deep neural network for environment perception of intelligent vehicles, and the proposed method can obviously improve the search accuracy [41].|$|E
50|$|A {{superimposed}} code such as Zatocoding {{is a kind}} of <b>hash</b> <b>code</b> {{that was}} popular in marginal punched-card systems.|$|R
40|$|Recently, hashing {{methods have}} been widely used in {{large-scale}} image retrieval. However, most existing hashing methods did not consider the hierarchical relation of labels, which means that they ignored the rich information stored in the hierarchy. Moreover, most of previous works treat each bit in a <b>hash</b> <b>code</b> equally, which does not meet the scenario of hierarchical labeled data. In this paper, we propose a novel deep hashing method, called supervised hierarchical deep hashing (SHDH), to perform <b>hash</b> <b>code</b> learning for hierarchical labeled data. Specifically, we define a novel similarity formula for hierarchical labeled data by weighting each layer, and design a deep convolutional neural network to obtain a <b>hash</b> <b>code</b> for each data point. Extensive experiments on several real-world public datasets show that the proposed method outperforms the state-of-the-art baselines in the image retrieval task. Comment: 9 page...|$|R
5000|$|... bitstate hashing (instead of storing whole states, {{only their}} <b>hash</b> <b>code</b> is remembered in a bitfield; this saves {{a lot of}} memory but voids completeness); ...|$|R
