2|10000|Public
3000|$|Based on the {{findings}} of Naïve Forward Backward LASSO Granger Causality and Random Forest Granger causality, we are proposing to use Random Forest Granger causality together with the concept of re-utilization of time series data by reversing the <b>data</b> <b>time</b> <b>stamps</b> in order to maximize advantages in terms of precision, false discovery rate, recall, and F 1 -score. The pseudo code for evaluating Bi-directional Random Forest Granger causality is as follow: [...]...|$|E
40|$|AIM: To {{investigate}} the microvascular (skeletal muscle tissue oxygenation; SmO 2) response to transfusion in patients undergoing elective complex spine surgery. METHODS: After IRB approval and written informed consent, 20 patients aged 18 to 85 {{years of age}} undergoing 3 ̆e 3 level anterior and posterior spine fusion surgery {{were enrolled in the}} study. Patients were followed throughout the operative procedure, and for 12 h postoperatively. In addition to standard American Society of Anesthesiologists monitors, invasive measurements including central venous pressure, continual analysis of stroke volume (SV), cardiac output (CO), cardiac index (CI), and stroke volume variability (SVV) was performed. To measure skeletal muscle oxygen saturation (SmO 2) during the study period, a non-invasive adhesive skin sensor based on Near Infrared Spectroscopy was placed over the deltoid muscle for continuous recording of optical spectra. All administration of fluids and blood products followed standard procedures at the Hospital for Special Surgery, without deviation from usual standards of care at the discretion of the Attending Anesthesiologist based on individual patient comorbidities, hemodynamic status, and laboratory <b>data.</b> <b>Time</b> <b>stamps</b> were collected for administration of colloids and blood products, to allow for analysis of SmO 2 immediately before, during, and after administration of these fluids, and to allow for analysis of hemodynamic data around the same time points. Hemodynamic and oxygenation variables were collected continuously throughout the surgery, including heart rate, blood pressure, mean arterial pressure, SV, CO, CI, SVV, and SmO 2. Bivariate analyses were conducted to examine the potential associations between the outcome of interest, SmO 2, and each hemodynamic parameter measured using Pearson 2 ̆ 7 s correlation coefficient, both for the overall cohort and within-patients individually. The association between receipt of packed red blood cells and SmO 2 was performed by running an interrupted time series model, with SmO 2 as our outcome, controlling for {{the amount of time spent}} in surgery before and after receipt of PRBC and for the inherent correlation between observations. Our model was fit using PROC AUTOREG in SAS version 9. 2. All other analyses were also conducted in SAS version 9. 2 (SAS Institute Inc., Cary, NC, United States). RESULTS: Pearson correlation coefficients varied widely between SmO 2 and each hemodynamic parameter examined. The strongest positive correlations existed between ScvO 2 (P = 0. 41) and SV (P = 0. 31) and SmO 2; the strongest negative correlations were seen between albumin (P = - 0. 43) and cell saver (P = - 0. 37) and SmO 2. Correlations for other laboratory parameters studied were weak and only based on a few observations. In the final model we found a small, but significant increase in SmO 2 at the time of PRBC administration by 1. 29 units (P = 0. 0002). SmO 2 values did not change over time prior to PRBC administration (P = 0. 6658) but following PRBC administration, SmO 2 values declined significantly by 0. 015 units (P 3 ̆c 0. 0001). CONCLUSION: Intra-operative measurement of SmO 2 during large volume, yet controlled hemorrhage, does not show a statistically significant correlation with either invasive hemodynamic, or laboratory parameters in patients undergoing elective complex spine surgery...|$|E
5000|$|... #Caption: Validating {{a ticket}} using the <b>data</b> and <b>time</b> <b>stamp</b> machine ...|$|R
5000|$|WITS Field Devices {{can record}} local data log files. These contain <b>data</b> with a <b>time</b> <b>stamp</b> and {{retrieved}} later by a Master Station. The logs are retrieved {{by means of}} DNP3 file transfer.|$|R
40|$|Using silicon-based {{recording}} electrodes, {{we recorded}} neuronal {{activity of the}} dorsal hippocampus and dorsomedial entorhinal cortex from behaving rats. The entorhinal neurons were classified as principal neurons and interneurons based on monosynaptic interactions and wave-shapes. The hippocampal neurons were classified as principal neurons and interneurons based on monosynaptic interactions, wave-shapes and burstiness. The data set contains recordings from 7, 736 neurons (6, 100 classified as principal neurons, 1, 132 as interneurons, and 504 cells that did not clearly fit into either category) obtained during 442 recording sessions from 11 rats (a total of 204. 5 hours) while they were engaged in one of eight different behaviours/tasks. Both original and processed <b>data</b> (<b>time</b> <b>stamp</b> of spikes, spike waveforms, result of spike sorting and local field potential) are included, along with metadata of behavioural markers. Community-driven data sharing may offer cross-validation of findings, refinement of interpretations and facilitate discoveries...|$|R
30|$|In our system, the {{secondary}} system is decentralized. We exploit Bitcoin mechanism {{to achieve an}} auction incentive. The original intention {{of this approach is}} that Bitcoin is a decentralized peer-to-peer(P 2 P) digital currency [49]. With the idea of blockchain in Bitcoin [50, 51], we can implement decentralized credit-based P 2 P transactions in distributed systems without mutual trust, by means of <b>data</b> encryption, <b>time</b> <b>stamping,</b> distributed consensus, and economic incentive. Thus, the blockchain technology ensures the normal and reliable circulation of Bitcoin.|$|R
40|$|Scheduling {{occurs in}} every {{organization}} without considering {{the nature of}} its activities. In this regard, numerous scholars have attempted to schedule via divergent methods including classical scheduling, genetic algorithm, neural network and fuzzy logic and so on. Studies in manufacturing scheduling mostly deal with priority rules without any consideration of the system states. An appropriate scheduling leads to significant enhancement of fairness in job scheduling. The term fairness can be transformed into a specific selection of job weights. There is no method of scheduling in which priority, time action (duration) and <b>time</b> <b>stamp</b> of jobs have simultaneously been considered. But the proposed method of scheduling can enhance the efficiency and reliability of manufacturing systems. To fulfill this target, first and foremost, the normalize method should be performed. This method allows <b>data</b> (<b>time</b> <b>stamp,</b> <b>time</b> action, priority) of jobs on different scales to be compared by bringing them to a common scale. Secondly, the jobs should be arranged based on three criteria which are priority, time action and <b>time</b> <b>stamp.</b> This sorting algorithm is programmed via MATLAB Distributed Computing Server (DCS) software. Eventually, to evaluate the proposed method of scheduling, simulation is operated. The simulated algorithm shows that applying the proposed method of scheduling increases the efficiency of simulated scheduler in comparison with applying the common method of scheduling. Besides the mentioned simulated algorithm, there is a mathematical proof to show the enhancement of reliability. ...|$|R
40|$|Previously {{there have}} been {{difficulties}} in accurately establishing the time correlation between events reported on ground via packet telemetry data and their actual time of occurrence in an onboard application. The Local Time Management System is a radiation hard component enabling users to overcome this problem by <b>time</b> <b>stamping</b> <b>data</b> at source. The high resolution and precision of the local time reference, with which <b>data</b> may be <b>time</b> <b>stamped,</b> is maintained automatically by this new component. ...|$|R
40|$|We {{report on}} the {{enhancements}} to MNEMOSINE, the Flight Test Instrumentation system designed and developed by the Department to meet the specific requirements of Flight Testing Ultra Light Machine. The system, initially conceived and put up to fulfill bot research and didactic activities carried out on the Department’s owned and operated TECNAM P 92, after dozens of Flight Test missions has shown it’s quality, reliability ad dependability. It has recently been signed an agreement between the Department and Nando Groppo, {{one of the leading}} Italian Ultra Light Machine (ULM) manufacturers, aimed to the execution of Flight Testing activities on Groppo’s brand new machine, in a joint effort. Using this occasion, and capitalizing the feedback from the completed Flight Test activities, MNEMOSINE has undergone a major improvement in order to be ready to be used as one of the principal resources for the planned certification of the XL. Enhancements and additions include a brand new, rough and accurate Air Data system, an experimental Flight Control force measurement system, a very accurate <b>Data</b> <b>Time</b> <b>Stamping</b> feature and finally a Top Switch counter...|$|R
40|$|Abstract—New {{experiments}} at FAIR require {{new concepts}} of data acquisition systems. Instead of building hardware trigger systems with strict latency limitations, {{it is intended}} to use self-triggered electronics. Front-end components should be time synchronized and will provide <b>data</b> with <b>time</b> <b>stamps.</b> <b>Data</b> streams from many of such components should be forwarded over a powerful sorting network to an event building farm. The Data Acquisition Backbone Core (DABC) is designed as a general purpose software framework for the implementation of such data acquisition systems. It is based on C++ and Java. A first version is now published and available...|$|R
40|$|Infectious {{diseases}} can {{be considered}} to spread over social networks of people or animals. Mainly owing {{to the development of}} data recording and analysis techniques, an increasing amount of social contact <b>data</b> with <b>time</b> <b>stamps</b> has been collected in the last decade. Such temporal data capture the dynamics of social networks on a timescale relevant to epidemic spreading and can potentially lead to better ways to analyze, forecast, and prevent epidemics. However, they also call for extended analysis tools for network epidemiology, which has, to date, mostly viewed networks as static entities. We review recent results of network epidemiology for such temporal network data and discuss future developments...|$|R
40|$|Wireless sensor {{networks}} were initially deployed for military applications. Gradually researchers {{found them to}} be very useful in applications like weather monitoring, target tracking, agriculture, industrial applications, and recently smart homes and kindergartens. All the WSN applications need partial or full time synchronization. Applications like acoustic ranging, target tracking or monitoring need a common notion of <b>time.</b> Every <b>data</b> is <b>time</b> <b>stamped</b> sensor nodes local clock. Two main approaches to time synchronization are receiver-receiver synchronization and sender-receiver synchronization. In this paper we analyze the receiver-receiver synchronization and discuss the results of simulation in network simulator. This study, design considerations and simulation methodology will help a lot to the designer for designing a time synchronization scheme or system...|$|R
40|$|This is the publisher’s final pdf. The {{published}} {{article is}} copyrighted by Human Kinetics, Inc. {{and can be}} found at: [URL] {{purpose of this study was}} to examine the physical activity patterns of older adults with intellectual disabilities (ID) in comparison with younger adults with ID and older adults without ID. A sample of 109 participants was included in the study. Sophisticated <b>data</b> reduction, <b>time</b> <b>stamped</b> technology, and multiple objective measures (i. e., pedometers and accelerometers) were used to determine physical activity intensities and walking patterns of participants. Results indicate that older adults with ID are performing less physical activity than comparison groups. A small proportion of older adults with ID (6...|$|R
40|$|During {{magnetic}} observatory data acquisition, the <b>data</b> <b>time</b> <b>stamp</b> is kept synchronized with {{a precise}} source of time. This is usually done using a GPS-controlled pulse per second (PPS) signal. For some observatories located {{in remote areas}} or where internet restrictions are enforced, only the magnetometer data are transmitted, limiting the capabilities of monitoring the acquisition operations. The magnetic observatory in Lanzhou (LZH), China, experienced an unnoticed interruption of the GPS PPS starting 7  March  2013. The data logger clock drifted slowly in time: in 6 months a lag of 27  s was accumulated. After a reboot on 2  April  2014 the drift became faster, − 2  s per day, before the GPS PPS could be restored on 8  July  2014. To estimate the time lags that LZH time series had accumulated, we compared it with data from other observatories located in East Asia. A synchronization algorithm was developed. Natural sources providing synchronous events {{could be used as}} markers to obtain the time lag between the observatories. The analysis of slices of 1  h of 1  s data at arbitrary UTC allowed estimating time lags with an uncertainty of ∼[*] 11  s, revealing the correct trends of LZH time drift. A precise estimation of the time lag was obtained by comparing data from co-located instruments controlled by an independent PPS. In this case, it was possible to take advantage of spikes and local noise that constituted precise time markers. It was therefore possible to determine a correction to apply to LZH <b>time</b> <b>stamps</b> to correct the data files and produce reliable 1  min averaged definitive magnetic data...|$|R
30|$|In {{addition}} to the gas measurement, we observed infrasonic waves at a station of the crater rim (site 4 in Fig.  1) where no effect by topographic barriers should be considered for their propagation. The station was equipped with a low-frequency microphone (Datamark SI 104; flat response in 0.1 – 1000  Hz) and a digitizer (Datamark LS- 8800) in which data were recorded into an SD card with a sampling frequency of 100  Hz. The <b>data</b> had GPS-synchronized <b>time</b> <b>stamps.</b>|$|R
40|$|A cabled {{system for}} {{collecting}} real-time seismic data {{has been developed}} and was deployed in spring of 2011. Nowadays (2013) this seismic station is {{being part of the}} Catalan Seismic Network managed by the IGC (Institut Geològic de Catalunya). The seismic system is part of Western Mediterranean Cabled Observatory, OBSEA (www. obsea. es). A key component in this cabled system is the use of IEEE 1588 standard that serves as a clock synchronization mechanism for the seismometer with Universal Time Coordinates (UTC) clock. This paper presents the seismic measurements results of the broadband seismometer. The seismic <b>data</b> are <b>time</b> <b>stamped</b> using a UTC clock which is traceable to within the desired level of precision of sub milliseconds through IEEE 1588 protocol. Peer ReviewedPostprint (published version...|$|R
40|$|The {{availability}} of large scale event <b>data</b> with <b>time</b> <b>stamps</b> {{has given rise}} to dynamically evolving knowledge graphs that contain temporal information for each edge. Reasoning over time in such dynamic knowledge graphs is not yet well understood. To this end, we present Know-Evolve, a novel deep evolutionary knowledge network that learns non-linearly evolving entity representations over time. The occurrence of a fact (edge) is modeled as a multivariate point process whose intensity function is modulated by the score for that fact computed based on the learned entity embeddings. We demonstrate significantly improved performance over various relational learning approaches on two large scale real-world datasets. Further, our method effectively predicts occurrence or recurrence time of a fact which is novel compared to prior reasoning approaches in multi-relational setting...|$|R
40|$|A voltage {{transient}} recorder can detect lightning induced transient voltages. The recorder detects a lightning induced transient voltage and adjusts input amplifiers to accurately record transient voltage magnitudes. The recorder stores voltage data from numerous monitored channels, or devices. The <b>data</b> is <b>time</b> <b>stamped</b> {{and can be}} output in real time, or stored for later retrieval. The {{transient recorder}}, in one embodiment, includes an analog-to-digital converter and a voltage threshold detector. When an input voltage exceeds a pre-determined voltage threshold, the recorder stores the incoming voltage magnitude and time of arrival. The recorder also determines if its input amplifier circuits clip the incoming signal or if the incoming signal is too low. If the input data is clipped or too low, the recorder adjusts the gain of the amplifier circuits to accurately acquire subsequent components of the lightning induced transients...|$|R
3000|$|Big <b>data</b> {{associated}} with <b>time</b> <b>stamp</b> is called big data stream. Sensor data, call centre records, click streams, and health- care data {{are examples of}} big data streams. Quality of service (QoS) parameters such as end-to-end delay, accuracy, and real-time processing are some constraints of big data stream processing. The most pre-requirement of big data stream mining in applications such as health-care is privacy preserving [54]. One of the common approaches to anonymize static data is k-anonymity. This approach is not directly applicable for the big data streams anonymization. The reasons are as follows [55]: [...]...|$|R
40|$|Abstract ⎯ Single-frequency {{simulcast}} networks use {{two or more}} Radio Base Stations (RBS) {{to transmit}} simultaneously the same signal on the same radio channel over the service area. To ensure correct operation and good performance, simulcast networks need accurate time and frequency synchronization. In this work, we designed a system for synchronizing RBS clocks by way of Network Time Protocol (NTP) packets. The goal is to allow simulcast operation when external sources of synchronization, such as GPS, are not available or when the operator prefers to distribute timing alternatively, in particular by IP network facilities used for <b>data</b> transport. <b>Time</b> <b>stamps</b> are disseminated by a master RBS and used in slave stations to steer local clocks. The mechanism proposed was modelled, simulated and implemented in an experimental prototype. Simulation and experimental measurement results meet requirements and demonstrate the practical feasibility of this approach for commercial application. Index Terms ⎯ Digital radio, land mobile radio cellular systems, radio broadcasting, synchronization, time dissemination. L I...|$|R
40|$|Sensing and {{detecting}} of {{the events}} through sensor networks is one of popular research areas. Sensors can help us not only to detect the pollution, but also to monitor the environment that is beyond our reach. When sensor nodes go on their missions, synchronization {{is a very important}} issue. If the clock in the all sensors is not synchronized, as they detect the event and send the data to the sink in the sensor network, the sequence of <b>data</b> with <b>time</b> <b>stamp</b> will be disorder. It is harder to read and identify data. Prior to our research, {{there are a lot of}} methods to synchronize all nodes in the sensor network, for example, Lightweight time synchronization for sensor networks (LTS), centralized multi-hop lightweight tree-based synchronization (CLTS), pulse-coupled, cluster-header and so on. The methods between the pulse-coupled and cluster-header have the same characters. When the sensor networks are synchronizing, the nodes in the pulse-coupled way and the cluster-header method will provide their own information to sync each other. In present study, we will compare the different between the pulse-coupled method and the cluster-header method...|$|R
5000|$|A phasor network {{consists}} of phasor measurement units (PMUs) dispersed throughout the electricity system, Phasor Data Concentrators (PDC) {{to collect the}} information and a Supervisory Control And Data Acquisition (SCADA) system at the central control facility. Such a network is used in Wide Area Measurement Systems (WAMS), {{the first of which}} began in 2000 by the Bonneville Power Administration. [...] The complete network requires rapid data transfer within the frequency of sampling of the phasor <b>data.</b> GPS <b>time</b> <b>stamping</b> can provide a theoretical accuracy of synchronization better than 1 microsecond. [...] "Clocks need to be accurate to ± 500 nanoseconds to provide the one microsecond time standard needed by each device performing synchrophasor measurement." [...] For 60 Hz systems, PMUs must deliver between 10 and 30 synchronous reports per second depending on the application. The PDC correlates the data, and controls and monitors the PMUs (from a dozen up to 60). [...] At the central control facility, the SCADA system presents system wide data on all generators and substations in the system every 2 to 10 seconds.|$|R
40|$|Degraded visual environments, such as {{brownout}} or whiteout, {{are addressed}} within the project ALLFlight (Assisted Low Level Flight and Landing on Unprepared Landing Sites) to demonstrate {{and evaluate the}} characteristics of different sensors for helicopter operations. Different sensor systems are mounted onto DLR’s research helicopter EC 135 and this sensor suite consists of standard color or black and white TV cameras, a millimeter wave radar system (AI- 130, ICx Radar Systems, Canada), an un-cooled thermal infrared camera (EVS- 1000, Max-Viz, USA), and an optical radar scanner (HELLAS-W, EADS, Germany). A high performance sensor co-computer (SCC) cluster architecture is responsible for data processing, which is installed into the helicopter’s experimental electronic cargo bay. Applied methods and the software architecture in terms of real <b>time</b> <b>data</b> acquisition, recording, <b>time</b> <b>stamping</b> and sensor <b>data</b> fusion will be described in this paper. A first approach for a pilot HMI is presented as well...|$|R
40|$|The {{objective}} of the project ALLFlight (Assisted Low Level Flight and Landing on Unprepared Landing Sites) is to demonstrate and evaluate the characteristics of different sensors for helicopter operations within degraded visual environments, such as brownout or whiteout. The sensor suite, which is mounted onto DLR’s research helicopter EC 135 consists of standard color or black and white TV cameras, an un-cooled thermal infrared camera (EVS- 1000, Max-Viz, USA), an optical radar scanner (HELLAS-W, EADS, Germany) and a millimeter wave radar system (AI- 130, ICx Radar Systems, Canada). Data processing is designed and realized by a sophisticated, high performance sensor co-computer (SCC) cluster architecture, which is installed into the helicopter’s experimental electronic cargo bay. This paper describes applied methods and the software architecture in terms of real <b>time</b> <b>data</b> acquisition, recording, <b>time</b> <b>stamping</b> and sensor <b>data</b> fusion. First concepts for a pilot HMI are presented as well...|$|R
40|$|Abstract: A generic {{temporal}} and fuzzy ontological framework (GTFOF) is presented, {{specific to the}} task of designing {{temporal and}} fuzzy database system. The framework includes both time and fuzzy dimension in deigning patient information system in a hospital environment. The proposed framework is essential for developing knowledge management systems (KMS) in healthcare environments. The importance of ontological models in the development of patient information system is well established and it provides the logical and formal mechanism for building KMS. Healthcare information systems may include complex <b>data</b> such as <b>time</b> <b>stamped</b> <b>data</b> and fuzzy data about patients. This paper highlights the importance of identifying the key concepts for building an ontological framework to manage both temporal and fuzzy information content. The proposed generic framework is capable of integrating and mapping the proposed ontological model into an effective database design for implementation...|$|R
40|$|Sharing {{real-time}} aggregate {{statistics of}} private data has given much {{benefit to the}} public to perform data mining for understanding important phenomena, such as Influenza outbreaks and traffic congestion. However, releasing time-series data with standard differential privacy mechanism has limited utility due to high correlation between data values. We propose FAST, an adaptive system to release real-time aggregate statistics under differential privacy with improved utility. To minimize overall privacy cost, FAST adaptively samples long time-series according to detected data dynamics. To improve the accuracy of <b>data</b> release per <b>time</b> <b>stamp,</b> filtering is used to predict data values at non-sampling points and to estimate true values from noisy observations at sampling points. Our experiments with three real data sets confirm that FAST improves the accuracy of time-series release and has excellent performance even under very small privacy cost...|$|R
40|$|International Telemetering Conference Proceedings / October 30 -November 02, 1995 / Riviera Hotel, Las Vegas, NevadaThe Telemetry Data Distribution System (TDDS) {{solves the}} need to record, archive, and {{distribute}} sounding rocket and satellite data on a compact, user-friendly medium, such as CD-Recordable discs. The TDDS also archives telemetry data on floppy disks, nine-track tapes, and magneto-optical disc cartridges. The PC-based, semi-automated, TDDS digitizes, <b>time</b> <b>stamps,</b> formats, and archives frequency modulated (FM) or pulse code modulated (PCM) telemetry data. An analog tape or a real-time signal may provide the telemetry data source. The TDDS accepts IRIG A, B, G, H, and NASA 36 analog code sources for <b>time</b> <b>stamp</b> <b>data.</b> The output <b>time</b> tag includes time, frame, and subframe status information. Telemetry <b>data</b> may be <b>time</b> <b>stamped</b> based upon a user-specified number of frames, subframes, or words. Once recorded, the TDDS performs data quality testing, formatting, and validation and logs the results automatically. Telemetry data is quality checked to ensure a good analog source track was selected. Raw telemetry data is formatted by dividing the data into records and appending header information. The formatted telemetry data is validated by checking consecutive time tags and subframe identification counter values (if applicable) to identify data drop-outs. After validation, the TDDS archives the formatted data {{to any of the}} following media types: CD-Recordable (CD-R) Disc (650 megabytes capacity); nine track tape (180 megabytes capacity); and erasable optical disc (499 megabytes capacity). Additionally, previously archived science data may be re-formatted and archived to a different output media...|$|R
40|$|Abstract—Sharing {{real-time}} aggregate {{statistics of}} private data {{is of great}} value to the public to perform data mining for understanding important phenomena, such as Influenza outbreaks and traffic congestion. However, releasing time-series data with standard differential privacy mechanism has limited utility due to high correlation between data values. We propose FAST, a novel framework to release real-time aggregate statistics under differential privacy based on filtering and adaptive sampling. To minimize the overall privacy cost, FAST adaptively samples long time-series according to the detected data dynamics. To improve the accuracy of <b>data</b> release per <b>time</b> <b>stamp,</b> FAST predicts <b>data</b> values at non-sampling points and corrects noisy observations at sampling points. Our experiments with real-world as well as synthetic data sets confirm that FAST improves the accuracy of released aggregates even under small privacy cost {{and can be used}} to enable a wide range of monitoring applications...|$|R
40|$|Barycentric {{corrections}} made to {{the timing}} of Kepler observations, necessitated by variations in light arrival time at the satellite, break the regular time-sampling of the <b>data</b> [...] the <b>time</b> <b>stamps</b> are periodically modulated. A consequence is that Nyquist aliases are split into multiplets that can be identified by their shape. Real pulsation frequencies are distinguishable from these aliases and their frequencies are completely recoverable, even in the super-Nyquist regime, that is, when the sampling interval is longer than half the pulsation period. We provide an analytical derivation of the phenomenon, alongside demonstrations with simulated and real Kepler data for δ Sct, roAp, and sdBV stars. For Kepler data sets spanning more than one Kepler orbital period (372. 5 d), there are no Nyquist ambiguities on the determination of pulsation frequencies, which are the fundamental data of asteroseismology. Comment: 14 pages, 13 figures of which 9 are colour. Submitted to MNRA...|$|R
40|$|Sharing {{real-time}} aggregate {{statistics of}} private data {{is of great}} value to the public to perform data mining for understanding important phenomena, such as Influenza outbreaks and traffic congestion. However, releasing time-series data with standard differential privacy mechanism has limited utility due to high correlation between data values. We propose FAST, a novel framework to release real-time aggregate statistics under differential privacy based on filtering and adaptive sampling. To minimize the overall privacy cost, FAST adaptively samples long time-series according to the detected data dynamics. To improve the accuracy of <b>data</b> release per <b>time</b> <b>stamp,</b> FAST predicts <b>data</b> values at non-sampling points and corrects noisy observations at sampling points. Our experiments with real-world as well as synthetic data sets confirm that FAST improves the accuracy of released aggregates even under small privacy cost {{and can be used}} to enable a wide range of monitoring applications...|$|R
40|$|In Software Defined Networks, {{where the}} network control plane can be {{programmed}} by updating switch rules, consistently updating switches is a challenging problem. In a per-packet consistent update (PPC), a packet either matches {{the new rules}} added or the old rules to be deleted, throughout the network, but not a combination of both. PPC must be preserved during an update to prevent packet drops and loops, provide waypoint invariance and to apply policies consistently. No algorithm exists today that confines changes required during an update to only the affected switches, yet preserves PPC and does not restrict applicable scenarios. We propose a general update algorithm called PPCU that preserves PPC, is concurrent and provides an all-or-nothing semantics for an update, irrespective of the execution speeds of switches and links, while confining changes to only the affected switches and affected rules. We use <b>data</b> plane <b>time</b> <b>stamps</b> to identify when the switches must move from the old rules to the new rules. For this, we use the powerful programming features provided to the data plane by the emerging programmable switches, which also guarantee line rate. We prove the algorithm, identify its significant parameters and analyze the parameters with respect to other algorithms in the literature...|$|R
40|$|The Small-Size Telescope with single-mirror (SST- 1 M) is a 4 m Davies-Cotton {{telescope}} and {{is among the}} proposed telescope designs for the Cherenkov Telescope Array (CTA). It is conceived to provide the high-energy ($>$ few TeV) coverage. The SST- 1 M contains proven technology for the telescope structure and innovative electronics and photosensors for the camera. Its design {{is meant to be}} simple, low-budget and easy-to-build industrially. Each device subsystem of an SST- 1 M telescope is made visible to CTA through a dedicated industrial standard server. The software is being developed in collaboration with the CTA Medium-Size Telescopes to ensure compatibility and uniformity of the array control. Early operations of the SST- 1 M prototype will be performed with a subset of the CTA central array control system based on the Alma Common Software (ACS). The triggered event <b>data</b> are <b>time</b> <b>stamped,</b> formatted and finally transmitted to the CTA data acquisition. The software system developed to control the devices of an SST- 1 M telescope is described, as well as the interface between the telescope abstraction to the CTA central control and the data acquisition system. Comment: In Proceedings of the 34 th International Cosmic Ray Conference (ICRC 2015), The Hague, The Netherlands. All CTA contributions at arXiv: 1508. 0589...|$|R
40|$|In {{common with}} many countries, {{emergency}} departments (EDs) in Australia are under stress. To reduce 'access block' (where service demands exceed the ED's capacity) work {{processes in the}} ED of a public hospital were analysed using animated simulation. This article describes organisational culture changes supported {{by the use of}} simulation as an impartial form of analysis and communication. <b>Data</b> collected included <b>time</b> <b>stamps</b> and booking schedules in the imaging department (ID), semi-structured interviews and patient flow observations from the ED to the ID. Implementation of ID staff-suggested improvements resulted in a 25 % increase in the capacity of the ultrasound department and a doubling of its ED cases. Improved communication between the ED and the ID, {{as a direct result of}} this project, has led to ongoing interdepartmental cooperation. One implication is that changes to health organisational culture can be assisted by computer simulations providing rapid and accurate predictions of change outcomes. Full Tex...|$|R
40|$|The {{majority}} of the database applications now a days deal with temporal data. Temporal records are known to change {{during the course of}} time and facilities to manage the multiple snapshots of these records are generally missing in conventional databases. Consequently, different temporal data models have been proposed and implemented {{as an extension of the}} temporal less database systems. In the single relation model, the present and past instances are stored in a single relation that makes its handling cumbersome and inefficient. This paper emphasize upon storing the past instances of the records in the multiple historical relations. The current relations will manage the recent snapshot of <b>data.</b> The tuple <b>time</b> <b>stamping</b> approach is used to timestamp the temporal records. This paper proposes a temporal model for the management of <b>time</b> varying <b>data</b> built on the top of conventional open source database. Indexing is used to enhance the performance of the model. The proposed model is also compared with the single relation model...|$|R
50|$|Vic Coin {{propose a}} {{solution}} that starts with a <b>time</b> <b>stamp</b> server. A <b>time</b> <b>stamp</b> server starts with taking a hash of a block of items to be <b>time</b> <b>stamped</b> and widely publishing the hash, such as in a newspaper or Usenet post. The <b>time</b> <b>stamp</b> provides assurance that the data in question must have existed at the time, {{in order to get}} into the hash. Each <b>time</b> <b>stamp</b> includes the previous <b>time</b> <b>stamp</b> in its hash, forming a chain, with each additional <b>time</b> <b>stamp</b> in forcing the ones before it.|$|R
40|$|The paper aims {{to analyze}} the {{importance}} of digital <b>time</b> <b>stamping</b> service and digital <b>time</b> <b>stamping</b> systems components, organization requirements for implementing digital <b>stamp</b> service system. <b>Time</b> <b>stamping</b> helps significantly increase the level of confidence currently required in a public-key infrastructure by {{making it possible to}} track timing of signing the documents. Therefore <b>time</b> <b>stamping</b> in many cases is becoming ultimate evidence resolving the status of documents. The paper describes following <b>Time</b> <b>stamping</b> System components - NMI and TSU interface, <b>Time</b> <b>stamping</b> unit (TSU), Archive, User interface, Control system, VPN router. The paper reflects findings from EC funded 6 th Framework project BALTICTIME (IST- 027751) ...|$|R
