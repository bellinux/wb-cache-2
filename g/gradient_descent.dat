5242|99|Public
25|$|Stochastic <b>gradient</b> <b>descent</b> runs many greedy {{searches}} {{from random}} initial locations.|$|E
25|$|Most employ {{some form}} of <b>gradient</b> <b>descent,</b> using {{backpropagation}} to compute the actual gradients. This is done by simply taking the derivative of the cost function {{with respect to the}} network parameters and then changing those parameters in a gradient-related direction.|$|E
25|$|Neural Turing {{machines}} couple LSTM {{networks to}} external memory resources, {{with which they}} can interact by attentional processes. The combined system is analogous to a Turing machine but is differentiable end-to-end, allowing it to be efficiently trained by <b>gradient</b> <b>descent.</b> Preliminary results demonstrate that neural Turing machines can infer simple algorithms, such as copying, sorting and associative recall from input and output examples.|$|E
40|$|We {{provide the}} first {{experimental}} results on non-synthetic datasets for the quasi-diagonal Riemannian <b>gradient</b> <b>descents</b> for neural networks introduced in [Ollivier, 2015]. These include the MNIST, SVHN, and FACE datasets {{as well as}} a previously unpublished electroencephalogram dataset. The quasi-diagonal Riemannian algorithms consistently beat simple stochastic <b>gradient</b> <b>gradient</b> <b>descents</b> by a varying margin. The computational overhead with respect to simple backpropagation is around a factor $ 2 $. Perhaps more interestingly, these methods also reach their final performance quickly, thus requiring fewer training epochs and a smaller total computation time. We also present an implementation guide to these Riemannian <b>gradient</b> <b>descents</b> for neural networks, showing how the quasi-diagonal versions can be implemented with minimal effort on top of existing routines which compute gradients...|$|R
40|$|Abstract. In this work, we {{extend a}} {{previously}} demonstrated entropy based groupwise registration method {{to include a}} free-form deformation model based on B-splines. We provide an efficient implementation using stochastic <b>gradient</b> <b>descents</b> in a multi-resolution setting. We demonstrate the method in application {{to a set of}} 50 MRI brain scans and compare the results to a pairwise approach using segmentation labels to evaluate the quality of alignment. Our results indicate that increasing the complexity of the deformation model improves registration accuracy significantly, especially at cortical regions. ...|$|R
40|$|In {{this paper}} we view {{learning}} as an unconstrained non-linear minimization problem {{in which the}} objective function {{is defined by the}} negative log-likelihood function and the search space by the parameter space of an origin constrained product unit neural spatial interaction model. We consider Alopex based global search, as opposed to local search based upon backpropagation of <b>gradient</b> <b>descents,</b> each in combination with the bootstrapping pairs approach to solve the maximum likelihood learning problem. Interregional telecommunication traffic flow data from Austria are used as test bed for comparing the performance of the two learning procedures. The study illustrates the superiority of Alopex based global search, measured in terms of Kullback and Leibler's information criterion...|$|R
25|$|Neural Turing {{machines}} (NTM) {{extend the}} capabilities of deep neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine but is differentiable end-to-end, allowing it to be efficiently trained with <b>gradient</b> <b>descent.</b> Preliminary results demonstrate that NTMs can infer simple algorithms such as copying, sorting and associative recall from input and output examples.|$|E
25|$|Affine scaling {{is one of}} {{the oldest}} {{interior}} point methods to be developed. It was developed in the Soviet Union in the mid-1960s, but didn't receive much attention until the discovery of Karmarkar's algorithm, after which affine scaling was reinvented multiple times and presented as a simplified version of Karmarkar's. Affine scaling amounts to doing <b>gradient</b> <b>descent</b> steps within the feasible region, while rescaling the problem to make sure the steps move toward the optimum faster.|$|E
25|$|Early on, deep {{learning}} was also applied to sequence learning with recurrent neural networks (RNNs) which are general computers and can run arbitrary programs to process arbitrary sequences of inputs. The depth of an RNN is unlimited and {{depends on the}} length of its input sequence. RNNs can be trained by <b>gradient</b> <b>descent</b> but suffer from the vanishing gradient problem. In 1992, it was shown that unsupervised pre-training of a stack of recurrent neural networks can speed up subsequent supervised learning of deep sequential problems.|$|E
40|$|We {{consider}} {{a family of}} parallel methods for constrained optimization based on projected <b>gradient</b> <b>descents</b> along individual coordinate directions. In the case of polyhedral feasible sets, local convergence towards a regular solution occurs unconstrained in a reduced space, allowing for the computation of tight asymptotic convergence rates by sensitivity analysis, this even when global convergence rates are unavailable or too conservative. We derive linear asymptotic rates of convergence in polyhedra for variants of the coordinate descent approach, including cyclic, synchronous, and random modes of implementation. Our results find application in stochastic optimization, and with recently proposed optimization algorithms based on Taylor approximations of the Newton step. Comment: 20 pages. A {{version of this paper}} will be submitted for publicatio...|$|R
40|$|We {{present an}} {{algorithm}} for decomposing a symmetric tensor of dimension n and order d as a sum of of rank- 1 symmetric tensors, extending the algorithm of Sylvester devised in 1886 for symmetric tensors of dimension 2. We exploit the known fact that every symmetric tensor is equivalently {{represented by a}} homogeneous polynomial in n variables of total degree d. Thus the decomposition corresponds to a sum of powers of linear forms. The impact of this contribution is two-fold. First it permits an efficient computation of the decomposition of any tensor of sub-generic rank, as opposed to widely used iterative algorithms with unproved convergence (e. g. Alternate Least Squares or <b>gradient</b> <b>descents).</b> Second, it gives tools for understanding uniqueness conditions, and for detecting the tensor rank...|$|R
40|$|International audienceIn this paper, a {{generative}} model based {{method for}} recovering both {{the shape and}} the reflectance of the surface(s) of a scene from multiple images is presented, assuming that illumination conditions are known in advance. Based on a variational framework and via <b>gradient</b> <b>descents,</b> the algorithm minimizes simultaneously and consistently a global cost functional with respect to both shape and reflectance. Contrary to previous works which consider specific individual scenarios, our method applies {{to a number of}} scenarios [...] mutiview stereovision, multiview photometric stereo, and multiview shape from shading. In addition, our approach naturally combines stereo, silhouette and shading cues in a single framework and, unlike most previous methods dealing with only Lambertian surfaces, the proposed method considers general dichromatic surfaces...|$|R
25|$|The {{critical}} {{points of}} Lagrangians occur at saddle points, {{rather than at}} local maxima (or minima). Unfortunately, many numerical optimization techniques, such as hill climbing, <b>gradient</b> <b>descent,</b> some of the quasi-Newton methods, among others, are designed to find local maxima (or minima) and not saddle points. For this reason, one must either modify the formulation to ensure that it's a minimization problem (for example, by extremizing {{the square of the}} gradient of the Lagrangian as below), or else use an optimization technique that finds stationary points (such as Newton's method without an extremum seeking line search) and not necessarily extrema.|$|E
25|$|First, a maximum-likelihood principle, {{based on}} the idea to {{increase}} the probability of successful candidate solutions and search steps. The mean of the distribution is updated such that the likelihood of previously successful candidate solutions is maximized. The covariance matrix of the distribution is updated (incrementally) such that the likelihood of previously successful search steps is increased. Both updates can be interpreted as a natural <b>gradient</b> <b>descent.</b> Also, in consequence, the CMA conducts an iterated principal components analysis of successful search steps while retaining all principal axes. Estimation of distribution algorithms and the Cross-Entropy Method are based on very similar ideas, but estimate (non-incrementally) the covariance matrix by maximizing the likelihood of successful solution points instead of successful search steps.|$|E
2500|$|<b>Gradient</b> <b>descent</b> {{similarly}} uses finite steps, {{here to find}} minima {{of functions}} ...|$|E
40|$|International audienceWe {{present an}} {{algorithm}} for decomposing a symmetric tensor of dimension n and order d as a sum of of rank- 1 symmetric tensors, extending the algorithm of Sylvester devised in 1886 for symmetric tensors of dimension 2. We exploit the known fact that every symmetric tensor is equivalently {{represented by a}} homogeneous polynomial in n variables of total degree d. Thus the decomposition corresponds to a sum of powers of linear forms. The impact of this contribution is two-fold. First it permits an eﬃcient computation of the decomposition of any tensor of sub-generic rank, as opposed to widely used iterative algorithms with unproved convergence (e. g. Alternate Least Squares or <b>gradient</b> <b>descents).</b> Second, it gives tools for understanding uniqueness conditions, and for detecting the tensor rank...|$|R
40|$|International audienceWe {{propose a}} {{generative}} model based method for recovering both {{the shape and}} the reﬂectance of the surface(s) of a scene from multiple images, assuming that illumination conditions and cameras calibration are known in advance. Based on a variational framework and via <b>gradient</b> <b>descents,</b> the algorithm minimizes simultaneously and consistently a global cost functional with respect to both shape and reﬂectance. Contrary to previous work which considers and specializes in a speciﬁc scenario, our method applies indiscriminately {{with a number of}} classical scenarios; in particular it works for classical stereovision, multiview photometric stereo and multiview shape from shading. Moreover, unlike most previous methods dealing with only Lambertian surfaces, the proposed method considers general dichromatic surfaces. We verify the method using synthetic and real data sets containing specular reﬂection...|$|R
40|$|We {{propose a}} {{generative}} model based method for recovering both {{the shape and}} the reflectance of the surface(s) of a scene from multiple images, assuming that illumination conditions and cameras calibration are known in advance. Based on a variational framework and via <b>gradient</b> <b>descents,</b> the algorithm minimizes simultaneously and consistently a global cost functional with respect to both shape and reflectance. The motivations for our approach are threefold. (1) Contrary to previous works which mainly consider specific individual scenarios, our method applies indiscriminately {{to a number of}} classical scenarios; in particular it works for classical stereovision, multiview photometric stereo and multiview shape from shading. It works with changing as well as static illumination. (2) Our approach naturally combines stereo, silhouette and shading cues in a single framework. (3) Moreover, unlike most previous methods dealing with only Lambertian surfaces, the propose...|$|R
2500|$|The weight updates of {{backpropagation}} {{can be done}} via stochastic <b>gradient</b> <b>descent</b> {{using the}} following equation: ...|$|E
2500|$|... 2004, Zlochin and Dorigo {{show that}} some {{algorithms}} are {{equivalent to the}} stochastic <b>gradient</b> <b>descent,</b> the cross-entropy method and algorithms to estimate distribution ...|$|E
2500|$|<b>Gradient</b> <b>descent</b> (alternatively, [...] "steepest descent" [...] or [...] "steepest ascent"): A (slow) {{method of}} {{historical}} and theoretical interest, {{which has had}} renewed interest for finding approximate solutions of enormous problems.|$|E
40|$|Modern {{buildings}} encompass complex {{dynamics of}} multiple electrical, mechanical, and control systems. One {{of the biggest}} hurdles in applying conventional model-based optimization and control methods to building energy management is the huge cost and effort of capturing diverse and temporally correlated dynamics. Here we propose an alternative approach which is model-free and data-driven. By utilizing high volume of data coming from advanced sensors, we train a deep Recurrent Neural Networks (RNN) which could accurately represent the operation's temporal dynamics of building complexes. The trained network is then directly fitted into a constrained optimization problem with finite horizons. By reformulating the constrained optimization as an unconstrained optimization problem, we use iterative <b>gradient</b> <b>descents</b> method with momentum to find optimal control inputs. Simulation results demonstrate proposed method's improved performances over model-based approach on both building system modeling and control...|$|R
40|$|A {{class of}} morphological/rank/linear (MRL) -filters is {{presented}} as a general nonlinear tool for image processing. They consist of a linear combination between a morphological/rank filter and a linear filter. A <b>gradient</b> steepest <b>descent</b> method is proposed to optimally design these filters, using the averaged least mean squares (LMS) algorithm. The filter design is viewed as a learning process, and convergenc...|$|R
40|$|Abstract. In this paper, a {{generative}} model based {{method for}} recovering both {{the shape and}} the reflectance of the surface(s) of a scene from multiple images is presented, assuming that illumination conditions are known in advance. Based on a variational framework and via <b>gradient</b> <b>descents,</b> the algorithm minimizes simultaneously and consistently a global cost functional with respect to both shape and reflectance. Contrary to previous works which consider specific individual scenarios, our method applies {{to a number of}} scenarios – mutiview stereovision, multiview photometric stereo, and multiview shape from shading. In addition, our approach naturally combines stereo, silhouette and shading cues in a single framework and, unlike most previous methods dealing with only Lambertian surfaces, the proposed method considers general dichromatic surfaces. 1 Introduction and Related Work Many methods have been proposed to recover the three-dimensional surface shape using multiple images during these last two decades [1]. On the other hand, for a long time, the estimation of surface radiance/reflectance was secondary. Even some recen...|$|R
2500|$|When <b>gradient</b> <b>descent</b> {{is applied}} to action , motor control can be {{understood}} in terms of classical reflex arcs that are engaged by descending (corticospinal) predictions. This provides a formalism that generalizes the equilibrium point solution – to the degrees of freedom problem [...] – to movement trajectories.|$|E
2500|$|One {{can also}} apply a {{widespread}} stochastic <b>gradient</b> <b>descent</b> method with iterative projection {{to solve this}} problem. The idea of this method is to update the dictionary using the first order stochastic gradient and project it on the constraint set [...] The step that occurs at i-th iteration is described by this expression: ...|$|E
2500|$|A {{commonly}} used cost function is the mean-squared error, which tries {{to minimize the}} average squared error between the network's output, , and the target value [...] over all the example pairs. Minimizing this cost function using <b>gradient</b> <b>descent</b> for the class of neural networks called multilayer perceptrons (MLP), produces the backpropagation algorithm for training neural networks.|$|E
5000|$|In 1981, Gene Golub {{offered a}} US$500 prize for [...] "the {{construction}} of a 3-term conjugate <b>gradient</b> like <b>descent</b> method for non-symmetric real matrices or a proof {{that there can be}} no such method". Faber and his co-author Thomas A. Manteuffel won this prize for their 1984 paper, in which they gave conditions for the existence of such a method and showed that, in general, there can be no such method.|$|R
30|$|The paper {{proposes a}} maximum {{likelihood}} sequence estimator (MLSE) receiver for satellite communications. The satellite channel model {{is composed of}} a nonlinear traveling wave tube (TWT) amplifier followed by a multipath propagation channel. The receiver is composed of a neural network channel estimator (NNCE) and a Viterbi detector. The natural <b>gradient</b> (NG) <b>descent</b> is used for training. Computer simulations show that the performance of our receiver {{is close to the}} ideal MLSE receiver in which the channel is perfectly known.|$|R
40|$|In {{this paper}} a new blind {{deconvolution}} algorithm as {{modification of the}} Bellini's 'Bussgang' is presented. Firstly, a novel version based on stochastic <b>Gradient</b> Steepest <b>Descent</b> error minimization technique is proposed. Then the Bayesian estimator used by Bellini is approximated with a flexible 'sigmoid' parameterized with adjustable amplitude and slope, and a gradient-based technique is proposed to adapt such parameters {{in order to avoid}} their unsuitable choices. Experimental results are shown to assess the usefulness of the new equalization method...|$|R
2500|$|Free energy minimisation formalises {{the notion}} of {{unconscious}} inference in perception [...] and provides a normative (Bayesian) theory of neuronal processing. The associated process theory of neuronal dynamics is based on minimising free energy through <b>gradient</b> <b>descent.</b> This corresponds to generalised Bayesian filtering (where ~ denotes a variable in generalised coordinates of motion and [...] is a derivative matrix operator): ...|$|E
2500|$|Note that [...] is a convex {{function}} of [...] and [...] As such, traditional <b>gradient</b> <b>descent</b> (or SGD) methods can be adapted, where {{instead of taking}} {{a step in the}} direction of the functions gradient, a step is taken in the direction of a vector selected from the function's sub-gradient. This approach has the advantage that, for certain implementations, the number of iterations does not scale with , the number of data points.|$|E
2500|$|Simulated {{annealing}} (SA) is a probabilistic {{technique for}} approximating the global optimum {{of a given}} function. Specifically, it is a metaheuristic to approximate global optimization in a large search space. [...] It is often used when the search space is discrete (e.g., all tours that visit a given set of cities). [...] For problems where finding an approximate global optimum {{is more important than}} finding a precise local optimum in a fixed amount of time, simulated annealing may be preferable to alternatives such as [...] <b>gradient</b> <b>descent.</b>|$|E
40|$|The paper {{proposes a}} maximum {{likelihood}} sequence estimator (MLSE) receiver for satellite communications. The satellite channel model {{is composed of}} a nonlinear traveling wave tube (TWT) amplifier followed by a multipath propagation channel. The receiver is composed of a neural network channel estimator (NNCE) and a Viterbi detector. The natural <b>gradient</b> (NG) <b>descent</b> is used for training. Computer simulations show that the performance of our receiver {{is close to the}} ideal MLSE receiver in which the channel is perfectly known. </p...|$|R
40|$|Abstract. The work {{describes}} all {{the necessary}} steps to solve the traveling salesperson problem. This optimization problem {{is very easy to}} formulate-and a lot of works do it-, but it is rather difficult to solve it. By using [1] as a main reference, we formulate an algorithm in a matrix form to solve the problem. The mathematical approach is based on Hopfield neural networks and uses the energy function with the <b>descent</b> <b>gradient</b> method. The algorithm in matrix form is easier to use or to write a computation program. The work has six sections. The section 5 describes the algorithm to solve the traveling salesperson problem and the section 6 contain an numerical example. Key-Words. Traveling salesperson problem, traveling salesperson algorithm, energy function, <b>descent</b> <b>gradient.</b> ...|$|R
40|$|International audienceThis paper tackles an {{important}} aspect of the variational problem underlying active contours: optimization by gradient flows. Classically, the definition of a gradient depends directly on the choice of an inner product structure. This consideration is largely absent from the active contours literature. Most authors, explicitely or implicitely, assume that the space of admissible deformations is ruled by the canonical L 2 inner prod-uct. The classical gradient flows reported in the literature are relative to this particular choice. Here, we investigate the relevance of using (i) other inner products, yielding other <b>gradient</b> <b>descents,</b> and (ii) other minimizing flows not deriving from any inner product. In particular, we show how to induce different de-grees of spatial consistency into the minimizing flow, in order to decrease the probability of getting trapped into irrelevant local minima. We report numerical experiments indicating that the sensitivity of the active contours method to initial conditions, which seriously limits its applicability and efficiency, is alleviated by our application-specific spatially coherent minimizing flows. We show that the choice of the inner product {{can be seen as a}} prior on the deformation fields and we present an extension of the definition of the gradient toward more general priors...|$|R
