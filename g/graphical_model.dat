2854|4950|Public
25|$|A Bayesian network, Bayes network, belief network, Bayes(ian) {{model or}} {{probabilistic}} directed acyclic <b>graphical</b> <b>model</b> is a probabilistic <b>graphical</b> <b>model</b> (a type of statistical model) {{that represents a}} set of random variables and their conditional dependencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network {{can be used to}} compute the probabilities of the presence of various diseases.|$|E
25|$|More {{recently}} {{deep learning}} {{was shown to}} be useful in semantic hashing where a deep <b>graphical</b> <b>model</b> of the word-count vectors is obtained from a large document set. Documents are mapped to memory addresses {{in such a way}} that semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by simply accessing other nearby addresses.|$|E
25|$|In 2011 it {{was shown}} by Polson and Scott that the SVM admits a Bayesian {{interpretation}} through the technique of data augmentation. In this approach the SVM {{is viewed as a}} <b>graphical</b> <b>model</b> (where the parameters are connected via probability distributions). This extended view allows for the application of Bayesian techniques to SVMs, such as flexible feature modeling, automatic hyperparameter tuning, and predictive uncertainty quantification. Recently, a scalable version of the Bayesian SVM was developed by Wenzel et al. enabling the application of Bayesian SVMs to big data.|$|E
40|$|In this paper, {{a multivariate}} {{distribution}} family introduced by Koehler and Symanowski (1995) is discussed as alternative assumption for <b>graphical</b> <b>models</b> which are typically connected with Conditional{Gaussian distributions. For that purpose, certain requirements which {{have to be}} fullled when formulating <b>graphical</b> <b>models</b> are checked. This leads {{to the introduction of}} <b>graphical</b> <b>models</b> with Koehler Symanowski distributions which are then investigated regarding some basic properties known for Gaussian <b>graphical</b> <b>models...</b>|$|R
40|$|We {{provide a}} {{classification}} of <b>graphical</b> <b>models</b> {{according to their}} representation as exponential families. Undirected <b>graphical</b> <b>models</b> with no hidden variables are linear exponential families (LEFs), directed acyclic <b>graphical</b> (DAG) <b>models</b> and chain graphs with no hidden variables, including DAG models with several families of local distributions, are curved exponential families (CEFs) and <b>graphical</b> <b>models</b> with hidden variables are stratified exponential families (SEFs). A SEF is a finite union of CEFs of various dimensions satisfying some regularity conditions. The main results of this paper are that <b>graphical</b> <b>models</b> are SEFs and that many <b>graphical</b> <b>models</b> are not CEFs. That is, roughly speaking, <b>graphical</b> <b>models</b> when viewed as exponential families correspond {{to a set of}} smooth manifolds of various dimensions and usually not to a single smooth manifold. These results are discussed in the context of model selection...|$|R
40|$|We derive {{standard}} imsets for undirected <b>graphical</b> <b>models</b> {{and chain}} <b>graphical</b> <b>models.</b> Standard imsets for undirected <b>graphical</b> <b>models</b> {{are described in}} terms of minimal triangulations for maximal prime subgraphs of the undirected graphs. For describing standard imsets for chain <b>graphical</b> <b>models,</b> we first define a triangulation of a chain graph. We then use the triangulation to generalize our results for the undirected graphs to chain graphs. Comment: Published at [URL] in the Bernoulli ([URL] by the International Statistical Institute/Bernoulli Society ([URL]...|$|R
25|$|Approaches that {{represent}} previous experiences directly {{and use a}} similar experience to form a local model are often called nearest neighbour or k-nearest neighbors methods. Deep learning is useful in semantic hashing where a deep <b>graphical</b> <b>model</b> the word-count vectors obtained from a large set of documents. Documents are mapped to memory addresses {{in such a way}} that semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by accessing all the addresses that differ by only a few bits from the address of the query document. Unlike sparse distributed memory that operates on 1000-bit addresses, semantic hashing works on 32 or 64-bit addresses found in a conventional computer architecture.|$|E
25|$|Quantum {{annealing}} is not {{the only}} technology for sampling. In a prepare and measure scenario, a universal quantum computer prepares a thermal state, which is then sampled by measurements. This can reduce the time required to train a deep restricted Boltzmann machine, and provide a richer and more comprehensive framework for deep learning than classical computing. The same quantum methods also permit efficient training of full Boltzmann machines and multi-layer, fully connected models and do not have well-known classical counterparts. Relying on an efficient thermal state preparation protocol starting from an arbitrary state, quantum-enhanced Markov logic networks exploit the symmetries and the locality structure of the probabilistic <b>graphical</b> <b>model</b> generated by a first-order logic template. This provides an exponential reduction in computational complexity in probabilistic inference, and, while the protocol relies on a universal quantum computer, under mild assumptions it can be embedded on contemporary quantum annealing hardware.|$|E
2500|$|In The Blind Watchmaker, Dawkins goes on {{to provide}} a <b>graphical</b> <b>model</b> of gene {{selection}} involving [...] entities he calls biomorphs. These are two-dimensional sets of line segments which bear relationships to each other, drawn {{under the control of}} [...] "genes" [...] that determine the appearance of the biomorph. By selecting entities from sequential generations of biomorphs, an experimenter can guide the evolution of the figures toward given shapes, such as [...] "airplane" [...] or [...] "octopus" [...] biomorphs.|$|E
40|$|We explore {{elliptical}} <b>graphical</b> <b>models</b> as a {{generalization of}} Gaussian <b>graphical</b> <b>models,</b> that is, we allow the population distribution to be elliptical instead of normal. Towards a statis-tical theory for such <b>graphical</b> <b>models,</b> consisting of estimation, testing and model selection, {{we consider the}} problem of estimating partial correlations. We derive the asymptotic distri-bution of a class of partial correlation matrix estimators based on affine equivariant scatter estimators...|$|R
40|$|<b>Graphical</b> <b>models</b> {{in their}} modern form {{have been around}} since the late 1970 s and appear today {{in many areas of the}} sciences. Along with the ongoing {{developments}} of <b>graphical</b> <b>models,</b> a number of different <b>graphical</b> <b>modeling</b> software programs have been written over the years. In recent years many of these software developments have taken place within the R community, either in the form of new packages or by providing an R interface to existing software. This book attempts to give the reader a gentle introduction to <b>graphical</b> <b>modeling</b> using R and the main features of some of these packages. In ad...|$|R
40|$|<b>Graphical</b> <b>models</b> offer a {{powerful}} tool for studying ecosystem function. Changes in relationships among extrinsic and intrinsic biological and environmental variables can be explored. We discuss the application of <b>graphical</b> <b>modeling</b> to ecological data and illustrate this with an example case study. Ecological datasets are characteristically small with few data points, covering only a short period of time, and with high seasonal variation. This high variation, along {{with the fact that the}} data sets are small, can present problems for <b>graphical</b> <b>modeling.</b> Despite this, in general, considerable insight into ecosystem function can be gained from the use of <b>graphical</b> <b>modeling...</b>|$|R
2500|$|Given the {{hypotheses}} about the currency and the constraints, the optimal decision rule is the model's prediction {{of what the}} animal's best foraging strategy should be. Possible examples of optimal decision rules could be the optimal number of food items that an animal should carry back to its nesting site or the optimal size of a food item that an animal should feed on. [...] Figure 1, shows {{an example of how}} an optimal decision rule could be determined from a <b>graphical</b> <b>model.</b> The curve represents the energy gain per cost (E) for adopting foraging strategy x. [...] Energy gain per cost is the currency being optimized. [...] The constraints of the system determine the shape of this curve. The optimal decision rule (x*) is the strategy for which the currency, energy gain per costs, is the greatest. [...] Optimal foraging models can look very different and become very complex, depending {{on the nature of the}} currency and the number of constraints considered. [...] However, the general principles of currency, constraints, and optimal decision rule remain the same for all models.|$|E
50|$|A <b>graphical</b> <b>model</b> or {{probabilistic}} <b>graphical</b> <b>model</b> (PGM) is a {{probabilistic model}} {{for which a}} graph expresses the conditional dependence structure between random variables. They are commonly used in probability theory, statistics—particularly Bayesian statistics—and machine learning.|$|E
50|$|This type of <b>graphical</b> <b>model</b> {{is known}} as a {{directed}} <b>graphical</b> <b>model,</b> Bayesian network, or belief network. Classic machine learning models like hidden Markov models, neural networks and newer models such as variable-order Markov models can be considered special cases of Bayesian networks.|$|E
40|$|Estimating dynamic {{networks}} {{from data}} {{is an active}} research area {{and it is one}} important direction in system biology. Estimating the structure of a network is about deciding {{the presence or absence of}} relationships between random variables. <b>Graphical</b> <b>models</b> describe conditional independence relationships. Gaussian <b>graphical</b> <b>models</b> are <b>graphical</b> <b>models</b> where it is assumed that the random variables follow a multivariate normal distribution. When Gaussian <b>graphical</b> <b>models</b> are applied in order to study large networks, they typically fail because the number of variables is much greater than the number of observations. Recently, penalized Gaussian <b>graphical</b> <b>models</b> have been proposed to estimate static networks in high-dimensional studies because of their statistical properties and computational tractability. We propose to use penalized Gaussian <b>graphical</b> <b>models</b> to estimate structured dynamic networks, for modeling slowly changing of dynamic networks, and to estimate particular structures such as scale-free dynamic networks in a small world setting. These models can be applied when estimating dynamic networks in high-dimensional environments. When multivariate dynamic data are binary or ordinal random variables, transformations based on probability distribution with fixed marginal can be used to do inference. We propose the Gaussian copula for non-Gaussian <b>graphical</b> <b>models</b> to overcome the assumption of Gaussianity. The problem of estimating dynamic networks becomes even more challenging when latent or hidden variables are involved in larger systems. State-space models have been proposed in order to study dynamic networks with latent variables. We propose a penalized Gaussian <b>graphical</b> <b>models</b> to estimate dynamic networks with latent structures...|$|R
40|$|This paper {{introduces}} <b>graphical</b> <b>models</b> as {{a natural}} environment in which to formulate and solve problems in genetics and related areas. Particular emphasis {{is given to the}} relationships among various local computation algorithms which have been developed within the hitherto mostly separate areas of <b>graphical</b> <b>models</b> and genetics. The potential of <b>graphical</b> <b>models</b> is explored and illustrated through a number of example applications where the genetic element is substantial or dominating...|$|R
40|$|We {{consider}} {{the problem of}} learning high-dimensional Gaussian <b>graphical</b> <b>models.</b> The <b>graphical</b> lasso {{is one of the}} most popular methods for estimating Gaussian <b>graphical</b> <b>models.</b> However, it does not achieve the oracle rate of convergence. In this paper, we propose the graphical nonconvex optimization for optimal estimation in Gaussian <b>graphical</b> <b>models,</b> which is then approximated by a sequence of convex programs. Our proposal is computationally tractable and produces an estimator that achieves the oracle rate of convergence. The statistical error introduced by the sequential approximation using the convex programs are clearly demonstrated via a contraction property. The rate of convergence can be further improved using the notion of sparsity pattern. The proposed methodology is then extended to semiparametric <b>graphical</b> <b>models.</b> We show through numerical studies that the proposed estimator outperforms other popular methods for estimating Gaussian <b>graphical</b> <b>models.</b> Comment: 3 figure...|$|R
50|$|A Bayesian network, belief {{network or}} {{directed}} acyclic <b>graphical</b> <b>model</b> is a probabilistic <b>graphical</b> <b>model</b> {{that represents a}} set of random variables and their conditional independencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network {{can be used to}} compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning.|$|E
50|$|Sudoku codes can be {{represented}} by probabilistic <b>graphical</b> <b>model</b> in which they {{take the form of}} a low-density parity-check code.|$|E
5000|$|... #Caption: Schematic {{overview}} {{of a deep}} belief net. Arrows represent directed connections in the <b>graphical</b> <b>model</b> that the net represents.|$|E
40|$|The mimR package (version 1. 1. 1) for <b>graphical</b> <b>modelling</b> in R is introduced. We {{present some}} {{facilities}} of mimR, namely those relating specifying models, editing models, fitting models and doing model search. We also discuss the entities needed for flexible <b>graphical</b> <b>modelling</b> {{in terms of}} an object structure. An example about a latent variable model is presented. 1 Introduction and background The mimR package provides facilities for <b>graphical</b> <b>modelling</b> in the statistical pro-gram...|$|R
40|$|Abstract Background Functioning and {{disability}} are universal human experiences. However, our current understanding of functioning from a comprehensive perspective is limited. The {{development of the}} International Classification of Functioning, Disability and Health (ICF) {{on the one hand}} and recent developments in <b>graphical</b> <b>modeling</b> on the other hand might be combined and open the door to a more comprehensive understanding of human functioning. The objective of our paper therefore is to explore how <b>graphical</b> <b>models</b> can be used in the study of ICF data for a range of applications. Methods We show the applicability of <b>graphical</b> <b>models</b> on ICF data for different tasks: Visualization of the dependence structure of the data set, dimension reduction and comparison of subpopulations. Moreover, we further developed and applied recent findings in causal inference using <b>graphical</b> <b>models</b> to estimate bounds on intervention effects in an observational study with many variables and without knowing the underlying causal structure. Results In each field, <b>graphical</b> <b>models</b> could be applied giving results of high face-validity. In particular, <b>graphical</b> <b>models</b> could be used for visualization of functioning in patients with spinal cord injury. The resulting graph consisted of several connected components which can be used for dimension reduction. Moreover, we found that the differences in the dependence structures between subpopulations were relevant and could be systematically analyzed using <b>graphical</b> <b>models.</b> Finally, when estimating bounds on causal effects of ICF categories on general health perceptions among patients with chronic health conditions, we found that the five ICF categories that showed the strongest effect were plausible. Conclusions <b>Graphical</b> <b>Models</b> are a flexible tool and lend themselves for a wide range of applications. In particular, studies involving ICF data seem to be suited for analysis using <b>graphical</b> <b>models.</b> </p...|$|R
40|$|<b>Graphical</b> <b>models</b> {{have been}} widely used in many applications, ranging from human {{behavior}} recognition to wireless signal detection. However, efficient inference and learning techniques for <b>graphical</b> <b>models</b> are needed to handle complex models, such as hybrid Bayesian networks. This thesis proposes extensions of expectation propagation, a powerful generalization of loopy belief propagation, to develop efficient inference and learning algorithms for <b>graphical</b> <b>models.</b> The first two chapters of the thesis present inference algorithms for generative <b>graphical</b> <b>models,</b> and the next two propose learning algorithms for conditional <b>graphical</b> <b>models.</b> First, the thesis proposes a window-based EP smoothing algorithm, as an alternative to batch EP, for hybrid dynamic Bayesian networks. For an application to digital wireless communications, window-based EP smoothing achieves estimation accuracy comparable to sequential Monte Carlo methods, but with more than 10 times less computational cost. Second, it combines tree-structured EP approximation with the junction tree algorith...|$|R
50|$|A Bayesian network, Bayes network, belief network, Bayes(ian) {{model or}} {{probabilistic}} directed acyclic <b>graphical</b> <b>model</b> is a probabilistic <b>graphical</b> <b>model</b> (a type of statistical model) {{that represents a}} set of random variables and their conditional dependencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network {{can be used to}} compute the probabilities of the presence of various diseases.|$|E
50|$|Restricted Boltzmann {{machines}} are {{a special case}} of Boltzmann machines and Markov random fields.Their <b>graphical</b> <b>model</b> corresponds to that of factor analysis.|$|E
50|$|Graphical {{models can}} still be used when the {{variables}} of choice are continuous. In these cases, the probability distribution is represented as a multivariate probability distribution over continuous variables. Each family of distribution will then impose certain properties on the <b>graphical</b> <b>model.</b> Multivariate Gaussian distribution {{is one of the}} most convenient distributions in this problem. The simple form of the probability, and the direct relation with the corresponding <b>graphical</b> <b>model</b> makes it a popular choice among researchers.|$|E
40|$|Probabilistic <b>graphical</b> <b>models,</b> such as Bayesian networks, allow {{representing}} {{conditional independence}} information of random variables. These relations are graphically {{represented by the}} presence and absence of arcs and edges between vertices. Probabilistic <b>graphical</b> <b>models</b> are nonunique representations of the independence information of a joint probability distribution. However, the concept of Markov equivalence of probabilistic <b>graphical</b> <b>models</b> is able to offer unique representations, called essential graphs. In this survey paper the theory underlying these concepts is reviewed...|$|R
40|$|Abstract. Present a <b>graphical</b> <b>modeling</b> design {{methodology}} of component-based software architecture for distributed real-time system. First of all, definitions of real-time application component model, types of component element {{that are used}} for describing software architecture, and types of collaboration relationship between components are introduced in a graphical design environment. Secondly a <b>graphical</b> <b>modeling</b> design method of software architecture for distributed real-time system is described. Finally, the <b>graphical</b> <b>modeling</b> method based on components defined is illustrated via an example...|$|R
40|$|We {{provide a}} {{classification}} of <b>graphical</b> <b>models</b> {{according to their}} representation as subfamilies of exponential families. Undirected <b>graphical</b> <b>models</b> with no hidden variables are linear exponential families (LEFs), directed acyclic <b>graphical</b> <b>models</b> and chain graphs with no hidden variables, including Bayesian networks with several families of local distributions, are curved exponential families (CEFs) and <b>graphical</b> <b>models</b> with hidden variables are stratified exponential families (SEFs). An SEF is a finite union of CEFs satisfying a frontier condition. In addition, we illustrate how one can automatically generate independence and non-independence constraints on the distributions over the observable variables implied by a Bayesian network with hidden variables. The relevance of these results for model selection is examined. ...|$|R
5000|$|... #Caption: Simplified Dynamic Bayesian Network. All the {{variables}} {{do not need}} to be duplicated in the <b>graphical</b> <b>model,</b> but they are dynamic, too.|$|E
50|$|PyMC is an {{open source}} Python library for Bayesian {{learning}} of general Probabilistic <b>Graphical</b> <b>Model</b> with advanced features {{and easy to use}} interface.|$|E
50|$|NMF {{can be seen}} as a two-layer {{directed}} <b>graphical</b> <b>model</b> {{with one}} layer of observed random variables and one layer of hidden random variables.|$|E
40|$|We {{introduce}} block-tree graphs as {{a framework}} for deriving efficient algorithms on <b>graphical</b> <b>models.</b> We define block-tree graphs as a tree-structured graph where each node is a cluster of nodes such that the clusters in the graph are disjoint. This differs from junction-trees, where two clusters connected by an edge always have at least one common node. When compared to junction-trees, we show that constructing block-tree graphs is faster, and finding optimal block-tree graphs has a much smaller search space. Applying our block-tree graph framework to <b>graphical</b> <b>models,</b> we show that, for some graphs, e. g., grid graphs, using block-tree graphs for inference is computationally more efficient than using junction-trees. For <b>graphical</b> <b>models</b> with boundary conditions, the block-tree graph framework transforms the boundary valued problem into an initial value problem. For Gaussian <b>graphical</b> <b>models,</b> the block-tree graph framework leads to a linear state-space representation. Since exact inference in <b>graphical</b> <b>models</b> can be computationally intractable, we propose to use spanning block-trees to derive approximate inference algorithms. Experimental results show the improved performance in using spanning block-trees versus using spanning trees for approximate estimation over Gaussian <b>graphical</b> <b>models.</b> Comment: 29 pages. Correction to version...|$|R
40|$|Probabilistic <b>graphical</b> <b>models</b> {{are one of}} {{the most}} {{influential}} and widely used techniques in machine learning. Powered by exponential gains in processor technology, <b>graphical</b> <b>models</b> have been successfully applied to a wide range of increasingly large and complex real-world problems. However, recent developments in computer architecture, large-scale computing, and data-storage have shifted the focus away from sequential performance scaling and towards parallelism and large-scale distributed systems. Therefore, in order for <b>graphical</b> <b>models</b> to continue to benefit from developments in computer architecture and remain a viable option in the clouds and beyond, we must discover and exploit the parallelism of learning and inference in probabilistic <b>graphical</b> <b>models.</b> In this thesis we explore how to design efficient parallel algorithms for probabilistic <b>graphical</b> <b>models</b> by framing learning and inference as iterative adaptive asynchronous computation. We first present our work on efficient parallel algorithms for loopy belief propagation and Gibbs sampling. We then describe GraphLab, a new parallel abstraction for designing and implementing iterative adaptive asynchronous computation. Finally, we conclude wit...|$|R
40|$|Abstract. <b>Graphical</b> <b>models</b> {{with higher}} order factors are an impor-tant tool for pattern {{recognition}} that has recently attracted considerable attention. Inference {{based on such}} models is challenging both from the view point of software design and optimization theory. In this article, we use the new C++ template library OpenGM to empirically compare in-ference algorithms {{on a set of}} synthetic and real-world <b>graphical</b> <b>models</b> with higher order factors that are used in computer vision. While infer-ence algorithms have been studied intensively for <b>graphical</b> <b>models</b> with second order factors, an empirical comparison for higher order models has so far been missing. This article presents a first set of experiments that intends to fill this gap. 1 Introduction and Related Work <b>Graphical</b> <b>models</b> have been used very successfully in pattern analysis, usually as probabilistic models in which the graph expresses conditional independence relations on a set of random variables [1, 2]. <b>Graphical</b> <b>models</b> are not restricte...|$|R
