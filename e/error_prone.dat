2709|28|Public
25|$|Compared to DQ2 {{serotyping}} of DQB1*0201 positive individuals (98%), {{the efficiency}} of DQ1 recognition is relatively low and <b>error</b> <b>prone.</b>|$|E
25|$|Much of the {{interior}} (such as the rood screen, the kneeling rail, and a medieval-style bench east of the rood screen) are suppositions based the <b>error</b> <b>prone</b> 1950s restoration. The interior also contains a reproduction wooden baptismal font, various items of furniture, and various ecclesiastical furnishings of period 17th-century origin, though not original to the church itself.|$|E
25|$|Multiplying {{numbers to}} {{more than a couple of}} decimal places by hand is tedious and <b>error</b> <b>prone.</b> Common {{logarithms}} were invented to simplify such calculations. The slide rule allowed numbers to be quickly multiplied to about three places of accuracy. Beginning in the early 20th century, mechanical calculators, such as the Marchant, automated multiplication of up to 10digit numbers. Modern electronic computers and calculators have greatly reduced the need for multiplication by hand.|$|E
40|$|International audienceThis paper {{presents}} a "didactic triangulation" strategy {{to cope with}} the problem of reliability of NLP applications for Computer Assisted Language Learning (CALL) systems. It is based on the implementation of basic but well mastered NLP techniques, and put the emphasis on an adapted gearing between computable linguistic clues and didactic features of the evaluated activities. We claim that a correct balance between noise (i. e. false error detection) - and silence (i. e. undetected errors) is not only an outcome of NLP techniques, but of an appropriate didactic integration of what NLP can do well - and what it cannot do. Based on this approach, ExoGen is a prototype for generating activities such as gapfill exercises. It integrates a module for error detection and description, which checks learners' answers against expected ones. Through the analysis of graphic, orthographic and morphosyntactic differences, it is able to diagnose problems like spelling errors, lexical mix-ups, <b>errors</b> <b>prone</b> agreement, conjugation <b>errors,</b> etc. The first evaluation of ExoGen outputs, based on the FRIDA learner corpus, has yielded very promising results, paving the way for the development of an efficient and general model adapted {{to a wide variety of}} activities...|$|R
40|$|Mobile Adhoc Network (MANET) is {{characterized}} by mobile hosts, dynamic topology, multi-hop wireless connectivity and infrastructureless ad hoc environment. The adhoc environment is accessible to both legitimate network users and malicious attackers. Moreover, as the wireless links are highly <b>errors</b> <b>prone</b> and can go down frequently due to mobility of nodes, therefore, energy efficient, secure and stable routing over MANET {{is still a very}} critical task due to highly dynamic environment. Moreover, the nodes in MANETs are typically powered by batteries which have limited energy reservoir and some times it also becomes very difficult to recharge or replace the battery of the nodes. Hence, power consumption becomes an important issue. The power consumption rate of each node must be evenly distributed to maximize the lifetime of ad hoc mobile networks, and the overall transmission power for each connection request must be minimized. In this paper, we compare a two types of Mobile adhocnetwork i. e., proactive and reactive for Energy Efficiency in Manet. Energy Efficient is basically termed as which consume a less energy. So, this paper gives comparison about various routing protocols related to Energy Efficiency...|$|R
40|$|IP Header {{compression}} mechanisms {{have always}} been an important topic for the research community to save bandwidth in the Internet. Due to the high license fees of 3 G bands and the upcoming integration of IP based multimedia services, it is of particular importance to reduce the IP header overhead even in the wireless format. Reducing the IP overhead gives the network providers the possibility for a faster return of investment on their 3 G networks and simultaneously enables real [...] time services by improving the latency of the IP packets over bandwidth limited links. Many compression methods exist already but they are either not designed for multimedia services, or not robust {{in the presence of}} <b>error</b> [...] <b>prone</b> links and therefore not suitable for wireless communication. For wireless environments robust header compression was introduced. Robust header compression was standardized by the Internet Engineering Task Force in RFC 3095 and will {{be an integral part of}} the 3 GPP [...] UMTS specification. This compression scheme was designed to operate in <b>error</b> [...] <b>prone</b> environments by providing error detection and correction mechanisms in combination with robustness for IP based data streams. A connection oriented approach removing packet inter [...] and intra [...] dependencies reduces the IP header significantly. This paper gives a solid performance evaluation for robust header compression showing both the bandwidth savings for the IP protocol stack and the quality of services at the application layer by the means video services...|$|R
25|$|High-throughput {{quantification}} of miRNAs is <b>error</b> <b>prone,</b> for {{the larger}} variance (compared to mRNAs) that comes with methodological problems. mRNA-expression is therefore often analyzed to check for miRNA-effects in their levels (e.g. in). Databases {{can be used to}} pair mRNA- and miRNA-data that predict miRNA-targets based on their base sequence. While this is usually done after miRNAs of interest have been detected (e. g. because of high expression levels), ideas for analysis tools that integrate mRNA- and miRNA-expression information have been proposed.|$|E
25|$|Properly spaced {{single strand}} {{breaks in the}} host DNA can trigger {{homology}} directed repair, which is less <b>error</b> <b>prone</b> than the non-homologous end joining that typically follows a double strand break. Providing a DNA repair template allows for the insertion of a specific DNA sequence at an exact location within the genome. The repair template should extend 40 to 90 base pairs beyond the Cas9 induced DNA break. The goal is for the cell's HDR process to utilize the provided repair template and thereby incorporate the new sequence into the genome. Once incorporated, this new sequence {{is now part of}} the cell's genetic material and passes into its daughter cells.|$|E
25|$|The {{starting}} gene can be mutagenised {{by random}} point mutations (by chemical mutagens or <b>error</b> <b>prone</b> PCR) and insertions and deletions (by transposons). Gene recombination can be mimicked by DNA shuffling of several sequences (usually {{of more than}} 70% homology) to jump into regions of sequence space between the shuffled parent genes. Finally, specific regions of a gene can be systematically randomised for a more focused approach based on structure and function knowledge. Depending on the method, the library generated will vary {{in the proportion of}} functional variants it contains. Even if an organism is used to express the gene of interest, by mutagenising only that gene, the rest of the organism’s genome remains the same and can be ignored for the evolution experiment (to the extent of providing a constant genetic environment).|$|E
40|$|The {{operational}} {{burden of}} maintaining encryption keys and signed zone files {{is the main}} hindrance to deploying (Domain Name System Security Extensions) DNSSEC. Companies try to tackle this problem by forcing their administrators to follow operational guide books in {{every step of the}} daily DNS activities. However, <b>errors</b> are <b>prone</b> to happen in every process where the human factor is involved. OpenDNSSEC is a turn-key solution for securing DNS zones with DNSSEC. It offers high performance and automatic key management. This project looks at error situations in securing DNS zones with OpenDNSSEC and how those can be avoided. The paper also makes recommendations for increasing the resilience level which OpenDNSSEC can offer against such situations. ...|$|R
40|$|Wireless {{channels}} usually face bursty errors, i. e., <b>errors</b> are <b>prone</b> {{to occur}} in clusters. These bit errors can be modeled using the Gilbert-Elliott model. When data packets are transferred over channels with bursty errors, packet error statistics {{are more important than}} bit error statistics to analyze the communication performance. This has been modeled as a discrete time Markov chain (DTMC) in previous work. However, whether this Markov chain is time-homogeneous has never been addressed. In this paper, we prove that the packet errors can be modeled only as a Markov chain without constant transition probabilities. Thus finding a constant transition matrix and then discussing performance is not accurate. Instead, a gap model is adopted and the packet error/error-free length distributions are thus analyzed...|$|R
40|$|Computational {{models in}} {{chemistry}} {{rely on a}} number of approximations. The effect of such approximations on observables derived from them is often unpredictable. Therefore, it is challenging to quantify the uncertainty of a computational result, which, however, is necessary to assess the suitability of a computational model. Common performance statistics such as the mean absolute <b>error</b> are <b>prone</b> to failure as they do not distinguish the explainable (systematic) part of the errors from their unexplainable (random) part. In this paper, we discuss problems and solutions for performance assessment of computational models based on several examples from the quantum chemistry literature. For this purpose, we elucidate the different sources of uncertainty, the elimination of systematic errors, and the combination of individual uncertainty components to the uncertainty of a prediction. Comment: 21 pages, 3 figures, 1 tabl...|$|R
25|$|Each duplex {{sequencing}} read {{contains a}} fixed 5-nucleotide sequence (shown in figures in Black color) located upstream of the 12-nucleotide tag sequence. The reads are filtered {{out if they}} do not have the expected 5-nucleotide sequence or have more than nine identical or ambiguous bases within each tag. The two 12-nucleotide tags at each end of reads are combined and moved to the read header. Two families of reads are formed that originate from the two strands of DNA. One family contains reads with αβ header originating from strand 1 and the second family contains reads with βα header originating from strand 2 (Figure 2). Then the reads are trimmed by removing the fixed 5 bp sequence and 4 <b>error</b> <b>prone</b> nucleotides located at the sites of ligation and end repair. The remaining reads are assembled to consensus sequences using single strand consensus sequences (SSCSs) assembly and duplex consensus sequences (DCSs) assembly.|$|E
25|$|The Transfer System on {{the other}} hand, was never popular with the racers. While very {{efficient}} and less <b>error</b> <b>prone</b> than the cumulative method (and allowing the ABA to run much quicker events with fewer finish line scores), the good racers generally did not like it since they would only get to race once, win their transfer moto and wait for hours if the races are big enough to race the mains ({{on the other}} hand you get to relax a little without the anxiety of doing poorly {{in the next two}} qualifying motos). This means the racers race less often in the ABA and the fun of BMX is in the racing, even if you are losing. Even racers who do not do well did not like the transfer system because the fewer actual races you are in, the less chance you will have to improve your skills in actual race conditions You also race two times less for your money paid for in entrance fees in the ABA in the Transfer system if you win your first moto.|$|E
2500|$|Captain Arief’s radio {{communications}} with ATC were <b>error</b> <b>prone</b> ...|$|E
40|$|We {{investigate}} {{a translation of}} SDL diagrams into the complete visual representation of Pictorial Janus (PJ) programs in order to analyze the specification by visual debugging and animation. We additionally introduce timing concepts to PJ (Timed PJ) for a mapping of the SDL timing statements. The concepts transforming SDL interaction and process diagrams into Timed PJ are outlined by an example sketching the transformation of an Ethernet CSMA/CD protocol specification. I. Introduction The specification and analysis of distributed systems is an <b>error</b> [...] <b>prone</b> activity. Even if the designer exercises utmost care {{there is a strong}} probability for introducing functional and timing errors during the early design phases. The complexity of distributed systems can hardly be managed within the classical programming environment. Designing complex systems is only manageable by advanced design tools. Thus, visualization and animation tools which support the analysis and execution of concurrent prog [...] ...|$|R
40|$|Generational {{techniques}} {{have been very}} successful in reducing the impact of garbage collection algorithms upon the performance of programs. However, {{it is impossible for}} designers of collection algorithms to anticipate the memory allocation behavior of all applications in advance. Existing generational collectors rely upon the applications programmer to tune the behavior of the collector to achieve maximum performance for each application. Unfortunately, because the many tuning parameters require detailed knowledge of both the collection algorithm and the program allocation behavior in order to be used effectively, such tuning is difficult and <b>error</b> [...] <b>prone.</b> We propose a new garbage collection algorithm that uses just two easily [...] understood tuning parameters that directly reflect the maximum memory and pause time constraints familiar to application programmers and users. Like generational collectors, ours divides memory into two spaces, one for short [...] lived, and another for long [...] lived [...] ...|$|R
40|$|We {{consider}} four essential modalities of assuring trade: namely, {{the use of}} self-governance structures, reputation entrepreneurs, na-tional {{courts and}} arbitration. We offer several implications from the comparison of these governance structures. First, self-governance and reputation entrepreneurs may seek to limit their member’s access to formal court enforcement. Second, {{in addition to being}} an enforcer of contracts, courts also operate as reputation propagators as because of their public quality and formal, generally discoverable, records. Third, a key function of modern arbitration is its ability to separate enforce-ment of awards from reputation. Given that type 1 and 2 <b>errors</b> are <b>prone</b> to occur, traders who receive negative judgments in court may lose much more than what the adjudicator orders them to pay. By maintaining confidentiality in arbitral awards, traders are more able to defend their claims without excessive risk to their reputations. Ad-ditional implications of the interactions between governance modalities are also considered...|$|R
2500|$|To {{accurately}} {{compute the}} gyro angle for a torpedo {{in a general}} engagement scenario, the target course, range, and bearing must be accurately known. During World War II, target course, range, and bearing estimates often had to be generated using periscope observations, which were highly subjective and <b>error</b> <b>prone.</b> The TDC was used to refine the estimates of the target's course, range, and bearing {{through a process of}} ...|$|E
2500|$|Surveys {{of cases}} of human {{aneuploidy}} syndromes have shown {{that most of them}} are maternally derived. [...] This raises the question: Why is female meiosis more <b>error</b> <b>prone?</b> The most obvious difference between female oogenesis and male spermatogenesis is the prolonged arrest of oocytes in late stages of prophase I for many years up to several decades. Male gametes on the other hand quickly go through all stages of meiosis I and II.|$|E
2500|$|Cellular DNA is {{regularly}} exposed to DNA damaging agents. [...] A DNA damage response (DDR) {{that is well}} regulated and intricate is usually employed {{to deal with the}} potential deleterious effects of the damage. [...] When DNA damage occurs, SUMO protein has been shown to act as a molecular glue to facilitate the assembly of large protein complexes in repair foci. [...] Also, SUMOylation can alter a protein's biochemical activities and interactions. [...] SUMOylation plays a role in the major DNA repair pathways of base excision repair, nucleotide excision repair, non-homologous end joining and homologous recombinational repair. [...] SUMOylation also facilitates <b>error</b> <b>prone</b> translesion synthesis.|$|E
40|$|The {{increased}} interest {{in the use of}} automated safety analysis is supported by the claim that manual safety analysis based on traditional techniques is <b>error</b> [...] <b>prone,</b> costly and not necessarily complete. It is also claimed that traditional techniques are not able to deal with the inherent complexities of software intensive systems. However, we show in this paper that a transition (from manual to automatic approaches) in the assessment process and technologies is accompanied by an inherent risk of obtaining false confidence, unless safeguards are provided. The safeguard presented in this paper integrates traditional deductive and inductive analysis techniques with model checking, a form of formal verification. The aim is to provide the safety analyst with a rigourous approach for the validation of formal models. The feasibility of the overall approach is illustrated in terms of a case study. Keywords: model checking, formal verification, traditional safety analysis techniques, [...] ...|$|R
40|$|There {{are three}} major points to this article: 1. Measurement error causes biases in {{regression}} fits. The line one would obtain {{if one could}} accurately measure exposure to environmental lead media will differ in important ways when one measures exposure with error. 2. The effects of measurement error vary from study [...] to [...] study. It is dangerous to take measurement error corrections derived from one study and apply them to data from entirely different studies or populations. 3. Measurement error can falsely invalidate a correct (complex mechanistic) model. If one builds a model such as the IEUBK carefully using essentially error [...] free lead exposure data, and applies this model in a different data set with <b>error</b> [...] <b>prone</b> exposures, the complex mechanistic model will almost certainly do {{a poor job of}} prediction, especially of extremes. While mean blood lead levels from such a process may be accurately predicted, in most cases one would expect serious under [...] or over [...] estimates of the proport [...] ...|$|R
40|$|Abstract. In a {{previous}} paper [1] {{we showed that}} Y-linked satellite-DNA sequences of Rumex (Polygonaceae) present reduced rates of evolution {{in relation to other}} autosomal satellite-DNA sequences. In the present paper, we re-analyze the same set of sequences by using the satDNA Analyzer 1. 2 software, specifically developed by us for analysis of satellite DNA evolution. We do not only confirm our previous findings but also prove that the satDNA Analyzer 1. 2 package constitutes a powerful tool for users interested in evolutionary analysis on satellite-DNA sequences. In fact, we are able to gather more accurate calculations regarding location of Strachan positions and evolutionary rates calculations, among others useful statistics. All results are displayed in a very comprehensive multicoloured graphic representation easy to use as an html file. Furthermore, satDNA Analyzer 1. 2 is a time saving feature since every utility is automatized and collected in a single software package, so the user does not need to use different programs. Additionally, it significantly reduces the rate of data miscalculations due to human <b>errors,</b> very <b>prone</b> to occur specially in large files. ...|$|R
5000|$|Captain Arief’s radio {{communications}} with ATC were <b>error</b> <b>prone</b> ...|$|E
5000|$|Poor {{document}} quality - for example, loss of formatting; <b>error</b> <b>prone</b> ...|$|E
5000|$|DNA {{double strand}} break repair in human bladder cancer is <b>error</b> <b>prone</b> and {{involves}} microhomology-associated end-joining ...|$|E
40|$|Circadian {{rhythms and}} {{restricted}} sleep length affect cognitive functions and, consequently, {{the performance of}} day to day activities. To date, {{no more than a}} few studies have explored the consequences of these factors on oculomotor behaviour. We have implemented a spatial cuing paradigm in an eye tracking experiment conducted four times of the day after one week of rested wakefulness and after one week of chronic partial sleep restriction. Our aim was to verify whether these conditions affect the number of a variety of saccadic task errors. Interestingly, we found that failures in response selection, i. e. premature responses and direction <b>errors,</b> were <b>prone</b> to time of day variations, whereas failures in response execution, i. e. omissions and commissions, were considerably affected by sleep deprivation. The former can be linked to the cue facilitation mechanism, while the latter to wake state instability and the diminished ability of top-down inhibition. Together, these results may be interpreted in terms of distinctive sensitivity of orienting and alerting systems to fatigue. Saccadic eye movements proved to be a novel and effective measure with which to study the susceptibility of attentional systems to time factors, thus, this approach is recommended for future research...|$|R
40|$|DNA double 鄄 strand break (DSB) is {{the most}} severe form of DNA damage, which is {{repaired}} mainly through high 鄄 fidelity homologous recombination (HR) or <b>error</b> 鄄 <b>prone</b> non 鄄 homologous end joining (NHEJ). Defects in the DNA damage response lead to genomic instability and ultimately predispose organs to cancer. Nicotinamide phosphoribosyltransferase (Nampt), which is involved in nicotinamide adenine dinucleotide metabolism, is overexpressed {{in a variety of}} tumors. In this report, we found that Nampt physically associated with CtIP and DNA 鄄 PKcs/Ku 80, which are key factors in HR and NHEJ, respectively. Depletion of Nampt by small interfering RNA (siRNA) led to defective NHEJ 鄄 mediated DSB repair and enhanced HR 鄄 mediated repair. Furthermore, the inhibition of Nampt expression promoted proliferation of cancer cells and normal human fibroblasts and decreased 茁 鄄 galactosidase staining, indicating a delay in the onset of cellular senescence in normal human fibroblasts. Taken together, our results suggest that Nampt is a suppressor of HR 鄄 mediated DSB repair and an enhancer of NHEJ 鄄 mediated DSB repair, contributing to the acceleration of cellular senescence. Key words Nampt, DNA 鄄 PKcs/Ku 80, CtIP, DNA repair, cellular senescence Original Articl...|$|R
30|$|Without feedback, {{the volunteers}} had a larger {{absolute}} <b>error</b> in the <b>prone</b> position {{compared to the}} supine position (Eprone[*]=[*] 156  mL vs Esupine[*]=[*] 125  mL, p[*]=[*] 0.012). This illustrates that it is harder for patients {{to get to the}} same level of breath-hold while lying on their stomach. With feedback, the volunteers no longer had increased difficulty with the prone position compared to the supine position. In fact, they seemed to perform better in the prone position (Eprone[*]=[*] 22  mL vs Esupine[*]=[*] 32  mL, p[*]=[*] 0.086). We speculate this is because the back provides a more stable platform to measure a mean distance to because there is less soft tissue such as fat and breast tissue to impair the measurements. In our department, approximately half of the CT-guided lung biopsies are performed in the prone position, so for these procedures visual biofeedback will be of increased importance.|$|R
5000|$|Designing a new {{cryptographic}} primitive is very {{time-consuming and}} very <b>error</b> <b>prone,</b> even for {{experts in the}} field.|$|E
5000|$|Error Resilience (ER), {{added in}} MPEG-4 Audio version 2 in 2000, used for {{transport}} over <b>error</b> <b>prone</b> channels ...|$|E
50|$|Some {{implementations}} of the VVPAT place a high cognitive {{burden on}} the voter and are extraordinarily <b>error</b> <b>prone.</b>|$|E
40|$|Thesis (M. Sc. (Chemistry)) [...] North-West University, Potchefstroom Campus, 2009. Carbonaceous aerosol {{components}} which {{consist of}} organic compounds (OC) and black carbon (BC) {{account for a}} large fraction of atmospheric particulate matter. Most information available on the abundance, properties, and effects of these components so far is based on measurement data of total carbon (TC = OC + BC). This data is increasingly complemented by measurements of water soluble organic carbon (WSOC), its macromolecular fraction (MWSOC), and individual organic compounds due to its environmental significance. WSOC are usually highly polar, oxygenated compounds containing two or more COOH, C=O and/or OH functional groups such as hydroxyamines, amino acids, polyalcohols, sugars, dicarboxylic acids, ketocarboxylic acids and dicarbonyls. These compounds contribute {{to the ability of}} particles to act as cloud condensation nuclei (CCN) and dicarboxylic acids especially can potentially affect the global climate by scattering incoming solar radiation, which counteracts the global warming caused by the increase of greenhouse gases. According to literature the burning of cellulose (biomass burning) generates smoke particles that were nearly 100 % watersoluble. The Vaal Triangle was recently declared as the first priority area in South Africa by the Minister of Environmental Affairs and Tourism on the 21 st of April 2006. The area comprises of heavy industrial activities, one power station, several commercial operations, motor vehicles as well as many households utilizing coal as an energy source. Ambient aerosol sampling for this study was done at 3 sites in the Vaal Triangle (Vereeniging, Vanderbijlpark and Sasolburg) during the winter of 2006 and summer of 2007 with Mini-volume portable air samplers. Aerosol samples were collected on pre-fired quartz filters. Gas and Ion chromatography were applied in analyzing the aerosol filters for specific dicarboxylic acids in the WSOC fraction. However, the GC-MS method required the water extracted samples to be derivatized before injection. This multiple synthesis pathway proved difficult and <b>errors</b> <b>prone</b> with potential dicarboxylic acid loss since the dicarboxylic acids are present in ng/m 3. This meant the GC-MS was only used as a quantitative technique. An alternative ion chromatographic method of analyzing dicarboxylic acids was developed. A new Dionex ICS- 3000 RFIC instrument along with its special licensed software (Virtual Column) was utilized. The Virtual Column software makes it possible to simulate possible separations of predetermined individual compounds within the WSOC fraction. The influence and impact of various parameters can be checked without wasting valuable sample. After a method was developed, it was tested practically by analyzing standard solutions. The optimized method was then used to analyze the field samples collected at the different sites. The ICS- 3000 RFIC with Virtual Column proved to be a convenient and appropriate technique. It showed that the dicarboxylic acid species oxalic, malonic, succinic, glutaric and phthalic as well as inorganic ions fluoride, chloride, nitrate and sulphate were present in the air of all the sites. The chromatographic profile of all the sites also closely resembled each other, be they residential, industrial or petrochemical. However, the methodology was only developed for qualitative analysis and further studies should develop the method further to include quantitative analysis as well. Master...|$|R
40|$|Modelling by {{geometric}} constraints is {{a promising}} method {{in the field}} of CAD/CAM. However, this process, closely related to computer programming, is also <b>prone</b> <b>error.</b> A geometric constraints based modeller should help the end user to find his mistakes, or, better, not to commit ones by watching the building process. The well known main cause of errors with these methods is the specification of redundant constraints, and sometimes conflicting constraints. It's also important to detect under-constrained parts of the system involving indecisiveness. This paper alludes to some usual numerical and probabilistic tools {{that could be used for}} this goal. There's also a decomposition method of constraints systems using tools of the graph theory. Since these two approaches have their own advantages it's worth combining them. All these methods will be studied first for systems of equations and then extended to the more specific case, but also more interesting for modelling, of systems o [...] ...|$|R
40|$|Measurement of {{the extent}} of damage in a real {{structure}} is extremely important in terms of any maintenance strategy. However, this measurement often turns out to be difficult, time consuming and <b>error</b> – <b>prone.</b> The necessity of a simple, fast and relatively inexpensive damage monitoring system with reliable measurements is growing for quite sometime. The paper proposes a camera based image analysis technique to quantify and classify damage in structures at various levels of scale. The general method has been applied to corroded plate specimens in the laboratory with the aim to identify the affected areas on a steel pile due to pitting corrosion. The method depends on the contrast of the corroded region with respect to its surroundings, performs intelligent edge detection through image processing techniques and computes each affected and closed region to predict the total area of the affected part along with its spatial distribution on a two dimensional plane. Moreover the performance of the camera allows defining a detection threshold and the so-called probability of detection (PoD) and probability of false alarms (PFA). PoD are suggested as functions of the area of the pitting for the construction of Receiver-Operating-Characteristic (ROC) curves. The methodology {{can be used as a}} tool for the owners/managers of the structure for objectively quantifying and localising the extent of pitting corrosion, rather than providing information through a subjective visual assessment. Moreover, it allows introducing the probability of detection and probability of false alarms in the decision chain and in risk analysis. The method is shown to be robust, reliable, simple and inexpensive...|$|R
