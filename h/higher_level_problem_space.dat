0|10000|Public
40|$|A hierarchical, time {{decomposition}} {{and coordination}} scheme for long horizon optimal control problems {{is suitable for}} parallel processing and adds {{a new dimension to}} results on large-scale dynamic optimization. Key Words-Dynamic optimization; decomposition and coordination; hierarchical optimization; parallel processing. Abstract-This paper presents a new method in solving long horizon optimal control problems. The original problem is decomposed along the time axis to form many smaller subproblems, and a <b>high</b> <b>level</b> <b>problem</b> is created that uses initial and terminal states of subproblems as coordination parameters. In such a scheme, the <b>high</b> <b>level</b> <b>problem</b> is a parameter optimization problem. Subproblems are optimal control problems having shorter time horizon, and are completely decoupled {{so that they can be}} solved in parallel. It is shown that the two-level problem has the same global optimum as the original one. Moreover, the <b>high</b> <b>level</b> <b>problem</b> is a convex programming problem if the original problem has a convex cost function and linear system dynamics. A parallel, two-level optimization algorithm is then presented, where the the <b>high</b> <b>level</b> <b>problem</b> is solved by Newton's method, and low level subproblems are solved by the Differential Dynamic Programming technique. Numerical testings on two examples are given to illustrate the idea, and to demonstrate the potential of the new method in solving long horizon problems under a parallel processing environment. • 1...|$|R
50|$|Phosphates are not {{toxic to}} people or animals {{unless they are}} present in very <b>high</b> <b>levels.</b> Digestive <b>problems</b> could occur from {{extremely}} <b>high</b> <b>levels</b> of phosphate.|$|R
40|$|This study {{examined}} drinking motives, alcohol use, and alcohol-related problems among White collegiate athletes and collegiate athletes of color (N = 113). Results indicated {{no differences in}} drinking motives between the two groups. Although White athletes reported <b>higher</b> <b>levels</b> of alcohol use, athletes of color reported <b>higher</b> <b>levels</b> of alcohol-related <b>problems.</b> Athletes of color with <b>high</b> <b>levels</b> of coping and conformity motives reported the <b>highest</b> <b>level</b> of alcohol-related <b>problems...</b>|$|R
40|$|DPLL based clause {{learning}} algorithms for satisfiability testing {{are known}} to work very well in practice. However, like most branch-and-bound techniques, their performance depends heavily on the variable order used in making branching decisions. We propose a novel way of exploiting the underlying problem structure to guide clause learning algorithms toward faster solutions. The key idea {{is to use a}} <b>higher</b> <b>level</b> <b>problem</b> description, such as a graph or a PDDL specification, to generate a good branching sequence as an aid to SAT solvers...|$|R
40|$|In {{this paper}} we propose a new {{approach}} to solve combinatorial optimization problems. Our approach is simple to implement but powerful in terms of performance and speed. We combine the strengths of a meta-heuristic approach with the integer programming method by partitioning the problem into two interrelated subproblems, where the <b>higher</b> <b>level</b> <b>problem</b> is solved by the metahueristic and the lower <b>level</b> <b>problem</b> is solved by integer programming. We discuss the selection of key variables to facilitate an effective partitioning, and test our approach on two real world cross-docking problems which is very popular {{in this part of the}} world. Our experimental results indicate that our new approach is very promising...|$|R
40|$|In {{order to}} improve the {{performance}} of heuristic search for finding optimal solutions, two <b>high</b> <b>level</b> <b>problem</b> solving strategies, namely, subgoal ordering and goal augmentation, have been developed. The essence of these two strategies is to make explicit the knowledge embedded in a general problem formulation {{which can be used}} to constrain the solution search space. These two strategies have been incorporated into a methodology which we previously developed for automatically generating admissible search heuristics. The effectiveness of these strategies is demonstrated by the application to robot optimal task planning problems. 1...|$|R
40|$|Abstract — We {{introduce}} {{methods of}} hierarchically decomposing {{three types of}} graph optimization problems: all-pairs shortest path, all-pairs maximum flow, and search. Each method uses a partition on the graph to create a <b>high</b> <b>level</b> <b>problem</b> and several lower <b>level</b> <b>problems.</b> The computations on each level are identical, so the low <b>level</b> <b>problems</b> can be further decomposed. In this way, the problems become fractal in nature. We use these decomposition methods to establish {{upper and lower bounds}} on the optimal criteria of each problem, which can be achieved with much less computation than what is required to solve the original problem. Also, for each problem, we find an optimal number of partitions that minimizes computation time. As the number of hierarchical levels increases, the computational complexity decreases at the expense of looser bounds. I...|$|R
5000|$|Rise Above - by transcending trivialities and oversimplifications, {{students}} work towards more inclusive principles and <b>higher</b> <b>level</b> formulations of <b>problems</b> ...|$|R
30|$|Third, {{there were}} greater {{opportunities}} for students to apply the new knowledge in solving problems (Chao et al. 2015; Mazur et al. 2015; Schultz et al. 2014) and engage in the discussion of <b>higher</b> <b>level</b> <b>problems</b> (Tsai et al. 2015). Consistent with Kettle’s (2013) students’ opinion, Bhagat et al. (2016) pointed out that working through problems in class was an effective and enjoyable learning activity of flipped classroom approach. Clark’s (2015) students also showed their preference toward flipped classroom approach since it provided more chances for a variety of instructional practices (e.g., project-based learning, real-world applications) rather than merely listening to lectures.|$|R
40|$|We {{investigated}} {{whether a}} successful career choice intervention reduces psychological problems, and whether this program was equally effective in participants with low and with <b>high</b> <b>levels</b> of psychological <b>problems.</b> Participants were 45 Dutch students (age 17 - 24) with career choice problems. They had above average <b>levels</b> of psychological <b>problems</b> {{before the start of}} the intervention. These problems decreased significantly following the intervention. With regard to vocational commitment development, the intervention was equally effective for participants with low or average and with (very) <b>high</b> <b>levels</b> of psychological <b>problems</b> {{before the start of the}} intervention...|$|R
40|$|We {{consider}} a variant on a pick-up and delivery problem with time windows, where both {{early and late}} delivery of items is penalized. This problem arises from an application {{in the area of}} defence, but has wider applications in delivery of perishable goods. We develop a heuristic for this problem- decomposing the <b>problem</b> into a <b>higher</b> <b>level</b> <b>problem</b> assigning items to be delivered to trips of vehicles (which we solve using tabu search) and a lower <b>level</b> <b>problem</b> choosing optimal delivery times for these trips (which we solve by an exact iterative approach). We compare this approach with an integer programming model. For quite small instances of the problem, the associated integer programming problem cannot be solved, but testing on small problems suggests the heuristic is a promising approach for finding good solutions. ...|$|R
40|$|This {{dissertation}} {{presents a}} new solution methodology for the Generalized Scheduling Problem (GSP) in operation scheduling of engineering systems. A complex operation scheduling problem is commonly divided into {{a hierarchy of}} scheduling activities based on time scales involved. The GSP seeks for better integration of individual scheduling activities. In this dissertation, the scheduling problem is formulated as a long horizon, large-scale dynamic optimization problem. By applying temporal decomposition, the problem is converted into a hierarchical, two-level optimization problem. Stackelberg game concepts are then used to define interrelationship of subproblems, and suggest proper coordination schemes. Specifically, a target coordination scheme is investigated. ^ The idea of target coordination is to use initial and terminal states of low-level subproblems as coordination terms. These coordination terms are then optimized by a <b>high</b> <b>level</b> <b>problem.</b> Consequently, the <b>high</b> <b>level</b> <b>problem</b> is a parameter optimization <b>problem,</b> and low <b>level</b> subproblems are optimal control problems having a shorter time horizon. In addition, low level subproblems are completely decoupled and can be solved in parallel. It is shown that the two-level problem has the same global optimum as the original problem, and that the high-level problem is a convex programming problem under appropriate conditions. ^ To realize the target coordination scheme, a parallel, two-level optimization algorithm is designed and tested for a class of unconstrained problems. It adopts a parallel Newton method at the <b>high</b> <b>level</b> and the Differential Dynamic Programming (DDP) at the low level. Two-level optimization design issues are addressed. The algorithm is extended to the constrained case, using Quadratic Programming at the <b>high</b> <b>level</b> and Constrained DDP techniques at the low level. The constrained algorithm is then applied to hydro generation scheduling problems. Testing results of both unconstrained and constrained cases show {{the potential of the}} temporal decomposition approach in tackling long-horizon problems under parallel processing environments. The temporal decomposition structure matches the current hierarchical scheduling practice, and serves as a basis for further study of GSP. ...|$|R
40|$|International audienceProduction {{exploitation}} of cardiac image analysis tools is {{hampered by the}} lack of proper IT infrastructure in health institutions, the non trivial integration of heterogeneous codes in coherent analysis procedures, and the need to achieve complete automation of these methods. HealthGrids are promising technologies to address these difficulties. This paper details how they can be complemented by <b>high</b> <b>level</b> <b>problem</b> solving environments such as workflow managers to improve the performance of applications both in terms of execution time and robustness of re- sults. Two of the most important important cardiac image analysis tasks are consid- ered, namely myocardium segmentation and motion estimation in a 4 D sequence. Results are shown on the corresponding pipelines, using two different execution environments on the EGEE grid production infrastructure...|$|R
40|$|AbstractThis paper {{develops}} a transformational paradigm by which nonnumerical algorithms {{are treated as}} fixed point computations derived from very <b>high</b> <b>level</b> <b>problem</b> specifications. We begin by presenting an abstract functional problem specification language SQ+, which is shown to express any partial recursive function in a fixed point normal form. Next, we give a nondeterministic iterative schema {{that in the case}} of finite iteration generalizes the “chaotic iteration” of Cousot and Cousot for computing fixed points of monotone functions efficiently. New techniques are discussed for recomputing fixed points of distributive functions efficiently. Numerous examples illustrate how these techniques for computing and recomputing fixed points can be incorporated within a transformational programming methodology to facilitate the design and verification of nonnumerical algorithms...|$|R
40|$|SOFTNET is a packet-radio concept under {{development}} in Sweden. The network is distributed and all nodes are programmable via the network during normal operation. This concept represents an unconventional {{approach to the}} protocol issue and offers elegant solutions to the <b>higher</b> <b>level</b> communication <b>problems.</b> This paper gives a programming model of the network, along with some illustrating examples. I...|$|R
50|$|A {{rating of}} 3 {{indicates}} {{a significant degree}} of concern, based on either current or anticipated asset quality problems. Credit unions in this category may have only a moderate <b>level</b> of <b>problem</b> assets. However, these credit unions may be experiencing negative trends, inadequate loan underwriting, poor documentation, higher risk investments, inadequate lending and investment controls and monitoring that indicate a reasonable probability of increasingly <b>higher</b> <b>levels</b> of <b>problem</b> assets and high-risk concentration.|$|R
40|$|Abstract. DPLL based clause {{learning}} algorithms for satisfiability testing {{are known}} to work very well in practice. However, like most branch-and-bound techniques, their performance depends heavily on the variable order used in making branching decisions. We propose a novel way of exploiting the underlying problem structure to guide clause learning algorithms toward faster solutions. The key idea {{is to use a}} <b>higher</b> <b>level</b> <b>problem</b> description, such as a graph or a PDDL specification, to generate a good branching sequence as an aid to SAT solvers. The sequence captures hierarchical structure that is lost in the CNF translation. We show that this leads to exponential speedups on grid and randomized pebbling problems. The ideas we use originate from the analysis of problem structure recently used in [1] to study clause learning from a theoretical perspective. ...|$|R
40|$|The {{origins of}} the Soar {{architecture}} {{can be traced back}} to the seminal research of Allen Newell and Herbert Simon on symbol systems, heuristic search, goals, <b>problem</b> <b>spaces,</b> and production systems. Since its official inception in 1982, Soar has evolved through six major releases, as both an AI architecture and as the basis for a unified theory of cognition. This paper traces this evolutionary path, starting with Soar's intellectual roots, and then proceeding through the stages defined by the six major system releases. Each stage is characterized with respect to a hierarchy of four levels of analysis: the knowledge <b>level,</b> the <b>problem</b> <b>space</b> <b>level,</b> the symbolic architecture level, and the implementation level...|$|R
40|$|We give a brief {{introduction}} to sequential spatial processes. We discuss their definition, formulate a Markov property, and indicate why such processes are natural tools in tackling <b>high</b> <b>level</b> vision <b>problems.</b> We {{focus on the}} problem of tracking a variable number of moving objects through a video stream, and discuss the relationship with the popular Hough transform. A list of pointers to the literature concludes the paper...|$|R
40|$|This paper {{addresses}} a <b>high</b> <b>level</b> spatio-temporal <b>problem,</b> namely "absolute orientation", which arises in visual-odometry (using stereo), or registering two models created by different Structure from Motion (SFM) reconstructions. We compare the very popular method due to Horn using quaternions {{and our own}} independently derive method using the orthonormal rotation matrix R. We also introduce a novel approach for outlier rejection using spectral clustering...|$|R
40|$|The {{current study}} sought {{to examine the effects}} of {{gambling}} attitudes and beliefs on problem gambling behaviour across three cohorts, namely Generation X, Baby Boomers, and the Silent Generation. Individuals from Northern and Southern Ontario completed either an online or paper version of a questionnaire that included the Canadian Problem Gambling Index (CPGI) and South Oaks Gambling Screen (SOGS) to measure problem gambling, the Gambling Attitudes Scales (GAS) to measure gambling attitudes, and the Gambling Attitudes and Beliefs Survey (GABS), Gamblers? Beliefs Questionnaire (GBQ), and Gambling Related Cognitions Scale (GRCS) to measure gambling beliefs. A sample of 308 participants consisted of 101 individuals from Generation X, 139 from the Baby Boom cohort, and 68 from the Silent Generation. Analyses focused on differences between Baby Boomers and their surrounding cohorts, but especially on differences between the older two cohorts. For Baby Boomers, higher scores on the GBQ Luck/Perseverance scale and GRCS Illusion of Control scale were associated with <b>higher</b> <b>levels</b> of <b>problem</b> gambling than the Silent Generation. However, Generation X?s scores on GBQ Luck/Perseverance were associated with <b>higher</b> <b>levels</b> of <b>problem</b> gambling than the Baby Boom cohort. Furthermore, <b>higher</b> <b>levels</b> of the Perceived Inability to Stop Gambling variable on the GRCS was more associated with problem gambling status in the Baby Boom cohort than the Silent Generation. Attitudes associated with problem gambling were not found to differ among cohorts. These results suggest that while cohorts may not differ in types or levels of distorted beliefs, they differ in how such distortions relate to problem gambling. Therefore, according to our results, Baby Boomers who have distortions about luck or illusions of control over gambling have <b>higher</b> <b>levels</b> of <b>problem</b> gambling than those with similar distortions from the Silent Generatio...|$|R
40|$|Individuals {{with the}} short {{variant of the}} {{serotonin}} transporter linked polymorphic region gene are more susceptible than individuals homozygous for the long allele {{to the effects of}} stressful life events on risk for internalizing and externalizing problems. We tested whether individual differences in coping style explained this increased risk for problem behavior among youth who were at both genetic and environmental risk. Participants included 279 children, ages 8 – 11, from the Children's Experiences and Development Study. Caregivers and teachers reported on children's internalizing and externalizing symptoms, and caregivers and children on children's exposure to harsh parenting and parental warmth in middle childhood, and traumatic events. Children reported how frequently they used various coping strategies. Results revealed that short/short homozygotes had <b>higher</b> <b>levels</b> of internalizing <b>problems</b> compared with long allele carriers and that short allele carriers had <b>higher</b> <b>levels</b> of externalizing <b>problems</b> compared with long/long homozygotes under conditions of high cumulative risk. Moreover, among children who were homozygous for the short allele, those who had more cumulative risk indicators less frequently used distraction coping strategies, which partly explained why they had <b>higher</b> <b>levels</b> of internalizing <b>problems.</b> Coping strategies did not significantly mediate Gene × Environment effects on externalizing symptoms...|$|R
40|$|This paper {{addresses}} a <b>high</b> <b>level</b> spatio-temporal <b>problem,</b> namely &quot;absolute orientation&quot;, which arises in visualodometry (using stereo), or registering two models created by different Structure from Motion (SFM) reconstructions. We compare the very popular method due to Horn 1 using quaternions {{and our own}} independently derived method using the orthogonal rotation matrix R. We also introduce a novel approach for outlier rejection using spectral clustering 2, 3. Introduction: Absolute orientation, Horn 1, is a <b>higher</b> <b>level</b> spatio-temporal <b>problem</b> that arises in visualodometry, graphics, mesh registration, and extending 3 D models from SFM or stereo. The problem statement is simple: given two sets of matched points, p and q ∈ 3, find the similarity transform that takes p into q, rejecting all outliers. By similarity, we mean the translation, rotation and scale such that qi = s * R * pi +t i, here s is the scale ambiguity (arising if p and q are SFM outputs), R is the 3 x 3 rotation matrix, and t is the translation...|$|R
40|$|We {{propose a}} new low-overhead {{framework}} for representing and utilizing problem symmetry in propositional satisfiability algorithms. While many previous approaches {{have focused on}} symmetry extraction as a key component, the novelty in our strategy lies in using <b>high</b> <b>level</b> <b>problem</b> description to pass on symmetry information to the SAT solver in a simple and concise form, {{in addition to the}} usual CNF formula. This information, comprising of the so-called symmetry sets and variable classes, captures variable semantics relevant to symmetry and is utilized dynamically to prune the search space. This allows us to address many limitations of alternative approaches like symmetry breaking predicates, implicit pseudo-Boolean representations, general group-theoretic methods, ZBDDs, etc. We demonstrate the efficacy of our technique through a solver called Sym-Chaff, which achieves exponential speedup over DPLL-based SAT solvers on problems from both theory and practice, often by simply using natural tags or annotation in the problem specification. ...|$|R
40|$|Recent {{emergence}} of effective solvers for propositional satisfiability (SAT) and related problems {{has led to}} new methods for solving computationally challenging industrial problems, such as NP-hard search problems in planning, software design, and hardware verification. This has produced a demand for tools which allow users to write <b>high</b> <b>level</b> <b>problem</b> specifications which are automatically reduced to SAT. We {{consider the case of}} specifications in first order logic with reduction to SAT by grounding. For realistic problems, the resulting SAT instances can be prohibitively large. A key technique in SAT solvers is unit propagation, which often significantly reduces instance size before search for a solution begins. We define ”lifted unit propagation”, which is executed before grounding. We show that instances produced by a grounding algorithm with lifted unit propagation are never larger than those produced by normal grounding followed by UP, and demonstrate experimentally that they are sometimes much smaller...|$|R
40|$|Thirty-one first-year {{psychology}} {{graduate students}} in a computer applications course completed a set of structured problems, unstructured problems, and data-screening problems in each of two statistical computing environments: a menu-based interface (SPSS for Windows) and a traditional command-based interface (SPSSx). Performance on the menu-based interface was generally superior to performance on the command-based interface. More of the structured problems were completed successfully within the menu-based interface. The menu-based interface also facilitated error identification, was rated as easier to use, and was preferred nearly 4 to 1 over the command-based interface. For the unstructured problems, students identified more relationships in the data set and issued more statistical commands when working with the menu-based interface. These {{findings are consistent with}} the interpretation that because the menu-based interface requires fewer mental resources to be dedicated to the mechanics of analysis, more resources are available to devote to <b>higher</b> <b>level</b> <b>problem</b> solving...|$|R
40|$|We {{introduce}} {{a method of}} hierarchically decomposing graph optimization problems to obtain approximate solutions with low computation. The method uses a partition on the graph to convert the original <b>problem</b> to a <b>high</b> <b>level</b> <b>problem</b> and several lower <b>level</b> <b>problems.</b> On each <b>level,</b> the resulting <b>problems</b> are {{in exactly the same}} form as the original one, so they can be further decomposed. In this way, the problems become fractal in nature. We use best-case and worst-case instances of the decomposed problems to establish upper and lower bounds on the optimal criteria, and these bounds are achieved with significantly less computation than what is required to solve the original problem. We show that as the number of hierarchical levels increases, the computational complexity approaches O(n) at the expense of looser approximation bounds. For regular lattice graphs, we provide constant factor bounds on the approximation error. We demonstrate the fractal decomposition method on three example problems related to cooperative routing: shortest path matrix, maximum flow matrix, and cooperative search. Large-scale simulations show that this fractal decomposition method is computationally fast and can yield good results for practical problems. ...|$|R
40|$|Abstract. Production {{exploitation}} of cardiac image analysis tools is {{hampered by the}} lack of proper IT infrastructure in health institutions, the non trivial integration of heterogeneous codes in coherent analysis procedures, and the need to achieve complete automation of these methods. HealthGrids are promising technologies to address these difficulties. This paper details how they can be complemented by <b>high</b> <b>level</b> <b>problem</b> solving environments such as workflow managers to improve the performance of applications both in terms of execution time and robustness of re-sults. Two of the most important important cardiac image analysis tasks are consid-ered, namely myocardium segmentation and motion estimation in a 4 D sequence. Results are shown on the corresponding pipelines, using two different execution environments on the EGEE grid production infrastructure. Keywords. Cardiac image analysis, workflow enactment, grid computing, production. 1. Production-level Cardiac Image Sequences Analysis Cardiac diseases are one of the major causes of death in industrial countries. Cardiac di-agnosis is increasingly relying on temporal sequences of 3 D images acquired using fas...|$|R
40|$|This article {{presents}} a new low-overhead framework for representing and utilizing problem symmetry in propositional satisfiability algorithms. While many previous approaches {{have focused on}} symmetry extraction as a key component, the novelty in the proposed strategy lies in using <b>high</b> <b>level</b> <b>problem</b> description to pass on symmetry information to the SAT solver in a simple and concise form, {{in addition to the}} usual CNF formula. This information, comprising of the so-called symmetry sets and variable classes, captures variable semantics relevant to “complete multi-class symmetry ” and is utilized dynamically to prune the search space. This allows us to address many limitations of alternative approaches like symmetry breaking predicates, implicit pseudo-Boolean representations, general group-theoretic methods, and ZBDDs. We demonstrate the efficacy of our technique through a solver called SymChaff, which achieves exponential speedup over DPLL-based SAT solvers on problems from both theory and practice, often by simply using natural tags or annotation in the problem specification...|$|R
40|$|The Assessement of Estuarine Trophic Status (ASSETS) {{assessment}} method is a Pressure-State-Response model {{that has been}} used most recently to complete the update of the National Estuarine Eutrophication Assessment (NEEA), an examination of a decade of change in nutrient related impacts in 141 U. S. estuaries. It has also been applied to systems from Europe and Asia and shows that similar eutrophic symptoms occur in coastal waterbodies around the globe. The model includes three components: Influencing Factors which are a combination of natural system susceptibility and human-related nutrient loads, Overall Eutrophic Condition based on the combined staus of five indicators (chlorophyll a, macroalgae, dissolved oxygen, seagrass distribution, and nuisance/toxic blooms), and Future Outlook which examines how conditions will change in the future. The three components are then combined into a single rating for a system. The NEEA results show that eutrophication is a widespread problem in U. S. systems with 65 % of assessed systems showing moderate to <b>high</b> <b>level</b> <b>problem...</b>|$|R
40|$|We briefly {{introduce}} the memory based approaches to emulate machine intelligence in VLSI hardware, describing {{the challenges and}} advantages. Implementation of artificial intelligence techniques in VLSI hardware is a practical and difficult problem. Deep architectures, hierarchical temporal memories and memory networks {{are some of the}} contemporary approaches in this area of research. The techniques attempt to emulate low level intelligence tasks and aim at providing scalable solutions to <b>high</b> <b>level</b> intelligence <b>problems</b> such as sparse coding and contextual processing...|$|R
40|$|Constraint Programming (CP) is a {{powerful}} technique for solving large-scale combinatorial (optimisation) problems. Constraint solving a given problem proceeds in two phases: modelling and solving. Effective modelling has an huge impact {{on the performance of}} the solving process. This thesis presents a framework in which the users are not required to make modelling decisions, concrete CP models are automatically generated from a <b>high</b> <b>level</b> <b>problem</b> specification. In this framework, modelling decisions are encoded as generic rewrite rules applicable to many different problems. First, modelling decisions are divided into two broad categories. This categorisation guides the automation of each kind of modelling decision and also leads us to the architecture of the automated modelling tool. Second, a domain-specific declarative rewrite rule language is introduced. Thanks to the rule language, automated modelling transformations and the core system are decoupled. The rule language greatly increases the extensibility and maintainability of the rewrite rules database. The database of rules represents the modelling knowledge acquired after analysis of expert models. This database must be easily extensible to best benefit from the active research on constraint modelling. Third, the automated modelling system Conjure is implemented as a realisation of these ideas; having an implementation enables empirical testing of the quality of generated models. The ease with which rewrite rules can be encoded to produce good models is shown. Furthermore, thanks to the generality of the system, one needs to add {{a very small number of}} rules to encode many transformations. Finally, the work is evaluated by comparing the generated models to expert models found in the literature for a wide variety of benchmark problems. This evaluation confirms the hypothesis that expert models can be automatically generated starting from <b>high</b> <b>level</b> <b>problem</b> specifications. An method of automatically identifying good models is also presented. In summary, this thesis presents a framework to enable the automatic generation of efficient constraint models from problem specifications. It provides a pleasant environment for both problem owners and modelling experts. Problem owners are presented with a fully automated constraint solution process, once they have a precise description of their problem. Modelling experts can now encode their precious modelling expertise as rewrite rules instead of merely modelling a single problem; resulting in reusable constraint modelling knowledge...|$|R
40|$|Abstract- Nowadays, {{many papers}} are {{developing}} {{to improve the}} software quality control. In our paper {{we are going to}} help the developers to maintain the source code and identifiers and we will show the textual similarity between source code and related <b>high</b> <b>level</b> faults. The developers are improving the source code library. So, if the software development environment provides similarities between the source code and the <b>high</b> <b>level</b> <b>problems</b> then it will be quite easier for the developers to keep the software quality ahead. In our proposing system the candidate identifiers needs to implement in eclipse IDE for that we need a plug-in called COde COmprehension Nurturant Using Traceability (COCONUT). This paper also reports on two controlled experiments performed with master’s and bachelor’s students. The quality of identifiers, comments in the produced source code with or without coconut. The approach presented in this paper relates to approaches aimed at applying IR techniques for traceability recovery and for quality improvement/assessment. So that quality of the source code lexicon will be improved. Thus the usefulness of the coconut is taken as a feature of software development environments...|$|R
50|$|Socioeconomic {{status and}} length of time left alone can bring forth other {{negative}} effects. In one study, middle school students left home alone {{for more than three}} hours a day reported <b>higher</b> <b>levels</b> of behavioral <b>problems,</b> <b>higher</b> rates of depression, and lower levels of self-esteem than other students.|$|R
40|$|It {{has been}} found that fluorodiepoxides are {{effective}} antifoaming agents for epoxy processing when applied at a level as low as 0. 1 to 0. 2 percent, and there is no adverse effect on the properties of the cured products. The novelty of the fluorodiepoxide is that it is reactive and can copolymerize with a conventional epoxy resin {{at the same time in}} the same way in forming an inter-crosslinked polymer network. However, when applied at a <b>high</b> <b>level,</b> compatibility <b>problems</b> may prevail...|$|R
40|$|We {{evaluated}} {{the effects of}} training novel and existing mands during functional communication training (FCT) to decrease problem behavior for 2 children. A functional analysis (Phase 1) identified mands for FCT. Phase 2 used distinct stimulus conditions to train novel and existing mands. Phase 3 evaluated allocation of responding within a concurrent-schedules design. When reinforcement for either mand was concurrently available, the children used existing mands more than novel mands, but <b>higher</b> <b>levels</b> of <b>problem</b> behavior occurred with existing mands...|$|R
