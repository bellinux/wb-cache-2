18|390|Public
5000|$|Cold pressed or Cold <b>extraction</b> <b>means</b> [...] "that the oil was not heated over {{a certain}} {{temperature}} (usually 27 C) during processing, thus retaining more nutrients and undergoing less degradation". [...] The difference between Cold Extraction and Cold Pressed is regulated in Europe, where {{the use of}} a centrifuge, the modern method of extraction for large quantities, must be labelled as Cold Extracted, while only a physically pressed olive oil may be labelled as Cold Pressed. In many parts of the world, such as Australia, producers using centrifugal extraction still label their products as Cold Pressed.|$|E
40|$|Wireless {{motion sensor}} network for {{monitoring}} motion {{in a process}} comprising at least one wireless sensor node for measuring at least one physical quantity related to motion or orientation, feature <b>extraction</b> <b>means</b> for deriving a feature for the measured quantities, a wireless transmitter connected to the feature <b>extraction</b> <b>means</b> for transmitting the derived feature, and the wireless receiver receiving derived features from other sensor nodes, the network further comprising a reasoning node for collecting features transmitted by {{the at least one}} wireless sensor node comprising a collaborative reasoning engine for determining further features based on features received by a wireless receiver wherein the further features are determined by calculation and/or a rule set; the wireless motion sensor network comprising a feedback and/or actuation means for intervening in or influencing a monitored process based on the output of the collaborative reasoning engine...|$|E
40|$|An {{embodiment}} of an apparatus (200) for computing filter coefficients for an adaptive filter (210) for filtering a microphone signal {{so as to}} suppress an echo due to a loudspeaker signal includes <b>extraction</b> <b>means</b> (250) for extracting a stationary component signal or a non-stationary component signal from the loudspeaker signal or from a signal derived from the loudspeaker signal, and computing means (270) for computing the filter coefficients for the adaptive filter (210) {{on the basis of}} the extracted stationary component signal or the extracted non-stationary component signal...|$|E
30|$|The <b>mean</b> <b>extraction</b> {{recoveries}} of hetrombopag {{from three}} different QC levels were 88.1, 85.0, 91.0  %, respectively. The <b>mean</b> <b>extraction</b> recovery of IS was 75.2  %. The results demonstrated that the extraction process is efficient.|$|R
40|$|OBJECTIVE: To {{examine the}} {{differences}} in Willingness to pay (WTP) for an extraction, a filling, and cleaning of teeth among older adults with varying levels of Oral Health-related Quality of Life (OHQoL). BACKGROUND: OHQoL has been used extensively to measure utilities as reported by individuals of interest. Currently there are no reports that examine the WTP of individuals at various levels of OHQoL. METHODS: A convenience sample of adults 60 years or older were recruited. Besides other domains, questionnaires {{were used to assess}} WTP (extraction, filling, and cleaning of teeth), OHQoL (using Oral Impacts on Daily Performance-OIDP), McArthur scale, and access to care. RESULTS: Tamil ethnicity was related to higher WTP for an <b>extraction</b> (<b>mean</b> ratio, 1. 63 - 3. 98; 95 % Confidence Interval [CI]), increase of age in years was related to lower WTP for <b>extraction</b> (<b>mean</b> ratio, 0. 96 - 1. 00 [95 %CI]) and increasing OIDP score was related to lower WTP for <b>extractions</b> (<b>mean</b> ratio, 0. 80 - 0. 99 [95 %CI]). Tamil ethnicity was associated with higher WTP for fillings (mean ratio, 2. 69 - 6. 44 [95 %CI]); higher age in years was associated with lower WTP for fillings (mean ratio, 0. 94 - 0. 99 [95 %CI]), and higher OIDP scores was trending to be associated to lower WTP for filling (mean ratio, 0. 80 - 1. 00 [95 %CI]). Tamil Ethnicity was also associated with higher WTP for cleaning (mean ratio, 2. 14 - 7. 19 [95 %CI]), higher age in years was also associated with cleaning (mean ratio, 0. 94 - 0. 99 [95 %CI]). CONCLUSION: Individuals with higher OIDP scores tended to have lower WTP for extraction, filling and cleaning; with significant differences reported for extraction. Rahul Nairn, Robert Ye...|$|R
40|$|Introduction: Competence based {{education}} {{is becoming more}} important in dentistry and medicine. In dentistry clinical skills are assessed using longitudinal assessments or structured objective clinical tests. We have previously presented the assessment of competence in surgical extractions however the success rate for this was poor. The opportunity to alter staffing levels and timetabling arose and we present the influence of this on the achievement of competence. Methods: The competence assessments and portfolios of two consecutive years of dental undergraduates were examined after completing their surgical extraction course. The first cohort received 9 sessions of teaching spread over 2 years with one staff supervisor per session. The second cohort received 10 sessions with varying numbers of staff supervisors. Results: The first cohort required 210 staff sessions and performed 275 surgical <b>extractions</b> (<b>mean</b> 4), and 23 % achieved competence. The second cohort required 240 staff sessions and performed 403 surgical <b>extractions</b> (<b>mean</b> 6), and 66 % achieved competence. Thirty six extra sessions were provided for students in the second cohort who failed to complete their competence during the allocated blocks and following this 99 % of the second cohort achieved competence. These differences are significant (P < 0. 01). Conclusion: It is possible to demonstrate competence in large numbers of undergraduates in surgical extraction. The process can be influenced by staffing and timetabling changes which focus student experience and learning. Â© 2009 Blackwell Munksgaard. Link_to_subscribed_fulltex...|$|R
40|$|Tracter is {{introduced}} as a dataflow framework particularly useful for speech recognition. It {{is designed to}} work on-line in real-time as well as off-line, and is the feature <b>extraction</b> <b>means</b> for the Juicer transducer based decoder. This paper places Tracter in context amongst the dataflow literature and other commercial and open source packages. Some design aspects and capabilities are discussed. Finally, a fairly large processing graph incorporating voice activity detection and feature extraction is presented {{as an example of}} Tracter’s capabilites. Index Terms: Dataflow, speech recognition, open source. ...|$|E
30|$|Selection of text feature item is a {{basic and}} {{important}} matter for text mining and information retrieval. Feature <b>extraction</b> <b>means</b> that according to the certain feature extraction metrics, the extract is relevant to the original feature subsets from initial feature sets of test sets, so as to reduce the dimensionality of feature vector spaces. During feature extraction, the uncorrelated or superfluous features will be deleted. As a method of data preprocessing of the learning algorithm, feature extraction can better improve the accuracy of learning algorithm and shorten the time. Compared with other machine learning methods, deep learning is able to detect complicated interactions from features, learn lower level features from nearly unprocessed original data, mine charateristics that {{is not easy to}} be detected, hand class members with high cardinal numbers, and process untapped data.|$|E
40|$|The {{description}} {{refers to}} a liquid/liquid extraction process which is in particular suitable for the separation and enrichment of microbial metabolic products from aqueous phases by means of organic phases. The process according to the invention {{is characterized by the}} fact that one of the two phases, preferably the <b>extraction</b> <b>means,</b> is bound to suitable carriers, preferably open-pored porous sintered glass compact, by means of a known extraction process. The connected phase is located in the pores such that the mass transfer takes place at the pore openings {{on the surface of the}} carrier when the two phases are mixed. A fast and clean phase separation on completion of the intermixture phase is simply obtained in systems which tend to form stable emulsions, so that the non-bound phase is removed from the carrier loaded with the bound phase...|$|E
40|$|Recent {{improvements}} in tracking and feature <b>extraction</b> <b>mean</b> that speaker-dependent lip-reading of continuous speech using a medium size vocabulary (around 1000 words) is realistic. However, {{the recognition of}} previously unseen speakers {{has been found to}} be a very challenging task, because of the large variation in lip-shapes across speakers and the lack of large, tracked databases of visual features, which are very expensive to produce. By adapting a technique that is established in speech recognition but has not previously been used in lip-reading, we show that error-rates for speaker-independent lip-reading can be very significantly reduced. Furthermore, we show that error-rates can be even further reduced by the additional use of Deep Neural Networks (DNN). We also find that there is no need to map phonemes to visemes for context-dependent visual speech transcription...|$|R
40|$|Business cycle {{estimates}} are typically {{the output of}} a two-stage filtering process: a statistical agency first publishes seasonally adjusted data, and from this an econometrician estimates the cycle. In many cases the two filtering procedures used are not compatible, because two different agents are acting on the data independently. This paper derives formulas to state the signal <b>extraction</b> <b>Mean</b> Squared Error (MSE) that results from such two-stage filtering, assuming an ARIMA model-based framework for a finite sample of data. We also look at the “mixed ” and “direct ” techniques of Kaiser and Maravall (2005) for obtaining implied models for the cycle, and show that the direct approach can generate optimal estimates in the finite-sample context as well. Several two-stage filtering procedures are analyzed theoretically, and the methods are demonstrated and compared on a simulated time series...|$|R
50|$|The second {{referendum}} {{was held}} in the communes of Limanu, Costinești {{as well as in the}} city of Mangalia and asked voters to say whether they were in favor or against the use, by Chevron, of shale gas <b>extraction</b> by <b>means</b> of hydraulic fracturing. This referendum failed to meet the required turnout as well.|$|R
30|$|There {{are several}} {{possible}} reasons for why extractive industries, more than others, {{tend to be}} violent. Perhaps, {{the high level of}} capital investment raises the stakes for industry and makes them less able to tolerate dissent (Gartner and Regan 1996). Perhaps, extraction industries are more prone to violence for the same reasons that they are prone to corruption—the industry tends to couple monopoly market control without accountability (O’Higgins 2006). Finally, {{it may be that the}} isolated geography of resource <b>extraction</b> <b>means</b> that state capacity is less, making violence more possible (Wood 2010; Herreros and Criado 2009). This connection between extractive industries and violence is important because indigenous groups often have contested rights over the land from which the natural resource is being extracted (Inter-American Commission on Human Rights 2010 – 2011), and ethnic and economic minorities are disproportionately victims when violence occurs (Downey et al. 2010).|$|E
30|$|For extractions on {{powder and}} on solid {{specimens}} for vibrational tests, extractions were run both {{in an independent}} (or parallel) and in a successive (or serial) way. Independent <b>extraction</b> <b>means</b> that a sample is submitted to one extraction in a given solvent, and its properties are measured before and after this single treatment. In this case, hot water extraction from vibrational specimens was run (for 8  h) at 70 °C (not using Soxhlet). Successive extractions means that a given group of specimens is submitted to extraction, first by the less polar solvent (HX), then oven dried (brief process: weighted, air-dry stabilized, weighted, mechanically tested, and dried again for solid wood specimens), extracted by the next solvent (DM), and so on. In this case, a “standard” Soxhlet extraction (< 95 °C) was run for water (complete description of conditions {{can be found in}} Table  1).|$|E
40|$|From an {{efficiency}} viewpoint, information <b>extraction</b> <b>means</b> to filter the relevant por-tions of natural language texts {{as fast as}} possible. Given an extraction task, differ-ent pipelines of algorithms can be devised that provide the same precision and recall but that vary in their run-time due to dif-ferent pipeline schedules. While recent re-search investigated how to determine the run-time optimal schedule for a collection or a stream of texts, this paper goes one step beyond: we analyze the run-times of efficient schedules {{as a function of}} the het-erogeneity of the texts and we show how this heterogeneity is characterized from a data perspective. For extraction tasks on heterogeneous big data, we present a self-supervised online adaptation approach that learns to predict the optimal schedule de-pending on the input text. Our evaluation suggests that the approach will significant-ly improve efficiency on collections and streams of texts of high heterogeneity. ...|$|E
40|$|A high {{performance}} liquid chromatographic method {{has been developed}} for the determination of rafoxanide and closantel in ovine plasma. Acetonitrile and chloroform {{were used for the}} <b>extraction.</b> The <b>mean</b> recoveries were 78. 69 % and 80. 59 % for rafoxanide and closantel, respectively. This method was applied to the characterization of rafoxanide plasma kinetics following oral administration of therapeutic doses to sheep...|$|R
40|$|We {{evaluate}} our {{recently developed}} conformal method for quantitative shape extraction from unorganized 3 D oriented point clouds. The conformal method {{has been tested}} previously on real, noisy, 3 D data. Here {{we focus on the}} empirical evaluation of its performance on synthetic, ground truth data, and comparisons with other methods for quantitative <b>extraction</b> of <b>mean</b> and Gauss curvatures presented in the literature...|$|R
40|$|The radical {{efficiency}} {{increase of}} accelerator beam <b>extraction</b> by <b>means</b> of bent crystals is reached {{by reducing the}} longitudinal dimension and bending angle of crystals. The usage of short crystals results in reduction of dechanneling losses and in increase of average number of crystal crossings by particle. Both factors allowed to get a 70 GeV beam extraction efficiency more than 80...|$|R
40|$|Abstract: Data {{hiding and}} {{extraction}} schemes are growing in today’s communication world suitable to {{rapid growth of}} data tracking and tampering attacks. Data hiding, a type of steganography, embeds information into digital media for the reason of identification, annotation, and copyright. In this narrative techniques are used for addressing the data-hiding method and estimate these techniques in glow of three applications: copyright protection, tamper proofing, and augmentation data embedding. Thus we necessitate a proficient and vigorous data hiding schemes to defend from these attacks. In this project the blindly extraction method is measured. Blindly <b>extraction</b> <b>means</b> the novel host and the embedding carriers are not necessitate to be recognized. Here, the hidden data embedded to the host signal, via multicarrier SS embedding. The hidden data is extracted from the digital media like audio, video or image. The extraction algorithm used to extract the hidden data from digital media is Multicarrier Iterative Generalized Least Squares (M-IGLS) ...|$|E
40|$|In {{this paper}} image {{denoising}} {{scheme based on}} fuzzy Gaussian membership function. For a given corrupted image at first converted to the fuzzy values using the fuzzification method. Then extract all patches with overlaps, after extracting all patches each patch is to be permuted and apply the fuzzy Gaussian membership function. <b>Extraction</b> <b>means</b> all patches with overlaps, refer to these as coordinates in high-dimensional space, and arrange them such that they are chained in the shortest possible path. The obtained ordering, applying the fuzzy defuzzification method to convert fuzzy values to the crisp values to what should be a normal signal. This enables us to get high-quality recovery of the clean image by applying relatively simple one-dimensional smoothing operations to the reordered set of pixels. The performance {{of this approach is}} experimentally verified on a diversity of images and noise levels. The results presented here demonstrate that proposed technique is on similarity or more than the existing state of the art, in terms of both peak signal-to-noise ratio and subjective visual quality...|$|E
40|$|This is an open-access article {{distributed}} {{under the}} terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. The western Amazon is the most biologically rich part of the Amazon basin and is home to a great diversity of indigenous ethnic groups, including some of the world’s last uncontacted peoples living in voluntary isolation. Unlike the eastern Brazilian Amazon, it is still a largely intact ecosystem. Underlying this landscape are large reserves of oil and gas, many yet untapped. The growing global demand is leading to unprecedented exploration and development in the region. Without improved policies, the increasing scope and magnitude of planned <b>extraction</b> <b>means</b> that environmental and social impacts are likely to intensify. We review the most pressing oil- and gas-related conservation policy issues confronting the region. These include the need for regional Strategic Environmental Impact Assessments and the adoption of roadless extraction techniques. We also consider the conflicts where the blocks overlap indigenous peoples’ territories...|$|E
40|$|With a view {{to analyze}} {{metribuzin}} residues in soil samples, we need to use special and suitable extraction methods with high efficiency. Five simple and rapid extraction methods (solid phase extraction (SPE) with hydrophilic-lipophilic balance (HLB), SPE with multi-walled carbon nanotubes (MWCNTs), ultrasonic, quick, easy, cheap, effective, rugged and safe (QuECheRS) method, and liquid-solid extraction) coupled to gas chromatography {{were used for the}} analysis of metribuzin herbicide residues in soils. Mean recovery values of analyte were > 80 %. Extracts were analyzed by a gas chromatographic (GC) system equipped with an electron capture detector (ECD). The order of mean recovery values of metribuzin for the five extraction methods is: SPE with HLB&# 160;> SPE with MWCNTs > ultrasonic > QuECheRS > liquid-solid <b>extraction.</b> <b>Mean</b> recovery of analyte depends on the type of soil. The results of this study show that SPE with HLB extraction method is the best option for extracting metribuzin in selected soils...|$|R
40|$|The {{levels of}} trypsin {{inhibitor}} activity were higher in both kabuli and desi seeds of chickpea than their chymotrypsin inhibitor activity. Mean {{values for the}} trypsin and chymotrypsin inhibitor units in dhal and seed samples of desi were higher as compared with kabuli cultivars. The presence of seed coat reduced the protein <b>extraction.</b> <b>Mean</b> values of polyphenolic compounds in seed samples of desi were {{more than twice that}} of kabuli and these differences disappeared in dhal samples indicating the distribution of these compounds mainly in the seed coat. The in vitro protein digestibility studies showed larger differences between desi seed and dhal samples when compared with kabuli seed and dhal samples. Polyphenolic compounds exhibited a highly significant and negative corretation (r = 0. 872 **) with in vitro digestibility of protein and a significant positive correlation with trypsin (r = 0. 612 *) and chymotrypsin (r = 0. 507 *) inhibitor activitie...|$|R
40|$|A {{microwave}} {{pulse compressor}} operated with a superposition of eigen quasi-degenerated modes is considered. A proper choice of eigen frequencies and Q-factors allows essential reduction of diffraction losses during power storage. Peculiarities of power <b>extraction</b> by <b>means</b> of a high-power RF switch are analyzed. 30 GHz projects of multi-megawatt compressors based on: 1) dual-mode circular cross-section cavity {{as well as}} 2) four-mirror cavity with active grating are discussed...|$|R
40|$|Access to {{housekeeping}} telemetry is recurrent {{throughout the whole}} life of a satellite. Flight control teams, spacecraft manufacturer engineers and subsystem experts are all interested in accessing telemetry parameters {{to make sure that}} the satellite is running nominally. This is usually fulfilled by a traditional web server embedded within the control centre which performs extraction and calibration of the raw telemetry. However, this centralized solution is usually available only at the beginning of the operations preparation. The raw telemetry which is produced outside the control centres, e. g. at the satellite supplier premises or on launch pads, is rarely exploited by experts who may miss useful information because of the lack of widespread <b>extraction</b> <b>means.</b> Hence the idea of developing a universal portable application called PrestoDecom offering the same facilities as those inside a control centre, allowing work on raw telemetry archives anywhere, especially when the control centre is inaccessible. This innovative software was successfully used in 2003 to validate the satellite simulator on the PROTEUS multiplatform project. PrestoDecom has been rapidly adopted inside CNES by a growing user community conquered by its simplicity and efficiency. Today 2 / 3 of CNES satellite projects have already invested into PrestoDecom an...|$|E
40|$|Content based image {{retrieval}} (CBIR) {{become a}} challenging problem due to large {{size of the}} image database because difficulty in recognizing images, difficulty in devising a query and evaluating results in terms of semantic gap, computational load to manage large data files and overall retrieval time. To solve this problems Feature extraction is initial and important step {{in the design of}} content based image retrieval system. Feature <b>extraction</b> <b>means</b> extracting unique and valuable information from the image, this features are termed as signature of image. In CBIR system, feature extraction of the image in the database is done offline therefore it does not contribute significantly in computational complexity. Generally Human eyes tend to differentiate images based on color, therefore mostly color features are used in CBIR. When image contain just an object, Color moment is mostly used to represent color features especially. Regularity, directionality, smoothness and coarseness are some of the texture properties perceived by human eye. Gabor filter and wavelet transform for texture feature extraction has proved to be very effective in describing visual content via multi-resolution analysis. The paper mainly gives result of CBIR using combination color moment for color feature and Gabor wav late for texture feature. Also paper gives retrieval of images from medical database...|$|E
40|$|Background: The western Amazon is {{the most}} biologically rich part of the Amazon basin and {{is home to a}} great {{diversity}} of indigenous ethnic groups, including some of the world’s last uncontacted peoples living in voluntary isolation. Unlike the eastern Brazilian Amazon, it is still a largely intact ecosystem. Underlying this landscape are large reserves of oil and gas, many yet untapped. The growing global demand is leading to unprecedented exploration and development in the region. Methodology/Principal Findings: We synthesized information from government sources to quantify the status of oil development in the western Amazon. National governments delimit specific geographic areas or ‘‘blocks’ ’ that are zoned for hydrocarbon activities, which they may lease to state and multinational energy companies for exploration and production. About 180 oil and gas blocks now cover, 688, 000 km 2 of the western Amazon. These blocks overlap the most species-rich part of the Amazon. We also found that many of the blocks overlap indigenous territories, both titled lands and areas utilized by peoples in voluntary isolation. In Ecuador and Peru, oil and gas blocks now cover {{more than two-thirds of the}} Amazon. In Bolivia and western Brazil, major exploration activities are set to increase rapidly. Conclusions/Significance: Without improved policies, the increasing scope and magnitude of planned <b>extraction</b> <b>means</b> that environmental and social impacts are likely to intensify. We review the most pressing oil- and gas-related conservatio...|$|E
50|$|Thorpe is of Danish <b>extraction</b> and <b>means</b> farm, Thorp, and Thewles {{was likely}} {{the name of}} a family that possessed land here in the Middle Ages: the {{earliest}} occurrence of the full name is 'Thorpp' Thewles' in 1265. The surname Thewles probably comes from the Old English theawleas 'immoral', though the meaning of the placename is the Farm of the Thewles Family rather than, as sometimes reported, the Immoral Farm.|$|R
60|$|Thus {{lived and}} thus died this {{extraordinary}} person; a person, though of <b>mean</b> <b>extraction</b> and obscure life, yet when his character {{comes to be}} fully and truly known, it will be read with pleasure, profit, and admiration.|$|R
40|$|Recent {{measurements}} of 120 GeV proton <b>extraction</b> by <b>means</b> of a bent silicon crystal at the CERN-SPS accelerator are summarized. The existence of multi-pass extraction {{has been proven}} by blocking first-pass extraction: using a crystal covered with an amorphous layer, extracted beam with high efficiency was observed, which provides a direct proof {{for the importance of}} the multi-pass mechanism. This opens new possibilities in the design and optimization of a bent crystal extraction scheme...|$|R
40|$|In this work, the {{principles}} of chemical engineering, plant biology, and phytochemistry were successfully applied to the separation of bioactive compounds from plant materials, taking the taxane extraction and purification as an example. A new model, the Solute Distribution Model, was proposed and used to guide the experimental work. Based on the model, two approaches, the Dual-Solvent Extraction approach and the ExtractionAdsorption approach, were developed {{in order to enhance}} both selectivity and recovery rate in bioseparation. Separation from Taxus Canadensis of all valuable products, including paclitaxel, 9 -DHB III, 10 -DAB III, Baccatin III, and other possible by-products, was studied experimentally. Dynamic Pressurized Liquid Extraction, a new extraction technique in the area of natural product mass production, was applied, along with conventional solvent <b>extraction</b> <b>means.</b> Based on the results of this work, the Solute Distribution Model, the Dual-Solvent Extraction approach and the Extraction-Adsorption approach were found to be effective tools for the development of cost-efficient and environmentally friendly technologies in large-scale production of valuable products from plant materials. One novel taxane extraction and purification process, integrated as DPLE-SPE-Chromatography, was invented and proofed. Certain interesting experimental phenomena, particular to the system examined, were observed and explained with fundamental principles. Compared to the reported results of previous studies, the newly developed process could potentially offer higher product yields, milder operating conditions, and lower negative environmental impacts and, therefore, better economics. (Abstract shortened by UMI. ...|$|E
30|$|In this paper, the {{researched}} {{literature was}} synthesized {{based on the}} qualitative content analysis and the aforementioned research questions. The content analysis is {{used to analyze the}} literature and to identify the occurrences of specified information systematically. Within the qualitative content analysis, the information is extracted, formatted and evaluated to answer the research questions. The most important aspect is the extraction of the information to gain the required information. <b>Extraction</b> <b>means</b> to read the text, separating the text in different parts, and to decide which of the given text parts contain information that is relevant for the researcher. The relevant information is then assigned to previously defined categories [92]. In the qualitative content analysis, this assignment is called coding and is induced by a coding unit. The coding unit is a text passage, which is connected to a certain category or content. The assignment to the defined categories is performed by the researcher, called coder [152]. However, the researcher and his understanding and interpretation influence the extraction and text interpretation [92, 152]. When the coding process is done by more than one researcher, a common understanding and interpretation is required [152]. The defined categories are connected with the research questions and are not fixed. During the extraction process, they can be altered and new categories can also be added. As a result of the extraction, a vast amount of data are collected which can be used for information formatting and evaluation to answer the research questions. For information evaluation, the researcher compares the different literature sources to identify communalities and differences [92].|$|E
40|$|Image {{classification}} {{has earned}} enormous attention {{due to the}} advent of modem day applications involving image base information and now an extensive research {{has been carried out}} in this field. Developing computationally efficient algorithms for image classification without compromising the classification accuracy is of primary importance. A typical approach to the image classification problem consists of pre-processing. feature extraction. feature selection and classifier design. Pre- processing includes segmentation of an image to separate individual objects from an image, removing of any erroneous noise from an image, image cropping, image normalization and thinning etc. Feature <b>extraction</b> <b>means</b> finding out various attributes as well as characteristics of objects found within an image. These characteristics, usually called features are used to describe an object. Selection of features is probably {{the most important factor in}} achieving high recognition rite. The extracted features must be invariant to the distortions and variations such as translational, rotational and scale of the objects. These selected features from the objects are used as an input to the classifier. Efficient feature selection for image classification is one such component of a classification algorithm that has received considerable attention. In this dissertation, we propose to adopt a framework of sparse representations to address the problem of feature selection for image classification. A new cost function for sparse feature selection is constructed using two major components namely: sparsity and Fisher's discrimination power. Three algorithms are proposed for the solution of sparse representation problem. The algorithm I is inspired by Orthogonal Matching Pursuit in which we choose the best basis function in each iteration from the overcomplete dictionary that maximized the cost function. Algorithm 2 is the generalized form of algorithm I which gives the global solution of the problem. The solution of proposed cost function is presented in algorithm 3. The performance of the algorithms are quantified both in terms of the Fisher's discrimination power and classification accuracy for different number of selected features. LibSVM classifier is used for classification. Efficiency and robustness of the developed algorithm is demonstrated through experiments with COIL- 20 damsel with different noise and occlusion levels. A comparison between the cost function with and without reconstruction error is also carried out to illustrate the importance of discrimination power with respect to reconstruction error in classification applications. In recent years, filter bank approaches including wavelets and overcomplete multiresolution dictionaries have been used extensively in image classification. The key issue in filter bank based classification methods is the selection of the best and the moist compact representation out of the feature set generated by the overcomplete dictionary. In feature selection for classification, two important factors are separability between the different classes and the compactness or sparsity of the selected feature set. The formulation of the proposed sparse representation for image classification method is further improved by using the separability for the measure of discrimination. In this dissertation, we propose a new approach for feature selection in the framework of sparse representations by combining the separability and sparseness into a single cost function. Orthogonal Matching Pursuit algorithm is employed to extract a sparse set of discriminative features and LibSVM classifier is used for the consequent classification. Efficiency and robustness of the developed algorithm is demonstrated through experiments with different image databases and different noise and occlusion levels. The proposed algorithm is compared Shih Fu Chang algorithm A comparison between the proposed cost function and a conventional cost function that only considers the energy of the selected features is also presented to illustrate the improvement in performance. The proposed cost function is offered high accuracy with a small number of features...|$|E
40|$|The error {{correcting}} {{capabilities of}} the Calderbank-Shor-Steane [[7, 1, 3]] quantum code, together with a fault-tolerant syndrome <b>extraction</b> by <b>means</b> of several ancilla states, have been numerically studied. A simple probability expression to characterize the code ability for correcting an encoded qubit has been considered. This probability, as a correction quality criterion, permits the error correction capabilities among different recovery schemes to be compared. The memory error threshold is calculated {{by means of the}} best method of those considered...|$|R
40|$|We {{investigate}} {{the possibility of}} web information discovery and <b>extraction</b> by <b>means</b> of a modular architecture analysing separately the multiple forms of information presentation, such as free text, structured text, URLs and hyperlinks, by independent knowledge-based modules. First experiments in discovering a relatively easy target, general company descriptions, suggests that web information can be e#ciently retrieved in this way. Thanks to the separation of data types, individual knowledge bases can be much simpler than those used in information extraction over unified representations...|$|R
40|$|We {{introduce}} a debiasing scheme that solves the more-noise-than-entropy problem which {{can occur in}} Helper Data Systems when the source is very biased. We perform a condensing step, similar to Index Based Syndrome coding, that reduces {{the size of the}} source space {{in such a way that}} some source entropy is lost while the noise entropy is greatly reduced. In addition, our method allows for even more entropy <b>extraction</b> by <b>means</b> of a `spamming' technique. Our method outperforms solutions based on the one-pass von Neumann algorithm...|$|R
