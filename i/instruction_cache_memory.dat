7|3434|Public
5000|$|The {{earliest}} academic {{publication of}} trace cache was [...] "Trace Cache: a Low Latency Approach to High Bandwidth Instruction Fetching". [...] This widely acknowledged paper was presented by Eric Rotenberg, Steve Bennett, and Jim Smith at 1996 MICRO conference. An earlier publication is (US patent 5381533), by Alex Peleg and Uri Weiser of Intel Corp., [...] "Dynamic flow <b>instruction</b> <b>cache</b> <b>memory</b> organized around trace segments independent of virtual address line", {{a continuation of}} an application filed in 1992, later abandoned.|$|E
50|$|To further {{conserve}} encoding space, most registers {{are expressed}} in opcodes using three or four bits, the latter via an opcode prefix in 64-bit mode, while at most one operand to an instruction can be a memory location. However, this memory operand {{may also be the}} destination (or a combined source and destination), while the other operand, the source, can be either register or immediate. Among other factors, this contributes to a code size that rivals eight-bit machines and enables efficient use of <b>instruction</b> <b>cache</b> <b>memory.</b> The relatively small number of general registers (also inherited from its 8-bit ancestors) has made register-relative addressing (using small immediate offsets) an important method of accessing operands, especially on the stack. Much work has therefore been invested in making such accesses as fast as register accesses, i.e. a one cycle instruction throughput, in most circumstances where the accessed data is available in the top-level cache.|$|E
40|$|The speed {{disparity}} between processor and memory subsystems has been bridged in many existing large-scale scientific computers and microprocessors {{with the help}} of instruction buffers or instruction caches. In this paper we classify these buffers into traditional instruction buffers, conventional instruction caches and prefetch queues, detail their prominent features, and evaluate the performance of buffers in several existing systems, using trace driven simulation. We compare these schemes with a recently proposed queue-based <b>instruction</b> <b>cache</b> <b>memory.</b> An implementation independent performance metric is proposed for the various organizations and used for the evaluations. We analyze the simulation results and discuss the effect of various parameters such as prefetch threshold, bus width and buffer size on performance...|$|E
40|$|The paper {{presents}} a new unifying formalism introduced to effectively support the automatic generation of assembly test programs {{to be used}} as SBST (Software Based Self-Testing) for both data and <b>instruction</b> <b>cache</b> <b>memories.</b> In particular, the new formalism allows the description of the target memory, of the selected March Test algorithm, and the way this has to be customize to adapt it to the selected cach...|$|R
40|$|This work takes a {{fresh look}} at the {{simulation}} of <b>cache</b> <b>memories.</b> It introduces the technique of static cache simulation that statically predicts a large portion of cache references. To efficiently utilize this technique, a method to perform efficient on-the-fly analysis of programs in general is developed and proved correct. This method is combined with static cache simulation for a number of applications. The application of fast <b>instruction</b> <b>cache</b> analysis provides a new framework to evaluate <b>instruction</b> <b>cache</b> <b>memories</b> that outperforms even the fastest techniques published. Static cache simulation is shown {{to address the issue of}} predicting cache behavior, contrary to the belief that <b>cache</b> <b>memories</b> introduce unpredictability to real-time systems that cannot be efficiently analyzed. Static <b>cache</b> simulation for <b>instruction</b> <b>caches</b> provides a large degree of predictability for real-time systems. In addition, an architectural modification through bit-encoding is introduced that provides fu [...] ...|$|R
40|$|Abstract. Energy {{consumption}} {{is a fundamental}} barrier in taking full advantage of today and future semiconductor manufacturing technologies. This paper presents our recent research activities and results on estimating and reducing energy consumption in nanometer technology system LSIs. This includes techniques and tools for (i) estimating instantaneous energy consumption of embedded processors during an application execution, and (ii) reducing leakage energy in <b>instruction</b> <b>cache</b> <b>memories</b> {{by taking advantage of}} value-dependence of SRAM leakage due to within-die Vth variation. Key words: energy estimation, energy characterization, nanometer technology...|$|R
40|$|Safety-critical {{electronics}} components require thermal {{and electrical}} stress phases {{at the end}} of manufacturing test to screen weak devices. It is possible to optimize the stress induced during the screening phase of Burn-In by running in parallel different types of stress procedures. In previous works, stress procedures of CPU, RAM memory and FLASH memory have been interleaved using DMA and leveraging on <b>instruction</b> <b>CACHE</b> <b>memory.</b> This paper presents a novel approach for optimizing stress procedures at CPU level using an Evolutionary Algorithm. The evolutionary-based framework improves the stress of the CPU procedure when it runs in presence of a parallel stress schema. The manuscript also reports the results gathered by exploiting the evolutionary strategy in a device used in common automotive systems...|$|E
40|$|By {{connecting}} many simple general-purpose RISC CPUs with a Network-on-Chip memory system, the Epiphany co-processor architecture provides promising power-efficiency. This thesis presents ParallErlang, {{a modified}} Erlang Runtime System, capable of running some actors on the Epiphany co-processor. The {{complete lack of}} caches is dealt with by introducing an Epiphany backend to the HiPE Erlang compiler, and a software implementation of an <b>instruction</b> <b>cache.</b> <b>Memory</b> system inconsistency is dealt with by constructing a sequence of instructions with fence semantics, and having HiPE inline this "fence" where required. Finally, performance and power-efficiency is measured and evaluated, and while no benchmark show any improvement over an ARM Coretex-A 9 CPU, benchmarks also indicate that should overheads be possible to eliminate, an improvement of over two orders of magnitude could be possible, bringing power-efficiency superior to the ARM...|$|E
40|$|Duplex {{architectures}} naturally provide high safety because {{errors are}} easily detected {{by comparing the}} behavior of the two subsystems of the duplex. But, when an error occurs, it is usually long and difficult to know which subsystem fails; then it is difficult to intend a graceful degradation and to ensure the system availability. The improved duplex architecture proposed uses a powerful online test technique, control flow checking through signature analysis, which allows fast diagnostics and reconfiguration. Signature analysis has been already studied by many authors for CISC processors. The authors extend it to the RISC processors and particularly to the Motorola MC 88100 because this architecture has been designed for the high workload needed by onboard equipment. According to many particular features of the RISC machines (instruction pipelining and delayed branching), this extension is very complex. In order to keep a fair chip number on the computer board, the authors also studied an original implementation of the signature analyzer in the processor <b>instruction</b> <b>cache</b> <b>memory...</b>|$|E
40|$|High-Performance Computing : 6 th International Symposium, ISHPC 2005, Nara, Japan, September 7 - 9, 2005, First International Workshop on Advanced Low Power Systems, ALPS 2006, Revised Selected PapersEnergy {{consumption}} {{is a fundamental}} barrier in taking full advantage of today and future semiconductor manufacturing technologies. This paper presents our recent research activities and results on estimating and reducing energy consumption in nanometer technology system LSIs. This includes techniques and tools for (i) estimating instantaneous energy consumption of embedded processors during an application execution, and (ii) reducing leakage energy in <b>instruction</b> <b>cache</b> <b>memories</b> {{by taking advantage of}} value-dependence of SRAM leakage due to within-die V th variation...|$|R
50|$|One issue {{brought up}} in the {{research}} was that it takes about 1.88 stack-machine instructions to do the work of a register machine's RISC instruction.Competitive out-of-order stack machines therefore require about twice as many electronic resources to track instructions ("issue stations"). This might be compensated by savings in <b>instruction</b> <b>cache</b> and <b>memory</b> and <b>instruction</b> decoding circuits.|$|R
40|$|International SoC Design Conference : October 15 - 16 : KoreaEnergy {{consumption}} {{is a fundamental}} barrier in taking full advantage of today and future semiconductor manufacturing technologies. We present our recent research activities and results on estimating and reducing dynamic and static energy under realtime constraints in embedded systems. This includes techniques and tools for (i) estimating instantaneous energy consumption of embedded processors during an application execution, (ii) reducing energy consumption by optimally mapping functions and data items to the scratch-pad memory (SPM), the cacheable, and noncacheable memory regions of the processor memory space, (iii) reducing the energy consumption of SPM by partitioning it into two sections with different dynamic vs. static power dissipations, (iv) reducing leakage energy in <b>instruction</b> <b>cache</b> <b>memories</b> {{by taking advantage of}} value-dependence of SRAM leakage due to within-die Vth variation, (v) choosing higher threshold voltage and compensating the delay-violating cache-lines by additional cache ways, and (vi) reducing energy of the logic-part of processor cores by statically implementing multiple same-ISA cores with different energy and performance characteristics...|$|R
40|$|This thesis {{presents}} methodologies {{for improving}} system performance and energy consumption by optimizing the memory hierarchy performance. The processor-memory performance gap {{is a well-known}} problem that is predicted to get worse, as the performance gap between processor and memory is widening. The author describes a method to estimate the best L 1 cache configuration for a given application. In addition, three methods are presented to improve the performance and reduce energy in embedded systems by optimizing the instruction memory. Performance estimation is an important procedure to assess {{the performance of the}} system and to assess the effectiveness of any applied optimizations. A cache memory performance estimation methodology is presented in this thesis. The methodology is designed to quickly and accurately estimate the performance of multiple cache memory configurations. Experimental results showed that the methodology is on average 45 times faster compared to a widely used tool (Dinero IV). The first optimization method is a software-only method, called code placement, was implemented to improve the performance of <b>instruction</b> <b>cache</b> <b>memory.</b> The method involves careful placement of code within memory to ensure high cache hit rate when code is brought into the cache memory. Code placement methodology aims to improve cache hit rates to improve cache memory performance. Experimental results show that by applying the code placement method, a reduction in cache miss rate by up to 71 %, and energy consumption reduction of up to 63 % are observed when compared to application without code placement. The second method involves a novel architecture for utilizing scratchpad memory. The scratchpad memory is designed as a replacement of the <b>instruction</b> <b>cache</b> <b>memory.</b> Hardware modification was designed to allow data to be written into the scratchpad memory during program execution, allowing dynamic control of the scratchpad memory content. Scratchpad memory has a faster memory access time and a lower energy consumption per access compared to cache memory; the usage of scratchpad memory aims to improve performance and lower energy consumption of systems compared to system with cache memory. Experimental results show an average energy reduction of 26. 59 % and an average performance improvement of 25. 63 % when compared to a system with cache memory. The third is an application profiling method using statistical information to identify applications hot-spots. Application profiling is important for identifying section in the application where performance degradation might occur and/or where maximum performance gain can be obtained through optimization. The method was applied and tested on the scratchpad based system described in this thesis. Experimental results show the effectiveness of the analysis method in reducing energy and improving performance when compared to previous method for utilizing the scratchpad memory based system (average performance improvement of 23. 6 % and average energy reduction of 27. 1 % are observed) ...|$|E
40|$|In {{this report}} we compare {{the cost and}} {{performance}} {{of a new kind}} of restricted <b>instruction</b> <b>cache</b> architecture [...] the stall cache [...] against several other conventional cache architectures. The stall cache minimizes the size of an on-chip <b>instruction</b> <b>cache</b> by caching only those instructions whose instruction fetch phase collides with the memory access phase of a preceding load or store instruction. Many existing machines provide a single cycle external <b>cache</b> <b>memory</b> [6, 17, 2]. Our results show that, under this assumption, the stall cache always outperforms an equivalent sized on-chip <b>instruction</b> <b>cache,</b> reducing external <b>memory</b> access stalls by approximately 10 %. In addition we present results for a system using an onchip data cache, and for one with a double width data bus and short instruction prefetch buffer...|$|R
40|$|For {{microprocessors}} {{that attempt}} to exploit instruction level parallelism, {{it is necessary to}} have a large window of candidate instructions from which to issue from. With loop prediction, we can predict the number of times that a loop will iterate, as well as the paths that will be followed inside the loop body. With this type of prediction, several basic blocks can be prefetched and stored in a dedicated loop bu#er, reducing the number of <b>instruction</b> <b>cache</b> and <b>memory</b> requests, while providing a large window of instructions for speculative execution...|$|R
30|$|Filling {{the cache}} on call and return only removes {{another source of}} interference: there is no {{competition}} for the main <b>memory</b> access between <b>instruction</b> <b>cache</b> and data cache. In traditional architectures, there is a subtle dependency between the <b>instruction</b> <b>cache</b> and <b>memory</b> access for a load or store instruction. For example, a load or store {{at the end of}} the processor pipeline competes with an instruction fetch that results in a cache miss. One of the two instructions is stalled for additional cycles by the other instruction. With a data cache, this situation can be even worse. The worst-case scenario for the memory stall time for an instruction fetch or a data load is two miss-penalties when both cache reads are a miss.|$|R
30|$|LEON 3 [7] is a 32 -bit synthesizable soft-processor that is {{compatible}} with SPARC V 8 architecture: it has a seven-stage pipeline and Harvard architecture. It uses separate <b>instruction</b> and data <b>cache</b> <b>memories</b> and supports multiprocessor configurations: in particular, an SMP-aware configuration is well supported thanks to available memory management unit and snooping unit for cache coherence. It represents a soft-processor for aerospace applications. LEON 3 is described {{by means of an}} open-source VHDL model and provides full configurability by means of the Gaisler Research IP Library (GRLIB).|$|R
5000|$|The 68010, {{released}} in 1982, has a [...] "loop mode" [...] {{which can be}} considered a tiny and special-case <b>instruction</b> <b>cache</b> that accelerates loops that consist of only two instructions. The 68020, {{released in}} 1984, replaced that with a typical <b>instruction</b> <b>cache</b> of 256 bytes, being the first 68k series processor to feature true on-chip <b>cache</b> <b>memory.</b>|$|R
40|$|Introduction 1. 1 Issues about Computer Hardware and Software With {{dramatic}} changes in technology ahead, how do we approach the problem of high-performance architecture design and high-performance engineering and scientific computing? For example, the new technology makes feasible massive parallelism. How much additional effort should be invested in increasing the performance on a single processor before we seek higher levels of performance on multiple processors? There are no simple answer to these questions. The latter is always essential to the first. We need a combination of solutions, and what we choose almost certainly will be application dependent, since at this stage we have not constructed a general machine that would be equally effective for all high performance applications yet. In the past, we have seen many different techniques used in hardware to improve performance, such things as <b>instruction</b> buffers, <b>cache</b> <b>memories,</b> pipelined execution and RISC compute...|$|R
40|$|<b>Cache</b> <b>memories</b> {{are crucial}} to obtain high {{performance}} on contemporary computing systems. However, sometimes they have been avoided in real-time systems due to their lack of determinism. Unfortunately, most of the published techniques to attain predictability when using <b>cache</b> <b>memories</b> are complex to apply, precluding their use on real applications. This paper proposes a memory hierarchy such that, when combined with a careful pre-existing selection of the <b>instruction</b> <b>cache</b> contents, it brings {{an easy way to}} obtain predictable yet high-performance results. The purpose is to make possible the use of <b>instruction</b> <b>caches</b> in realistic realtime systems, with the ease of use in mind. The hierarchy is founded on a conventional <b>instruction</b> <b>cache</b> based scheme plus a simple memory assist, whose operation offers a very predictable behaviour and good performance thanks to the addition of a dedicated locking state memory. ...|$|R
40|$|Power {{consumption}} {{is currently a}} critical design constraint in embedded applications, and application software can have a substantial impact on it. 1 Data-intensive computation applications typically employ digital signal processors (DSPs) to improve the application’s support for real-time processing. DSPs are characterized by complex architectures: a deep pipeline, very long instruction word (VLIW) <b>instructions,</b> <b>memory</b> <b>caches,</b> and sometimes, superscalar architectures. This complexity {{makes it difficult to}} develop suitable modeling methodologies for analyzing power consumption in these architectures. At present, it’s possible to conduct powe...|$|R
40|$|Embedded {{microprocessor}} <b>cache</b> <b>memories</b> {{suffer from}} limited observability and controllability creating problems during in-system tests. This paper presents a procedure to transform traditional march tests into software-based self-test programs for set-associative <b>cache</b> <b>memories</b> with LRU replacement. Among {{all the different}} cache blocks in a microprocessor, testing <b>instruction</b> <b>caches</b> represents a major challenge due to limitations in two areas: 1) test patterns which must be composed of valid instruction opcodes and 2) test result observability: the results can only be observed through the results of executed instructions. For these reasons, the proposed methodology will concentrate on the implementation of test programs for <b>instruction</b> <b>caches.</b> The main contribution of this work lies {{in the possibility of}} applying state-of-the-art memory test algorithms to embedded <b>cache</b> <b>memories</b> without introducing any hardware or performance overheads and guaranteeing the detection of typical faults arising in nanometer CMOS technologie...|$|R
40|$|We present Raexplore, a {{performance}} modeling framework for architecture exploration. Raexplore enables rapid, auto-mated, and systematic search of architecture design space by combining hardware counter-based performance characteri-zation and analytical performance modeling. We demonstrate Raexplore for two recent manycore processors IBM Blue-Gene/Q compute chip and Intel Xeon Phi, targeting {{a set of}} scientific applications. Our framework is able to capture com-plex interactions between architectural components including <b>instruction</b> pipeline, <b>cache,</b> and <b>memory,</b> and to achieve a 3 – 22 % error for same-architecture and cross-architecture per-formance predictions. Furthermore, we apply our framework to assess the two processors, and discover and evaluate a list of architectural scaling options for future processor designs. 1...|$|R
50|$|Support for Harvard cache, i.e. split {{data and}} <b>instruction</b> <b>caches,</b> {{as well as}} support for unified <b>caches.</b> <b>Memory</b> {{operations}} are strictly load/store, but allow for out-of-order execution. Support for both big and little-endian addressing with separate categories for moded and per-page endianness. Support for both 32-bit and 64-bit addressing.|$|R
40|$|<b>Cache</b> <b>memories</b> can {{contribute}} to significant performance advantages due to the gap between CPU and memory speed. They have traditionally been thought of as contributors to unpredictability because the user {{can not be sure}} of exactly how much time will elapse while a memory-operation is performed. In a real-time system, the <b>cache</b> <b>memory</b> may contribute to a missed deadline by actually making the system slower, but this is rare. To avoid this problem, the developers of real-time systems have run the program in the old-fashioned way; with disabled cache -just to be safe. Turning the cache off, however, will also make other features like instruction pipelining less beneficial so the new processors will not give the performance speedup as they were meant to give. The first methods to determine the boundaries of the execution time in computer systems with <b>cache</b> <b>memories</b> were presented in the late eighties - twenty years after the first <b>cache</b> <b>memories</b> were designed. Today, fifteen years later, further methods have been developed to determine the execution time with <b>cache</b> <b>memories</b> [...] . that were state-of-the-art fifteen years ago. This thesis presents a method of generating worst-case execution time scenarios and measure the execution time during those. Several important properties can be measured. These include cache-related pre-emption delay, miss-ratio levels of software, and <b>instruction</b> <b>cache</b> miss-ratio threshold levels for increased system performance. Besides the dynamic measurement method, a statical procedure to determine the maximum <b>instruction</b> <b>cache</b> miss-ratio level is presented. Experimental results from this research show that the indirect cache cost of a pre-emption is very high - {{more than three times the}} execution cost of the context-switch functions themselves. Another result shows that the tested computer system without caching will not cause a missed deadline if the <b>instruction</b> <b>cache</b> is enabled...|$|R
40|$|This {{report is}} {{dedicated}} to the processor characterization method and software cost esti-mation technique used in the Polis Codesign tool environment. The processor characteri-zation method has been exercised by applying it to the ARM processor family. In particu-lar, two processors, ARM 7 TDMI and ARM 920 T, have been examined. An improved method is proposed, which is supported and partially automated by two utility tools. The improved method is based on an iterative two-pass technique. The first pass involves proc-essor characteristic extraction from generic software templates. The second pass improves the parameters from the first pass using a validation method. The results obtained during the exercise are presented and discussed. In particular the effect of <b>instruction</b> and data <b>cache</b> <b>memory</b> is addressed. The estimation technique used today is well suited for proces-sors without cache, but processors with cache calls for new techniques...|$|R
30|$|Dynamic features, which model a large {{execution}} history, are {{problematic for}} WCET analysis. Especially interferences between different features {{result in a}} state space explosion for the analysis. The proposed architecture is an in-order pipeline with minimized <b>instruction</b> dependencies. The <b>cache</b> <b>memory</b> consists of a method cache containing whole methods and a data cache that is split for stack-allocated data and heap-allocated data. The pipeline can be extended to a dual-issue pipeline when the instructions are compiler scheduled. For further performance enhancements, we propose a CMP system with time-sliced arbitration of the main memory access. Running each task on its own core in a CMP system eliminates scheduling, and the related cache thrashing, from the analysis. The schedule of the memory access becomes an input for WCET analysis. With nonuniform time slices, the arbiter schedule {{can be adapted to}} balance the utilization of the individual cores.|$|R
40|$|It {{has been}} {{claimed that the}} {{execution}} time of a program can often be predicted more accurately on an uncached system than on a system with <b>cache</b> <b>memory</b> [5, 20]. Thus, caches are often disabled for critical real-time tasks to ensure the predictability required for scheduling analysis. This work shows that instruction caching can be exploited to gain execution speed without sacrificing predictability. A new method called Static Cache Simulation is introduced which uses control-flow {{information provided by the}} back-end of a compiler. This simulator statically predicts the caching behavior of {{a large portion of the}} <b>instruction</b> <b>cache</b> references of a program. In addition, a fetch-frommemory bit is added to the instruction encoding which indicates whether an instruction shall be fetched from the <b>instruction</b> <b>cache</b> or from main memory. This bitencoding approach provides a significant speedup in execution time (factor 3 - 8) over systems with a disabled <b>instruction</b> <b>cache</b> without any sacrifice in [...] ...|$|R
40|$|Great Lakes Symposium on VLSI (GLSVLSI) 2008 : May 4 - 6, 2008 : Orlando, FloridaShare of leakage in <b>cache</b> <b>memories</b> is {{increasing}} with technology scaling. Studies {{show that most}} stored bits in <b>instruction</b> <b>caches</b> are zero, and hence, asymmetric SRAM cells which dissipate less leakage when storing 0, effectively reduce leakage with negligible performance penalty. We show that by carefully choosing register operands of instructions, {{it is possible to}} further increase the number of 0 bits, and hence, increase leakage savings in <b>instruction</b> <b>cache.</b> This compiler technique is performed off-line and introduces absolutely no delay penalty since processor registers are all the same. Experimental results of our benchmarks show up to 33 % (averaging 30. 35 %) improvement in leakage...|$|R
40|$|Abstract—Soft errors {{induced by}} {{energetic}} particle strikes in on-chip <b>cache</b> <b>memories</b> {{have become an}} increasing challenge in designing new generation reliable microprocessors. Previous efforts have exploited information redundancy via parity/ECC codings or cacheline duplication for information integrity in on-chip <b>cache</b> <b>memories.</b> Due to various performance, area/size, and energy constraints in various target systems, many existing unoptimized protection schemes may eventually prove significantly inadequate and ineffective. In this paper, we propose a new framework for conducting comprehensive studies and characterization on the reliability behavior of <b>cache</b> <b>memories,</b> {{in order to provide}} insight into cache vulnerability to soft errors as well as design guidance to architects for highly efficient reliable on-chip <b>cache</b> <b>memory</b> design. Our work is based on the development of new lifetime models for data and tag arrays residing in both the data and <b>instruction</b> <b>caches.</b> Those models facilitate the characterization of cache vulnerability of stored items at various lifetime phases. We then exemplify this design methodology by proposing reliability schemes targeting at specific vulnerable phases. Benchmarking is carried out to showcase the effectiveness of our approach. Index Terms—Cache, reliability, soft error, temporal vulnerability factor. Ç...|$|R
40|$|The use of caches poses a {{difficult}} tradeoff for architects of real-time systems. While caches provide significant performance advantages, {{they have also}} been viewed as inherently unpredictable since {{the behavior of a}} cache reference depends upon the history of the previous references. The use of caches will only be suitable for realtime systems if a reasonably tight bound on the performance of programs using <b>cache</b> <b>memory</b> can be predicted. This paper describes an approach for bounding the worstcase <b>instruction</b> <b>cache</b> performance of large code segments. First, a new method called Static Cache Simulation is used to analyze a program’s control flow to statically categorize the caching behavior of each instruction. A timing analyzer, which uses the categorization information, then estimates the worst-case <b>instruction</b> <b>cache</b> performance for each loop and function in the program. 1...|$|R
40|$|Abstract — Traditional level-one <b>instruction</b> <b>caches</b> {{and data}} caches for {{embedded}} systems typically {{have the same}} capacities. Configurable caches either shut down {{a part of the}} cache to suit applications needing a small cache or employ a large cache and high associativity for applications needing to reduce miss rate and energy. However, increasing associativity is energy-costly compared with increasing capacity. We have extended the traditional configurable cache and made the whole on-chip <b>cache</b> <b>memory</b> capacity available to both <b>instruction</b> and data <b>caches.</b> The capacity can then be co-allocated between the data and the <b>instruction</b> <b>caches.</b> Compared with way shutdown and way concatenation, the capacity co-allocation cache provides a better solution than increasing associativity. Four out of 17 benchmarks from Mibench benefit from the capacity coallocation cache. Energy reduction can be up to 24 %, with an average of 5 %, compared to a traditional configurable cache. ...|$|R
40|$|<b>Cache</b> <b>memory</b> {{performance}} {{is very important}} in the overall performance of modern CPUs. One of the many techniques used to improve it is the split of on-chip <b>cache</b> <b>memory</b> in two separate <b>Instruction</b> and Data <b>caches.</b> The current CPU organizations usually have per core separate L 1 caches and unified L 2 caches. This paper presents the results of simulating different CPU organizations with unified and separate L 2 <b>Instruction</b> and Data <b>caches</b> using Marss-x 86, a Cycle-Accurate full system simulator. The results indicate that separating the L 2 <b>cache</b> <b>memory</b> provides higher overall CPU IPC. The highest improvement is 3 % and is achieved in a quad-core CPU model with shared L 3 cache. Analyzing the hardware costs and complications of separating L 2 cache might be an interesting future work direction...|$|R
40|$|Multi-core chips {{have been}} {{increasingly}} {{adopted by the}} mi-croprocessor industry. For real-time systems to exploit multi-core architectures, it is required to obtain both tight and safe estimates of worst-case execution times (WCETs). Estimating WCETs for multi-core platforms is very challenging because of the possible interferences between cores due to shared hard-ware resources such as shared <b>caches,</b> <b>memory</b> bus, etc. This paper proposes a compile-time approach to reduce shared <b>instruction</b> <b>cache</b> interferences between cores to tighten WCET estimations. Unlike [28], which accounts for all possi-ble conflicts caused by tasks running on the other cores when estimating the WCET of a task, our approach drastically re-duces the amount of inter-core interferences. This is done by controlling {{the contents of the}} shared <b>instruction</b> <b>cache(s),</b> by caching only blocks statically known as reused. Experimental results demonstrate the practicality of our approach. ...|$|R
5000|$|The 68020 added many {{improvements}} {{over the}} 68010 including a 32-bit {{arithmetic logic unit}} (ALU), 32-bit external data and address buses, extra instructions and additional addressing modes. The 68020 (and 68030) had a proper three-stage pipeline. Though the 68010 had a [...] "loop mode", which sped loops through what was effectively a tiny <b>instruction</b> <b>cache,</b> it held only two short instructions and was thus little used. The 68020 replaced this with a proper <b>instruction</b> <b>cache</b> of 256 bytes, the first 68k series processor to feature true on-chip <b>cache</b> <b>memory.</b> The previous 68000 and 68010 processors could only access word (16-bit) and long word (32-bit) data in memory if it were word-aligned (located at an even address). The 68020 had no alignment restrictions on data access. Naturally, unaligned accesses were slower than aligned accesses because they required an extra memory access.|$|R
40|$|<b>Cache</b> <b>memories</b> {{have been}} widely used in order {{to bridge the gap between}} high speed {{processors}} and relatively slow main memories. However they are a source of predictability problems. A lot of progress has been achieved to model caches, in order to determine safe and precise bounds on (i) tasks' WCETs in the presence of caches; (ii) cacherelated preemption delays. An alternative approach to cope with <b>cache</b> <b>memories</b> in real-time systems is to lock their contents so as to make memory access times and cacherelated preemption delays entirely predictable. In this paper, we focus on <b>instruction</b> <b>caches</b> and we describe the state of the art in the so-called static cache locking technique and its related algorithms. Then benefits and problems with this approach are discussed. Eventually refinements and an enhanced technique, dynamic cache locking, are sketched...|$|R
