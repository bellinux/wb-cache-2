4|4075|Public
25|$|A bank is {{required}} to compare the total expected losses with the total eligible provisions. If the <b>expected</b> <b>loss</b> <b>amount</b> {{is less than the}} provisions, the supervisor must consider if this is a true picture of reality, and then include the difference in Tier II capital. The expected losses for equity exposures under the PD/LGD approach is deducted 50% from Tier I and 50% from Tier II capital.|$|E
40|$|Conditional Value-at-Risk (CVaR) {{measures}} the <b>expected</b> <b>loss</b> <b>amount</b> beyond VaR. It has vast advantage over VaR {{because of its}} property of coherence. This paper gives an analytical solution in a complete market setting to the risk reward problem faced by a portfolio manager whose portfolio needs to be continuously rebalanced to minimize risk taken (measured by CVaR) while meeting the reward goal (measured by expected return). The optimal portfolio is identified whenever it exists, and the associated minimal risk is calculated. An example in the Black-Scholes framework is cited where dynamic hedging strategy is calculated and the efficient frontier is plotted. ...|$|E
40|$|There are {{two things}} that may be {{responsible}} for differences between the <b>expected</b> <b>loss</b> <b>amount</b> (for a contract or an entire portfolio) and the actual loss amount that is experienced: errors in estimating the long term average (parameter error) and random good or bad luck (process risk). This paper presents a method for using historical data to establish a model for process risk. Because the method does not require individual claim data, it is especially suitable for reinsurance companies for whom individual claim data may not be available. It can also be used when data is obtained from the aggregate policy year and accident year calls that are filed with rating bureaus. Essentially, the method treats the experience of multiple contract years as if each year were a random sample drawn from a single population consisting of all the outcomes that could have occurred, The techniques of Time Series Decomposition are used to restate the historical data on an "'as if current levels " basis. Decomposition is then used to isolate the random fluctuations (process variance). A generalization of the Central Limit Theorem allows a model of these fluctuations to be constructed. While derived from aggregate portfolio experience, the model's divisibility property allows it to be scaled down to accurately reflect the aggregate loss distribution of an individual contract or policy...|$|E
40|$|This thesis {{develops}} {{an alternative}} approach to modelling the <b>expected</b> <b>loss</b> cost of an insurance portfolio that allows for dependence between the frequency and severity components of the aggregate claims process. The traditionally used independent aggregate claims model is extended to define a dependent model, thus allowing for a correlation between the claim counts and claim amounts. A Generalized Linear Model framework is developed for the aggregate claims model in the dependent setting using a conditional severity model and marginal frequency model. We find that the pure premium in the dependent aggregate claims model {{is the product of}} a marginal mean frequency, a modified marginal mean severity and a correction term. This dependent modelling approach is then compared with the independent aggregate claims model GLM structure. It is shown that the <b>expected</b> total <b>loss</b> <b>amount</b> derived in the independent model is in fact a special case of the dependent model...|$|R
40|$|We {{study the}} {{asymptotic}} {{behavior of the}} difference between the values at risk VaR(L) and VaR(L+S) for heavy tailed random variables L and S for application in sensitivity analysis of quantitative operational risk management {{within the framework of the}} advanced measurement approach of Basel II (and III). Here L describes the <b>loss</b> <b>amount</b> of the present risk profile and S describes the <b>loss</b> <b>amount</b> caused by an additional loss factor. We obtain different types of results according to the relative magnitudes of the thicknesses of the tails of L and S. In particular, if the tail of S is sufficiently thinner than the tail of L, then the difference between prior and posterior risk amounts VaR(L+S) - VaR(L) is asymptotically equivalent to the expectation (<b>expected</b> <b>loss)</b> of S. ...|$|R
40|$|The study investigates {{protective}} {{responses in}} low probability and high loss risk situations. Particularly, it (1) detects individual protection valuations to variations in probability versus to variations in loss for payment decisions and choice decisions, (2) elicits the threshold probability in individuals’ minds {{that make them}} consider having protective measure, (3) calculates relative risk aversion. The results of the experiment indicate that as the probability of <b>loss</b> and <b>loss</b> <b>amount</b> increases, individuals tend to buy/pay more for protection. They are more responsive to the variation in probabilities than to the variation in <b>loss</b> <b>amounts</b> when they decide whether to buy the protective measure or not: choice decision. Yet, {{the opposite is true}} when they decide the amount of willingness to pay for buying the protective measure: payment decision. In addition, bid <b>expected</b> <b>loss</b> values have a bimodal distribution. Consistent with previous studies, individuals (particularly women) are found to be risk averse for low probabilities. experiments, risk, insurance...|$|R
40|$|Adaptive {{behaviour}} {{requires an}} ability to obtain rewards by choosing between different risky options. Financial gambles {{can be used to}} study effective decision-making experimentally, and to distinguish processes involved in choice option evaluation from outcome feedback and other contextual factors. Here, we used a paradigm where participants evaluated ‘mixed’ gambles, each presenting a potential gain and a potential loss and an associated variable outcome probability. We recorded neural responses using autonomic monitoring, electroencephalography (EEG) and functional neuroimaging (fMRI), and used a univariate, parametric design to test for correlations with the eleven economic parameters that varied across gambles, including expected value (EV) and amount magnitude. Consistent with behavioural economic theory, participants were risk-averse. Gamble evaluation generated detectable autonomic responses, but only weak correlations with outcome uncertainty were found, suggesting that peripheral autonomic feedback does not {{play a major role in}} this task. Long-latency stimulus-evoked EEG potentials were sensitive to expected gain and expected value, while alpha-band power reflected <b>expected</b> <b>loss</b> and <b>amount</b> magnitude, suggesting parallel representations of distinct economic qualities in cortical activation and central arousal. Neural correlates of expected value representation were localized using fMRI to ventromedial prefrontal cortex, while the processing of other economic parameters was associated with distinct patterns across lateral prefrontal, cingulate, insula and occipital cortices including default-mode network and early visual areas. These multimodal data provide complementary evidence for distributed substrates of choice evaluation across multiple, predominantly cortical, brain systems wherein distinct regions are preferentially attuned to specific economic features. Our findings extend biologically-plausible models of risky decision-making while providing potential biomarkers of economic representations that can be applied to the study of deficits in motivational behaviour in neurological and psychiatric patients...|$|R
40|$|This paper {{discusses}} different {{measures for}} quantifying regional hurricane loss. The main measures {{used in the}} past are normalized percentage loss and dollar value loss. In this research, we show that these measures are useful but may not properly reflect the size of the population influenced by hurricanes. A new loss measure is proposed that reflects the hurricane impact on people occupying the structure. For demonstrating the differences among these metrics, regional loss analysis was conducted for Florida. The regional analysis was composed of three modules: the hazard module stochastically modeled the wind occurrence in the region; the vulnerability module utilized vulnerability functions developed in this research to calculate the loss; and the financial module quantified the hurricane loss. In the financial module, we calculated three loss metrics for certain region. The first metric is the average annual loss (AAL) which represents the <b>expected</b> <b>loss</b> per year in percentage. The second is the average annual dollar loss which represents the <b>expected</b> dollar <b>amount</b> <b>loss</b> per year. The third is the average annual population-weighted loss (AAPL) —a new measure proposed in this research. Compared with the AAL, the AAPL reflects the number of people influenced by the hurricane. The advantages of the AAPL are illustrated using three different analysis examples: (1) conventional regional loss analysis, (2) mitigation potential analysis, and (3) forecasted future loss analysis due to the change in population...|$|R
40|$|The paper {{presents}} algorithms {{for insurance}} technical provisions {{taking into account}} losses, which are incurred but not reported. Evaluation of insurance technical provisions {{for the kinds of}} insurance, such as Motor Third Party Liability (MTPL) Insurance, Property Insurance and some others, have difficulties in assessing the impact of the losses from insurance claims incurred requiring a longer time for the settlement of insurance claims. These insurance requirements are mainly associated with health insurance in the MTPL Insurance, losses related to compensation for moral injuries, as well as on life care and life-long pension. To run these payments, you need to know the financial indicators for the period of settlement of loss (such as the effective interest rate, investment income, etc.) In the article the procedures for the most accurate forecast possible <b>losses</b> for the <b>expected</b> excess of <b>loss</b> <b>amount</b> for a treaty year are provided, using the loss experience of the previous years of the occurrence with their development. However, certain adjustments should be made to take account of the impact of losses from previous years for the current period. This article describes how outstanding losses have to be projected on a year of reporting, so that they are correspond to the current value...|$|R
5000|$|Consider [...] and let q be {{an initial}} guess for [...] The <b>expected</b> <b>loss</b> {{evaluated}} at q isIn {{order to minimize}} the <b>expected</b> <b>loss,</b> we move the value of q {{a little bit to}} see whether the <b>expect</b> <b>loss</b> will rise or fall.Suppose we increase q by 1 unit. Then the change of <b>expected</b> <b>loss</b> would be ...|$|R
5000|$|Suppose that u is {{increased}} by 1 unit. Then the <b>expected</b> <b>loss</b> {{will be changed}} by [...] on changing u to 4. If, [...] u=5, the <b>expected</b> <b>loss</b> isand any change in u will increase the <b>expected</b> <b>loss.</b> Thus u=5 is the median. The Table below shows the <b>expected</b> <b>loss</b> (divided by [...] ) for different values of u.|$|R
40|$|The {{intention}} of a loss provision is {{the anticipation of}} credit's <b>expected</b> <b>losses</b> by adjusting the book values of the credits. Furthermore, this loan loss provision has to {{be compared to the}} <b>expected</b> <b>loss</b> according to Basel II and if necessary, equity has to be adjusted. This however assumes that the loan loss provision and the <b>expected</b> <b>loss</b> are comparable, which is only valid conditionally in current loan loss provisioning methods according to IAS. The provisioning and accounting model developed in this paper overcomes the before mentioned shortcomings and is consistent with an economic rationale of <b>expected</b> <b>losses.</b> We introduce a de¯nition of <b>expected</b> <b>loss</b> referring to the whole maturity of the loan and show that this measure can be reasonably compared with loan loss provisions. Additionally, this model is based on a close-to-market valuation of the loan. Suggestions for changes in current accounting and capital requirement rules are provided. loan <b>loss</b> provision, <b>expected</b> <b>loss,</b> IAS, Basel II...|$|R
5000|$|Having made {{explicit}} the <b>expected</b> <b>loss</b> {{for each}} given [...] separately, we can define a decision rule [...] by specifying for each [...] an action [...] that minimizes the <b>expected</b> <b>loss.</b> This {{is known as}} a generalized Bayes rule with respect to [...] There may be more than one generalized Bayes rule, since there may be multiple choices of [...] that achieve the same <b>expected</b> <b>loss.</b>|$|R
50|$|As {{the number}} of rounds increases, eventually, the <b>expected</b> <b>loss</b> will exceed the {{standard}} deviation, many times over. From the formula, {{we can see the}} standard deviation is proportional to the square root of {{the number of}} rounds played, while the <b>expected</b> <b>loss</b> is proportional to the number of rounds played. As the number of rounds increases, the <b>expected</b> <b>loss</b> increases at a much faster rate. This is why it is impossible for a gambler to win in the long term. It is the high ratio of short-term standard deviation to <b>expected</b> <b>loss</b> that fools gamblers into thinking that they can win.|$|R
50|$|<b>Expected</b> <b>loss</b> is not time-invariant, {{but rather}} {{needs to be}} recalculated when {{circumstances}} change. Sometimes both the probability of default and the loss given default can both rise, giving two reasons that the <b>expected</b> <b>loss</b> increases.|$|R
40|$|A semiparametric {{estimator}} {{based on}} an unknown density isuniformly adaptive if the <b>expected</b> <b>loss</b> of the estimator converges to the asymptotic <b>expected</b> <b>loss</b> of the maximum liklihood estimator based on teh true density (MLE), and if convergence {{does not depend on}} either the parameter values or the form of the unknown density. Without uniform adaptivity, the asymptotic <b>expected</b> <b>loss</b> of the MLE need not approximate the <b>expected</b> <b>loss</b> of a semiparametric estimator for any finite sample I show that a two step semiparametric estimator is uniformly adaptive for the parameters of nonlinear regression models with autoregressive moving average errors. adaptive, ARMA, semiparametric, uniform convergence,...|$|R
50|$|As {{the number}} of rounds increases, eventually, the <b>expected</b> <b>loss</b> will exceed the {{standard}} deviation, many times over. From the formula, {{we can see the}} standard deviation is proportional to the square root of {{the number of}} rounds played, while the <b>expected</b> <b>loss</b> is proportional to the number of rounds played. As the number of rounds increases, the <b>expected</b> <b>loss</b> increases at a much faster rate. This is why it is practically impossible for a gambler to win in the long term (if they don't have an edge). It is the high ratio of short-term standard deviation to <b>expected</b> <b>loss</b> that fools gamblers into thinking that they can win.|$|R
40|$|This {{experimental}} study, first, {{compares the}} individual valuations of two risk reduction mechanisms: self-insurance and self-protection. Second, it investigates these valuations when the <b>loss</b> <b>amount</b> is ambiguous, and compare these values with valuations when <b>loss</b> <b>amounts</b> are known. results confirm that there exists no "framing effect" {{due to the}} two risk reduction mechanisms. Ambiguity in the <b>loss</b> <b>amount</b> has a weak impact on the valuation, and using different representations of ambiguity {{does not change the}} valuation. Moreover, the mean ratios of ambiguous to risky bids are greater than one for low <b>loss</b> <b>amounts</b> indicating ambiguity aversion. These ratios are not significantly different from one for high <b>loss</b> <b>amounts</b> regardless of the probability of loss levels. Finally, 28 percent of the sample behaved consistent with the predictions of "anchoring and adjustment", while only 6 percent supported the "maximin" predictions. self-insurance, self-protection, risk, uncertainty...|$|R
40|$|We {{propose a}} general method for reranker {{construction}} which targets choosing the candidate {{with the least}} <b>expected</b> <b>loss,</b> rather than the most probable candidate. Different approaches to <b>expected</b> <b>loss</b> approximation are considered, including estimating from the probabilistic model used to generate the candidates, estimating from a discriminative model trained to rerank the candidates, and learning to approximate the <b>expected</b> <b>loss.</b> The proposed methods are applied to the parse reranking task, with various baseline models, achieving significant improvemen...|$|R
40|$|Lending is {{associated}} with credit risk. Modelling the loss stochastically, the cost of credit risk is the <b>expected</b> <b>loss.</b> In credit business {{the probability that the}} debtor will default in payments within one year, often is the only reliable quantitative parameter. Modelling the time to default as continuous variable corresponds to an exponential distribution. We calculate the <b>expected</b> <b>loss</b> of a trade with several cash flows, even if the distribution is not exponential. Continuous rating migration data show that the exponential distribution is not adequate in general. The distribution can be calibrated using rating migrations without a parametric model. A practitioner, however, will model time as a discrete variable. We show that the <b>expected</b> <b>loss</b> in the discrete model is a linear approximation of the <b>expected</b> <b>loss</b> in the continuous model and discuss the consequences. Finally, as costs for the <b>expected</b> <b>loss</b> cannot be charged up-front, the credit spread over risk-free interest is derived. Point process, credit valuation, hazard rate, kernel smoothing test...|$|R
5000|$|At first, {{this may}} appear rather {{different}} from the Bayes rule approach of the previous section, not a generalization. However, notice that the Bayes risk already averages over [...] in Bayesian fashion, and the Bayes risk may be recovered as the expectation over [...] of the <b>expected</b> <b>loss</b> (where [...] and [...] ). Roughly speaking, [...] minimizes this expectation of <b>expected</b> <b>loss</b> (i.e., is a Bayes rule) {{if and only if}} it minimizes the <b>expected</b> <b>loss</b> for each [...] separately (i.e., is a generalized Bayes rule).|$|R
50|$|Generally {{considered}} {{a blend of}} the chain-ladder and <b>expected</b> claims <b>loss</b> reserving methods, the Bornhuetter-Ferguson method uses both reported or paid losses {{as well as an}} a priori <b>expected</b> <b>loss</b> ratio to arrive at an ultimate loss estimate. Simply, reported (or paid) losses are added to a priori <b>expected</b> <b>losses</b> multiplied by an estimated percent unreported. The estimated percent unreported (or unpaid) is established by observing historical claims experience.|$|R
30|$|The {{intention}} of a loan loss provision is {{the anticipation of}} the loan’s <b>expected</b> <b>losses</b> by adjusting the book value of the loan. Furthermore, this loan loss provision has to {{be compared to the}} <b>expected</b> <b>loss</b> according to Basel II and, {{in the case of a}} difference, liable equity has to be adjusted. This however assumes that the loan loss provision and the <b>expected</b> <b>loss</b> are based on a similar economic rationale, which is only valid conditionally in current loan loss provisioning methods according to IFRS. Therefore, differences between loan <b>loss</b> provisions and <b>expected</b> <b>losses</b> should only result from different approaches regarding the parameter estimation within each model and not due to different assumptions regarding the outcome of the model. The provisioning and accounting model developed in this paper overcomes the before-mentioned shortcomings and is consistent with an economic rationale of <b>expected</b> <b>losses.</b> Additionally, this model is based on a close-to-market valuation of the loan that is in favor of the basic idea of IFRS. Suggestions for changes in current accounting and capital requirement rules are provided.|$|R
40|$|This paper {{uses the}} {{possibility}} distribution approach {{to estimate the}} insurance <b>loss</b> <b>amount.</b> A special class of parametric possibility distributions is used to model insurance loss variables. The parameters of the possibility distribution are estimated by combining statistical analysis of sample data and domain knowledge provided by actuarial experts. Insurance premiums are calculated using possibilistic mean and possibilistic variation. Estimation of possibility distribution of aggregate <b>loss</b> <b>amount</b> is also discussed. Key word: insurance loss distribution, premium, possibility distribution, possibilistic mean, possibilistic variation, and aggregate <b>loss</b> <b>amount</b> 1...|$|R
30|$|The {{first term}} {{on the right}} hand side of Equation 17 {{represents}} the expected gain utility from reserving the current product and continuing the search. The second term -pγ∫ ^z+c_i/β_z (z-β x+c_i)^ldΦ _i(x) represents the <b>expected</b> <b>loss</b> utility from buying the current product and stopping the search. The third term -pγ∫ ^z_a (z-β z+c_i)^ldΦ _i(x) represents the <b>expected</b> <b>loss</b> utility from buying the reserved product z and stopping the search if it is available. The last term -(1 -p)γ∫ ^b_a (x-β x+c_i)^ldΦ _i(x) is the <b>expected</b> <b>loss</b> utility from buying the current product and stopping the search {{if it is not}} available.|$|R
5000|$|... #Subtitle level 2: Treatment of <b>expected</b> <b>losses</b> and {{recognition}} of provision ...|$|R
50|$|From the model, one can {{conclude}} that the amount a firm spends to protect information should generally be {{only a small fraction}} of the <b>expected</b> <b>loss</b> (i.e., the <b>expected</b> value of the loss resulting from a cyber/information security breach). More specifically, the model shows that it is generally uneconomical to invest in information security activities (including cybersecurity or computer security related activities) more than 37 percent of the <b>expected</b> <b>loss</b> that would occur from a security breach. The Gordon-Loeb model also shows that, for a given level of potential <b>loss,</b> the optimal <b>amount</b> to spend to protect an information set does not always increase with increases in the information set’s vulnerability. In other words, organizations may derive a higher return on their security activities by investing in cyber/information security activities that are directed at improving the security of information sets with a medium level of vulnerability.|$|R
5000|$|... {{are chosen}} {{in order to}} {{minimise}} the prior <b>expected</b> <b>loss</b> in estimating ...|$|R
50|$|For example, after 10 {{rounds at}} 1 unit per round, the {{standard}} deviation will be 2 x 1 x sqrt(10 x 18/38 x 20/38) = 3.16 units. After 10 rounds, the <b>expected</b> <b>loss</b> will be 10 x 1 x 5.26% = 0.53. As you can see, standard deviation is many times {{the magnitude of the}} <b>expected</b> <b>loss.</b>|$|R
5000|$|... minimum-variance mean-unbiased {{estimator}} (MVUE), {{minimizes the}} risk (<b>expected</b> <b>loss)</b> of the squared-error loss-function.|$|R
40|$|Abstract: When {{evaluating}} point estimators {{by means}} of general <b>loss</b> functions, the <b>expected</b> <b>loss</b> is not always minimal, similar {{to the case of}} mean-biased estimators, whose mean squared error can be reduced by accounting for the mean-bias. Depending on the loss function, the so-called Lehmann-bias can be significantly more important than the mean-bias of an estimator. Although a simple decomposition does not hold for <b>expected</b> <b>losses</b> as it does for the mean squared error, the <b>expected</b> <b>loss</b> can still be reduced by correcting for the Lehmann-bias. An asymptotic and a bootstrap-based correction are suggested and compared in small samples for the exponential distribution {{by means of}} Monte Carlo simulation...|$|R
40|$|This paper {{examines}} estimations of the <b>expected</b> <b>losses</b> {{of loans}} and the provisioning techniques imposed by Czech regulatory norms. The current Czech regulatory quidelines define {{the criteria for}} loan classification and specify the minimum coefficients used for creating provisions. These coefficients correlate each individual loan with an <b>expected</b> <b>loss.</b> Anticipating {{the implementation of the}} New Basel Capital Accord on banking in the Czech Republic, {{it can be argued that}} it will be necessary to revise not only the capital adequacy framework, but also procedures connected with provisioning. The estimation procedures of both <b>expected</b> and unexpected <b>losses</b> should interact. This paper emphasizes the potential weak points of the current regulatory framework in the Czech Republic concerning the estimation technique of <b>expected</b> <b>losses</b> and its relation to the estimation technique of unexpected <b>losses.</b> <b>expected</b> losses; classification; provisions; capital adequacy...|$|R
5000|$|A common example {{involves}} estimating [...] "location." [...] Under typical statistical assumptions, {{the mean}} or average is the statistic for estimating location that minimizes the <b>expected</b> <b>loss</b> experienced under the squared-error loss function, while the median is the estimator that minimizes <b>expected</b> <b>loss</b> experienced under the absolute-difference loss function. Still different estimators would be optimal under other, less common circumstances.|$|R
40|$|This paper {{presents}} a new approach, {{based on the}} Merton model, to decomposing corporate bond spreads into the <b>expected</b> <b>loss,</b> bond risk premium and liquidity premium components. The approach focuses on establishing the bond risk premium using the equity risk premium and the hedge ratio, which are estimated using a dividend discount model and a BEKK-GARCH model. The analysis focuses on non-financial European BBB-rated corporate bonds and distinguishes explicitly between German, French, Spanish and Italian firms. The {{results show that the}} bond risk premium is the largest component. While the <b>expected</b> <b>loss</b> component made the greatest contribution to the strong widening of the spreads around the turn of 2008 / 09, the spreads were then heavily dominated by the bond risk premium and investors received relatively low or, at times, no compensation for <b>expected</b> <b>losses.</b> The safe interest rate and the sovereign CDS premiums are key determinants of the <b>expected</b> <b>loss</b> component and the bond risk premium...|$|R
5000|$|... #Subtitle level 2: Measuring the {{potential}} <b>loss</b> <b>amount</b> due to market risk ...|$|R
40|$|Purpose – The paper {{attempts}} {{to analyze the}} volatility of returns and <b>expected</b> <b>losses</b> of Islamic bank financing. In particular, it takes the case of Indonesian Islamic banking industry. Design/methodology/approach – The paper uses Value at Risk (VaR) approach to compute the volatility (risk) of returns and <b>expected</b> <b>losses</b> of Islamic bank financing. In particular, it uses variance-covariance method to calculate VaR of multi-asset portfolios (groups of equity-, debt- and service-based financing). Findings – First of all, equity and debt-based financing produce sustainable returns of bank financing. Moreover, they are also very resilient during unfavorable economic conditions. Second, the performance of service-based financing is very sensitive to the economic conditions. Lastly, VaR computation on the volatility of returns and <b>expected</b> <b>losses</b> of bank financing finds that risk of investment and <b>expected</b> <b>losses</b> are well managed. Practical implications – The paper demands Islamic banks to keep intensifying equity-based financing rather than only debt-based financing and improve the banking services to support the performance of service-based financing. Originality/value – To {{the best of the}} author's knowledge, this is the first paper to assist the volatility of returns and <b>expected</b> <b>losses</b> of the Islamic banking financing in Indonesian. Banking, Debts, Equity capital, Financial risk, Financing, Indonesia, Islam...|$|R
