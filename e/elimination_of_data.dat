25|10000|Public
50|$|Several {{standards}} {{exist for}} the secure removal of data and the <b>elimination</b> <b>of</b> <b>data</b> remanence.|$|E
30|$|Fuzzy {{logic is}} {{assembled}} to model permeability with the filtered independent variable. By the accuracy analysis, the correlation coefficients reached 0.76 with the <b>elimination</b> <b>of</b> <b>data</b> points in scale error. The application is practical for the ultra-low permeability-to-middle permeability stratum, and the calculation precision is high. As for the stratum of the thin layer or in frequent sedimentary changes, the adaptability is poor.|$|E
40|$|This {{document}} presents novel algorithms for {{detection of}} noisy and flickering pixels from Burst Alert telescope event data and subsequent <b>elimination</b> <b>of</b> <b>data</b> from such pixels {{to create a}} filtered event file. The approach adopted for this purpose {{is quite different from}} the current practises and focuses more on the temporal variation of data in the detector pixels over long intervals of time against the current algorithms which follow a pixel based approach. Comment: 5 Pages, 2 Figure...|$|E
5000|$|... {{evaluation}} of the quality <b>of</b> available experimental <b>data,</b> <b>elimination</b> <b>of</b> wrong <b>data,</b> finding <b>of</b> outliers ...|$|R
5000|$|An {{implementation}} of this paradigm can [...] "unroll" [...] a loop internally. This allows throughput to scale with chip complexity, easily utilizing hundreds <b>of</b> ALUs. The <b>elimination</b> <b>of</b> complex <b>data</b> patterns makes much of this extra power available.|$|R
3000|$|<b>Elimination</b> <b>of</b> the “outliers” <b>data</b> from {{computed}} {{scalar data}} (Wilson and Turcotte 2003) that enter {{the program as}} follows: [...]...|$|R
40|$|The {{effects of}} {{genotype}} and relationship errors on linkage results are evaluated {{in three of}} the Genetic Analysis Workshop 12 asthma genome scans. A number of errors are detected in the samples. While the evidence for linkage is not striking in any data set with or without error, in some cases the difference in test statistic could support different conclusions. The results provide empirical evidence for the predicted effects of genotype and relationship error and highlight the need for rigorous detection and <b>elimination</b> <b>of</b> <b>data</b> error in complex trait studies. © 2001 Wiley-Liss, Inc. link_to_subscribed_fulltex...|$|E
40|$|Abstract. In the {{relational}} model of data the Boyce-Codd-Heath nor-mal form, commonly just known as Boyce-Codd normal form, guarantees the <b>elimination</b> <b>of</b> <b>data</b> redundancy {{in terms of}} functional dependencies. For efficient means of data processing the industry standard SQL per-mits partial data and duplicate rows of data to occur in database sys-tems. Consequently, the combined class of uniqueness constraints and functional dependencies is more expressive than the class of functional dependencies itself. Hence, the Boyce-Codd-Heath normal form is not suitable for SQL databases. We characterize the associated implication problem of the combined class {{in the presence of}} NOT NULL constraints axiomatically, algorithmically and logically. Based on these results we are able to establish a suitable normal form for SQL. ...|$|E
40|$|The {{advent of}} large {{electronic}} text corpora has generated {{a range of}} technologies for their search and interpretation. Variation in document length {{can be a problem}} for these technologies, and several normalization methods for mitigating its effects have been proposed. This paper assesses the effectiveness of such methods in specific relation to exploratory multivariate analysis. The discussion is in four main parts. The first part states the problem, the second describes some normalization methods, the third identifies poor estimation of the population probability of variables as a factor that compromises the effectiveness of the normalization methods for very short documents, and the fourth proposes <b>elimination</b> <b>of</b> <b>data</b> matrix rows representing documents which are too short to be reliably normalized and suggests ways of identifying the relevant documents...|$|E
30|$|Clouds are {{responsible}} for scalar sensor offloading to find suitable CAPs. In addition, clouds continuously adjust camera status throughout the monitoring region, using the event detection data stored in database servers, focusing on minimizing the possible coverage overlaps, and resulting in the <b>elimination</b> <b>of</b> redundant <b>data.</b>|$|R
50|$|Some DVD players, line doublers, and {{personal}} video recorders {{are designed to}} detect and remove 2:3 pulldown from telecined video sources, thereby reconstructing the original 24 frame/s film frames. This technique is known as “reverse” or “inverse” telecine. Benefits of reverse telecine include high-quality non-interlaced display on compatible display devices and the <b>elimination</b> <b>of</b> redundant <b>data</b> for compression purposes.|$|R
40|$|Performances of {{classifiers}} {{are affected}} by imbalanced data because instances in the minority class are often ignored. Imbalanced data often occur in many application domains including flood. If flood cases are misclassified, the impact of flood {{is higher than the}} misclassification of non-flood cases. Numerous resampling techniques such as undersampling and oversampling have been used to overcome the problem of misclassification <b>of</b> imbalanced <b>data.</b> However, the undersampling and oversampling techniques suffer from <b>elimination</b> <b>of</b> relevant <b>data</b> and overfitting, which may lead to poor classification results. This paper proposes a Fuzzy Distance-based Undersampling (FDUS) technique to increase classification accuracy. Entropy estimation is used to generate fuzzy thresholds which are used to categorise the instances in majority and minority classes into membership functions. The performance of FDUS was compared with three techniques based on Fmeasure and G-mean, experimented on flood data. From the results, FDUS achieved better F-measure and G-mean compared to the other techniques which showed that the FDUS was able to reduce the <b>elimination</b> <b>of</b> relevant <b>data...</b>|$|R
40|$|Microsoft SQL Server 2008 offers {{technologies}} for performing On-Line Analytical Processing (OLAP), directly on data stored in data warehouses, instead {{of moving the}} data into some offline OLAP tool. This brings certain benefits, such as <b>elimination</b> <b>of</b> <b>data</b> copying and better integration with the DBMS compared with off-line OLAP tools. This report reviews SQL Server support for OLAP, solution architectures, tools and components involved. Standard storage options are discussed but {{the focus of this}} report is relational storage of OLAP data. Scalability test is conducted to measure performance of Relational OLAP (ROLAP) storage option. The scalability test shows that when ROLAP storage mode is used, query response time grows linearly with dataset size. A tutorial is appended to demonstrate how to perform OLAP tasks using SQL Server in practice. Hemsida...|$|E
40|$|A {{methodology}} {{is proposed}} {{to facilitate the}} construction of gridded bathymetry data {{for the use of}} hydrodynamic models on the continental shelf. It relies on the carrying out of three successive tasks: Automatic selection of records of better quality among multiple sets of overlapping data; <b>Elimination</b> <b>of</b> <b>data</b> points located on land; Taking into account of the shoreline as bathymetric data. Algorithms are proposed to perform sorting of the records according their quality as well as masking by the coastline. The suggested method facilitates the updating of bathymetry data and optimizes their use. It enables automatic execution of all the tasks and building of new digital bathymetry models in a few hours, without action from the operator. The method has been tested many times on the continental shelf of North-West Europe. The bathymetric data so generated do not require corrections, and the hydrodynamic models on which thes...|$|E
40|$|Abstract—Data {{races in}} multi-threaded {{programs}} are a com-mon source of serious software failures. Their undefined behavior {{may lead to}} intermittent failures with unforeseeable, and in embedded systems, even life-threatening consequences. To mit-igate these risks, various detection tools have been created to help identify potential data races. However, these tools produce thousands of data race warnings, often in text-based format, which makes the manual assessment process slow and error-prone. Through visualization, we aim {{to speed up the}} data race assessment process by reducing the amount of information to be investigated, and to provide a versatile interface that quality as-surance engineers can use to investigate data race warnings. The ultimate goal of our integrated software suite, called RaceView, is to improve the usability of the data race information {{to such an extent that}} the <b>elimination</b> <b>of</b> <b>data</b> races can be incorporated into the regular software development process. Index Terms—multi-threading; static analysis; data race detec-tion; user interface; graph visualization; graph navigation I...|$|E
40|$|By {{distinguishing}} nested attributes as Decomposable and Non-Decomposable, it is {{proved that}} for all nested relations, unnesting and then renesting on the same attribute yields the original relation subject only to the <b>elimination</b> <b>of</b> duplicate <b>data.</b> Therefore, the statement that was popular in nested relations research: "Unnesting and then nesting on the same attribute of a nested relation does not always yield the original relation" is reconsidered...|$|R
40|$|Record linkage is a {{critical}} problem in duplicate data elimination. It is used to detect and eliminateduplicate <b>data.</b> The <b>elimination</b> <b>of</b> duplicate <b>data</b> will increase the quality <b>of</b> <b>data.</b> Record Linkage problem willtake high computational cost {{because of the large}} number of record comparisons. The comparison of records isinefficient in large data warehouses. Blocking methods are used to group the records to minimize the number ofrecord comparisons. This paper explains the existing blocking methods and its comparison and discusses theselection of token-based blocking key for record comparisons...|$|R
30|$|We {{have proved}} that for common {{speakers}} a cost-efficient cross-lingual adaptation {{can be done}} even with a training dataset smaller than the usual databases for training speech recognizers. Especially, the forced alignment tool {{has proven to be}} very useful. In the case of disabled speakers, the task itself is challenging. However, we succeeded in the automatic <b>elimination</b> <b>of</b> the <b>data</b> which was not suitable for training, and generally, we can say that the proposed method leads to significant reduction of time-consuming expertwork.|$|R
40|$|<b>Elimination</b> <b>of</b> <b>data</b> {{dependencies}} through value prediction {{can help}} to extract more parallelism. High accuracy value prediction is thus essential in achieving significant performance improvement by exposing more ILP. Various data value prediction methods have been studied [SS 97],[WF 97]. In this paper, the Shifting Locality Stride Predictor (SLSP) and Repeating Peak Predictor(RPP) are introduced, which will have lower miss rates than available computational predictors(Stride and last value) on certain common value patterns. Their implementation and integration with existing predictors through a counter based selector are discussed, followed by {{a comparison of the}} performance between the proposed predictors and Stride and Last Value predictor. The results strengthened our belief that these predictors {{can help to}} improve prediction accuracy by an average of 1. 9 % and 3. 1 % respectively. We also observed that prediction accuracy will increase with the prediction table size and level off after certain size, though it is not sensitive to the initial values of selection counter. ...|$|E
40|$|In this paper, a {{scalable}} scheme, configurable via register-transfer level parameters, {{for full}} register bypassing {{in a modern}} embedded processor architecture, termed By-oRISC, is presented. The register bypassing specification is parameterized regarding the number of homogeneous register file read and write ports {{and the number of}} pipeline stages of the processor. The performance characteristics (cycle time, chip area) of the proposed technique have been evaluated for FPGA target implementa-tions of the synthesizable ByoRISC model. It is proved that, a full bypassing network is a viable solution for the <b>elimination</b> <b>of</b> <b>data</b> hazards when servicing instructions with multiple read and write operands. While the maximum clock frequency is re-duced by 17. 9 % in average, when using partial versus full forwarding, the positive effect of custom computation eliminates this effect by providing cycle speedups of 3. 9 × to 5. 5 × and corresponding execution time speedups for a ByoRISC testbed processor of 3. 6 ×. Individual application speedups of up to 9. 4 × have also been obtained...|$|E
40|$|This {{document}} specifies an Internet standards track {{protocol for}} the Internet community, and requests discussion {{and suggestions for}} improvements. Please refer to the current edition of the "Internet Official Protocol Standards " (STD 1) for the standardization state and status of this protocol. Distribution of this memo is unlimited. Copyright Notice Copyright (C) The Internet Society (2002). All Rights Reserved. This document defines a format for using compressed data as a Cryptographic Message Syntax (CMS) content type. Compressing data before transmission provides a number of advantages, including the <b>elimination</b> <b>of</b> <b>data</b> redundancy which could help an attacker, speeding up processing by {{reducing the amount of}} data to be processed by later steps (such as signing or encryption), and reducing overall message size. Although there have been proposals for adding compression at other levels (for example at the MIME or SSL level), these don’t address the problem of compression of CMS content unless the compression is supplied by an external means (for example by intermixing MIME and CMS). 1...|$|E
40|$|Abstract — This paper {{presents}} a DHT-based grid resource indexing and discovery (DGRID) approach. With DGRID, resource-information data is stored {{on its own}} administrative domain and each domain, represented by an index server, is virtualized to several nodes (virtual servers) subjected {{to the number of}} resource types it has. Then, all nodes are arranged as a structured overlay network or distributed hash table (DHT). Comparing to existing grid resource indexing and discovery schemes, the benefits of DGRID include improve security of domains, increase availability <b>of</b> <b>data,</b> and <b>elimination</b> <b>of</b> stale <b>data.</b> Index Terms — Grid, resource indexing and discovery, DHT, availabilit...|$|R
40|$|This paper {{deals with}} the {{optimization}} <b>of</b> <b>data</b> routing processes and with optimization of deployed quality of service mechanisms in computer networks. The paper addresses the problems related to requirement of monitoring and managing of network infrastructures with attention given to data routing mechanisms in network and often used QoS mechanisms. This paper also presents the concept of tool for automated network traffic management in order to network traffic optimization by identifying input curve α(t) and service curve β(t) with application of mechanisms for traffic shaping and adaptive <b>elimination</b> <b>of</b> aggressive <b>data</b> flows. Proposed methods are experimentaly verified and compared with conventional methods...|$|R
40|$|Abstract—Wireless {{sensor network}} (WSN) {{is a kind}} of energy {{constrained}} network, by using data fusion technology, the <b>elimination</b> <b>of</b> redundant <b>data,</b> can save energy, prolong the network life purpose. Data fusion in wireless sensor network can realize different protocol layers, Based on the introduction of wireless sensor network and data fusion related knowledge, prove that the arithmetic mean method is effective, and use OPNET software tool for network simulation, finally, analysis results and conclude, verify effects of the arithmetic average fusion algorithm for wireless sensor network. Key words-WSN; Data fusion; Average method; OPNET I. OVERVIE...|$|R
40|$|Abstract [...] Field {{inspection}} {{is one of}} {{the core}} operations of municipalities. There are over ten types of field inspection in a typical municipality, including inspection of building, road digging, food sales, food processing services, ad signs, malls, city development projects, cattle slaughterhouses, lodging services, and the inspection team itself. Inspection activities are carried out either periodically, randomly, or in response to complaints. The number and variety of inspections make inspection complex. Therefore, managing it manually is inefficient. Another aspect of complexity is the amount of integration required with other systems. To function properly, every type of inspection needs to integrate with several other systems. Building inspection requires integration with the building licenses system; food sales shops inspection requires integration with the shop licenses system, and so on. In addition, all types of inspections need to integrate with core systems, such as those of human resources, finance, violation management, and citizens ’ database. The objectives of this research is a component-based integrated architecture that efficiently manages field inspection in municipalities, robust communication between management team and task force, optimal use of resources based on geographical information, and <b>elimination</b> <b>of</b> <b>data</b> duplication or redundancy...|$|E
40|$|Abstract Microarrays allow {{researchers}} {{to measure the}} expression of thousands of genes in a single experiment. Before statistical comparisons can be made, the data must be assessed for quality and normalisation procedures must be applied, of which many have been proposed. Methods of comparing the normalised data are also abundant, and no clear consensus has yet been reached. The {{purpose of this paper}} was to compare those methods used by the EADGENE network on a very noisy simulated data set. With the a priori knowledge of which genes are differentially expressed, it is possible to compare the success of each approach quantitatively. Use of an intensity-dependent normalisation procedure was common, as was correction for multiple testing. Most variety in performance resulted from differing approaches to data quality and the use of different statistical tests. Very few of the methods used any kind of background correction. A number of approaches achieved a success rate of 95 % or above, with relatively small numbers of false positives and negatives. Applying stringent spot selection criteria and <b>elimination</b> <b>of</b> <b>data</b> did not improve the false positive rate and greatly increased the false negative rate. However, most approaches performed well, and it is encouraging that widely available techniques can achieve such good results on a very noisy data set. </p...|$|E
30|$|We {{envisage}} our cell-based broad-phase microarchitectures fabricated {{as part of}} an IC {{that would}} also execute the remainder of the interactive application program loop. Using a single platform would allow for the <b>elimination</b> <b>of</b> <b>data</b> transfer overheads. This concept has been successfully adopted to integrate a CPU and GPU within some Intel Core processors [42] as well as AMD accelerated processing units (APUs) [43], such as those in the PlayStation 4. Within the spectrum of platforms readily available today, our microarchitectures could naturally reside within the fixed-function logic of GPUs, as there is already a significant focus on relocating many elements of the interactive application program loop to these platforms [44]. The remainder of the program loop could utilise the programmable elements of the GPU. Adding one of the microarchitectures would not compromise GPU programmability, as all GPUs include some fixed-function logic such as rasterisation. This is unlikely to change due to power-density limits as well as the lacklustre throughput achieved when traditionally fixed-function elements have been reimplemented using the programmable elements of a GPU [45]. Moreover, it is not prohibitively expensive to include one of the microarchitectures, as the large production volumes of commodity platforms amortises the cost [27]. Therefore, there exists sufficient motivation for the fabrication of our logic as part of a future GPU.|$|E
40|$|Cluster {{analysis}} {{has played a}} key role in data understanding. When such an important data mining task is extended to the context <b>of</b> <b>data</b> streams, it becomes more challenging since the data arrive at a mining system in one-pass manner. The problem is even more difficult when the clustering task is considered in a sliding window model which requiring the <b>elimination</b> <b>of</b> outdated <b>data</b> must be dealt with properly. We propose SWEM algorithm that exploits the Expectation Maximization technique to address these challenges. SWEM is not only able to process the stream in an incremental manner, but also capable to adapt to changes happened in the underlying stream distribution. <br /...|$|R
40|$|The {{exchange}} of information between a Radiology Information System (RIS) and a PACS is essential to optimizing the utility of a PACS. Some of the benefits awarded by implementing an interface include a reduction or <b>elimination</b> <b>of</b> repetitious <b>data</b> entry, the availability of more accurate information on the PACS, a reduction in workload for the technologists, registration clerks, transcriptionists, etc, and the availability <b>of</b> more accurate <b>data</b> for automating the PACS. This paper discusses the Georgetown experience of interfacing an HIS/RIS and PACS, by describing {{the development of the}} interface and its impact on clinical operations...|$|R
40|$|This paper {{presents}} {{evidence for}} the existence of ‘set-points’ for subjective wellbeing. Our results derive from a 10 -year longitudinal study in which subjective wellbeing has been measured using a single question of general life satisfaction. The process <b>of</b> <b>data</b> analysis is driven by logic based on the theory of subjective wellbeing homeostasis. This analysis involves the iterative <b>elimination</b> <b>of</b> raw <b>data,</b> from 7, 356 individual respondents, based on confidence limits. All results are projected onto a 0 – 100 point scale. We demonstrate {{evidence for the}} existence of set-points lying between 71 and 90 points, with an average set-point-range of 18 – 20 points for each person. The implications and limitations of these findings are discussed...|$|R
40|$|Mitra {{demonstrates}} that memory erasure {{can cause the}} observer {{to end up in}} a different sector of the multiverse with a different destiny, events in the future remote to any possible influence of the observer having radically different probabilities. The concept only applies to an observer defined by a structure of information, so cannot apply to the physical bodies of human observers. However, Everett defines the functional identity of the observer as the contents of the memory, a structure of information, thus in principle Mitra's effect would apply. Here it is shown that not only is this very minimal definition of the observer entirely in accord with the subjective experience of identity of human observers, it is also the only possible definition of a transtemporal observer in a no-collapse universe. With respect to human observers, selective <b>elimination</b> <b>of</b> <b>data</b> is not possible due to the distributed manner in which information is stored in the neural network of the brain, but addition of data works on exactly the same principle. Thus, while it would not be possible to alter the destiny of the observer to reduce the probability of unwanted events to background probability, as in the example Mitra uses to demonstrate the principle, {{it would be possible to}} increase the probability of desired events. This would appear to verify certain aspects of folklore which otherwise appear entirely mythical and imaginary...|$|E
40|$|Abstract. SIMD {{extension}} {{is one of}} {{the most}} common and effective technique to exploit data-level parallelism in today’s processor designs. However, the performance of SIMD architectures is limited by some constraints such as mismatch between the storage and the computational formats and using data permutation instructions during vectorization. In our previous work we have proposed two architectural modifications, the extended subwords and the Matrix Register File (MRF) to alleviate the limitations. The extended subwords, uses four extra bits for every byte in a media register and it provides additional parallelism. The MRF allows flexible row-wise as well as column-wise access to the register file and it eliminates data permutation instructions. We have validated the combination of the proposed techniques by studying the performance of some multimedia kernels. In this paper, we analysis each proposed technique separately. In other words, we answer the following questions in this paper. How much of the performance gain is a result of the additional parallelism? and how much is due to the <b>elimination</b> <b>of</b> <b>data</b> permutation instructions? The results show that employing the MRF and extended subwords separately obtains the speedup less than 1 and 1. 15, respectively. In other words, our results indicate that using either extended subwords or the MRF techniques is insufficient to eliminate most pack/unpack and rearrangement overhead instructions on SIMD processors. The combination of both techniques, on the other hand, yields much more performance benefits than each technique. ...|$|E
40|$|The {{purpose of}} this 1982 {{national}} survey of all operational prepaid health plans, or PHPs (including health maintenance organizations), was to provide information on the current coverage of PHP mental health and substance abuse services, benefits and service provision, general and mental health organization characteristics, mental health service costs, and {{physical and mental health}} service utilization. ^ Two survey instruments were designed, pretested and distributed to all operational PHPs throughout the United States. A total of 237 PHPs were surveyed, of which 205 (86. 50 percent) completed and returned both questionnaires. ^ One result of the rapid growth in the PHP field {{over the past ten years}} has been the expansion in both the number of PHPs as well as the organizational characteristics of these PHPs. However, little attention in the research literature has been given to the application of empirical results to the PHP arrangements. This project has attempted to contribute to current knowledge regarding prepaid mental health services from a national perspective, and explore, on a preliminary descriptive basis, the variety of potential service delivery arrangements for physical and mental health services (total services) and for mental health services. ^ The study emphasized that PHPs must continue to monitor the costs and utilization of mental health services, particularly in light of the apparent <b>elimination</b> <b>of</b> <b>data</b> collection and statistical summary responsibilities within the federal government regarding PHP activities as well as the proposed legislation to eliminate mandated mental health and substance abuse services from basic health plan benefits for federally qualified PHPs. ...|$|E
50|$|The {{metabolic}} pathway of trospium in humans {{has not been}} fully defined. Of the 10% of the dose absorbed, metabolites account for approximately 40% of the excreted dose following oral administration. The major {{metabolic pathway}} is hypothesized as ester hydrolysis with subsequent conjugation of benzylic acid to form azoniaspironortropanol with glucuronic acid. Cytochrome P450 {{is not expected to}} contribute significantly to the <b>elimination</b> <b>of</b> trospium. <b>Data</b> taken from in vitro human liver microsomes investigating the inhibitory effect of trospium on seven cytochrome P450 isoenzyme substrates (CYP1A2, 2A6, 2C9, 2C19, 2D6, 2E1, and 3A4) suggest a lack of inhibition at clinically relevant concentrations.|$|R
50|$|In {{computer}} architecture, register renaming is {{a technique}} that eliminates the false data dependencies arising from the reuse of architectural registers by successive instructions {{that do not have}} any real data dependencies between them. The <b>elimination</b> <b>of</b> these false <b>data</b> dependencies reveals more instruction-level parallelism in an instruction stream, which can be exploited by various and complementary techniques such as superscalar and out-of-order execution for better performance.|$|R
40|$|This work {{presents}} {{new method}} of local information density estimation in the ECG. Its principle is the controlled <b>elimination</b> <b>of</b> the <b>data</b> by cancelling the time-frequency plane coefficients of an electrocardiogram with simultaneous analysis of resulted inaccuracy of basic diagnostic parameters. In some {{regions of the}} standard ECG recording cutting out a given amount <b>of</b> <b>data</b> influences more the diagnostic parameters distortion than in the others. These regions are well related to the waves start- and endpoints. Furthermore, we computed the time function representing the typical diagnostic data stream density in an electrocardiogram. This function is {{the background of the}} ECG compression with use of adaptively modified characteristics, and is useful for comparing of distortion and assessment of differences in the ECG signals...|$|R
