4|2460|Public
40|$|Real-time fluid {{simulation}} is {{an active}} field of research in computer graphics, but they usually focus on visual impact rather than physical accuracy. However, by combining a lattice Boltzmann model with the parallel computing power of a graphics processing unit, both real-time compute capability and satisfactory physical accuracy are now achievable. The implementation of an optimised 3 D real-time thermal and turbulent fluid flow solver with a performance of half a billion lattice node updates per second is described in detail. The effects of the hardware <b>error</b> <b>checking</b> <b>code</b> and the competition between appropriate boundary conditions and performance capabilities are discussed...|$|E
40|$|Information {{integrity}} in cache memories {{is a fundamental}} requirement for dependable computing. Conventional architectures for enhancing cache reliability using check codes {{make it difficult to}} trade between the level of data integrity and the chip area requirement. We focus on transient fault tolerance in primary cache memories and develop new architectural solutions to maximize fault coverage when the budgeted silicon area is not sufficient for the conventional configuration of an <b>error</b> <b>checking</b> <b>code.</b> The underlying idea is to exploit the corollary of reference locality in the organization and management of the code. A higher protection priority is dynamically assigned to the portions of the cache that are more error-prone and have a higher probability of access. The error-prone likelihood prediction is based on the access frequency. We evaluate the effectiveness of the proposed schemes using a trace-driven simulation combined with software error injection using four different fault m [...] ...|$|E
40|$|It is {{difficult}} to write device drivers. One factor is that writing low-level code for accessing devices and manipulating their registers is tedious and error-prone. For many system-on-chip based systems, buggy hardware, imprecise documentation, and code reuse worsen the situation further. This paper presents HAIL (Hardware Access Interface Language), a language-based approach to simplify device access programming and generate <b>error</b> <b>checking</b> <b>code</b> against bugs in software, hardware, and documentation. HAIL is a domain-specific language that specifies all aspects of a device’s programming interface and the access methods in a particular system and OS. A compiler automatically checks the specification and translates it into C code for device access, with optional debugging code. The generated code can be included directly into device driver code. In the paper, we argue that HAIL lowers development effort, incurs minimal runtime overhead, and reduces device access related bugs. We also show that the HAIL specification can be reused for different operating systems, thereby reducing porting costs...|$|E
50|$|PhpStorm {{provides}} a rich code editor for PHP with syntax highlighting, extended code formatting configuration, on-the-fly <b>error</b> <b>checking,</b> and <b>code</b> completion.|$|R
50|$|Provides minimal {{run-time}} <b>error</b> <b>checking</b> {{to reduce}} <b>code</b> size and increase performance. An optional error-checking infrastructure is provided {{to assist in}} debugging during application development.|$|R
40|$|As {{society has}} become more reliant on electronics, the need for fault {{tolerant}} ICs has increased. This has resulted in signi cant research into both fault tolerant controller design, and mechanisms for datapath fault tolerance insertion. By treating these two issues separately, previous work has failed to address compatibility issues, as well as e cient codesign methodologies. In this paper, we present a uni ed approach to detecting control and datapath faults through the datapath, along with a method for fault identi cation and recon guration. By detecting control faults in the datapath, we avoid the area and performance overhead of detecting control faults through duplication or <b>error</b> <b>checking</b> <b>codes.</b> The result is a complete design methodology for self recovering architectures capable of far more e cient solutions than previous approaches. ...|$|R
40|$|In {{this thesis}} a code-editor was {{implemented}} {{as a part}} of a bigger web-based system for solving programming assignments in the course TDT 4100. The editor was created in order to allow the students of the class to focus solely on writing code, and not on setting up the surrounding framework (installing programming languages and IDEs, setting up projects, etc.). The editor supports syntax highlighting, <b>error</b> <b>checking,</b> <b>code</b> completion, multiples classes, and running of tests, along with all of the more basic editor functionality such as block indentation, bracket matching, line-numbers, etc. The editor is embedded into each problem contained in an assignments, which allows students to solve basic and intermediate programming challenges directly in their web-browser, without the need for any setup. The system also utilizes several gamification elements, as described in the thesis preliminary study, Gamification of Assignment Systems (Åse, 2014). Responsive web design principles were used while implementing the system, which allows students to check their ranks and scores from any device. This was done in order to foster competition between the students, which will in turn increase motivation even further. The results from the experiments performed indicate that the editor is well suited for use on programming assignments in courses such as TDT 4100, TDT 4110 and TDT 4120, or any other course which has assignments that can be tested programmatically, as the editor has a low response time even for very large programs (64 KB). However, the editor is not suited for courses such as TDT 4180, or other GUI-programming courses, since the he editor is currently limited to displaying console output and test-results...|$|E
40|$|We {{employ the}} methods {{presented}} in the previous chapter for decoding corrupted codewords, encoded using sparse parity <b>check</b> <b>error</b> correcting <b>codes.</b> We show the similarity between the equations derived from the TAP approach and those obtained from belief propagation, and examine their performance as practical decoding methods...|$|R
40|$|We have {{developed}} a simple and efficient algorithm to identify each member of a large collection of DNA-linked objects {{through the use of}} hybridization, and have applied it to the manufacture of randomly assembled arrays of beads in wells. Once the algorithm has been used to determine the identity of each bead, the microarray {{can be used in a}} wide variety of applications, including single nucleotide polymorphism genotyping and gene expression profiling. The algorithm requires only a few labels and several sequential hybridizations to identify thousands of different DNA sequences with great accuracy. We have decoded tens of thousands of arrays, each with 1520 sequences represented at ∼ 30 -fold redundancy by up to ∼ 50, 000 beads, with a median error rate of < 1 × 10 − 4 per bead. The approach makes use of <b>error</b> <b>checking</b> <b>codes</b> and provides, for the first time, a direct functional quality control of every element of each array that is manufactured. The algorithm can be applied to any spatially fixed collection of objects or molecules that are associated with specific DNA sequences. Microarray technology, devised for the analysis of complex biological systems, uses the ability of a DNA strand to hybridize specifically to its complement to extract 1000 s of measurements at a time from a single sample (Watson and Crick 1953; Souther...|$|R
40|$|We {{propose a}} method {{based on the}} {{magnetization}} enumerator to determine the critical noise level for Gallager type low density parity <b>check</b> <b>error</b> correcting <b>codes</b> (LDPC). Our method provides an appealingly simple interpretation to the relation between different decoding schemes, and provides more optimistic critical noise levels than those reported in the information theory literature...|$|R
40|$|We motivate {{and discuss}} a novel {{functional}} programming construct that allows convenient modular run-time nonstandard interpretation via reflection on closure environments. This map-closure construct encompasses both {{the ability to}} examine {{the contents of a}} closure environment and to construct a new closure with a modified environment. From the user’s perspective, map-closure is a powerful and useful construct that supports such tasks as tracing, security logging, sandboxing, <b>error</b> <b>checking,</b> profiling, <b>code</b> instrumentation and metering, run-time code patching, and resource monitoring. From the implementor’s perspective, map-closure is analogous to call/cc. Just as call/cc is a non-referentiallytransparent mechanism that reifies the continuations that are only implicit in programs written in direct style, map-closure is a nonreferentially- transparent mechanism that reifies the closure environments that are only implicit in higher-order programs. Just as CPS conversion is a non-local but purely syntactic transformation that can eliminate references to call/cc, closure conversion is a non-local but purely syntactic transformation that can eliminate references to map-closure. We show how the combination of map-closure and call/cc can be used to implement set! as a procedure definition and a local macro transformation...|$|R
40|$|Database {{integrity}} {{is a central}} underlying issue {{in the implementation of}} database technology. Trust in the correctness of the data that is held by the database system is a prerequisite for using the data in business, research or decision making applications. This paper will begin by discussing the areas that pose challenges in ensuring database security and reliability. It will look at the benefits and limitations of possible hardware and software solution strategies, especially with respect to the considerations of system overhead and the effect on system performance. It will specifically discuss the use of <b>error</b> <b>checking</b> and correction <b>codes</b> to address {{integrity is}}sues and consider how these codes may be used to help improve the performance of database systems...|$|R
40|$|We {{report on}} the {{implementation}} of a reverse-reconciliated coherent-state continuous-variable quantum key distribution system, with which we generated secret keys at a rate of more than 2 kb/ s over 25 km of optical fiber. Time multiplexing is used to transmit both the signal and phase reference in the same optical fiber. Our system includes all experimental aspects required for a field implementation of a quantum key distributionsetup. Real-time reverse reconciliation is achieved by using fast and efficient low-density parity <b>check</b> <b>error</b> correcting <b>codes.</b> info:eu-repo/semantics/publishe...|$|R
40|$|We obtain exact {{expressions}} for the {{asymptotic behaviour}} {{of the average}} probability of the block decoding error for ensembles of regular low density parity <b>check</b> <b>error</b> correcting <b>codes,</b> by employing diagrammatic techniques. Furthermore, we show how imposing simple constraints on the code ensemble (that can be practically implemented in linear time), allows one to suppress the error probability for codes with more than 2 checks per bit, to an arbitrarily low power of N. As such we provide a practical route to a (sub-optimal) expurgated ensemble. Comment: 15 pages 13 figure...|$|R
40|$|Abstract. We {{propose a}} method to {{determine}} the critical noise level for decoding Gallager type low density parity <b>check</b> <b>error</b> correcting <b>codes.</b> The method {{is based on the}} magnetization enumerator (M), rather than on the weight enumerator (W) presented recently in the information theory literature. The interpretation of our method is appealingly simple, and the relation between the different decoding schemes such as typical pairs decoding, MAP, and finite temperature decoding (MPM) becomes clear. Our results are more optimistic than those derived via the methods of information theory and are in excellent agreement with recent results from another statistical physics approach. ...|$|R
40|$|We {{determine}} the critical noise level for decoding low density parity <b>check</b> <b>error</b> correcting <b>codes</b> {{based on the}} magnetization enumerator (), {{rather than on the}} weight enumerator () employed in the information theory literature. The interpretation of our method is appealingly simple, and the relation between the different decoding schemes such as typical pairs decoding, MAP, and finite temperature decoding (MPM) becomes clear. In addition, our analysis provides an explanation for the difference in performance between MN and Gallager codes. Our results are more optimistic than those derived via the methods of information theory and are in excellent agreement with recent results from another statistical physics approach. Comment: 9 pages, 5 figure...|$|R
40|$|We {{propose a}} method to {{determine}} the critical noise level for decoding Gallager type low density parity <b>check</b> <b>error</b> correcting <b>codes.</b> The method {{is based on the}} magnetization enumerator (), rather than on the weight enumerator () presented recently in the information theory literature. The interpretation of our method is appealingly simple, and the relation between the different decoding schemes such as typical pairs decoding, MAP, and finite temperature decoding (MPM) becomes clear. Our results are more optimistic than those derived via the methods of information theory and are in excellent agreement with recent results from another statistical physics approach. Comment: 10 pages, 4 figures, 8 th IMA int. conf. on cryptography and Codin...|$|R
40|$|ITS is a {{powerful}} and user-friendly software package permitting {{state of the art}} Monte Carlo solution of linear time-independent couple electron/photon radiation transport problems, with or without the presence of macroscopic electric and magnetic fields of arbitrary spatial dependence. Our goal has been to simultaneously maximize operational simplicity and physical accuracy. Through a set of preprocessor directives, the user selects one of the many ITS codes. The ease with which the makefile system is applied combines with an input scheme based on order-independent descriptive keywords that makes maximum use of defaults and internal <b>error</b> <b>checking</b> to provide experimentalists and theorists alike with a method for the routine but rigorous solution of sophisticated radiation transport problems. Physical rigor is provided by employing accurate cross sections, sampling distributions, and physical models for describing the production and transport of the electron/photon cascade from 1. 0 GeV down to 1. 0 keV. The availability of source code permits the more sophisticated user to tailor the codes to specific applications and to extend the capabilities of the codes to more complex applications. Version 5. 0, the latest version of ITS, contains (1) improvements to the ITS 3. 0 continuous-energy codes, (2) multigroup codes with adjoint transport capabilities, and (3) parallel implementations of all ITS codes. Moreover the general user friendliness of the software has been enhanced through increased internal <b>error</b> <b>checking</b> and improved <b>code</b> portability...|$|R
50|$|In {{terms of}} the OSI model for networks, DNP3 {{specifies}} a layer 2 protocol. It provides multiplexing, data fragmentation, <b>error</b> <b>checking,</b> link control, prioritization, and layer 2 addressing services for user data. It also defines a Transport function (somewhat similar to the function of layer 4) and an Application Layer (layer 7) that defines functions and generic data types suitable for common SCADA applications. The DNP3 frame strongly resembles, but is not identical to the IEC 60870-5 FT3 frame. It makes heavy use of cyclic redundancy <b>check</b> <b>codes</b> to detect <b>errors.</b>|$|R
40|$|As modern {{computation}} platforms {{become increasingly}} complex, their programming interfaces are {{increasingly difficult to}} use. This complexity is especially inappropriate given the relatively simple core functionality {{that many of the}} computations implement. We present a new approach for obtaining so ware that executes on modern computing platforms with complex programming interfaces. Our approach starts with a simple seed program, written {{in the language of the}} developer's choice, that implements the desired core functionality. It then systematically generates inputs and observes the resulting outputs to learn the core functionality. It finally automatically regenerates new code that implements the learned core functionality on the target computing platform. This regenerated code contains both (a) boilerplate code for the complex programming interfaces that the target computing platform presents and (b) systematic <b>error</b> and vulnerability <b>checking</b> <b>code</b> that makes the new implementations robust and secure. By providing a productive new mechanism for capturing and encapsulating knowledge about how to use modern complex interfaces, this new approach promises to greatly reduce the developer effort required to obtain secure, robust so ware that executes on modern computing platforms...|$|R
50|$|First, {{because there}} isn't {{any form of}} macros <b>error</b> <b>checking</b> (as there is for C or {{assembly}} language), {{it is possible to}} make macros which will not work.Indeed, for the C language, the syntax of each macro is replaced by what has been declared by the preprocessor. Only after that does the compiler <b>check</b> the <b>code.</b>|$|R
40|$|The Land Condition Trend Analysis (LCTA) users inter-face program assists natural {{resources}} managers at U. S. military installations {{to manage and}} summarize LCTA inventory and monitoring data. The LCTA users interface program provides mechanisms to view and summarize vegetation, wildlife, soils, and land use data. This manual contains instructions for installing the software, viewing data, and summarizing data. Comprehensive descrip-tions for each data analysis summary include required input data, output data and file formats, quality control data <b>checks,</b> <b>error</b> <b>codes,</b> and data summary use descriptions. The LCTA users interface program requires a 386 IBM compatible personal computer with at least 4 MB random access memory (RAM) and 200 MB hard disk space. The system must be equipped with MS-DOS ® 3. 1 or above, Windows ™ 3. 1 or above, and SQLBase ® 5. 0 or above...|$|R
40|$|Abstract We motivate {{and discuss}} a novel {{functional}} programming constructthat allows convenient modular run-time nonstandard interpretation via reflection on closure environments. This map-closure con-struct encompasses both {{the ability to}} examine {{the contents of a}} closure environment and to construct a new closure with a modi-fied environment. From the user's perspective, map-closure is apowerful and useful construct that supports such tasks as tracing, security logging, sandboxing, <b>error</b> <b>checking,</b> profiling, <b>code</b> in-strumentation and metering, run-time code patching, and resource monitoring. From the implementor's perspective, map-closureis analogous to call/cc. Just as call/cc is a non-referentially-transparent mechanism that reifies the continuations that are only implicit in programs written in direct style, map-closure is a non-referentially-transparent mechanism that reifies the closure environments that are only implicit in higher-order programs. Just asCPS conversion is a non-local but purely syntactic transformation that can eliminate references to call/cc, closure conversionis a non-local but purely syntactic transformation that can eliminate references to map-closure. We show how the combinationof map-closure and call/cc can be used to implement set! asa procedure definition and a local macro transformation. Categories and Subject Descriptors D. 3. 2 [Language Classifica-tions]: Applicative (functional) languages; D. 3. 3 [Language constructs and features]: Procedures, functions, and subroutines General Terms Design, Languages Keywords Referential transparency, Lambda lifting 1. Motivation Nonstandard interpretation is a powerful tool, with a wide varietyof important applications. Typical techniques for performing nonstandard interpretation are compile-time only, require modificationof global resources, or require rewriting of code to abstract over portions subject to nonstandard semantics. This paper proposes aconstruct to support modular run-time nonstandard interpretation...|$|R
40|$|Growing {{computer}} system sizes {{and levels of}} integration have made memory reliability a primary concern, necessitating strong memory error protection. As such, large-scale systems typically employ <b>error</b> <b>checking</b> and correcting <b>codes</b> to trade redundant storage and band-width for increased reliability. While stronger memory protection {{will be needed to}} meet reliability targets in the future, it is undesirable to further increase the amount of storage and bandwidth spent on redundancy. We propose a novel family of single-tier ECC mecha-nisms called Bamboo ECC to simultaneously address the conflicting requirements of increasing reliability while maintaining or decreasing error protection overheads. Relative to the state-of-the-art single-tier error protection, Bamboo ECC codes have superior correction capabilities, all but elim-inate the risk of silent data corruption, and can also increase redun-dancy at a fine granularity, enabling more adaptive graceful down-grade schemes. These strength, safety, and flexibility advantages translate to a significantly more reliable memory system. To demon-strate this, we evaluate a family of Bamboo ECC organizations in the context of conventional 72 b and 144 b DRAM channels and show the significant error coverage and memory lifespan improvements of Bamboo ECC relative to existing SEC-DED, chipkill-correct and double-chipkill-correct schemes. 1...|$|R
40|$|While {{the users}} of {{completed}} applications are heavily moving from desktop {{to the web}} browser, the majority of developers are still working with desktop IDEs such as Eclipse or Visual Studio. In contrast to professional installable IDEs, current web-based code editors are simple text editors with extra features. They usually understand lexical syntax and can do highlighting and indenting, but lack many of the features seen in modern desktop editors. In this paper, we present CoRED, a browser-based collaborative real-time code editor for Java applications. CoRED is a complete Java editor with <b>error</b> <b>checking</b> and automatic <b>code</b> generation capabilities, extended with some features commonly associated with social media. As a proof of the concept, we have extended CoRED to support Java based Vaadin framework for web applications. Moreover, CoRED can be used either as a stand-alone version or {{as a component of}} any other software. It is already used as a part of browser based Arvue IDE. Author Keywords Development tools, collaboration architectures, Vaadin. ACM Classification Keywords D. 2. 3. c. Program editors. D. 3. 2. h. Development tools. H. 5. 3. c. Computer-supported cooperative work. J. 8. s. Web site management/development tools...|$|R
40|$|The {{programming}} language RIGAL {{is a tool}} for parsing, (context condition <b>checking,</b> <b>error</b> recovery), <b>code</b> optimization, code generation and static analysis of {{programs as well as}} programming of preprocessors and translators. In this paper we summarize our experiences of writing compilers and other software in RIGAL and discuss how the RIGAL constructs are used in writing different phases of a compiler. An advanced type system with compile-time and run-time checks is also described. 1. Introduction Compiler construction systems usually are based on the specification of the context-free grammar of the source language. While tools such as YACC [1, 3] and CDL- 2 [6] do perform parsing and attribute computation synchronously, the semantic actions are written in a different language (e. g. C). Attribute grammars are used both for parsing and code generation, e. g. in MUG 2 [7]. Trees with labelled branches present conveniently the abstract syntax of the program in the Vienna semantic definition [...] ...|$|R
40|$|A {{system for}} {{configuring}} telemetry transponder cards uses {{a database of}} <b>error</b> <b>checking</b> protocol data structures, each containing data to implement at least one CCSDS protocol algorithm. Using a user interface, a user selects at least one telemetry specific <b>error</b> <b>checking</b> protocol from the database. A compiler configures an FPGA with {{the data from the}} data structures to implement the <b>error</b> <b>checking</b> protocol...|$|R
40|$|International audienceWe {{investigate}} {{the use of}} Low Rank Parity <b>Check</b> <b>Codes,</b> originally designed for cryptography applications {{in the context of}} Power Line Communication. Particularly, we propose a new code design and an efficient probabilistic decoding algorithm. The main idea of decoding Low Rank Parity <b>Check</b> <b>Codes</b> is based on calculations of vector spaces over a finite field math formula. Low Rank Parity <b>Check</b> <b>Codes</b> {{can be seen as the}} identical of Low Density Parity <b>check</b> <b>codes.</b> We compare the performance of this code against the Reed-Solomon Code through a Power Line Communication channel...|$|R
40|$|Manual by Andrew G. Dean. revised for Version 6. 03, January 1996. Program {{produced}} through {{collaboration between}} The Division of Surveillance and Epidemiology Studies, Epidemiology Program Office, Centers for Disease Control and the Global Programme on AIDS, World Health Organization. Suggested citation: Dean AG, Dean JA, Coulombier D, Brendel KA, SmithDC, Burton AH, Dicker RC, Sullivan K, Fagan RF, Arner, TG. Epi Info, Version 6 : a word processing, database, and statistics program {{for public health}} on IBM compatible microcomputers. Centers for Disease Control and Prevention, Atlanta, Georgia, U. S. A., 1996. Introduction [...] 1. How To Use This Manual [...] 2. What Is Epi Info, Version 6 ? [...] 3. What's New in Version 6 ? [...] 4. Installing Epi Info [...] 5. Running Epi Info [...] [...] Level I: Word Processing Functions [...] 6. Using EPED, the Epidemiologist 22 ̆ 0 ac 2 ̆ 122 s Editor, As aGeneral Word Processor [...] 7. Creating Questionnaires Using EPED [...] [...] Level I: Entering and Analyzing Data Without Programming [...] 8. Entering Data Using the ENTER Program [...] 9. ANALYSIS: Producing Lists, Frequencies, Tables, Statistics, and Graphs from Epi Info Files [...] [...] Level II: More Refined Data Entry and Analysis [...] 10. The <b>CHECK</b> Program: Optional <b>Error</b> <b>Checking,</b> <b>Coding,</b> and Skip Patterns During Data Entry [...] 11. Writing Programs and Preparing Data for ANALYSIS [...] 12. Example of Epidemic Investigation : an Epidemic of Paralytic Shellfish Poisoning [...] 13. More ANALYSIS: Writing New Files; Restructuring Records; Communicating with the Screen and Printer; Summary Records [...] 14. CSAMPLE: Analyzing Data from Complex Survey Samples [...] [...] Level II: Other Epi Info Functions [...] 15. STATCALC and EPITABLE: Two Statistical Calculators [...] 16. EXPORT: Producing Files for Use in Other Database and Statistical Systems [...] 17. IMPORTing Data Files from Other Programs [...] 18. MERGE: Merging and Updating Files and Records [...] 19. Duplicate Data Entry and Validation [...] Two Approaches [...] 20. EPIGLUE/EPI 6 : Menuing and Executive Health Information Shell with Hypertext [...] 21. EPIAID, the Tutorial and Interactive Text Function in EPED [...] [...] Level III: Advanced Features of Epi Info [...] 22. Programming the Data Entry Process with. CHK Files [...] 23. Example: Programs For Nutritional Anthropometry [...] 24. Precise Control of Tables: the REPORT Command [...] 25. Linking Files Together: Relational Features of ANALYSIS, MERGE, and ENTER [...] 26. Example: NETSS, a Disease Surveillance System, using Relational File Structure and Hypertext Output [...] 27. Generating Artificial Data Files and Random Numbers; [...] 28. Writing EPIAID Programs [...] [...] Using Epi Info in Special Environments [...] 29. Using Epi Info with Languages Other Than English [...] 30. Portable Computers and Epidemiologic Field Investigation [...] [...] Reference Section [...] 31. Limitations of Speed and Memory, Debugging, and What To Do When Problems Occur [...] 32. Statistics: Understanding the Results [...] 33. EPED Commands [...] 34. EPIAID Commands [...] 35. ENTER and CHECK Commands [...] 36. ANALYSIS Commands [...] 37. Epi Info File Structure [...] 38. Epi Info and Local Area Networks (LANs) [...] 39. The Programmer 22 ̆ 0 ac 2 ̆ 122 s Toolkit and the REC 2 QES and MAKELIST Utilities [...] [...] Functional Index [...] Epidemic Investigation [...] Disease Surveillance [...] [...] Alphabetic Index...|$|R
50|$|A {{fixed rate}} erasure code, usually {{with a fairly}} high rate, is applied as a 'pre-code' or 'outer code'. This pre-code may itself be a {{concatenation}} of multiple codes, for example in the code standardized by 3GPP a high density parity <b>check</b> <b>code</b> derived from the binary Gray sequence is concatenated with a simple regular low density parity <b>check</b> <b>code.</b> Another possibility would be a concatenation of a Hamming code with a low density parity <b>check</b> <b>code.</b>|$|R
5000|$|... "Data link {{arrangement}} with <b>error</b> <b>checking</b> and retransmission control".|$|R
40|$|Error {{correcting}} codes prevent loss {{of integrity}} in data transmission. Low Density Parity <b>Check</b> <b>codes</b> are {{a family of}} codes that are specified by sparse matrices. Using the Nelder-Mead Downhill Simplex Evolution to design an irregular Low Density Parity <b>Check</b> <b>code,</b> we hope to improve upon the accuracy of decoding...|$|R
40|$|Checking {{procedures}} for processed nuclear data at Los Alamos are described. Both continuous energy and multi-group nuclear data are verified by locally developed <b>checking</b> <b>codes</b> which use basic physics knowledge and common-sense rules. A list of nuclear data problems {{which have been}} identified with help of these <b>checking</b> <b>codes</b> is also given...|$|R
40|$|Abstract—Foward error {{correction}} (FEC) {{scheme based on}} low density parity <b>check</b> <b>codes</b> (LDPC) codes is presented in this paper. We show that LDPC codes provide a significant system performance enhancement {{with respect to the}} state-of-the-art FEC schemes employed in optical communication systems. Index Terms—Forward {{error correction}}, long-haul transmis-sion, low-density parity <b>check</b> <b>codes,</b> optical communications. I...|$|R
5000|$|<b>Checking</b> <b>coding</b> rules (Differences between PMD rule {{violations}} and FaultHunter rule violations) ...|$|R
40|$|A {{high quality}} time code and word clock {{synchronization}} {{is essential to}} prevent audio drop outs and flutters in sound studios. A bad adjustment of time code generators respectively word clock synchronizers requires extensive <b>error</b> <b>checks</b> in synchronization networks. For this reason, a new measurement method is presented which enables sound engineers to measure longitudinal time code jitter and to <b>check</b> time <b>code</b> / word clock synchronization. Annoying audible artifacts in sound studios {{can be traced back}} on measured time code jitter and on loss of synchronization of the used time code / word clock synchronizer. With this method a time code jitter measurement accuracy of +/- 20 microsecond can be achieved. Entnommen aus TEMA</a...|$|R
