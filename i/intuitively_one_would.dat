73|10000|Public
25|$|Two-slit {{diffraction}} is {{a famous}} {{example of the}} strange behaviors that waves regularly display, that are not intuitively associated with particles. The overlapping waves from the two slits {{cancel each other out}} in some locations, and reinforce each other in other locations, causing a complex pattern to emerge. <b>Intuitively,</b> <b>one</b> <b>would</b> not expect this pattern from firing a single particle at the slits, because the particle should pass through one slit or the other, not a complex overlap of both.|$|E
25|$|<b>Intuitively</b> <b>one</b> <b>would</b> want an h {{very close}} to zero, however when using floating-point operations, the {{smallest}} number won't give the best approximation of a derivative. As h grows smaller the difference between f (a + h) and f(a) grows smaller, cancelling out the most significant and least erroneous digits and making the most erroneous digits more important. As a result the smallest number of h possible will give a more erroneous approximation of a derivative than a somewhat larger number. This {{is perhaps the most}} common and serious accuracy problem.|$|E
50|$|In the {{definitions}} above, the convention that 0·log 0 = 0 is assumed, since limx → 0 x log x = 0. <b>Intuitively,</b> <b>one</b> <b>would</b> expect that {{an event of}} zero probability to contribute nothing towards entropy.|$|E
40|$|Though {{the theory}} of quantum error {{correction}} is intimately related to the classical coding theory, in particular, one can construct quantum error correction codes (QECCs) from classical codes with the dual containing property, this does not necessarily imply that the computational complexity of decoding QECCs {{is the same as}} their classical counterparts. Instead, decoding QECCs can be very much different from decoding classical codes due to the degeneracy property. <b>Intuitively,</b> <b>one</b> expect degeneracy <b>would</b> simplify the decoding since two different errors might not and need not be distinguished in order to correct them. However, we show that general quantum decoding problem is NP-hard regardless of the quantum codes being degenerate or non-degenerate. This finding implies that no considerably fast decoding algorithm exists for the general quantum decoding problems, and suggests the existence of a quantum cryptosystem based on the hardness of decoding QECCs. Comment: 5 pages, no figure. Final version for publicatio...|$|R
40|$|The current {{algebraic}} {{models for}} nondeterminism {{focus on the}} notion of possibility rather than necessity, and con sequently equate (nondeterministic) terms that <b>one</b> <b>intuitively</b> <b>would</b> not consider equal. Furthermore, existing models for nondeterminism depart radically from the standard models for (equational) specifications of deterministic operators. <b>One</b> <b>would</b> prefer that a specification language for nondeterministic operators be based on an extension of the standard model concepts, preferably {{in such a way that}} the reasoning system for (possibly nondeterministic) operators becomes the standard equational one whenever restricted to the deterministic operators [...] the objective should be to minimize the departure from the standard frameworks. In this paper we define a specification language for nondeterministic operators and multialgebraic semantics. The first complete reasoning system for such specifications is introduced. We also define a transformation of specifications of nondeterm [...] ...|$|R
30|$|Since FT-IR spectra are of {{the bulk}} material, {{it could be argued}} that there is an {{increase}} in the concentration of weaker (longer), more reactive Mo-S bonds produced with increasing decomposition pressure. While this finding is <b>intuitively</b> unappealing as <b>one</b> <b>would</b> reasonably expect a shortening of bond lengths with increase in pressure (Pietosa et al. 2008), a possible explanation is that pressure induces structural changes which in turn induce creation of new catalytically active sites. Though recent investigations of MoS 2 nanoclusters using direct space DFT calculations have found that an increase in S atom coordination of Mo atom results in increasing Mo-S bond lengths (McBride and Head 2009), no such investigations were carried out in this study. Work is ongoing with a view to quantifying the interlayer rotations observed using XRD and the creation of new catalytically active sites and the unique FT-IR shifts observed through the preparation of more ATM-derived MoS 2 using more discreet changes in decomposition pressures.|$|R
5000|$|When {{two systems}} are in equilibrium, {{they have the}} same {{thermodynamic}} temperature T. Thus <b>intuitively,</b> <b>one</b> <b>would</b> expect β (as defined via microstates) to be related to T in some way. This link is provided by Boltzmann's fundamental assumption written as ...|$|E
50|$|Stochastic {{regression}} was {{a fairly}} successful attempt to correct {{the lack of an}} error term in regression imputation by adding the average regression variance to the regression imputations to introduce error. Stochastic regression shows much less bias than the above-mentioned techniques, but it still missed one thing - if data are imputed then <b>intuitively</b> <b>one</b> <b>would</b> think that more noise should be introduced to the problem than simple residual variance.|$|E
50|$|Two-slit {{diffraction}} is {{a famous}} {{example of the}} strange behaviors that waves regularly display, that are not intuitively associated with particles. The overlapping waves from the two slits {{cancel each other out}} in some locations, and reinforce each other in other locations, causing a complex pattern to emerge. <b>Intuitively,</b> <b>one</b> <b>would</b> not expect this pattern from firing a single particle at the slits, because the particle should pass through one slit or the other, not a complex overlap of both.|$|E
2500|$|<b>Intuitively,</b> <b>one</b> {{can think}} of fX(x)dx as being the {{probability}} of X falling within the infinitesimal interval [...]|$|R
50|$|<b>Intuitively,</b> <b>one</b> {{can think}} of fX(x) dx as being the {{probability}} of X falling within the infinitesimal interval x.|$|R
5000|$|Sometimes, the {{adjunction}} {{is written}} as [...] <b>Intuitively,</b> <b>one</b> {{may think of}} Y as being glued onto X via the map f.|$|R
5000|$|In {{information}} theory, for any classical {{random variable}} , the classical Shannon entropy [...] {{is a measure}} of how uncertain we are about the outcome of [...] For example, if [...] is a probability distribution concentrated at one point, the outcome of [...] is certain and therefore its entropy [...] At the other extreme, if [...] is the uniform probability distribution with [...] possible values, <b>intuitively</b> <b>one</b> <b>would</b> expect [...] is associated with the most uncertainty. Indeed such uniform probability distributions have maximum possible entropy [...]|$|E
5000|$|<b>Intuitively</b> <b>one</b> <b>would</b> want an h {{very close}} to zero, however when using floating-point operations, the {{smallest}} number won't give the best approximation of a derivative. As h grows smaller the difference between f (a + h) and f(a) grows smaller, cancelling out the most significant and least erroneous digits and making the most erroneous digits more important. As a result the smallest number of h possible will give a more erroneous approximation of a derivative than a somewhat larger number. This {{is perhaps the most}} common and serious accuracy problem.|$|E
50|$|Data which {{potentially}} could {{assist in}} {{early detection of}} a bioterrorism event include many categories of information. Health-related data such as that from hospital computer systems, clinical laboratories, electronic health record systems, medical examiner record-keeping systems, 911 call center computers, and veterinary medical record systems could be of help; researchers are also considering the utility of data generated by ranching and feedlot operations, food processors, drinking water systems, school attendance recording, and physiologic monitors, among others. <b>Intuitively,</b> <b>one</b> <b>would</b> expect systems which collect more than one type of data to be more useful than systems which collect only one type of information (such as single-purpose laboratory or 911 call-center based systems), and be less prone to false alarms, and {{this appears to be}} the case.|$|E
40|$|When magicians perform {{spectacles}} {{that seem}} to defy the laws of nature, they do so by manipulating psychological reality. Hence, the principles underlying the art of conjuring are potentially of interest to psychological science. Here, we argue that perceptual and cognitive principles governing how humans experience hidden things and reason about them {{play a central role}} in many magic tricks. Different from tricks based on many other forms of misdirection, which require considerable skill on the part of the magician, many elements of these tricks are essentially self-working because they rely on automatic perceptual and cognitive processes. Since these processes are not directly observable, even experienced magicians may be oblivious to their central role in creating strong magical experiences and tricks that are almost impossible to debunk, even after repeated presentations. We delineate how insights from perceptual psychology provide a framework for understanding why these tricks work so well. Conversely, we argue that studying magic tricks that work much better than <b>one</b> <b>intuitively</b> <b>would</b> believe provides a promising heuristic for charting unexplored aspects of perception and cognition. status: publishe...|$|R
40|$|Consider the {{contrast}} in interpretation between the examples in (1) and (2) below. While example (1) {{is compatible with}} events that unfold in very different ways, the addition of one by one in (2) constrains how the leaving events can proceed. <b>Intuitively,</b> <b>one</b> by one is an event modifier that targets a plural participant in the event...|$|R
40|$|Shrinking process {{technologies}} and growing chip sizes have profound effects on process variation. This leads to Chip Multiprocessors where not all cores operate at maximum frequency. Instead of simply disabling these slower cores or using guard banding (running {{all at the}} frequency of the slowest logic block), we investigate keeping them active, and examine the performance and power efficiency of using frequency-heterogeneous CMPs on multithreaded workloads. With uniform workload partitioning, <b>one</b> <b>intuitively</b> <b>would</b> expect slower cores to be bottlenecks and degrade performance. However, with non-uniform workload partitioning, we find that using both slow and high frequency cores improves performance and reduces energy consumption over just running faster cores. Thread scheduling and workload partitioning naturally play significant roles in these improvements. We find that using under-performing cores improves performance by 14 % while achieving energy savings (over a homogeneous twothread baseline) of up to 16 % across the NAS and SPEC-OMP benchmarks on quad-core AMD platform. Workload balancing via dynamic partitioning yields results within 5 % of the overall ideal value. Finally, we show feasible methods to determine at run-time whether using a heterogeneous configuration is beneficial...|$|R
5000|$|Since {{the time}} of Laplace (1799), {{scientists}} had been puzzled as to why pressure variations measured at the Earth's surface associated with the semi-diurnal solar tide dominate those of the diurnal tide in amplitude, when <b>intuitively</b> <b>one</b> <b>would</b> expect the diurnal (daily) passage of the sun to dominate. Lord Kelvin (1882) had proposed the so-called [...] "resonance" [...] theory, wherein the semi-diurnal tide would be [...] "selected" [...] over the diurnal oscillation if the atmosphere was somehow able to oscillate freely at a period of very close to 12 hours, {{in the same way}} that overtones are selected on a vibrating string. By the second half of the twentieth century, however, observations had failed to confirm this hypothesis, and an alternative hypothesis was proposed that something must instead suppress the diurnal tide. In 1961, Manfred Siebert suggested that absorption of solar insolation by tropospheric water vapour might account for the reduction of the diurnal tide. However, he failed to include a role for stratospheric ozone. This was rectified in 1963 by the Australian physicist Stuart Thomas Butler and his student K.A. Small who showed that stratospheric ozone absorbs an even greater part of the solar insolation.|$|E
5000|$|According to {{a theory}} {{put forward by}} {{economics}} professor Peter Leeson, trial by ordeal may have been effective at sorting the guilty from the innocent. On the assumption that defendants were believers in divine intervention for the innocent, then only the truly innocent would choose to endure a trial; guilty defendants would confess or settle cases instead. Therefore, the theory goes, church and judicial authorities could routinely rig ordeals so that the participants—presumably innocent—could pass them. To support this theory, Leeson points to the great latitude given to the priests in administering the ordeal and interpreting {{the results of the}} ordeal. He also points to the overall high exoneration rate of accused persons undergoing the ordeal, when <b>intuitively</b> <b>one</b> <b>would</b> expect a very high proportion of people carrying a red hot iron to be badly burned and thus fail the ordeal. Peter Brown explains the persistence and eventual withering of the ordeal by stating that it helped promote consensus in a society where people lived in close quarters and there was little centralized power. In a world where [...] "the sacred penetrated into the chinks of the profane and vice-versa" [...] the ordeal was a [...] "controlled miracle" [...] that served as a point of consensus when one of the greatest dangers to the community was feud. From this analysis, Brown argues that the increasing authoritativeness of the state lessened the need and desire for the ordeal as an instrument of consensus, which ultimately led to its disappearance.|$|E
50|$|Both {{paradigms}} acknowledge a {{role for}} all mechanisms (except possibly for that of random selection of niches in the first paradigm), but emphasis on the various mechanisms varies. The first paradigm stresses the paramount importance of interspecific competition, whereas the second paradigm tries to explain many cases which {{are thought to be}} due to competition in the first paradigm, by reinforcement of reproductive barriers and/or random selection of niches. - Many authors believe in the overriding importance of interspecific competition. <b>Intuitively,</b> <b>one</b> <b>would</b> expect that interspecific competition is of particular importance in all those cases in which sympatric species (i.e., species occurring together in the same area) with large population densities use the same resources and largely exhaust them. However, Andrewartha and Birch (1954,1984) and others have pointed out that most natural populations usually don’t even approach exhaustion of resources, and too much emphasis on interspecific competition is therefore wrong. Concerning the possibility that competition has led to segregation in the evolutionary past, Wiens (1974, 1984) concluded that such assumptions cannot be proven, and Connell (1980) found that interspecific competition as a mechanism of niche segregation has been proven only for some pest insects. Barker (1983), in his review of competition in Drosophila and related genera, which are among the best known animal groups, concluded that the idea of niche segregation by interspecific competition is attractive, but that no study has yet been able to show a mechanism responsible for segregation. Without specific evidence, the possibility of random segregation can never be excluded, and assumption of such randomness can indeed serve as a null-model. - Many physiological and morphological differences between species can prevent hybridization. Evidence for niche segregation as the result of reinforcement of reproductive barriers is especially convincing in those cases in which such differences are not found in allopatric but only in sympatric locations. For example, Kawano (2002) has shown this for giant rhinoceros beetles in Southeast Asia. Two closely related species occur in 12 allopatric (i.e., in different areas) and 7 sympatric (i.e., in the same area) locations. In the former, body length and length of genitalia are practically identical, in the latter, they are significantly different, and much more so for the genitalia than the body, convincing evidence that reinforcement is an important factor (and possibly the only one) responsible for niche segregation. - The very detailed studies of communities of Monogenea parasitic on the gills of marine and freshwater fishes by several authors have shown the same. Species use strictly defined microhabitats and have very complex copulatory organs. This and the fact that fish replicas are available in almost unlimited numbers, makes them ideal ecological models. Many congeners (species belonging to the same genus) and non-congeners were found on single host species. The maximum number of congeners was nine species. The only limiting factor is space for attachment, since food (blood, mucus, fast regenerating epithelial cells) is in unlimited supply as long as the fish is alive. Various authors, using a variety of statistical methods, have consistently found that species with different copulatory organs may co-occur in the same microhabitat, whereas congeners with identical or very similar copulatory organs are spatially segregated, convincing evidence that reinforcement and not competition is responsible for niche segregation.|$|E
3000|$|... 2 <b>Intuitively,</b> <b>one</b> {{circumvents}} {{the expensive}} matrix-matrix multiplication with a domino-like chain of 2 J− 1 (less expensive) matrix-vector multiplications per transmitted symbol vector. This became possible by replacing the inverse of a matrix-matrix multiplication in the RZF with a sum of weighted matrix powers.|$|R
5000|$|<b>Intuitively,</b> <b>one</b> {{can think}} of the radical of I as {{obtained}} by taking all the possible roots of elements of I. Equivalently, the radical of I is the pre-image of the ideal of nilpotent elements (called nilradical) in [...] The latter shows [...] is an ideal itself, containing I.|$|R
40|$|<b>Intuitively</b> <b>one</b> {{might expect}} {{that the quality of}} {{statistical}} estimates can not worsen if they are based on more data. We show in a least-squares linear regression setting that this intuition is wrong. Adding data may worsen the quality of parameter estimates, and in fact may even cause a design sequence to lose strong consistency...|$|R
40|$|One of {{the main}} issues in the {{discussion}} on standard deontic logic (SDL) is the representation of contrary-to-duty (CTD) obligations. A well-known example is Forrester’s (1984) paradox of the gentle murderer: it is forbidden to kill, but if one kills, one ought to kill gently. <b>Intuitively,</b> <b>one</b> <b>would</b> feel that thes...|$|E
30|$|<b>Intuitively,</b> <b>one</b> <b>would</b> {{expect the}} most “honest” signals {{to come from}} users who are most open about sharing {{anything}} on social media. Given that someone shares details about their food consumption, the movies they watch, their {{day at the office}} and so on, we might also expect them to truthfully report on being sick for example. We hypothesize that users with a significant fraction of tweets about their private, daily lives provide better data for now-casting than more reserved users.|$|E
3000|$|... [...]. A large {{value of}} disconnectivity {{indicates}} a better {{separation of the}} point sets. The projections onto all of the eigenvectors should be examined as {{we do not know}} a priori which direction to follow while splitting. Although <b>intuitively</b> <b>one</b> <b>would</b> suggest to split along the direction of the principal axis, we observed that in many cases that approach was not the best. Also, let us note that as the ordering of the points is not known a priori, their projection onto the eigenvectors of their covariance matrix, provides a natural way of ordering.|$|E
5000|$|The second {{derivative}} test consists here of sign {{restrictions of}} {{the determinants of}} a certain set of n - m submatrices of the bordered Hessian. <b>Intuitively,</b> <b>one</b> can think of the m constraints as reducing the problem to one with n - m free variables. (For example, the maximization of [...] subject to the constraint [...] {{can be reduced to}} the maximization of [...] without constraint.) ...|$|R
5000|$|This has {{the form}} of a {{logarithmic}} derivative. <b>Intuitively,</b> <b>one</b> may think of t as the logarithm of some element s of F, in which case, this condition is analogous to the ordinary chain rule. But it must be remembered that F is not necessarily equipped with a unique logarithm; one might adjoin many [...] "logarithm-like" [...] extensions to F. Similarly, an exponential extension is a simple transcendental extension that satisfies ...|$|R
2500|$|This fact gives {{a general}} {{procedure}} for constructing any Frenet ribbon. [...] <b>Intuitively,</b> <b>one</b> can {{cut out a}} curved ribbon from a flat piece of paper. [...] Then by bending the ribbon out into space without tearing it, one produces a Frenet ribbon. [...] In the simple case of the slinky, the ribbon is several turns of an annulus in the plane, and bending it up into space corresponds to stretching out the slinky.|$|R
40|$|Abstract: Sets {{can play}} an {{important}} role in circumscription’s ability to deal in a general way with certain aspects of commonsense reasoning. Aresult of Kueker indicates that sentences that <b>intuitively</b> <b>one</b> <b>would</b> want circumscription to prove, are nonetheless not so provable in a formal setting devoid of sets. Furthermore, when sets are introduced, first-order circumscription handles these cases very easily, obviating the need for second-order circumscription. The Aussonderungs axiom of ZF set theory plays an intuitive role in this shift back to a first-order language. descriptors: commonsense, circumscription, sets I...|$|E
40|$|<b>Intuitively,</b> <b>one</b> <b>would</b> {{expect that}} {{internet}} search volume would contain valuable information about investor sentiment for a company. With {{the development of}} new data sources, such as Google Trends, this relationship can be more easily and objectively examined. This paper seeks {{to examine the relationship between}} a company’s stock price volatility and its Google search volume. A small cross-section of twenty companies is considered, and the goal of this paper is to demonstrate the power of Google Trends data in hope of initiating further research. Using a conventional GARCH framework for financial market volatility, an economically and statistically significant contemporaneous relationship between Google search volume and equity volatility is found...|$|E
40|$|Suppose {{there exists}} a market for yield futures {{contracts}} as well as ordinary futures contracts for price. <b>Intuitively</b> <b>one</b> <b>would</b> think that a combined use of yield contracts and and futures price contracts ought to provide a reasonable strategy for locking in revenue. In the paper this is made precise - it is shown that revenue can be approximately locked in by a combined, dynamic use of these to markets. This procedure is perfect if the correlation between yield and price is zero. The relevant strategy is characterized: It depends only on observable price information in these two separate markets, not on the specification of parameters in utility functions of the agents involved...|$|E
40|$|<b>Intuitively,</b> <b>one</b> is {{inclined}} to think that traffic-responsive signal control is the most efficient control policy. In this paper, however, we show that for an intersection of two routes connecting one origin-destination pair where only one route is subject to congestion, anticipatory signal control performs better than traffic-responsive signal control. Furthermore, the unfolded logic behind this result suggests that the superiority of anticipatory signal control also extends to other networks. status: publishe...|$|R
40|$|What is the {{physical}} {{shape of the}} π-electron system in a large macromolecule such as a conjugated polymer? Although <b>intuitively</b> <b>one</b> may argue that any departure from rigidity by bending or twisting should disrupt conjugation, leading {{to the formation of}} discrete chromophores, single-molecule and ensemble time-resolved studies support the notion that the π-bond is remarkably persistent in space: even individual chromophores can be bent and twisted, so that caution is warranted when interpreting a wide range of polarisation-based spectroscopies...|$|R
50|$|<b>Intuitively,</b> <b>one</b> {{can view}} a unit as the {{smallest}} testable {{part of an}} application. In procedural programming, a unit could be an entire module, {{but it is more}} commonly an individual function or procedure. In object-oriented programming, a unit is often an entire interface, such as a class, but could be an individual method. Unit tests are short code fragments created by programmers or occasionally by white box testers during the development process. It forms the basis for component testing.|$|R
