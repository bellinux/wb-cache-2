1670|928|Public
5|$|USB is {{a serial}} bus, using four {{shielded}} wires for the USB2.0 variant: two for power (VBUS and GND), and two for differential data signals (labelled as D+ and D− in pinouts). Non-Return-to-Zero Inverted (NRZI) <b>encoding</b> <b>scheme</b> {{is used for}} transferring data, with a sync field to synchronize the host and receiver clocks. D+ and D− signals are transmitted on a differential pair, providing half-duplex data transfers for USB2.0. Mini and micro connectors have their GND connections moved from pin #4 to pin #5, while their pin #4 serves as an ID pin for the On-The-Go host/client identification.|$|E
25|$|In 1992, Thompson {{developed}} the UTF-8 <b>encoding</b> <b>scheme</b> together with Rob Pike. The UTF-8 encoding {{has since become}} the dominant character encoding for the World Wide Web, accounting {{for more than half}} of all web pages.|$|E
25|$|In {{computer}} text applications, the GB <b>encoding</b> <b>scheme</b> {{most often}} renders simplified Chinese characters, while Big5 most often renders traditional characters. Although neither encoding has an explicit {{connection with a}} specific character set, {{the lack of a}} one-to-one mapping between the simplified and traditional sets established a de facto linkage.|$|E
30|$|The {{quantization}} step {{is unique to}} irreversible <b>encoding</b> <b>schemes.</b> Reversible <b>encoding</b> <b>schemes</b> do not perform any quantization of coefficients; without quantization, and without loss of precision in intermediate steps (including decorrelation and colour space transformation), all values are encoded without loss.|$|R
50|$|In Parquet, {{compression}} {{is performed}} column by column hence enabling different <b>encoding</b> <b>schemes</b> {{to be used}} for text and integer data. In addition this strategy also keeps the door open for newer and better <b>encoding</b> <b>schemes</b> to be implemented as they are invented.|$|R
50|$|In direct <b>encoding</b> <b>schemes</b> the {{genotype}} directly maps to the phenotype. That is, every neuron {{and connection}} in the neural network is specified directly and explicitly in the genotype. In contrast, in indirect <b>encoding</b> <b>schemes</b> the genotype specifies indirectly how that network should be generated.|$|R
25|$|A modern HDD records data by {{magnetizing}} a {{thin film}} of ferromagnetic material on a disk. Sequential changes {{in the direction of}} magnetization represent binary data bits. The data is read from the disk by detecting the transitions in magnetization. User data is encoded using an <b>encoding</b> <b>scheme,</b> such as run-length limited encoding, which determines how the data is represented by the magnetic transitions.|$|E
25|$|In the Republic of China (Taiwan), {{which uses}} {{traditional}} Chinese characters, the Ministry of Education's Chángyòng Guózì Biāozhǔn Zìtǐ Biǎo (常用國字標準字體表, Chart of Standard Forms of Common National Characters) lists 4,808 characters; the Cì Chángyòng Guózì Biāozhǔn Zìtǐ Biǎo (次常用國字標準字體表, Chart of Standard Forms of Less-Than-Common National Characters) lists another 6,341 characters. The Chinese Standard Interchange Code (CNS11643)—the official national encoding standard—supports 48,027 characters, while {{the most widely}} used <b>encoding</b> <b>scheme,</b> BIG-5, supports only 13,053.|$|E
25|$|The 270Mbit/s {{interface}} supports 525-line, interlaced video at a 59.94Hz field rate (29.97Hz frame rate), and 625-line, 50Hz interlaced video. These formats {{are highly}} compatible with NTSC and PAL-B/G/D/K/I systems respectively; and the terms NTSC and PAL are often (incorrectly) {{used to refer}} to these formats. (PAL is a composite color <b>encoding</b> <b>scheme,</b> and the term does not define the line-standard, though it is most usually encountered with 625i) while the serial digital interface— other than the obsolete 143Mbit/s and 177Mbit/s forms- is a component standard).|$|E
5000|$|... {{support for}} various {{character}} <b>encoding</b> <b>schemes</b> including Unicode, ...|$|R
50|$|<b>Encoding</b> <b>schemes</b> {{are used}} to convert {{coordinate}} integers into binary form to provide additional compression gains. Encoding designs, such as the Golomb code and the Huffman code, have been incorporated into genomic data compression tools. Of course, <b>encoding</b> <b>schemes</b> entail accompanying decoding algorithms. Choice of the decoding scheme potentially affects the efficiency of sequence information retrieval.|$|R
40|$|The {{processing}} of {{speech in the}} mammalian auditory periphery is {{discussed in terms of}} the spatio-temporal nature of the distribution of the cochlear response and the novel <b>encoding</b> <b>schemes</b> this permits. Algorithms to detect specific morphological features of the response and the novel <b>encoding</b> <b>schemes</b> of the response patterns are also considered for the extraction of stimulus spectral parameters...|$|R
25|$|Modern HDDs {{present a}} {{consistent}} interface {{to the rest}} of the computer, no matter what data <b>encoding</b> <b>scheme</b> is used internally. Typically a DSP in the electronics inside the HDD takes the raw analog voltages from the read head and uses PRML and Reed–Solomon error correction to decode the sector boundaries and sector data, then sends that data out the standard interface. That DSP also watches the error rate detected by error detection and correction, and performs bad sector remapping, data collection for Self-Monitoring, Analysis, and Reporting Technology, and other internal tasks.|$|E
25|$|Since {{simplified}} Chinese conflated many characters {{into one}} {{and since the}} initial version of the GB <b>encoding</b> <b>scheme,</b> known as GB2312-80, contained only one code point for each character, {{it is impossible to}} use GB2312 to map to the bigger set of traditional characters. It is theoretically possible to use Big5 code to map to the smaller set of simplified character glyphs, although there is little market for such a product. Newer and alternative forms of GB have support for traditional characters. In particular, mainland authorities have now established GB 18030 as the official encoding standard for use in all mainland software publications. The encoding contains all East Asian characters included in Unicode 3.0. As such, GB 18030 encoding contains both simplified and traditional characters found in Big-5 and GB, as well as all characters found in Japanese and Korean encodings.|$|E
500|$|Mathematician Alan Turing, who {{had been}} {{appointed}} to the nominal post of Deputy Director of the Computing Machine Laboratory at the University of Manchester in September 1948, devised a base 32 <b>encoding</b> <b>scheme</b> based on the standard ITA2 5-bit teleprinter code, which allowed programs and data to be written to and read from paper tape. The ITA2 system maps each of the possible 32 binary values that can be represented in 5 bits (25) to a single character. Thus [...] "10010" [...] represents [...] "D", [...] "10001" [...] represents [...] "Z", and so forth. Turing changed {{only a few of}} the standard encodings; for instance, 00000 and 01000, which mean [...] "no effect" [...] and [...] "linefeed" [...] in the teleprinter code, were represented by the characters [...] "/" [...] and [...] "@" [...] respectively. Binary zero, represented by the forward slash, was the most common character in programs and data, leading to sequences written as [...] "///////////////". One early user suggested that Turing's choice of a forward slash was a subconscious choice on his part, a representation of rain seen through a dirty window, reflecting Manchester's [...] "famously dismal" [...] weather.|$|E
40|$|Several binary rule <b>encoding</b> <b>schemes</b> {{have been}} {{proposed}} for Pittsburgh-style classifier systems. This paper focus on the analysis of how rule encoding may bias the scalability of learning maximally general and accurate rules by classifier systems. The theoretical analysis of maximally general and accurate rules using two different binary rule <b>encoding</b> <b>schemes</b> showed some theoretical results with clear implications to the scalability of any genetic-based machine learning system that uses the studied <b>encoding</b> <b>schemes.</b> Such results are clearly relevant since one of the binary representations studied is widely used on Pittsburgh-style classifier systems, and shows an exponential shrink of the useful rules available as the problem size increases...|$|R
2500|$|... — Roman Czyborra's site {{contains}} an exhaustive history of Cyrillic character set <b>encoding</b> <b>schemes.</b>|$|R
5000|$|<b>Encoding</b> <b>schemes</b> 8b/10b {{have found}} a heavy use in digital audio storage applications, namely ...|$|R
2500|$|First clue: [...] "Let us put the {{description}} of M into the first standard form of §6". Section 6 describes the very specific [...] "encoding" [...] of machine M on the tape of a [...] "universal machine" [...] U. This requires the reader to know some idiosyncrasies of Turing's universal machine U and the <b>encoding</b> <b>scheme.</b>|$|E
2500|$|Because XOR is linear, an {{attacker}} {{may be able}} {{to manipulate}} an encoded pointer by overwriting only the lower bytes of an address. [...] This can allow an attack to succeed if the attacker is able to attempt the exploit multiple times or is able to complete an attack by causing a pointer to point to one of several locations (such as any location within a NOP sled). [...] Microsoft added a random rotation to their <b>encoding</b> <b>scheme</b> to address this weakness to partial overwrites.|$|E
2500|$|The Varicode {{is a kind}} of Fibonacci code {{where the}} {{boundaries}} between character codes are marked by two or more consecutive zeros. Like all Fibonacci codes, since no character code contains more than one consecutive zero, the software can easily identify the spaces between characters, regardless of the length of the character. The idle sequence, sent when an operator is not typing, is a continuous sequence of phase-shifts, which do not print on the screen. Martinez arranged the character alphabet so that, as in Morse code, the more frequently occurring characters have the shortest encodings, while rarer characters use longer encodings. [...] He named this <b>encoding</b> <b>scheme</b> [...] "varicode".|$|E
40|$|In {{order to}} image complex {{geological}} structures, seismic sur-veys acquire an increasingly {{large amount of}} data. While the resulting data sets enable higher-resolution images of the sub-surface, they also contain redundant information and require large computational resources for processing. One approach for mitigating this trend is blended imaging, which combines the original shot records into {{a smaller number of}} blended shots at the expense of crosstalk in the final image. Since the cost of imaging is roughly proportional to the number of shots, blended imaging directly leads to a faster imaging process. In contrast to the existing shot <b>encoding</b> <b>schemes,</b> we establish a novel connection between blended imaging and dimension-ality reduction using the Johnson-Lindenstrauss lemma. We introduce three new shot <b>encoding</b> <b>schemes</b> based on random projections and evaluate their performance. Our experiments on three data sets show that our random shot <b>encoding</b> <b>schemes</b> are competitive with existing shot <b>encoding</b> <b>schemes</b> and out-perform decimated shot encoding for small numbers of shots...|$|R
40|$|Due {{to larger}} buses (length, width) and deep {{sub-micron}} effects where coupling capacitances between bus lines {{are in the}} same order of magnitude as base capacitances, power consumption of interconnects starts to {{have a significant impact on}} a system's total power consumption. We present novel address bus <b>encoding</b> <b>schemes</b> that take coupling effects into consideration. The basis is a physical bus model that quantifies coupling capacitances. As a result, we report power/energy savings on the address buses of up to 56 % compared to the best known ordinary power/energy efficient <b>encoding</b> <b>schemes.</b> Thereby, we exceed the only to-date approach that also takes coupling effects into consideration. Moreover, our <b>encoding</b> <b>schemes</b> do not assume any a priori knowledge that is particular to a specific application...|$|R
50|$|It {{is used in}} many video <b>encoding</b> <b>schemes</b> — both analog {{and digital}} — and also in JPEG encoding.|$|R
2500|$|In {{computers}} and telecommunication systems, writing systems {{are generally not}} codified as such, but graphemes and other grapheme-like units that are required for text processing are represented by [...] "characters" [...] that typically manifest in encoded form. There are many , such as ISO/IEC 8859-1 (a character repertoire and <b>encoding</b> <b>scheme</b> oriented toward the Latin script), CJK (Chinese, Japanese, Korean) and bi-directional text. Today, many such standards are re-defined in a collective standard, the ISO/IEC 10646 [...] "Universal Character Set", and a parallel, closely related expanded work, The Unicode Standard. Both are generally encompassed by the term Unicode. In Unicode, each character, in every language's writing system, is (simplifying slightly) given a unique identification number, known as its code point. Computer operating systems use code points to look up characters in the font file, so the characters can be displayed on the page or screen.|$|E
2500|$|In 1938 Georges Valensi {{demonstrated}} an <b>encoding</b> <b>scheme</b> {{that would allow}} color broadcasts to be encoded {{so they could be}} picked up on existing black-and-white sets as well. In his system the output of the three camera tubes were re-combined to produce a single [...] "luminance" [...] value that was very similar to a monochrome signal and could be broadcast on the existing VHF frequencies. The color information was encoded in a separate [...] "chrominance" [...] signal, consisting of two separate signals, the original blue signal minus the luminance (B'–Y'), and red-luma (R'–Y'). These signals could then be broadcast separately on a different frequency; a monochrome set would tune in only the luminance signal on the VHF band, while color televisions would tune in both the luminance and chrominance on two different frequencies, and apply the reverse transforms to retrieve the original RGB signal. The downside to this approach is that it required a major boost in bandwidth use, something the FCC was interested in avoiding.|$|E
5000|$|Unicode Transformation Format (UTF) (similar <b>encoding</b> <b>scheme)</b> ...|$|E
30|$|GA is {{normally}} operated on coded variable values {{rather than on}} actual ones. The literature survey shows that there exists many <b>encoding</b> <b>schemes,</b> but {{two of them are}} commonly used. These are binary encoding and real encoding [12]. The performance of GA is highly dependent on these <b>encoding</b> <b>schemes</b> which {{play a vital role in}} determining the reliability of GA [16]. An inadequately coded GA may not deliver the results in expected time [12].|$|R
30|$|The {{encapsulation}} and <b>encoding</b> <b>schemes</b> are {{stated in}} the DVB-H standard [8]. The design of the decoding method, however, is open.|$|R
30|$|This paper {{introduces}} {{a family of}} pure analog chaotic dynamic <b>encoding</b> <b>schemes</b> based on the baker’s map function. We first discuss the coding scheme using the original baker’s map function, including its <b>encoding</b> and decoding <b>schemes.</b> Mean square error analysis indicates that the intrinsic unbalanced protection of its input results in an unsatisfying performance. Based on that, two improvement <b>encoding</b> <b>schemes</b> are proposed—mirrored baker’s and single-input baker’s system. These two schemes provide sufficient protection to all encoded analog sources. The various decoding methods for the original baker’s coding system are extended to the modified systems. Compared to the classical tent map analog code, the improved baker’s map <b>encoding</b> <b>schemes</b> achieve a better balance between the anomalous and weak distortion and have advantageous performance in a wide practical SNR range. Moreover, our improved <b>encoding</b> <b>schemes</b> also exhibit competition or even better performance than the classical analog joint source-channel coding scheme, especially in the low SNR range, while maintaining much lower complexity in the decoding procedure. We also compare the analog and conventional digital systems using turbo code to transmit analog source signals. The digital systems suffer from granularity noise due to quantization, large decoding latency, and threshold effect. Comparatively, the analog coding scheme has a graceful performance degradation and outperforms over a wide SNR region.|$|R
5000|$|CHARSET_ENCODING: Registry's {{character}} <b>encoding</b> <b>scheme</b> {{for this}} set ...|$|E
5000|$|... #Subtitle level 2: SIT example {{recordings}} and <b>encoding</b> <b>scheme</b> ...|$|E
5000|$|Unicode (Unicode) - A {{character}} <b>encoding</b> <b>scheme</b> {{that encompasses}} all character sets.|$|E
40|$|We fill {{a gap in}} the {{systematically}} analyzed {{space of}} available techniques for state-of-the-art dependency parsing by comparing non-projective strategies for graph-based parsers. Using three languages with varying frequency of non-projective constructions, we compare the non-projective approximation algorithm with pseudo-projective parsing. We also analyze the differences between different <b>encoding</b> <b>schemes</b> for pseudo-projective parsing. We find only minor differences between the <b>encoding</b> <b>schemes</b> for pseudo-projective parsing, and that the non-projective approximation algorithm is superior to pseudo-projective parsing...|$|R
50|$|The {{government}} of Tamil Nadu endorses its own TAB/TAM standards for 8-bit encoding and other, older <b>encoding</b> <b>schemes</b> {{can still be}} found on the WWW.|$|R
50|$|New <b>encoding</b> <b>schemes</b> allow {{reducing}} {{data rate}} even further. For example, MPEG-4/AVC {{is considered to}} be twice as efficient as MPEG-2, originally used for HD broadcast.|$|R
