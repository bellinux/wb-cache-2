4|10000|Public
50|$|The TPU ASICs {{are mounted}} in a {{heatsink}} assembly, which can {{fit in a}} hard drive slot within a <b>data</b> <b>center</b> <b>rack,</b> according to Google Distinguished Hardware Engineer Norm Jouppi.|$|E
40|$|This work reports an {{overview}} about the contributions that photonics can add for {{the evolution of}} Data Centers, and it is pointed out how the combination of new photonic devices and optical communications can be considered as a valid recipe for high performance Data Center and their integration in the telecommunication network. In particular optical communication systems can satisfy all the aspects regarding the increasingly intelligent, low-latency and high-bandwidth connections among data centers. Furthermore, inside the <b>data</b> <b>center</b> <b>rack,</b> multiple processing cores are being integrated into a single photon-based chip to achieve higher processing power and energy consumption reduction...|$|E
30|$|Figure  1 [33] {{presents}} a typical data center model with {{the configuration of}} a cold aisle and airflow pattern. There {{are a number of}} Cloud Users (CUs) who rent servers from a Cloud Provider (CP) to obtain required computing capacity. In response, the CP will turn on a certain number of servers which generate waste heat. To remove the waste heat inside the <b>data</b> <b>center,</b> <b>rack</b> fans suck the cold air provided by CRAC and blow it through servers. Then, the cold air absorbs the waste heat and is ejected from the rack rear. CRAC consumes a large amount of energy in this process. To guarantee the QoS and system reliability, data center operation must comply with the QoS and “soft” temperature constraints.|$|E
5000|$|M9000 - Up to 64 {{processor}} sockets, {{one or two}} <b>data</b> <b>center</b> <b>racks</b> ...|$|R
40|$|The thermal {{performance}} of <b>data</b> <b>centers</b> is numerically studied for different configurations of computer room air conditioning (CRAC) units and physical separations of cold and hot aisles. Temperature distribution, {{air flow characteristics}} and thermal management of <b>data</b> <b>centers</b> <b>racks</b> array are predicted and evaluated for the different arrangements. Measureable performance indices: supply/return heat index (SHI/RHI), return temperature index (RTI) and return cooling index (RCI) are {{used to measure the}} thermal management effectiveness of <b>data</b> <b>center</b> <b>racks.</b> The results showed that: (i) hot air recirculation, cold air bypass and the measurable performance indices of the racks strongly depend on the racks location in the racks array, (ii) the CRACs units layout affects the thermal managements of the racks array especially the sides and middle racks in the array, and (iii) using cold aisle containments enhances the {{thermal performance}} of the <b>data</b> <b>center...</b>|$|R
40|$|We {{introduce}} SLIM, {{a hybrid}} storage system that uses disks within <b>data</b> <b>center</b> <b>racks</b> to implement persistent caches for storage area networks (SANs). SLIM leverages compression and block overwrites to reduce traffic on <b>data</b> <b>center</b> oversubscribed network links {{and to improve}} performance for low-cost SANs. We evaluate SLIM using various microbenchmarks and industry standard benchmarks. Our results show that SLIM reduces network storage traffic by 40 %- 90 % and significantly increases application performance in bandwidth-constrained environments. 1...|$|R
40|$|Abstract—Both {{performance}} and energy cost are important concerns for current data center operators. Traditionally, however, IT and mechanical engineers have separately optimized the cyber vs. physical aspects of data center operations. In contrast, the work {{presented in this}} paper jointly considers both the IT- cyber- and the physical systems in data centers, the eventual goal being to develop {{performance and}} power management techniques that holistically operate to control the entire complex of data center installations. Toward this end, we propose a balance of payments model for holistic power and performance management. As an example of coordinated data center management system, the energy-aware cyber-physical system (EaCPS) uses an application controller on the cyber side to guarantee application performance, and on the physical side, it utilizes electric current-aware capacity management (CACM) to smartly place executables to reduce the energy consumption of each chassis present in a <b>data</b> <b>center</b> <b>rack.</b> A web application, representative of a multi-tier web site, is used to evaluate the performance of the controller on the cyber side, the CACM control on the physical side, and of the holistic EaCPS methods in a mid-size, instrumented data center. Results indicate that coordinated EaCPS outperforms the cyber and physical control modules working separately...|$|E
5000|$|In September 2013, Ebay {{announced}} that they were [...] "deploying the world’s largest modular <b>data</b> <b>center,</b> with 44 <b>rack</b> positions and 1.4Megawatts of power" [...] using HP EcoPODs.|$|R
40|$|Graduation date: 2015 Worldwide, many {{organizations}} are pursuing {{higher energy efficiency}} by reducing power consumption of their processes, systems, and supporting infrastructure. The rapid growth of the information technology (IT) industry and the miniaturization of semiconductors have resulted in substantial increases in energy consumption and power density of IT equipment, and, subsequently, heat generated by <b>data</b> <b>center</b> equipment contained within <b>data</b> <b>center</b> <b>racks.</b> Energy efficiency and thermal management effectiveness are two major issues facing <b>data</b> <b>centers</b> due to increases in heat dissipated from <b>data</b> <b>center</b> <b>racks.</b> Higher <b>data</b> <b>center</b> energy efficiency will lower {{total cost of ownership}} (TCO) and enable organizations to better manage increasing computing and network demands. To improve <b>data</b> <b>center</b> energy efficiency, efforts have been focused on novel center-level and rack-level cooling technologies to remove the heat generated by high-density servers. The research presented herein investigates the operational energy performance of a <b>data</b> <b>center</b> evaporative cooling system and the manufacturing energy requirements for a server-scale microchannel heat exchanger (MCHX). Energy monitoring and analysis was conducted to evaluate an evaporative cooling system installed at a <b>data</b> <b>center</b> located in Gresham, OR. A holistic metric and measurement approach is developed to evaluate the impact of changes for <b>data</b> <b>center</b> infrastructure and information technology (IT) equipment. It was found that the developed metric is more responsive to changes in cooling power and environmental conditions than commonly used metrics. Further, the evaporative cooling technology was shown to be more efficient and effective than conventional cooling technology. Liquid cooling has been demonstrated as an effective strategy to provide a reliable environment for servers and to reduce the load on conventional cooling systems. While microchannel process technology (MPT) -based devices offer a space-efficient approach to liquid cooling of high-density servers, MPT device manufacturing, in particular device patterning and bonding, {{has been shown to be}} energy intensive. A weld depth model for bonding of MPT devices is developed and used to understand the capabilities and limitations of the laser welding process. Energy analysis is conducted for the production of a MCHX device to liquid cool the warm exiting air from server racks. Analysis of the patterning, photochemical machining (PCM), and bonding, diffusion bonding and laser welding, processes revealed a considerable reduction in cumulative energy demand (CED) and global warming potential (GWP) when laser welding is used in place of diffusion bonding. This environmental impact reduction was due to reduced process time, reduced energy use, and improved process yield...|$|R
5000|$|In {{contrast}} to large <b>data</b> <b>centers</b> or <b>rack</b> servers, the mobile server {{is designed for}} on-the-road development and ad hoc deployment. It allows for quick deployment and can be easily transported (hand carried) into emergency, disaster or temporary environments where traditional servers are not feasible due to their power requirements, size, etc. The main beneficiaries of so-called [...] "server on the go" [...] technology include network managers, software or database developers, training centers, military personnel, law enforcement, forensics, emergency relief and service organizations.|$|R
40|$|In <b>Data</b> <b>Centers</b> each <b>rack</b> uses a Top-of-Rack (ToR) as a {{first level}} switch to connect servers to the {{aggregation}} switches. The current paper focuses on a hybrid electrical/optical ToR design, which first, adapts the servers Ethernet traffic to the optical TDMA operation of the core network for supporting optical switching in the <b>Data</b> <b>Center’s</b> upper layer and second, it employs optical switching of traffic at the rack level. The proposed ToR architecture {{is based on an}} Ethernet switch and FPGA port extensions realizing the required functions to support 20 10 Gbps connections, exploit the network routing resources and handle effectively virtual queues...|$|R
40|$|This chapter {{presents}} {{a study on}} examining and improving thermal efficiency of a <b>data</b> <b>center.</b> It reports {{the development of a}} computational fluid dynamics (CFD) model for the performance analysis of computer room air conditioners, detailed rack-by-rack inlet and exit temperatures and 3 D thermal mapping of the <b>data</b> <b>center</b> and <b>racks.</b> In particular, the model identified the potential high-temperature zone within the computer rack and provided a detailed 3 D analysis of how cold air moved through the <b>data</b> <b>center.</b> The study determined the impact on cooling resources such as equipment layout, air flow rate, floor tiles, heat load distribution, and other supplementary cooling strategies. It also proposed temperature estimates for given rack loadings. The developed CFD model can be used to develop cooling strategies for achieving better thermal performance in <b>data</b> <b>centers...</b>|$|R
40|$|The {{purpose of}} this thesis is to examine {{possible}} waste heat recovery methods in <b>data</b> <b>centers.</b> Predictions indicate {{that in the next}} decade <b>data</b> <b>center</b> <b>racks</b> may dissipate 70 kW of heat, up from the current levels of 10 - 15 kW. Due to this increase, solutions must be found to increase the efficiency of <b>data</b> <b>center</b> cooling. This thesis will examine possible waste heat recovery technologies which will improve energy efficiency. Possible approaches include phase change materials, thermoelectrics, thermomagnetics, vapor compression cycles, absorbtion and adsorbtion systems. After a thorough evaluation of the possible waste heat engines, the use of an ejector heat pump was evaluated in detail. The principle behind an ejector heat pump is very similar to a vapor compression cycle. However, the compressor is replaced with a pump, boiler and an ejector. These three components require less moving parts and are more cost effective then a comparable compressor, despite a lower efficiency. This system will be examined under general operating conditions in a <b>data</b> <b>center.</b> The heat load is around 15 - 20 kW and air temperatures near 85 °C. A parametric study is conducted to determine the viability and cost effectiveness of this system in the <b>data</b> <b>center.</b> Included will be various environmentally friendly working fluids that satisfy the low temperature ranges found in a <b>data</b> <b>center.</b> It is determined that Ammonia presents the best option as a working fluid for this application. Using this system a Coefficient Of Performance of 1. 538 at 50 °C can be realized. This will result in an estimated 373, 000 kW-hr saved over a year and a $ 36, 425 reduction in annual cost. Finally, recommendations for implementation are considered to allow for future design and testing of this viable waste heat recovery device. M. S. Committee Chair: Dr. Yogendra Joshi; Committee Member: Dr. S. Mostafa Ghiaasiaan; Committee Member: Dr. Sheldon Jete...|$|R
50|$|A micro <b>data</b> <b>center</b> (MDC) is {{a smaller}} or {{containerized}} (modular) <b>data</b> <b>center</b> architecture {{that is designed}} to solve different sets of problems that take different types of compute workload that does not require to traditional facilities. Whereas the size may vary from rack to container, a micro <b>data</b> <b>center</b> may include fewer than 4 servers in a single 19-inch rack. It may come with built-in security systems, cooling systems, and/or fire protection. Typically there are standalone rack-level systems containing all the components of a 'traditional' <b>data</b> <b>center.</b> including in <b>rack</b> cooling, power supply, power backup, security, fire and suppression. They could be rapidly deployed indoors or outdoors or also in rugged terrains.|$|R
40|$|A high compute density <b>data</b> <b>center</b> {{of today}} is {{characterized}} as one consisting {{of thousands of}} racks each with multiple computing units. The computing units include multiple microprocessors, each dissipating approximately 250 W of power. The heat dissipation from a rack containing such computing units exceeds 10 KW. Today's <b>data</b> <b>center,</b> with 1000 <b>racks,</b> over 30, 000 square feet, requires 10 MW of power for the computing infrastructure. A 100, 000 square foot <b>data</b> <b>center</b> of tomorrow will require 50 MW of power for the computing infrastructure. Energy required to dissipate this heat will be an additional 20 MW. A hundred thousand square foot planetary scale <b>data</b> <b>center,</b> with five thousand 10 KW racks, would cost ~$ 44 million per year (@ $ 100 /MWh) just to power the servers & $ 18 million per year to power the cooling infrastructure for the <b>data</b> <b>center...</b>|$|R
40|$|Modern {{high-performance}} computing systems and <b>data</b> <b>centers</b> are implemented as many-core server systems. Current {{state of the}} art <b>data</b> <b>centers</b> have server <b>racks</b> with pluggable boards where each board has many multi-core processors and memory units. These boards are connected via electrical or optical cables. In such systems, communication bandwidth between the high-speed microprocessor cores and the memory is limited. To leverage full performance of these powerful chips, it is required to provide high memory bandwidth as well as effective power delivery and heat removal solutions. To address these challenges in high performance computing systems, we present a 3 D packaging solution that includes a novel silicon interposer with electrical, optical, and fluidic (trimodal) interconnects and through-silicon vias (TSVs). Th...|$|R
40|$|Capacity of the {{distributed}} {{storage system}} (DSS) is often {{discussed in the}} context of tradeoff between storage overhead and repair bandwidth. This paper considers capacity-achieving coding for the clustered form of distributed storage that reflects practical storage networks. The suggested coding scheme is shown to exactly regenerate the arbitrary failed node with minimum required bandwidth, i. e., the proposed scheme is a minimum-bandwidth-regenerating (MBR) code of clustered DSSs with general parameter setting. The proposed code is a generalization of the existing MBR code designed for nonclustered DSSs. This code can be implemented in <b>data</b> <b>centers</b> with multiple <b>racks</b> (clusters), depending on the desired system parameters. Comment: 9 pages, part of this paper is submitted to IEEE ISIT 201...|$|R
40|$|Group {{communication}} systems (e. g., multicast, DHTs) {{have emerged as}} basic primitives for several large-scale distributed systems. Most existing systems that implement these primitives often assume a flat topology of overlay nodes. For instance, many DHTs assume that all overlay links are often homogeneous in their capacities, costs and other such characteristics. Similarly, multicast protocols create overlay trees without {{taking into account the}} physical location of nodes. While a few systems do consider latency between different overlay nodes, the fact that metrics such as latency change continuously often translates into additional complexity in constantly measuring and reorganizing nodes. Modern trends in hierarchical <b>data</b> <b>center</b> designs and global services running across several geographically disparate <b>data</b> <b>centers</b> pose unique challenges and opportunity to revisit the design of group {{communication systems}} with location awareness in-built into these systems from the ground up. In this paper, we present the design and architecture of one such system called DC 2, in which nodes are aware of their location within the <b>data</b> <b>center</b> (e. g., <b>rack,</b> aggregation switch) and organize group communications in order to minimize the expensive cross-data center links or cross-hierarchy links as much as possible. In our experiments using a real prototype deployed over 700 virtual nodes running over 15 physical machines, we found that DC 2 minimizes message latencies by several orders of magnitude, and reduces node and link stress by a factor of 2 to 3 ×...|$|R
40|$|Servers have {{substantially}} {{increased their}} workload capacity {{over the years}} as an effect of a more digitalized way of living. With server capacity growing, so does the corresponding heat accumulation in <b>data</b> <b>centers</b> and server <b>racks.</b> There is little clear consensus of how heat as an environmental threat to server racks is mitigated. This study addresses how system administrators can address thermal threats to server racks through its own methodology and instrumentation; an approach based on heat mitigation standards and own experimentation. An own rack health indicator has been created. InteliRack facilitates its own calculation of a health metric for server racks. Based on principles of thermal segregation in terms server placement and temperature relationships in the rack. Various experimentation of technical and physical nature was implemented to manipulate server performance and analyze effects of realistic and unrealistic scenarios. Through experimentation and detailed data collection, the study has {{come to the conclusion that}} short term physical and technical manipulation of the rack does not interfere with heat behavior to a considerable extent. The project provides its own approach to create an health indicator for monitored racks based on heat convection principles and temperature data...|$|R
5000|$|Dec 2016: Tuangru named Key Vendor in Research and Markets [...] "Data <b>Center</b> <b>Rack</b> Servers Market - Global Forecast to 2021" [...] report ...|$|R
30|$|IT {{operations}} are integral to most business organizations around the world. The business communities need {{to rely on}} the information systems to run their organizational operations. Therefore, a company may incur loss due to disruptions and unavailability of information systems. It is necessary to have an IT infrastructure, which houses all the information systems to minimize all kind of disruptions and obstacles related to information systems. This reliable IT infrastructure is called <b>data</b> <b>center.</b> The cost to run a <b>data</b> <b>center</b> is generally associated with power, cooling, networking and storage equipment. A <b>data</b> <b>center</b> houses thousand of information and computing systems deployed in computer racks. A rack is an Electronic Industries Association enclosure, which is 2 meters high, 0.61 meters wide and 0.76 meters deep. A standard rack accommodates 40 - 42 computing units and dense rack configuration servers (Blade rack) will accommodate 200 computing units. The heat dissipated by a standard rack is 10 KW and Blade rack will dissipate heat up to 30 KW. Hence, a <b>data</b> <b>center</b> containing 2000 <b>racks</b> may require 20 MW power [1]. Increasingly, PC-based computing and storage services are relocating to Internet services. While early Internet services were mostly informational, many recent Web applications offer services that previously resided in the client, including email, photo and video storage and office applications. The shift from PC-based computing services to server-side computing is driven primarily not only for the improvements in services, such as the ease of management (no configuration or backups needed) and ubiquity of access (a browser is all you need), but also by the advantages it offers to vendors. Now a days, Software as a service provides faster application development because it is easier for software vendors to make changes and improvements. Instead of updating millions of clients, vendors need to coordinate improvements and fixes inside their <b>data</b> <b>centers</b> and can restrict hardware deployment to a few well-tested configurations. Moreover, <b>data</b> <b>center</b> economics allows many application services to run at a low cost per user. For example, servers may be shared with thousands of active users (and many more inactive ones), resulting in better utilization. Similarly, the computation itself may become cheaper in a shared service (e.g., an email attachment received by multiple users can be stored once rather than many times). Finally, servers and storage in a <b>data</b> <b>center</b> can be easier to manage than the desktop or laptop equivalent because they are under the control of a single, knowledgeable entity [2]. Though each <b>data</b> <b>center</b> is different based on the operations, facilities and the average cost per year to operate, a large <b>data</b> <b>center</b> costs between $ 10 million to $ 25 million. 42 % of costs are associated with hardware, software, uninterrupted power supplies, and networking. 58 % of the expenses is due to heating, air conditioning, property and sales tax. In a traditional <b>data</b> <b>center,</b> most of the cost is consumed by infrastructure for maintenance. A few surveys have estimated the maintenance cost up to 80 % of the total cost. As <b>data</b> <b>centers</b> have become important aspects in business organization, it is imperative to examine the cost-revenue dynamics and design an effective way to optimize it. Cisco in its Global Cloud Index (GCI) Reports- 2012, forecasted <b>data</b> <b>center</b> traffic to shoot up to 554 exabytes (EB) per month by 2016, from 146 exabytes. There are a few techniques to optimize cost and profit. Cobb-Douglas is a widely used production model, but {{to the best of our}} knowledge, has never been used in the study of optimization issues arising in <b>data</b> <b>centers.</b>|$|R
50|$|<b>Data</b> <b>center</b> {{personnel}} - All <b>data</b> <b>center</b> personnel {{should be}} authorized {{to access the}} <b>data</b> <b>center</b> (key cards, login ID’s, secure passwords, etc.). <b>Data</b> <b>center</b> employees are adequately educated about <b>data</b> <b>center</b> equipment and properly perform their jobs. Vendor service personnel are supervised when doing work on <b>data</b> <b>center</b> equipment. The auditor should observe and interview <b>data</b> <b>center</b> employees to satisfy their objectives.|$|R
40|$|Abstract. Cloud {{computing}} <b>data</b> <b>centers</b> can {{be called}} cloud computing centers. It has put forward newer and higher demands for <b>data</b> <b>centers</b> {{with the development of}} cloud computing technologies. This paper will discuss what are cloud computing <b>data</b> <b>centers,</b> cloud computing <b>data</b> <b>center</b> construction, cloud computing <b>data</b> <b>center</b> architecture, cloud computing <b>data</b> <b>center</b> management and maintenance, and the relationship between cloud computing <b>data</b> <b>centers</b> and clouds...|$|R
50|$|Modular <b>data</b> <b>centers</b> {{typically}} come in {{two types}} of form factors. The more common type, referred to as containerized <b>data</b> <b>centers</b> or portable modular <b>data</b> <b>centers,</b> fits <b>data</b> <b>center</b> equipment (servers, storage and networking equipment) into a standard shipping container, which is then transported to a desired location. Containerized <b>data</b> <b>centers</b> typically come outfitted with their own cooling systems. Cisco makes {{an example of this}} type of <b>data</b> <b>center,</b> called the Cisco Containerized <b>Data</b> <b>Center.</b>|$|R
40|$|<b>Data</b> <b>centers</b> now play an {{important}} role in modern IT infrastructures. Although much research effort has been made in the field of green <b>data</b> <b>center</b> computing, performance metrics for green <b>data</b> <b>centers</b> have been left ignored. This paper is devoted to identify and implement energy efficiency and green computing performance metrics in <b>data</b> <b>centers.</b> The metrics helps <b>data</b> <b>center</b> managers to measure and implement cost and power savings in <b>data</b> <b>centers.</b> A metrics based energy efficiency model for categorizing <b>data</b> <b>center</b> into measurable units is proposed which divides <b>data</b> <b>center</b> into four measurable areas and maps metrics to measure their efficiency and performance. The results generated after applying Power Usage Effectiveness metrics clearly demonstrate poor performance of <b>data</b> <b>center</b> with PUE value of 3. 2, which indicates very inefficient <b>data</b> <b>center...</b>|$|R
50|$|<b>Data</b> <b>Center</b> Design Consultant (DCDC)Established in 2011, <b>Data</b> <b>Center</b> Design Consultants {{demonstrate}} {{knowledge in}} <b>data</b> <b>center</b> design.|$|R
50|$|The Telecommunications Industry Association's Telecommunications Infrastructure Standard for <b>Data</b> <b>Centers</b> {{specifies}} {{the minimum}} requirements for telecommunications infrastructure of <b>data</b> <b>centers</b> and computer rooms including single tenant enterprise <b>data</b> <b>centers</b> and multi-tenant Internet hosting <b>data</b> <b>centers.</b> The topology proposed {{in this document}} {{is intended to be}} applicable to any size <b>data</b> <b>center.</b>|$|R
30|$|A {{similarity}} between cloud and traditional <b>data</b> <b>center</b> {{is that both}} {{can be used for}} data storage. Cloud is an example of off-premises computing, whereas <b>data</b> <b>centers</b> are being used on premise storing system. Nowadays, <b>data</b> <b>centers</b> are effectively being utilized in cloud computing. Cloud services are now being provided through <b>data</b> <b>centers,</b> which house cloud services and cloud-related resources. Cloud service providers also own <b>data</b> <b>centers,</b> which is located in different geographical location, for provisioning of uninterrupted services in case of outage and unpredictable situations. IaaS (Infrastructure as a service), which provides facilities like virtual machines, storage and load balancing maintains a large pool of resources in <b>data</b> <b>centers.</b> <b>Data</b> <b>centers,</b> which are largely being used for cloud computing are called cloud <b>data</b> <b>center.</b> Lately the demarcation of the terms has disappeared and all are referred as <b>data</b> <b>centers.</b> Existing <b>data</b> <b>centers</b> are often restructured with modern equipment so that it can take advantage of greater performance and energy efficient facilities of cloud computing. The entire process of modernization of <b>data</b> <b>center</b> is called <b>data</b> <b>center</b> transformation [15].|$|R
40|$|This work {{describes}} two <b>data</b> <b>center</b> efficiency metrics: Power Usage Effectiveness (PUE) and Compute Power Efficiency (CPE). PUE {{characterizes the}} {{fraction of the total}} <b>data</b> <b>center</b> power used for IT work. CPE characterizes the overall <b>data</b> <b>center</b> efficiency, considering IT equipment utilization as well as how power is used in the <b>data</b> <b>center.</b> The PUE results from three <b>data</b> <b>center</b> studies are presented here. The data suggests that a carefully designed and managed <b>data</b> <b>center</b> has a PUE of 2. 0. More studies are required to determine the range of values for the typical <b>data</b> <b>center.</b> A <b>data</b> <b>center</b> infrastructure and energy cost model is presented to compare hardware costs to infrastructure and energy costs. The impact of PUE on these costs is examined to illustrate the impact of <b>data</b> <b>center</b> efficiency on the total cost of operating a <b>data</b> <b>center...</b>|$|R
5000|$|Modules - <b>data</b> <b>center</b> modules are purpose-engineered modules and {{components}} to offer scalable <b>data</b> <b>center</b> capacity. They typically use standardized components, which make them easily added, integrated or retrofitted into existing <b>data</b> <b>centers,</b> and cheaper {{and easier to}} build. In a colocation environment, the <b>data</b> <b>center</b> module is a <b>data</b> <b>center</b> within a <b>data</b> <b>center,</b> with its own steel walls and security protocol, and its own cooling and power infrastructure. “A number of colocation companies have praised the modular approach to <b>data</b> <b>centers</b> to better match customer demand with physical build outs, and allow customers to buy a <b>data</b> <b>center</b> as a service, paying only for what they consume.” ...|$|R
50|$|There {{are three}} World <b>Data</b> <b>Centers</b> for Oceanography:World <b>Data</b> <b>Center,</b> Silver Spring, Maryland, United States,World <b>Data</b> <b>Center,</b> Moscow, Russia, andWorld <b>Data</b> <b>Center,</b> Tianjin, People's Republic of China.They {{are part of}} the World <b>Data</b> <b>Center</b> System {{initiated}} in 1957 to provide a mechanism for data exchange, and they operate under guidelines issued by the International Council of Scientific Unions (ICSU).|$|R
50|$|Generally, <b>data</b> <b>center</b> {{services}} {{fall into}} two categories: services provided to a <b>data</b> <b>center</b> or services provided from a <b>data</b> <b>center.</b>|$|R
50|$|A network-neutral <b>data</b> <b>center</b> (or carrier-neutral <b>data</b> <b>center)</b> is a <b>data</b> <b>center</b> (or carrier hotel) {{which allows}} {{interconnection}} between multiple telecommunication carriers and/or colocation providers. Network-neutral <b>data</b> <b>centers</b> exist {{all over the}} world and vary in size and power.|$|R
40|$|<b>Data</b> <b>Center</b> as a Service (DCaaS) {{facilitates}} {{to clients}} as alternate outsourced physical <b>data</b> <b>center,</b> {{the expectations of}} business community to fully automate these <b>data</b> <b>centers</b> to run smoothly. Geographically distributed <b>data</b> <b>centers</b> and its connectivity has major role in next generation <b>data</b> <b>centers.</b> In order to deploy the reliable connections between distributed <b>data</b> <b>centers</b> the SDN based security and logical firewalls are attractiveand enviable. We present the middleware security framework for software defined <b>data</b> <b>centers</b> inter-connectivity, the proposed security framework {{will be based on}} some learning processes,which will reduce the complexity and manage very large number of secure connections in real-world <b>data</b> <b>centers.</b> In this paper we will focus on two main objectives; (1) proposing simple and yet scalable techniques for security and analysis, (2) Implementing and evaluating these techniques on real-world <b>data</b> <b>centers...</b>|$|R
50|$|<b>Data</b> <b>Center</b> Interconnect (DCI): <b>Data</b> <b>center</b> {{interconnect}} {{solutions are}} intended to extend the benefits of multi-tenant private clouds across multiple <b>data</b> <b>centers.</b>|$|R
40|$|Photograph {{of female}} {{workers in a}} textile mill, showing a single woman {{with her back to}} the camera in the foreground, ca. 1930. At center, a long line of {{machinery}} can be seen extending from the right of the image to the far left. Along the <b>center</b> <b>rack</b> of the machine, numerous spools of thread are visible. Above the thread, groups of cylindrical plates extending from the top can be seen. Lines of thread can be seen running between the cylindrical plates back to the <b>center</b> <b>rack.</b> Below the machinery, a large empty table area is visible. Numerous small, cone-shaped objects can be seen strewn about the area. At center, a single woman is seen {{with her back to the}} camera. The woman wears a long dress with an apron tied to the front. The short-haired woman also wears lightly-colored high heel shoes as she stands before the machinery. Behind the row of machinery, the faces of two other female workers are barely visible. In the extreme background along the right wall, several cylindrical wheels can be seen with hanging thread extending to the floor below. Pipes move across the ceiling of the factory, and hanging lights can also be seen. To the left, a large clock sits at the top of a rectangular pillar...|$|R
