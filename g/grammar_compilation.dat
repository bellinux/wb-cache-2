3|10|Public
40|$|ABSTRACT. The {{most popular}} method for {{computing}} LALR(1) lookaheads {{is not the}} most efficient one, mainly (as we conjecture) due to certain deficiencies in the description of core algorithms. Similarly, full LR(1) parser generators are not as common as their LALR(1) counterparts, despite their usefulness regarding conventional semantic routine attachment and attribute <b>grammar</b> <b>compilation.</b> We address these two issues by describing an efficient algorithm to compute either LALR(1) or LR(1) lookahead sets without requiring any modification of the underlying transitive closure algorithm. 1...|$|E
40|$|Constituency parsing {{with rich}} grammars re-mains a {{computational}} challenge. Graph-ics Processing Units (GPUs) have previously {{been used to}} accelerate CKY chart evalua-tion, but gains over CPU parsers were mod-est. In this paper, we describe a collection of new techniques that enable chart evaluation at close to the GPU’s practical maximum speed (a Teraflop), or around a half-trillion rule eval-uations per second. Net parser performance on a 4 -GPU system is over 1 thousand length- 30 sentences/second (1 trillion rules/sec), and 400 general sentences/second for the Berkeley Parser Grammar. The techniques we introduce include <b>grammar</b> <b>compilation,</b> recursive sym-bol blocking, and cache-sharing. ...|$|E
40|$|We {{describe}} a modular HPSG implementation, {{based on a}} set of tools rather than a single monolithic engine such as ALE. With these tools we can use techniques for much faster compiling and parsing than ALE. We use two-stage <b>grammar</b> <b>compilation</b> with partial execution and a concurrent process implementation of the chart for fast parsing. We compile HPSG lexical rules into Prolog rules used at run-time by the morphological preprocess, thus eliminating lexical rule expansion at compile-time as in ALE. This gives fast lexicon compilation, and also allows further exploitation of lexical rules to eliminate empty categories. 1 Introduction Using the ALE system (Carpenter and Penn, 1994) to develop an English grammar, we found {{that it was possible to}} implement successfully almost everything in (Pollard and Sag, 1994), but development was hindered by slow compilation, especially lexicon compilation. By contrast, using the SAX parsing system (Matsumoto et al., 1994) with a DCG-based grammar, bot [...] ...|$|E
40|$|This paper {{describes}} {{the extension of}} the system DyALog to compile tabular parsers from Feature Tree Adjoining <b>Grammars.</b> The <b>compilation</b> process uses intermediary 2 -stack automata to encode various parsing strategies and a dynamic programming interpretation to break automata derivations into tabulable fragments. 1...|$|R
40|$|Grammatical Framework (GF) is a grammar {{formalism}} {{which supports}} interlingua-based translation, library-based <b>grammar</b> engineering, and <b>compilation</b> to speech recognition grammars. We show how these features {{can be used}} in the construction of portable high-precision domain-specific speech translators...|$|R
40|$|Breadth-depth grammars [3] {{extend the}} {{context-free}} ones by allowing breadth-first derivations. Breadth-depth languages include the context-free ones and are recognized by monostatic (one-state) automata having a double-ended queue (dequeue) as their memory. Examples of breadth-depth <b>grammars</b> for <b>compilation</b> and scheduling problems are included {{to argue for}} their applicability. We define deterministic monostatic dequeue automata and investigate their languages. Main results are their incomparability with context-free deterministic languages and linear-time recognizability. We introduce the LL(1) grammars, {{as an extension of}} the classical top-down deterministically parsable context-free grammars, and we present a recursive descent parsing algorithm using a FIFO queue. Work supported by ESPRIT Basic Research Actions ASMICS and by Ministero dell'Università e della Ricerca Scientifica e Tecnologica MURST 60...|$|R
40|$|This paper {{describes}} {{the extension of}} the system DyALog to compile tabular parsers from Feature Tree Adjoining <b>Grammars.</b> The <b>compilation</b> process uses intermediary 2 -stack automata to encode various parsing strategies and a dynamic programming interpretation to break automata derivations into tabulable fragments. 1. Introduction This paper {{describes the}} extension of the system DyALog in order to produce tabular parsers for Tree Adjoining Grammars [TAGs] and focuses on some practical aspects encountered during the process. By tabulation, we mean that traces of (sub) computations, called items, are tabulated in order to provide computation sharing and loop detection (as done in Chart Parsers). The system DyALog 1 handles logic programs and grammars (DCG). It has two main components, namely an abstract machine that implements a generic fix-point algorithm with subsumption checking on objects, and a bootstrapped compiler. The compilation process first compiles a grammar into a Push-Down [...] ...|$|R
40|$|In {{this paper}} we view Lexicalized Tree Adjoining <b>Grammars</b> as the <b>compilation</b> {{of a more}} {{abstract}} and modular layer of linguistic description :the metagrammar (MG). MG provides a hierarchical representation of lexicosyntactic descriptions and principles that capture the well-formedness of lexicalized structures, expressed using syntactic functions. This {{makes it possible for}} a tool to compile an instance of MG into an LTAG, automatically performing the relevant combinations of linguistic phenomena. We then describe the instantiation of an MG for Italian and French. The work for French was performed starting with an existing LTAG, which has been augmented as a result. The work for Italian was performed by systematic contrast with the French MG. The automatic compilation gives two parallel LTAG, compatible for multilingual NLP applications...|$|R
40|$|This {{thesis is}} about a method for {{speeding}} up natural-language analysis using a novel compilation technique. As its input, the compiler takes a unification-based linguistic formalism (non-deterministic finite-state automata, where transitions are labeled by attribute-value matrices according to a finite type logic with a simple-inheritance type hierarchy). As its output, the compiler generates machine instructions for the PowerPC chip, a pipelined RISC processor with superscalar instruction dispatch. Because of its fine-grained knowledge about the task, the compiler is able to perform optimizations {{that would be very}} difficult to achieve using traditional techniques. Examples include heuristics for Static Branch Prediction, data cache control and scheduling the machine instructions to benefit from superscalarity, so that certain unifications are executed in parallel. The system is evaluated by measuring the time it takes to extract noun groups in texts of some thousand words in length. On Apple PowerMacintosh machines, this task could be accomplished in fractions of a millisecond, theoretically corresponding to a speed of up to 21 million tokens per second. Hence, the generated code is so efficient that unification and pattern matching become neglectible factors in the overall performance of a natural-language system. Due to the achieved speed, the presented techniques could form the foundation technology of new, real-time NLP applications. Keywords: Natural Language Engineering, Finite-State Automata, Unification-Based <b>Grammar</b> Formalisms, <b>Compilation,</b> PowerP...|$|R
40|$|Available at [URL] audienceGrammar {{engineering}} {{is the task}} of designing and implementing linguistically motivated electronic descriptions of natural language (so-called grammars). These grammars are expressed within well-defined theoretical frameworks, and offer a fine-grained description of natural language. While grammars were first used to describe syntax, that is to say, the relations between constituents in a sentence, they often go beyond syntax and include semantic information. Grammar engineering provides precise descriptions {{which can be used}} for natural language understanding and generation, making these valuable resources for various natural language applications, including textual entailment, dialogue systems, or machine translation. The first attempts at designing large-scale resource grammars were costly because of the complexity of the task (Erbach 1990) and of the number of persons that were needed (see e. g. Doran et al. 1997). Advances in the field have {{led to the development of}} environments for semi-automatic grammar engineering, borrowing ideas from <b>compilation</b> (<b>grammar</b> {{engineering is}} compared with software development) and machine learning. This special issue reports on new trends in the field, where grammar engineering benefits from elaborate high-level methodologies and techniques, dealing with various issues (both theoretical and practical) ...|$|R
40|$|Recenzenci pracy: Stanisław Ambroszkiewicz, Krzysztof Cetnarowicz. Praca doktorska. Akademia Górniczo-Hutnicza im. Stanisława Staszica (Kraków), 2005. Bibliogr. k. 145 - 152. Overview {{of related}} technologies, grid computing, fabric management, requirements, main objective, general goals, {{example of a}} large scale {{computer}} centre, requirements on the framework, use cases, framework architecture, key concepts, pan - high level description language, motivation, low level description, introduction to the pan language, main constructs, data manipulation language, types, validation, user defined functions, global variables, pan in examples, language <b>grammar,</b> pan compiler, <b>compilation</b> and dependency model, configuration trees and reference structures of pan templates, complexity of full compilation, theoretical model of templates recompilation, compilation parallelisation, experimental results, design {{and implementation of the}} configuration management system, configuration database, CDB, server modules, configuration cache manager, CCM, graphical and command line user interfaces, chapter summary, design and implementation of the installation management system, automated installation infrastructure, AII, node configuration manager, NCM, software repository, SWRep, software package manager agent, SPMA, software development methodology, quality assurance, framework development, framework deployment, experiments and results, CERN Computer Centre, future plans for quattor at CERN, other projects and centers, goals achieved, other potential areas, future work...|$|R
40|$|It {{is widely}} agreed among {{scholars}} {{that the third}} part of the Hebrew canon, the Writings, is a miscellaneous collection of materials, as its name would seem to suggest. My thesis re-examines this assumption. The introduction sets out the critical issues, outlines the thesis and charts the larger picture from which the thesis makes a limited contribution. Chapter one explains my approach. In critical conversation with Brevard Childs and his adherents, I examine the need for contours within the canonical context that respect the discrete voice of each book, while understanding it {{in relationship to the}} larger collection in which it is located. The canon is not like a street map, rather, it is more like a topographical map providing contour and depth to the canonical terrain. Taking Childs’ approach one step further; I examine the formation of the Twelve Minor prophets and the Psalter in order to develop a redaction critical <b>grammar</b> for the <b>compilation</b> of texts into collections that serves as a methodological check for the project. This grammar includes the use of catchwords or phrases to bind adjacent books near their seams, the juxtaposition of similar or contrasting themes, framing devices, and superscriptions to provide an overall structure. Chapter two analyzes the formation of the Writings in antiquity. There were a number of different conceptions of sacred literature within Judaism, but probably within temple circles the canon of the Jews was closed prior {{to the end of the}} first century C. E. The Prologue to Ben Sira testifies to a tripartite arrangement of the Jewish canon, and 4 Ezra, which provides solid evidence that the canon was closed sometime prior to the end of the first century C. E., confirms the antiquity of a tripartite arrangement. Chapter three explores the various orders for the Writings. Within the conceptual world of Judaism, the concern with the order of the books is not the result of the invention of the codex or long scroll, but rather arises from the holiness attributed to these books in association with their strong connection to the temple and its sacred space. Despite the consensus that there are a vast number of orders for the collection, in fact there is only evidence that the Masoretic (Leningrad Codex) and the Talmudic (Baba Batra 14 b) orders existed prior to the twelfth century C. E. The grouping of the Megilloth in the Masoretic tradition is probably not the result of liturgical practices within Judaism, as is commonly thought, which leaves room to re-examine the antiquity of this order. Both arrangements reveal a similar logic of association among the books of the Writings with the possible exception of Ruth. Chapter four explores the location of Ruth in the Former Prophets between Judges and Samuel and in the Writings after Proverbs and before the Psalter. Ruth has been purposefully figured into the Former Prophets and then later was integrated into the Writings after Proverbs as a wisdom book. In this case, different orders bear witness to the search for meaningful associations within the canon. Chapter five probes Esther’s position as part of the sub-collection of Lamentations, Esther, Daniel and Ezra-Nehemiah, in which it always follows Lamentations and is juxtaposed to Daniel. Within this canonical frame I explore Esther’s links to Daniel 1 - 6 and Lamentations 5 and the way this sets in relief Esther’s theology. Chapter six briefly observes some compilational phenomena in Song of Songs, Ecclesiastes and Lamentations. I also examine the structure of the Megilloth as a whole and the forces at work in this sub-collection. The thesis concludes, due to historical and exegetical reasons, that the codification of the Megilloth into a collection is an integral part of the canonical process rather than a formal feature that is the result of some supposed effort to close the canon. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R

