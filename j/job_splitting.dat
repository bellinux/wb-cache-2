19|85|Public
50|$|Given {{significant}} {{idle time}} at the second work center (from waiting for the job to be finished at the first work center), <b>job</b> <b>splitting</b> may be used.|$|E
40|$|Under current analysis, soft {{real-time}} tardiness bounds {{applicable to}} global earliest-deadline-first scheduling and related policies depend on per-task worst-case execution times. By splitting job budgets to create subjobs with shorter periods and worst-case execution times, such bounds {{can be reduced}} to near zero for implicit-deadline sporadic task systems. However, doing so could potentially cause more preemptions and create problems for synchronization protocols. This paper analyzes this tradeoff between theory and practice by presenting an overhead-aware schedulability study pertaining to <b>job</b> <b>splitting.</b> In this study, real overhead data from a scheduler implementation in LITMUSRT was factored into schedulability analysis. This study shows that despite practical issues affecting <b>job</b> <b>splitting,</b> it can still yield substantial reductions in tardiness bounds for soft real-time systems. ...|$|E
40|$|AbstractThis paper {{considers}} {{the problem of}} scheduling jobs with release dates on a single machine to minimize the total weighted completion time. A branch and bound algorithm is proposed which incorporates three special features that contribute to its efficiency. Firstly, quickly computed lower bounds are obtained using a procedure {{which is based on}} <b>job</b> <b>splitting.</b> The <b>job</b> <b>splitting</b> methodology is shown to be applicable to a range of total weighted completion time scheduling problems. Secondly, the branching rule includes a release date adjustment mechanism which increases release dates at certain nodes of the tree with a view to tightening lower bounds. Thirdly, the branch and bound algorithm includes a new dominance rule for eliminating nodes of the search tree. Computational experience on problems with up to 50 jobs indicates that the proposed algorithm is superior to other known algorithms...|$|E
40|$|In {{classical}} shop scheduling, {{the tasks}} corresponding {{to a job}} may not be executed in parallel, i. e., their processing times may not overlap. In case these tasks are processes, independent of each other, this assumption is no longer justified. We consider corresponding scheduling problems where each <b>job</b> <b>splits</b> {{into a number of}} pairwise independent processes that have to be executed on dedicated machines...|$|R
40|$|Fork-join {{queueing}} networks model {{a network}} of parallel servers in which an arriving <b>job</b> <b>splits</b> {{into a number of}} subtasks that are serviced in parallel. Fork-join queues can be used to model disk arrays. A response time approximation of the fork-join queue is presented that attempts to comply with the additional constraints of modelling a disk array. This approximation is compared with existing analytical approximations of the fork-join queueing network. ...|$|R
30|$|All MR <b>jobs</b> <b>split</b> into Map and Reduce tasks. Reducer tasks {{minimize}} {{a set of}} intermediate values {{associated with}} each intermediate key, generated by the map function and breaks down the output to a smaller set of values. A reducer writes output (key, value) pairs to both disk and memory via the namespace abstraction for further processing. The developers decide the number of reducers. Even if they have too many reducers {{and they can not}} determine the optimal number of reducers, context-switching overhead of reducer tasks will have a significant impact of performance.|$|R
40|$|A zero-one (0 - 1) linear {{programming}} formulation of multiproject and job-shop scheduling problems is presented {{that is more}} general and computationally tractable than other known formulations. It can accommodate {{a wide range of}} real-world situations including multiple resource constraints, due dates, <b>job</b> <b>splitting,</b> resource, substitutability, and concurrency and nonconcurrency of job performance requirements. Three possible objective functions are discussed; minimizing total throughput time for all projects: minimizing the time by which all projects are completed (i. e., minimizing makespan); and minimizing total lateness or lateness penalty for all projects. ...|$|E
40|$|We {{consider}} a single machine scheduling problem {{to minimize the}} weighted completion time variance. This problem {{is known to be}} NP-hard. We propose a heuristic and a lower bound based on <b>job</b> <b>splitting</b> and the Viswanathkumar and Srinivasan procedure. The test on more than 2000 instances shows that this lower bound is very tight and the heuristic yields solutions very close to optimal ones since the gap between the solution given by the heuristic and the lower bound is very small. Scheduling Single machine Weighted completion time variance Lower bound Heuristic...|$|E
40|$|Splitting jobs among {{machines}} often {{result in}} improved {{customer service and}} reduction in throughput time. Implicit in determining a schedule, there is a lot-sizing decision specifying how jobs are to be split. This research considers the problem of lot-sizing and scheduling jobs with varying processing times, non-common due dates, and sequencedependent set-up times on parallel machines. The objective is to minimize the sum of total tardiness. The system is non-preemptive. Most of the research work performed on parallel machines scheduling does not consider <b>job</b> <b>splitting.</b> This research proposes a simulated annealing method. Computational simulations indicate that this procedure {{can be used to}} solve large-scale problems of practical size...|$|E
30|$|Open {{image in}} new window {{ensures that the}} <b>job</b> cannot be <b>split.</b>|$|R
5000|$|Quit Your Band, Get a <b>Job</b> (2014) - <b>split</b> single with The Murderburgers ...|$|R
40|$|Abstract. This paper {{deals with}} the load-balancing of {{machines}} in a real-world job-shop scheduling problem, with identical machines. The load-balancing algorithm allocates <b>jobs,</b> <b>split</b> into lots, on identical machines, with objectives to reduce job total throughput time and to improve machine utilization. A genetic algorithm is developed, whose fitness function evaluates the load-balancing in the generated schedule. This load-balancing algorithm is used within a multi-objective genetic algorithm, which minimizes average tardiness, number of tardy jobs, setup times, idle times of machines and throughput times of jobs. The performance of the algorithm is evaluated with real-world data and compared to the results obtained with no loadbalancing...|$|R
40|$|In {{this paper}} we {{investigate}} {{the use of}} lot streaming in non-permutation flowshop scheduling problems. The objective is to minimize the makespan subject to the standard flowshop constraints, but {{where it is now}} permitted to reorder jobs between machines. In addition, the jobs can be divided into manageable sublots, a strategy known as lot streaming. Computational experiments show that lot streaming reduces the makespan up to 43 % {{for a wide range of}} instances when compared to the case in which no <b>job</b> <b>splitting</b> is applied. The benefits grow as the number of stages in the production process increases but reach a limit. Beyond a certain point, the division of jobs into additional sublots does not improve the solution...|$|E
40|$|In this paper, {{we address}} a {{scheduling}} problem for minimizing total weighted tardiness. The {{background for the}} paper {{is derived from the}} automobile gear manufacturing process. We consider the bottleneck operation of heat treatment stage of gear manufacturing. Real-life scenarios like unequal release times, incompatible job families, nonidentical job sizes, heterogeneous batch processors, and allowance for <b>job</b> <b>splitting</b> have been considered. We have developed a mathematical model which takes into account dynamic starting conditions. The problem considered in this study is NP-hard in nature, and hence heuristic algorithms have been proposed to address it. For real-life large-size problems, the performance of the proposed heuristic algorithms is evaluated using the method of estimated optimal solution available in literature. Extensive computational analyses reveal that the proposed heuristic algorithms are capable of consistently obtaining near-optimal statistically estimated solutions in very reasonable computational time...|$|E
40|$|Summary form only given. Scheduling {{policies}} are proposed for parallelizing data intensive particle physics analysis applications on computer clusters. Particle physics analysis jobs require {{the analysis of}} tens of thousands of particle collision events, each event requiring typically 200 ms processing time and 600 KB of data. Many jobs are launched concurrently by a large number of physicists. At a first view, particle physics jobs seem to be easy to parallelize, since particle collision events can be processed independently one from another. However, since large amounts of data need to be accessed, the real challenge resides in making an efficient use of the underlying computing resources. We propose several job parallelization and scheduling policies aiming at reducing job processing times and at increasing the sustainable load of a cluster server. Since particle collision events are usually reused by several jobs, cache based <b>job</b> <b>splitting</b> strategies considerably increase cluster utilization and reduce job processing times. Compared with straightforward job scheduling on a processing form, cache based first in first out <b>job</b> <b>splitting</b> speeds up average response times by an order of magnitude and reduces job waiting times in the system's queues from hours to minutes. By scheduling the jobs out of order, according to the availability of their collision events in the node disk caches, response times are further reduced, especially at high loads. In the delayed scheduling policy, job requests are accumulated during a time period, divided into subjob requests according to a parameterizable subjob size, and scheduled at the beginning of the next time period according to the availability of their data segments within the disk node caches. Delayed scheduling sustains a load close to the maximal theoretically sustainable load of a cluster, but at the cost of longer average response times. Finally we propose an adaptive delay scheduling approach, where the scheduling delay is adapted to the current load. This last scheduling approach sustains very high loads and offers low response times at normal loads...|$|E
40|$|Abstract. One of {{the main}} goals of the CrossGrid Project [1] is to provide {{explicit}} support to parallel and interactive compute- and dataintensive applications. The CrossBroker job manager provides services {{as part of the}} CrossGrid middleware and allows execution of parallel MPI applications on Grid resources in a transparent and automatic way. This document describes the design and implementation of the key components responsible for an efficient and reliable execution of MPI <b>jobs</b> <b>splitted</b> over multiple Grid sites, executed either in an on-line or batch manner. We also provide details on the overheads introduced by our system, as well as an experimental study showing that our system is well-suited for embarrassingly parallel applications. ...|$|R
50|$|On {{arrival at}} the fork point, a <b>job</b> is <b>split</b> into N sub-jobs which are served by each of the N servers. After service, sub-job wait until all other sub-jobs have also been processed. The sub-jobs are then rejoined and leave the system.|$|R
30|$|A MapReduce <b>job</b> <b>splits</b> a dataset {{into chunks}} of data such that map tasks are {{processed}} {{in a parallel}} manner. The framework sorts the outputs of maps, which are then input {{to reduce the number}} of tasks involved. Inputs and outputs of a job are stored in a file system. The framework manages scheduling tasks, monitors tasks and re-executes failed tasks. The HDFS is a distributed file system that uses master/slave architecture and that is capable of storing large volumes of data. MapReduce jobs are executed on a cluster of nodes while carrying out large-scale data operations that are scalable and feasible to execute in a reasonable amount of time for complex or large datasets. MapReduce works in parallel with several clusters of computers, making it easy to scale to large datasets.|$|R
40|$|Following the {{progress}} in information theory and telecommunication, telework {{appeared as a}} work organization system {{at the end of}} the 60 s. It allows people to work at a distance from the employer. It adds to productivity and effectiveness of enterprises, helps relocation of jobs in economically backward regions and positively assists employment (it helps part time job, job-sharing, work-sharing, office-sharing, <b>job</b> <b>splitting).</b> However, the introduction of telework on a wider scale may lead to disintegration of the collective forms of workers' organizations, which in due course may cause atomization of labour and exclusion of a part of the population from the organized social dialogue. The author analyses the definitions and particular types of telework, including home based telework, the legal status of home based workers, and their elementary employment rights. Digitalizacja i deponowanie archiwalnych zeszytów RPEiS sfinansowane przez MNiSW w ramach realizacji umowy nr 541 /P-DUN/ 201...|$|E
40|$|In {{this paper}} we address a {{scheduling}} problem for minimising total weighted tardiness. The motivation for the paper comes from the automobile gear manufacturing process. We consider the bottleneck operation of heat treatment stage of gear manufacturing. Real life scenarios like unequal release times, incompatible job families, non-identical job sizes and allowance for <b>job</b> <b>splitting</b> have been considered. A mathematical model taking into account dynamic starting conditions has been developed. Due to the NP-hard nature of the problem, a few heuristic algorithms have been proposed. The performance of the proposed heuristic algorithms is evaluated: (a) in comparison with optimal solution for small size problem instances, and (b) in comparison with `estimated optimal solution' for large size problem instances. Extensive computational analyses reveal that the proposed heuristic algorithms are capable of consistently obtaining near-optimal solutions (that is, statistically estimated one) in very reasonable computational time...|$|E
40|$|AbstractIn {{this paper}} we address a {{hierarchical}} scheduling problem for n jobs to be processed on a suitable number of parallel machines: the jobs require random amounts of processing time, no <b>job</b> <b>splitting</b> is allowed, and random precedence constraints between the jobs are present. We present two stochastic 2 -stage heuristics, called LB and GLB, for the solution of the problem and analyze their asymptotic behavior. We find conditions on the processing times and on the depth of the precedence graph, that is, the degree of inherent parallelism of the jobs, that guarantee the expected and almost sure asymptotic relative optimality of both heuristics. As regards optimality in expectation, our results for the LB heuristic nicely extend those of Dempster et al. ((1983), Math. Oper. Res. 8, 525 – 537), which hold when there are no precedence constraints; for the new GLB heuristic we obtain stronger results. Optimal convergence almost surely of the two heuristics is established under identical conditions...|$|E
50|$|Traditionally {{the post}} of Serjeant at Arms was filled by a retired {{military}} officer, but in 2008 a civil servant, Jill Pay, {{was selected as the}} first woman to hold the appointment. At the same time the <b>job</b> was <b>split,</b> with many of the duties transferred to the new post of chief executive.|$|R
50|$|A related {{model is}} the split-merge model, for which {{analytical}} results exist. Exact results for the split-merge queue are given by Fiorini and Lipsky.Here on arrival a <b>job</b> is <b>split</b> into N sub-tasks which are serviced in parallel. Only when all the tasks ﬁnish servicing and have rejoined can the next job start. This leads to a slower response time on average.|$|R
5000|$|Today {{they are}} used in many {{different}} fields of work, completing all <b>jobs</b> from <b>splitting</b> wood to removing engines from vans. Tungsten is often added for weight as an upgrade, as well as six foot handles for the heavier jobs that require added force and [...] "massive blows" [...] such as cutting automobile frames, slicing brake rotors, rough body work, home construction, home de-construction, etc.|$|R
40|$|The ATLAS {{production}} {{system has been}} successfully used to run production of simulation data at an unprecedented scale. Up to 10000 jobs were processed in one day. The experiences obtained operating the system on several grid flavours was essential to perform a user analysis using grid resources. First tests of the distributed analysis system were then performed. In the preparation phase data was registered in the LHC File Catalog (LFC) and replicated in external sites. For the main test, few resources were used. All these tests are only a first step towards the validation of the computing model. The ATLAS management computing board decided to integrate the collaboration efforts in distributed analysis in only one project, GANGA. The goal is to test the reconstruction and analysis software {{in a large scale}} Data production using Grid flavors in several sites. GANGA allows trivial switching between running test jobs on a local batch system and running large-scale analyses on the Grid; it provides <b>job</b> <b>splitting</b> and merging, and includes automated job monitoring and output retrieval...|$|E
40|$|In {{external}} {{electron beam}} therapy arbitrarily shaped inserts (cutouts) {{are used to}} define {{the contours of the}} irradiated field. This thesis describes the implementation and verification of a software system to calculate output factors for cutouts using Monte Carlo simulations. The design goals were: (1) A stand-alone software system running on a single workstation. (2) Task oriented graphical user interface with shape input capability. (3) Implementation on Mac OS XRTM (10 A. x Tiger). (4) CPU multicore support by <b>job</b> <b>splitting.</b> (5) EGSnrc (Patch level V 4 -r 2 - 2 - 5) for particle transport and dose scoring. (6) Validation for clinical use. The system, called Cutout Manager, can calculate output factors with 1 % statistical error in 20 minutes on Mac Pro computer (Intel XeonRTM, 4 cores). When the BEAMnrc linac model correctly reproduces percentage depth doses in the buildup region and around R 100, calculated and measured output factors are in good agreement with precision measurements of circular cutouts at 100 cm source-to-surface distance (SSD) and extended SSD. Cutout Manager simulations are consistent with measurements of clinical cutouts within a 2 % error margin...|$|E
40|$|We {{developed}} mixed {{integer programming}} (MIP) models and hybrid genetic-local search algorithms for the scheduling problem of unrelated parallel machines with job sequence and machine-dependent setup times and with <b>job</b> <b>splitting</b> property. The first contribution {{of this paper}} is to introduce novel algorithms which make splitting and scheduling simultaneously with variable number of subjobs. We proposed simple chromosome structure which is constituted by random key numbers in hybrid genetic-local search algorithm (GAspLA). Random key numbers are used frequently in genetic algorithms, but it creates additional difficulty when hybrid factors in local search are implemented. We developed algorithms that satisfy the adaptation of results of local search into the genetic algorithms with minimum relocation operation of genes’ random key numbers. This is the second contribution of the paper. The third contribution {{of this paper is}} three developed new MIP models which are making splitting and scheduling simultaneously. The fourth contribution of this paper is implementation of the GAspLAMIP. This implementation let us verify the optimality of GAspLA for the studied combinations. The proposed methods are tested on a set of problems taken from the literature and the results validate the effectiveness of the proposed algorithms...|$|E
50|$|My Summer with Des is a 1998 TV movie comedy drama {{created by}} English writer Arthur Smith. Shown to {{coincide}} with the beginning of World Cup 1998, the story is set during the European football championships in 1996, where football fan Martin finds his life is going from bad to worse after losing his <b>job</b> and <b>splitting</b> up with his girlfriend. It starred Neil Morrissey, Rachel Weisz and Des Lynam.|$|R
40|$|In {{simulation}} {{studies of}} parallel processors, {{it is useful}} to consider the following abstraction of a parallel program. A job is partitioned into n processes, whose running times are independent random variables X 1; : : :; Xn. As a measure of performance we consider the normalized job completion time S = maxfX i g= P n i= 1 X i. We consider a simple approximation to the expected value of S, valid asymptotically whenever the X i 's are bounded, and assess its accuracy as a function of n both theoretically and experimentally. The approximation is easy to compute and involves only the first two moments of X i. 1 Introduction In this paper we study a simple performance metric for parallel programs. We are interested in so-called fork-join programs, of the following type. A <b>job</b> <b>splits</b> into n processes that run independently, then wait for the last one to complete. As a measure of performance we consider the ratio S := maxfT i g D; (1) where T i is the time taken by process i, an [...] ...|$|R
5000|$|The Federal Bureau of Investigation (FBI) {{claimed that}} Aref {{is tied to}} Mullah Krekar, the founder of Ansar al-Islam. [...] When Aref left Iraq as a refugee in 1994, he lived in Syria for 5 years. During that time he was {{approved}} by the UN as a refugee to be sent to a third country, which ended up being the US. While in Syria, Aref worked first as a gardener for a rich businessman, and then for the Damascus Office of the IMK (Islamic Movement in Kurdistan), an Islamic Kurdish group which had worked with the US to oppose Saddam Hussein, and which helped Kurdish refugees in Syria. IMK was never claimed to be a terrorist organization. Mullah Krekar was an IMK official who, at the end of 2001, two years after Aref had left Syria and the IMK <b>job,</b> <b>split</b> from IMK to form Ansar al Islam, which is a designated terrorist organization. While Aref had met Krekar briefly a couple of times through his IMK job, he did not really know him, and was opposed to his extremist politics.|$|R
40|$|Attribution License, which permits {{unrestricted}} use, distribution, {{and reproduction}} in any medium, provided the original work is properly cited. We developed mixed integer programming (MIP) models and hybrid genetic-local search algorithms for the scheduling problem of unrelated parallel machines with job sequence and machine-dependent setup times and with <b>job</b> <b>splitting</b> property. The first contribution {{of this paper}} is to introduce novel algorithms which make splitting and scheduling simultaneously with variable number of subjobs. We proposed simple chromosome structure which is constituted by random key numbers in hybrid genetic-local search algorithm (GAspLA). Randomkey numbers are used frequently in genetic algorithms, but it creates additional difficulty when hybrid factors in local search are implemented. We developed algorithms that satisfy the adaptation of results of local search into the genetic algorithms with minimum relocation operation of genes ’ random key numbers. This is the second contribution of the paper. The third contribution {{of this paper is}} three developed new MIP models which are making splitting and scheduling simultaneously. The fourth contribution of this paper is implementation of the GAspLAMIP. This implementation let us verify the optimality of GAspLA for the studied combinations. The proposedmethods are tested on a set of problems taken from the literature and the results validate the effectiveness of the proposed algorithms. 1...|$|E
40|$|For {{real-time}} systems, most of {{the analysis}} involves efficient or exact schedulability checking. While this is important, analysis is often {{based on the assumption}} that the task parameters such as execution requirements and inter-arrival times between jobs are known exactly. In most cases, however, only a worst-case estimate of these quantities is available at the time of analysis. It is therefore imperative that schedulability analysis hold for better parameter values (Sustainable Analysis). On the other hand, if the task or system parameters turn out to be worse off, then the analysis should tolerate some deterioration (Robust Analysis). Robust analysis is especially important, because the implication of task schedulability is often weakened in the presence of optimizations that are performed on its code, or dynamic system parameters. In this work, we define and address sustainability and robustness questions for analysis of embedded real-time software that is modeled by conditional real-time tasks. Specifically, we show that, while the analysis is sustainable for changes in the task such as lower job execution times and increased relative deadlines, it is not the case for code changes such as <b>job</b> <b>splitting</b> and reordering. We discuss the impact of these results in the context of common compiler optimizations, and then develop robust schedulability techniques for operations where the original analysis is not sustainable...|$|E
40|$|The {{distributed}} {{data analysis}} using Grid resources {{is one of}} the funda- mental applications in high energy physics to be addressed and realized before the start of LHC data taking. The needs to manage the resources are very high. In every experiment up to a thousand physicist will be submitting analysis jobs into the Grid. Appropriate user interfaces and helper applications have to be made available to assure that all users can use the Grid without too much expertise in Grid technology. These tools enlarge the number of Grid users from a few production adminis- trators to potentially all participating physicists. The GANGA job management system ([URL] devel- oped as a common project between the ATLAS and LHCb experiments provides and integrates these kind of tools. GANGA provides a sim- ple and consistent way of preparing, organizing and executing analysis tasks within the experiment analysis framework, implemented through a plug-in system. It allows trivial switching between running test jobs on a local batch system and running large-scale analyzes on the Grid, hiding Grid technicalities. We will be reporting on the plug-ins and our experiences of distributed data analysis using GANGA within the ATLAS experiment and the EGEE/LCG infrastructure. The integration and interaction with the ATLAS data management system DQ 2 /DDM into GANGA is a key functionality. In combination with the <b>job</b> <b>splitting</b> mechanism large amounts of analysis jobs can be sent to the locations of data following the ATLAS computing model. GANGA supports tasks of user analysis with reconstructed data and small scale production of Monte Carlo data...|$|E
50|$|After a short {{successful}} spell as {{head coach}} of Croatian side Hajduk Split from August 2009 to February 2010, Reja opted to quit his <b>job</b> in <b>Split</b> {{in order to become}} the new manager of S.S. Lazio. He was unveiled as the new Lazio head coach the following day, replacing Davide Ballardini. He turned the fortunes of a club in dismay, guiding it out of the relegation zone and into a mid-table finish in the season.|$|R
50|$|Željko Kerum {{was born}} in the village of Ogorje (part of Muć) in the Dalmatian hinterland. He {{graduated}} from the Technical High School in Split, Croatia in 1978. In 1981 he got his first <b>job</b> in the <b>Split</b> construction company Melioracija spending a year in Iraq working on military bases.|$|R
40|$|A hybrid genetic {{algorithm}} (HGA) approach is proposed for a lot-streaming flow shop scheduling problem, {{in which a}} <b>job</b> (lot) is <b>split</b> {{into a number of}} smaller sublots so that successive operations can be overlapped. The objective is the minimization of the mean weighted absolute deviation of job completion times from due dates...|$|R
