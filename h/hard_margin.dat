46|13|Public
5000|$|A perennial, {{succulent}} shrub {{of up to}} 2 m {{tall and}} about as wide, with trunk that rises only a bit above ground, from which many branches spread that become erect. The spine shields form a continuous <b>hard</b> <b>margin</b> along the branch angles.|$|E
30|$|Besides {{testing the}} {{performance}} {{ability of the}} six local shape descriptors described in Section 4.2, we examine the potential of MKL to fuse multiple features with different representation properties. MKL has attracted significant amount of attention in CV research domain. In this paper, the soft margin MKL algorithm introduced by Xu et al. [39] is adopted, where a kernel slack variable is first introduced {{for each of the}} base kernels when learning the kernel. This approach is advancement over the MKL framework generally regarded as the <b>hard</b> <b>margin</b> MKL [40], which imposes sparsity on a category of features and selects the features that best optimize the object function. In fact, it has been pointed out in [39] that the <b>hard</b> <b>margin</b> MKL is a method which only selects the base kernels with minimum objective. This could easily lead to overfitting problem, particularly in a situation where the base kernels contain noisy features. Following the notion of standard <b>hard</b> <b>margin</b> SVM, it is believed that data from two classes can be separated by a <b>hard</b> <b>margin.</b> However, to enable usability of SVM in real applications the slack variables were introduced to the <b>hard</b> <b>margin</b> SVM, which allows some training errors to be incorporated to the training data, thereby minimizing the overfitting problem [39]. This concept inspired the development of soft margin MKL, which instead introduces kernel slack variable for each of the base kernels [39].|$|E
40|$|AbstractThe {{support vector machine}} (SVM) {{represents}} a new and very promising technique for machine learning tasks involving classification, regression or novelty detection. Improvements of its generalization ability {{can be achieved by}} incorporating prior knowledge of the task at hand. We propose a new hybrid algorithm consisting of signal-adapted wavelet decompositions and <b>hard</b> <b>margin</b> SVMs for waveform classification. The adaptation of the wavelet decompositions is tailored for <b>hard</b> <b>margin</b> SV classifiers with radial basis functions as kernels. It allows the optimization of the representation of the data before training the SVM and does not suffer from computationally expensive validation techniques. We assess the performance of our algorithm against the background of current concerns in medical diagnostics, namely the classification of endocardial electrograms and the detection of otoacoustic emissions. Here the performance of <b>hard</b> <b>margin</b> SVMs can significantly be improved by our adapted preprocessing step...|$|E
5000|$|It has smooth, thick, {{straight}} leaves. These {{are dark}} grey-green, and lightly spotted (though juvenile plants sometimes have some rough tubercles). In full sun or under stress, the leaves can assume a purple colour. The leaves have <b>hard,</b> waxy, white <b>margins</b> and keels. The upper {{surface of the}} leaves are slightly concave. They are lanceolate and trigonous, with a point that is rounded but with a tiny spike.|$|R
40|$|Abstract. Various {{problems}} in nonnegative quadratic programming {{arise in the}} training of large margin classifiers. We derive multiplicative updates for these problems that converge monotonically to the desired solutions for <b>hard</b> and soft <b>margin</b> classifiers. The updates differ strikingly in form from other multiplicative updates used in machine learning. In this paper, we provide complete proofs of convergence for these updates and extend previous work to incorporate sum and box constraints in addition to nonnegativity. ...|$|R
60|$|It was now {{perhaps a}} third of the way in, and we {{ourselves}} already <b>hard</b> on the <b>margin</b> of the sea, so that the soft sand rose over my shoes. There was no more to do whatever but to wait, to look as much as we were able at the creeping nearer of the boat, and as little as we could manage at the long impenetrable front of the sandhills, over which the gulls twinkled and behind which our enemies were doubtless marshalling.|$|R
40|$|In the {{interest}} of deriving classifiers that are robust to outlier observations, we present integer programming formulations of Vapnik’s support vector machine (SVM) with the ramp loss and <b>hard</b> <b>margin</b> loss. The ramp loss allows a maximum error of 2 for each training observation, while the <b>hard</b> <b>margin</b> loss calculates error by {{counting the number of}} training observations that are misclassified outside of the margin. SVM with these loss functions is shown to be a consistent estimator when used with certain kernel functions. Based on results on simulated and real-world data, we conclude that SVM with the ramp loss is preferred to SVM with the <b>hard</b> <b>margin</b> loss. Data sets for which robust formulations of SVM perform comparatively better than the traditional formulation are characterized with theoretical and empirical justification. Solution methods are presented that reduce computation time over industry-standard integer programming solvers alone. ...|$|E
40|$|We {{study the}} {{relationship}} between Support Vector Machines (SVM) and Least Squares SVM (LS-SVM). Our main result shows that under mild conditions, LS-SVM for binaryclass classifications {{is equivalent to the}} <b>hard</b> <b>margin</b> SVM based on the well-known Mahalanobis distance measure. We further study the asymptotics of the <b>hard</b> <b>margin</b> SVM when the data dimensionality tends to infinity with a fixed sample size. Using recently developed theory on the asymptotics of the distribution of the eigenvalues of the covariance matrix, we show that under mild conditions, the equivalence result holds for the traditional Euclidean distance measure. These equivalence results are further extended to the multi-class case. Experimental results confirm the presented theoretical analysis. ...|$|E
40|$|Maximal margin based {{frameworks}} {{have emerged}} as {{a powerful tool for}} supervised learning. The extension of these ideas to the unsupervised case, however, is problematic since the underlying optimization entails a discrete component. In this paper, we first study the computational complexity of maximal <b>hard</b> <b>margin</b> clustering and show that the <b>hard</b> <b>margin</b> clustering problem can be precisely solved in O(nd+ 2) time where n is the number of the data points and d is the dimensionality of the input data. However, since {{it is well known that}} many datasets commonly ‘express’ themselves primarily in far fewer dimensions, our interest is in evaluating if a careful use of dimensionality reduction can lead to practical and effective algorithms. We build upon these observations and propose a new algorithm that gradually increases the number of features used in the separation model in each iteration, and analyze the convergence properties of this scheme. We report o...|$|E
40|$|In this paper, we {{formulate}} the Multilinear Support Tensor Machines (MSTMs) {{problem in}} a similar to the Non-negative Matrix Factorization (NMF) algorithm way. A novel set of simple and robust multiplicative up-date rules are proposed {{in order to find}} the multilinear classifier. Updates rules are provided for both <b>hard</b> and soft <b>margin</b> MSTMs and the existence of a bias term is also investigated. We present results on standard gait and action datasets and report faster convergence of equivalent classification performance in comparison to standard MSTMs. ...|$|R
40|$|We present multiplicative updates {{for solving}} <b>hard</b> and soft <b>margin</b> support vector {{machines}} (SVM) with non-negative kernels. They follow {{as a natural}} extension of the updates for non-negative matrix factorization. No additional param- eter setting, such as choosing learning, rate is required. Ex- periments demonstrate rapid convergence to good classifiers. We analyze the rates of asymptotic convergence of the up- dates and establish tight bounds. We test the performance on several datasets using various non-negative kernels and report equivalent generalization errors to that of a standard SVM. Comment: 4 pages, 1 figure, 1 tabl...|$|R
40|$|Making {{profit is}} the {{foundation}} every modern company rests on. No profit, no business. During the passed 10 - 15 years making profits in the computer retailing industry has been <b>harder</b> and <b>harder.</b> <b>Margins</b> has has lessened and due to the simplification of computers and increased easiness to understand computers even more unqualified people may compete in the business. As an effort to increase the profitability of the industry niching was discussed as possible for success and earning money on added value, which is the theme running all through this thesis. The theories this thesis rests on are basically theories about a high quality service, which is the main foundation for any success: no high quality offered, no success. The high competition in this industry {{is one of the}} corner stones of the thesis as well due to the heavy competition in the investigated industry. A third important and clear corner stone are the theories about rules of public procurement which Government owned, or partially owned companies, has to follow. The fact that added value is a subgroup of customer value is natural, but in difference to added value, the concept of customer value is clearly defined, while the meaning of added value is rather ambiguous and undefined. The theories about added value stretches from, spoken from Telia as telephone answering services to co-production of values, some theories even say that added value can only be defined by customers and not by the industry offering it. In this thesis, 25 industrial companies was to be interviewed, of which 14 accepted or had their key responsible in question available. This gives a response frequency of nearly 60 %. Five out of the 14 answering companies have to follow the rules of public procurement. The interviews have been carried out by Perspektiv Undersökningar by phone during the fortnight before mid summer 2004. Eight questions regarding added value, its importance and what added values to pay for have been asked. Very clear in this study was partially the importance of added value, and rather surprising after heaving read the theories, how unanimous the respondents were, who all of them mentioned added value to be the relation between customer and retailer and the importance of suppliers ability to understand and see what the customer need. It is very clear that the ability to make money on added value is limited as the customer regards the added value of relation as obvious for any business transaction at all. However, the customer is prepared to pay a higher price to the supplier offering added value, than to suppliers just caring for transactions. Validerat; 20101217 (root...|$|R
40|$|Abstract. We {{propose a}} novel online kernel {{classifier}} algorithm that converges to the <b>Hard</b> <b>Margin</b> SVM solution. The same update rule {{is used to}} both add and remove support vectors from the current classifier. Experiments suggest that this algorithm matches the SVM accuracies after a single pass over the training examples. This algorithm is attractive when one seeks a competitive classifier with large datasets and limited computing resources. ...|$|E
40|$|Abstract. In a classication problem, <b>hard</b> <b>margin</b> SVMs tend to min-imize the {{generalization}} error by maximizing the margin. Regularization is obtained with soft margin SVMs which improve performances by relax-ing the {{constraints on the}} margin maximization. This article shows that comparable performances can be obtained in the linearly separable case with the Ho{Kashyap learning rule associated to early stopping meth-ods. These methods are applied on a non-destructive control application for a 4 -class problem of rail defect classication. ...|$|E
40|$|Support Vector Machine {{is one of}} {{the most}} {{classical}} approaches for classification and regression. Despite being studied for decades, obtaining practical algorithms for SVM is still an active research problem in machine learning. In this paper, we propose a new perspective for SVM via saddle point optimization. We provide an algorithm which achieves (1 -ϵ) -approximations with running time Õ(nd+n√(d / ϵ)) for both separable (<b>hard</b> <b>margin</b> SVM) and non-separable cases (ν-SVM), where n is the number of points and d is the dimensionality. To the best of our knowledge, the current best algorithm for <b>hard</b> <b>margin</b> SVM achieved by Gilbert algorithm gartner 2009 coresets requires O(nd / ϵ) time. Our algorithm improves the running time by a factor of √(d) /√(ϵ). For ν-SVM, besides the well known quadratic programming approach which requires Ω(n^ 2 d) time joachims 1998 making,platt 199912, no better algorithm is known. In the paper, we provide the first nearly linear time algorithm for ν-SVM. We also consider the distributed settings and provide distributed algorithms with low communication cost via saddle point optimization. Our algorithms require Õ(k(d +√(d/ϵ))) communication cost where k is the number of clients, almost matching the theoretical lower bound...|$|E
5000|$|In April 2005, Hunter was, reportedly, {{offered a}} $1 million bonus to join SAC Capital Partners. Nicholas Maounis, founder of Amaranth Advisors, {{refused to let}} Hunter go. Maounis named Hunter co-head of the firm's energy desk and gave him control of his own trades. In 2006 his {{analysis}} led him to believe that 2006-07 winter's gas prices will rise relative to {{the summer and fall}} - accordingly Hunter went long on the winter delivery contracts, simultaneously shorting the near (summer/fall) contracts. When the market took a sharp turn against this view, the fund was <b>hard</b> pressed for <b>margin</b> money to maintain the positions. Once the margin requirements crossed USD 3 billion, around September 2006, the fund offloaded some of these positions, ultimately selling them entirely to JP Morgan and Citadel for USD 2.5 billion. The fund ultimately took a $6.6-billion loss and had to be dissolved entirely.|$|R
40|$|The dual {{formulation}} of the support vector machine (SVM) objective function is an instance of a nonnegative quadratic programming problem. We reformulate the SVM objective function as a matrix factorization problem which establishes a connection with the regularized nonnegative matrix factorization (NMF) problem. This allows us to derive a novel multiplicative algorithm for solving <b>hard</b> and soft <b>margin</b> SVM. The algorithm follows as {{a natural extension of}} the updates for NMF and semi-NMF. No additional parameter setting, such as choosing learning rate, is required. Exploiting the connection between SVM and NMF formulation, we show how NMF algorithms {{can be applied to the}} SVM problem. Multiplicative updates that we derive for SVM problem also represent novel updates for semi-NMF. Further this unified view yields algorithmic insights in both directions: we demonstrate that the Kernel Adatron algorithm for solving SVMs can be adapted to NMF problems. Experiments demonstrate rapid convergence to good classifiers. We analyze the rates of asymptotic convergence of the updates and establish tight bounds. We test them on several datasets using various kernels and report equivalent classification performance to that of a standard SVM. ...|$|R
5000|$|... 30 April 1995: Middlesbrough {{clinch the}} Division One {{championship}} - {{and the only}} automatic promotion place to the Premier League for this season - by beating Luton Town in the final game at 93-year-old Ayresome Park {{and at the end}} of Bryan Robson's first season in management. Middlesbrough will relocate to the new 30,000-seat Riverside Stadium in August. Meanwhile, up in the Premier League, the final full month of the season ends with Manchester United still pushing Blackburn Rovers <b>hard,</b> with the <b>margin</b> now five points wide with four games to go, as Blackburn blew a chance to extend their lead today as they lost 2-0 at West Ham. Liverpool are the only other team now in with a mathematical chance of winning the title, and all minds at Anfield are focused on the final day of the season when they welcome former manager Kenny Dalglish and his Blackburn Rovers side for what could very well be the title decider. At the other end of the table, Ipswich Town and Leicester City both had their relegation confirmed earlier in the month, while Norwich City have sunk into the relegation zone and Crystal Palace are still in it - but only on goal difference, and they have a game in hand over 18th-placed West Ham United. http://www.manchesterunited-mad.co.uk/footydb/loadgen.asp?Day=17&Month=Apr&ssnno=124&teamno=356 ...|$|R
40|$|Gradient-based {{optimizing}} of gaussian kernel functions is considered. The gradient for {{the adaptation}} of scaling and rotation of the input space is computed to achieve invariance against linear transformations. This is done by using the exponential map as a parameterization of the kernel parameter manifold. By restricting the optimization to a constant trace subspace, the kernel size can be controlled. This is, for example, useful to prevent overfitting when minimizing radius-margin generalization performance measures. The concepts are demonstrated by training <b>hard</b> <b>margin</b> support vector machines on toy data...|$|E
40|$|Recently {{ensemble}} methods like ADABOOST {{have been}} applied successfully in many problems, while seemingly defying the problems of overfitting. ADABOOST rarely overfits in the low noise regime, however, we show that it clearly does so for higher noise levels. Central {{to the understanding of}} this fact is the margin distribution. ADABOOST {{can be viewed as a}} constraint gradient descent in an error function with respect to the margin. We find that ADABOOST asymptotically achieves a <b>hard</b> <b>margin</b> distribution, i. e. the algorithm concentrates its resources on a few hard-to-learn patterns that are interestingly very similar to Support Vectors. A <b>hard</b> <b>margin</b> is clearly a sub-optimal strategy in the noisy case, and regularization, in our case a "mistrust" in the data, must be introduced in the algorithm to alleviate the distortions that single difficult patterns (e. g. outliers) can cause to the margin distribution. We propose several regularization methods and generalizations of the original ADABOOST algorithm to achieve a soft margin. In particular we suggest (1) regularized ADABOOST(REG) where the gradient decent is done directly with respect to the soft margin and (2) regularized linear and quadratic programming (LP/QP-) ADABOOST, where the soft margin is attained by introducing slack variables. Extensive simulations demonstrate that the proposed regularized ADABOOST-type algorithms are useful and yield competitive results for noisy data...|$|E
40|$|A {{sequential}} importance sampling {{algorithm is}} {{developed for the}} distribution that results when a matrix of independent, but not identically distributed, Bernoulli random variables is conditioned on a given sequence of row and column sums. This conditional distribution arises {{in a variety of}} applications and includes as a special case the uniform distribution over zero-one tables with specified margins. The algorithm uses dynamic programming to combine <b>hard</b> <b>margin</b> constraints, combinatorial approximations, and additional non-uniform weighting in a principled way to give state-of-the-art results. Comment: 39 pages (13 pages main text, 26 pages supplementary material); supersedes arXiv: 0906. 100...|$|E
40|$|We {{shed light}} on the {{discrimination}} between patterns belonging to two different classes by casting this decoding problem into a generalized prototype framework. The discrimination process is then separated into two stages: a projection stage that reduces the dimensionality of the data by projecting it on a line and a threshold stage where the distributions of the projected patterns of both classes are separated. For this, we extend the popular mean-of-class prototype classification using algorithms from machine learning that satisfy a set of invariance properties. We report a simple yet general approach to express different types of linear classification algorithms in an identical and easy-to-visualize formal framework using generalized prototypes where these prototypes are used to express the normal vector and offset of the hyperplane. We investigate nonmargin classifiers such as the classical prototype classifier, the Fisher classifier, and the relevance vector machine. We then study <b>hard</b> and soft <b>margin</b> cl assifiers such as the support vector machine and a boosted version of the prototype classifier. Subsequently, we relate mean-of-class prototype classification to other classification algorithms by showing that the prototype classifier is a limit of any soft margin classifier and that boosting a prototype classifier yields the support vector machine. While giving novel insights into classification per se by presenting a common and unified formalism, our generalized prototype framework also provides an efficient visualization and a principled comparison of machine learning classification...|$|R
40|$|Oxygen minimum zones (OMZs; {{midwater}} {{regions with}} O 2 concentrations < 0. 5 ml l? 1) are mid-water features that intercept continental margins at bathyal depths (100 – 1000 m). They are particularly well {{developed in the}} Eastern Pacific Ocean, the Arabian Sea and the Bay of Bengal. Based on analyses of data from these regions, we consider (i) how benthic habitat heterogeneity is manifested within OMZs, (ii) which aspects of this heterogeneity exert the greatest influence on alpha and beta diversity within particular OMZs and (iii) how heterogeneity associated with OMZs influences regional (gamma) diversity on continental margins. Sources of sea-floor habitat heterogeneity within OMZs include bottom-water oxygen and sulphide gradients, substratum characteristics, bacterial mats, and variations in the organic matter content of the sediment and pH. On some <b>margins,</b> <b>hard</b> grounds, formed of phosphorites, carbonates or biotic substrata, represent distinct subhabitats colonized by encrusting faunas. Most of the heterogeneity associated with OMZs, however, is created by strong sea-floor oxygen gradients, reinforced by changes in sediment characteristics and organic matter content. For the Pakistan margin, combining these parameters revealed clear environmental and faunal differences between the OMZ core and {{the upper and lower}} boundary regions. In all Pacific and Arabian Sea OMZs examined, oxygen appears to be the master driver of alpha and beta diversity in all benthic faunal groups for which data exist, as well as macrofaunal assemblage composition, particularly in the OMZ core. However, other factors, notably organic matter quantity and quality and sediment characteristics, come into play as oxygen concentrations begin to rise. The influence of OMZs on meiofaunal, macrofaunal and megafaunal regional (gamma) diversity is difficult to assess. Hypoxia is associated with a reduction in species richness in all benthic faunal groups, but there is also evidence for endemism in OMZ settings. We conclude that, on balance, OMZs probably enhance regional diversity, particularly in taxa such as Foraminifera, which are more tolerant of hypoxia than others. Over evolutionary timescales, they may promote speciation by creating strong gradients in selective pressures and barriers to gene flow...|$|R
40|$|The {{objectives}} {{of this study}} were to (a) identify the pathogenic agent causing symptoms in the upper respiratory tract using clinical examination only, (b) determine the agreement between clinical examination and laboratory diagnostics, (c) evaluate the use of lissamine green dye in the diagnosis of a feline herpes viral infection and (d) determine the interobserver reliability between a qualified veterinarian and an ophthalmologic specialist. 99 cats with upper respiratory tract symptoms were clinically examined; simultaneously, a sample for laboratory analyses using real time PCR was taken. 22 of these 99 animals were individually examined by the vet and the specialist, with only the specialist using a slit-lamp biomicroscope and the veterinarian using common methods for examination used by practitioners. All 99 cats showed clinical symptoms of cat flu. The PCR determined a pathogenic agent in 63 of them. In 57 cats, the veterinarian diagnosed only one causing agent (monoinfection) whilst in 42 cats the symptoms indicated a mixed infection caused by multiple pathogenic agents. The PCR also detected a monoinfection in 40 cats and a mixed infection in 23. The veterinarian diagnosed a feline herpes viral infection in 33 of the cats and in 45. 5 % (15 / 33 animals), the diagnosis was supported by the results of the PCR. Another 33 cats were diagnosed with the feline calicivirus and in 91. 0 % (30 / 33) the PCR also detected the feline calicivirus. Five of these 33 cats showed signs of massive conjunctivae ulceration whilst another two cats had plaque-like deposits on the conjunctiva. A Chlamydophila felis infection was diagnosed by the veterinarian in 63 animals and supported by the PCR result in 23 animals (36. 5 %). The veterinarian diagnosed 27 cats with an infection caused by Mycoplasma felis, and was supported by a positive PCR for this agent in 66. 6 % (18 / 27) of these animals. The diagnosis of feline herpes virus and feline calicivirus by only using clinical symptoms is well established and was proved to be successful in this clinical study. The pathognomic symptoms of Chlamydophila felis and Mycoplasma felis are not particularly well suited for differentiating between the two agents. It was noted that the ulcerative lesions caused by a feline calicivirus infection were usually limited to the oral cavity, tongue <b>margins</b> <b>hard</b> palate, tonsils and lungs, and this also seemed to be conjunctivally pathogenic in our study. Therefore, it can be assumed that not only the feline herpes virus causes viral conjunctivitis in cats. The findings by the veterinarian and the specialist were largely consistent with each other and it was observed that a practising veterinarian is able to identify the pathogenic agent causing the symptoms in upper respiratory tract infections. Furthermore, {{there seemed to be no}} advantage in using a slit lamp biomicroscope. Lissamine green was shown to be useful for the visualisation of ulcerations and fine dendritic or punctual lesions in the cornea at the early stage of a herpes virus infection. It was not possible to visualise the dendritic corneal lesions using fluorescein. Also lissamine green proved to be uncomplicated in its application. As a final conclusion of this study it should be noted that the different agents of the cat flu can be well determined by clinical examination...|$|R
40|$|International audienceTo set {{the values}} of the {{hyperparameters}} of a support vector machine (SVM), the method of choice is cross-validation. Several upper bounds on the leave-one-out error of the pattern recognition SVM have been derived. One of the most popular is the radius-margin bound. It applies to the <b>hard</b> <b>margin</b> machine, and, by extension, to the 2 -norm SVM. In this article, we introduce the first quadratic loss multi-class SVM: the M-SVM^ 2. It {{can be seen as a}} direct extension of the 2 -norm SVM to the multi-class case, which we establish by deriving the corresponding generalized radius-margin bound...|$|E
40|$|In {{this paper}} we {{describe}} and analyze sublinear-time approximation algorithms for some optimization problems arising in machine learning, such as training linear classifiers and finding minimum enclosing balls. Our algorithms {{can be extended}} to some kernelized versions of these problems, such as SVDD, <b>hard</b> <b>margin</b> SVM, and L 2 -SVM, for which sublinear-time algorithms were not known before. These new algorithms {{use a combination of}} a novel sampling techniques and a new multiplicative update algorithm. We give lower bounds which show the running times of many of our algorithms to be nearly best possible in the unit-cost RAM model. 1...|$|E
40|$|The goal of {{this article}} is to {{investigate}} and suggest tech-niques for health condition monitoring and diagnosis using machine learning from sensor data. In particular, this arti-cle overview and discusses support vector machines meth-ods such as <b>hard</b> <b>margin</b> and soft margin problems. In or-der to investigate the abnormalities and classify a large set of data an iterative Support Vector Machine algorithm was constructed. However, similar techniques could be applied to analyze or monitor for abnormality various other com-plex devices or even computer methods. Key words Support Vector Machines, Health condition monitoring, Novelty detection and Machine learning methods...|$|E
40|$|Clustering {{of similar}} data items is an {{important}} technique in mining useful patterns. To enhance the performance of Clustering, training or learning {{is an important}} task. A constraint learning semi-supervised methodology is proposed which incorporates SVM and Normalized Pointwise Mutual Information Computation Strategy to increase the relevance {{as well as the}} performance efficiency of clustering. The SVM Classifier is of <b>Hard</b> <b>Margin</b> Type to roughly classify the initial set. A recursive re-clustering approach is proposed for achieving higher degree of relevance in the final clustered set by incorporating ENNPI algorithm. An overall enriched F-Measure value of 94. 09 % is achieved as compared to existing algorithms...|$|E
40|$|Abstract. <b>Hard</b> <b>margin</b> support vector {{machines}} (HM-SVMs) have a risk {{of getting}} overfitting {{in the presence of}} the noise. Soft margin SVMs deal with this problem by the introduction of the capacity control term and obtain the state of the art performance. However, this disposal leads to a relatively high computational cost. In this paper, an alternative method, greedy stagewise algorithm, named GS-SVMs is presented to deal with the overfitting of HM-SVMs without the introduction of capacity control term. The most attractive property of GS-SVMs is that its computational complexity scales quadratically with the size of training samples in the worst case. Extensive empirical comparisons confirm the feasibility and validity GS-SVMs. ...|$|E
40|$|Boosting methods {{maximize}} a hard classification margin and {{are known}} as powerful techniques that do not exhibit overfitting for low noise cases. Also for noisy data boosting will try to enforce a <b>hard</b> <b>margin</b> and thereby give too much weight to outliers, which then leads to the dilemma of non-smooth fits and overfitting. Therefore we propose three algorithms to allow for soft margin classification by introducing regularization with slack variables into the boosting concept: (1) AdaBoost reg and regularized versions of (2) linear and (3) quadratic programming AdaBoost. Experiments show {{the usefulness of the}} proposed algorithms in comparison to another soft margin classifier: the support vector machine...|$|E
40|$|AbstractInvestigation of the {{nutrient}} disease in oil palm motivates {{the need for}} a programmed detection system. Automated detection using vision system and pattern recognition are implemented to detect the symptoms of nutrient diseases and also to classify the disease group. In this paper, Support Vector Machine (SVM) is evaluated as classifier with three different kernels namely linear kernel, polynomial kernel with soft margin and polynomial kernel with <b>hard</b> <b>margin.</b> Initial results show that the recognition of oil palm leaves is possible to be performed by SVM classifier. Based on the best performance result, polynomial kernel with soft margin is capable of classifying nutrient diseases accurately in the oil palm leaves with accuracy of 95 % of correct classification...|$|E
40|$|Abstract—We give sublinear-time {{approximation}} algorithms {{for some}} optimization problems arising in machine learning, such as training linear classifiers and finding minimum enclosing balls. Our algorithms {{can be extended}} to some kernelized versions of these problems, such as SVDD, <b>hard</b> <b>margin</b> SVM, and L 2 -SVM, for which sublinear-time algorithms were not known before. These new algorithms {{use a combination of}} a novel sampling techniques and a new multiplicative update algorithm. We give lower bounds which show the running times of many of our algorithms to be nearly best possible in the unitcost RAM model. We also give implementations of our algorithms in the semi-streaming setting, obtaining the first low pass polylogarithmic space and sublinear time algorithms achieving arbitrary approximation factor. I...|$|E
40|$|Abstract. We {{introduce}} a novel, robust data-driven regularization strat-egy called Adaptive Regularized Boosting (AR-Boost), {{motivated by a}} desire to reduce overfitting. We replace AdaBoost’s <b>hard</b> <b>margin</b> with a regularized soft margin that trades-off between a larger margin, at the expense of misclassification errors. Minimizing this regularized exponen-tial loss results in a boosting algorithm that relaxes the weak learning assumption further: it can use classifiers with error greater than 1 2. This enables a natural extension to multiclass boosting, and further reduces overfitting in both the binary and multiclass cases. We derive bounds for training and generalization errors, and relate them to AdaBoost. Fi-nally, we show empirical results on benchmark data that establish the robustness of our approach and improved performance overall. ...|$|E
40|$|Abstract The {{standard}} SVR formulation for real-valued function approximation on multidimensional spaces {{is based}} on the ɛ-insensitive loss function, where errors are considered not correlated. Due to this, local information in the feature space which can be useful to improve the prediction model is disregarded. In this paper we address this problem by defining a generalized quadratic loss where the co-occurrence of errors is weighted according to a kernel similarity measure in the feature space. We show that the resulting dual problem can be expressed as a <b>hard</b> <b>margin</b> SVR in a different feature space when the co-occurrence error matrix is invertible. We compare our approach against a standard SVR on two regression tasks. Experimental results seem to show an improvement in the performance...|$|E
40|$|We give sublinear-time {{approximation}} algorithms {{for some}} optimization problems arising in machine learning, such as training linear classifiers and finding minimum enclosing balls. Our algorithms {{can be extended}} to some kernelized versions of these problems, such as SVDD, <b>hard</b> <b>margin</b> SVM, and L 2 -SVM, for which sublinear-time algorithms were not known before. These new algorithms {{use a combination of}} a novel sampling techniques and a new multiplicative update algorithm. We give lower bounds which show the running times of many of our algorithms to be nearly best possible in the unit-cost RAM model. We also give implementations of our algorithms in the semi-streaming setting, obtaining the first low pass polylogarithmic space and sublinear time algorithms achieving arbitrary approximation factor. ...|$|E
40|$|The {{classical}} perceptron rule {{provides a}} varying {{upper bound on}} the maximum margin, namely {{the length of the}} current weight vector divided {{by the total number of}} updates up to that time. Requiring that the perceptron updates its internal state whenever the normalized margin of a pattern is found not to exceed a certain fraction of this dynamic upper bound we construct a new approximate maximum margin classifier called the perceptron with dynamic margin (PDM). We demonstrate that PDM converges in a finite number of steps and derive an upper bound on them. We also compare experimentally PDM with other perceptron-like algorithms and support vector machines on <b>hard</b> <b>margin</b> tasks involving linear kernels which are equivalent to 2 -norm soft margin. Comment: 16 page...|$|E
