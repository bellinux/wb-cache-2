8|169|Public
50|$|Again in 1990 and 1991, Congress {{provided}} {{only half}} of the year's funding to NOAA, requesting that agencies that used Landsat data provide the funding for the other six months of the upcoming year.In 1992, various efforts were made to procure funding for follow on Landsats and continued operations, {{but by the end of}} the year EOSAT ceased processing Landsat data. Landsat 6 was finally launched on October 5, 1993, but was lost in a launch <b>failure.</b> <b>Processing</b> of Landsat 4 and 5 data was resumed by EOSAT in 1994. NASA finally launched Landsat 7 on April 15, 1999.|$|E
40|$|Abstract. Arc {{suppression}} coil {{has been}} applied widely in power system. It focuses on the fault that arc suppression coil often appears in the process, and divides {{to the nature of}} the faults. We also take corresponding treatment methods for different faults at the same time. This article introduces the <b>failure</b> <b>processing</b> methods of the arc suppression coil in detail, expounding respectively that the general <b>failure</b> <b>processing</b> methods, the abnormal <b>failure</b> <b>processing</b> methods of arc suppression coil, the action <b>failure</b> <b>processing</b> methods and the discontinued <b>failure</b> <b>processing</b> methods...|$|E
40|$|Abstract. The {{vacuum system}} control {{strategy}} based on event-driven approach is considered for monitoring startup status, stable operation status, shutdown status and <b>failure</b> <b>processing</b> of {{transmission electron microscope}} (TEM). Hierarchical structure is adopted so as to reasonably compartmentalize the vacuum control tasks. For startup operation, control logic flow of vacuum system is designed by event-based control mode. Finally, through the actual operation of the test {{results indicate that the}} vacuum control system fulfills design requirements and the event-based control mode is effective. 1...|$|E
5000|$|Exception Handling [...] - [...] where <b>failures</b> in <b>processing</b> {{can result}} in an {{alternate}} pipeline being processed ...|$|R
40|$|This paper {{analyzes}} {{the characteristics of}} the enterprise after-sale service records for field failure data, and summarizes the types of field data. Maximum likelihood estimation method and the least squares method are presented for the complexity and difficulty of field <b>failure</b> data <b>processing,</b> and Monte Carlo simulation method is proposed. Monte Carlo simulation, the relatively simple calculation method, is an effective method, whose result is closed to that of the other two methods. Through the after-sale service records analysis of a specific electromagnetic flowmeter enterprises, this paper illustrates the effectiveness of field <b>failure</b> data <b>processing</b> methods...|$|R
5000|$|The same {{probability}} curves {{are used}} to model non-detection and slow response failures. These are very different processes, and {{it is unlikely that}} identical curves could model their behaviour. Furthermore, it is uncertain as to whether such curves could be applied to situations in which detection <b>failures</b> or <b>processing</b> difficulties are the primary dominating factors of influence.|$|R
40|$|As an {{important}} means to compose independent services together {{to fulfill a}} function, service composition is widely applied in different applications. However, the process of composition is complex and error-prone, which makes a formal modeling and analysis method highly desirable. A BPEL process based on Petri net (BPEL-Net) model is presented in this paper, which is capable of capturing behavior of the participating services accurately. A set of rules are proposed to convert atomic activity and structural activity of BPEL process into BPEL-Net model, the transactional properties of services and <b>failure</b> <b>processing</b> between services are also characterized by BPEL-Net. Based on the states of constructed BPEL-Net, we advance the concept of transfer matrix to analyze reliability and related properties. What’s more, we put forward two simplification schemas for BPEL- Net. Finally, a specific example is given to simulate analytical process with tool Matlab, {{the results show that}} the method can be a good solution to analyze the reliability of BPEL processes. </p...|$|E
40|$|Abstract—As an {{important}} means to compose independent services together {{to fulfill a}} function, service composition is widely applied in different applications. However, the process of composition is complex and error-prone, which makes a formal modeling and analysis method highly desirable. A BPEL process based on Petri net (BPEL-Net) model is presented in this paper, which is capable of capturing behavior of the participating services accurately. A set of rules are proposed to convert atomic activity and structural activity of BPEL process into BPEL-Net model, the transactional properties of services and <b>failure</b> <b>processing</b> between services are also characterized by BPEL-Net. Based on the states of constructed BPEL-Net, we advance the concept of transfer matrix to analyze reliability and related properties. What’s more, we put forward two simplification schemas for BPEL-Net. Finally, a specific example is given to simulate analytical process with tool Matlab, {{the results show that}} the method can be a good solution to analyze the reliability of BPEL processes. Index Terms—Service composition, BPEL, transactional properties, transfer matrix, Petri net, Matla...|$|E
40|$|Decision-making {{in product}} quality is an {{indispensable}} stage in product development, {{in order to}} reduce product development risk. Based on the identification of the deficiencies of quality function deployment (QFD) and failure modes and effects analysis (FMEA), a novel decision-making method is presented that draws upon a knowledge network of failure scenarios. An ontological expression of failure scenarios is presented together with a framework of failure knowledge network (FKN). According to the roles of quality characteristics (QCs) in <b>failure</b> <b>processing,</b> QCs are set into three categories namely perceptible QCs, restrictive QCs, and controllable QCs, which present the monitor targets, control targets and improvement targets respectively for quality management. A mathematical model and algorithms based on the analytic network process (ANP) is introduced for calculating the priority of QCs with respect to different development scenarios. A case study is provided according to the proposed decision-making procedure based on FKN. This methodology is applied in the propeller design process {{to solve the problem of}} prioritising QCs. This paper provides a practical approach for decision-making in product quality...|$|E
40|$|Background: There is a {{need for}} greater {{understanding}} of the epidemiology of primary care patient safety in order to generate solutions to prevent future harm. Aim: To estimate the rate of <b>failures</b> in <b>processing</b> actions requested in hospital discharge summaries and to determine factors associated with these failures. Design and setting: We undertook a retrospective records review. Our study population was emergency admissions for patients aged ≥ 75 years, drawn from ten practices in three areas of England. Method: One GP researcher reviewed the records for 300 patients after hospital discharge to determine the rate of compliance with actions requested in the discharge summary and to estimate the rate of associated harm from non-compliance. Where GPs documented decision making contrary to what was requested, these instances did not constitute failures. Data were also collected on time taken to process discharge communications. Results: There were <b>failures</b> in <b>processing</b> actions requested in 46...|$|R
40|$|This {{guide is}} a {{reference}} {{for understanding the}} various aspects of microelectromechanical systems, or MEMS, {{with an emphasis on}} device reliability. Material properties, <b>failure</b> mechanisms, <b>processing</b> techniques, device structures, and packaging techniques common to MEMS are addressed in detail. Design and qualification methodologies provide the reader with the means to develop suitable qualification plans for the insertion of MEMS into the space environment...|$|R
40|$|The {{spur gear}} in {{question}} {{was found to have}} cracks at most of the spline roots (~ 80 %). The crack surface showed brittle mode of failure. Analysis suggests hydrogen embrittlement as the mechanism of <b>failure.</b> Improper <b>processing</b> appears to be the reason for hydrogen embrittlement. A detailed analysis of the failure has been presented in this report and a few recommendations suggested for preventing the recurrence of similar failure...|$|R
40|$|IntroductionThe {{electrocardiogram}} (ECG) is {{an important}} tool in the initial evaluation of patients with congestive heart failure and in the monitoring of these patients. The purpose of this work is to describe the different electrocardiographic abnormalities encountered in patients monitored for chronic heart failure. MethodsA retrospective study of major electrocardiographic abnormalities, involving 1622 patients with chronic heart failure, followed in the heart <b>failure</b> <b>processing</b> unit (UTIC) of UH Ibn Rochd of Casablanca, between January 2008 and October 2014. ResultsThe mean age of patients was 68. 6 years (± 12 years, ranging from 18 to 104 years), with a sex ratio of 1. 67. The main cardiovascular risk factors identified were: hypertension (46. 1 %), diabetes (38. 8 %), tobacco (38. 6 %) and dyslipidemia (14. 9 %) cases. The etiology of heart fealure was in order of frequency: ischemic heart disease (55. 6 %), DCM (27. 5 %), hypertensive heart disease (9. 8 %) and valvulopathy (5. 7 %). The major éléctrocardigraphique abnormalities were: repolarization disorders (41. 9 %), the Q wavenecrosis (18. 7 %) and atrial fibrillation (11. 8 %). The conduction disorders have been dominated by the left bundle branch block (23. 9 %), followed by the first atrio-ventricular block (11. 02 %). The signs of cavitary hypertrophy was noted especially on the left side: left ventricular hypertrophy was observed in 11. 4 % of cases, while the left atrial hypertrophy was observed in 3. 3 % of cases. Recorded rhythm disorders had been dominated by the atrial fibrillation (11. 8 %) followed by the ventricular rhythm disorder (8. 8 %). ConclusionElectrocardiographic abnormalities are common in patients with chronic heart failure and a normal ECG must review the diagnosis. The conrol of rhythm disorders and prevention of its complications is a major pillar {{in the management of}} chronic heart failure...|$|E
40|$|Efforts {{have been}} taken seriously by the {{government}} {{as one of the}} national agendas to provide affordable own house for every Malaysian in this country (Budget 2014), nevertheless there are some obstacles that interfere with the government to realize that goal. That barrier is the negative and bad practices of business ethic attitudes among developers during the implementation of the housing project are often featured in the media. The attitude of business ethic developers was motivated and influenced by ethical attitudes conducted by all stakeholders in the housing projects. Brenner and Molander, (1977), Moore, (1999) and Wiley, (1995) posit that, developers with the practice of good business ethic attitudes, will enhance and upgrade the housing project performance. If the unethical business practices continue to be practiced by developers, it will affect the cost to business organizations (McMahon and Harvey, (2007) and Trevino & Victor, (1992), spirit of work motivation of employees (Beu and Buckley, (2001), Karpoff, Lee and Martin, (2008) and self government and the community on the implementation of housing projects undertaken (Jones and Kavanagh, (1996), McMahon and Harvey(1996). Despite the positive growth of housing projects, there are things that many overlook. Previous research has proven the performance issues of housing projects that often burden the purchaser. For example, buyers not only have to bear the financial burden from the bank, but there are also plagued with frustration due to <b>failure</b> <b>processing</b> the dreamt home. The Ministry of Housing and Local Government (KPKT) has received complaints about housing project that was especially abandoned projects that affect the buyer in terms of the financial burden, including the repayment of loan installments and rental payments occupied home buyers. In addition, home buyers {{are more likely to be}} blacklisted by the financiers of the financial institutions, if they fail to settle outstanding debts and this will cause in difficulties of obtaining next loan facilities. (Abu Bakar, 2009) ...|$|E
40|$|This paper {{analyzes}} historical {{successes and}} <b>failures</b> in meat <b>processing</b> using {{a case study}} methodology, especially {{as it relates to}} possible changes in Canadian market access. Cases include: IPB and economies of size; Canada Packers labor failures; and Tyson and Certified Angus branding strategies. Agribusiness,...|$|R
50|$|The MMME Department {{offers the}} {{following}} services to private industry, among others: consultancy services, special in-house training in materials science and special topics in metallurgical engineering, assorted testing services involving materials characterization and analysis, pilot plant testing in mineral <b>processing,</b> <b>failure</b> analysis, and mining designs.|$|R
40|$|A <b>processing</b> <b>failure</b> of void {{formation}} {{has been}} observed in 3 D IC microbumps due to small solder volume. We prepared the sandwiched Ni/Sn 2. 3 Ag/Ni microbumps with 4 μm and 11 μm thick solders and reflowed them at 260 ◦C to study the mechanism of void formation in the processing. Due to the thin solder, intermetallic compound formation of Ni 3 Sn 4 from the two interfaces of the solder joint can physically bridge each other. When that happens, the degree of freedom of motion in the direction normal to the interfaces is removed. Consequently, when the remaining molten solder is drained by side wall reaction, large voids form in the joint. This is a unique mode of <b>processing</b> <b>failure</b> because of the smaller and smaller volume of solder joints in the trend of miniaturization...|$|R
40|$|AbstractmRNA was {{prepared}} from autopsy liver samples from a homozygote for α 1 -antitrypsin deficiency (PiZZ) {{and from a}} normal (PiMM) subject. Both preparation gave equivalent synthesis of α 1 -antitrypsin in a wheat germ cell-free system. This suggests that the deficiency of plasma α 1 -antitrypsin associated with the Z variant {{is due to a}} <b>failure</b> of <b>processing</b> and secretion of the protein rather than of its synthesis. It is likely that it is the resultant intracellular accumulation of the Z protein rather than a deficiency of protease inhibitor that is the primary cause of the liver pathology associated with this variant...|$|R
40|$|Explicit {{memory errors}} may occur when {{individuals}} fail to retrieve information about items previously studied (item memory) {{or about the}} learning context (source memory). We examined electrophysiological measures during recognition failure {{in order to determine}} the influence of retrieval orientation for item versus source information. Recognition failure was associated with brain potentials distinct from those associated with success. Furthermore, source-memory failures were associated with earlier-onset brain potentials with a more anterior distribution compared to item-memory <b>failures.</b> Neurocognitive <b>processing</b> was thus modulated by retrieval orientation so as to differentially influence neural correlates of successful versus unsuccessful retrieval...|$|R
40|$|Fifty years ago, John von Neumann {{compared}} {{the architecture of}} the brain with that of computers that he invented and which is still in use today. In those days, the organisation of computers was based on concepts of brain organisation. Here, we give an update on current results on the global organisation of neural systems. For neural systems, we outline how the spatial and topological architecture of neuronal and cortical networks facilitates robustness against <b>failures,</b> fast <b>processing,</b> and balanced network activation. Finally, we discuss mechanisms of self-organization for such architectures. After all, the organization of the brain might again inspire computer architecture...|$|R
25|$|On 25 November 2010, NAB {{suffered}} a system malfunction {{resulting in the}} <b>failure</b> of accounts <b>processing.</b> As a result, around 60,000 banking transactions were lost, {{and had to be}} manually recovered. The malfunction was caused by a corruption of an irreplaceable system file. This issue has been dubbed by some commentators as one of the biggest failures {{in the history of the}} Australian banking system.|$|R
30|$|The overall {{performance}} of LDPC decoding depends significantly {{on the number}} of decoding iterations. Large numbers of iterations may result in unacceptably long delays that may, in turn, lead to <b>failures</b> in real-time <b>processing.</b> Therefore, programmers typically set a limit on the maximum number of iterations allowed by LDPC. If the parity-check equation is satisfied, decoding is completed even before the maximum number of iteration is reached.|$|R
50|$|On 25 November 2010, NAB {{suffered}} a system malfunction {{resulting in the}} <b>failure</b> of accounts <b>processing.</b> As a result, around 60,000 banking transactions were lost, {{and had to be}} manually recovered. The malfunction was caused by a corruption of an irreplaceable system file. This issue has been dubbed by some commentators as one of the biggest failures {{in the history of the}} Australian banking system.|$|R
5000|$|A {{focus on}} {{reliability}} and availability. Large-scale systems with {{hundreds or thousands}} of processing nodes are inherently more susceptible to hardware failures, communications errors, and software bugs. Data-intensive computing systems are designed to be fault resilient. This typically includes redundant copies of all data files on disk, storage of intermediate processing results on disk, automatic detection of node or <b>processing</b> <b>failures,</b> and selective re-computation of results.|$|R
40|$|This paper {{describes}} the different steps in software <b>failure</b> data <b>processing</b> {{in order to}} monitor the software development and to quantify the operational reliability. Processing consists in (a) filtering the raw data {{in order to keep}} only those corresponding to software failures without duplicate, (b) partitioning of data into sub-sets according for instance to failure severity or fault location, (c) performing descriptive analyses, (d) analyzing the trend and (e) when possible and needed, applying reliability growth models. These steps are part of an overall method experienced at LAAS on several real-life software systems. The goals of the paper are twofold: first, present the method and, second, show its benefits through its application to data collected on a switching system...|$|R
40|$|The {{semiconductor}} industry is the aggregate collection of companies {{engaged in the}} design and fabrication of semiconductor devices. To establish the good company of semiconductor, the development of failure analysis organization must be made. Failure analysis is the process of collecting and analyzing data to determine the cause of a failure. It is an important discipline in many branches of manufacturing industry, such as the electronics industry, where it is a vital tool used in the development of new products and for the improvement of existing products. The failure analysis process relies on collecting failed components for subsequent examination of the cause or causes of failure using a wide array of methods, especially microscopy and spectroscopy. This project will discuss about the failure analysis step and procedure. The aim of this project is to design a new backside decapsulation method in order to improve the <b>failure</b> analysis <b>processing</b> time and root cause finding. This project contains of several parts which are electrical analysis, IC decapsulation with chemical analysis and fault isolation step. Comparison between the old method and new method result has been made and the effectiveness of the sample has been measured by performing analysis on 20 samples. The simulation and measurement results show 85. 5 % of improvement in term of <b>failure</b> analysis <b>processing</b> time by comparing with old method. In term of root cause identification, the root cause has been successfully identify for all samples which were related to EOS, ESD and Wafer Fabrication related failure classified as Poly and GOX. Based on this result, the success rate for 20 samples by using new backside decapsulation technique is 100 %...|$|R
40|$|The {{regulation}} of antigen processing and presentation to MHC class I-restricted cytolytic T lymphocytes was studied in cells infected with murine cytomegalovirus. Recognition by cytolytic T lymphocytes of the phosphoprotein pp 89, the immunodominant viral antigen {{expressed in the}} immediate-early phase of infection, was selectively prevented during the subsequent expression of viral early genes. The surface expression of MHC class I glycoproteins and their capacity to present externally added pp 89 -derived antigenic peptides were not affected. Because recognition of several other antigens occurred during the early phase, a general <b>failure</b> in <b>processing</b> and presentation was excluded. Since neither rate of synthesis, amount, stability, nor nuclear transport of pp 89 was modified, the failure in recognition indicates a selective interference with pp 89 antigen processing and presentation. ...|$|R
30|$|Traumatic {{memories}} are intrusive and rigid {{areas that are}} dominated by non-integrated free floating emotions, which disrupt the normal process of making connections, disturbing the production of ordinary memories (Hartmann 1998; Varvin et al. 2012). Therefore, these memories will mostly come back as emotional and sensory states, given that the individual {{is not able to}} represent them verbally due to the <b>failure</b> of <b>processing</b> information on a symbolic level (that {{may be due to the}} core of Post Traumatic Stress Disorder), becoming unable to integrate traumatic experiences with other life experiences (Van der Kolk 1996). The inability of deaf people to communicate their fears and unknown experiences results in the experience of several negative emotions during or after the stressful, traumatic events, ensuring traumatisation and increasing the probability of developing a trauma-related disorder (Rosenman 2002).|$|R
40|$|Multicast {{capability}} at {{the network}} layer is extremely important for real-time multipoint, multimedia applications. We consider protocols for address management, connection control, and data transport {{in the context of}} real-time, multicast communications. We first describe two architectures [...] - one semi-distributed, the other fully distributed [...] - and associated protocols for dynamically managing multicast addresses and performing connection control (i. e. maintaining connection state for a session) for multiparty applications. An underlying IP-based internetworking environment is assumed. We evaluate our two designs, and compare them with three other schemes, based on the following criteria: blocking probability and consistency, address acquisition delay, load on address management entities, robustness against <b>failures,</b> and <b>processing</b> and communications overhead. With the distributed scheme the probability of blocking for address acquisition is reduced by several orders of magnitude w [...] ...|$|R
40|$|Performance of {{electronic}} {{devices such as}} low voltage connector, relays and switches, flexible circuits, bonded devices, and high current circuits critically depends on surface chemistry. Understanding and controlling this chemistry through use of corrosion inhibitors and surface chemistry can eliminate <b>failures</b> and optimize <b>processing</b> and design. Examples will cite application of ISS (Ion Scattering Spectroscopy) and SIMS (Secondary Ion Mass Spectroscopy) surface analysis to investigate corrosion, degradation, segregation, and contamination {{of electronic}} devices, including correlation of oxide thickness, diffusion, and contamination with performance...|$|R
40|$|In this paper, two Erlang {{models of}} a two-machine, one-buffer {{production}} line are discussed. Both are {{extensions of the}} exponential production line model and treat random <b>processing,</b> <b>failure,</b> and repair times. In the first, worker intervention occurs only when a failure takes place; in the second maintenance occurs whenever a machine is idle due to starvation or blockage. Numerical results from the first model are indistinguishable {{from those of the}} exponential model; a substantial increase in throughput is observed in the second. I...|$|R
40|$|The {{magnetic}} field {{investigation of the}} Cluster four-spacecraft mission {{is designed to provide}} intercalibrated measurements of the B {{magnetic field}} vector. The instrumentation and data processing of the mission are discussed. The instrumentation is identical on the four spacecraft. It consists of two triaxial fluxgate sensors and of a <b>failure</b> tolerant data <b>processing</b> unit. The combined analysis of the four spacecraft data will yield such parameters as the current density vector, wave vectors, and the geometry and structure of discontinuities...|$|R
40|$|ICMPv 6 is {{the newest}} version of {{internet}} control message protocol, whose main purpose is to send error message indicating packet <b>processing</b> <b>failure.</b> It is know that ICMPv 6 is technologically vulnerable. One of those vulnerabilities is the ICMPv 6 RA flooding vulnerability, {{which can lead to}} systems in Local Area Network slow down or full stop. This paper will discuss Windows (XP, 7, 8. 1) and Linux Ubuntu 14 operating systems resistance to RA flooding attack research and countermeasures to minimize this vulnerability...|$|R
5000|$|As of 2016, Cheriton {{is working}} with Stanford {{students}} on transactional memory, making memory systems that are resilient to <b>failures.</b> [...] "In-memory <b>processing</b> leads to dramatically faster computers — in some cases speeding up applications {{by a factor of}} 100,000. It changes the complete nature of how a business can run. We’re trying to lower the cost and to fit these systems in existing memory structures and reduce the number of components to make them more reliable and more secure," [...] said Cheriton in a 2016 interview.|$|R
40|$|We {{describe}} a distributed architecture for managing multicast addresses {{in the global}} Internet. A multicast address space partitioning scheme is proposed, based on the unicast host address and a per-host address management entity. By noting that port numbers {{are an integral part}} of end-to-end multicast addressing we present a single, unified solution to the two problems of dynamic multicast address management and port resolution. We then present a framework for the evaluation of multicast address management schemes, and use it to compare our design with three recently proposed approaches, as well as a random allocation strategy. The criteria used for the evaluation are blocking probability and consistency, address acquisition delay, the load on address management entities, robustness against <b>failures,</b> and <b>processing</b> and communications overhead. With the distributed scheme the probability of blocking for address acquisition is reduced by several orders of magnitude, to insignificant lev [...] ...|$|R
40|$|Measurement of the {{temporal}} behaviour of laser beam properties like total beam power, intensity distribution, polarization or frequency can be necessary for {{a better understanding of}} laser dynamics as well as processing results in laser material working. In this paper, we report on the measurements with a linear 12 -segment array of pyroelectric detectors, which allow parallel registration of the detector signals with a bandwidth of 1 MHz. Although the relevant time constants of the thermal processes in CO 2 laser material processing in many cases are greater than the time constants, we are reporting here, knowledge of {{the temporal}} behaviour of the laser beam parameters can be important for special applications, e. g. precise material working with small workpiece dimensions. Pulse modulation capability, e. g. by pump power modulation, increases the flexibility of a laser source, but disregarding laser dynamics can lead to undesired <b>processing</b> results or <b>failure</b> of <b>processing...</b>|$|R
