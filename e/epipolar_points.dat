2|23|Public
50|$|Since {{the optical}} {{centers of the}} cameras lenses are distinct, each center {{projects}} onto a distinct point into the other camera's image plane. These two image points are denoted by eL and eR are called epipoles or <b>epipolar</b> <b>points.</b> Both epipoles eL and eR in their respective image planes and both optical centers OL and OR lie on a single 3D line.|$|E
40|$|Due to {{the rapid}} error growth of {{navigation}} systems using low-cost inertial measurement units {{there is a need}} to fuse the information with complementary sensors. In this paper a monocular camera is used to aid the system. Unlike SLAM-like approaches the problem of estimating the location of each feature point viewed in a scene is avoided, instead estimated <b>epipolar</b> <b>points</b> on the image plane are used. By maintaining a buffer of past views, the method mimics a short-term visual memory which imposes multiple constraints on the estimation problem. The result is a Sigma-Point Kalman filter in square-root form with a linear and efficient time-update. A simulation study is presented indicating the filter's capacity to constrain the rate of error growth of an inertial navigation system. The filter may also find useful applications when fusing with additional sensors. QC 20110407 </p...|$|E
50|$|An epipolar {{line is a}} {{function}} of the position of point X in the 3D space, i.e. as X varies a set of epipolar lines is generated in both images. Since the 3D lineOL-X passes through the optical center of the lens OL, the corresponding epipolar line in the right image must pass through the epipole eR (and correspondingly for epipolar lines in the left image). All epipolar lines in one image contain the <b>epipolar</b> <b>point</b> of that image. In fact, any line which contains the <b>epipolar</b> <b>point</b> is an <b>epipolar</b> line since it can be derived from some 3D point X.|$|R
40|$|The viewing graph {{represents}} {{a set of}} views that are re-lated by pairwise relative geometries. In the context of Structure-from-Motion (SfM), the viewing graph is the in-put to the incremental or global estimation pipeline. Much effort has been put towards developing robust algorithms to overcome potentially inaccurate relative geometries in the viewing graph during SfM. In this paper, we take a fun-damentally different approach to SfM and instead focus on {{improving the quality of}} the viewing graph before apply-ing SfM. Our main contribution is a novel optimization that improves the quality of the relative geometries in the view-ing graph by enforcing loop consistency constraints with the <b>epipolar</b> <b>point</b> transfer. We show that this optimization greatly improves the accuracy of relative poses in the view-ing graph and removes the need for filtering steps or robust algorithms typically used in global SfM methods. In addi-tion, the optimized viewing graph can be used to efficiently calibrate cameras at scale. We combine our viewing graph optimization and focal length calibration into a global SfM pipeline that is more efficient than existing approaches. To our knowledge, ours is the first global SfM pipeline capable of handling uncalibrated image sets. 1...|$|R
40|$|We {{propose a}} method for guiding a {{photographer}} to rotate her/his smartphone camera to obtain an image that overlaps with another image of the same scene. The other image is taken by another photographer from a different viewpoint. Our method is applicable even when the images do not have overlapping fields of view. Straightforward applications of our method include sharing attention to regions of interest for social purposes, or adding missing images to improve structure for motion results. Our solution uses additional images of the scene, which are often available since many people use their smartphone cameras regularly. These images may be available online from other photographers who are present at the scene. Our method avoids 3 D scene reconstruction; it relies instead on a new representation that consists of the spatial orders of the scene points on two axes, x and y. This representation allows a sequence of points to be chosen efficiently and projected onto the photographers images, using <b>epipolar</b> <b>point</b> transfer. Overlaying these epipolar lines on the live preview of the camera produces a convenient interface to guide the user. The method was tested on challenging datasets of images and succeeded in guiding a photographer from one view to a non-overlapping destination view...|$|R
40|$|We {{present a}} new {{approach}} to camera calibration {{as a part of a}} complete and practical system to recover digital copies of sculpture from uncalibrated image sequences taken under turntable motion. In this paper we introduce the concept of the silhouette coherence of a set of silhouettes generated by a 3 D object. We show how the maximization of the silhouette coherence can be exploited to recover the camera poses and focal length. Silhouette coherence can be considered as a generalization of the well known epipolar tangency constraint for calculating motion from silhouettes or outlines alone. Further, silhouette coherence exploits all the information in the silhouette (not just at <b>epipolar</b> tangency <b>points)</b> and can be used in many practical situations where point correspondences or outer epipolar tangents are unavailable. We present an algorithm for exploiting silhouette coherence to efficiently and reliably estimate camera motion. We use this algorithm to reconstruct very high quality 3 D models from uncalibrated circular motion sequences, even when <b>epipolar</b> tangency <b>points</b> are not available or the silhouettes are truncated. The algorithm has been integrated into a practical system and has been tested on over 50 uncalibrated sequences to produce high quality photo-realistic models. Three illustrative examples are included in this paper. The algorithm is also evaluated quantitatively by comparing it to a state-of-the-art system that exploits only epipolar tangents...|$|R
40|$|We {{present a}} new video {{stabilization}} technique that uses projective scene re-construction to treat jittered video sequences. Unlike methods that recover the full three-dimensional {{geometry of the}} scene, this model accounts for simple geometric relations between <b>points</b> and <b>epipolar</b> lines. Using this level of scene understanding, we obtain the physical correctness of 3 D sta-bilization methods yet avoid their lack of robustness and computational costs. Our method consists of tracking feature points in the scene and using them to compute fundamental matrices that model stabilized camera mo-tion. We then project the tracked points onto the novel stabilized frames using <b>epipolar</b> <b>point</b> transfer and synthesize new frames using image-based frame warping. Since this model is only valid for static scenes, we develop a time-view reprojection that accounts for non-stationary points in a princi-pled way. This reprojection is based on modeling the dynamics of smooth inertial object motion in three-dimensional space and allows us to avoid the need to interpolate stabilization for moving objects from their static sur-rounding. Thus, we achieve an adequate stabilization when both the camera and the objects are moving. We demonstrate the abilities of our approach to stabilize hand-held video shots in various scenarios: scenes with no par-allax that challenge 3 D approaches, scenes containing non-trivial parallax effects, videos with camera zooming and in-camera stabilization, as well as movies with large moving objects...|$|R
40|$|The topological {{obstacles}} to the matching of smooth curves in stereo images are shown to occur at <b>epipolar</b> tangencies. Such <b>points</b> are good matching primitives, {{even when the}} image curves correspond to smooth surface profiles. An iterative scheme for improving camera calibration based on these results is derived and performance demonstrated on real data...|$|R
30|$|The {{second step}} is to {{generate}} piecewise <b>epipolar</b> curve image <b>points</b> from original stereo images. As with the previously reported methods, this step {{is based on the}} fact that the hyperbola-like epipolar curve for a reference image point exists between two control points that correspond to the minimum and maximum ground height of the reference image point.|$|R
40|$|Abstract—We {{present a}} new {{approach}} to camera calibration {{as a part of a}} complete and practical system to recover digital copies of sculpture from uncalibrated image sequences taken under turntable motion. In this paper, we introduce the concept of the silhouette coherence of a set of silhouettes generated by a 3 D object. We show how the maximization of the silhouette coherence can be exploited to recover the camera poses and focal length. Silhouette coherence can be considered as a generalization of the well-known epipolar tangency constraint for calculating motion from silhouettes or outlines alone. Further, silhouette coherence exploits all the geometric information encoded in the silhouette (not just at <b>epipolar</b> tangency <b>points)</b> and can be used in many practical situations where point correspondences or outer epipolar tangents are unavailable. We present an algorithm for exploiting silhouette coherence to efficiently and reliably estimate camera motion. We use this algorithm to reconstruct very high quality 3 D models from uncalibrated circular motion sequences, even when <b>epipolar</b> tangency <b>points</b> are not available or the silhouettes are truncated. The algorithm has been integrated into a practical system and has been tested on more than 50 uncalibrated sequences to produce high quality photo-realistic models. Three illustrative examples are included in this paper. The algorithm is also evaluated quantitatively by comparing it to a state-of-the-art system that exploits only epipolar tangents. Index Terms—Silhouette coherence, epipolar tangency, image-based visual hull, focal length estimation, circular motion, 3 D modeling. ...|$|R
30|$|The geo-positioning {{accuracy}} of the epipolar resampling algorithm can be measured by comparing the original and generated sensor models’ forward coordinate transformation results for <b>epipolar</b> image <b>point</b> pairs. Ground coordinates, G, extracted from the cubic grid in the effective ground space are used to compute image coordinates {{from each of the}} original reference image points, and the image coordinates are relocated into epipolar image space using the image transformation polynomial function. New ground coordinates, G′, can be calculated from these epipolar image coordinates using the newly generated RPCs. The differences between the coordinates, G-G′, represent the geo-positioning {{accuracy of}} the resampled epipolar image pair.|$|R
40|$|Imaging {{of falling}} objects is described. Multiple {{images of a}} falling object can be {{captured}} substantially simultaneously using multiple cameras located at multiple angles around the falling object. An epipolar geometry of the captured images can be determined. The images can be rectified to parallelize epipolar lines of the <b>epipolar</b> geometry. Correspondence <b>points</b> between the images can be identified. At least {{a portion of the}} falling object can be digitally reconstructed using the identified correspondence points to create a digital reconstruction...|$|R
40|$|The {{estimation}} of the 2 D relative motion of an indoor robot using monocular vision is presented. The camera calibration is known, and its motion is limited to be a planar one. These constraints {{are included in the}} robust regression of <b>epipolar</b> geometry from <b>point</b> matches. Motion is derived from the epipolar geometry. A sequence of 54 real images is used to test the algorithm. Accurate motion both in rotation and translation angles, of 0. 4 and 1. 7 deg, is successfully derived...|$|R
40|$|The {{problem of}} {{determining}} the camera motion from apparent contours or silhouettes of curved three-dimensional surfaces is considered. In a sequence of images is shown {{how to use the}} generalized epipolar constraint on apparent contours. One such constraint is obtained for each <b>epipolar</b> tangency <b>point</b> in each image pair. Thus in theory the motion can be calculated from the deformation of a single contour. A robust algorithm for computing the motion is presented based on the maximum likelihood estimate. It is shown how to generate initial estimates on the camera motion using only the tracked contours. It is also shown how to improve this estimate by maximizing the likelihood function. The algorithm has been tested on real image sequences. The result is compared to that of using only point features. The statistical evaluation shows that the technique gives accurate and stable result...|$|R
40|$|In {{this paper}} {{we will discuss}} {{structure}} and motion problems for curved surfaces. These will be studied using the silhouettes or apparent contours in the images. The problem of determining camera motion from the apparent contours of curved three-dimensional surfaces, is studied. It will be shown how special <b>points,</b> called <b>epipolar</b> tangency <b>points</b> or frontier points, {{can be used to}} solve this problem. A generalised epipolar constraint is introduced, which applies to points, curves, as well as to apparent contours of surfaces. The theory is developed for both continuous and discrete motion, known and unknown orientation, calibrated and uncalibrated, perspective, weak perspective and orthographic cameras. Results of an iterative scheme to recover the epipolar line structure from real image sequences using only the outlines of curved surfaces, is presented. A statistical evaluation is performed to estimate the stability of the solution. It is also shown how the motion of the camera from a sequence of images can be obtained from the relative motion between image pairs...|$|R
40|$|This paper {{presents}} a self-calibrating stereo vision sys-tem for human-computer interaction. Two low-cost off-the-shelf USB cameras {{are used to}} acquire stereo video, {{and based on the}} corresponding <b>epipolar</b> geometry a <b>point</b> of the three-space is tracked. The system allows us to use any point-like light source as 3 D cursor. The inter-nal parametrization of the cameras is obtained by self-calibration procedure, which requires only minimal actions from the user. As the internal parametrization is known, it is possible to recover the relative pose of the cameras and tracked point in either projective or euclidean space. Track-ing is smoothed in image domain using standard Kalman filter to reduce noise. 1...|$|R
40|$|Computing the epipolar {{geometry}} between cameras {{with very}} different viewpoints is often very difficult. The appearance of objects can vary greatly, {{and it is difficult}} to find corresponding feature points. Prior methods searched for corresponding <b>epipolar</b> lines using <b>points</b> on the convex hull of the silhouette of a single moving object. These methods fail when the scene includes multiple moving objects. This paper extends previous work to scenes having multiple moving objects by using the "Motion Barcodes", a temporal signature of lines. Corresponding epipolar lines have similar motion barcodes, and candidate pairs of corresponding epipoar lines are found by the similarity of their motion barcodes. As in previous methods we assume that cameras are relatively stationary and that moving objects have already been extracted using background subtraction...|$|R
40|$|The {{essential}} matrix, which encodes the <b>epipolar</b> constraint between <b>points</b> in two projective views, is {{a cornerstone}} of modern computer vision. Previous works have proposed different characterizations of the space of essential matrices as a Riemannian manifold. However, they either do not consider the symmetric {{role played by the}} two views, or do not fully take into account the geometric peculiarities of the epipolar constraint. We address these limitations with a characterization as a quotient manifold which can be easily interpreted in terms of camera poses. While our main focus in on theoretical aspects, we include applications to optimization problems in computer vision. This work was supported by grants NSF-IIP- 0742304, NSF-OIA- 1028009, ARL MAST-CTA W 911 NF- 08 - 2 - 0004, and ARL RCTA W 911 NF- 10 - 2 - 0016, NSF-DGE- 0966142, and NSF-IIS- 1317788...|$|R
5000|$|If the {{projection}} point xL is known, then the epipolar line eR-xR is known {{and the point}} X projects into the right image, on a point xR which must lie on this particular epipolar line. This means that for each point observed in one image the same point must be observed in the other image on a known epipolar line. This provides an epipolar constraint: {{the projection}} of X on the right camera plane xR must be contained in the eR-xR epipolar line. Note also that all points X e.g X1, X2, X3 on the OL-XL line will verify that constraint. It means {{that it is possible}} to test if two points correspond to the same 3D <b>point.</b> <b>Epipolar</b> constraints can also be described by the essential matrix or the fundamental matrix between the two cameras.|$|R
40|$|This paper {{addresses}} {{the problem of}} estimating the <b>epipolar</b> geometry from <b>point</b> correspondences between two images taken by uncalibrated perspective cameras. It is shown that Jepson's and Heeger's linear subspace technique for infinitesimal motion estimation can be generalized to the finite motion case by choosing an appropriate basis for projective space. This yields a linear method for weak calibration. The proposed algorithm has been implemented and tested on both real and synthetic images, and it is compared to other linear and non-linear approaches to weak calibration. 1 Introduction The geometric information contained in two images taken by uncalibrated perspective cameras is completely captured by the epipolar geometry of the two images, i. e., by {{the knowledge of the}} epipoles and of the homography, called epipolar transformation, relating the pencils of epipolar lines in the two images, or equivalently, by the knowledge of the fundamental matrix [8, 18]. This paper addresses [...] ...|$|R
40|$|Abstract—This paper {{introduces}} a novel antipodal-epipolar constraint on relative camera motion. By using antipodal points, {{which are available}} in large Field-of-View cameras, the translational and rotational motions of a camera are geometrically decoupled, allowing them to be separately estimated as two problems in smaller dimensions. We present a new formulation based on discrete camera motions, which works over a larger range of motions compared to previous differential techniques using antipodal points. The use of our constraints is demonstrated with two robust and practical algorithms, one based on RANSAC and the other based on Hough-like voting. As an application of the motion decoupling property, we also present a new structure-from-motion algorithm that does not require explicitly estimating rotation (it uses only the translation found with our methods). Finally, experiments involving simulations and real image sequences will demonstrate that our algorithms perform accurately and robustly, with some advantages over the state-of-the-art. Index Terms—Multiview geometry, antipodal <b>points,</b> <b>epipolar</b> constraint, structure and motion, Hough, robust estimation. ...|$|R
40|$|We {{describe}} {{in this paper}} closed-form solutions to the following problems in multi-view geometry of n'th order curves: #i# recovery of the fundamental matrix from 4 or moreconic matches in two views, #ii# recovery of the homography matrix from a single n'th order #n # 3 # matching curve and, in turn, recovery of the fundamental matrix from two matching n'th order planar curves, and #iii# 3 D reconstruction of a planar algebraic curve from two views. Although some of these problems, notably #i# and #iii#, were introduced in the past # 15, 3 #, our derivations are analytic with resulting closed form solutions. We have also conducted synthetic experiments on #i# and real image experiments on #ii# and #iii# with subpixel performance levels, thus demonstrating the practical use of our results. 1 Introduction A large body of research {{has been devoted to}} the problem of computing the <b>epipolar</b> geometry from <b>point</b> correspondences. The theory of fundamental matrix and its robust numerical computati [...] ...|$|R
40|$|Digital {{close range}} {{photogrammetry}} is extensively used {{because of its}} efficiency and significant decreasing time needed to collect data and process them and also applicability in different environmental conditions. to use this technology in an optimum way it needs image processing and computations {{in the form of}} a software that can meet our requirements of applying this technique. in this paper different stages of designing a software which use Matlab and Visual Basic environments for preparing 3 d model of an object in the form of point cloud is explained. the software uses sequence of images with the same resolution taken by different or the same non-metric cameras besides coordinates of control points as input. Then an operator must zoom and click on control points in images to prepare coordinate of control points in image coordinate system and select one image (generally middle image) as base or template image,this is the only stage that needs operator. to compute interior and exterior parameters of cameras, bundle adjustment is performed. then using image matching technique and <b>epipolar</b> geometry <b>points</b> of edges of base image is searched in other images. applying coordinates of corresponding points and collinearity equation,object space coordinate of point is computed in a least square adjustment and its position accuracy is evaluated. then all accepted points (irregular point cloud of object) is filtered to prepare a regular point cloud. all these stages are done automatically without operator interfere. in this paper above mentioned parts of software are explained and finally some practical applications of the software presented. 1...|$|R
40|$|Linear array {{scanners}} {{are used}} as a substitute of two-dimensional/frame digital cameras since they can produce high-resolution digital images comparable to scanned aerial photographs. On the other hand, digital frame cameras have inadequate size that is dictated by technical considerations/limitations. In general, rigorous sensor modelling involves {{the description of the}} physical process of data capture using such a sensor. For imaging systems, rigorous modelling incorporates the system’s interior and exterior orientation parameters. Such parameters might not be always available for linear array scanners (e. g., commercially available IKONOS scenes). Deriving these parameters requires numerous ground control points. Moreover, the estimation process is geometrically ill posed due to the narrow angular field of view of the imaging system. Recently, parallel projection has emerged as an approximate model (for high altitude scanners with narrow angular field of view) {{that can be used to}} represent the mathematical relationship between scene and object space coordinates using few parameters. This paper outlines the derivation of resampling approach of linear array scanner scenes according to <b>epipolar</b> geometry. Conjugate <b>points</b> should have no y-parallax in the resampled scenes. Moreover, they should have an x-parallax that is linearly proportional to the corresponding object height. Such requirements can only be met by projecting the original scenes into a common horizontal plane. The paper explains the selection of such plane to meet these specifications. Experimental results using IKONOS data demonstrate the feasibility of the approach. 1...|$|R
40|$|We {{explore the}} {{geometric}} and algebraic relations that exist between correspondences of points and lines in an arbitrary number of images. We propose {{to use the}} formalism of the Grassmann-Cayley algebra as {{the simplest way to}} make both geometric and algebraic statements in a very synthetic and effective way (i. e. allowing actual computation if needed). We have a fairly complete picture of the situation in the case of points: there are only three types of algebraic relations which are satisfied by the coordinates of the images of a 3 -D point: bilinear relations arising when we consider pairs of images among the $N$ and which are the well-known epipolar constraints, trilinear relations arising when we consider triples of images among the $N$, and quadrilinear relations arising when we consider four-tuples of images among the $N$. Moreover, we show that for a given triple of images, once the epipolar constraints are known, there is only one algebraically independent trilinear relation which can be used to predict the image coordinates of a point in a third image, given the coordinates of the images in the other two images, even in cases where the prediction by the <b>epipolar</b> constraints fails (<b>points</b> in the trifocal plane, or optical centers aligned). We also show that the trilinear relations imply the bilinear ones, i. e. the epipolar constraints. Finally, we show that the quadrilinear relations are algebraically dependent of the trilinear and bilinear ones, i. e. do not bring in any new information. In the case of lines, we show how the traditional perspective projection equation can be suitably generalized and {{that in the case of}} three images there exist two independent trilinear relations between the coordinates of the images of a 3 -D line...|$|R

