17|12|Public
40|$|Conducted {{research}} {{on social media}} analysis and engagement, user modeling from social media, social collaboration tools, web task models and <b>intelligent</b> <b>browsing,</b> web automaton and testing. - Published papers at top-rated conferences, received multiple best paper nominations. - Filed 20 + patents and received numerous patent awards. - Shipped research result on website testing into IBM’s Rational Functional Tester product. - Delivered research result to various IBM and government funded project. - Delivered research result in IBM’s client engagement...|$|E
40|$|Topics in {{education}} are changing with an ever faster pace. Especially {{in the field}} of life-long learning the aspects that need to be taught by information providers must keep up to date with the process in its field. The courseware watchdog is a comprehensive module which allows users to focus on existing subfields of a discipline, but thereby be aware of important drifts and tendencies in the field. 1 Module: Courseware Watchdog The courseware watchdog consists of four major components: 1. A focused crawler that gathers data from relevant educational media sources 2. A subjective clustering algorithm that allows to group educational media with similar contents together following different types of ontology similarity criteria 3. An <b>intelligent</b> <b>browsing</b> capability, which allows to understand similarities and dif-ferences of educational media sources 4. A mechanism for updating the ontology in order to reflect a drift of courseware topics These four components are elaborated in the following submodules. The significant difference to modules like the automatic extraction of metadata and the personal search engine is the attempt at using unsupervised learning methodology and <b>intelligent</b> <b>browsing</b> facilities in order to detect trends and tendencies rather than in classi-fying or retrieving according to existing structures. Thus, the courseware watchdog nicely complements these other work packages...|$|E
40|$|Abstract. Browsing and {{navigating}} into {{a document}} base can be significantly improved by an {{easy access to}} textual sources. Many efficient indexing and search techniques have been proposed in the literature. Word vectors are commonly used to approximate the notion of document content and to support matching algorithms during the retrieval process. Efficiency criteria push for linear non-recursive representations. The text of a document is never processed for its linguistic information content. The gap between the implicit content of (a set of) texts and the rich structured formats (i. e. networks) able support <b>intelligent</b> <b>browsing</b> is well known. In this paper the overall architecture of a language oriented methodology of document processing for a content driven retrieval is described. Lexical acquisitio...|$|E
40|$|The urban {{integrated}} {{pipeline network}} transfers information and energies {{day and night}} for the city, therefore, the information system of urban integrated pipeline network is crucial for urban planning and management. This system provides an effective platform to store, manage and analyze the integrated pipeline network data. This paper introduces the method of constructing the system based on the United States ESRI's MapObjects 2. 3, Visual Basic 6. 0 and SQL database and describes how to use the GIS technology for effective management of urban integrated pipelines. Further more, this system has a friendly interface to all the users with less investments. Overall, we divide the functions of this system into four parts: pipeline network management, pipeline network analysis, pipeline network maintenance and safety management. The system is applicable to the medium and small cities with some intelligentized day-to-day management functions, such as the conversion of the original data, <b>intelligent</b> <b>browse</b> of the maps, inquiring and statistic, profiles, measurement functions and so on. One of the browse functions is the "Eagle Eye " window, which offers a breviary map to the users. This system might be an useful and economical tool of high technology to manage medium and small zones. It will have abundant reference value for the similar system. 1...|$|R
40|$|This paper {{presents}} a new visualization approach to provide <b>intelligent</b> Web <b>browsing</b> support for electronic commerce (e-commerce) using data warehousing and data mining techniques. To overcome {{the limitations of}} current Web browsers which lack flexibility for customers to visualize products from different perspectives, a new visual data model which extends the conventional data warehouse schema is introduced to deal with intensive data volumes and complex transformations {{with a high degree}} of flexibility in terms of multi-perspective visualizations and morphing capacity in an interactive environment. The integration of dynamic object visualization, an interactive user interface and a flexible evaluation scheme provides an effective approach to close the gap between the "real world" and the "cyber world" from the business point of view. Department of ComputingRefereed conference pape...|$|R
40|$|The World Wide Web creates {{opportunities}} for search systems using adaptive distributed agents. This paper presents a threaded implementation of InfoSpiders, a client-based system that uses an evolving population of <b>intelligent</b> agents to <b>browse</b> the Web at query time. We consider different fitness functions based on network resource consumption {{and show that}} taxing agents in proportion to latency results in better efficiency without penalties {{in the quality of}} the retrieved documents. The too...|$|R
40|$|Abstract—Automatic {{discovery}} of semantic relations between resources {{is a key}} issue in Web-based intelligent applications such as document understanding and Web services. This paper explores how to automatically discover the latent semantic relations and their properties based on the existing association rules. Through building semantic matrix by the association rules, four semantic relations can be extracted using union and intersection in set theory. By building a cyclic graph model, the transitive path of association relation is discovered. Document-level keywords and domain-level keywords {{as well as their}} parameters are analyzed to improve the discovery accuracy. Rules can be gained from the experiments to optimize the discovery processes for relations and properties. Further experiments validate the effectiveness and efficiency of the relation discovery algorithms, which can be applied in Web search, <b>intelligent</b> <b>browsing</b> and Web service composition...|$|E
40|$|World Wide Web is a {{hypertext}} based, distributed {{information system}} that provides access to vast amounts of information in the internet. A fundamental problem with the Web is the difficulty of retrieving specific information of our interest, from the enormous number of resources that are available. In this paper, we develop a simple logic called WebLog {{that is capable of}} retrieving information from HTML (Hypertext Markup Language) documents in the Web. WebLog is inspired by SchemaLog, a logic for multidatabase interoperability. We demonstrate the suitability of WebLog for (a) querying and restructuring Web information, (b) exploiting partial knowledge users might have on the information being queried, and (c) dealing with the dynamic nature of information in the Web. We illustrate the simplicity and power of WebLog using a variety of applications involving real-life information in the Web. Keywords: World Wide Web, Structured documents, HTML, Logic, <b>Intelligent</b> <b>browsing,</b> Mixed-media [...] ...|$|E
40|$|Associated {{knowledge}} flow (AKF) is a sequential {{link between}} associated topics, {{which can be}} applied to <b>intelligent</b> <b>browsing</b> and personalized recommendation. One key problem is how to measure the knowledge delivery quantity (KDQ) on an AKF. In this paper, a computational method of knowledge delivery quantity on an AKF is proposed. Firstly, considering the keywords and associated relations between two nodes, four key factors for knowledge delivery quantity between two nodes are investigated. Secondly, based on the four factors, an algorithm is proposed to calculate the knowledge delivery quantity between two nodes. Thirdly, the knowledge delivery quantity of a node with adjacent nodes is calculated for the measurement of local knowledge delivery on an AKF. Lastly, according to the local knowledge delivery, the average knowledge delivery quantity is proposed to measure an AKF. Experimental results show that the proposed measurement method is accurate and effective...|$|E
40|$|In this paper, we {{describe}} a fully automatic video retrieval prototype system that uses an image or a video sequence of an interested identity as probe. The system {{is based on}} face vision techniques including face detection and tracking, face alignment and recognition. Given a film or TV sitcom, first face trajectories are extracted in video by head tracking that decompose the video into segments corresponding to certain identity, then frames containing faces of higher quality are selected and normalized according to face alignment results, and finally different segments are associated by face recognition. Experiments are carried out on news video, feature length film video and TV sitcom to show its effectiveness. Potential usage of our system includes <b>intelligent</b> DVD/VCD <b>browsing,</b> video database retrieval, meeting record browsing, etc...|$|R
40|$|Abstract. The {{rapid growth}} of the World Wide Web has {{complicated}} the process of web browsing by providing an overwhelming wealth of choices for the end user. To alleviate this burden, intelligent tools can do much of the drudge-work. This paper describes the SWAMI system. It combines multiple aspects of adaptive web technologies into a framework for an <b>intelligent</b> web <b>browsing</b> system. It uses a multi-agent system to represent {{the interests of the}} user dynamically and takes advantage of the active nature of agents to provide a platform for parallel look-ahead evaluation, page searching, and cooperative link recommendation swapping. The collection of agents reflects the user’s interests by self-organizing into a hierarchicy according to the evidence of apparent interest demonstrated by the user. Example results of the functioning prototype are presented, demonstrating its ability to infer and react to a user’s interests. ...|$|R
40|$|This paper {{shows how}} {{semantic}} web techniques {{can be applied}} to solving problems of distributed content creation, discovery, linking, aggregation, and reuse in health information portals, both from end-user's and content producer’s viewpoints. As a case study, the national semantic health portal HealthFinland is presented. It provides citizens with <b>intelligent</b> searching and <b>browsing</b> services to reliable and up-to-date health information created by various expert organizations and authorities in the field of health promotion in Finland. The system is based on a shared semantic metadata schema, ontologies, and mash-up ontology services. The content includes the metadata of thousands of web documents such as web pages, articles, reports, campaign information, news, services, and other information related to health...|$|R
40|$|Despite {{the great}} amount of work {{done in the last}} decade, {{retrieving}} information of interest from a large multimedia repository still remains an open issue. In this paper, we propose an <b>intelligent</b> <b>browsing</b> system based on a novel recommendation paradigm. Our approach combines usage patters with low-level features and semantic descriptors in order to predict users’ behavior and provide effective recommendations. The proposed paradigm is very general and can be applied to any type of multimedia data. In order to make the recommender system even more flexible, we introduce the concept of multichannel browser, i. e. a browser that allows concurrent browsing of multiple media channels. We implemented a prototype of the proposed system and tested the effectiveness of our approach in a virtual museum scenario. Experimental results have proved that the system greatly enhances users’ experience, thus encouraging further research in this direction...|$|E
40|$|To {{generate}} intelligent indexing {{that allows}} context-sensitive information retrieval, a {{system must be}} able to acquire knowledge directly through interaction with users. In this paper, we present the architecture for CID (Computer Integrated Documentation), a system that enables integration of various technical documents in a hypertext framework and includes an <b>intelligent</b> <b>browsing</b> system that incorporates indexing in context. CID's knowledge-based indexing mechanism allows case-based knowledge acquisition by experimentation. It utilizes on-line user information requirements and suggestions either to reinforce current indexing in case of success or to generate new knowledge in case of failure. This allows CID's intelligent interface system to provide helpful responses, even when no a priori user model is available. Our system in fact learns how to exploit a user model based on experience (from user feedback). We describe CID's current capabilities and provide an overview of our plans for extending the system...|$|E
40|$|Although much {{research}} {{has gone into}} natural language legal document analysis, practical solutions to support legal document drafting and reasoning are still limited in number and functionality. However given the textual basis of law there is much potential for NLP techniques {{to aid in the}} context of drafting legal documents, in particular contracts. Furthermore, there is a body of work focusing on the formal semantics of norms and legal notions which has direct applications in analysis of such documents. In this paper we present our attempt to use several off-the-shelf NLP techniques to provide a more intelligent contract editing tool to lawyers. We exploit these techniques to extract information from contract clauses to allow <b>intelligent</b> <b>browsing</b> of the contract. We use this surface analysis {{to bridge the gap between}} the English text of a contract to its formal representation, which is then amenable to automated deduction, specifically it allows us to identify conflicts in the contract. peer-reviewe...|$|E
40|$|Abstract — The Web, {{the largest}} {{unstructured}} database {{of the world}} has greatly improved access to the documents. As the number of Internet users and the number of accessible web pages grow, it is becoming increasingly difficult for users to find documents that are relevant to their particular needs. Users must either browse through a hierarchy of concepts to find the information they need or submit a query to a Search Engine and wade through hundreds of results most of them irrelevant. Web Crawlers {{are one of the most}} crucial components used by the Search Engines to collect pages from the Web. It is an <b>intelligent</b> means of <b>browsing</b> used by the Search Engine. The requirement of a web crawler that downloads most relevant web pages from such a large web is still a major challenge in the field of Informatio...|$|R
40|$|The {{recent trends}} in World Wide Web {{technologies}} create opportunities for client-based systems using distributed agents. We discuss {{the use of}} dynamic, locally driven search strategies for information discovery on the Web {{as opposed to the}} more static methods employed by search engines, in which search and retrieval are disjoint. This paper presents a threaded implementation of InfoSpiders, a client-based system that uses an adaptive population of <b>intelligent</b> agents to <b>browse</b> the Web at query time, based on local context. The efficiency of the multithreaded approach is confirmed by experiments in which speedups of an order of magnitude are achieved. We also consider di erent models of network resource consumption, and nd that taxing agents in proportion to download time results in better e ciency without penalties in the quality of the retrieved documents. The tool has been made available to the public as a Java applet...|$|R
40|$|Sports videos can be {{characterized}} as a sequence of recurrent semantic story units. Storing sports videos in this story-unit-based form will lead to develop an <b>intelligent</b> content-based retrieval, <b>browsing,</b> and summarization system. The storage requires segmentation of videos and semantic understanding of each segment. Since transcribed broadcasted video speech, the closed-caption text, can be the useful information source for semantic indexing of each story unit, this paper proposes a method to automatically segment the closed-caption text of sports videos into the semantic units. The proposed method firstly tries to segment the speech transcript into the scene units, a set of which composes a story unit, in a probabilistic framework based on Bayesian networks. Finding the boundaries of the set of the scene units enables us to generate the story units in the close-caption. In this paper, we discuss some experimental results and the potentiality for utilizing them for indexing of the video and speech summarization. ...|$|R
40|$|Environmental sound archives- casual {{recordings}} of people’s daily life- are easily collected by MP 3 players or camcorders with low cost and high reliability, and {{shared in the}} web-sites. There {{are two kinds of}} user generated recordings we {{would like to be able}} to handle in this thesis: Continuous long-duration personal audio and Soundtracks of short consumer video clips. These environmental recordings contain a lot of useful information (semantic concepts) related with activity, location, occasion and content. As a consequence, the environment archives present many new opportunities for the automatic extraction of information that can be used in <b>intelligent</b> <b>browsing</b> systems. This thesis proposes systems for detecting these interesting concepts on a collection of these real-world recordings. The first system is to segment and label personal audio archives- continuous {{recordings of}} an individual’s everyday experiences- into ’episodes ’ (relatively consistent acoustic situations lasting a few minutes or more) using the Bayesian Information Criterion and spectral clustering...|$|E
40|$|The {{average person}} with a {{networked}} computer can now understand why computers should have vision [...] to search the world's collections of digital video and images and "retrieve a picture of. " Computer vision for <b>intelligent</b> <b>browsing,</b> querying, and retrieval of imagery is needed now, and yet traditional approaches to computer vision remain far from a general solution to the scene understanding problem. In this paper I discuss {{the need for a}} solution based on combining high-level and low-level vision, that works in concert with input from a human user. The solution is based on: 1) Learning from the user what is important visually, and 2) Learning associations between text descriptions and visual data. I describe some recent results in these areas, and overview key challenges for future research in computer vision for digital libraries. 1 Introduction Collections of digital imagery are growing at a rapid pace. The contexts are broad, including areas such as entertainment (e. g. searching [...] ...|$|E
40|$|<b>Intelligent</b> <b>browsing</b> {{through a}} {{collection}} of reusable software components is facilitated with a computer having a video monitor and a user input interface such as a keyboard or a mouse for transmitting user selections, by presenting a picture of encyclopedia volumes with respective visible labels referring to types of software, in accordance with a metaphor in which each volume includes a page having a list of general topics under the software type of the volume and pages having lists of software components for {{each one of the}} generic topics, altering the picture to open one of the volumes in response to an initial user selection specifying the one volume to display on the monitor a picture of the page thereof having the list of general topics and altering the picture to display the page thereof having a list of software components under one of the general topics in response to a next user selection specifying the one general topic, and then presenting a picture of a set of different informative plates depicting different types of information about one of the software components in response to a further user selection specifying the one component...|$|E
40|$|The {{explosive}} growth of digital image collections on the Web sites {{is calling for}} an efficient and <b>intelligent</b> method of <b>browsing,</b> searching, and retrieving images. In this article, an artificial neural network (ANN) -based approach is proposed to explore a promising solution to the Web image retrieval (IR). Compared with other image retrieval methods, this new approach has the following characteristics. First of all, the Content-Based features have been combined with Text-Based features to im-prove retrieval performance. Instead of solely relying on low-level visual features and high-level concepts, we also take the textual features into consideration, which are automatically extracted from image names, alterna-tive names, page titles, surrounding texts, URLs, etc. Secondly, the Kohonen neural network model is intro-duced and led into the image retrieval process. Due to its self-organizing property, the cognitive knowledge is learned, accumulated, and solidified during the unsuper-vised training process. The architecture is presented to illustrate the main conceptual components and mecha-nism of the proposed image retrieval system. To dem-onstrate {{the superiority of the}} new IR system over other IR systems, the retrieval result of a test example is also given in the article. 1...|$|R
40|$|Sereno, B., Boursinou, E., Maxwell, K., & Angehrn, A. A. (2007). Supporting Social Interaction in Intelligent Competence Development Systems. In D. Griffiths, R. Koper & O. Liber (Eds.), Proceedings of the 2 nd TENCompetence Open Workshop (pp. 29 - 35). January, 11 - 12, 2007, Manchester, United Kingdom. This paper {{addresses}} {{the challenge of}} enhancing lifelong Competence Development and Management Systems with advanced features enhancing social interaction. Such features include network visualizations and <b>browsing,</b> <b>intelligent</b> agents and game dynamics aimed at supporting users seeking advice throughout their competence development process. In particular, we are exploring the design and impact of software agents providing personalized and contextualized stimulus and support based on user objectives and social network information. We describe how we envision embedding such agents in Competence Development Systems to help users “connect” to other users {{as well as to}} relevant competence development opportunities and relevant knowledge assets. Our ultimate objective is to increase system usage towards a sustainable level of knowledge exchange and creation. The work on this publication has been sponsored by the TENCompetence Integrated Project that is funded by the European Commission's 6 th Framework Programme, priority IST/Technology Enhanced Learning. Contract 027087 [[URL]...|$|R
40|$|Automatic documet {{summarization}} {{refers to}} the task of creating document surrogates that are smaller in sizu but reatin various characterstics of the original document, depending on the intended use. Good solution to this problem would have a great impact on today's society which is overloaded with information. While abstracts created by trained professionals involve rewriting of text, automatic summarization of documents has been focuesed on extracting sentences or keywords from text so that the overall summarysatisfies various criteria: optimal reducation of text for use in text indexing, coverage of document themes, and similar. As document summaries, in form of abstracts, have been generated by humans, it seems most natural to try to model human abstracting. However, evaluation of such model is difficult because of the text generation aspects. We apply machine learning algorithms to capture characteristics of human extracted summary sentences. In contrast to related studies, which typically rely on a minimal understanding of the semantic structure of documents, we start with deep syntatic analysis of the text. The we perform named entity consolidation and pronomial anaphora resolution. We extract elementary syntatic structures from indvidual sentences in the form of logical form triples, i. e., subject- predicate- object triples, and use semantic properties of nodes in the triples to build semantic graphs for both documents and corresponding summaries. We expect that extracted summaries would capture essential semantic relations within the document and thus their structures could be found within the document semantic graphs. We reduce the problem of summarization to acquiring machine learning models for mapping between the document graph and the graph of a summary. This means we learn models for extracting sub-structures from document semantic graphs which are chracteristic of human selected summaries. We use logical form triples as basic features and apply Support Vector Machines to learn the summarization model. For machine learning part we use tehniques similar to those from (4, 5). Using those tehniques we won KDD Cup 2003, a data mining competition held at the ACM SIGKDD conference (Knowledge Discovery and Data Mining). Our approach proved to be successful. The context of the document, captured by the semantic graph, helped identifying the key concepts and relations for summarization. Using semantic graph attributes improved the performance of the learned model. Visualizations of semantic graphs can be directly used as maps of documents for <b>intelligent</b> documnet <b>browsing.</b> Our future work will involve explorations of alternative semantic structures on additional data sets, including human generated abstracts. We will also explore how to better evaluate the quality of our summaries...|$|R
40|$|Navigating {{information}} spaces is {{an essential}} part of our everyday lives, and in order to design efficient and user-friendly information systems, it is important to understand how humans navigate and find the information they are looking for. We perform a large-scale study of human wayfinding, in which, given a network of links between the concepts of Wikipedia, people play a game of finding a short path from a given start to a given target concept by following hyperlinks. What distinguishes our setup from other studies of human Web-browsing behavior is that in our case people navigate a graph of connections between concepts, and that the exact goal of the navigation is known ahead of time. We study more than 30, 000 goal-directed human search paths and identify strategies people use when navigating information spaces. We find that human wayfinding, while mostly very efficient, differs from shortest paths in characteristic ways. Most subjects navigate through high-degree hubs in the early phase, while their search is guided by content features thereafter. We also observe a trade-off between simplicity and efficiency: conceptually simple solutions are more common but tend to be less efficient than more complex ones. Finally, we consider the task of predicting the target a user is trying to reach. We design a model and an efficient learning algorithm. Such predictive models of human wayfinding can be applied in <b>intelligent</b> <b>browsing</b> interfaces...|$|E
40|$|International audienceThis paper {{presents}} the Shot Boundary Detection system developed by LaBRI {{in the context}} of “Rough Indexing” paradigm. We work on compressed streams and we use only I and P frames information, (DC coefficients of I-Frames, motion vectors of PFrames and DC coefficients of prediction error) which allow us to be faster than many equivalent systems (10 times faster than real-time on TRECVID 2003 test set, and 3 times faster on 2004, because MPEG files structure is composed of only I and Pframes). In this context the application was not developed to classify shot change transition effects, the initial goal was to allow a real-time and <b>intelligent</b> <b>browsing</b> in video content for common users. The detection is performed in two stages:- Robust Global Camera Motion Estimation- Detection of P-Frame peaks (computation of motion and frame statistics), and of I-Frames (measuring similarity on successive compensated I frames). As we work with two types of frames (I and P), we associate two statistical models which give us two sets of ratio and threshold to calibrate the detector. The first TRECVID participation of LaBRI implies an evolution of the application for transitions effects distinction, which induces two new thresholds to calibrate. We generally obtain equivalent values of Recall and Precision (0. 72 on TRECVID 2003 test set). On TRECVID 2004 test set we obtain as best runs ri- 3 : 0. 723 (Recall) and 0. 606 (Precision); and ri- 4 : 0. 703 (Recall) and 0. 635 (Precision) ...|$|E
40|$|This paper {{presents}} {{the use of}} NLP techniques (text mining, text analysis) for development of specific tools {{that allow us to}} create linguistic resources related to the domain of "cultural heritage". In particular the project "The online dissemination of the historical artistic and scenic, regional heritage" was born in the framework of the collaboration between the Pisa ILC-CNR and the APT Basilicata (i. e. Agenzia di Promozione Territoriale della regione Basilicata) for experimenting and implementing strategies for the promotion and dissemination of regional heritage, particularly in initiatives related to tourism field. The ILC contribution consists in the definition of a model of linguistic analysis of texts, with automatic extraction of semantic information and terminology to be used for the categorization of materials and their guided browsing. We have experimented a particular methodology, dividing the automatic acquisition of texts and consequently of reference corpus, in two phases. The first phase aims at the extraction of documents from lists of links provided by experts worked with us to the project, {{for the creation of a}} repository of documents, whose selection criterion is geographical. In this case the documents can be considered as "grey literature" produced by the regional entities. On the basis of documents extracted from the web, by automatic spiders developed within TextPower technology, automatic parsers create the reference corpus for the cultural heritage domain. Relevant information and semantic concepts are then extracted from this corpus. All these semantically relevant elements (such as proper names, names of institutions, names of places, and other relevant terms) have been used as basis for further acquisitions of documents from heterogeneous sources, by using specialized crawlers that work on a bulk of text materials available on-line. Thus, it has been possible to use the extracted knowledge as basis for a new search strategy of text materials and increase the information available. In this Poster we present the <b>intelligent</b> <b>browsing</b> system that operates on text materials, recovered and annotated by means of the TextPower technology. Includes: Conference preprint, Powerpoint presentation, Abstract and Biographical notesXAInternationa...|$|E
40|$|The {{proliferation}} of technologies has allowed {{an exponential growth}} of knowledge in the biomedical field. Most of the information available and the diversity of information sources are a strong constraint to the full exploitation of this knowledge. The availability of systems for collecting and aggregating data and for semantic analysis has therefore become a priority. These systems allow a better exploitation of knowledge, {{in order to support}} both the public health, as well as economic growth based on knowledge. The project "Unique Social Network for Innovation in Biomedical Tuscany" (SUBITO) funded by Regione Toscana, has allowed us to specialize an <b>intelligent</b> <b>browsing</b> system: "DBT-Faccette", in order to facilitate the identification of links and potential scientific and technological synergies between researchers, government agencies and companies in Tuscany, which was the aim of the project. The knowledge base Corpus contains abstracts of the scientific arguments extracted from biomedical bibliographic database PubMed for free web-users, including citations for biomedical articles from scientific journals. Starting from this data (that can be considered "gray information"), the system is able to relate semantically relevant information and suggest synergies between private and public institutions or subjects. The goal is to build a network of "knowledge" that can be exploited by an intelligent navigation system. This approach allows the automatic reorganization of content, based on the semantically relevant concepts extracted from the text, and the user is able to dynamically discover the concepts relevant for the domain. In this way it is possible to carry out search refinements through the interrelated concepts. Compared to traditional search engines, in addition to the intelligent navigation system semantically driven, "DBT-Faccette" also offers the ability to present the results obtained using various ways of ranking and clustering, allowing the explicitation of the most relevant information. We have developed a "generalized" text enrichment technique, to point out all forms of knowledge identified in the text, through statistical and linguistic analysis technologies, free from predefined assumptions, structures and ontologies. All information extracted, are associated in a para-textual formalism: "text enrichment" of all known lexical, semantic, factual, named entities, terminologies. In this Poster we present "DBT-Faccette" and the results of some queries that are possible on SUBITO project contents. Includes: Conference preprint, Powerpoint presentation, Abstract and Biographical notesXAInternationa...|$|E

