35|1|Public
5000|$|November 2013 - XLCubed v7.6 - custom <b>drilldown</b> (Flex reporting), {{improved}} SharePoint integration ...|$|E
5000|$|Semantic <b>Drilldown</b> - {{provides}} a faceted browser interface for viewing the semantic {{data in a}} wiki ...|$|E
50|$|A list of OLAP {{features}} that are not supported by all vendors. All vendors support features such as parent-child, multilevel hierarchy, <b>drilldown.</b>|$|E
40|$|This article {{describes}} the Open Source portal software OpenBib, a project that was started by the author in 1997 and is being used in different projects at the University Library of Cologne (USB). After a basic overview of the origins, technology and the application at the USB the {{article describes}} some additional features such as RSS feeds, mashups, tagging, tag clouds, use analyses, <b>drilldowns</b> as well as catalogue enrichment. Many of these are characteristic for OPAC 2. 0...|$|R
50|$|Data {{drilling}} (also <b>drilldown)</b> {{refers to}} any of various operations and transformations on tabular, relational, and multidimensional data. The term has widespread use in various contexts, but is primarily associated with specialized software designed specifically for data analysis.|$|E
50|$|The {{software}} used includes Semantic MediaWiki, {{which enables}} {{the connection of}} people to events, places, and other people, as well as Semantic Forms, for ease of data entry, and Semantic <b>Drilldown</b> to let users construct their own data queries.|$|E
50|$|Stick to Your Guns is {{the first}} single from Sick Puppies' fifth album Fury. It was {{released}} on March 31, 2016 through <b>DrillDown</b> Entertainment Group LLC. It {{is the first}} single to feature Bryan Scott on vocals and guitars.|$|E
5000|$|Fury is {{the fifth}} studio album by the Australian rock band Sick Puppies. It was {{released}} on May 20, 2016 through <b>DrillDown</b> Entertainment Group LLC. It is the first album not to feature Shimon Moore and first to feature new vocalist Bryan Scott. The lead single [...] "Stick to Your Guns" [...] was released on March 31, 2016. The album cover was revealed on April 15, 2016. Pre-orders for the album began on April 22, 2016.|$|E
30|$|Wiki <b>Drilldown</b> The wiki <b>drilldown</b> enables {{users to}} “formulate” {{individual}} queries by menu options that provide selections from main categories (for example, data, theoretical frameworks, or assessment standards) through {{a hierarchy of}} properties, subproperties, and values. For example, a user can use the wiki <b>drilldown</b> feature to quickly locate all vulnerability assessments that employ an indicator-based approach and filter the information to identify which of these use expert judgment versus statistical methods for weighting indicators or how many use the Hazards of Place model as their theoretical framework.|$|E
40|$|The inner-city {{neighborhoods of}} America {{continue}} to struggle with the economic blight they have faced ever since American urban growth began to abandon the urban city core fifty years ago. One of the most salient characteristics of the American inner-city is how it is constantly overlooked by private investment. This has many negative effects on the economic livelihood of these neighborhoods, including leaving these {{areas of the city}} void of much of the retail its residents need for their own purchases and for local economic activity. Recent theories have focused on the idea {{that one of the reasons}} there is a lack of investment is because of an information gap that exists in the inner-city, through which inner-city economic and demographic conditions are not accurately represented in the market data used for retail development market analysis. This thesis researches how improved retail market analysis data can help spur more inner-city retail development, with a specific focus on how Social Compact!s 2009 Neighborhood Market <b>DrillDown</b> report for the City of Miami can support increased inner-city retail development in the city. The research looked at the history of inner-cities, the retail development process, and the use of <b>DrillDown</b> reports in Cleveland, Ohio and Houston, Texas, and then studied Miami!s economic development context and its developing strategy for the dissemination of the <b>DrillDown</b> report. It is concluded that the Neighborhood Market <b>DrillDown</b> reports have the potential to be an important enabler of increased inner-city retail development. (cont.) However, this success is completely contingent on the data!s passage through the Retail Market Information Flow framework that this thesis stipulates that actionable market data flows through in a city!s development process. The essence of the flow framework is that it is a series of networking and collaboration steps that determine how effectively a city!s public, private, and non-profit actors work together to support the use and acceptance of improved data and apply it effectively to retail development deals. by Dickson Benjamin Power. Thesis (M. C. P.) [...] Massachusetts Institute of Technology, Dept. of Urban Studies and Planning; and, (S. M. in Real Estate Development) [...] Massachusetts Institute of Technology, Dept. of Urban Studies and Planning, Center for Real Estate, 2009. This electronic version was submitted by the student author. The certified thesis is available in the Institute Archives and Special Collections. Cataloged from student submitted PDF version of thesis. Includes bibliographical references (p. 122 - 126) ...|$|E
30|$|Aggregating measure values {{along the}} {{hierarchies}} of diverse dimensions (i.e., rollup) generates a multidimensional sight on data, which {{is recognized as}} data cube or cube. Deaggregating the measures of a cube to a lower dimension level (i.e., <b>drilldown)</b> provides a more detailed cube. Selecting the subset of a cubes cells that respect a given selection condition (i.e., slicing) also develops a more detailed cube. Such modeled layer must be maintained using our two-state update layer.|$|E
40|$|We present TweetMotif, an {{exploratory}} search application for Twitter. Unlike traditional approaches to information retrieval, which present a simple list of messages, TweetMotif groups messages by frequent significant terms — a result set’s subtopics — which facilitate navigation and <b>drilldown</b> through a faceted search interface. The topic extraction system {{is based on}} syntactic filtering, language modeling, near-duplicate detection, and set cover heuristics. We have used Tweet-Motif to deflate rumors, uncover scams, summarize sentiment, and track political protests in real-time. A demo of TweetMotif, plus its source code, is available a...|$|E
40|$|The Project aims at {{improving}} {{the efficiency of}} hospitals and healthcare centres using Big Data Analytics to evaluate identified KPIs (Key Performance Indicators) of its various functions. The Dashboards designed using computer technology serves as an interactive and dynamic tool for various stakeholders, which helps in optimising performance of various functions and more so maximise the financial returns. The Project entails improving performance of patient servicing, operations and OPD departments, finance function, procurement function, HR function, etc. I developed KPIs and <b>drilldown</b> KPIs for various functions and assisted in designing and developing interactive Dashboards with dynamic charts...|$|E
40|$|This paper {{presents}} StreamXPlore, {{a system}} that enables users to explore historical stream data {{in order to determine}} what events to monitor in the future. At the heart of StreamXPlore is a new event modeling mechanism. StreamXPlore enables the specification, analysis, and mining of these new types of events. Event analysis enables event refinement using data-cube-style slice, dice, <b>drilldown,</b> and roll-up operations. Event data mining consists of event clustering for pattern identification. Preliminary experiments on a real stream of hydro-sensor data show that the proposed exploration mechanisms can indeed be helpful in identifying interesting patterns and are sufficiently fast for exploration of gigabyte-size stream repositories. 1...|$|E
40|$|International audienceOur aim is {{to define}} a {{framework}} supporting analysis in MDW with reductions. Firstly, we describe a modeling solution for reduced MDW. A schema of reduced MDW is composed of states. Each state {{is defined as a}} star schema composed of one fact and its related dimensions valid for a certain period of time. Secondly, we present a multi-state analysis framework. Extensions of classical <b>drilldown</b> and rollup operators are defined to support multi-states analyses. Finally we present a prototype of our framework aiming to prove the feasibility of concept. By implementing our extended operators, the prototype automatically generates appropriate SQL queries over metadata and reduced data...|$|E
40|$|This report offers {{economic}} potentials {{to reduce}} the number of expensive licenses. Double installations of software versions are detected and cut down. 0, 26 0, 26 0, 66 1, 09 1, 10 0, 00 0, 20 0, 40 0, 60 0, 80 1, 00 1, 20 DB Prog Virus OS Mail Figure 9 Software usage report to discover the number of concurrent similar applications installed on a computer For security reasons its from special interest that only 66 % of the groups computers are protected by virus scanners. This is a call for action to <b>drilldown</b> to the individual computers and install such tools to improve the IT securit...|$|E
40|$|A Cloud Computing {{system is}} {{intended}} to improve and automate the controlling single point operations. By using a single point of control, this goal is accomplished through the elimination of duplicate entry and the contribution of data integrity, detailed <b>drilldown,</b> simple training, manageable support, minimal IT maintenance, easy upgrades and reduced costs. Overall, the advantages of cloud computing usage fulfill the original intentions of business as it allows process manufacturers to manage their business as simply and efficiently as possible. Enterprise Resource Planning (ERP) software is designed to improve and auto-mate business processes operations. However, there are many unnecessary administrative, procedural costs and delays often associated with this practice. Examples include duplicate data entry, data corruption, increased training, complicated supplier relations, greater IT support and software incompatibilities. Purpose o...|$|E
40|$|Abstract * This paper {{describes}} the inference explanation capabilities of Cyc, a logical reasoning system {{that includes a}} huge “commonsense” knowledge base and an inference engine that supports both question answering and hypothesis generation. Cyc allows the user to compose queries by means of English templates, and tries to find answers via deductive reasoning. If deduction is fruitless Cyc resorts to abduction, filling in missing pieces of logical arguments with plausible conjectures to obtain provisional answers. Cyc presents its answers and chains of reasoning to the user in English, provides <b>drilldown</b> to external source references whenever possible, and reasons about its own proofs to determine optimal ways of presenting them to the user. When a chain of reasoning relies on conjectures introduced via abduction, the user can interac...|$|E
40|$|We {{propose a}} multi-dimensional {{language}} called nD-SQL {{with the following}} features: (i) nD-SQL supports queries that interoperate amongst multiple relational sources with heterogeneous schemas, including RDBMS and relational data marts, overcoming the mismatch between data and schema; (ii) it supports complex forms of restructuring that permit the visualization of n- dimensional data using the three physical dimensions of the relational model, viz., row, column, and relation; (iii) it captures sophisticated aggregations involving multiple granularities, to an arbitrary degree of resolution compared to CUBE, ROLLUP, and <b>DRILLDOWN.</b> We propose a formal model for a federation of relational sources and illustrate nD-SQL against it. We propose an extension to relational algebra, called restructuring relational algebra (RRA), capable of restructuring and aggregation. We propose an architecture {{for the implementation of}} an nD-SQL server, based on translating nD-SQL queries into equivalent RR [...] ...|$|E
40|$|Currently, network administrators {{must rely}} on labor-intensive {{processes}} for tracking network configurations and vulnerabilities, which requires {{a great deal of}} expertise and is error prone. The organization of networks and the interdependencies of vulnerabilities are so complex as to make traditional vulnerability analysis inadequate. We describe a Topological Vulnerability Analysis (TVA) approach that analyzes vulnerability dependencies and shows all possible attack paths into a network. From models of the network vulnerabilities and potential attacker exploits, we discover attack paths (organized as graphs) that convey the impact of individual and combined vulnerabilities on overall security. We provide sophisticated attack graph visualizations, with high-level overviews and detail <b>drilldown.</b> Decision support capabilities let analysts make optimal tradeoffs between safety and availability, and show how to best apply limited security resources. We employ efficient algorithms that scale well to larger networks. 1...|$|E
40|$|This paper {{presents}} {{a concept of}} an ICT-based Supply Chain Control Dashboard supporting control of the material flow and processes. The concept is developed based on a methodology for mapping, modelling, analysing and redesigning the value chains for extended manufacturing enterprises, the Control Model. The Supply Chain Dashboard supports the monitoring, analysis {{and management of the}} supply chain performance. It supports decision making by visually displaying in true time leading and lagging indicators in a supply chain process perspective. The dashboard offers support for the three application areas: monitoring, analysis and management and contain three types of indicators; performance, diagnostic and control, and allows <b>drilldown</b> and aggregation functionality The Supply Chain Dashboard concept serves as basis for a Supply Chain studio that will allow rapid decision making based on real-time information at an aggregated level along the entire value chain. ...|$|E
40|$|The {{data cube}} {{operator}} exemplifies {{two of the}} most important aspects of OLAP queries: aggregation and dimension hierarchies. In earlier work we presented Dwarf, a highly compressed and clustered structure for creating, storing and indexing data cubes. Dwarf is a complete architecture that supports queries and updates, while also including a tunable granularity parameter that controls the amount of materialization performed. However, it does not directly support dimension hierarchies. Rollup and <b>drilldown</b> queries on dimension hierarchies that naturally arise in OLAP need to be handled externally and are, thus, very costly. In this paper we present extensions to the Dwarf architecture for incorporating rollup data cubes, i. e. cubes with hierarchical dimensions. We show that the extended Hierarchical Dwarf retains all its advantages both in terms of creation time and space while being able to directly and efficiently support aggregate queries on every level of a dimension's hierarchy...|$|E
40|$|We {{present a}} virtual {{environment}} (VE) where point-to-point radio communication data sets can be explored and interpreted. A data set {{can be viewed}} with multiple viewing options: spatial mode for mode to recognize patterns in communication relations and hierarchical mode for viewing the structural organization of the participating communicators. It is possible to switch sequentially between the three views of a data set. With each view the user can select several tools to interpret the data set, like text and audio <b>drilldown</b> tools. To be able to extract persistent information, tools like markers and a report writer are available which store the information in text files. Because of the inherent three-dimensional nature of the three views, we implemented {{the system as a}} VE utilizing our twosided Responsive Workbench, an L-shaped stereo projection table. An intuitive user interface is available through the use of the Cubic Mouse, an interaction device developed at GMD...|$|E
40|$|Online Analytical Processing (OLAP) is a {{database}} paradigm {{that supports the}} rich analysis of multi-dimensional data. OLAP is often supported by a logical structure known as the Cube, a data model that provides an intuitive array-based perspective of the underlying data. However, supporting efficient OLAP query resolution in enterprise scale environments {{is an issue of}} considerable complexity. In practice, the difficulty of the problem is exacerbated by the existence of dimension hierarchies that sub-divide core dimensions into aggregation layers of varying granularity. Common hierarchy-sensitive query operations such as Rollup and <b>Drilldown</b> can be very costly. Moreover, facilities for the representation of more complex hierarchical relationships are not well supported by conventional techniques. This thesis presents a robust hierarchy and caching framework that supports the efficient and transparent manipulation of attribute hierarchies within relational environments. Experimental results show that compared to the current methods, very little additional overhead is introduced by the proposed framework, even when advanced functionality is exploite...|$|E
40|$|The skyline {{operator}} {{is important}} for multicriteria decision making applications. Although many recent studies developed efficient methods to compute skyline objects in a specific space, the fundamental problem on the semantics of skylines remains open: Why and in which subspaces is (or is not) an object in the skyline? Practically, users may also {{be interested in the}} skylines in any subspaces. Then, {{what is the relationship between}} the skylines in the subspaces and those in the super-spaces? How can we effectively analyze the subspace skylines? Can we efficiently compute skylines in various subspaces? In this paper, we investigate the semantics of skylines, propose the subspace skyline analysis, and extend the full-space skyline computation to subspace skyline computation. We introduce a novel notion of skyline group which essentially is a group of objects that are coincidentally in the skylines of some subspaces. We identify the decisive subspaces that qualify skyline groups in the subspace skylines. The new notions concisely capture the semantics and the structures of skylines in various subspaces. Multidimensional roll-up and <b>drilldown</b> analysis is introduced. We also develo...|$|E
40|$|Visibility {{is crucial}} to {{managing}} and securing today’s computers and networks. Visualization tools are a means to provide visibility into the invisible world of network computing. Many good tools exist that give administrators a view into parts of the total picture, but our year-long study of system administrators and their tools shows a strong need for end-to-end visualization of network activity that preserves {{the context of the}} information observed. End-to-end visualization will allow an administrator to focus on one communication end-point and see the applications, processes, sockets, and ports responsible for all communications there. Then, the user can follow an application’s communications across the network and see the remote host and ports involved. If the same user owns the remote point, a tattletale client installed there can give the port-socket-processapplication information for the remote end point as well. This end-to-end view will help administrators construct accurate mental models of their networks via a comprehensive overview and <b>drilldown</b> for greater detail. This paper presents the requirements we garnered from system administrators as we developed Network Eye. We then discuss two vertical prototypes and discuss users’ reactions to them and to the end-to-end view...|$|E
40|$|Paper {{presented}} at the <b>Drilldown</b> Session ?E? of the Africa Library Summit 2011 held at Misty Hills Hotel, Conference Centre and Spa, Muldersdrift, Gauteng, South Africa from Wednesday 11 th to Friday 13 th May 2011 School/Community libraries are veritable tools for rural development. Their all-inclusive and dual services have evidenced this {{to everyone in the}} communities they serve who might be interested in their services. Children are particularly a distinct and active group benefitting from the services provided by school/community libraries. They use the libraries for recreational reasons through reading books and magazines, listening to book talks and stories and participating in library-based games and other light activities. Through such library services,children grow up already used to reading and libraries, having established a good base for ardent readers as they slowly enter adulthood. Adults alike, also enjoy the library activities that usually come in packages for recreational, educational, cultural and informational reading. Other activities such as study circles offer the adults an opportunity to participate in the lifelong continuing education process. The duality of school/community libraries is therefore not only possible, but an emerging phenomenon in rural Africa...|$|E
40|$|This thesis {{proposes a}} formal {{model for a}} {{federation}} of relational databases with possibly heterogeneous schemas. The Federation Model is comprehensive enough for: (i) capturing the diversity of schemas arising in practice, allowing a symmetric treatment of data and schema, and (ii) capturing the complete space of dimensional representations of data, fully exploiting the n logical dimensions structured along the three physical dimensions implicit in the relational model [...] row, column, and relation. An n-dimensional query language called nD-SQL is also proposed. This language makes use of the Federation Model and is capable of: (a) resolving schematic discrepancies among a collection of relational databases or data marts with heterogeneous schemas, and (b) supporting {{a whole range of}} multiple granularity aggregation queries like CUBE, ROLLUP, and <b>DRILLDOWN,</b> but, to an arbitrary, user controlled, level of resolution. In addition, nD-SQL can express queries that restructure data conforming to any particular dimensional representation into any other. The semantics of nD-SQL is downward compatible with the popular SQL language. The thesis also proposes an extension to relational algebra, capable of restructuring, called restructuring relational algebra (RRA). (Abstract shortened by UMI. ...|$|E
40|$|SPECT 3 D is a multi-dimensional collisional-radiative code used to {{post-process}} {{the output}} from radiation-hydrodynamics (RH) and particle-in-cell (PIC) codes to generate diagnostic signatures (e. g., images, spectra) {{that can be}} compared directly with experimental measurements. This ability to post-process simulation code output plays {{a pivotal role in}} assessing the reliability of RH and PIC simulation codes and their physics models. SPECT 3 D has the capability to operate on plasmas in 1 -D, 2 -D, and 3 -D geometries. It computes a variety of diagnostic signatures that can be compared with experimental measurements, including: time-resolved and time-integrated spectra, space-resolved spectra and streaked spectra; filtered and monochromatic images; and x-ray diode signals. Simulated images and spectra can include the effects of backlighters, as well as the effects of instrumental broadening and time-gating. SPECT 3 D also includes a <b>drilldown</b> capability that shows where frequency-dependent radiation is emitted and absorbed as it propagates through the plasma towards the detector, thereby providing insights on where the radiation seen by a detector originates within the plasma. SPECT 3 D has the capability to model a variety of complex atomic and radiative processes that affect the radiation seen by imaging and spectral detectors in high energy density physics (HEDP...|$|E
40|$|Sophie and Rogier are {{students}} at Delft University of Technology. Sophie {{is a master}} student of Architecture. She searches for information mostly to get inspired. Sometimes she’s looking for a specific book and uses Amazon first, because she knows what the cover looks like, but can’t remember the author or title. Rogier is writing his master’s thesis at the faculty of Civil Engineering. He searches the library for relevant publications. He is very insecure about the search results he’s getting back from the library catalogue and various bibliographic databases. Did he get the key publications? TU Delft Library’s websites and databases are not supporting either Sophie or Rogier’s needs. Sophie and Rogier are fictional customers of TU Delft Library. They are personas constructed from a context mapping study. Six different personas give our customers voices and faces. They help us specify the services and systems that our customers expect {{now and in the}} future, such as Discover ([URL] which will replace the traditional WebOPAC. With Discover, Sophie and Rogier can use Google-like queries and <b>drilldown</b> facets to find both books from the catalogue and full text articles (from Elsevier, DOAJ, Repositories and more). Records are enriched with cover images, abstracts, user ratings and LibraryThing tags. Discover is built with Meresco open source component library, which uses the Lucene search engine. Library TU DelftDelft University of Technolog...|$|E
40|$|Because of {{practical}} limits in characterizing the safety profiles of therapeutic products prior to marketing, manufacturers and regulatory agencies perform post-marketing surveillance {{based on the}} collection of adverse reaction reports (&quot;pharmacovigilance&quot;). The resulting databases, while rich in real-world information, are notoriously difficult to analyze using traditional techniques. Each report may involve multiple medicines, symptoms, and demographic factors, {{and there is no}} easily linked information on drug exposure in the reporting population. KDD techniques, such as association finding, are well-matched to the problem, but are difficult for medical staff to apply and interpret. To deploy KDD effectively for pharmacovigilance, Lincoln Technologies and GlaxoSmithKline collaborated to create a webbased safety data mining web environment. The analytical core is a high-performance implementation of the MGPS (Multi-Item Gamma Poisson Shrinker) algorithm described previously by DuMouchel and Pregibon, with several significant extensions and enhancements. The environment offers an interface for specifying data mining runs, a batch execution facility, tabular and graphical methods for exploring associations, and <b>drilldown</b> to case details. Substantial work was involved in preparing the raw adverse event data for mining, including harmonization of drug names and removal of duplicate reports. The environment can be used to explore both drug-event and multi-way associations (interactions, syndromes). It has been used to study age/gender effects, to predict the safety profiles of proposed combination drugs, and to separate contributions of individual drugs to safety problems in polytherapy situations...|$|E
40|$|Business Intelligence (BI) {{concept has}} {{continued}} to {{play a vital role}} in its ability for managers to make quality business decision to resolve the business needs of the organization. BI applications comes handy which allows managers to query, comprehend, and evaluate existing data within their organizations in order to obtain functional knowledge which then assist them in making improved and informed decisions. Data warehouse (DW) is pivotal and central to BI applications in that it integrates several diverse data sources, mainly structured transactional databases. However, current researches in the area of BI suggest that, data is no longer always presented in only to structured databases or format, but they also can be pulled from unstructured sources to make more power the managers’ analysis. Consequently, the ability to manage this existing information is critical for the success of the decision making process. The operational data needs of an organization are addressed by the online transaction processing (OLTP) systems which is important to the day-to-day running of its business. Nevertheless, they are not perfectly suitable for sustaining decision-support queries or business questions that managers normally needs to address. Such questions involve analytics including aggregation, <b>drilldown,</b> and slicing/dicing of data, which are best supported by online analytical processing (OLAP) systems. Data warehouses support OLAP applications by storing and maintaining data in multidimensional format. Data in an OLAP warehouse is extracted and loaded from multiple OLTP data sources (including DB 2, Oracle, SQL Server and flat files) using Extract, Transfer, and Load (ETL) tools. This thesis seeks to develop DW and BI system to support the decision makers and business strategist at Crystal Entertainment in making better decision using historical structured or unstructured data...|$|E
30|$|A {{successful}} implementation of disaster risk reduction options and strategies demands appropriate mechanisms to communicate and transfer the overall knowledge on risk and its underlying drivers to the various stakeholders involved in the decision-making process. Vulnerability assessments {{are the product of}} the state-of-the-art in science and integrate large volumes of data and sophisticated analysis. However, as the knowledge and the volume of scientific works on vulnerability assessments multiply steadily, it is becoming increasingly difficult for the practice and science community to keep track of all these developments effectively and to use it towards disaster risk reduction. Consequently, the main intention of making vulnerability assessments comparable by using a practical and structured access to the existing, complex, and growing knowledge field is to foster exchange of knowledge, as well as learning among disciplines. VuWiki can serve as a knowledge management tool for a broad research and practice community and, in this capacity, contribute to interactions between science and practice in terms of knowledge transfer. VuWiki also comprises the potential to bridge the “implementation gap” by serving as an interactive platform that helps sort through and convey the relevant knowledge for a specific context so that the knowledge is used and put into practice. For example, a national authority that may want to develop new guidelines for community flood risk management based on risk and vulnerability assessments can use the features in VuWiki such as wiki <b>drilldown</b> (see Sect. 4.3) to get an overview of the relevant parameters for flood vulnerability at the community scale of assessment and be able to discern which of the many studies are the most applicable to their particular needs. It {{should be noted that the}} usefulness of VuWiki in this regard is dependent on community involvement criteria, and the depth and extent to which the wiki is populated with additional vulnerability assessments beyond those currently represented. Second, VuWiki is “just” a tool that provides access to knowledge in a structured way. Due to copyright reasons, data licensing, and other legal limitations, it cannot provide the actual journal articles itself.|$|E
30|$|Kingsley and Dahj [17] {{proposed}} a tree-based SQM approach for efficient low-cost service management {{with a particular}} focus on the over-the-top (OTT) applications. The SQM-tree had four levels {{that focused on the}} 3 G mobile networks services classes, i.e. Streaming, interactive and down to the OTT applications. The system connected to a cloud application so as to provide reporting and Big Data throughput. SparkSQL was used to query the stored data allowing a <b>drilldown</b> to worst cellular cells, devices and subscribers. One of the drawbacks of that proposed system was the class of service classification which was specific to 3 G mobile networks and should be reviewed for a different technology. Two other methods, still focusing on the OTT internet services, were proposed by Fiadinoet al [18], resulting {{in the development of a}} framework called RCATool. The RCATool used the domain name server (DNS) protocol to detect and diagnose the traffic anomalies in the network. Diagnostic features such as devices information, error codes and the hostnames were used in the investigation of problems. The first method applied to the entropy of the diagnostic features while the second method considered the statistical distribution of features such as the traffic. Miyazawa and Nishimura [19] {{proposed a}}n RCA approach in investigating service failures in a fixed-mobile converged network. The approach used alarms classification and a hierarchical alarm data model on different types of alarms such as the resource alarms, the performance alarms and the service alarms to pinpoint the cause of network failures. In essence [18, 19], analysed only one type of services, which is far from the current reality of mobile networks implementation. Cai et al. [20] provided an intensive investigation of fault diagnosis using Bayesian network. Although Bayesian network works well for fault diagnosis even for complex industrial systems, it has drawbacks for non-permanent faults that provides only weak signals and for online fault diagnosis which are very slow. In line with the above, Hong et al. [21] investigated fault diagnosis for the circuit-switched fall-back service. Although the investigation used detailed data of the signalling procedures from different mobile operators, the problem finding mechanism was manual and not subscriber-oriented.|$|E
40|$|Currently, {{there are}} both methodological and {{practical}} barriers that together preclude a substantial use of theoretically sound approaches, {{such as the}} ones recommended by the Highway Safety Manual (HSM), for traffic safety management. Although the state-of-the-art provides theoretically sound approaches such as the Empirical Bayes method, there are still various important capabilities missing. Methodological barriers include among others (i) lack of a theoretically sound approach for corridor-level network screening, (ii) lack of a comprehensive approach for estimation of Safety Performance Functions based on a simultaneous consideration of both crash patterns and associated explanatory variables, and (iii) lack of theoretically sound methods to forecast crash patterns at the regional level. In addition, the use of existing theoretically sound approaches {{such as the ones}} recommended by the HSM are associated with important practical barriers including 1) significant data integration requirements, 2) a special schema is needed to enable analysis using specialized software, 3) time-consuming and intensive processes are involved, 4) substantial technical knowledge is needed, 5) visualization capabilities are limited, and 6) coordination across various data owners is required. Considering the above barriers, most practitioners use theoretically unsound methodologies to perform traffic safety analyses for highway safety improvement programs. This research proposes a single comprehensive framework to address all the above barriers to enable the use of theoretically sound methodologies for network wide traffic safety analyses. The proposed framework provides access through a single platform, Business Intelligence (BI), to theoretically sound methods and associated algorithms, data management and integration tools, and visualization capabilities. That is, the proposed BI framework provides methods and mechanisms to integrate and process data, generate advanced and theoretically sound analytics, and visualize results through intuitive and interactive web-based dashboards and maps. The proposed BI framework integrates data using Extract-Load-Transform process and creates a traffic safety data warehouse. Algorithms are implemented to use the data warehouse for network screening analysis of roadway segments, intersections, ramps, and corridors. The methodology proposed and implemented here for corridor-level network screening represents an important expansion to the existing methods recommended by the HSM. Corridor-level network screening is important for decision makers because it enables to rank corridors rather than sites so as to provide homogenous infrastructure to minimize changes within relatively short distances. Improvements are recommended for long sections of roadways that could include multiple sites with the potential for safety improvements. Existing corridor screening methodologies use observed crash frequency as a performance measure which does not consider regression-to-the-mean bias. The proposed methodology uses expected crash frequency as a performance measure and searches corridors using a sliding window mechanism which addresses crash location reporting errors by considering the same section of roadway multiple times using overlapping windows. The proposed BI framework includes a comprehensive methodology for the estimation of SPFs considering simultaneously local crash patterns and site characteristics. The current state-of-the-art uses predefined crash site types to create single clusters of data to generate regression models, SPFs, for the estimation of predicted crash frequency. It is highly unlikely for all crash sites within a single predefined cluster/type to have similar crash patterns and associated explanatory characteristics. That is, there could be sites within a cluster/type with different crash patterns and explanatory characteristics. Hence, assigning a single predefined SPF to all sites within a type is not necessarily the best approach to minimize the estimation error. To address this issue, a mathematical program was formulated to determine simultaneously cluster memberships for crash sites and the corresponding SPFs. Cluster memberships are determined using both crash patterns and associated explanatory variables. A solution algorithm coupling simulation annealing and maximum log likely estimation was implemented and tested. Results indicated that multiple SPFs for a crash and/or facility type can maximize the probability of observing the available data to increase accuracy and reliability. The estimated SPFs using the proposed approach were implemented within the BI framework for network screening. The results illustrate that the gain in predicted crashes provided by the SPFs translates into superior rankings for sites and corridors with the potential for safety improvements. A performance-based safety program requires the forecasting, at the regional level, of safety performance measures and establish targets to reduce fatalities and serious injuries. This is in contrast to the analysis required for traffic safety management where forecasts are required at the site or corridor level. For regional level forecasting, historically, theoretically unsound methods such as extrapolation or simple moving-average models have been used. To address this issue, this study proposed deterministic and stochastic time series models to forecast performance measures for performance-based safety programs. Results indicated that stochastic time series, a seasonal autoregressive integrated moving average model, provides the required statistically sound forecasts. In summary, the fundamental contributions of this research include: (i) a theoretically sound methodology for corridor level network screening, (ii) a comprehensive methodology for the estimation of local SPFs considering simultaneously crash patterns and associated explanatory variables, and (iii) a theoretically sound methodology to forecast performance measures to set realistic targets for performance-based safety programs. In addition, this study implemented and tested the above contributions along with existing algorithms for traffic safety network screening within a single BI platform. The result is a single web-based BI framework to enable integration and management of source data, generation of theoretically sound analyses, and visualization capabilities through intuitive dashboards, <b>drilldown</b> menus, and interactive maps...|$|E

