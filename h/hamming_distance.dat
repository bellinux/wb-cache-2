2506|220|Public
5|$|There {{also exist}} {{algorithms}} for finding two satisfying assignments {{that have the}} maximal <b>Hamming</b> <b>distance</b> from each other.|$|E
5|$|Richard Wesley Hamming (February 11, 1915 – January 7, 1998) was an American {{mathematician}} {{whose work}} had many implications for computer engineering and telecommunications. His contributions include the Hamming code (which {{makes use of}} a Hamming matrix), the Hamming window, Hamming numbers, sphere-packing (or Hamming bound), and the <b>Hamming</b> <b>distance.</b>|$|E
5|$|Hamming {{set himself}} {{the task of}} solving this problem, which he {{realised}} would have an enormous range of applications. Each bit can only be a zero or a one, so if you know which bit is wrong, then it can be corrected. In a landmark paper published in 1950, he introduced a concept {{of the number of}} positions in which two code words differ, and therefore how many changes are required to transform one code word into another, which is today known as the <b>Hamming</b> <b>distance.</b> Hamming thereby created a family of mathematical error-correcting code, which are called Hamming codes. This not only solved an important problem in telecommunications and computer science, it opened up a whole new field of study.|$|E
30|$|From Fig.  10 g, {{it can be}} {{observed}} that as the watermark embedding strength increases the normalized <b>hamming</b> <b>distances</b> approach towards threshold distance. The reason may be the higher strength watermark slightly disturbed the SVD coefficients. But, all the normalized <b>hamming</b> <b>distances</b> are below the selected threshold. Figure  10 h, i shows that all the normalized <b>hamming</b> <b>distances</b> are less than the predetermined threshold. Hence, the proposed method is robust against scaling and JPEG compression.|$|R
30|$|Computation of the <b>hamming</b> <b>distances</b> between glowworm i and {{the other}} glowworms by using (14).|$|R
3000|$|... [...]) are {{distinct}} for each fingerprint. In our experiment, <b>Hamming</b> <b>distances</b> are computed {{to measure the}} similarity or dissimilarity of the genuine keys and impostor’s keys. The observation is shown using histogram in Figure 8 b. In this case, the <b>Hamming</b> <b>distances</b> are distributed from 37.50 % (minimum) to 61.33 % (maximum) {{with a mean of}} 50 % and standard division of 0.0309. Maximum impostors’ keys (i.e., 99.85 % impostors’ keys) differ from genuine key with the range of 40 % to 60 % <b>Hamming</b> <b>distances.</b> Even when the similarity of the impostor’s key is maximum (i.e., 62.5 % bits are similar), the attacker needs to guess 96 bits (i.e., 296 trials in brute force attacks) to crack the genuine key.|$|R
25|$|In {{the half}} uniform {{crossover}} scheme (HUX), exactly {{half of the}} nonmatching bits are swapped. Thus first the <b>Hamming</b> <b>distance</b> (the number of differing bits) is calculated. This number is divided by two. The resulting number is {{how many of the}} bits that do not match between the two parents will be swapped.|$|E
25|$|Sphere packing on {{the corners}} of a {{hypercube}} (with the spheres defined by <b>Hamming</b> <b>distance)</b> corresponds to designing error-correcting codes: if the spheres have radius t, then their centers are codewords of a (2t+1)-error-correcting code. Lattice packings correspond to linear codes. There are other, subtler relationships between Euclidean sphere packing and error-correcting codes. For example, the binary Golay code {{is closely related to}} the 24-dimensional Leech lattice.|$|E
25|$|In telecommunication, a Hamming code is {{a linear}} error-correcting code. Hamming codes can detect {{up to two}} {{simultaneous}} bit errors, and correct single-bit errors; thus, reliable communication is possible when the <b>Hamming</b> <b>distance</b> between the transmitted and received bit patterns is {{less than or equal}} to one. By contrast, the simple parity code cannot correct errors, and can only detect an odd number of errors. Hamming codes are of fundamental importance in coding theory and remain of practical use in modern computer design. Hamming codes were invented in 1950 by Richard Hamming at Bell Labs.|$|E
3000|$|DIST (Protocol 2) {{provides}} a secure computation of the modified <b>Hamming</b> <b>distances</b> between Alice's probe q and Bob's [...]...|$|R
40|$|We {{describe}} hashing of {{data bases}} {{as a problem}} of information and coding theory. It is shown that the triangle inequality for the <b>Hamming</b> <b>distances</b> between binary vectors may essentially decrease the computational efforts of {{a search for a}} pattern in the data base. Introduction of the Lee distance in the space, which consists of the <b>Hamming</b> <b>distances,</b> leads to a new metric space where the triangle inequality can be effectively used...|$|R
5000|$|... #Caption: Search {{space for}} the {{normalized}} problem. The center string aaaa and aaab leads to <b>Hamming</b> <b>distances</b> 1,2,1 and 2,1,1, respectively.|$|R
25|$|An {{elementary}} {{cellular automaton}} rule is specified by 8 bits, and all elementary cellular automaton rules {{can be considered}} {{to sit on the}} vertices of the 8-dimensional unit hypercube. This unit hypercube is the cellular automaton rule space. For next-nearest-neighbor cellular automata, a rule is specified by 25=32 bits, and the cellular automaton rule space is a 32-dimensional unit hypercube. A distance between two rules can be defined by the number of steps required to move from one vertex, which represents the first rule, and another vertex, representing another rule, {{along the edge of the}} hypercube. This rule-to-rule distance is also called the <b>Hamming</b> <b>distance.</b>|$|E
500|$|The Hamming bound, {{also known}} as the sphere-packing or volume bound is a limit on the {{parameters}} of an arbitrary block code. It is from an interpretation in terms of sphere packing in the <b>Hamming</b> <b>distance</b> into the space of all possible words. [...] It gives an important limitation on the efficiency with which any error-correcting code can utilize the space in which its code words are embedded. [...] A code which attains the Hamming bound {{is said to be a}} perfect code. Hamming codes are perfect codes.|$|E
2500|$|Bregman divergences {{can also}} be defined between matrices, between functions, and between {{measures}} (distributions). [...] Bregman divergences between matrices include the Stein's loss and von Neumann entropy. [...] Bregman divergences between functions include total squared error, relative entropy, and squared bias; see the references by Frigyik et al. below for definitions and properties. Similarly Bregman divergences have also been defined over sets, through a submodular set function which {{is known as the}} discrete analog of a convex function. The submodular Bregman divergences subsume a number of discrete distance measures, like the <b>Hamming</b> <b>distance,</b> precision and recall, mutual information and some other set based distance measures (see Iyer & Bilmes, 2012) for more details and properties of the submodular Bregman.) ...|$|E
40|$|In {{this article}} {{we focus on the}} channel {{decoding}} problem in presence of a-priori information. In particular, assuming that the a-priori information reliability is not perfectly estimated at the receiver, we derive a novel analytical framework for evaluating the decoder's performance. It is derived the important result that a good code, i. e., a code which allows to fully exploit the potential benefit of a-priori information, must associate information sequences with high <b>Hamming</b> <b>distances</b> to codewords with low <b>Hamming</b> <b>distances.</b> Basing on the proposed analysis, we analyze the performance of random codes and turbo codes...|$|R
30|$|THRIVE {{system can}} be used with any {{existing}} biometric modality whose templates can be transformed into a binary vector and decision can be based on <b>Hamming</b> <b>distances</b> between binary templates.|$|R
50|$|The SDM calculates <b>Hamming</b> <b>distances</b> {{between the}} {{reference}} address and each location address. For each distance {{which is less}} or equal to the given radius the corresponding location is selected.|$|R
2500|$|By {{comparing}} extant sequences, one {{can determine}} the amount of sequence divergence. This raw measurement of divergence provides information about the number of changes that have occurred along the path separating the sequences. The simple count of differences (the <b>Hamming</b> <b>distance)</b> between sequences will often underestimate the number of substitution because of multiple hits (see homoplasy). Trying to estimate the exact number of changes that have occurred is difficult, and usually not necessary. Instead, branch lengths (and path lengths) in phylogenetic analyses are usually expressed in the expected number of changes per site. The path length is the product of the duration of the path in time and the mean rate of substitutions. [...] While their product can be estimated, the rate and time are not identifiable from sequence divergence.|$|E
2500|$|Since Shallit's statement, Dembski has (as of May 2010) {{published}} four peer-reviewed {{papers in}} information theory venues {{associated with the}} IEEE professional society. The papers deal with active information {{in the context of}} searches for solutions to problems. Quantified active information is introduced in [...] "Conservation of Information in Search: Measuring the Cost of Success." [...] A second paper, [...] "Evolutionary Synthesis of Nand Logic: Dissecting a Digital Organism," [...] claims to deconstruct the evolution simulation Avida by uncovering the sources of active information in the program. A third paper discusses the role of Jacob Bernoulli's principle of indifference in the analysis of evolution. The most recent paper, [...] "Efficient Per Query Information Extraction from a Hamming Oracle," [...] calculates the performance of various search algorithms which use the <b>Hamming</b> <b>distance</b> to search for a single string of a certain length in the set of all strings of this length.|$|E
5000|$|<b>Hamming</b> <b>distance</b> : Given two vectors [...] the <b>Hamming</b> <b>distance</b> between [...] and , {{denoted by}} , is defined {{to be the}} number of {{positions}} in which [...] and [...] differ.|$|E
40|$|For any prime p, λ-constacyclic {{codes of}} length p^s over R=F_p^m + uF_p^m are {{precisely}} {{the ideals of}} the local ring R_λ= R[x]/〈 x^p^s-λ〉, where u^ 2 = 0. In this paper, we first investigate the <b>Hamming</b> <b>distances</b> of cyclic codes of length p^s over R. The minimum <b>Hamming</b> <b>distances</b> of all cyclic codes of length p^s over R are determined. Moreover, an isometry between cyclic and α-constacyclic codes of length p^s over R is established, where α is a nonzero element of F_p^m, which carries over the results regarding cyclic codes corresponding to α-constacyclic codes of length p^s over R...|$|R
30|$|The {{output of}} the {{matching}} module, the two <b>hamming</b> <b>distances,</b> represents a point in 2 D distance space. To compute the final matching distance, the genuine and imposter classes based on the training set must be defined. The pairs of <b>hamming</b> <b>distances</b> computed between every two iris images of the same individual constitute the points belonging to the genuine class. The imposter class is comprised of the pairs of <b>hamming</b> <b>distances</b> explaining the dissimilarity between every two iris images of different individuals. Here, to ascertain the fusion strategy means to map all the points lying in the distance space into a 1 D space in which the points of different classes gain maximum separability. For this purpose, the SVM is adopted to determine the separating boundary between the genuine and imposter classes. Using different kernels {{makes it possible to}} define linear and nonlinear boundaries and consequently a variety of linear and nonlinear fusion rules. The position and distance of the new test point relative to the decision boundary determine the sign and absolute value of the fused distance, respectively.|$|R
30|$|There {{are mainly}} two {{objectives}} in our experiment. First, we investigate {{the impact of}} data encoding into cover images (i.e., synthetic fingerprints from DB 4 of each database) and accuracy of data decoding from stego images. In the next part of our experiment, we measure the randomness of cryptographic key generated from fingerprints of genuine users {{with respect to the}} key generated from impostor’s fingerprints. In this regard, the <b>Hamming</b> <b>distances</b> between the genuine and impostor’s keys are measured and corresponding histograms are plotted. Here, we consider all possible cases of attacks when different entities are compromised and the <b>Hamming</b> <b>distances</b> are computed for each case.|$|R
5000|$|... #Caption: A {{two-dimensional}} visualisation of the <b>Hamming</b> <b>distance.</b> The {{color of}} each pixel indicates the <b>Hamming</b> <b>distance</b> between the binary representations of its x and y coordinates, modulo 16, in the 16-color system.|$|E
50|$|As for all digital codes, {{the error}} {{detection}} and correction abilities of polynomial codes {{are determined by}} the minimum <b>Hamming</b> <b>distance</b> of the code. Since polynomial codes are linear codes, the minimum <b>Hamming</b> <b>distance</b> is equal to the minimum weight of any non-zero codeword. In the example above, the minimum <b>Hamming</b> <b>distance</b> is 2, since 01001 is a codeword, and there is no nonzero codeword with only one bit set.|$|E
50|$|The <b>Hamming</b> <b>distance</b> is used {{to define}} some {{essential}} notions in coding theory, such as error detecting and error correcting codes. In particular, a code C {{is said to be}} k-errors detecting if any two codewords c1 and c2 from C that have a <b>Hamming</b> <b>distance</b> less than k coincide; otherwise, a code is k-errors detecting if, and only if, the minimum <b>Hamming</b> <b>distance</b> between any two of its codewords is at least k+1.|$|E
40|$|In this paper, we {{investigate}} the multiple attribute decision making (MADM) problems {{for evaluating the}} archives websites’ performance with interval intuitionistic fuzzy information. Then, based on the TOPSIS method, calculation steps for solving MADM problems for evaluating the archives websites’ performance with interval intuitionistic fuzzy information are given. The weighted <b>Hamming</b> <b>distances</b> between every alternative and positive ideal solution and negative ideal solution are calculated. Then, according to the weighted <b>Hamming</b> <b>distances,</b> the relative closeness degree to the positive ideal solution is calculated to rank all alternatives. Finally, an illustrative example for evaluating the archives websites’ performance with interval intuitionistic fuzzy information is given...|$|R
30|$|Finally, we {{note that}} it is {{straightforward}} to develop the skipping criteria for efficient search {{of the list of}} the test error patterns in the OSD-based decoding schemes. For instance, one can consider the <b>Hamming</b> <b>distances</b> for one or more segments of the MRIPs between the received hard decisions (before the decoding) and the temporary decisions obtained using the test error patterns from the list. If any or all of the <b>Hamming</b> <b>distances</b> are above given thresholds, the test error pattern can be discarded without re-encoding and calculating its Euclidean distance. For the Q= 2 segments OSD being considered, our empirical results indicate that the thresholds of the number of bit errors in the first and the second segments should be ⌈ 0.35 dmin⌉and dmin, respectively.|$|R
40|$|Authors {{are listed}} in inverse {{alphabetical}} order. Abstract: Thousands of different forms (words) are associated with thousands of different meanings (concepts) in a language computer model. Reasonable agreement with reality is found {{for the number of}} languages in a family and the <b>Hamming</b> <b>distances</b> between languages. ...|$|R
5000|$|For any pair of length- words [...] and [...] over , the <b>Hamming</b> <b>distance</b> [...] is {{the number}} of {{positions}} [...] at which [...] Further, define reverse-Hamming distance as [...] Similarly, reverse-complement <b>Hamming</b> <b>distance</b> is [...] (where [...] stands for reverse complement) ...|$|E
5000|$|... #Caption: 4-bit binary tesseractfor finding <b>Hamming</b> <b>distance.</b>|$|E
5000|$|... 4-of-8 code extension: As an {{alternative}} to the IBM transceiver code (which is a 4-of-8 code with a <b>Hamming</b> <b>distance</b> of 2), it is also possible to define a 4-of-8 excess-3 code extension achieving a <b>Hamming</b> <b>distance</b> of 4, if only denary digits are to be transferred.|$|E
40|$|In {{this paper}} we study a special type of quasi-cyclic (QC) codes called skew QC codes. This set of codes is {{constructed}} using a non-commutative ring called the skew polynomial rings F [x; θ]. After {{a brief description}} of the skew polynomial ring F [x; θ] it is shown that skew QC codes are left submodules of the ring Rls = (F [x; θ]/(x s − 1)) l. The notions of generator and parity-check polynomials are given. We also introduce the notion of similar polynomials in the ring F [x; θ] and show that parity-check polynomials for skew QC codes are unique up to similarity. Our search results lead to the construction of several new codes with <b>Hamming</b> <b>distances</b> exceeding the <b>Hamming</b> <b>distances</b> of the previously best known linear codes with comparable parameters. I...|$|R
40|$|The {{nonlinearity}} {{profile of}} a Boolean function is the sequence of its minimum <b>Hamming</b> <b>distances</b> nl_r(f) to all functions of degrees at most r, for r≥ 1. The nonlinearity {{profile of a}} vectorial function is the sequence of the minimum <b>Hamming</b> <b>distances</b> between its component functions and functions of degrees at most r, for r≥ 1. The profile of the multiplicative inverse functions has been lower bounded in a previous paper by the same author. No other example of an infinite class of functions with unbounded algebraic degree has been exhibited since then, whose nonlinearity profile could be efficiently lower bounded. In this preprint, we lower bound the whole nonlinearity profile of the simplest Dillon bent function (x,y) xy^ 2 ^n/ 2 - 2, x,y∈ F_ 2 ^n/ 2...|$|R
40|$|The {{similarity}} {{patterns of}} the genetic code result from similar codons encoding similar messages. We develop a new mathematical model to analyze these patterns. The physicochemical characteristics of amino acids objectively quantify their differences and similarities; the Hamming metric does {{the same for the}} 64 codons of the codon set. (<b>Hamming</b> <b>distances</b> equal the number of different codon positions: AAA and AAC are at 1 -distance; codons are maximally at 3 -distance.) The CodonPolytope, a 9 -dimensional geometric object, is spanned by 64 vertices that represent the codons and the Euclidian distances between these vertices correspond one-to-one with intercodon <b>Hamming</b> <b>distances.</b> The CodonGraph represents the vertices and edges of the polytope; each edge equals a Hamming 1 -distance. The mirror reflection symmetry group of the polytope is isomorphic to the largest permutation symmetry group of the codon set that preserves <b>Hamming</b> <b>distances.</b> These groups contain 82, 944 symmetries. Many polytope symmetries coincide with the degeneracy and similarity {{patterns of the}} genetic code. These code symmetries are strongly related with the face structure of the polytope with smaller faces displaying stronger code symmetries. Splitting the polytope stepwise into smaller faces models an early evolution of the code that generates this hierarchy of code symmetries. The canonical code represents a class of 41, 472 codes with equivalent symmetries; a single class among an astronomical number of symmetry classes comprising all possible codes...|$|R
