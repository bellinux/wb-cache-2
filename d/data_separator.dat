10|8|Public
50|$|The FD1791-FD1797 series added {{internal}} support for double density (MFM) modulation, {{compatible with the}} IBM System/34 disk format. They required an external <b>data</b> <b>separator.</b>|$|E
5000|$|A <b>data</b> <b>separator</b> and {{a double}} density disk {{controller}} (based on the WD 1791 chip) {{were made by}} Percom (a Texas peripheral vendor), LNW, Tandy and others. The Percom Doubler added the ability to boot and use Double Density Floppies (they provided their own modified TRSDOS called DoubleDOS), and included the <b>Data</b> <b>Separator.</b> The LNDoubler added {{the ability to read}} and write from 5¼" [...] diskette drives for over 1.2 Mb of storage. Near the end of the Model I's lifespan in 1982, upgrades were offered to replace the original controller with a double density one.|$|E
50|$|The WD2791-WD2797 series {{added an}} {{internal}} <b>data</b> <b>separator</b> using an analog phase-locked loop, with some external passive components {{required for the}} VCO. They took a 1 MHz or 2 MHz clock and were intended for 8 in and 5.25 in drives.|$|E
50|$|Due to {{the second}} fact, an unmodified 4FDC {{can not be used}} with 8-inch drives that don't have single-density <b>data</b> <b>separators</b> on the drive electronics. Later Cromemco disk {{controllers}} such as the 16FDC and 64FDC contained both single and double density <b>data</b> <b>separators</b> and the 64FDC also supplied write pre-compensation.|$|R
5000|$|The segment tag {{delimiter}} is an '=' {{rather than}} a <b>data</b> element <b>separator</b> ...|$|R
5000|$|The {{component}} <b>data</b> element <b>separator</b> and <b>data</b> element <b>separator</b> are the [...] "first level" [...] and [...] "second level" [...] <b>separators</b> of <b>data</b> {{elements within}} a message segment. Referring {{to them as}} + and : for brevity, the + separates top-level or composite data elements, and : separates second-level data elements nested within composite data elements. Trailing empty (or null) data elements and their leading separators are omitted to reduce message size.|$|R
50|$|The WD1770, WD1772, and WD1773 {{added an}} {{internal}} digital <b>data</b> <b>separator</b> and write precompensator, {{eliminating the need}} for external passive components but raising the clock rate requirement to 8 MHz. They supported double density, despite the apparent regression of the part number, and were packaged in 28-pin DIP packages.|$|E
50|$|This {{version of}} the 810 Happy board was a plug-in board with a better <b>data</b> <b>separator</b> and used sockets already in place on the 810 {{internal}} board {{without the need for}} any soldering or permanent modification.In addition to the buffered reading and writing with zero latency and faster serial i/o, it made backups of floppies.|$|E
50|$|An {{aftermarket}} add-on board, the FDCX4 Double Density Upgrade Board for the 4FDC, {{was designed}} and marketed by JVB Electronics. The FDCX4 was a daughter board assembly that replaced the WD1771 single density disk controller chip on the 4FDC with a FD1791 (early production) or Fujitsu MB8876A (later production) double-density controller chip. The FDCX4, in addition to using an analog phase-locked-loop <b>data</b> <b>separator</b> in all modes, also used write-precompensation. These features allowed the FDCX4 equipped 4FDC to reliably use the Persci 277 drives, {{as well as other}} drives, in double-density mode.|$|E
40|$|The {{separation}} of the searched data from the rest is an important task in data mining. Three separation/classification methods are presented. We use a singularity exponent in classifiers {{that are based on}} distances of patterns to a given (classified) pattern. The approximation of so called probability distribution mapping function of the distribution of points from the viewpoint of distances from a given point {{in the form of a}} scaling exponent power of a distance is presented together with a way how to state it. Considering data as points in a metric space, three methods are based on transformed distances of neighbors of a given point in a multidimensional space via functions that use different estimates of scaling exponent. Classifiers â€“ <b>data</b> <b>separators</b> utilizing knowledge about explored data distribution in a space and suggested expressions of the scaling exponent are presented. Experimental results on both synthetic and real-life data show interesting behavior (classification accuracy) of classifiers in comparison with other well-known approaches...|$|R
40|$|Solution gas/oil ratio (GOR) and oil {{formation}} {{volume factor}} (FVF) are normally obtained from differential or flash liberation tests. However, neither the differential liberation process nor the flash liberation process can represent the fluid flow in petroleum reservoirs. Therefore, {{data obtained from}} any of the two test procedures must be adjusted to approximate the fluid behavior in the reservoir. At low pressures, the conventional method of adjustment yields negative values of solution GOR and values of oil FVF of less than 1. This, of course, is not physically correct. This paper presents a new method for adjusting the differential liberation <b>data</b> to <b>separator</b> conditions. The new method overcomes the limitations of the conventional adjustment method and makes the low-pressure extension of the curves of solution GOR and oil FVF more accurate. The method is {{based on the fact that}} data obtained from both the differential and flash liberation tests should yield the same value of oil relative density at reservoir conditions. The new method is tested using 425 PVT files, yielding results that are consistent with the physical behavior of solution GOR and oil FVF...|$|R
40|$|The {{determination}} {{of the number of}} groups in a dataset, theircomposition and the most relevant measurements to be considered in clusteringthe data, is a high-demanding task, especially when the a priori information onthe dataset is limited. Three different genetic approaches are introduced in thispaper as tools for automatic data clustering and features selection. They differin the adopted codification of the grouping problem, not in the evolutionaryoperator and parameters. Two of them deals with the grouping problem in adeterministic framework. The first directly approaches the grouping problem asa combinatorial one. The second tries to determine some relevant points in thedata domain to be used in clustering <b>data</b> as group <b>separators.</b> A probabilisticframework is then introduced with the third one, which starts specifying thestatistical model from which data are assumed to be drawn. The evolutionaryapproaches are, finally, compared with respect to classical partitional clusteringalgorithms on simulated data and on Fisher’s Iris dataset used as a benchmark...|$|R
5000|$|Enhanced Small Disk Interface (ESDI) was a disk {{interface}} {{designed by}} Maxtor Corporation {{in the early}} 1980s to be a follow-on to the ST-412/506 interface. ESDI improved on ST-506 by moving certain parts that were traditionally kept on the controller (such as the <b>data</b> <b>separator)</b> into the drives themselves, and also generalizing the control bus such that more kinds of devices (such as removable disks and tape drives) could be connected. ESDI used the same cabling as ST-506 (one 34-pin common control cable, and a 20-pin data channel cable for each device), and thus could easily be retrofitted to ST-506 applications.|$|E
5000|$|Although {{demand for}} Model I drives greatly {{exceeded}} supply at first, since the interface lacked a separate external <b>data</b> <b>separator,</b> {{it was very}} unreliable in practice. Much of the unreliability was due to bugs in Radio Shack's early version(s) of TRSDOS. The 1771 could not report its status for a short interval (several instruction cycles) after it received a command. A common method of handling this was to issue a command to the 1771, perform several [...] "NOP" [...] instructions, then query the 1771 for command status. Early TRSDOS neglected to use the required wait period, instead querying the chip immediately after issuing a command, and thus false status was often returned to the OS, causing various errors and crashes. If the 1771 was handled correctly by the OS, it was actually fairly reliable.|$|E
5000|$|Aster {{computers}} {{was based}} {{in the small town}} of Arkel near the town of Gorinchem.Initially Aster computer b.v. was called MCP (Music print Computer Product), because it was specialized in producing computer assisted printing of sheet music. The director of the company was interested in Microprocessor technology and noticed there was a market for selling kits to computer building amateurs, so they started selling electronic kits to hobbyists, and employed four persons at that time [...] They also assembled kits for people without soldering skills, especially the [...] "junior Computer" [...] from Elektor (a copy of the KIM-1), and the ZX80 from Sinclair. Among the kits sold there were also alternative floppy disk drives for TRS-80 computers. But these needed the infamous TRS-80 expansion interface, which was very expensive, and had a very unreliable floppy disk controller because it used the WD1771 floppy disc controller chip without an external [...] "data separator". To fix this problem MCP developed a small plugin board which could be plugged into the socket for the WD1771, and which contained a <b>data</b> <b>separator,</b> and a socket for the WD1791 to support dual-density operation. Still, the expansion interface was expensive and due to its design it was also unreliable. So they decided to also develop their own alternative in the form of an improved floppy disk controller and printer interface that could be built right into a floppy disk enclosure. The lack of RAM expansion offered by this solution was solved by a service in which the 16 KB RAM chips inside the base unit would be replaced by 64 KB RAM chips.While this went on MCP renamed itself to MCP CHIP but ran into problems with the German computer magazine CHIP, and had to return to its former name. At that time MCP did also sell imported home computers like the TRS-80, the Video Genie, (another TRS-80 clone), the Luxor ABC 80 and the Apple II.They also sold the exotic Olivetti M20, a very early 16-bit personal computer that was one of the very few systems to use a Z8000 CPU.|$|E
40|$|DOE GO 13028 - 0001 DESCRIPTION/ABSTRACT This {{report is}} {{a summary of}} the work {{performed}} by Teledyne Energy Systems to understand high pressure electrolysis mechanisms, investigate and address safety concerns related to high pressure electrolysis, develop methods to test components and systems of a high pressure electrolyzer, and produce design specifications for a low cost high pressure electrolysis system using lessons learned throughout the project. Included in this report are <b>data</b> on <b>separator</b> materials, electrode materials, structural cell design, and dissolved gas tests. Also included are the results of trade studies for active area, component design analysis, high pressure hydrogen/oxygen reactions, and control systems design. Several key pieces of a high pressure electrolysis system were investigated in this project and the results will be useful in further attempts at high pressure and/or low cost hydrogen generator projects. An important portion of the testing and research performed in this study are the safety issues that are present in a high pressure electrolyzer system and that they can not easily be simplified to a level where units can be manufactured at the cost goals specified, or operated by other than trained personnel in a well safeguarded environment. The two key objectives of the program were to develop a system to supply hydrogen at a rate of at least 10, 000 scf/day at a pressure of 5000 psi, and to meet cost goals of $ 600 / kW in production quantities of 10, 000 /year. On these two points TESI was not successful. The project was halted due to concerns over safety of high pressure gas electrolysis and the associated costs of a system which reduced the safety concerns...|$|R
40|$|<b>Data</b> format: CSV <b>Separator</b> character: '#' This dataset {{contains}} 1151 commits manually {{labeled with}} maintenance activities ("c" for corrective, "p" for perfective, "a" for adaptive) {{according to the}} definition by Mockus et al. in "Mockus, A. and Votta, L. G., 2000, October. Identifying Reasons for Software Changes using Historic Databases. In icsm (pp. 120 - 130) ". In addition, this dataset also contains further information (features) extracted from the commits: 	The source code changes performed by the commit author {{as part of a}} given commit (statement added, statement removed, etc.) 	 		The source code change taxonomy is detailed in "Fluri, B. and Gall, H. C., 2006, June. Classifying change types for qualifying change couplings. In Program Comprehension, 2006. ICPC 2006. 14 th IEEE International Conference on (pp. 35 - 45). IEEE. " 	 	 	A binary indication (1 / 0) whether a given commit contains any of the keywords from a pre-computed (according to a word frequency analysis) set of keywords indicative of each maintenance activity. The dataset consists of commits sampled from the following open source projects: 	RxJava 	hbase 	elasticsearch 	intellij-community 	hadoop 	drools 	kotlin 	restlet-framework-java 	orientdb 	camel 	spring-framework This dataset is a supporting material for the paper "Boosting Automatic Commit Classification Into Maintenance Activities By Utilizing Source Code Changes", to appear in PROMISE 2017...|$|R
3000|$|Sub-routine 1 : (<b>Data</b> <b>separator)</b> Sensitive {{transactions}} (T*) { if [...] [...] s⊆ T, where [...] [...] s ∈ S, T∈ D} {{are separated}} {{out from the}} non-sensitive transactions (T’) {{in order to reduce}} the size of dataset need to be processed further.|$|E

