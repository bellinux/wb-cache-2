496|423|Public
25|$|Disagreements {{between the}} {{government}} and the FARC continued on the mechanism for ratification of a final agreement. In November, the government gave its support to a bill submitted by senator Roy Barreras (Partido de la U) organizing a plebiscite on a final agreement. In Havana, the FARC responded negatively to the idea of the plebiscite, insisting on a constituent assembly. With the support of the government's congressional majority, the bill regulating the plebiscite was adopted by Congress in December 2015. As per the statutory law regulating the plebiscite, approval requires support equivalent to 13% of the registered electorate for the winning option, a one-time exception to the existing law regulating plebiscites (Law 1757 of 2015) which has a turnout quorum of 50%. The reduction of the quorum, and the change from a turnout threshold to a <b>decision</b> <b>threshold,</b> was controversial. Additionally, in the plebiscite voters would vote on the final agreement as a whole rather than article-by-article, something which also created some criticisms, primarily from Uribe's Democratic Centre. Following its adoption by Congress, the law passed to the Constitutional Court for a mandatory revision.|$|E
5000|$|Supermajority <b>decision</b> <b>threshold</b> {{requirements}} are {{often found in}} small deliberative groups where these {{requirements are}} sometimes adopted {{in an attempt to}} increase protection of varied interests within the group. The requirements may be formally stated or may be unstated (for example, when an organization is described as having a [...] "consensus culture").|$|E
50|$|No {{internal}} structure. Because a p-box retains {{little information}} about any internal structure within the bounds, {{it does not}} elucidate which distributions within the p-box are most likely, nor whether the edges represent very unlikely or distinctly likely scenarios. This could complicate decisions in some cases if an edge of a p-box encloses a <b>decision</b> <b>threshold.</b>|$|E
40|$|If {{humans are}} faced with {{difficult}} choices when making decisions, the ability to slow down responses becomes critical {{in order to avoid}} suboptimal choices. Current models of decision making assume that the subthalamic nucleus (STN) mediates this function by elevating <b>decision</b> <b>thresholds,</b> thereby requiring more evidence to be accumulated before responding [1, 2, 3, 4, 5, 6, 7, 8 and 9]. However, direct electrophysiological evidence for the exact role of STN during adjustment of <b>decision</b> <b>thresholds</b> is lacking. Here, we show that trial-by-trial variations in STN low-frequency oscillatory activity predict adjustments of <b>decision</b> <b>thresholds</b> before subjects make a response. The relationship between STN activity and <b>decision</b> <b>thresholds</b> critically depends on the subjects’ level of cautiousness. While increased oscillatory activity of the STN predicts elevated <b>decision</b> <b>thresholds</b> during high levels of cautiousness, it predicts decreased <b>decision</b> <b>thresholds</b> during low levels of cautiousness. This context-dependent relationship may be mediated by increased influence of the medial prefrontal cortex (mPFC) -STN pathway on <b>decision</b> <b>thresholds</b> during high cautiousness. Subjects who exhibit a stronger increase in phase alignment of low-frequency oscillatory activity in mPFC and STN before making a response have higher <b>decision</b> <b>thresholds</b> and commit fewer erroneous responses. Together, our results demonstrate that STN low-frequency oscillatory activity and corresponding mPFC-STN coupling are involved in determining how much evidence subjects accumulate before making a decision. This finding might explain why deep-brain stimulation of the STN can impair subjects’ ability to slow down responses and can induce impulsive suboptimal decisions...|$|R
40|$|SummaryIf {{humans are}} faced with {{difficult}} choices when making decisions, the ability to slow down responses becomes critical {{in order to avoid}} suboptimal choices. Current models of decision making assume that the subthalamic nucleus (STN) mediates this function by elevating <b>decision</b> <b>thresholds,</b> thereby requiring more evidence to be accumulated before responding [1 – 9]. However, direct electrophysiological evidence for the exact role of STN during adjustment of <b>decision</b> <b>thresholds</b> is lacking. Here, we show that trial-by-trial variations in STN low-frequency oscillatory activity predict adjustments of <b>decision</b> <b>thresholds</b> before subjects make a response. The relationship between STN activity and <b>decision</b> <b>thresholds</b> critically depends on the subjects’ level of cautiousness. While increased oscillatory activity of the STN predicts elevated <b>decision</b> <b>thresholds</b> during high levels of cautiousness, it predicts decreased <b>decision</b> <b>thresholds</b> during low levels of cautiousness. This context-dependent relationship may be mediated by increased influence of the medial prefrontal cortex (mPFC) -STN pathway on <b>decision</b> <b>thresholds</b> during high cautiousness. Subjects who exhibit a stronger increase in phase alignment of low-frequency oscillatory activity in mPFC and STN before making a response have higher <b>decision</b> <b>thresholds</b> and commit fewer erroneous responses. Together, our results demonstrate that STN low-frequency oscillatory activity and corresponding mPFC-STN coupling are involved in determining how much evidence subjects accumulate before making a decision. This finding might explain why deep-brain stimulation of the STN can impair subjects’ ability to slow down responses and can induce impulsive suboptimal decisions...|$|R
40|$|Abstract. This paper {{presents}} {{equipment and}} procedures for on-card (in-situ) performance testing of biometric on-card comparison implemen-tations using pre-existing databases of biometric samples. A DTW-based on-line signature on-card comparison implementation serves as an exam-ple test object. The test results presented are false match rates and false non-match rates over a range of <b>decision</b> <b>thresholds</b> on a per-test-subject basis. The results reveal considerable differences in the comparison-score frequency distribution among test subjects, which necessitates the setting of user-dependent <b>decision</b> <b>thresholds</b> or comparison-score normalization...|$|R
50|$|For {{identification}} (one-to-many template matching) or verification (one-to-one template matching), {{a template}} created by imaging an iris {{is compared to}} stored template(s) in a database. If the Hamming distance is below the <b>decision</b> <b>threshold,</b> a positive identification has effectively been made because of the statistical extreme improbability that two different persons could agree by chance ("collide") in so many bits, given the high entropy of iris templates.|$|E
50|$|The drift-diffusion model (DDM) is a {{well defined}} model, that {{probably}} implements an optimum decision procedure for 2AFC. It is the continuous analog of a random walk model.The DDM assumes that in a 2AFC task, the subject is accumulating evidence for one or other of the alternatives at each time step, and integrating that evidence until a <b>decision</b> <b>threshold</b> is reached. As the sensory input which constitutes the evidence is noisy, the accumulation to the threshold is stochastic rather than deterministic - this {{gives rise to the}} directed random walk-like behavior.The DDM has been shown to describe accuracy and reaction times in human data for 2AFC tasks.|$|E
5000|$|According to this theory, {{recognition}} {{decisions are}} based on the strength of a memory trace in reference to a certain <b>decision</b> <b>threshold.</b> A memory that exceeds this threshold is perceived as old, and trace that does not exceed the threshold is perceived as new. According to this theory, remember and know responses are products of different degrees of memory strength. There are two criteria on a decision axis; a point low on the axis is associated with a know decision, and a point high on the axis is associated with a remember decision. If memory strength is high, individuals make a [...] "remember" [...] response, and if memory strength is low, individuals make a [...] "know" [...] response.|$|E
30|$|The Yogarajah’s method {{consists}} {{in analyzing the}} distribution of the error signal in a facial region to determine the <b>decision</b> <b>thresholds</b> from the obtained Gaussian parameters.|$|R
40|$|This paper {{presents}} {{equipment and}} procedures for on-card (in-situ) performance t esting of biometric on-card comparison implementations using pre-existing databa ses of biometric samples. A DTW-based on-line signature on-card comparison imple mentation serves as an example test object. The test results presented are false match rates and false non-match rates over a range of <b>decision</b> <b>thresholds</b> on a per-test-subject basis. The results reveal considerable differences in the comp arison-score frequency distribution among test subjects, which necessitates the setting of user-dependent <b>decision</b> <b>thresholds</b> or comparison-score normalization...|$|R
3000|$|... 2 q+ 2), where A and B (B≤ 0 ≤A) are the <b>decision</b> <b>thresholds</b> for {{the desired}} miss {{detection}} and false alarm probabilities, which correspond to H [...]...|$|R
50|$|Disagreements {{between the}} {{government}} and the FARC continued on the mechanism for ratification of a final agreement. In November, the government gave its support to a bill submitted by senator Roy Barreras (Partido de la U) organizing a plebiscite on a final agreement. In Havana, the FARC responded negatively to the idea of the plebiscite, insisting on a constituent assembly. With the support of the government's congressional majority, the bill regulating the plebiscite was adopted by Congress in December 2015. As per the statutory law regulating the plebiscite, approval requires support equivalent to 13% of the registered electorate for the winning option, a one-time exception to the existing law regulating plebiscites (Law 1757 of 2015) which has a turnout quorum of 50%. The reduction of the quorum, and the change from a turnout threshold to a <b>decision</b> <b>threshold,</b> was controversial. Additionally, in the plebiscite voters would vote on the final agreement as a whole rather than article-by-article, something which also created some criticisms, primarily from Uribe's Democratic Centre. Following its adoption by Congress, the law passed to the Constitutional Court for a mandatory revision.|$|E
3000|$|... where η is the <b>decision</b> <b>threshold.</b> For a given {{false alarm}} probability, {{determining}} the <b>decision</b> <b>threshold</b> {{needs to know}} the conditional probability density function p(ξ|H [...]...|$|E
40|$|Bit {{error rate}} (BER) {{monitoring}} {{is the ultimate}} goal of performance monitoring in all digital transmission systems as well as optical fiber transmission systems. To achieve this goal, optimization of the <b>decision</b> <b>threshold</b> must also be considered because BER is dependent on the level of <b>decision</b> <b>threshold.</b> In this paper, we analyze a pseudo-error counting scheme and propose an algorithm to achieve both BER monitoring and adaptive <b>decision</b> <b>threshold</b> optimization in optical fiber transmission systems. To verify the effectiveness of the proposed algorithm, we conduct computer simulations in both Gaussian and non-Gaussian distribution cases. According to the simulation results, BER and the optimum <b>decision</b> <b>threshold</b> can be estimated with the errors of 40 -Gb/s transmission systems...|$|E
40|$|It {{is known}} that {{sequential}} signal detection based on the sequential probability ratio test (SPRT) is the shortest on an average among other tests. Most {{of the work in}} sequential signal detection assumes independent, identically distributed noise. The <b>decision</b> <b>thresholds</b> are not exact. This is due to the complexity involved in doing an exact analysis of the sequential detector. In this paper, we propose a sequential sign detector for detecting constant signals in first order Markov noise. The distribution of the sufficient statistic is derived as a solution to the discrete diffusion equations. This leads to the derivation of the exact values of the <b>decision</b> <b>thresholds</b> and the average sample number...|$|R
3000|$|... d–f. This {{shows that}} for all capture devices, the marks {{labelled}} as “smeared”, “twisted lightly” and “strong twist” have markedly lower EV than the other distortion classes across all <b>decision</b> <b>thresholds</b> for EVA, {{in agreement with the}} ground truth.|$|R
40|$|This {{report is}} an {{exposition}} of <b>decision</b> lists and <b>threshold</b> <b>decision</b> lists. A {{version of this}} is to appear as a chapter in a book on Boolean functions, but the report itself is relatively self-contained. The key areas explored are the representation of Boolean functions by <b>decision</b> lists and <b>threshold</b> <b>decision</b> lists; properties of classes of decision list; and algorithmic questions associated with decision lists. ...|$|R
40|$|This thesis {{investigates the}} {{performance}} of free-space optical (FSO) communication systems in a turbulent atmosphere employing optical amplifiers (OAs) to extend transmission reach and wavelength-division multiplexing (WDM) to improve capacity. This system performance is considered {{in the presence of}} amplified spontaneous emission (ASE) noise, scintillation, beam spreading, atmospheric attenuation and interchannel crosstalk. In this work, the modulation scheme used is the on-off keying non-return-to-zero and the main performance metric employed is the average bit error rate (BER). Various performance evaluation methods are used to estimate system performance. Analysis of single link, cascaded OA and WDM FSO communication systems are given and the implications of using both adaptive (to channel state) and non-adaptive <b>decision</b> <b>threshold</b> schemes are analysed. The benefits of amplifier saturation, for example in the form of effective scintillation reduction when a non-adaptive <b>decision</b> <b>threshold</b> scheme is utilised at the receiver for different atmospheric turbulence regimes, are presented. Monte Carlo simulation techniques are used to model the probability distributions of system parameters such as the optical signal power, amplified spontaneous emission noise, optical signal to noise ratio and the average bit error rate due to scintillation. It is found that {{the performance of}} an adaptive <b>decision</b> <b>threshold</b> is superior to a non-adaptive <b>decision</b> <b>threshold</b> for both saturated and fixed gain preamplified receivers and the ability of a saturated gain OA to suppress scintillation is only meaningful for system performance when a non-adaptive <b>decision</b> <b>threshold</b> is used at the receiver. In a saturated gain preamplified system, the optimum non-adaptive <b>decision</b> <b>threshold</b> is investigated. An OA cascade can be successfully used to extend reach in FSO communication systems and specific system implementations are presented. The optimal cascade scheme with a non-adaptive receiver would use frequent low gain saturated amplification although this has a cost implication. Furthermore, a saturated gain amplified WDM FSO system with a non-adaptive <b>decision</b> <b>threshold</b> is superior to a non-amplified WDM FSO system with an adaptive <b>decision</b> <b>threshold...</b>|$|E
40|$|The {{performance}} of a free-space optical (FSO) communication system in a turbulent atmosphere employing an optical amplifier (OA) cascade to extend reach is investigated. Analysis of both single and cascaded OA FSO communication links is given {{and the implications of}} using both adaptive (to channel state) and non-adaptive <b>decision</b> <b>threshold</b> schemes are analysed. The benefits of amplifier saturation, for example in the form of effective scintillation reduction when a non-adaptive <b>decision</b> <b>threshold</b> scheme is utilised at the receiver for different atmospheric turbulence regimes, are presented. Monte Carlo simulation techniques are used to model the probability distributions of the optical signal power, noise and the average bit error rate due to scintillation for the cascade. The {{performance of}} an adaptive <b>decision</b> <b>threshold</b> is superior to a non-adaptive <b>decision</b> <b>threshold</b> for both saturated and fixed gain preamplified receivers and the ability of a saturated gain OA to suppress scintillation is only meaningful for system performance when a non-adaptive <b>decision</b> <b>threshold</b> is used at the receiver. An OA cascade can be successfully used to extend reach in FSO communication systems and specific system implementations are presented. The optimal cascade scheme with a non-adaptive receiver would use frequent low gain saturated amplification...|$|E
30|$|It {{should be}} {{also noted that}} p(FA) and p(miss) in Tables  6 and 7 do not relate to ATWV {{performance}} but to MTWV performance (i.e., with the a posteriori best <b>decision</b> <b>threshold).</b> In this way, systems with MTWV = 0.0 (i.e., {{those that do not}} generate detections at best <b>decision</b> <b>threshold)</b> obtain p(FA)= 0.0 and p(miss)= 1.0.|$|E
40|$|Abstract — This paper {{presents}} a dynamic {{particle swarm optimization}} based search for optimal fusion configuration of sensors in distributed detection network in presence of a nonstationary binary symmetric channel. The wireless channel in sensor networks is a non-stationary random process, which moves the optima of the original problem, otherwise static. The optimal fusion configuration minimizes the probability of error and involves optimal setting of the <b>decision</b> <b>thresholds</b> for the sensors and the optimal fusion rule used at the fusion center. The optimal <b>decision</b> <b>thresholding</b> {{of a group of}} sensors in a distributed detection network is a proven intractable problem. Previously a particle swarm optimizer has been used to solve this problem. In this paper a dynamic particle swarm optimizer is used to evolve the optimal <b>decision</b> <b>thresholds</b> of the group of sensors in presence of a non-stationary channel. Performance of the dynamic particle swarm optimizer is compared to the simple particle swarm optimizer in which the particle swarm optimization is restarted after detection of a change. The results demonstrate the effectiveness of the particle swarm in achieving the optima in highly dynamic environments. Dynamic adaptation in the particle swarms towards changes in fitness landscape makes particle swarm optimization an ideal algorithm for multi objective optimization in non-stationary sensor networks. I...|$|R
40|$|Diagnosis and {{treatment}} {{is a complex}} interaction of subjective information and impressions, objective data, individual needs and preferences, and resource constraints. The interpretation of quantitative information relies on thresholds or intervals (which are pairs of thresholds.) For example, “normal limits ” generally refers to the interval in which a parameter value for a general population falls. Above or below these limits is the region of abnormality. The quantitative question becomes: is the patient above or below a threshold? Prior to this question comes the query: what value of threshold should trigger a response? In this Brief Observation we explain the difficulty of setting <b>decision</b> <b>thresholds,</b> and we propose an approach. <b>Decision</b> <b>thresholds</b> are difficult to set because of diverse and complicated uncertainties. Thresholds are often based on clinical trials with populations which may not reliably reflect the population to which the individual patient belongs. Both patient and physician are sometimes quite uncertain about {{the relevance of the}} clinical trials to their specific case. Greenfield et al (1) note that randomized control trials, which underlie clinical guidelines and <b>decision</b> <b>thresholds,</b> typically enroll patients with less severe disease and exclude older patients, making the resulting thresholds of uncertain applicability to the excluded populations. Feinstein and Horwitz (2) warn against th...|$|R
30|$|The <b>decision</b> <b>thresholds</b> {{were on the}} {{conservative}} side because they were obtained by assuming that the sample estimates of the test statistics C_ 40 and C_ 42 have equal variances under different hypotheses, and ignored the effects of additive noise. The performance could be improved by taking these issues into account.|$|R
30|$|Properly {{choosing}} the <b>decision</b> <b>threshold</b> {{to mitigate the}} higher energy due to pilots.|$|E
3000|$|... [...]. In other words, the {{proposed}} simplified detection strategy is robust against local <b>decision</b> <b>threshold</b> mismatches.|$|E
30|$|It {{is worth}} {{mentioning}} that whether the theoretical lower bound {{mentioned in the}} Appendix given with (23) exists. As Proposition 1 implies, lower bound could be reached in case {{the behavior of the}} decision device could be characterized statistically, since (23) {{is a function of the}} <b>decision</b> <b>threshold,</b> ζ. For the energy detector, it is known that optimal <b>decision</b> <b>threshold</b> theoretically exists as expressed in ([44] § III.B).|$|E
40|$|The aim of {{this study}} was to provide a {{framework}} to evaluate bibliometric indicators as decision support tools from a decision making perspective and to examine the information value of early career publication rate as a predictor of future productivity. We used ROC analysis to evaluate a bibliometric indicator as a tool for binary decision making. The dataset consisted of 451 early career researchers in the mathematical sub-field of number theory. We investigated the effect of three different definitions of top performance groups—top 10, top 25, and top 50  %; the consequences of using different thresholds in the prediction models; and the added prediction value of information on early career research collaboration and publications in prestige journals. We conclude that early career performance productivity has an information value in all tested decision scenarios, but future performance is more predictable if the definition of a high performance group is more exclusive. Estimated optimal <b>decision</b> <b>thresholds</b> using the Youden index indicated that the top 10  % decision scenario should use 7 articles, the top 25  % scenario should use 7 articles, and the top 50  % should use 5 articles to minimize prediction errors. A comparative analysis between the <b>decision</b> <b>thresholds</b> provided by the Youden index which take consequences into consideration and a method commonly used in evaluative bibliometrics which do not take consequences into consideration when determining <b>decision</b> <b>thresholds,</b> indicated that differences are trivial for the top 25 and the 50  % groups. However, a statistically significant difference between the methods was found for the top 10  % group. Information on early career collaboration and publication strategies did not add any prediction value to the bibliometric indicator publication rate in any of the models. The key contributions of this research is the focus on consequences in terms of prediction errors and the notion of transforming uncertainty into risk when we are choosing <b>decision</b> <b>thresholds</b> in bibliometricly informed decision making. The significance of our results are discussed from the point of view of a science policy and management...|$|R
3000|$|... are 1.1 {{times larger}} than the local minimum, a {{temporal}} segmentation point is declared to occur at the minimum location. Since the decision rule is not based on absolute values and thresholds, rather on relative values of extrema, it is more robust to data variation (like type of dance) and no empirically derived <b>decision</b> <b>thresholds</b> are used.|$|R
40|$|Performance {{status of}} the Adaptive Rain Fade Compensation includes: (1) The rain fade {{protocol}} is functional detecting fades, providing an additional 10 dB of margin and seamless transitions to and from coded operation; (2) The stabilization of the link margins and the optimization of rain fade <b>decision</b> <b>thresholds</b> has resulted in improved BER performance; (3) Characterization of the fade compensation algorithm is ongoing...|$|R
3000|$|... [...]), {{we assume}} that each sensor makes use of an actual <b>decision</b> <b>threshold</b> which is uniformly {{distributed}} in [...]...|$|E
3000|$|... and the <b>decision</b> <b>threshold</b> τ {{are adopted}} in the simulations, respectively. The user number is set to be 8.|$|E
3000|$|... where L is the <b>decision</b> <b>threshold,</b> {{chosen as}} {{to achieve a}} given {{operating}} point on the sensor's ROC curve.|$|E
40|$|The {{issue of}} a priori {{threshold}} setting in speaker verification is a key problem for field applications. In {{the context of the}} CAVE project, we compared several methods for estimating speaker-independent and speaker-dependent <b>decision</b> <b>thresholds.</b> Relevant parameters are estimated from development data only, i. e. without resorting to additional client data. The various approaches are tested on the Dutch SESP database...|$|R
40|$|A new {{approach}} to adaptive threshold selection for classification of peaks of audio spectra is presented. We here extend the previous work on classification of sinusoidal and noise peaks based {{on a set of}} spectral peak descriptors in a twofold way: on one hand we propose a compact sinusoidal model where all the modulation parameters are defined with respect to the analysis window. This fact is of great importance as we recall that the STFT spectra are closely related to the analysis window properties. On the other hand, we design a threshold selection algorithm that allows us to control the <b>decision</b> <b>thresholds</b> in an intuitive manner. The <b>decision</b> <b>thresholds</b> calculated from the relationships established between the noise power in the signal and the distributions of sinusoidal peaks assures that all peaks described as sinusoidal will be correctly classified. We also show that the threshold selection algorithm can be used for different types of analysis windows with only a slight parameter readjustment. 1...|$|R
40|$|Optimal {{decision-making}} requires balancing fast but error-prone {{and more}} accurate but slower decisions through adjustments of <b>decision</b> <b>thresholds.</b> Here, we demonstrate two distinct correlates of such speed-accuracy adjustments by recording subthalamic nucleus (STN) activity and electroencephalography in 11 Parkinson's disease patients during a perceptual decision-making task; STN low-frequency oscillatory (LFO) activity (2 - 8 Hz), coupled to activity at prefrontal electrode Fz, and STN beta activity (13 - 30 Hz) coupled to electrodes C 3 /C 4 close to motor cortex. These two correlates differed {{not only in}} their cortical topography and spectral characteristics {{but also in the}} relative timing of recruitment and in their precise relationship with <b>decision</b> <b>thresholds.</b> Increases of STN LFO power preceding the response predicted increased thresholds only after accuracy instructions, while cue-induced reductions of STN beta power decreased thresholds irrespective of instructions. These findings indicate that distinct neural mechanisms determine whether a decision will be made in haste or with caution...|$|R
