121|581|Public
5000|$|A {{source node}} has only {{outgoing}} branches (represents an independent variable). As a special case, an <b>input</b> <b>node</b> {{is characterized by}} having one or more attached arrows pointing away from the node and no arrows pointing into the node. Any open, complete SFG will {{have at least one}} <b>input</b> <b>node.</b>|$|E
5000|$|Take {{the case}} of an {{inverting}} operational amplifier configuration. If the resistor between the single output node and the inverting <b>input</b> <b>node</b> is [...] and the resistor between a source voltage and the inverting <b>input</b> <b>node</b> is , then the ideal gain for such a circuit at the output terminal is defined, ideally, to be: ...|$|E
5000|$|... : Atomically gets {{data from}} its <b>input</b> <b>node</b> and propagates {{it to its}} output node.|$|E
3000|$|... e. The figure {{shows that}} with 55 <b>input</b> <b>nodes</b> and 10 hidden nodes, the {{performance}} is improved {{compared with the}} experiment of 220 <b>input</b> <b>nodes.</b>|$|R
5000|$|Extension {{neural network}} has a neural network like appearance. Weight vector resides between the <b>input</b> <b>nodes</b> and output nodes. Output nodes are the {{representation}} of <b>input</b> <b>nodes</b> by passing them through the weight vector.|$|R
50|$|In the {{training}} stages, {{the probability that}} a hidden node will be dropped is usually 0.5; for <b>input</b> <b>nodes,</b> this should be much lower, intuitively because information is directly lost when <b>input</b> <b>nodes</b> are ignored.|$|R
5000|$|Exhibit (d) [...] is an <b>input</b> <b>node.</b> In this case, [...] is {{multiplied by}} the gain [...]|$|E
5000|$|Forward path. A {{path from}} an <b>input</b> <b>node</b> (source) to an output node (sink) {{that does not}} re-visit any node.|$|E
50|$|Forward path: A {{path from}} an <b>input</b> <b>node</b> to an output node {{in which no}} node is touched more than once.|$|E
40|$|In {{this paper}} we propose an {{approach}} to variable selection that uses a neural-network model as the tool to determine which variables are to be discarded. The method performs a backward selection by successively removing <b>input</b> <b>nodes</b> in a network trained with the complete set of variables as <b>inputs.</b> <b>Input</b> <b>nodes</b> are removed, along with their connections, and remaining weights are adjusted {{in such a way}} that the overall input-output behavior learnt by the network is kept approximately unchanged. A simple criterion to select <b>input</b> <b>nodes</b> to be removed is developed. The proposed method is tested on a famous example of system identification. Experimental results show that the removal of <b>input</b> <b>nodes</b> from the neural network model improves its generalization ability. In addition, the method compares favorably with respect to other feature reduction methods...|$|R
5000|$|... : Atomically gets {{data from}} both its <b>input</b> <b>nodes</b> and loses it.|$|R
5000|$|Step 3 {{improves}} {{upon the}} <b>input</b> <b>nodes</b> [...] and their errors [...] as follows.|$|R
5000|$|The sum of {{currents}} at the <b>input</b> <b>node</b> {{implies that}} [...] Substituting for [...] from (3) leads to [...] or [...]|$|E
5000|$|... : Atomically gets {{data from}} its <b>input</b> <b>node</b> and propagates {{it to its}} output node if the filter {{condition}} [...] is satisfied; loses the data otherwise.|$|E
5000|$|... : Gets {{data from}} its <b>input</b> <b>node,</b> {{temporarily}} stores {{it in an}} internal buffer of size , and propagates it to its output node (whenever this output node is ready to take data).|$|E
40|$|Designing {{neural network}} (NN) to predict time series {{is not a}} trivial task. Some kind of science and art {{required}} to perform this task. One {{of the most important}} factors in building NN for time series prediction is the number of <b>input</b> <b>nodes</b> (Lags) [3]. Usually; the number of <b>input</b> <b>nodes</b> is chosen by the method "garbage in – garbag...|$|R
40|$|Performance of RBF network {{depends on}} the choice of basis functions, <b>input</b> <b>nodes,</b> hidden nodes and so on. Hence, the study of these {{components}} is important for selecting a good network structure. This paper investigates the properties of RBF network in relation to system identification. Network properties such as network expansion, choice of basis function, <b>input</b> <b>nodes</b> assignment, underfitting and overfitting were investigated. 1...|$|R
5000|$|In the CC4 network, {{which is}} a three-stage network, the number of <b>input</b> <b>nodes</b> is one more than {{the size of the}} {{training}} vector, with the extra node serving as the biasing <b>node</b> whose <b>input</b> is always 1. For binary input vectors, the weights from the <b>input</b> <b>nodes</b> to the hidden neuron (say of index j) corresponding to the trained vector is given by the following formula: ...|$|R
50|$|Once sizes {{have been}} chosen, the logical {{effort of the}} output of the gate is the sum of the widths of all {{transistors}} whose source or drain is in contact with the output node. The logical effort of each input to the gate is the sum of the widths of all transistors whose gate is in contact with that <b>input</b> <b>node.</b>|$|E
50|$|If {{the upper}} FET stage were {{operated}} alone using its source as <b>input</b> <b>node</b> (that is, common-gate (CG) configuration), {{it would have}} a good voltage gain and wide bandwidth. However, its low input impedance would limit its usefulness to very low-impedance voltage drivers. Adding the lower FET results in a high input impedance, allowing the cascode stage to be driven by a high-impedance source.|$|E
5000|$|Figure 3 shows a MOSFET common-source {{amplifier}} with {{an active}} load. Figure 4 shows the corresponding small-signal circuit when a load resistor RL is added at the output node and a Thévenin driver of applied voltage VA and series resistance RA is added at the <b>input</b> <b>node.</b> The limitation on bandwidth in this circuit {{stems from the}} coupling of parasitic transistor capacitance Cgd between gate and drain and the series resistance of the source RA. (There are other parasitic capacitances, but they are neglected here as they have only a secondary effect on bandwidth.) ...|$|E
50|$|In {{a single}} stage {{interconnect}} network, the <b>input</b> <b>nodes</b> {{are connected to}} output via a single stage of switches.|$|R
3000|$|... are the ‘child’ {{variables}} of the <b>input</b> <b>nodes.</b> The respective roots and ranks {{can be determined}} from the roots and ranks from S.|$|R
3000|$|... respectively, {{represent}} the weights connecting the output layer and the hidden layer and the weights connecting the hidden layer and the <b>input</b> <b>nodes.</b>|$|R
50|$|Referring to the {{schematic}} shown, {{the section}} marked in red forms the input stage and error amplifier. The inverting <b>input</b> (<b>node</b> where emitters of Q1 & Q2 are connected) is low-impedance and hence sensitive {{to changes in}} current. Resistors R1 - R4 set up the quiescent bias conditions and are chosen such that the collector currents of Q1 & Q2 are the same. In most designs, active biasing circuitry is used instead of passive resistive biasing, and the non-inverting input may also be modified to become low impedance like the inverting input in order to minimise offsets.|$|E
5000|$|A Boolean circuit with [...] input bits is a {{directed}} acyclic graph {{in which}} every node (usually called gates in this context) is either an <b>input</b> <b>node</b> of in-degree 0 labeled {{by one of the}} [...] input bits, an AND gate, an OR gate, or a NOT gate. One of these gates is designated as the output gate. Such a circuit naturally computes a function of its [...] inputs. The size of a circuit is the number of gates it contains and its depth is the maximal length of a path from an input gate to the output gate.|$|E
5000|$|The {{input voltage}} of the Wilson current mirror is [...] The <b>input</b> <b>node</b> {{is a low}} {{impedance}} node so its voltage remains approximately constant during operation at [...] volts. The equivalent voltage for the standard two-transistor mirror is only one base-emitter drop, , or half that of the Wilson mirror. The headroom (the potential difference between the opposite power rail and the input of the mirror) available to the circuitry that generates the input current to the mirror is the difference of the power supply voltage and the mirror input voltage. The higher input voltage and higher minimum output voltage of the Wilson current mirror configuration may become problematic for circuits with low supply voltages, particularly supply voltages less than three volts as are sometimes found in battery powered devices.|$|E
40|$|Understanding {{structural}} controllability of {{a complex}} network requires to identify a Minimum <b>Input</b> <b>nodes</b> Set (MIS) of the network. It {{has been suggested that}} finding an MIS is equivalent to computing a maximum matching of the network, where the unmatched nodes constitute an MIS. However, maximum matching of a network is often not unique, and finding all MISs may provide deep insights to the controllability of the network. Finding all possible <b>input</b> <b>nodes,</b> which form the union of all MISs, is computationally challenging for large networks. Here we present an efficient enumerative algorithm for the problem. The main idea is to modify a maximum matching algorithm to make it efficient for finding all possible <b>input</b> <b>nodes</b> by computing only one MIS. We rigorously proved the correctness of the new algorithm and evaluated its performance on synthetic and large real networks. The experimental results showed that the new algorithm ran several orders of magnitude faster than the existing method on large real networks...|$|R
40|$|A common {{approach}} to controlling complex networks is to directly control {{a subset of}} <b>input</b> <b>nodes,</b> which then controls the remaining nodes via network interactions. While techniques have been proposed for selecting <b>input</b> <b>nodes</b> based on either performance metrics or controllability, a unifying approach based on joint consideration of performance and controllability is an open problem. In this paper, we develop a submodular optimization framework for selecting <b>input</b> <b>nodes</b> based on joint performance and controllability in structured linear descriptor systems. We develop our framework for arbitrary linear descriptor systems. In developing our framework, we first prove that selecting a minimum-size set of <b>input</b> <b>nodes</b> for controllability is a matroid intersection problem that can be solved in polynomial-time in the network size. We then prove that input selection to maximize a performance metric with controllability as a constraint is equivalent to maximizing a monotone submodular function with two matroid basis constraints, and derive efficient approximation algorithms with provable optimality bounds for input selection. Finally, we present a graph controllability index metric, which characterizes the largest controllable subgraph of a given complex network, and prove its submodular structure, leading to input selection algorithms that trade-off performance and controllability. We provide improved optimality guarantees for known systems such as strongly connected networks, consensus networks, networks of double integrators, and networks where all system parameters (e. g., link weights) are chosen independently and at random...|$|R
5000|$|... {{such that}} , for all [...]If [...] is a channel, then [...] {{is called the}} set of <b>input</b> <b>nodes</b> of [...] and [...] is called the set of output nodes of [...]|$|R
50|$|A SOA {{developer}} defines message {{flows in}} the IBM Integration Toolkit by including {{a number of}} message flow nodes, each of which represents a set of actions that define a processing step. The {{way in which the}} message flow nodes are joined together determine which processing steps are carried out, in which order, and under which conditions. A message flow includes an <b>input</b> <b>node</b> that provides the source of the messages that are processed, which can be processed in one or more ways, and optionally deliver it through one or more output nodes. The message is received as a bit stream, without representational structure or format, and is converted by a parser into a tree structure that is used internally in the message flow. Before the message is delivered to a final destination, it is converted back into a bit stream.|$|E
5000|$|The Wilson {{current mirror}} {{achieves}} the high output impedance of equation (6) by negative feedback {{rather than by}} emitter degeneration as cascoded mirrors or sources with resistor degeneration do. The node impedance of the only internal node of the mirror, the node at the emitter of Q3 and the collector of Q2, is quite low. [...] At low frequency, that impedance is given by [...] For a device biased at 1 mA having a current gain of 100, this evaluates to 0.26 ohms at 25 deg. C. Any change in output current with output voltage results in {{a change in the}} emitter current of Q3 but very little change in the emitter node voltage. The change in [...] is fedback through Q2 and Q1 to the <b>input</b> <b>node</b> where it changes the base current of Q3 in a way that reduces the net change in output current, thus closing the feedback loop.|$|E
5000|$|Current mirrors are {{frequently}} {{used in the}} signal path of an integrated circuit, for example, for differential to single-ended signal conversion within an operational amplifier. At low bias currents, the impedances in the circuit are high enough {{that the effect of}} frequency may be dominated by device and parasitic capacitances shunting the input and output nodes to ground, lowering the input and output impedances. The collector-base capacitance, , of Q3 is one component of that capacitive load. The collector of Q3 is the output node of the mirror and its base is the <b>input</b> <b>node.</b> When any current flows in , that current becomes an input to the mirror and the current is doubled at the output. Effectively the contribution from Q3 to the total output capacitance is [...] If the output of the Wilson mirror is connected to a relatively high impedance node, the voltage gain of the mirror may be high. In that case the input impedance of the mirror may be affected by the Miller Effect because of , although the low input impedance of the mirror mitigates this effect.|$|E
40|$|In {{this paper}} a network will be {{considered}} an acyclic graph. It has several <b>input</b> <b>nodes</b> (<b>inputs)</b> and some (at least one) output nodes (outputs). The nodes of the network are characterized by fan-in (the number of incoming edges) and fan-out (the number of outgoing edges), while the network has a certain size (the numbe...|$|R
30|$|Different {{types and}} numbers of the <b>input</b> <b>nodes</b> lead to {{differentiated}} outcomes. As mentioned above, the effective parameters selected by the traditional/modified GMDH network fed into the BP network for analysis and prediction.|$|R
50|$|On each arc {{there is}} a {{statistical}} weight. Using back propagation the neural network learns the necessary pattern to recognize the prediction. It is trained by repeatedly exposing it to examples {{of the problem and}} learning the significance (weights) of the <b>input</b> <b>nodes.</b>|$|R
