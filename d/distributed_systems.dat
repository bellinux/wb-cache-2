10000|10000|Public
5|$|Inspired by the curve-shortening flow on smooth curves, {{researchers}} have studied methods for flowing polygons {{so that they}} stay polygonal, with applications including pattern formation and synchronization in <b>distributed</b> <b>systems</b> of robots. Length-preserving polygonal flows {{can be used to}} solve the carpenter's rule problem.|$|E
25|$|<b>Distributed</b> <b>systems</b> are {{groups of}} {{networked}} computers, {{which have the}} same goal for their work.|$|E
25|$|In {{the early}} 1980s Dijkstra and Carel S. Scholten {{proposed}} the Dijkstra–Scholten algorithm for detecting termination in <b>distributed</b> <b>systems.</b>|$|E
25|$|So far {{the focus}} has been on {{designing}} a <b>distributed</b> <b>system</b> that solves a given problem. A complementary research problem is studying the properties of a given <b>distributed</b> <b>system.</b>|$|R
40|$|This work {{investigates the}} amount of {{information}} about failures required to simulate a synchronous <b>distributed</b> <b>system</b> by an asynchronous <b>distributed</b> <b>system</b> prone to crash-recovery failures. A failure detection sequencer SigmaCR for the crash-recovery failure model is defined, which outputs information about crashes and recoveries and {{about the state of the}} crashed or recovered processes. Using the simulation technique of a synchronizer, it is shown that in general it is impossible to implement a synchronizer in an asynchronous <b>distributed</b> <b>system</b> with an arbitrary number of concurrent crash-recovery faults. It is shown that a synchronizer is implementable given SigmaCR and an asynchronous <b>distributed</b> <b>system</b> with at least one correct process. Furthermore, it is proven that SigmaCR can be emulated in a synchronous <b>distributed</b> <b>system</b> and hence can be regarded as the weakest failure detection device suitable to implement a synchronizer in the crash-recovery failure model...|$|R
40|$|This paper {{designed}} a fault tolerance for soft real time <b>distributed</b> <b>system</b> (FTRTDS). This {{system is designed}} to be independently on specific mechanisms and facilities of the underlying real time <b>distributed</b> <b>system.</b> It {{is designed to be}} distributed on all the computers in the <b>distributed</b> <b>system</b> and controlled by a central unit. Besides gathering information about a target program spontaneously, it provides information about the target operating system and the target hardware in order to diagnose the fault before occurring, so it can handle the situation before it comes on. And it provides a <b>distributed</b> <b>system</b> with the reactive capability of reconfiguring and reinitializing after the occurrence of a failure...|$|R
25|$|Shared-memory {{programs}} can be extended to <b>distributed</b> <b>systems</b> if the underlying operating system encapsulates the communication between nodes and virtually unifies the memory across all individual systems.|$|E
25|$|Active in {{the field}} of Artificial Neural Networks since 1989. Current {{research}} programmes within the group are focused on the improvement of man-machine-interfaces, robot-force-control, eye-tracking experiments, machine vision, virtual reality and <b>distributed</b> <b>systems.</b>|$|E
25|$|Finally, DHTs {{must deal}} with more {{traditional}} <b>distributed</b> <b>systems</b> issues such as load balancing, data integrity, and performance (in particular, ensuring that operations such as routing and data storage or retrieval complete quickly).|$|E
50|$|A quorum is {{the minimum}} number of votes that a {{distributed}} transaction has to obtain {{in order to be}} allowed to perform an operation in a <b>distributed</b> <b>system.</b> A quorum-based technique is implemented to enforce consistent operation in a <b>distributed</b> <b>system.</b>|$|R
40|$|<b>Distributed</b> <b>System</b> {{design is}} a highly {{complicated}} and non-trivial task. The problem {{is characterized by the}} need to design multi-threaded, multi-processor, and multi-media systems. Design frameworks such as Open Distributed Processing (ODP), the ITU/ISO standard, provide significant, theoretical, guidance and aids towards the provision of successful <b>distributed</b> <b>system</b> designs. The purpose of the DSE 4 DS project is to develop these theoretical aids into practical design support that will assist designers in the construction of <b>distributed</b> <b>system</b> specifications...|$|R
40|$|Approximate {{controllability}} {{problem for}} a linear <b>distributed</b> control <b>system</b> with possibly unbounded input operator, connected in a series to another <b>distributed</b> <b>system</b> without control is investigated. An initial state of the second <b>distributed</b> <b>system</b> is considered as a control parameter. Applications to control partial equations governed by hyperbolic controller, and to control delay systems governed by hereditary controller are considered...|$|R
25|$|The use of {{concurrent}} processes that communicate by message-passing {{has its roots}} in operating system architectures studied in the 1960s. The first widespread <b>distributed</b> <b>systems</b> were local-area networks such as Ethernet, which was invented in the 1970s.|$|E
25|$|Distributed {{computing}} also {{refers to}} the use of <b>distributed</b> <b>systems</b> to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers, which communicate with each other by message passing.|$|E
25|$|In {{order to}} perform coordination, <b>distributed</b> <b>systems</b> employ {{the concept of}} {{coordinator}}s. The coordinator election problem is to choose a process from {{among a group of}} processes on different processors in a distributed system to act as the central coordinator. Several central coordinator election algorithms exist.|$|E
40|$|In this paper, {{we present}} a {{distributed}} algorithm for dynamic data replication of an object in a <b>distributed</b> <b>system.</b> The algorithm changes the replication scheme, i. e., number of replicas and their location in the <b>distributed</b> <b>system,</b> to optimize the amount of communication. In other words, the algorithm dynamically adapts the replication scheme of an object to the pattern of read-write requests in the <b>distributed</b> <b>system.</b> We prove {{that the cost of}} the algorithm is within a constant factor of the lower bound. 1. Introduction 1. 1 Motivation The replication scheme of a <b>distributed</b> <b>system</b> determines how many replicas of each object are created, and to which processors these replicas are allocated. This scheme critically affects the performance of a <b>distributed</b> <b>system,</b> since reading an object locally is less costly than reading it from a remote site, Therefore in a read-intensive network a widely distributed replication is mandated. On the other hand, an update of an object is usually wri [...] ...|$|R
40|$|Approved {{for public}} release; {{distribution}} in unlimited. The idea of deploying a <b>distributed</b> network intrusion <b>system</b> using Therminator is explored in this thesis. There are many advantages {{in having a}} <b>distributed</b> <b>system</b> compared to a standalone network intrusion system. The underlying principle of Therminator is modeling network traffic on conversation exchange models. Using Zippo, a new implementation of Therminator, the experimental setup consisted of multiple sensors reporting individual findings to a central server for aggregated analysis. Different scenarios of network attacks and intrusions were planned to investigate {{the effectiveness of the}} <b>distributed</b> <b>system.</b> The network attacks were taken from the M. I. T Lincoln Lab 1999 Data Sets. The <b>distributed</b> <b>system</b> was subjected to different combinations of network attacks {{in various parts of the}} network. The results were then analyzed to understand the behavior of the <b>distributed</b> <b>system</b> in response to the different attacks. In general, the <b>distributed</b> <b>system</b> detected all attacks under each scenario. Some surprising observations also indicated attack responses occurring in unanticipated scenarios. These results are subject to further investigation. Defence Science & Technology Agency Singapor...|$|R
40|$|The Environmental IoT is {{a project}} where we {{investigate}} the potential of an integrated <b>distributed</b> <b>system</b> consisting of an Internet of Things (IoT) and a Cloud Computing infrastructure. The resulting complex <b>distributed</b> <b>system</b> {{will be used to}} support deep understanding of the natural environment inter-dependencies and the management of the natural environment through appropriate interventions. In this paper, we discuss our approach to program this resulting complex <b>distributed</b> <b>system</b> with high-level system specifications in the environmental science context. The high-level specification encapsulates environmental science concepts and conveys the system's overall goals. The approach consists of three refinement steps that translate the high-level specification into the accordingly behaviour on the resulting <b>distributed</b> <b>system.</b> This process captures the abstract requirements of scientists and supports runtime adaptation...|$|R
25|$|Hydrogen piping can {{in theory}} be avoided in <b>distributed</b> <b>systems</b> of {{hydrogen}} production, where hydrogen is routinely made on site using medium or small-sized generators which would produce enough hydrogen {{for personal use}} or perhaps a neighborhood. In the end, a combination of options for hydrogen gas distribution may succeed.|$|E
25|$|In August 2009, BMC {{had over}} 450 {{software}} applications to primarily manage mainframes and <b>distributed</b> <b>systems,</b> {{as well as}} virtual and cloud computing IT environments. Generally, the software is used to help information technology managers, typically in large enterprises or agencies, manage operations, make IT more efficient, remediate issues, increase compliance and lower IT costs.|$|E
25|$|The {{situation}} is {{further complicated by}} the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and <b>distributed</b> <b>systems</b> (see below for more detailed discussion). Nevertheless, {{as a rule of}} thumb, high-performance parallel computation in a shared-memory multiprocessor uses parallel algorithms while the coordination of a large-scale distributed system uses distributed algorithms.|$|E
5000|$|<b>Distributed</b> <b>system</b> {{architectures}} for timeliness and dependability QoS ...|$|R
25|$|There {{are many}} {{cases in which}} the use of a single {{computer}} would be possible in principle, but the use of a <b>distributed</b> <b>system</b> is beneficial for practical reasons. For example, it may be more cost-efficient to obtain the desired level of performance by using a cluster of several low-end computers, in comparison with a single high-end computer. A <b>distributed</b> <b>system</b> can provide more reliability than a non-distributed system, as there is no single point of failure. Moreover, a <b>distributed</b> <b>system</b> may be easier to expand and manage than a monolithic uniprocessor system.|$|R
40|$|We {{outline the}} formal {{modelling}} of a software system {{to support the}} scheduling and rescheduling of trains. The current (prototype) system supports only centralized rescheduling, but in practice rescheduling is done on an area basis, and a <b>distributed</b> <b>system</b> is required. Developing a <b>distributed</b> <b>system</b> involves the notions of delegability of functions to adjust schedules and distributability of functions to analyse them for conformance to regulations. We formalize these notions {{in terms of a}} more abstract, generic specification and then instantiate this to generate a specification of the <b>distributed</b> <b>system...</b>|$|R
25|$|Moreover, a {{parallel}} algorithm {{can be implemented}} either in {{a parallel}} system (using shared memory) or in a distributed system (using message passing). The traditional boundary between parallel and distributed algorithms (choose a suitable network vs. run in any given network) does not lie {{in the same place}} as the boundary between parallel and <b>distributed</b> <b>systems</b> (shared memory vs. message passing).|$|E
25|$|In quasi-opportunistic {{supercomputing}} a {{large number}} of geographically disperse computers are orchestrated with built-in safeguards. The quasi-opportunistic approach goes beyond volunteer computing on a highly <b>distributed</b> <b>systems</b> such as BOINC, or general grid computing on a system such as Globus by allowing the middleware to provide almost seamless access to many computing clusters so that existing programs in languages such as Fortran or C can be distributed among multiple computing resources.|$|E
25|$|Bader {{serves on}} the Steering Committees of the International Parallel and Distributed Processing Symposium (IPDPS) and HiPC conferences, and was the General co-Chair for IPDPS in 2004—2005, and Vice General Chair for HiPC in 2002—2004. David has {{previously}} chaired several conference program committees, was the Program Chair for HiPC 2005, and a Program Vice-Chair for IPDPS 2006. Bader was the General Chair of the 24th IPDPS, was held on April 19–23, 2010 in Atlanta, Georgia. He served as the Editor-in-Chief of the IEEE Transactions on Parallel and <b>Distributed</b> <b>Systems</b> (TPDS), from 2013-2017 and serves as an Associate Editor-in-Chief of the Journal of Parallel and Distributed Computing (JPDC). Bader has been {{an associate editor of}} several journals, including IEEE Transactions on Parallel and <b>Distributed</b> <b>Systems,</b> IEEE DSOnline, Parallel Computing, and the ACM Journal of Experimental Algorithmics, and has published over 210 articles in peer reviewed journals and conferences. From July 2003 to June 2007, Bader was also the chair of the IEEE Computer Society's Technical Committee on Parallel Processing.|$|E
5000|$|There {{are many}} {{cases in which}} the use of a single {{computer}} would be possible in principle, but the use of a <b>distributed</b> <b>system</b> is beneficial for practical reasons. For example, it may be more cost-efficient to obtain the desired level of performance by using a cluster of several low-end computers, in comparison with a single high-end computer. A <b>distributed</b> <b>system</b> can provide more reliability than a non-distributed system, as there is no single point of failure. Moreover, a <b>distributed</b> <b>system</b> may be easier to expand and manage than a monolithic uniprocessor system.|$|R
40|$|Abstract—A <b>distributed</b> <b>system</b> is a {{collection}} of pro-cessor-memory pairs connected by communication links. The reliability of a <b>distributed</b> <b>system</b> can be expressed using the distributed program reliability, and <b>distributed</b> <b>system</b> reliability analysis. The computing reliability of a <b>distributed</b> <b>system</b> is an NP-hard problem. The distribution of programs & data-files can affect the system reliability. The reliability-oriented task assign-ment problem, which is NP-hard, is to find a task distribution such that the program reliability or system reliability is maximized. For example, efficient allocation of channels to the different cells can greatly improve the overall network throughput, {{in terms of the number}} of calls successfully supported. This paper presents a genetic algorithm-based reliability-oriented task assignment methodology (GAROTA) for computing the-DTA reliability problem. The proposed algorithm uses a genetic algorithm t...|$|R
5000|$|... #Subtitle level 2: Types of {{transparency}} in <b>distributed</b> <b>system</b> ...|$|R
25|$|The School {{is one of}} {{a handful}} that offer degrees in Human-Computer Interaction. The School offers master's degrees in Human-Computer Interaction Design, Music Informatics, Bioinformatics, Chemical Informatics, Security Informatics, and Computer Science, and Ph.D. degrees in Computer Science and in Informatics. Specialization areas for the Ph.D. in Computer Science include {{artificial}} intelligence, databases, <b>distributed</b> <b>systems,</b> formal methods, high-performance computing, programming languages, and security. The Informatics Ph.D. program offers tracks in bioinformatics, cheminformatics, complex systems, human-computer interaction design, logic and mathematical foundations of informatics, music informatics, security informatics, and social informatics.|$|E
25|$|In November 2006, Bader was {{selected}} by Sony, Toshiba, and IBM, to direct the first Center of Competence for the Cell Processor. Bader also serves on the Internet2 Research Advisory Council. Bader was elected as an IEEE Fellow in 2009. Since 2011, {{he has been working}} with the Georgia Tech Research Institute on the Proactive Discovery of Insider Threats Using Graph Analysis and Learning project. Dr. Bader also plays leadership roles in: Computing Research Association (CRA) Board, NSF Advisory Committee on Cyberinfrastructure, Council on Competitiveness High Performance Computing Advisory Committee, IEEE Computer Society Board of Governors and Editor-in-Chief, IEEE Transactions on Parallel and <b>Distributed</b> <b>Systems.</b>|$|E
500|$|Directional {{couplers}} {{and power}} dividers have many applications. [...] These include providing a signal sample for measurement or monitoring, feedback, combining feeds {{to and from}} antennae, antenna beam forming, providing taps for cable <b>distributed</b> <b>systems</b> such as cable TV, and separating transmitted and received signals on telephone lines.|$|E
5000|$|Hopsan - <b>Distributed</b> <b>system</b> {{simulation}} tool {{using the}} TLM method ...|$|R
5000|$|The entire <b>distributed</b> <b>system</b> {{guarantees}} (<b>distributed</b> CO and) serializability, and ...|$|R
40|$|Formal {{verification}} {{means to}} rigorously explore the correctness of system designs expressed as mathematical models, most likely {{with the assistance}} of modern computers. Original approaches were to model and express a <b>distributed</b> <b>system</b> using existing theoretical tools such as Petri Nets. Nevertheless the main problems of such approaches are the restrictions imposed by formal tools and the human factor of simplify and model a <b>distributed</b> <b>system.</b> We propose a way to do formal verification of a <b>distributed</b> <b>system</b> by modeling the communication of the system as a concurrent program, instantiating the <b>distributed</b> <b>system</b> using threads and atomic queues and testing/verifying directly to the source code with specialized verifiers for concurrent programs. As an example, we show the verification of a distributed threshold signer using CBMC verifying properties such as memory leaks, index out of bounds, and data races...|$|R
