201|383|Public
2500|$|Consider an {{implementation}} of the Game of Life. A domain expert (or business analyst) might want to specify what should happen when someone is setting up a starting configuration of the game grid. To do this, {{he might want to}} give an example of a number of steps taken by a person who is toggling cells. Skipping over the narrative part, he might do this by writing up the following scenario into a plain text document (which is the type of <b>input</b> <b>document</b> that JBehave reads): ...|$|E
50|$|For instance, {{given two}} transformations T1 and T2, the two can be {{connected}} so that an input XML document is transformed by T1 {{and then the}} output of T1 is fed as <b>input</b> <b>document</b> to T2. Simple pipelines like the one described above are called linear; a single <b>input</b> <b>document</b> always goes through the same sequence of transformations to produce a single output document.|$|E
50|$|FIPS 140-2, {{issued on}} 25 May 2001, takes account {{of changes in}} {{available}} technology and official standards since 1994, and of comments received from the vendor, tester, and user communities. It was the main <b>input</b> <b>document</b> to the international standard ISO/IEC 19790:2006 Security requirements for cryptographic modules issued on 1 March 2006.|$|E
40|$|In this paper, {{we use the}} {{information}} redundancy in multilingual input to correct errors in machine translation and thus {{improve the quality of}} multilingual summaries. We consider the case of multidocument summarization, where the <b>input</b> <b>documents</b> are in Arabic, and the output summary is in English. Typically, information that makes it to a summary appears in many different lexical-syntactic forms in the <b>input</b> <b>documents.</b> Further, the use of multiple machine translation systems provides yet more redundancy, yielding different ways to realize that information in English. We demonstrate how errors in the machine translations of the <b>input</b> Arabic <b>documents</b> can be corrected by identifying and generating from such redundancy, focusing on noun phrases. ...|$|R
40|$|International audienceThis paper {{presents}} an XML partitioning technique that allows main- memory query engines to process {{a class of}} XQuery queries, that we dub iterative queries, on arbitrarily large <b>input</b> <b>documents.</b> We provide a static analysis technique to recognize these queries. The static analysis is based on paths extracted from queries and does not need additional schema information. We then provide an algorithm using path information for partitioning the <b>input</b> <b>documents</b> of iter- ative queries. This algorithm admits a streaming implementation, whose effectiveness is experimentally validated...|$|R
40|$|The eld {{of digital}} {{document}} content analysis includes many important tasks, for example page segmentation or zone classi cation. It {{is impossible to}} build eective solutions for such problems and evaluate their performance without a reliable test set, that contains both <b>input</b> <b>documents</b> and expected results of segmentation and classication. In this paper we present GROTOAP | a test set useful for train-ing and performance evaluation of page segmentation and zone classication tasks. The test set contains input articles in a digital form and corresponding ground truth les. All <b>input</b> <b>documents</b> included in the test set have been selected from DOAJ database, which indexes articles published un-der CC-BY license. The whole test set is available under the same license...|$|R
50|$|Some {{might argue}} that, for SRS, the input is {{the words of}} {{stakeholders}} and, therefore, SRS validation {{is the same as}} SRS verification. Thinking this way is not advisable as it only causes more confusion. It is better to think of verification as a process involving a formal and technical <b>input</b> <b>document.</b>|$|E
5000|$|Streaming transformations: in {{previous}} versions the entire <b>input</b> <b>document</b> {{had to be}} read into memory {{before it could be}} processed, and output could not be written until processing had finished (although Saxon does have a streaming extension). The working draft allows XML streaming which will be useful for processing documents too large to fit in memory, or when transformations are chained in XML Pipelines.|$|E
50|$|Adaptive Recognition - {{a method}} {{based on a}} {{combination}} of two types of printed character recognition algorithms: multifont and omnifont. The system generates an internal font for each <b>input</b> <b>document</b> based on well printed characters using a dynamic adjustment (adaptation) to the specific input symbols. Thus, the method combines the omnitude and the technological efficiency of the omnifont approach with the high font recognition accuracy that dramatically improves the recognition rate.|$|E
40|$|The {{field of}} digital {{document}} content analysis includes many important tasks, for example page segmentation or zone classification. It {{is impossible to}} build effective solutions for such problems and evaluate their performance without a reliable test set, that contains both <b>input</b> <b>documents</b> and expected results of segmentation and classification. In this paper we present GROTOAP — a test set useful for training and performance evaluation of page segmentation and zone classification tasks. The test set contains input articles in a digital form and corresponding ground truth files. All <b>input</b> <b>documents</b> included in the test set have been selected from DOAJ database, which indexes articles published under CC-BY license. The whole test set is available under the same license. National Centre for Research and Development (NCBiR) Grant No. SP/I/ 1 / 77065 / 10 Łukasz Bolikowsk...|$|R
50|$|The {{original}} {{document is}} not changed; rather, a new document is created {{based on the}} content of an existing one. Typically, <b>input</b> <b>documents</b> are XML files, but anything from which the processor can build an XQuery and XPath Data Model can be used, such as relational database tables or geographical information systems.|$|R
50|$|SystemT {{comprises}} {{the following three}} main components: (1) AQL, a declarative rule language with a similar syntax to SQL; (2) Optimizer, which accepts AQL statements as input and generates high-performance algebraic execution plans; and (3) Executing engine, which executes the plan generated by the Optimizer and performs information extraction over <b>input</b> <b>documents.</b>|$|R
5000|$|XStream: XStream is {{a simple}} {{functional}} transformation language for XML documents based on CAML. XML transformations written in XStream are evaluated in streaming: when possible, parts of the output are computed and produced while the <b>input</b> <b>document</b> is still being parsed. Some transformations can thus be applied to huge XML documents which would not even fit in memory. The XStream compiler is distributed {{under the terms of}} the CeCILL free software license.|$|E
50|$|A {{turnaround}} {{document is}} {{a document that}} has been output from a computer, some extra information added to it, {{and then returned to}} become an <b>input</b> <b>document.</b> For example, meter cards are produced for collecting readings from gas meters, photocopiers, water meters etc. These are filled in by the customer and then returned to the company for scanning using ICR (Intelligent Character Recognition) so that the system can produce the bills for the customer. Earlier versions used punched cards, sometimes with mark sense technology.|$|E
5000|$|Consider an {{implementation}} of the Game of Life. A domain expert (or business analyst) might want to specify what should happen when someone is setting up a starting configuration of the game grid. To do this, {{he might want to}} give an example of a number of steps taken by a person who is toggling cells. Skipping over the narrative part, he might do this by writing up the following scenario into a plain text document (which is the type of <b>input</b> <b>document</b> that JBehave reads): ...|$|E
40|$|Theoretical thesis. Spine title: Topic {{coherence}} after document restructuring. Bibliography: leaves 52 - 56. 1. Introduction [...] 2. Background [...] 3. Literature review [...] 4. Method [...] 5. Results [...] 6. discussion [...] 7. Conclusion. This thesis examined whether simple preprocessing {{of documents}} such as lemmatising text, or removing or weighting {{certain parts of}} speech, could generate better quality topics, faster, using Latent Dirichlet Allocation (LDA) topic modelling. Past work has generally attempted to improve topic modelling performance by making changes to the topic modelling algorithm itself. This study examines the simpler option of transforming the <b>input</b> <b>documents</b> to the LDA algorithm. Topic quality was assessed {{on a range of}} measures that examined both topic interpretability, and how well the topics represented the source documents. The results indicate that topic quality was improved, and the time to generate the topics was less, if the <b>input</b> <b>documents</b> were reduced to only nouns, or nouns and adjectives, when the numbers of topics to be generated was 200 or 500 topics. This study also found that even when the number of topics to generate was not large, <b>input</b> <b>documents</b> could be reduced to select parts of speech to speed the generation of topics, with no loss of topic quality. The implications of these results are that very large data sets may benefit from being lemmatised and reduced to simply the nouns prior to topic modelling. Mode of access: World wide web 1 online resource (x, 56 leaves) table...|$|R
50|$|GC Mail {{can be used}} now {{to replace}} the {{existing}} less efficient and less secure methods of exchanging data between local authorities and the Police. Local authorities that deliver Housing and Council Tax benefits are {{taking part in the}} e-Transfers programme, which is e-enabling the process for delivery of Local Authority <b>Input</b> <b>Documents</b> (LAIDs) and Local Authority Claim Information (LACIs).|$|R
40|$|We discuss query {{evaluation}} for XML-based server systems {{where the}} same query is evaluated on every incoming XML message. In a typical scenario, {{many of the}} incoming messages will be highly similar to each other. Current XML query evaluators reevaluate the query from scratch on every message. We call substructures that occur in many <b>input</b> <b>documents</b> template fragments, and introduce a novel template folding method that allows to move the work of evaluating the query on recurring document substructures from the query execution engine into the query compiler. Similar to constant folding, our method avoids run-time evaluation of intermediate results whose value only depends on information that is already available at compile time. For XPath location paths, we propose a representation for such invariant intermediate results, and show {{how it can be}} incorporated into query execution plans. Such augmented execution plans improve query performance when evaluating the same query on subsequent <b>input</b> <b>documents...</b>|$|R
5000|$|XQuery: XQuery {{is a full}} {{functional}} language, {{despite having}} [...] "query" [...] in the name. It is a de facto standard used by Microsoft, Oracle, DB2, MarkLogic, etc., is {{the foundation for the}} XRX web programming model, and has a W3C recommendation for versions 1.0. XQuery is not written in XML itself like XSLT is, so its syntax is much lighter. The language is based on XPath 2.0. XQuery programs cannot have side-effects, just like XSLT and provides almost the same capabilities (for instance: declaring variables and functions, iterating over sequences, using W3C schema types), even though the program syntax are quite different. XQuery is logic driven, using FOR, WHERE and function composition (e.g. fn:concat("", generate-body (...) , [...] "")). In contrast, XSLT is data-driven (push processing model) where certain conditions of the <b>input</b> <b>document</b> trigger the execution of templates rather than the code executing in the order in which it is written.|$|E
5000|$|Operations such as {{arithmetic}} and boolean comparison require atomic {{values as}} their operands. If an operand returns a node (for example, [...] ), then the node is automatically atomized {{to extract the}} atomic value. If the <b>input</b> <b>document</b> has been validated against a schema, then the node will typically have a type annotation, and this determines the type of the resulting atomic value (in this example, the [...] attribute might have the type [...] ). If no schema is in use, the node will be untyped, {{and the type of}} the resulting atomic value will be [...] Typed atomic values are checked to ensure that they have an appropriate type for the context where they are used: for example, {{it is not possible to}} multiply a date by a number. Untyped atomic values, by contrast, follow a weak typing discipline: they are automatically converted to a type appropriate to the operation where they are used: for example with an arithmetic operation an untyped atomic value is converted to the type [...]|$|E
3000|$|Information Overload [...] The statistic-based methods exploit {{external}} text corpus {{to enrich}} the <b>input</b> <b>document.</b> Nevertheless, the real meanings of words in the document may be overwhelmed by {{the large amount of}} introduced external texts. Furthermore, they can only acquire very limited useful knowledge about the words in the <b>input</b> <b>document</b> since they just use the statistical information of two words in the external texts actually.|$|E
40|$|Abstract. This is {{a summary}} of the {{mathematical}} aspects of the design of dbacl, a digramic Bayesian classifier for text documents. dbacl computes maximum (relative) entropy models for text corpora and can compute the Bayesian posterior distribution for a given document in terms of any number of previously computed models. Both learning and classifying is O(n) in the number of lines of the <b>input</b> <b>documents.</b> 1...|$|R
40|$|This {{appendix}} {{is divided}} into three sections. The first section contains abstracts of each of the eight computer programs in the system, instructions for keypunching the three <b>input</b> <b>documents,</b> and computer operating instructions pertaining to each program. The second section contains system flowcharts for the entire system as well as program flowcharts for each program. The last section contains PL/l program listings of each program...|$|R
40|$|International audienceWe {{address the}} problem of {{learning}} automatically to map heterogeneous semi-structured documents onto a mediated target XML schema. We adopt a machine learning approach where the mapping between <b>input</b> and target <b>documents</b> is learned from a training corpus of documents. We first introduce a general stochastic model of semi structured documents generation and transformation. This model relies on the concept of meta-document which is a latent variable providing a link between <b>input</b> and target <b>documents.</b> It allows us to learn the correspondences when the <b>input</b> <b>documents</b> are expressed in a large variety of schemas. We then detail an instance of the general model for the particular task of HTML to XML conversion. This instance is tested on three different corpora using two different inference methods: a dynamic programming method and an approximate LaSO-based method...|$|R
30|$|After {{incorporating}} the knowledge graph, we can add some relations that {{are derived from}} the <b>input</b> <b>document</b> to the keyterm graph. Consider two anchor nodes v_a_i and v_a_j. If their corresponding keyterms kt_i and kt_j occur in the same window, an edge is added between v_a_i and v_a_j. For each cluster, we build a keyterm graph. Then we combine all the keyterm graphs together and obtain the keyterm graph for the <b>input</b> <b>document,</b> which is the so-called document keyterm graph.|$|E
40|$|This paper {{presents}} {{a system that}} performs skill extraction from text documents. It outputs a list of professional skills {{that are relevant to}} a given input text. We argue that the system can be practical for hiring and management of personnel in an organization. We make use of the texts and the hyperlink graph of Wikipedia, as well as a list of professional skills obtained from the LinkedIn social network. The system is based on first computing similarities between an <b>input</b> <b>document</b> and the texts of Wikipedia pages and then using a biased, hub-avoiding version of the Spreading Activation algorithm on the Wikipedia graph in order to associate the <b>input</b> <b>document</b> with skills. ...|$|E
40|$|XML path queries {{form the}} basis of complex {{filtering}} of XML data. Most current XML path query processing techniques can be divided in two groups. Navigation-based algorithms compute results by analyzing an <b>input</b> <b>document</b> one tag at a time. In contrast, index-based algorithms take advantage of precomputed numbering schemes over the input XML document. In this paper we introduce a new indexbased technique, Index-Filter, to answer multiple XML path queries. Index-Filter uses indexes built over the document tags to avoid processing large portions of the <b>input</b> <b>document</b> that are guaranteed not to be part of any match. We analyze Index-Filter and compare it against Y-Filter, a stateof -the-art navigation-based technique. We show that both techniques have their advantages, and we discuss the scenarios under which each technique is superior to the other one. In particular, we show that while most XML path query processing techniques work off SAX events, in some cases it pays off to preprocess the <b>input</b> <b>document,</b> augmenting it with auxiliary information {{that can be used to}} evaluate the queries faster. We present experimental results over real and synthetic XML documents that validate our claims...|$|E
50|$|The XSLT Mapper {{displays}} <b>input</b> <b>documents</b> on the left, and {{the target}} on the right. To map data, simply drag source nodes and drop them on the target, connecting the data sources to the desired data output. On the XSLT Source tab, the XSLT is displayed composed, based on the source-target relationship defined in the mapping operation. The code being generated is standard W3C XSLT and XPath code.|$|R
40|$|Distributed {{representation}} learned with {{neural networks}} has recently {{shown to be}} effective in modeling natural languages at fine granularities such as words, phrases, and even sentences. Whether and how such an approach can be extended to help model larger spans of text, e. g., documents, is intriguing, and further investigation would still be desirable. This paper aims to enhance neural network models for such a purpose. A typical problem of document-level modeling is automatic summarization, which aims to model documents in order to generate summaries. In this paper, we propose neural models to train computers not just to pay attention to specific regions and content of <b>input</b> <b>documents</b> with attention models, but also distract them to traverse between different content of a document so as to better grasp the overall meaning for summarization. Without engineering any features, we train the models on two large datasets. The models achieve the state-of-the-art performance, and they significantly benefit from the distraction modeling, particularly when <b>input</b> <b>documents</b> are long. Comment: Published in IJCAI- 2016 : the 25 th International Joint Conference on Artificial Intelligenc...|$|R
30|$|Average {{values for}} soil loss, likeliness of {{connectivity}} and sediment <b>input</b> are <b>documented</b> in Table  1 {{at the county}} level.|$|R
40|$|Document {{normalization}} is {{an interactive}} process that transforms raw legacy documents into semantically well-formed and linguistically controlled documents {{with the same}} communicative intention content. A paradigm for content analysis has been implemented to select candidate semantic representations of the communicative content of an <b>input</b> <b>document.</b> This implementation reuses the formal content specification of a multilingual controlled authoring system. As a consequence, a candidate semantic representation can not only {{be associated with a}} text {{in the language of the}} <b>input</b> <b>document,</b> but also in all the languages supported by the system. This paper presents how multilingual versions of an input legacy document can be obtained interactively with a proposed implementation, and discusses the advantages and limitations of this kind of normalizing translation. ...|$|E
40|$|The {{need for}} text {{summarization}} is crucial {{as we enter}} the era of information overload. In this paper we present an automatic summarization system, which generates a summary for a given <b>input</b> <b>document.</b> Our system is based on identification and extraction of important sentences in the <b>input</b> <b>document.</b> We listed a set of features that we collect as part of summary generation process. These features were stored using vector representation model. We defined a ranking function which ranks each sentence as a linear combination of the sentence features. We also discussed about discourse coherence in summaries and techniques to achieve coherent and readable summaries. The experiments showed that the summary generated is coherent the selected features are really helpful in extracting the important information in the document. ...|$|E
40|$|Term {{extraction}} {{is one of}} {{the layers}} in the ontology development process which has the task to extract all the terms contained in the <b>input</b> <b>document</b> automatically. The purpose of this process is to generate list of terms that are relevant to the domain of the <b>input</b> <b>document.</b> In the literature there are many approaches, techniques and algorithms used for term extraction. In this paper we propose a new approach using particle swarm optimization techniques in order to improve the accuracy of term extraction results. We choose five features to represent the term score. The approach has been applied to the domain of religious document. We compare our term extraction method precision with TFIDF, Weirdness, GlossaryExtraction and TermExtractor. The experimental results show that our propose approach achieve better precision than those four algorithm...|$|E
40|$|This note {{describes}} how the Greenstone digital library system uses "plugins" to import documents and metadata in different formats, and associate metadata with the appropriate documents. Plugins that import documents can perform their own format conversion internally, or take advantage of existing conversion programs. Metadata can be read from the <b>input</b> <b>documents,</b> or from separate metadata files, or are computed from the documents themselves. New plugins can be written for novel situations...|$|R
40|$|Industry 4. 0 standards, such as AutomationML, {{are used}} to specify {{properties}} of mechatronic elements in terms of views, such as electrical and mechanical views of a motor engine. These views have to be integrated {{in order to obtain}} a complete model of the artifact. Currently, the integration requires user knowledge to manually identify elements in the views that refer to the same element in the integrated model. Existing approaches are not able to scale up to large models where a potentially large number of conflicts may exist across the different views of an element. To overcome this limitation, we developed Alligator, a deductive rule-based system able to identify conflicts between AutomationML documents. We define a Datalog-based representation of the AutomationML <b>input</b> <b>documents,</b> and a set of rules for identifying conflicts. A deductive engine is used to resolve the conflicts, to merge the <b>input</b> <b>documents</b> and produce an integrated AutomationML document. Our empirica l evaluation of the quality of Alligator against a benchmark of AutomationML documents suggest that Alligator accurately identifies various types of conflicts between AutomationML documents, and thus helps increasing the scalability, efficiency, and coherence of models for Industry 4. 0 manufacturing environments...|$|R
40|$|Abstract:Clustering {{is one of}} {{the most}} {{traditional}} data mining techniques for knowledge extraction of mass data storages and high dimensional dataset in the years of research, Un preprocessed data (Articles, prepositions,conjunctions,adverbs… [...] etc.) may leads to irrelevant clusters. In this paper we are proposing feature set extraction of preprocessing of <b>input</b> <b>documents</b> and document weights can be computed based on term frequency and inverse document frequencies then computes the mutation based centroid for each iteration, except initial iteration during clustering of documents. I...|$|R
