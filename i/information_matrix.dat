1843|587|Public
25|$|The Fisher <b>information</b> <b>matrix</b> is non-singular.|$|E
25|$|The Fisher <b>information</b> <b>matrix</b> is {{continuous}} as {{a function}} of the parameters, θ.|$|E
25|$|To {{predict the}} {{information}} filter the <b>information</b> <b>matrix</b> and vector {{can be converted}} back to their state space equivalents, or alternatively the information space prediction can be used.|$|E
40|$|We present {{properties}} of <b>information</b> <b>matrices</b> of binary designs, espe-cially neighbor balanced designs, in several interference models. Since {{we are interested}} in optimality of designs, we analyse such {{properties of}} <b>information</b> <b>matrices</b> as complete symmetry and maximality of the trace. We study one- and two-dimensional interference models, where neighbor effects are fixed or random, and where observations can be correlated...|$|R
25|$|The main {{advantage}} of the information filter is that N measurements can be filtered at each timestep simply by summing their <b>information</b> <b>matrices</b> and vectors.|$|R
40|$|Abstract — Sensor fusion methods combine noisy {{measurements}} of common variables observed by several sensors, typically by averaging <b>information</b> <b>matrices</b> and vectors of the measurements. It {{may be the}} case, however, that some sensors have observed certain exclusive variables on their own. Examples include robots exploring different areas or cameras observing {{different parts of the}} scene in map merging or multi-target tracking scenarios. Iteratively averaging exclusive information is not efficient, since only one sensor provides the data, and the remaining ones echo this information, i. e., {{there is no need for}} consensus. We contribute with a method that averages the <b>information</b> <b>matrices</b> and vectors associated only to the common variables. Sensors use this averaged common information to locally estimate the exclusive variables. Our estimates are equivalent to the ones obtained by averaging the complete <b>information</b> <b>matrices</b> and vectors. The proposed method preserves properties of convergence, unbiased mean, and consistency, and improves the memory, communication, and computation costs. I...|$|R
25|$|Considered purely as a matrix, it {{is known}} as the Fisher <b>information</b> <b>matrix.</b> Considered as a {{measurement}} technique, where it is used to estimate hidden parameters in terms of observed random variables, {{it is known}} as the observed information.|$|E
2500|$|Fisher information, <b>information</b> <b>matrix,</b> its {{relationship}} to covariance matrix of ML estimates ...|$|E
2500|$|If it is {{inconvenient}} {{to compute}} the inverse of the Fisher <b>information</b> <b>matrix,</b> ...|$|E
50|$|In robotics, GraphSLAM is a Simultaneous {{localization}} and mapping algorithm {{which uses}} sparse <b>information</b> <b>matrices</b> produced by generating a graph of observation interdependencies (two observations are related if they contain {{data about the}} same landmark).|$|R
40|$|In this article, the {{problems}} of unbiased estimation of parameters in linear and quadratic mixture models have been revisited. We have studied some standard mixture designs for this purpose. The <b>information</b> <b>matrices</b> of the parameters involved in the linear and quadratic mixture models have been displayed for these standard designs. Some generalizations of the axial designs have been proposed and a comparative study of these designs, in respect of their <b>information</b> <b>matrices,</b> has been considered. Other types of mixture models viz. Becker’s homogeneous model and Draper- St. John’s model have also been briefly studied and the estimation problems have been addressed by using some axial type designs...|$|R
40|$|The exponentiated gamma (EG) {{distribution}} and Fisher <b>information</b> <b>matrices</b> for complete, Type I, and Type II censored observations are obtained. Asymptotic variances {{of the different}} estimators are derived. Also, we consider different estimators and compare their performance through Monte Carlo simulations...|$|R
2500|$|The Fisher <b>information</b> <b>matrix</b> is a [...] matrix with element [...] {{defined as}} ...|$|E
2500|$|These {{expectations}} and variances {{appear in the}} four-parameter Fisher <b>information</b> <b>matrix</b> (section titled [...] "Fisher information," [...] "four parameters") ...|$|E
2500|$|... where Einstein's {{summation}} convention {{over the}} repeating indices has been adopted; I jk denotes the j,k-th {{component of the}} inverse Fisher <b>information</b> <b>matrix</b> I−1, and ...|$|E
40|$|Behavior Design Corp, GroupFire Inc, Intel China Res Ctr, LEXIS-NEXISThis paper {{proposed}} a new query translation method {{based on the}} mutual <b>information</b> <b>matrices</b> of terms in the Chinese and English corpora. Instead of looking up a bilingual phrase dictionary, the compositional phrase (the translation of phrase...|$|R
40|$|Within the {{framework}} of linear vector Gaussian channels with arbitrary signaling, closed-form expressions for the Jacobian of the {{minimum mean square error}} and Fisher <b>information</b> <b>matrices</b> with respect to arbitrary parameters of the system are calculated in this paper. Capitalizing on prior research where the minimum mean square error and Fisher <b>information</b> <b>matrices</b> were linked to information-theoretic quantities through differentiation, closed-form expressions for the Hessian of the mutual information and the differential entropy are derived. These expressions are then used to assess the concavity properties of mutual information and differential entropy under different channel conditions and also to derive a multivariate version of the entropy power inequality due to Costa. Comment: 33 pages, 2 figures. A shorter version {{of this paper is to}} appear in IEEE Transactions on Information Theor...|$|R
3000|$|We shall define {{information}} vectors and <b>information</b> <b>matrices</b> as [...] u_i(t) = H_i^T R_i^- 1 ∑ _j= 1 ^m_i(t)β _ij^[l](t) z_ij(t) and U_i(t) = H_i^T R_i^- 1 H_i, respectively, as well {{as their}} sums across neighbors: [...] y_i(t) = ∑ _j∈J_i u_j(t) and B_i(t) = ∑ _j∈J_i U_j(t) [6, 7].|$|R
2500|$|In the {{information}} filter, or inverse covariance filter, the estimated covariance and estimated state {{are replaced by}} the <b>information</b> <b>matrix</b> and information vector respectively. These are defined as: ...|$|E
2500|$|... {{where the}} Fisher <b>information</b> <b>matrix</b> [...] is the {{expectation}} of the Hessian of [...] and renders the expression independent of the chosen parameterization. Combining the previous equalities we get ...|$|E
2500|$|... {{such that}} [...] is {{the density of}} the multivariate normal {{distribution}} [...] Then, we have an explicit expression for the inverse of the Fisher <b>information</b> <b>matrix</b> where [...] is fixed ...|$|E
40|$|AbstractIn recent years, the skew-normal models {{introduced}} by Azzalini (1985) [1]–and their multivariate generalizations from Azzalini and Dalla Valle (1996) [4]–have enjoyed an amazing success, although an important literature {{has reported that}} they exhibit, {{in the vicinity of}} symmetry, singular Fisher <b>information</b> <b>matrices</b> and stationary points in the profile log-likelihood function for skewness, with the usual unpleasant consequences for inference. It has been shown (DiCiccio and Monti (2004) [23], DiCiccio and Monti (2009) [24] and Gómez et al. (2007) [25]) that these singularities, in some specific parametric extensions of skew-normal models (such as the classes of skew-t or skew-exponential power distributions), appear at skew-normal distributions only. Yet, an important question remains open: in broader semiparametric models of skewed distributions (such as the general skew-symmetric and skew-elliptical ones), which symmetric kernels lead to such singularities? The present paper provides an answer to this question. In very general (possibly multivariate) skew-symmetric models, we characterize, for each possible value of the rank of Fisher <b>information</b> <b>matrices,</b> the class of symmetric kernels achieving the corresponding rank. Our results show that, for strictly multivariate skew-symmetric models, not only Gaussian kernels yield singular Fisher <b>information</b> <b>matrices.</b> In contrast, we prove that systematic stationary points in the profile log-likelihood functions are obtained for (multi) normal kernels only. Finally, we also discuss the implications of such singularities on inference...|$|R
50|$|Common {{visualizations}} for traceability <b>information</b> are <b>matrices,</b> graphs, lists, and hyperlinks.|$|R
40|$|It {{is shown}} that the <b>information</b> <b>matrices</b> of maximal {{parameter}} subsystems in linear models are linear functions of the moment matrices. With this result, design problems for maximal parameter subsystems are shown to be equivalent to design problems for the full parameter vector of a minimally parameterized model. Design optimality Experimental design Moment matrix Reparameterization...|$|R
2500|$|Asymptotic normality: as {{the sample}} size increases, the {{distribution}} of the MLE tends to the Gaussian distribution with mean [...] and covariance matrix equal to the inverse of the Fisher <b>information</b> <b>matrix.</b>|$|E
2500|$|In the Fisher <b>information</b> <b>matrix,</b> and the {{curvature}} of the log likelihood function, the logarithm of the geometric variance of the reflected variable (1−X) and the logarithm of the geometric covariance between X and (1−X) appear: ...|$|E
2500|$|These {{logarithmic}} variances and covariance are {{the elements}} of the Fisher <b>information</b> <b>matrix</b> for the beta distribution. [...] They are also a measure of the curvature of the log likelihood function (see section on Maximum likelihood estimation).|$|E
3000|$|... where ε {{is a small}} {{positive}} scalar. The {{algorithm is}} derived by decomposing the global Kalman filter for the whole network and adding the consensus term at the filtering level [5]. Notice that the algorithm requires communication of information vectors (size n× 1) and <b>information</b> <b>matrices</b> (size n×n) between the neighbors, {{in addition to the}} exchange of state estimates (size n× 1).|$|R
40|$|AbstractLocal {{asymptotic}} normality is {{established for the}} likelihood ratios of multivariate linear processes generated by independent and identically distributed random vectors. The average Fisher informations for both infinite order AR and infinite order MA processes are derived and are presented in frequency domain. The frequency domain formulae for average Fisher <b>information</b> <b>matrices</b> of finite dimensional parameter models including finite order multivariate ARMA models are also presented...|$|R
5000|$|... {{where the}} [...] {{are known as}} working weights or working weight matrices. They are {{symmetric}} and positive-definite. Using the EIM helps ensure {{that they are all}} positive-definite (and not just the sum of them) over much of the parameter space. In contrast, using Newton-Raphson would mean the observed <b>information</b> <b>matrices</b> would be used, and these tend to be positive-definite in a smaller subset of the parameter space.|$|R
2500|$|The {{first of}} these {{expressions}} shows that the variance of s2 is equal to , which is slightly greater than the σσ-element of the inverse Fisher <b>information</b> <b>matrix</b> [...] Thus, s2 is not an efficient estimator for σ2, and moreover, since s2 is UMVU, we can conclude that the finite-sample efficient estimator for σ2 does not exist.|$|E
2500|$|The {{variance}} of this estimator {{is equal to}} the μμ-element of the inverse Fisher <b>information</b> <b>matrix</b> [...] This implies that the estimator is finite-sample efficient. Of practical importance {{is the fact that the}} standard error of [...] is proportional to , that is, if one wishes to decrease the standard error by a factor of 10, one must increase the number of points in the sample by a factor of 100. This fact is widely used in determining sample sizes for opinion polls and the number of trials in Monte Carlo simulations.|$|E
2500|$|Maximum-likelihood {{estimation}} finally transcended heuristic justification in a proof {{published by}} Samuel S. Wilks in 1938, now called [...] "Wilks' theorem". The theorem {{shows that the}} error in the logarithm of likelihood values for estimates from multiple independent samples is χ² distributed, which enables determination of a confidence region around any one estimate of the parameters. The only difficult part of the proof depends on the expected value of the Fisher <b>information</b> <b>matrix,</b> which is provided by a theorem by Fisher. Wilks continued to improve on the generality of the theorem throughout his life, with his most general proof published in 1962.|$|E
40|$|A {{numerical}} {{algorithm is}} proposed to compute Cramér-Raotype bounds. The Cramér-Rao-type bounds are derived from <b>information</b> <b>matrices</b> of marginals of the joint pdf of the system at hand. The key ingredient is message-passing on a factor graph of the system. The method {{can be applied to}} a wide class of estimation problems. As an illustration, the problem of estimating the parameters of an AR model is considered. 1...|$|R
40|$|Abstract: When the {{necessary}} {{conditions for a}} BIBD are satisfied, but no BIBD exists, there is no simple answer for the optimal design problem. This paper identifies the E-optimal <b>information</b> <b>matrices</b> for any such irregular BIBD setting {{when the number of}} treatments is no larger than 100. A- and D-optimal designs are typically not E-optimal. An E-optimal design for 15 treatments in 21 blocks of size 5 is found...|$|R
40|$|In {{this paper}} methods are {{presented}} for obtaining parametric measures {{of information from}} the non-parametric ones and from <b>information</b> <b>matrices.</b> The properties of these measures are examined. The one-dimensional parametric measures which are derived from the non-parametric are superior to Fisher's information measure because they are free from regularity conditions. But if we impose the regularity conditions of the Fisherian theory of information, these measures become linear functions of Fisher's measure...|$|R
