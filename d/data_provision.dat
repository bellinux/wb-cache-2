198|461|Public
5|$|The Economist Intelligence Unit, the {{business}} information {{arm of the}} Economist Group, gathered results from two internet questionnaires, one of business schools and one of their students and recent graduates, and used them to rate business schools located all over the world. Information provided by the schools made up 80% of the ranking, with student and alumni responses accounting for only 20%. Factors in the evaluation included faculty:student ratio, GMAT scores of incoming students, student body diversity, foreign languages offered, percentage of graduates finding jobs within three months after graduation, percentage of graduates finding jobs through the school's career service, graduates' salaries and the comparison of pre-enrollment and post-graduation salaries, and student/alumni evaluations of the program, facilities, services, and alumni network. Results were tabulated using a smoothing method incorporating the three previous years' results. The organization used strict <b>data</b> <b>provision</b> thresholds, {{with the result that}} some highly regarded schools were omitted from the list of 100 ranked schools.|$|E
25|$|The Beyond 2011 Programme was {{established}} in 2011 to look at alternatives to the traditional census approach. The UK Statistics Authority will coordinate activity with its counterparts in the devolved administrations of Scotland and Northern Ireland which have also set up reviews of the future approach to population <b>data</b> <b>provision.</b> In 2012 six options were identified by the Beyond 2011 Programme for further consideration, ranging from a full 10-year census to rolling or smaller scale annual surveys, some supplemented by administrative data capture.|$|E
2500|$|... "while {{we believe}} colleges and {{universities}} may want to cooperate in providing data to publications {{for the purposes of}} rankings, we believe such <b>data</b> <b>provision</b> should be limited to data which is collected in accord with clear, shared professional standards (not the idiosyncratic standards of any single publication), and to data which is required to be reported to state or federal officials or which the institution believes (in accord with good accountability) should routinely be made available to any member of the public who seeks it." ...|$|E
40|$|Abstract: Adequate data {{management}} and <b>data</b> <b>provisioning</b> {{are among the}} most important topics to cope with the information explosion intrinsically associated with simulation applications. Today, data exchange with and between simulation applications is mainly accomplished in a file-style manner. These files show proprietary formats and have to be transformed according to the specific needs of simulation applications. Lots of effort has to be spent to find appropriate data sources and to specify and implement data transformations. In this paper, we present SIMPL – an extensible framework that provides a generic and consolidated abstraction for {{data management}} and <b>data</b> <b>provisioning</b> in simulation workflows. We introduce extensions to workflow languages and show how they are used to model the <b>data</b> <b>provisioning</b> for simulation workflows based on data management patterns. Furthermore, we show how the framework supports a uniform access to arbitrary external data in such workflows. This removes the burden from engineers and scientists to specify low-level details of data management for their simulation applications and thus boosts their productivity. ...|$|R
50|$|High Speed Voice and Data Link (HVDL) is a {{high speed}} voice and <b>data</b> <b>provisioning</b> method that allows telcos and ISPs to provide up to three voice {{channels}} and data (up to 1Mbit/s) on a copper pair over extremely long local loops.|$|R
3000|$|Velocity {{refers to}} the <b>data</b> <b>provisioning</b> rate and to the time within which it is {{necessary}} to act on them. Every minute about 400.000 tweets on Twitter are posted, 200 millions of e-mails are sent, and 2 millions of Google search queries are submitted [40]; [...]...|$|R
5000|$|... • Geoinformatics (management {{of spatial}} <b>data,</b> <b>provision</b> of spatial data {{services}} and coordination of geoinformation related activities) ...|$|E
50|$|The Weihenstephan Standards 2000 {{comprise}} {{the guidelines for}} standard BDE specifications for bottling plants; the Weihenstephan Standards 2005 describe the interfaces and <b>data</b> <b>provision</b> for bottling and packaging plants in the beverage industry.|$|E
50|$|Geographers' A-Z Map Company are now {{producing}} {{digital data}} Providing both the business user and high street customer {{exactly what they}} want through the additional technology of print on demand, custom map creation and digital <b>data</b> <b>provision.</b>|$|E
40|$|Business {{applications}} rely typically on databases {{for storing}} and processing their data (database-driven applications, or DBAPs). Testing DBAPs requires testing the application logic plus {{the interaction between}} the application logic and the database. Thus, DBAP test cases consist of input and output parameter values, the function to be tested, and an initial database state (i. e., DBAP test data). Various test <b>data</b> <b>provisioning</b> methods exist, such as manual test data design, generators for synthetic test data, and live-system snapshots. Many criteria and factors influence which method is optimal for a given project setting, such as costs, quality, data privacy, etc. This paper presents our methodology for guiding software development projects towards the DBAP test <b>data</b> <b>provisioning</b> method best suited for them...|$|R
40|$|The term Big Data has an {{increased}} and tautological occurrence in scientific publications. It {{is of interest}} how and whether the <b>data</b> <b>provisioning</b> is able to support enterprises in the handling and value creation of this emerging issue. Considering the amount of growing publication and the fuzzy nature of this term, an overview is requested to avoid duplications to gain relevant findings and to identify potential research gaps. To address this issue, a general literature review is accomplished, which extrapolates and clusters discussed research fields and potential gaps. It becomes apparent that {{a huge part of}} the research is technical driven. Moreover, no identified paper addresses the research area of functional <b>data</b> <b>provisioning.</b> This initiates further investigations to discuss whether Big Data itself negate such intention or research has missed it and improvements regarding Big Data are possible...|$|R
40|$|The article {{examines}} the retention and use of biometric <b>data</b> <b>provisions</b> of the Protection of Freedoms Act 2012, {{in the context of}} the expansion of police powers to take biometric samples, the adverse ECtHR decision in S and Marper v UK, and the New Labour government's response. Following an analysis of the new provisions, it considers whether they are likely to be compliant with the ECHR...|$|R
50|$|In 2004, AIMS shifted {{its focus}} from simple <b>data</b> <b>provision</b> to the {{development}} of information management capacity in the Government of the Islamic Republic of Afghanistan (GIRoA) and the broader humanitarian community, especially in the areas of Geographic Information Systems GIS and database development.|$|E
5000|$|The <b>Data</b> <b>Provision</b> Requirements 2012. Periodically, RTOs {{must submit}} data {{compliant}} with the Australian Vocational Education and Training Management Information Statistical Standard (AVETMISS). This includes information about students, their courses and qualifications completed, {{and provides the}} mechanism for national reporting about the VET system.|$|E
50|$|The {{standard}} {{includes the}} uniform provision {{and structure of}} data from the control units of various manufacturers. The Weihenstephan Standards 2000 comprise the guidelines for standard BDE specifications for bottling plants; the Weihenstephan Standards 2005 describe the interfaces and <b>data</b> <b>provision</b> for bottling and packaging plants in the beverage industry.|$|E
40|$|Computer-based {{simulations}} {{become more}} and more important, e. g., to imitate real-world experiments such as crash tests, which would otherwise be too expensive or not feasible at all. Thereby, simulation workflows may be used to control the interaction with simulation tools performing necessary numerical calculations. The input data needed by these tools often come from diverse data sources that manage their data in a multiplicity of proprietary formats. Hence, simulation workflows additionally have to carry out many complex <b>data</b> <b>provisioning</b> tasks. These tasks filter and transform heterogeneous input data {{in such a way that}} underlying simulation tools can properly ingest them. Furthermore, some simulations use different tools that need to exchange data between each other. Here, even more complex data transformations are needed to cope with the differences in data formats and data granularity as they are expected by involved tools. Nowadays, scientists conducting simulations typically have to design their simulation workflows on their own. So, they have to implement many low-level data transformations that realize the <b>data</b> <b>provisioning</b> for and the data exchange between simulation tools. In doing so, they waste time for workflow design, which hinders them to concentrate on their core issue, i. e., the simulation itself. This thesis introduces several novel concepts and methods that significantly alleviate the design of the complex <b>data</b> <b>provisioning</b> in simulation workflows. Firstly, it addresses the issue that most existing workflow systems offer multiple and diverse <b>data</b> <b>provisioning</b> techniques. So, scientists are frequently overwhelmed with selecting certain techniques that are appropriate for their workflows. This thesis discusses how to conquer the multiplicity and diversity of available techniques by their systematic classification. The resulting classes of techniques are then compared with each other considering relevant functional and non-functional requirements for <b>data</b> <b>provisioning</b> in simulation workflows. The major outcome of this classification and comparison is a set of guidelines that assist scientists in choosing proper <b>data</b> <b>provisioning</b> techniques. Another problem with existing workflow systems is that they often do not support all kinds of data resources or data management operations required by concrete computer-based simulations. So, this thesis proposes extensions of conventional workflow languages that offer a generic solution to <b>data</b> <b>provisioning</b> in arbitrary simulation workflows. These extensions allow for specifying any data management operation that may be described via the query or command languages of involved data resources, e. g., arbitrary SQL statements or shell commands. The proposed extensions of workflow languages still do not remove the burden from scientists to specify many complex data management operations using low-level query and command languages. Hence, this thesis introduces a novel pattern-based approach that even further enhances the abstraction support for simulation workflow design. Instead of specifying many workflow tasks, scientists only need to select a small number of abstract patterns to describe the high-level simulation process they have in mind. Furthermore, scientists are familiar with the parameters to be specified for the patterns, because these parameters correspond to terms or concepts that are related to their domain-specific simulation methodology. A rule-based transformation approach offers flexible means to finally map high-level patterns onto executable simulation workflows. Another major contribution is a pattern hierarchy arranging different kinds of patterns according to clearly distinguished abstraction levels. This facilitates a holistic separation of concerns and provides a systematic framework to incorporate different kinds of persons and their various skills into workflow design, e. g., not only scientists, but also data engineers. Altogether, the pattern-based approach conquers the data complexity associated with simulation workflows, which allows scientists to concentrate on their core issue again, namely on the simulation itself. The last contribution is a complementary optimization method to increase the performance of local data processing in simulation workflows. This method introduces various techniques that partition relevant local data processing tasks between the components of a workflow system in a smart way. Thereby, such tasks are either assigned to the workflow execution engine or to a tightly integrated local database system. Corresponding experiments revealed that, even for a moderate data size of about 0. 5 MB, this method is able to reduce workflow duration by nearly a factor of 9...|$|R
40|$|Provider {{issues in}} quality-constrained <b>data</b> <b>provisioning</b> Formal {{frameworks}} exist that allow service providers and users {{to negotiate the}} quality of a service. While these agreements usually include non-functional service properties, the quality of the information offered by a provider is neglected. Yet, in important application scenarios, notably in those based on the Service-Oriented computing paradigm, the outcome of complex workflows is directly affected by the quality of the data involved. In this paper, we propose a model for formal data quality agreements between data providers and data consumers, and analyze its feasibility by showing how a provider may take data quality constraints into account as part of its <b>data</b> <b>provisioning</b> process. Our analysis of the technical issues involved suggests that this is a complex problem in general, although satisfactory algorithmic and architectural solutions can be found under certain assumptions. To support this claim, we describe an algorithm for dealing with constraints on the completeness of a query result with respect to a reference data source, and outline an initial provider architecture for managing more general data quality constraints. 1...|$|R
40|$|The {{delivery}} of data in pervasive systems {{has to deal}} with end host mobility. One problem is how to create appropriate, applicationlevel <b>data</b> <b>provisioning</b> topologies, termed <b>data</b> brokers, to best match underlying network connectivity, end user locations, and the locales of their network access. Another problem is how to balance workloads in such overlay networks, in response to mobility and to changes in available processing and communication resources. This paper improves the performance of <b>data</b> <b>provisioning</b> by dynamically changing broker topologies and end users' assignments to brokers. Specifically, using publish /subscribe as a communication paradigm, a new abstraction, termed an opportunistic event channel, enables dynamic broker creation, deletion, and movement. Experimental and simulation results demonstrate the ability of opportunistic channels to optimize event delivery and processing when end users move across di#erent network access points. The technique is to `opportunistically' follow network-level hando#s across network access points with application-level hando#s of a user's broker functionality to a new, `closer' broker. The potential load imbalances across brokers caused by such hando#s are also addressed...|$|R
50|$|GT&T {{provides}} wireless TDMA and GSM services {{under the}} names Cellink and Cellink Plus; a prepaid service {{is also available}} with the 'Freedom Phone' and its prepaid cards. Their <b>data</b> <b>provision</b> is through the GT&T Frame Relay Network, hosted with Nortel Solutions. DSL service is available for business and residential customers through their Guyana Online (GOL) service; internet traffic for GT&T is handled by UUNET Technologies.|$|E
5000|$|Ieodo Ocean Research Station is {{an ocean}} {{platform}} constructed by South Korea {{and placed on}} the submerged Socotra Rock in the East China Sea. The stated purpose of the platform is the collection of meteorological <b>data,</b> <b>provision</b> for maritime safety, and fisheries monitoring. [...] However, as South Korea and China both claim that Socotra Rock lies in their respective Exclusive Economic Zones, the platform does have strategic implications.|$|E
50|$|UBM plc is {{a global}} {{business-to-business}} (B2B) events organiser headquartered in London, United Kingdom. It {{has a long history}} as a multinational media company. Its current main focus is on B2B events, but its principal operations have included live media and business-to-business communications, marketing services and <b>data</b> <b>provision,</b> and it principally serves the technology, healthcare, trade and transport, ingredients and fashion industries. UBM is listed on the London Stock Exchange and is a constituent of the FTSE 250 Index.|$|E
3000|$|In this article, {{we present}} a new {{architecture}} that provides to Internet users the capability to obtain any type of data acquired from different heterogeneous sensing infrastructures (SIs), exposed in a uniform way. The <b>data</b> <b>provisioning</b> is very flexible and it meets the user requirements. This result is achieved by accomplishing {{a high level of}} abstraction of sensing technologies and sensed data. The architecture has been designed considering the following main purposes: [...]...|$|R
40|$|We {{develop an}} {{integrative}} simulation environment supporting different scientific domains. The characteristics of this environment are: Easy-to-use environment for: modelling, testing, debugging, repairing {{and execution of}} simulation workflows Flexible, robust, user-friendly, enabling human interaction Management of data and information in scientific workflows <b>Data</b> <b>provisioning</b> Support for multiple data sources: sensors, databases, etc. Visualization of simulation results, visualization workflows, utilization of GPUs Support for integration of scientific computations, simulation software and hardware resource...|$|R
40|$|Recent {{years have}} shown an {{increasing}} trend to move applications and services into cloud infrastructures. Cloud-based applications typically consist of distributed components which are connected {{and communicate with}} each other. Automating the deployment and management of these components {{is one of the}} major challenges in IT world. The OASIS TOSCA standard provides a meta-model for describing the structure of composite cloud-based applications, which provides automation for deployment and management of these applications. TOSCA-based applications may be executed via the OpenTOSCA (a run-time environment for TOSCA-based applications) environment, which has been developed by the University of Stuttgart. Simulation applications deal with heterogeneous and huge data sources. Adequate data management and <b>data</b> <b>provisioning</b> for these applications are some of the most significant challenges for simulation applications. SIMPL is a framework which provides a generic approach for data management and <b>data</b> <b>provisioning</b> in simulation applications. SIMPL frees users to deal with any low-level details of data sources and corresponding data management operations. Both the TOSCA standard and the SIMPL framework are based on workflows. The first goal of this master's thesis is to combine the TOSCA standard with the SIMPL framework in order to enable the generic <b>data</b> <b>provisioning</b> and <b>data</b> management approach offered by SIMPL as an integral part of the TOSCA standard. A further and main part of this work is to design and implement TOSCA Service Templates for provisioning and executing bone simulations in cloud environments. Different variants of a TOSCA Service Template realizing a bone simulation in a cloud-native way have to be developed and implemented. In other words, a SaaS solution for PANDAS bone simulation is provided in the scope of this master's thesis with the help of TOSCA and SIMPL technologies...|$|R
50|$|The Beyond 2011 Programme was {{established}} in 2011 to look at alternatives to the traditional census approach. The UK Statistics Authority will coordinate activity with its counterparts in the devolved administrations of Scotland and Northern Ireland which have also set up reviews of the future approach to population <b>data</b> <b>provision.</b> In 2012 six options were identified by the Beyond 2011 Programme for further consideration, ranging from a full 10-year census to rolling or smaller scale annual surveys, some supplemented by administrative data capture.|$|E
5000|$|... "while {{we believe}} colleges and {{universities}} may want to cooperate in providing data to publications {{for the purposes of}} rankings, we believe such <b>data</b> <b>provision</b> should be limited to data which is collected in accord with clear, shared professional standards (not the idiosyncratic standards of any single publication), and to data which is required to be reported to state or federal officials or which the institution believes (in accord with good accountability) should routinely be made available to any member of the public who seeks it." ...|$|E
50|$|NFC tokens {{combined}} with a Bluetooth token may operate in several models, thus working in both a connected and a disconnected state. NFC authentication works when closer than one foot (.3 meters). The NFC protocol bridges short distances to reader while the Bluetooth connection serves for <b>data</b> <b>provision</b> with token both to enable authentication. Also when the Bluetooth link is not connected, the token may serve the locally stored authentication information in coarse positioning to NFC reader and relieves from exact positioning to a connector.|$|E
40|$|AbstractWireless Sensor Networks(WSNs) is {{spatially}} {{distributed in}} sensor nodes without relay. A Mobile Data Investor, M-Investor {{is used to}} gather the data from sensor node and upload the data into data sink. When one M-Investor is moving, gathers the data from each and every nodes of entire network and upload the data into data sink. Also it raises distance/time constraints. The proposed system uses multiple M-Investors that are formed by portioning a network {{into a number of}} small sub networks. Each of them gathers the data by dynamically moving through a number of smallest sub tours in the entire network, upload the data into a data sink and it reduces distance/time constraints. The proposed system introduces supportive caching policies for minimizing electronic <b>Data</b> <b>provisioning</b> cost in Social Wireless Sensor Networks (SWSNET). SWSNETs are formed by mobile data investor, such as data enabled phones, electronic book readers etc., sharing common interests in electronic content, and physically gathering together in public places. Electronic object caching in such SWSNETs are shown to be able to reduce the <b>Data</b> <b>provisioning</b> cost which depends heavily on the service and pricing dependences among various stakeholders including Data providers (DP), network service providers, and End Customers (EC) ...|$|R
40|$|This paper proposes an {{autonomic}} Grid {{data management}} architecture based on virtualized distributed file systems and WSRF-compliant management services. Autonomic functions are {{integrated into the}} services to provide self-managing control over the different entities of Grid-wide file system session, in accordance with high-level objectives, and operate together to automatically achieve the desired <b>data</b> <b>provisioning</b> behaviors. Important autonomic features are implemented in this system on cache configuration, data replication and session redirection. Experiments with the prototype demonstrate that this architecture can automatically and substantially improve the performance and reliability of Grid data access. 1...|$|R
5000|$|Fostering of {{self-regulation}} {{among the}} media to guarantee privacy {{and the protection of}} personal data, by encouraging more respect for the usage in relation to the <b>data</b> protection <b>provisions</b> ...|$|R
5000|$|... {{while we}} believe colleges and {{universities}} may want to cooperate in providing data to publications {{for the purposes of}} rankings, we believe such <b>data</b> <b>provision</b> should be limited to data which is collected in accord with clear, shared professional standards (not the idiosyncratic standards of any single publication), and to data which is required to be reported to state or federal officials or which the institution believes (in accord with good accountability) should routinely be made available to any member of the public who seeks it.|$|E
50|$|To {{support the}} ethical {{screening}} process, the Council on Ethics works with RepRisk ESG Business Intelligence, a global research firm and provider of environmental, social and governance (ESG) risk data. RepRisk monitors {{the companies in}} the Norwegian Pension Fund’s portfolio for issues such as severe human rights violations, particularly regarding child labor, forced labour, and violations of individual rights in conflict areas as well as gross environmental degradation and corruption. RepRisk {{has been working with}} the Council on Ethics since 2009 and in 2014, re-won the tender for ESG <b>data</b> <b>provision</b> for 2014-2017.|$|E
50|$|While making missionaries {{familiar}} with the technical terms and the doctrines of other scientists, sending them technical literature, commented by himself, Moritz von Leonhardi gradually dissolved the boundaries between informant and scientist. Thereby he allowed the missionaries {{to have their own}} scientific opinion. Also Leonhardi published the writings of the missionaries under their names, which was an unusual method for an armchair-researcher, to appear only as an editor and not as an author. Amongst discussions in terms of content, a controversy about methodological issues of <b>data</b> <b>provision,</b> field-research and scientific handling of sources was provoked.|$|E
30|$|Effectively, this {{approach}} implies {{that if a}} fraction f of P’s neighbors are in its ISP, P will download a proportion f of traffic from them. This model does not consider {{the effects of the}} heterogeneity of peer’s connections on <b>data</b> <b>provisioning.</b> Nevertheless, note that {{this approach}} becomes accurate as the number of peers online grows and approximates reality for a large enough number of peers. This happens because, for a large number of peers, the average capacity of peers inside and outside the ISP becomes equivalent and so does the chance of selecting peers inside and outside the ISP for some download.|$|R
30|$|The work {{presented}} in this paper builds on the well established mechanism of caching in distributed systems for performance improvement purposes. However, the use and effectiveness of context caches has not been evaluated or demonstrated. The Context Provisioning Architecture employs a caching mechanism at the context broker, which positively affects the mean query satisfaction time between context consumers and providers. We have analysed the relative performance of various cache replacement policies using the OMNET++ discrete event simulator. Our analysis has revealed that different caching strategies display contrasting behaviour under different scope distribution scenarios, with OF policy performing better for short scoped context data and SE performing better for long scoped context data. Based on this observation, we have devised a novel bipartite caching strategy for use in context <b>data</b> <b>provisioning</b> that allows utilization of the OF and SE policies for SV and LV scoped context <b>data</b> during context <b>provisioning.</b>|$|R
40|$|Abstract — In {{this paper}} we propose the use of {{statistical}} QoS guarantees for transmission over the wireless channel. Here, instead of QoS assurances, we propose to guarantee the percentage of time the QoS requirements are satisfied. We present an associated scheduling algorithm for the opportunistic multiple access system. We compare the proposed scheduler with popular schedulers from the literature. We observe that the statistical QoS guarantee is an attractive alternative to the assured QoS service for the wireless platform, since such strict QoS assurances decrease the wireless system performance significantly. Index Terms – Wireless packet <b>data</b> <b>provisioning,</b> resource allocation, scheduling, QoS, 3 G, 1 xEV-DO. I...|$|R
