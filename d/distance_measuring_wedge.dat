0|10000|Public
40|$|<b>Distance</b> <b>measures</b> {{are used}} to {{quantify}} {{the extent to which}} information is preserved or altered by quantum processes, and thus are indispensable tools in quantum information and quantum computing. In this paper we propose a new <b>distance</b> <b>measure</b> for mixed quantum states, which we call the dynamic <b>distance</b> <b>measure,</b> and we show that it is a proper <b>distance</b> <b>measure.</b> The dynamic <b>distance</b> <b>measure</b> is defined in terms of a measurable quantity, which makes it suitable for applications. In a final section we compare the dynamic <b>distance</b> <b>measure</b> with the well-known Bures <b>distance</b> <b>measure...</b>|$|R
50|$|<b>Distance</b> <b>measures</b> (cosmology) for {{comparison}} with other <b>distance</b> <b>measures.</b>|$|R
40|$|<b>Distance</b> <b>measures</b> play an {{important}} role in cluster analysis. There is no single <b>distance</b> <b>measure</b> that best fits for all types of the clustering problems. So, it is important to find set of <b>distance</b> <b>measures</b> for different clustering techniques on datasets that yields optimal results. In this paper, an attempt has been made to evaluate ten different <b>distance</b> <b>measures</b> on eight clustering techniques. The quality of the <b>distance</b> <b>measures</b> has been computed on basis of three factors: accuracy, inter-cluster and intra-cluster distances. The performance of clustering algorithms on different <b>distance</b> <b>measures</b> has been evaluated on three artificial and six real life datasets. The experimental results reveal that the performance and quality of different <b>distance</b> <b>measures</b> vary with the nature of data as well as clustering techniques. Hence choice of <b>distance</b> <b>measure</b> must be done on basis of dataset and clustering technique...|$|R
40|$|<b>Distance</b> <b>measures</b> are {{indispensable}} {{tools in}} quantum information processing and quantum computing. This since {{they can be}} used to quantify to what extent information is preserved, or altered, by quantum processes. In this paper we propose a new <b>distance</b> <b>measure</b> for mixed quantum states, that we call the dynamic <b>distance</b> <b>measure,</b> and show that it is a proper <b>distance</b> <b>measure.</b> The dynamic <b>distance</b> <b>measure</b> is defined in terms of a measurable quantity, which make it very suitable for applications. In a final section we compare the dynamical <b>distance</b> <b>measure</b> with the well-known Bures distance. Comment: 8 pages, no figure...|$|R
40|$|A novel <b>distance</b> <b>measure</b> for distance-based speaker {{segmentation}} is proposed. This <b>distance</b> <b>measure</b> is nonparametric, {{in contrast}} to common <b>distance</b> <b>measures</b> used in speaker segmentation systems, which often assume a Gaussian distribution when <b>measuring</b> the <b>distance</b> between 1 two audio segments. This <b>distance</b> <b>measure</b> is essentially a k-nearestneighbor <b>distance</b> <b>measure.</b> Non-vowel segment removal in preprocessing stage is also proposed. Speaker segmentation performance is tested on artificially created conversations from the TIMIT database and two AMI conversations. For short window lengths, Missed Detection Rated is decreased significantly. For moderate window lengths, a decrease in both Missed Detection and False Alarm Rates occur. The computational cost of the <b>distance</b> <b>measure</b> is high for long window lengths. Index Terms: speaker segmentation, <b>distance</b> <b>measure,</b> k-nearest-neighbo...|$|R
40|$|This paper {{presents}} an improved <b>distance</b> <b>measure</b> for speaker clustering in speaker diarization systems. The proposed phonetic subspace mixture (PSM) model introduces phonetic {{information to the}} ΔBIC <b>distance</b> <b>measure.</b> Therefore, the new PSM model-based ΔBIC <b>distance</b> <b>measure</b> can remove the effect of phonetic content on the diarization results. The typical ΔBIC <b>distance</b> <b>measure</b> {{can be seen as}} a special case of the new ΔBIC <b>distance</b> <b>measure.</b> Our experiment results show that the new distance measurement consistently improves the speaker diarization performance on three datasets. Index Terms: BIC, phonetic information, speaker diarization 1...|$|R
40|$|The aim of {{this paper}} is to present {{aggregation}} methods of individual preferences scores by means of <b>distance</b> <b>measures.</b> Three groups of <b>distance</b> <b>measures</b> are discussed: measures  which use preference distributions for all pairs of objects (e. g. Kemeny’s <b>measure,</b> Bogart’s <b>measure),</b> <b>distance</b> <b>measures</b> based on ranking data (e. g. Spearman distance, Podani <b>distance)</b> and <b>distance</b> <b>measures</b> using permissible transformations to ordinal scale (GDM 2 distance). Adequate distance formulas are presented and the aggregation of individual preference by using separate <b>distance</b> <b>measures</b> was carried out with the use of the R program...|$|R
40|$|We {{propose that}} pseudometric, a subadditive <b>distance</b> <b>measure,</b> has {{sufficient}} properties to {{be a good}} structure to perform nearest neighbor pattern classification. There exist some theoretical results that asymptotically guarantee the classification accuracy of k-nearest neighbor when the sample size grows larger. These results hold true under the assumption that the <b>distance</b> <b>measure</b> is a metric. The results still hold for pseudometrics up to some technicality. Whether the results are valid for the non-subadditive <b>distance</b> <b>measures</b> is still left unanswered. Pseudometric is also practically appealing. Once we have a subadditive <b>distance</b> <b>measure,</b> the measure will have at least one significant advantage over the non-subadditive; one can directly plug such <b>distance</b> <b>measure</b> into systems which exploit the subadditivity to perform faster nearest neighbor search techniques. This work focuses on pseudometrics for time series. We propose two frameworks for studying and designing subadditive <b>distance</b> <b>measures</b> and a few examples of <b>distance</b> <b>measures</b> resulting from the frameworks. One framework is more general than the other and can be used to tailor distances from the other framework to gain better classification performance. Experimental results of nearest neighbor classification of the designed pseudometrics in comparison with well-known existing <b>distance</b> <b>measures</b> including Dynamic Time Warping showed that the designed <b>distance</b> <b>measures</b> are practical for time series classification...|$|R
40|$|In {{this paper}} a further {{generalization}} of differential evolution based data classification method is proposed, demonstrated and initially evaluated. The differential evolution classifier is a nearest prototype vector based classifier that applies a global optimization algorithm, differential evolution, {{for determining the}} optimal values for all free parameters of the classifier model during the training phase of the classifier. The earlier version of differential evolution classifier that applied individually optimized <b>distance</b> <b>measure</b> for each new data set to be classified is generalized here so, that instead of optimizing a single <b>distance</b> <b>measure</b> for the given data set, we take a further step by proposing an approach where <b>distance</b> <b>measures</b> are optimized individually for each feature of the data set to be classified. In particular, <b>distance</b> <b>measures</b> for each feature are selected optimally from a predefined pool of alternative <b>distance</b> <b>measures.</b> The optimal <b>distance</b> <b>measures</b> are determined by differential evolution algorithm, which is also determining the optimal values for all free parameters of the selected <b>distance</b> <b>measures</b> in parallel. After determining the optimal <b>distance</b> <b>measures</b> for each feature together with their optimal parameters, we combine all featurewisely determined <b>distance</b> <b>measures</b> to form a single total <b>distance</b> <b>measure,</b> {{that is to be}} applied for the final classification decisions. The actual classification process is still based on the nearest prototype vector principle; A sample belongs to the class represented by the nearest prototype vector when measured with the above referred optimized total <b>distance</b> <b>measure.</b> During the training process the differential evolution algorithm determines optimally the class vectors, selects optimal distance metrics for each data feature, and determines the optimal values for the free parameters of each selected <b>distance</b> <b>measure.</b> Based on experimental results with nine well known classification benchmark data sets, the proposed approach yield a statistically significant improvement to the classification accuracy of differential evolution classifier. Web of Science 40104082407...|$|R
40|$|The optimal <b>distance</b> <b>measure</b> {{for a given}} {{discrimination}} task {{under the}} nearest neighbor framework {{has been shown to}} be the likelihood that a pair of measurements have different class labels [5]. For implementation and efficiency considerations, the optimal <b>distance</b> <b>measure</b> was approximated by combining more elementary <b>distance</b> <b>measures</b> defined on simple feature spaces. In this paper, we address two important issues that arise in practice for such an approach: (a) What form should the elementary <b>distance</b> <b>measure</b> in each feature space take? We motivate the need to use the optimal <b>distance</b> <b>measure</b> in simple feature spaces as the elementary distance measures; such <b>distance</b> <b>measures</b> have the desirable property that they are invariant to distance-respecting transformations. (b) How do we combine the elementary <b>distance</b> <b>measures?</b> We present the precise statistical assumptions under which a linear logistic model holds exactly. We benchmark our model with three other methods on a challenging face discrimination task and show that our approach is competitive with the state of the art. 1...|$|R
40|$|Recently, {{the optimal}} <b>distance</b> <b>measure</b> {{for a given}} object {{discrimination}} task under the nearest neighbor framework was derived [1]. For ease of implementation and efficiency considerations, the optimal <b>distance</b> <b>measure</b> was approximated by combining more elementary <b>distance</b> <b>measures</b> defined on simple feature spaces. In this paper, we address two important issues that arise in practice for such an approach: (a) What form should the elementary <b>distance</b> <b>measure</b> in each feature space take? We motivate the need to use optimal <b>distance</b> <b>measures</b> in simple feature spaces as the elementary distance measures; such <b>distance</b> <b>measures</b> have the desirable property that they are invariant to distance-respecting transformations. (b) How do we combine the elementary <b>distance</b> <b>measures?</b> We present the precise statistical assumptions under which a linear logistic model holds exactly. We benchmark our model with three other methods on a challenging face discrimination task and show that our approach is competitive {{with the state of}} the art...|$|R
30|$|While the <b>distance</b> <b>measure</b> is ‘Squared Euclidean’, the Nearest {{clustering}} {{method is}} the worst method of all. If the <b>distance</b> <b>measure</b> is considered as ‘Cosine’, the clustering methods Nearest and Within perform a worse result than the other methods. While some consider ‘Pearson correlation’ as a <b>distance</b> <b>measure</b> in hierarchical clustering analysis, the clustering method Between is not as suitable as other clustering methods. As {{it can be seen}} from Table 2, when the <b>distance</b> <b>measure</b> is considered as ‘Minkowsky’, the results of clustering methods Nearest and Centroid according to Rand’s statistics are worse than in all other methods. The Nearest clustering method also shows the worst performance for the <b>distance</b> <b>measure</b> ‘Block’. If the <b>distance</b> <b>measure</b> is considered as ‘Customized’, Rand’s C statistics show that Nearest and Centroid clustering methods give the worst performances.|$|R
40|$|<b>Distance</b> <b>measure</b> {{plays an}} {{important}} role in clustering data points. Choosing the right <b>distance</b> <b>measure</b> for a given dataset is a non-trivial problem. In this paper, we study various <b>distance</b> <b>measures</b> and their effect on different clustering techniques. In addition to the standard Euclidean distance, we use Bit-Vector based, Comparative Clustering based, Huffman code based and Dominance based <b>distance</b> <b>measures.</b> We cluster both synthetic datasets and one real life dataset using the above <b>distance</b> <b>measures</b> by employing k-means, matrix partitioning and dominance based clustering algorithms. We analyse the results of our study using a real life dataset of cricket and compare the accuracy of various techniques using synthetic datasets. ...|$|R
50|$|Statistical <b>distance</b> <b>measures</b> {{are mostly}} not metrics {{and they need}} not be symmetric. Some types of <b>distance</b> <b>measures</b> are {{referred}} to as (statistical) divergences.|$|R
30|$|<b>Distance</b> <b>measures</b> play {{a central}} role in {{evolving}} the clustering technique. Due to the rich mathematical background and natural implementation of l_p <b>distance</b> <b>measures,</b> researchers were motivated to use them in almost every clustering process. Beside l_p <b>distance</b> <b>measures,</b> there exist several <b>distance</b> <b>measures.</b> Sargent introduced a special type of <b>distance</b> <b>measures</b> m(ϕ) and n(ϕ) which is closely related to l_p. In this paper, we generalized the Sargent sequence spaces through introduction of M(ϕ) and N(ϕ) sequence spaces. Moreover, it is shown that both spaces are BK-spaces, and one is a dual of another. Further, we have clustered the two-moon dataset by using an induced M(ϕ)-distance measure (induced by the Sargent sequence space M(ϕ)) in the k-means clustering algorithm. The clustering result established the efficacy of replacing the Euclidean <b>distance</b> <b>measure</b> by the M(ϕ)-distance measure in the k-means algorithm.|$|R
40|$|Orientation coding based palmprint {{verification}} methods, such {{as competitive}} code, palmprint orientation code and robust line orientation code, are state-of-the-art verification algorithms with fast matching speeds. Orientation code {{makes use of}} two types of <b>distance</b> <b>measure,</b> SUM_XOR (angular <b>distance)</b> and OR_XOR (Hamming distance), yet {{little is known about}} the similarities and differences between these two <b>distance</b> <b>measures.</b> In this paper, we propose a unified <b>distance</b> <b>measure,</b> of which both SUM_XOR and OR_XOR can be regarded as special cases, and provide some principles for determining the parameters of the unified distance. Experimental results show that, using the same feature extraction and coding methods, the unified <b>distance</b> <b>measure</b> gets lower equal error rates than the original <b>distance</b> <b>measures.</b> Department of Computin...|$|R
40|$|Clustering plays a {{vital role}} in the various areas of {{research}} like Data Mining, Image Retrieval, Bio-computing and many a lot. <b>Distance</b> <b>measure</b> {{plays an important role in}} clustering data points. Choosing the right <b>distance</b> <b>measure</b> for a given dataset is a biggest challenge. In this paper, we study various <b>distance</b> <b>measures</b> and their effect on different clustering. This paper surveys existing <b>distance</b> <b>measures</b> for clustering and present a comparison between them based on application domain, efficiency, benefits and drawbacks. This comparison helps the researchers to take quick decision about which <b>distance</b> <b>measure</b> to use for clustering. We conclude this work by identifying trends and challenges of research and development towards clustering...|$|R
40|$|Abstract. ELKI is {{a unified}} {{software}} framework, {{designed as a}} tool suitable for evaluation of different algorithms on high dimensional real-valued feature-vectors. A special case of high dimensional real-valued feature-vectors are time series data where traditional <b>distance</b> <b>measures</b> like Lp-distances can be applied. However, also {{a broad range of}} spe-cialized <b>distance</b> <b>measures</b> like, e. g., dynamic time-warping, or general-ized <b>distance</b> <b>measures</b> like second order distances, e. g., shared-nearest-neighbor distances, have been proposed. The new version ELKI 0. 2 now is extended to time series data and offers a selection of these <b>distance</b> <b>measures.</b> It can serve as a visualization- and evaluation-tool for the behavior of different <b>distance</b> <b>measures</b> on time series data. ...|$|R
40|$|In this article, we {{introduce}} a differential evolution based classifier with extension for selecting automatically the applied <b>distance</b> <b>measure</b> from a predefined pool of alternative <b>distances</b> <b>measures</b> to suit optimally for classifying the particular data set at hand. The proposed method extends the earlier differential evolution based nearest prototype classifier by extending the optimization process by optimizing {{not only the}} required parameters for <b>distance</b> <b>measures,</b> but also optimizing {{the selection of the}} <b>distance</b> <b>measure</b> it self in order to find the best possible <b>distance</b> <b>measure</b> for the particular data set at hand. It has been clear for some time that in classification, usual euclidean distance is often not the best choice, and the optimal <b>distance</b> <b>measure</b> depends on the particular properties of the data sets to be classified. So far solving this issue have been subject to a limited attention in the literature. In cases where some consideration to this is problem is given, there has only been testing with couple <b>distance</b> <b>measure</b> to find which one applies best to the data at hand. In this paper we have attempted to take one step further by applying a systematic global optimization approach for selecting the best <b>distance</b> <b>measure</b> from a set of alternative measures for obtaining the highest classification accuracy for the given data. In particular, we have generated pool of <b>distance</b> <b>measures</b> for the purpose and developed a model on how the differential evolution based classifier can be extended to optimize the selection of the <b>distance</b> <b>measure</b> for given data. The obtained results are demonstrating, and also confirming further on the earlier findings reported in the literature, that often some other <b>distance</b> <b>measure</b> than the most commonly used euclidean distance is the best choice. The selection of <b>distance</b> <b>measure</b> {{is one of the most}} important factor for obtaining best classification accuracy, and should thereby be emphasized more in future research. The results also indicate that it is possible to build a classifier that is selecting the optimal <b>distance</b> <b>measure</b> for the given data automatically. It is also recommended that the proposed extension the differential evolution based classifier is clearly efficient alternative in solving classification problems. Web of Science 3912105701056...|$|R
40|$|We propose three <b>distance</b> <b>measures</b> {{for genetic}} search space. One is a <b>distance</b> <b>measure</b> in the {{population}} space that is useful for understanding the working mechanism of genetic algorithms. Another is a <b>distance</b> <b>measure</b> in the solution space for K-grouping problems. This {{can be used for}} normalization in crossover. The third is a level <b>distance</b> <b>measure</b> for genetic algorithms, which is useful for measuring problem difficulty with respect to genetic algorithms. We show that the proposed measures are metrics and the measures are efficiently computed...|$|R
30|$|In general, {{researchers}} especially nonstatisticians use {{cluster analysis}} methods and <b>distance</b> <b>measures</b> in different conditions. In addition, {{they choose to}} use the most famous cluster analysis methods and <b>distance</b> <b>measures,</b> which are available in statistical packages, without evaluating the validity of different conditions. When the different conditions are considered, drawn inferences are dubious, and may lead the decision-makers to incorrect decisions. It is noted that, {{with respect to the}} selection of a <b>distance</b> <b>measures,</b> the researcher must be aware that their choice can often significantly affect the results of the clustering. For example, some <b>distance</b> <b>measures</b> are inappropriate when different conditions of the variables are not met. On this point, the determination of the correct <b>distance</b> <b>measures</b> to use under various cases is the main motivation of researchers working on this subject to determine which <b>distance</b> <b>measures</b> should be used in case of different conditions.|$|R
50|$|Manhattan <b>distance</b> <b>measures</b> <b>distance</b> {{following}} only axis-aligned directions.|$|R
30|$|Regression {{in column}} (3) {{includes}} the <b>distance</b> <b>measure</b> between {{field of study}} and occupation, but it excludes the two dummy variables indicating the mismatch status. According to the estimated coefficient associated with the <b>distance</b> <b>measure,</b> labor earnings diminish 6 % for each 0.10 increase in Disti. In column (4), the sample is restricted to mismatched individuals, and the estimated coefficient for the <b>distance</b> <b>measure</b> is quite {{similar to the one}} in column (3). Restricting the analysis to matched workers, the estimated coefficient for the <b>distance</b> <b>measure</b> becomes non-significant [column (5)].|$|R
40|$|Abstract—The <b>measure</b> of <b>distance</b> {{between two}} fuzzy sets is a {{fundamental}} tool within fuzzy set theory, however, <b>distance</b> <b>measures</b> currently within the literature use a crisp value to represent the distance between fuzzy sets. A real valued <b>distance</b> <b>measure</b> is developed into a fuzzy <b>distance</b> <b>measure</b> which better reflects the uncertainty inherent in fuzzy sets and a fuzzy directional <b>distance</b> <b>measure</b> is presented, which accounts for the direction of change between fuzzy sets. A multiplicative version is explored as a full maximal assignment is computationally intractable so an intermediate solution is offered. I...|$|R
40|$|The {{objective}} {{of this paper is}} to introduce a <b>distance</b> <b>measure</b> for intuitionistic fuzzy numbers. Firstly the existing <b>distance</b> <b>measures</b> for intuitionistic fuzzy sets are analyzed and compared with the help of some examples. Then the new <b>distance</b> <b>measure</b> for intuitionistic fuzzy numbers is proposed based on interval difference. Also in particular the type of <b>distance</b> <b>measure</b> for triangle intuitionistic fuzzy numbers is described. The metric properties of the proposed measure are also studied. Some numerical examples are considered for applying the proposed measure and finally the result is compared with the existing ones...|$|R
40|$|Abstract: The Normalized Geometric and Normalized Hamming <b>distance</b> <b>measures</b> of Intuitionistic Fuzzy Multi sets (IFMS) are {{presented}} in depth in this paper. Due to the wide applications in various fields, the <b>distance</b> <b>measure</b> plays {{a vital role in}} Intuitionistic Fuzzy sets (IFS). We extend the <b>distance</b> <b>measure</b> of IFS to IFMS as there are possibilities of multi membership, non membership for the same element. To demonstrate the efficiency of the proposed measures, the properties of <b>distance</b> <b>measures</b> are analysed. As the proposed method is mathematically valid, it can be applied to any decision making problems, medical diagnosis, engineering problems, pattern recognition, etc. The application of medical diagnosis and pattern recognition shows that the proposed <b>distance</b> <b>measures</b> are much simpler, well suited one to use with linguistic variables...|$|R
40|$|In {{this thesis}} I {{have worked with}} medical image registration, in {{particular}} registration of multimodal images. I have tested the image registration software package FAIR developed by Jan Modersitzki, with particular focus on the choice of <b>distance</b> <b>measure</b> in image registration. I have tested the <b>distance</b> <b>measures</b> included in FAIR, in particular Normalized Gradient Field, the <b>distance</b> <b>measure</b> developed by Modersizki, and compared them. In addition, I have developed a new <b>distance</b> <b>measure</b> called Normalized Hessian Fields and implemented it with the FAIR software. This {{has been compared to}} the other <b>distance</b> <b>measures.</b> I have also tested a combination of Normalized Gradient and Hessian Fields. The registration has been performed on multimodal brain data and kidney data provided by Haukeland University Hospital, Bergen...|$|R
40|$|Abstract <b>Distance</b> <b>measures</b> play {{a central}} role in {{evolving}} the clustering technique. Due to the rich mathematical background and natural implementation of l p l_p <b>distance</b> <b>measures,</b> researchers were motivated to use them in almost every clustering process. Beside l p l_p <b>distance</b> <b>measures,</b> there exist several <b>distance</b> <b>measures.</b> Sargent introduced a special type of <b>distance</b> <b>measures</b> m (ϕ) m(ϕ) and n (ϕ) n(ϕ) which is closely related to l p l_p. In this paper, we generalized the Sargent sequence spaces through introduction of M (ϕ) M(ϕ) and N (ϕ) N(ϕ) sequence spaces. Moreover, it is shown that both spaces are BK-spaces, and one is a dual of another. Further, we have clustered the two-moon dataset by using an induced M (ϕ) M(ϕ) -distance measure (induced by the Sargent sequence space M (ϕ) M(ϕ)) in the k-means clustering algorithm. The clustering result established the efficacy of replacing the Euclidean <b>distance</b> <b>measure</b> by the M (ϕ) M(ϕ) -distance measure in the k-means algorithm...|$|R
40|$|Abstract. The line {{spectrum}} pair (LSP) frequency represer. iation has recent:y been proposed {{as an alternative}} linear prediction (LP) parametric representation. In the context of speech coding, this representation shows better quantization properties than the other LP parametric representations. In the present paper, the LSP representation is studied for speech recognition. Several <b>distance</b> <b>measures</b> based on this representation are investigated on a steady-state vowel recognition task. The weighted LSP <b>distance</b> <b>measure</b> is found to result in the best performance. The performance of the weighted LSP <b>distance</b> <b>measure</b> is {{compared with that of}} the other popular LP <b>distance</b> <b>measures</b> (such as the Itakura. cepstral, weighted cepstral, root-power-sum, log 'area ratio and reflection coefficient <b>distance</b> <b>measures).</b> The weighted LSP <b>distance</b> <b>measure</b> is found to perform significantly better than these popular LP <b>distance</b> <b>measures.</b> Zu~,,zmmenfassung. Die LSP ({{line spectrum}} pair) Darstellung wurde vorgestellt als eine Variante des klassischen LPC Verfahrens. In dem Zusammenhang der Sprachkodierung zeigt dieses Verfahren bessere Quamisierungseigenschaften denn andere LPC-Strukturen. In diesem Artikel wird die LSP Darstellung studiert innerhalb des Rahmens der automatischen Spracherkennung. Versehiedene Distanzmessungen basierend auf diesem Verfahren wurden getestet im Rahmen einer isolierten Vokalerkennungsau~abe. Die ge,vichtete LSP Distanzmessung erwies sich am erfolgreichsten. Sic wurde vergliche...|$|R
40|$|De Wachter M., Demuynck K., Wambacq P., Van Compernolle D., "A locally {{weighted}} <b>distance</b> <b>measure</b> {{for example}} based speech recognition", Proceedings IEEE {{international conference on}} acoustics, speech, and signal processing - ICASSP' 2004, vol. I, pp. 181 - 184, May 17 - 21, 2004, Montréal, Quebec, Canada. State-of-the-art speech recognition relies on a state-dependent <b>distance</b> <b>measure.</b> In HMM systems, the <b>distance</b> <b>measure</b> is trained into state-dependent covariance matrices using a maximum likelihood or discriminative criterion. This “automatic” adjustment of the <b>distance</b> <b>measure</b> is traditionally considered an inherent advantage of HMMs over DTW recognizers, as those typically rely on a uniform Euclidean distance. In this paper we show how to incorporate a non-uniform weighted <b>distance</b> <b>measure</b> into an example based recognition system. By doing so we manage to combine the superior segmental behaviour of DTW with the near-optimal acoustic <b>distance</b> <b>measure</b> as found in HMMs. The non-uniform <b>distance</b> <b>measure</b> enforces modifications to the k nearest neighbours search, an essential component in our large vocabulary DTW approach. We show that the complexity of our solution remains within bounds. The validity of the full approach is verified by experimental results on the Resource Management and TIDigits tasks. status: publishe...|$|R
40|$|In recent years, {{evaluating}} graph distance {{has become}} more and more important in a variety of real applications and many graph <b>distance</b> <b>measures</b> have been proposed. Among all of those <b>measures,</b> structure-based graph <b>distance</b> has become the research focus due to its independence of the definition of cost function. However, the existing structure-based graph <b>distance</b> <b>measures</b> have low degree of precision because only node and edge information of graphs are employed in these graphs metrics. To improve the precision of graph <b>distance</b> <b>measure,</b> we define a substructure abundance vector (SAV) to capture more substructure information of a graph. Furthermore, based on the SAV, we propose unified graph <b>distance</b> <b>measures</b> which are generalization of the existing structurebased graph <b>distance</b> <b>measures.</b> In general, the unified graph <b>distance</b> <b>measures</b> can evaluate graph distance in much finer grain. We also show that unified graph <b>distance</b> <b>measures</b> based on occurrence mapping and some of their variants are metrics. Finally, we apply the unified graph distance metric and its variants to the population evolution analysis and construct distance graphs of marker networks in three populations, which reflect the single nucleotide polymorphism (SNP) linkage disequilibrium (LD) differences among these populations...|$|R
40|$|The {{objective}} of this thesis is to develop and generalize further the differential evolution based data classification method. For many years, evolutionary algorithms have been successfully applied to many classification tasks. Evolution algorithms are population based, stochastic search algorithms that mimic natural selection and genetics. Differential evolution is an evolutionary algorithm that has gained popularity because of its simplicity and good observed performance. In this thesis a differential evolution classifier with pool of distances is proposed, demonstrated and initially evaluated. The differential evolution classifier is a nearest prototype vector based classifier that applies a global optimization algorithm, differential evolution, to determine the optimal values for all free parameters of the classifier model during the training phase of the classifier. The differential evolution classifier applies the individually optimized <b>distance</b> <b>measure</b> for each new data set to be classified is generalized to cover a pool of distances. Instead of optimizing a single <b>distance</b> <b>measure</b> for the given data set, {{the selection of the}} optimal <b>distance</b> <b>measure</b> from a predefined pool of alternative measures is attempted systematically and automatically. Furthermore, instead of only selecting the optimal <b>distance</b> <b>measure</b> from a set of alternatives, an attempt is made to optimize the values of the possible control parameters related with the selected <b>distance</b> <b>measure.</b> Specifically, a pool of alternative <b>distance</b> <b>measures</b> is first created and then the differential evolution algorithm is applied to select the optimal <b>distance</b> <b>measure</b> that yields the highest classification accuracy with the current data. After determining the optimal <b>distance</b> <b>measures</b> for the given data set together with their optimal parameters, all determined <b>distance</b> <b>measures</b> are aggregated to form a single total <b>distance</b> <b>measure.</b> The total <b>distance</b> <b>measure</b> is applied to the final classification decisions. The actual classification process is still based on the nearest prototype vector principle; a sample belongs to the class represented by the nearest prototype vector when measured with the optimized total <b>distance</b> <b>measure.</b> During the training process the differential evolution algorithm determines the optimal class vectors, selects optimal distance metrics, and determines the optimal values for the free parameters of each selected <b>distance</b> <b>measure.</b> The results obtained with the above method confirm that the choice of <b>distance</b> <b>measure</b> {{is one of the most}} crucial factors for obtaining higher classification accuracy. The results also demonstrate that it is possible to build a classifier that is able to select the optimal <b>distance</b> <b>measure</b> for the given data set automatically and systematically. After finding optimal <b>distance</b> <b>measures</b> together with optimal parameters from the particular <b>distance</b> <b>measure</b> results are then aggregated to form a total distance, which will be used to form the deviation between the class vectors and samples and thus classify the samples. This thesis also discusses two types of aggregation operators, namely, ordered weighted averaging (OWA) based multi-distances and generalized ordered weighted averaging (GOWA). These aggregation operators were applied in this work to the aggregation of the normalized distance values. The results demonstrate that a proper combination of aggregation operator and weight generation scheme play an important role in obtaining good classification accuracy. The main outcomes of the work are the six new generalized versions of previous method called differential evolution classifier. All these DE classifier demonstrated good results in the classification tasks...|$|R
40|$|Background. <b>Measuring</b> <b>wedged</b> hepatic venous {{pressure}} and hepatic {{venous pressure}} gradient as indices of portal pressure is being increasingly used {{in assessing the}} prognosis and response to pharmacological treatment for portal hypertension in cirrhotic patients. Aim. To re-evaluate the agreement and correlation between wedged hepatic pressures and directly measured portal pressures. Methods. and agreem...|$|R
40|$|We {{present some}} {{similarity}} and <b>distance</b> <b>measures</b> between intuitionistic fuzzy sets (IFSs). Thus, we propose two semi-metric <b>distance</b> <b>measures</b> between IFSs. The measures {{are applied to}} classification of shapes and handwritten Arabic sentences described with intuitionistic fuzzy information. The experimental results permitted to do a comparative analysis between intuitionistic fuzzy similarity and <b>distance</b> <b>measures,</b> which can facilitate the selection of such measure in similar applications...|$|R
40|$|This paper explores <b>distance</b> <b>measures</b> {{based on}} genetic {{operators}} for genetic programming using tree structures. The consistency between genetic operators and <b>distance</b> <b>measures</b> {{is a crucial}} point for analytical measures of problem di#culty, such as fitness distance correlation, and for measures of population diversity, such as entropy or variance. The contribution {{of this paper is}} the exploration of possible definitions and approximations of operator-based edit <b>distance</b> <b>measures...</b>|$|R
40|$|Clustering time–course gene {{expression}} data {{is a common}} tool to find co–regulated genes and groups of genes with similar temporal or spatial expression patterns. The <b>distance</b> <b>measure</b> used for clustering has major impact on {{the properties of the}} resulting clusters. As technical problems can easily distort the microarray data {{there is a need for}} <b>distance</b> <b>measures</b> which are able to deal with outliers. Here we present new so-called ”Jackknife ” <b>distance</b> <b>measures</b> which can handle outlier time points. In a simulation study on a publicly available dataset from yeast the utility of such <b>distance</b> <b>measures</b> is investigated...|$|R
