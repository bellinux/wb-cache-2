31|0|Public
25|$|Microsoft markets {{at least}} a dozen {{different}} editions of Microsoft SQL Server, aimed at different audiences and for workloads ranging from small single-machine applications to large <b>Internet-facing</b> applications with many concurrent users.|$|E
500|$|Python {{has a large}} {{standard}} library, commonly {{cited as}} one of Python's greatest strengths, providing tools suited to many tasks. This is deliberate and {{has been described as}} a [...] "batteries included" [...] Python philosophy. For <b>Internet-facing</b> applications, many standard formats and protocols (such as MIME and HTTP) are supported. Modules for creating graphical user interfaces, connecting to relational databases, generating pseudorandom numbers, arithmetic with arbitrary precision decimals, manipulating regular expressions, and doing unit testing are also included.|$|E
2500|$|A typical Windows user has {{administrative}} privileges, {{mostly for}} convenience. Because of this, any program the user runs has unrestricted {{access to the}} system. As with other operating systems, Windows users are able to follow the principle of least privilege and use non-administrator accounts. [...] Alternatively, they can reduce the privileges of specific vulnerable <b>Internet-facing</b> processes, such as Internet Explorer.|$|E
5000|$|A forward proxy is an <b>Internet-facing</b> proxy used to {{retrieve}} {{from a wide}} range of sources (in most cases anywhere on the Internet).|$|E
50|$|Microsoft markets {{at least}} a dozen {{different}} editions of Microsoft SQL Server, aimed at different audiences and for workloads ranging from small single-machine applications to large <b>Internet-facing</b> applications with many concurrent users.|$|E
50|$|A reverse proxy is (usually) an <b>Internet-facing</b> proxy {{used as a}} {{front-end}} {{to control}} and protect access to a server on a private network, commonly also performing tasks such as load-balancing, authentication, decryption or caching. Websites could use reverse proxy to reroute traffic to avoid censorship.|$|E
50|$|A typical Windows user has {{administrative}} privileges, {{mostly for}} convenience. Because of this, any program the user runs has unrestricted {{access to the}} system. As with other operating systems, Windows users are able to follow the principle of least privilege and use non-administrator accounts. Alternatively, they can reduce the privileges of specific vulnerable <b>Internet-facing</b> processes, such as Internet Explorer.|$|E
50|$|DNSSEC {{does not}} provide {{confidentiality}} of data; in particular, all DNSSEC responses are authenticated but not encrypted. DNSSEC does not protect against DoS attacks directly, though it indirectly provides some benefit (because signature checking allows the use of potentially untrustworthy parties; this is true only if the DNS server is using a self-signed certificate,not recommended for <b>Internet-facing</b> DNS servers).|$|E
50|$|Defense.Net is {{a privately}} held American {{information}} technology and services company. The company's business is to protect <b>Internet-facing</b> infrastructures - such as e-commerce web sites - against all forms of Distributed Denial of Service (DDoS) attacks at the network. Defense.Net operates a constellation of DDoS mitigation sites around the Internet which are capable of filtering and removing DDoS attacks real-time.|$|E
50|$|Particularly {{when dealing}} with <b>Internet-facing</b> or business-critical systems, a {{sysadmin}} must have a strong grasp of computer security. This includes not merely deploying software patches, but also preventing break-ins and other security problems with preventive measures. In some organizations, computer security administration is a separate role responsible for overall security and the upkeep of firewalls and intrusion detection systems, but all sysadmins are generally responsible {{for the security of}} computer systems.|$|E
50|$|Shellshock, {{also known}} as Bashdoor, is a family of {{security}} bugs in the widely used Unix Bash shell, {{the first of which}} was disclosed on 24 September 2014. Many <b>Internet-facing</b> services, such as some web server deployments, use Bash to process certain requests, allowing an attacker to cause vulnerable versions of Bash to execute arbitrary commands. This can allow an attacker to gain unauthorized access to a computer system.|$|E
5000|$|In September 2014, Shellshock was {{disclosed}} as {{a family}} of security bugs in the widely used Unix Bash shell; most vulnerabilities of Shellshock were found using the fuzzer AFL.. (Many <b>Internet-facing</b> services, such as some web server deployments, use Bash to process certain requests, allowing an attacker to cause vulnerable versions of Bash to execute arbitrary commands. This can allow an attacker to gain unauthorized access to a computer system.) ...|$|E
50|$|One of the {{functions}} of a DNS server is to translate a domain name into an IP address that applications need to connect to an Internet resource such as a website. This functionality is defined in various formal internet standards that define the protocol in considerable detail. DNS servers are implicitly trusted by <b>internet-facing</b> computers and users to correctly resolve names to the actual addresses that are registered by the owners of an internet domain.|$|E
5000|$|Python {{has a large}} {{standard}} library, commonly {{cited as}} one of Python's greatest strengths, providing tools suited to many tasks. This is deliberate and {{has been described as}} a [...] "batteries included" [...] Python philosophy. For <b>Internet-facing</b> applications, many standard formats and protocols (such as MIME and HTTP) are supported. Modules for creating graphical user interfaces, connecting to relational databases, generating pseudorandom numbers, arithmetic with arbitrary precision decimals, manipulating regular expressions, and doing unit testing are also included.|$|E
50|$|PostgreSQL, often simply Postgres, is an object-relational {{database}} management system (ORDBMS) {{with an emphasis on}} extensibility and standards compliance. As a database server, its primary functions are to store data securely and return that data in response to requests from other software applications. It can handle workloads ranging from small single-machine applications to large <b>Internet-facing</b> applications (or for data warehousing) with many concurrent users; on macOS Server, PostgreSQL is the default database; and it is also available for Microsoft Windows and Linux (supplied in most distributions).|$|E
50|$|Windows Vista defines four {{integrity}} levels: Low (SID: S-1-16-4096), Medium (SID: S-1-16-8192), High (SID: S-1-16-12288), and System (SID: S-1-16-16384). By default, processes {{started by}} a regular user gain a Medium IL and elevated processes have High IL. By introducing integrity levels, MIC allows classes of applications to be isolated, enabling scenarios like sandboxing potentially-vulnerable applications (such as <b>Internet-facing</b> applications). Processes with Low IL are called low-integrity processes, which have less access than processes with higher ILs where the Access control enforcement is in Windows.|$|E
5000|$|Vulnerability: Older {{implementations}} of IAX2 were {{vulnerable to}} resource exhaustion DoS attacks {{that are available}} to the public. While no solutions existed for these issues, the best practices included limiting UDP port access to specific trusted IP addresses. <b>Internet-facing</b> IAX2 ports are considered vulnerable and should be monitored closely. The fuzzer used to detect these application vulnerabilities was posted on milw0rm and is included in the VoIPer development tree. These issues were briefly mentioned in the IAX RFC 5456 on page 94. This flaw does not exist in up-to-date installations of Asterisk or other PBXes.|$|E
50|$|LMTP is {{designed}} {{as an alternative}} to normal SMTP for situations where the receiving side does not have a mail queue, such as a mail storage server acting as a Mail Delivery Agent (MDA). Mail queues are an inherent requisite of SMTP. In situations in which mail queues are not possible, LMTP is desirable, since a mail storage server should manage only its mail store without having to allocate more storage for a mail queue. This is not possible with SMTP when there are multiple recipients for a mail message. SMTP can only indicate successful delivery or failure for all or none of the recipients, creating the need for a separate queue to handle the failed recipients. LMTP, on the other hand, can indicate success or failure to the client for each recipient, allowing the client to handle the queueing instead. The client in this case would typically be an <b>Internet-facing</b> mail gateway. LMTP is not intended for use over wide area networks. In other words, the Message transfer agent (MTA) still handles all outgoing mail, including the mail stream from the LMTP, to another mail server located somewhere on the Internet.|$|E
40|$|Logging Recommendations for <b>Internet-Facing</b> Servers In {{the wake}} of IPv 4 {{exhaustion}} and deployment of IP address sharing techniques, this document recommends that <b>Internet-facing</b> servers log port number and accurate timestamps {{in addition to the}} incoming IP address. Status of This Memo This memo documents an Internet Best Current Practice. This document {{is a product of the}} Internet Engineering Task Force (IETF). It represents the consensus of the IETF community. It has received public review and has been approved for publication by th...|$|E
40|$|AbstractIn {{this paper}} we propose an {{approach}} for non-intrusive, automated vulnerability assessment of services in distributed systems. Most of existing vulnerability assessment techniques rely on active testing, part of penetration-testing (pen-testing) programs, which assume a series of scanning, probing and exploitation techniques {{in order to identify}} possible system vulnerabilities. These approaches are considered highly effective in identifying possible vulnerable points in the network. However, their use is not always possible and recommended for testing on-line, mission-critical services. Therefore, this paper proposes an approach that combines the non-intrusive capabilities of Shodan tool with well-established vulnerability databases (National Vulnerability Database – NVD). The result is a comprehensive approach for non-intrusive vulnerability assessment of <b>Internet-facing</b> services, where hosts and services are interrogated from Shodan search engine and possible vulnerabilities and metrics are automatically extracted from National Vulnerability Database...|$|E
40|$|Managing the {{partitioning}} {{of resources}} between uncooperating applications {{is a fundamental}} requirement of an operating environment. Traditional operating environments only manage low-level resources which presents an impedance mismatch for <b>internet-facing</b> applications with service levels {{defined in terms of}} application-level transactions. The Multi-tasking Virtual Machine (MVM) and associated Resource Management API (RM) provide basic mechanisms for managing multiple applications within a Java ™ operating environment. RM separates mechanism and policy and takes the unusual position of delegating rate-based management of resources to the policy level. This report describes the design and implementation of policies that provide flexible resource partitioning among applications and shows their effectiveness using microbenchmarks and an application level benchmark. The latter demonstrates the partitioning of an application-specific resource among a set of application instances using exactly the same policies as used for machine-level resources...|$|E
40|$|SkyNET is a stealth {{network that}} connects hosts to a botmaster through a mobile drone. The network is {{comprised}} of machines on home Wi-Fi networks in a proximal urban area, and one or more autonomous attack drones. The SkyNET is used by a botmaster to command their botnet(s) without using the Internet. The drones are programmed to scour an urban area and compromise wireless networks. Once compromised, the drone attacks the local hosts. When a host is compromised it joins both the <b>Internet-facing</b> botnet, and the sun-facing SkyNET. Subsequent drone flights are used to issue command and control without ever linking the botmaster to the botnet via the Internet. Reverse engineering the botnet, or enumerating the bots, does not reveal {{the identity of the}} botmaster. An analyst is forced to observe the autonomous attack drone to bridge the command and control gap. In this paper we present a working example, SkyNET complete with a prototype attack drone, discuss the reality of using such a command and control method, and provide insight on how to prevent against such attacks. ...|$|E
40|$|Enterprise network administrators {{worldwide}} are {{in various}} stages of preparing for or deploying IPv 6 into their networks. The administrators face different challenges than operators of Internet access providers and have reasons for different priorities. The overall problem for many administrators will be to offer <b>Internet-facing</b> services over IPv 6 while continuing to support IPv 4, and while introducing IPv 6 access within the enterprise IT network. The overall transition will take most networks from an IPv 4 -only environment to a dual-stack network environment and eventually an IPv 6 -only operating mode. This document helps provide a framework for enterprise network architects or administrators who may be faced with many of these challenges as they consider their IPv 6 support strategies. Status of This Memo This document is not an Internet Standards Track specification; it is published for informational purposes. This document {{is a product of the}} Internet Engineering Task Force (IETF). It represents the consensus of the IETF community. It has received public review and has been approved for publication by th...|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimitedNetworks are constantly bombarded with malicious or suspicious network traffic by attackers attempting to execute their attack operations. One {{of the most}} prevalent types of traffic observed on the network is scanning traffic from reconnaissance efforts. This thesis investigates the use of network tarpits to slow automated scanning or confuse human adversaries. We identify distinguishing tarpit signatures and shortcomings of existing tarpit applications as uncovered by Degreaser (a tarpit scanner), and implement improved features into a new tarpit application called Greasy. We conduct several experiments using a select set of metrics to measure the impact of implementing new tarpitting capabilities and other improvements in Greasy, particularly Greasy 0 s ability to deceive Degreaser, degree of stickiness compared to LaBrea, and potential processing overhead as observed by packet latency. Our experimental results show that we effectively mitigate the two tarpit signatures used by Degreaser 0 s tarpit identification heuristics. And although Greasy may not hold the stickiest connections, compared to LaBrea in persist mode, it successfully improves its tarpitting capabilities, while still evading detection. More importantly, the above results are obtained by deploying Greasy on an <b>Internet-facing</b> / 24 subnet; this allows us to measure Greasy 0 s ability to interact with real-world network traffic. Furthermore, Greasy offers a modularized extensible tarpit platform for future tarpit development. Networks are constantly bombarded with malicious or suspicious network traffic by attackers attempting to execute their attack operations. One of the most prevalent types of traffic observed on the network is scanning traffic from reconnaissance efforts. This thesis investigates the use of network tarpits to slow automated scanning or confuse human adversaries. We identify distinguishing tarpit signatures and shortcomings of existing tarpit applications as uncovered by Degreaser (a tarpit scanner), and implement improved features into a new tarpit application called Greasy. We conduct several experiments using a select set of metrics to measure the impact of implementing new tarpitting capabilities and other improvements in Greasy, particularly Greasy 0 s ability to deceive Degreaser, degree of stickiness compared to LaBrea, and potential processing overhead as observed by packet latency. Our experimental results show that we effectively mitigate the two tarpit signatures used by Degreaser 0 s tarpit identification heuristics. And although Greasy may not hold the stickiest connections, compared to LaBrea in persist mode, it successfully improves its tarpitting capabilities, while still evading detection. More importantly, the above results are obtained by deploying Greasy on an <b>Internet-facing</b> / 24 subnet; this allows us to measure Greasy 0 s ability to interact with real-world network traffic. Furthermore, Greasy offers a modularized extensible tarpit platform for future tarpit development. Civilian, Naval Postgraduate Schoo...|$|E
40|$|Abstract. The {{transformation}} {{and integration of}} government services, enabled {{by the use of}} new technologies such as application servers and Web services, is fundamental to reduce the cost of government and improving service outcomes to citizens. Many core Government information systems comprise applications running on legacy mainframes, databases and transaction processing monitors. As Governments worldwide provide direct access over the Internet to these legacy applications from the general public, they may be exposed to workloads well above the origin design parameters of these back-end systems. This creates a significant risk of high profile failures for Government agencies whose newly integrated systems become overloaded. In this paper we describe how we conducted a performance assessment of a business-critical, <b>Internet-facing</b> Web services that integrated new and legacy systems from two Australian Government agencies. We leveraged prototype tools from our own research along with known techniques in performance modeling. We were able to clearly demonstrate that the existing hardware and software would be adequate to handle the predicted workload for the next financial year. We were also able to do ‘what-if ’ analysis and predict how the system can perform with alternative strategies to scale the system. We conclude by summarizing the lessons learnt, including the importance of architecture visibility, benchmarking data quality, and measurement feasibility due to issues of outsourcing, privacy legislation and cross-agency involvement...|$|E
40|$|Attackers can get {{physical}} {{control of}} a computer in sleep (S 3 /suspend-to-RAM), if it is lost, stolen, or the owner is being coerced. High-value memory-resident secrets, including disk encryption keys, and private signature/encryption keys for PGP, may be extracted (e. g., via cold-boot or DMA attacks), by physically accessing such a computer. Our goal is to alleviate threats of extracting secrets from a computer in sleep, without relying on an <b>Internet-facing</b> service. We propose Hypnoguard to protect all memory-resident OS/user data across S 3 suspensions, by first performing an in-place full memory encryption before entering sleep, and then restoring the plaintext content at wakeup-time through an environment-bound, password-based authentication pro- cess. The memory encryption key is effectively “sealed” in a Trusted Platform Module (TPM) chip with the measurement of the execution environment supported by CPU’s trusted execution mode (e. g., Intel TXT, AMD-V/SVM). Password guessing within Hypnoguard may cause the memory content to be permanently inaccessible, while guessing without Hypnoguard is equivalent to brute-forcing a high- entropy key (due to TPM protection). We achieved full memory encryption/decryption {{in less than a}} second on a mainstream computer (Intel i 7 - 4771 CPU with 8 GB RAM, taking advantage of multi-core processing and AES-NI), an apparently acceptable delay for sleep-wake transitions. To the best of our knowledge, Hypnoguard provides the first wakeup-time secure environment for authentication and key unlocking, without requiring per-application changes...|$|E
40|$|AbstractOrganizations are {{increasingly}} awakening {{to the need}} to manage organizational knowledge. On the other hand, the explosion of <b>internet-facing</b> systems and adoption of information technologies by organizations and governments encourages the development of a scenario where the knowledge of tool that solves a given problem is becoming important. The workflow tool is one of which provides computer support to manage business processes. This paper aims to examine how this tool can contribute to the process of knowledge generation. The theoretical framework of knowledge management is presented, the concepts surrounding the workflow tool and its features. The search strategy was used a multiple case study based on three organizations that use the workflow tool. For that, it was selected as a reference of research each automated process. Initially, it was created a matrix that relates the characteristics of workflow phases of knowledge creation. Then a questionnaire was designed based on relationships established in the array to verify that they occur in three organizations surveyed. The results indicate that the workflow tool contributes to the process of knowledge creation, since nine of the thirteen established relationships obtained degree of agreement above 80 %. Even the four answers which were below 80 %, the percentage of agreement obtained was between 53. 3 % and 66. 7 %. Other three had not been marked, but obtained percentages between 86. 7 % and 93. 3 %. In addition, participants described the benefits after the implementation of workflow tool for business processes in their organizations, such as centering, control and ease of sharing of tasks performed...|$|E
40|$|Abstract—Cloud computing, {{featured}} by shared servers {{and location}} independent services, {{has been widely}} adopted by various businesses to increase computing efficiency, and reduce opera-tional costs. Despite significant benefits and interests, enterprises {{have a hard time}} {{to decide whether or not}} to migrate thousands of servers into the cloud because of various reasons such as lack of holistic migration (planning) tools, concerns on data security and cloud vendor lock-in. In particular, cloud security has become the major concern for decision makers, due to the nature weakness of virtualization – the fact that the cloud allows multiple users to share resources through <b>Internet-facing</b> interfaces can be easily taken advantage of by hackers. Therefore, setting up a secure environment for resource migration becomes the top priority for both enterprises and cloud providers. To achieve the goal of security, security policies such as firewalls and access control have been widely adopted, leading to significant cost as additional resources need to employed. In this paper, we address the challenge of the security-aware virtual server migration, and propose a migration strategy that minimizes the migration cost while promising the security needs of enterprises. We prove that the proposed security-aware cost minimization problem is NP-hard and our solution can achieve an approximate factor of 2. We perform an extensive simulation study to evaluate the performance of the proposed solution under various settings. Our simulation results demonstrate that our approach can save 53 % moving cost for a single enterprise case, and 66 % for multiple enterprises case comparing to a random migration strategy...|$|E
40|$|In 2014, Singapore Management University (SMU) {{undertook}} the reworking of its ten-year-old legacy intranet {{web portal}} eWise, and successfully {{replaced it with}} the new intranet, iNet. At lower maintenance costs than eWise, iNet offered modern design, easy consolidation and renewal of content, better integrated core operational functions, vastly improved user interface, and opportunities to collaborate. The case presents {{an overview of the}} journey of this transformation at SMU, discussing the key challenges faced by Goh and his team in terms of choice of technology, skills required, resources management and change management, including cultural issues and resistance to change. It discusses the development process and the decisions taken regarding the technical platform, implementation approach, features incorporated, and the taxonomy and topology of the website. Given the sheer size of the organisation and its information and legacy structure, iNet was undoubtedly an achievement. Yet, issues remained: How to accelerate iNet’s low adoption rate amongst users? Should the portal be expanded to unify the whole of SMU through a single integrated web portal? Would that bring in synergies across the university or would the different departments continue to operate in silos? What would be the right time for upgrading the intranet portal to ensure future readiness and scalability of the overall IT solution? This case is appropriate for undergraduate or post-graduate information system courses. It will provide students an understanding of an enterprise web solution (intranet, extranet or <b>internet-facing</b> all-encompasses web portal) and its role in driving connectivity, streamlining processes and enabling synergies in an organisation...|$|E
40|$|A key {{characteristic}} {{that has led}} to the early adoption of public cloud computing is the utility pricing model that governs the cost of compute resources consumed. Similar to public utilities like gas and electricity, cloud consumers only pay for the resources they consume and only for the time they are utilized. As a result and pursuant to a Cloud Service Provider 2 ̆ 7 s (CSP) Terms of Agreement, cloud consumers are responsible for all computational costs incurred within and in support of their rented computing environments whether these resources were consumed in good faith or not. While initial threat modeling and security research on the public cloud model has primarily focused on the confidentiality and integrity of data transferred, processed, and stored in the cloud, little {{attention has been paid to}} the external threat sources that have the capability to affect the financial viability of cloud-hosted services. Bounded by a utility pricing model, <b>Internet-facing</b> web resources hosted in the cloud are vulnerable to Fraudulent Resource Consumption (FRC) attacks. Unlike an application-layer DDoS attack that consumes resources with the goal of disrupting short-term availability, a FRC attack is a considerably more subtle attack that instead targets the utility model over an extended time period. By fraudulently consuming web resources in sufficient volume (i. e. data transferred out of the cloud), an attacker is able to inflict significant fraudulent charges to the victim. This work introduces and thoroughly describes the FRC attack and discusses why current application-layer DDoS mitigation schemes are not applicable to a more subtle attack. The work goes on to propose three detection metrics that together form the criteria for detecting a FRC attack from that of normal web activity and an attribution methodology capable of accurately identifying FRC attack clients. Experimental results based on plausible and challenging attack scenarios show that an attacker, without knowledge of the training web log, has a difficult time mimicking the self-similar and consistent request semantics of normal web activity necessary to carryout a successful FRC attack...|$|E
40|$|As the {{convergence}} between our physical and digital worlds continue {{at a rapid}} pace, securing our digital information is vital to our prosperity. Most current typical computer systems are unwittingly helpful to attackers through their predictable responses. In everyday security, deception plays {{a prominent role in}} our lives and digital security is no different. The use of deception has been a cornerstone technique in many successful computer breaches. Phishing, social engineering, and drive-by-downloads are some prime examples. The work in this dissertation is structured to enhance the security of computer systems by using means of deception and deceit. ^ Deception-based security mechanisms focus on altering adversaries 2 ̆ 7 perception of computer systems in a way that can confuse them and waste their time and resources. These techniques exploit adversaries 2 ̆ 7 biases and present them with a plausible alternative to the truth bringing a number of unique advantages to computer security. In addition, deception has been widely used in many areas of computing for decades and security is no different. However, deception has only been used haphazardly in computer security. ^ In this dissertation we present a framework where deception can be planned and integrated into computer defenses. We posit how the well-known Kerckhoffs 2 ̆ 7 s principle has been misinterpreted to drive the security community away from deception-based mechanisms. We present two schemes that employ deception to protect users 2 ̆ 7 passwords during transmission and at rest when they are stored on a computer server. Moreover, we designed and built a centralized deceptive server that can be hooked to <b>internet-facing</b> servers giving them the ability to return deceptive responses. These three schemes are designed, implemented, and analyzed for their security and performance. ^ The use of deception in security, and in computing in general, shows some fruitful results. This dissertation discusses some of the unique advantages of such mechanisms and presents a framework to show how they can be integrated into computer defenses. Also, it provides three practical schemes that employ deception in their design to address some existing security challenges. We postulate that the use of deception can effectively enhance the effectiveness of current security defenses and present novel ways to address many security challenges. ...|$|E

