282|264|Public
500|$|The {{refinement}} of Bernoulli's Golden Theorem, regarding {{the convergence of}} theoretical probability and <b>empirical</b> <b>probability,</b> was taken up by many notable later day mathematicians like De Moivre, Laplace, Poisson, [...] Chebyshev, Markov, Borel, Cantelli, Kolmogorov and Khinchin. The complete proof of the Law of Large Numbers for the arbitrary random variables was finally provided during first half of 20th century.|$|E
2500|$|... {{the members}} of the {{ensemble}} can be understood as the states of the systems in experiments repeated on independent systems which have been prepared in a similar but imperfectly controlled manner (<b>empirical</b> <b>probability),</b> in the limit of an infinite number of trials.|$|E
5000|$|Percentile {{times in}} <b>empirical</b> <b>probability</b> {{distributions}} (for failure or repair).|$|E
40|$|For a {{dynamical}} system {{far from}} equilibrium, {{one has to}} deal with <b>empirical</b> <b>probabilities</b> defined through time-averages, and the main problem is then how to formulate an appropriate statistical thermodynamics. The common answer is that the standard functional expression of Boltzmann-Gibbs for the entropy should be used, the <b>empirical</b> <b>probabilities</b> being substituted for the Gibbs measure. Other functional expressions have been suggested, but apparently with no clear mechanical foundation. Here it is shown how a natural extension of the original procedure employed by Gibbs and Khinchin in defining entropy, with the only proviso of using the <b>empirical</b> <b>probabilities,</b> leads for the entropy to a functional expression which is in general different from that of Boltzmann [...] Gibbs. In particular, the Gibbs entropy is recovered for <b>empirical</b> <b>probabilities</b> of Poisson type, while the Tsallis entropies are recovered for a deformation of the Poisson distribution. Comment: 8 pages, LaTex source. Corrected some misprint...|$|R
50|$|An {{advantage}} of estimating <b>probabilities</b> using <b>empirical</b> <b>probabilities</b> {{is that this}} procedure is relatively free of assumptions.|$|R
50|$|A {{disadvantage}} in using <b>empirical</b> <b>probabilities</b> arises in estimating probabilities which are either {{very close to}} zero, or very close to one. In these cases very large sample sizes would be {{needed in order to}} estimate such probabilities to a good standard of relative accuracy. Here statistical models can help, depending on the context, and in general one can hope that such models would provide improvements in accuracy compared to <b>empirical</b> <b>probabilities,</b> provided that the assumptions involved actually do hold.|$|R
50|$|The <b>empirical</b> <b>probability,</b> {{relative}} frequency, or experimental {{probability of}} an event is {{the ratio of the}} number of outcomes in which a specified event occurs to the total number of trials, not in a theoretical sample space but in an actual experiment. In a more general sense, <b>empirical</b> <b>probability</b> estimates probabilities from experience and observation.|$|E
5000|$|The {{relative}} frequency (or <b>empirical</b> <b>probability)</b> {{of an event}} is the absolute frequency normalized {{by the total number}} of events: ...|$|E
50|$|For a set {{empirical}} measurements sampled {{from some}} probability distribution, the Freedman-Diaconis rule {{is designed to}} minimize {{the difference between the}} area under the <b>empirical</b> <b>probability</b> distribution and the area under the theoretical probability distribution.|$|E
40|$|B) The Ribosomal Database (RDP) nai&#x 308;ve Bayesian {{classifier}} assigns each 16 S sequence to {{a reference}} taxonomy with associated <b>empirical</b> <b>probabilities</b> based on oligonucleotide frequencies;|$|R
3000|$|... only occurs once. In that case, one could, e.g., build a {{histogram}} for {{the values}} of p and compute <b>empirical</b> <b>probabilities</b> for each bin or discretize the distribution function for p [8, 10].|$|R
3000|$|For two users A and B {{contributing}} to the same article we define a pairwise dominance order based on the <b>empirical</b> <b>probabilities</b> for undo, redo, and third party edits (that is, based on the values r [...]...|$|R
50|$|The term a-posteriori probability, in {{its meaning}} as {{equivalent}} to <b>empirical</b> <b>probability,</b> {{may be used}} in conjunction with a priori probability which represents an estimate of a probability not based on any observations, but based an deductive reasoning.|$|E
5000|$|Often {{maintains}} {{that history is}} nothing but mythmaking and that different histories {{are not to be}} compared on such traditional academic standards as accuracy, <b>empirical</b> <b>probability,</b> logical consistency, relevancy, completeness, fairness, honesty, etc., but on moral or political grounds ...|$|E
50|$|In {{statistical}} terms, the <b>empirical</b> <b>probability</b> is {{an estimate}} or estimator of a probability. In simple cases, where {{the result of}} a trial only determines whether or not the specified event has occurred, modelling using a binomial distribution might be appropriate and then the empirical estimate is the maximum likelihood estimate. It is the Bayesian estimate for the same case if certain assumptions are made for the prior distribution of the probability. If a trial yields more information, the <b>empirical</b> <b>probability</b> can be improved on by adopting further assumptions {{in the form of a}} statistical model: if such a model is fitted, it can be used to derive an estimate of the probability of the specified event.|$|E
40|$|New tests {{based on}} the ratio of {{generalized}} variances are presented to compare covariance matrices from dependent normal populations. Monte Carlo simulation concluded that the tests considered controlled the Type I error, providing <b>empirical</b> <b>probabilities</b> that {{were consistent with the}} nominal level stipulated...|$|R
3000|$|Now, {{this almost}} fits the score-based simple decoder framework, {{except for that}} the terms inside the {{logarithm}} are not independent for different positions i. To overcome this problem, we could try to replace the <b>empirical</b> <b>probabilities</b> f̂ by the actual probabilities f, but to compute f [...]...|$|R
40|$|In recent years, {{there has}} been growing {{interest}} in reasoning with uncertainty in logic programming and deductive databases. However, most frameworks proposed thus far are either non-probabilistic in nature or based on subjective probabilities. In this paper, we {{address the problem of}} incorporating <b>empirical</b> <b>probabilities</b> [...] that is, probabilities obtained from statistical findings [...] in deductive databases. To this end, we develop a formal model-theoretic basis for such databases. We also present a sound and complete algorithm for checking the consistency of such databases. Moreover, we develop consistency-preserving ways to optimize the algorithm for practical usage. Finally, we show how query answering for empirical deductive databases can be carried out. Keywords: deductive databases, <b>empirical</b> <b>probabilities,</b> model semantics, constraint satisfaction, optimizations, query answering 1 Introduction Uncertainty management plays a central role in everyday human decision maki [...] ...|$|R
5000|$|... {{the members}} of the {{ensemble}} can be understood as the states of the systems in experiments repeated on independent systems which have been prepared in a similar but imperfectly controlled manner (<b>empirical</b> <b>probability),</b> in the limit of an infinite number of trials.|$|E
5000|$|The {{first step}} of the DUDE scheme is to {{calculate}} the empirical distribution of symbols in each possible two-sided context along the noisy sequence [...] Formally, a given two-sided context [...] that appears once or more along [...] determines an <b>empirical</b> <b>probability</b> distribution over , whose value at the symbol [...] is ...|$|E
50|$|<b>Empirical</b> <b>probability</b> {{distributions}} {{related to}} Thomae's function appear in DNA sequencing. The human genome is diploid, having two strands per chromosome. When sequenced, small pieces ("reads") are generated: for each {{spot on the}} genome, an integer number of reads overlap with it. Their ratio is a rational number, and typically distributed similarly to Thomae's function.|$|E
40|$|The {{sequence}} of α-trimmings of <b>empirical</b> <b>probabilities</b> {{is shown to}} converge, in the Painlevé– Kuratowski sense, on the class of probability measures endowed with the weak topology, to the α-trimming of the population probability. Such a result {{is applied to the}} study of the asymptotic behaviour of central regions based on the trimming of a probability...|$|R
40|$|Professor Freudenburg {{believes}} that {{there is room for}} improvement in Risk analysis, particularly in drawing on systematic studies of human behavior in the calculation of real, <b>empirical</b> <b>probabilities</b> of failure. The need is argued to be especially acute where technological Risks are associated with low expected probabilities of failure and are managed by human organizations for extended periods of time. This permits complacency to set in...|$|R
5000|$|The {{phenomenon}} of statistical stability [...] {{is one of}} the most surprising physical phenomena consists in weak dependence of statistics (i.e., functions of the sample) on the sample size, if this size is large. This effect is typical, for example, for relative frequencies (<b>empirical</b> <b>probabilities)</b> of mass events and averages. This phenomenon is widespread and so can be regarded as a fundamental natural phenomenon.|$|R
50|$|The {{refinement}} of Bernoulli's Golden Theorem, regarding {{the convergence of}} theoretical probability and <b>empirical</b> <b>probability,</b> was taken up by many notable later day mathematicians like De Moivre, Laplace, Poisson, Chebyshev, Markov, Borel, Cantelli, Kolmogorov and Khinchin. The complete proof of the Law of Large Numbers for the arbitrary random variables was finally provided during first half of 20th century.|$|E
50|$|It {{follows from}} the law of large numbers that the <b>empirical</b> <b>probability</b> of success {{in a series of}} Bernoulli trials will {{converge}} to the theoretical probability. For a Bernoulli random variable, the expected value is the theoretical probability of success, and the average of n such variables (assuming they are independent and identically distributed (i.i.d.)) is precisely the relative frequency.|$|E
5000|$|In {{probability}} and statistics, a realization, observation, or observed value, of {{a random}} variable {{is the value}} that is actually observed (what actually happened). The random variable itself is the process dictating how the observation comes about. Statistical quantities computed from realizations without deploying a statistical model are often called [...] "empirical", as in empirical distribution function or <b>empirical</b> <b>probability.</b>|$|E
30|$|Although log-linear {{models are}} also useful in {{modelling}} {{this type of}} problem, logistic regression is preferred due to: (1) fewer and thus more significant variables and (2) direct interpretation of the estimated coefficients in measuring the <b>empirical</b> <b>probabilities</b> of events. Moreover, BCL models provide a simultaneous representation of the odds of being in one category relative to being in a designated category, called the baseline category, for all pairs of categories.|$|R
30|$|The above {{analysis}} has yielded confirmatory {{effects of such}} factors as active participation in social networks, CSR willingness, transformed sociocultural values and lessons from past failures on determining: i) entrepreneurial creativity (RQ 1); perseverance (RQ 2); decisiveness (RQ 3); and, perceived likelihood of success (RQ 4), together with empirically established relationships among them and computed <b>empirical</b> <b>probabilities,</b> controlling for geographical differences. This final section offers a brief discussion with remarks on useful insights.|$|R
40|$|We {{show that}} the {{empirical}} mass function associated with a sequence of i. i. d. discrete random variables converges in l r at the (n/log 2 n) 1 / 2 rate, for all r ≥ 2. For r < 2 the rate is shown to fail for heavy tailed distributions. The threshold case of r = 2 is explored in detail. Key words: Law of the iterated logarithm, <b>Empirical</b> <b>probabilities,</b> l 2 distanc...|$|R
5000|$|The phrase a-posteriori {{probability}} is {{also used}} {{as an alternative to}} <b>empirical</b> <b>probability</b> or relative frequency. The use of the phrase [...] "a-posteriori" [...] is reminiscent of terms in Bayesian statistics, but is not directly related to Bayesian inference, where a-posteriori probability is occasionally used to refer to posterior probability, which is different even though it has a confusingly similar name.|$|E
50|$|A direct {{estimate}} {{could be}} found by {{counting the number of}} men who satisfy both conditions to give the <b>empirical</b> <b>probability</b> of the combined condition. An alternative estimate {{could be found}} by multiplying the proportion of men who are over 6 feet in height with the proportion of men who prefer strawberry jam to raspberry jam, but this estimate relies {{on the assumption that the}} two conditions are statistically independent.|$|E
5000|$|ROV {{is often}} contrasted with more {{standard}} techniques of capital budgeting, such as {{discounted cash flow}} (DCF) analysis / net present value (NPV). [...] Under this [...] "standard" [...] NPV approach, future expected cash flows are present valued under the <b>empirical</b> <b>probability</b> measure at a discount rate that reflects the embedded risk in the project; see CAPM, APT, WACC. Here, only the expected cash flows are considered, and the [...] "flexibility" [...] to alter corporate strategy in view of actual market realizations is [...] "ignored"; see below as well as Valuing flexibility under Corporate finance. The NPV framework (implicitly) assumes that management is [...] "passive" [...] {{with regard to their}} Capital Investment once committed. Some analysts account for this uncertainty by adjusting the discount rate, e.g. by increasing the cost of capital, or the cash flows, e.g. using certainty equivalents, or applying (subjective) [...] "haircuts" [...] to the forecast numbers, or via probability-weighting as in rNPV. [...] Even when employed, however, these latter methods do not normally properly account for changes in risk over the project's lifecycle and hence fail to appropriately adapt the risk adjustment.|$|E
40|$|<b>Empirical</b> <b>probabilities</b> are {{provided}} for good, marginal, and poor flying weather for ferrying the Space Shuttle Orbiter from Edwards AFB, California, to Kennedy Space Center, Florida, and from Edwards AFB to Marshall Space Flight Center, Alabama. Results are given by month for each overall route plus segments of each route. The criteria for defining {{a day as}} good, marginal, or poor and the method of computing the relative frequencies and conditional probabilities for monthly reference periods are described...|$|R
5000|$|When an {{experiment}} is conducted, one (and only one) outcome results - [...] although this outcome may {{be included in}} any number of events, all of which would be said to have occurred on that trial. After conducting many trials of the same experiment and pooling the results, {{an experiment}}er can begin to assess the <b>empirical</b> <b>probabilities</b> of the various outcomes and events that can occur in the experiment and apply the methods of statistical analysis.|$|R
40|$|Legal persons (i. e., {{entities}} such as corporations, companies, partnerships, firms, associations, and foundations) may commit financial crimes or employ fraudulent {{activities like}} money laundering, tax fraud, or bankruptcy fraud. Therefore, in the Netherlands legal persons are automatically screened for misuse {{based on a}} set of so called risk indicators. These indicators, which are based on the data obtained from, among others, the Netherlands Chamber of Commerce, the Dutch police, and the Dutch tax authority, encompass information about certain suspicious behaviours and past insolvencies or convictions (criminal records). In order to determine whether there is an increased risk of fraud, we have devised a number of scoring functions to give a legal person a score on each risk indicator based on the registered information about the legal person and its representatives. These individual scores are subsequently combined and weighed into a total risk score that indicates whether a legal person is likely to commit fraud based on all risk indicators. This contribution reports on our two ranking approaches: one based on the <b>empirical</b> <b>probabilities</b> of the indicators and the other based on the information entropy rate of the <b>empirical</b> <b>probabilities...</b>|$|R
