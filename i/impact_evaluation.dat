1580|1319|Public
5|$|In July 2013, {{the station}} owners {{sent a letter}} of concern to the state, seeking that the Environmental Impact Report for the museum and {{footbridge}} consider a breadth of possible impacts, particularly from the footbridge. The museum itself received a Finding Of No Significant Impact in March 2014. The Environmental <b>Impact</b> <b>Evaluation</b> for the footbridge, released in July 2014, analyzed seven alternatives for the footbridge location. Alternative 5a, located east of the baggage building and including a section to the garage, was the preferred alternative. The state committed $20 million towards {{the cost of the}} potential footbridge.|$|E
25|$|Demographic and Health Surveys (DHS): provide {{data for}} {{monitoring}} and <b>impact</b> <b>evaluation</b> indicators {{in the areas}} of population, health, and nutrition.|$|E
25|$|On July 3, 2012 the Connecticut Department of Transportation {{released}} an Environmental Assessment and Environmental <b>Impact</b> <b>Evaluation,</b> detailing {{a proposal to}} replace the now-vacant Terminal B with updates and facilities intended to improve access {{and ease of use}} for Bradley travelers.|$|E
40|$|Until {{recently}} rigorous <b>impact</b> <b>evaluations</b> {{have been}} {{rare in the}} area of finance and private sector development. One reason for this is the perception that many policies and projects in this area lend themselves less to formal evaluations. However, a vanguard of new <b>impact</b> <b>evaluations</b> on areas as diverse as fostering microenterprise growth, microfinance, rainfall insurance, and regulatory reform demonstrates that in many circumstances serious evaluation is possible. The {{purpose of this paper is}} to synthesize and distil the policy and implementation lessons emerging from these studies, use them to demonstrate the feasibility of <b>impact</b> <b>evaluations</b> in a broader array of topics, and thereby help prompt new <b>impact</b> <b>evaluations</b> for projects going forward. ...|$|R
50|$|It {{supports}} <b>impact</b> <b>evaluations</b> {{of specific}} child labour interventions.|$|R
25|$|Strengthening {{information}} systems and <b>impact</b> <b>evaluations.</b> Utilities often have only very limited information {{available on the}} quality of their services and their assets. Regulators and governments often have even less, and there are frequently only rudimentary data available on how projects and programs have influenced coverage and service quality. The strengthening of local and national {{information systems}} and <b>impact</b> <b>evaluations</b> is therefore a priority for the region.|$|R
50|$|A {{number of}} {{additional}} {{organizations have been}} established to promote <b>impact</b> <b>evaluation</b> globally, including Innovations for Poverty Action, the World Bank's Strategic <b>Impact</b> <b>Evaluation</b> Fund (SIEF), the World Bank’s Development <b>Impact</b> <b>Evaluation</b> (DIME) Initiative, the Institutional Learning and Change (ILAC) Initiative of the CGIAR, and the Network of Networks on <b>Impact</b> <b>Evaluation</b> (NONIE).|$|E
50|$|UEL has {{produced}} around 70 pieces of work throughout {{the last seven}} years including the Westfield Transport Observation, Newham <b>Impact</b> <b>Evaluation</b> and LOCOG <b>Impact</b> <b>Evaluation.</b>|$|E
50|$|Lichfield, N. (1996) - Community <b>Impact</b> <b>Evaluation.</b>|$|E
40|$|This {{is a list}} of <b>impact</b> <b>evaluations</b> of {{interventions}} in low-income and middle-income countries published between January and May 2010. If you would like to submit a newly published study for inclusion in the next 'Recent Impact Evaluations', please submit details of the study to database@ 3 ieimpact. org. All listed studies will also be included in the 3 ie database of <b>impact</b> <b>evaluations</b> accessible at: [URL]...|$|R
50|$|Strengthening {{information}} systems and <b>impact</b> <b>evaluations.</b> Utilities often have only very limited information {{available on the}} quality of their services and their assets. Regulators and governments often have even less, and there are frequently only rudimentary data available on how projects and programs have influenced coverage and service quality. The strengthening of local and national {{information systems}} and <b>impact</b> <b>evaluations</b> is therefore a priority for the region.|$|R
40|$|This paper {{discusses}} {{the role that}} <b>impact</b> <b>evaluations</b> should play in scaling up. Credible <b>impact</b> <b>evaluations</b> are needed {{to ensure that the}} most effective programs are scaled up at the national or international levels. Scaling up is possible only if a case can be made that programs that have been successful on a small scale would work in other contexts. Therefore the very objective of scaling up implies that learning fro...|$|R
5000|$|... #Subtitle level 2: Connecticut Environmental <b>Impact</b> <b>Evaluation</b> ...|$|E
50|$|While {{there is}} {{agreement}} {{on the importance of}} <b>Impact</b> <b>Evaluation,</b> and a consensus is emerging around the use of counterfactual evaluation methods, there has also been widespread debate in recent years on both the definition of <b>Impact</b> <b>Evaluation</b> and the use of appropriate methods (see White 2009 for an overview).|$|E
5000|$|How {{well did}} the {{intervention}} work? (Outcome and <b>impact</b> <b>evaluation)</b> ...|$|E
30|$|Once implemented, every {{program should}} be {{subjected}} to periodic <b>impact</b> <b>evaluations,</b> {{to determine whether the}} program is achieving its objectives.|$|R
50|$|It {{supports}} {{the integration of}} child labour considerations into <b>impact</b> <b>evaluations</b> in policy areas with a bearing on child labour.|$|R
40|$|This {{report is}} part of an {{evaluation}} commissioned by the Policy and Operations Evaluation Department (IOB) of the Netherlands Ministry of Foreign Affairs. It belongs to a series of <b>impact</b> <b>evaluations</b> of renewable energy and development programmes supported by the Netherlands, with a focus on the medium and long-term effects of these programmes on end-users or final beneficiaries. A characteristic of these studies is the use of mixed methods, that is, quantitative research techniques in combination with qualitative techniques. The purpose of the <b>impact</b> <b>evaluations</b> is to account for assistance provided and to draw lessons from the findings for improvement of policy and policy implementation. The results of these <b>impact</b> <b>evaluations</b> will serve as inputs to a policy evaluation of the "Promoting Renewable Energy Programme" (PREP) to be concluded in 2014...|$|R
5000|$|... #Subtitle level 2: Organizations {{promoting}} <b>Impact</b> <b>Evaluation</b> of Development Interventions ...|$|E
5000|$|... #Subtitle level 3: Fostering {{capacities}} in <b>Impact</b> <b>Evaluation</b> in Latin America ...|$|E
5000|$|Patton, Michael Q, 2008 Advocacy <b>Impact</b> <b>Evaluation,</b> Journal of Multi Disciplinary Evaluation ...|$|E
40|$|Value chain {{interventions}} are rarely evaluated as rigorously as interventions in agricultural production or health. This {{is due to}} various reasons, including the intrinsic complexity of value chain interventions, intricate contextual support factors, presence of multilevel system actors, constant adaption to market and nonmarket forces and the cost associated with conducting an evaluation. This paper discusses a range of approaches and benchmarks that can guide future design of value chain <b>impact</b> <b>evaluations.</b> Twenty studies were reviewed to understand the status and direction of value chain <b>impact</b> <b>evaluations.</b> A majority of the studies focus on evaluating the impact {{of only a few}} interventions, at several levels within the value chains. Few <b>impact</b> <b>evaluations</b> are based on well-constructed, well-conceived comparison groups. Most of them rely on use of propensity score matching to construct counterfactual groups and estimate treatment effects. Instrumental variables and difference-in-difference approaches are the common empirical approaches used for mitigating selection bias due to unobservables. More meaningful value chain <b>impact</b> <b>evaluations</b> should be prioritized from the beginning of any project and a significant amount of rigor should be maintained; targeting a good balance of using model-based and theory-based approaches...|$|R
5000|$|Though J-PAL {{was founded}} as a {{research}} center, its activities have expanded to encompass three areas: <b>impact</b> <b>evaluations,</b> policy outreach, and capacity building.|$|R
40|$|This {{baseline}} report {{is part of}} an evaluation commissioned by the Policy and Operations Evaluation Department (IOB) of the Netherlands Ministry of Foreign Affairs. It belongs to a series of <b>impact</b> <b>evaluations</b> of renewable energy and development programmes supported by the Netherlands, with a focus on the medium and long term effects of these programmes on end-users or final beneficiaries. A characteristic of these studies is the use of mixed methods, being quantitative research techniques, in combination with qualitative techniques, to get insight in the magnitude of effects. The purpose of the <b>impact</b> <b>evaluations</b> is to account for assistance provided and to draw lessons from the findings for improvement of policy and policy implementation. The results of these <b>impact</b> <b>evaluations</b> will be input to a policy evaluation of the “Promoting Renewable Energy Programme” (PREP) to be concluded in 2014...|$|R
50|$|With {{the support}} of the International Development Research Centre (IDRC-Canada), CEDLAS will aim to develop a {{capacity}} building initiative based on training courses on <b>impact</b> <b>evaluation,</b> scholarships and other activities to foster professional education. The objective is to provide training activities in <b>impact</b> <b>evaluation</b> in the region on a regular basis. More information {{can be found in the}} website.|$|E
50|$|Gertler is {{considered}} an early pioneer in the randomized evaluation of social programs in developing countries. He co-led the <b>impact</b> <b>evaluation</b> of the Mexican government welfare program Oportunidades, {{as well as the}} Rwandan government's roll-out of results-based financing for health. As Chief Economist for the World Bank Human Development Network (2004-2006), he helped to establish a culture of rigorous <b>impact</b> <b>evaluation.</b>|$|E
50|$|<b>Impact</b> <b>evaluation</b> {{designs are}} {{identified}} {{by the type of}} methods used to generate the counterfactual and can be broadly classified into three categories - experimental, quasi-experimental and non-experimental designs - that vary in feasibility, cost, involvement during design or after implementation phase of the intervention, and degree of selection bias. White (2006) and Ravallion (2008) discuss alternate <b>Impact</b> <b>Evaluation</b> approaches.|$|E
50|$|Environmental <b>impacts</b> <b>evaluation</b> of Gandak Canal Irrigation Project of Eastern U.P., India and Guidelines for its Management. Int. J. Environmental studies, ENGLAND, Vol.32, pp. 137-149, 1988.|$|R
40|$|Until {{recently}} rigorous <b>impact</b> <b>evaluations</b> {{have been}} {{rare in the}} area of finance and private sector development. One reason for this is the perception that many policies and projects in this area lend themselves less to formal evaluations. However, a vanguard of new <b>impact</b> <b>evaluations</b> on areas as diverse as fostering microenterprise growth, micro-finance, rainfall insurance, and regulatory reform demonstrates that in many circum-stances serious evaluation is possible. The {{purpose of this paper is}} to synthesize and distill the policy and implementation lessons emerging from these studies, use them to demonstrate the feasibility of <b>impact</b> <b>evaluations</b> in a broader array of topics, and thereby help prompt new <b>impact</b> <b>evaluations</b> for projects going forward. JEL codes: O 16, O 17, C 93 The recent external review of World Bank research noted that “perhaps the most important role of Bank research is to learn what works, and to widely disseminate the results ” (Banerjee and others 2006, p. 148). Rigorous <b>impact</b> <b>evaluations,</b> which compare the outcomes of a program or policy against an explicit counter-factual of what would have happened without the program or policy are one of the most important tools that can be used, along with appropriate economic theory, for understanding “what works. ” Despite this, until recently impact evalu-ations have been rare, especially outside the areas of health and education. 1 This is now particularly apparent {{in the area of}} finance and private sector develop-ment, where the recent financial crisis has prompted renewed attention to knowing what works in terms of getting finance to consumers and firms, and in getting the private sector growing again. ...|$|R
40|$|Cash {{transfer}} (CT) programmes {{have become}} an important tool of social protection and poverty reduction strategies in low- and middle-income countries. However, most of their <b>impact</b> <b>evaluations</b> pay little attention to economic and productive activities. The From Protection to Production (PtoP) project aims to study the impact of CT programmes on household economic decision-making and the local economy. 1 This research project is implemented jointly by the United Nations Food and Agriculture Organization (FAO) and UNICEF, and builds on ongoing or planned <b>impact</b> <b>evaluations</b> in seven sub-Saharan African countries: Ethiopia, Ghana...|$|R
5000|$|Gertler, Martinez, Premand, Rawlings and Vermeersch (2011) <b>Impact</b> <b>Evaluation</b> in Practice, Washington, DC:The World Bank ...|$|E
5000|$|Criteria {{include the}} risk evaluation, risk {{acceptance}} and <b>impact</b> <b>evaluation</b> criteria. These are conditioned by: ...|$|E
5000|$|Member of the Board of Commissioners, International Initiative for <b>Impact</b> <b>Evaluation</b> (3ie), April 2007 - present ...|$|E
50|$|<b>Impact</b> <b>Evaluations</b> {{which have}} to compare average {{outcomes}} {{in the treatment}} group, irrespective of beneficiary participation (also referred to as ‘compliance’ or ‘adherence’), to outcomes in the comparison group {{are referred to as}} intention-to-treat (ITT) analyses. <b>Impact</b> <b>Evaluations</b> which compare outcomes among beneficiaries who comply or adhere to the intervention in the treatment group to outcomes in the control group are referred to as treatment-on-the-treated (TOT) analyses. ITT therefore provides a lower-bound estimate of impact, but is arguably of greater policy relevance than TOT in the analysis of voluntary programs.|$|R
40|$|A {{letter report}} {{issued by the}} General Accounting Office with an {{abstract}} that begins "This report examines the use of <b>impact</b> <b>evaluations</b> to determine program effectiveness for early childhood programs. GAO (1) describes the value of conducting <b>impact</b> <b>evaluations,</b> (2) describes their current use in evaluating selected early childhood education and care programs and (3) discusses the value {{of other types of}} early childhood education and care studies the Department of Health and Human Services (HHS) and Education promote and sponsor. GAO found that many researchers consider <b>impact</b> <b>evaluations</b> to be the best method of determining {{the extent to which the}} program itself is causing participant outcomes. Two federal programs that focus on early childhood education [...] Head Start and Even Start [...] are now being studied using <b>impact</b> <b>evaluations.</b> Both of these programs are intended to improve children's school readiness and educational outcomes, including enhanced literacy. HHS is conducting two studies on its Head Start program, which will cost about $ 28. 3 million, and Education will conduct a $ 21 million study on its Even Start program. Finally, HHS and Education promote and sponsor many types of research and evaluation studies. The value of a varied study agenda is that it provides agencies with answers to a broad range of questions about program operation and allows them to align research with the focus of the program. ...|$|R
50|$|There {{are five}} key {{principles}} relating to internal validity (study design) and external validity (generalizability) which rigorous <b>impact</b> <b>evaluations</b> should address: confounding factors, selection bias, spillover effects, contamination, and impact heterogeneity.|$|R
