7|45|Public
40|$|Capturing {{the fine}} {{details on the}} surface of small objects is a real {{challenge}} to many conventional surveying methods. Our paper discusses the investigation of several data acquisition technologies, such as arm scanner, structured light scanner, terrestrial laser scanner, object line-scanner, DSLR camera, and mobile phone camera. A palm-sized embossed sculpture reproduction was used as a test object; it has been surveyed by all the instruments. The result point clouds and meshes were then analyzed, using the arm scanner’s dataset as reference. In addition to general statistics, the results have been evaluated based both on 3 <b>D</b> <b>deviation</b> maps and 2 <b>D</b> <b>deviation</b> graphs; the latter allows even more accurate analysis of the characteristics of the different data acquisition approaches. Additionally, own-developed local minimum maps were created that nicely visualize the potential level of detail provided by the applied technologies. Besides the usual geometric assessment, the paper discusses the different resource needs (cost, time, expertise) of the discussed techniques. Our results proved that even amateur sensors operated by amateur users can provide high quality datasets that enable engineering analysis. Based on the results, the paper contains an outlook to potential future investigations in this field...|$|E
40|$|We {{consider}} {{the problem of}} wrapping around an object, of which two views are available, a reference surface and recovering the resulting parametric #ow using direct computations #via spatio-temporal derivatives#. The well known examples are a#ne #ow models and 8 -parameter #ow models [...] - both describing a #ow #eld of a planar reference surface. We extend those classic #ow models {{to deal with a}} Quadric reference surface and work out the explicit parametric form of the #ow #eld. As a result we derive a simple warping algorithm that maps between two views and leaves a residual #ow proportional to the 3 <b>D</b> <b>deviation</b> of the surfacefrom a virtual quadric surface. The applications include image morphing, model building, image stabilization, and disparate view correspondence...|$|E
40|$|The {{institutional}} and policy features of Eurozone represent {{a field of}} special interest to analyse the process of integration. The aim of the paper is to investigate {{on the existence of}} a trade-off between the compliance to fiscal rules and trust as a proxy of institutional legitimacy in Eurozone. At this scope the relation between trust in the European Central Bank and structural adjustment together with unemployment and inflation, in 11 Eurozone countries from 1999 to 2013 is tested. The empirical results show that a) discretionary policy measures have an opposite sign impact on trust; b) in peripheral countries the structural adjustment is the main variable affecting trust in ECB; c) unemployment plays a key role; <b>d)</b> <b>deviation</b> from the objective of 2 % of inflation is not significant. These outcomes evidence the existence of a trade-off between the fiscal rules commitments and the European institutional consolidation process...|$|E
40|$|The {{asymmetric}} unit of {{the title}} compound, C 8 H 10 N 2 O 2, contains two independent molecules, which are linked by weak N&# 8212;H [...] . O hydrogen-bonding interactions between the amino and nitro groups. The independent molecules are both approximately planar with r. s. <b>d.</b> <b>deviations</b> of 0. 0216 and 0. 0161 &# 8197;&# 197;...|$|R
40|$|International audienceWe present {{rheological}} experiments of confined suspensions at moderate concentrations. The {{analysis is}} {{carried out in the}} framework of a previous study on particle suspensions [Davit and Peyla, EPL, 83, 64001 (2008) ] where simulations revealed the presence of unusual effects attributed to confinement, i. e. when the gap size (h) becomes closer to the particle size (<b>d).</b> <b>Deviations</b> from the usual viscosity trends were found. The present work investigates these features further and confirms the important role of the confinement. Extensions are made from the classical approach to the case of confined suspensions where the importance of the reduced gap h/d is taken into account...|$|R
5000|$|... <b>d</b> = {{downside}} <b>deviation</b> (commonly {{known in}} the financial community as 'downside risk'). Note: By extension, d² = downside variance.|$|R
40|$|Control-on-display {{interfaces}} {{enable a}} direct and intuitive manipulation by inducing control directly through the visual stimuli, thereby reducing information-processing stages and improving feed-forward property. The visual information displayed on such devices act not only as visual stimuli but also as controllers. This study investigated the effect of visual stimulus on users' response behavior while using touch screen. Three characteristics of visual stimulus were investigated: (a) shape of visual stimulus requesting simple tap reaction, (b) contrast between figure and background, and (c) existence of precue on upcoming event. The effects of the selected factors were tested using the response time and the response accuracy measured by 2 <b>D</b> <b>deviation</b> vectors. A full factorial experimental design, followed by a {{multivariate analysis of variance}} and pairwise comparisons showed that the shift of the center of area from the circumcenter of the stimuli affects the location where fingertip touched, the different background contrast affects both the time and accuracy, and it is confirmed that precue speeds up the response times and improves accuracy in control-on-display interaction. close 0...|$|E
40|$|The {{main object}} {{of this study}} was to use a {{geometric}} morphometric approach to quantify the left-right symmetry of talus bones. Methods Analysis was carried out using CT scan images of 11 pairs of intact tali. Two important geometric parameters, volume and surface area, were quantified for left and right talus bones. The geometric shape variations between the right and left talus bones were also measured using deviation analysis. Furthermore, location of asymmetry in the geometric shapes were identified. Results Numerical results showed that talus bones are bilaterally symmetrical in nature, and the difference between the surface area of the left and right talus bones was less than 7. 5 %. Similarly, the difference in the volume of both bones was less than 7. 5 %. Results of the three-dimensional (3 <b>D)</b> <b>deviation</b> analyses demonstrated the mean deviation between left and right talus bones were in the range of- 0. 74 mm to 0. 62 mm. It was observed that in eight of 11 subjects, the deviation in symmetry occurred in regions that are clinically les...|$|E
40|$|AbstractIntroduction Assessing {{quantity}} {{and distribution of}} pathological cerebral tau protein aggregation with PET in suspected tau associated neurodegeneration is currently in the scientific focus and may also become more common in clinical practice, {{particularly with regard to}} novel therapies directed against tau pathology (Dani, Brooks, and Edison 2015). For [18 F]FDG PET, semi-automated preprocessing for a standardized evaluation by means of 3 -dimensional stereotactical surface projections (3 DSSP) and z-score comparison to healthy control databases is in use for more than 20 years now (Minoshima et al. 1995). In analogy, we developed a handy preprocessing tool (Tau Image Processing, TIP) for F 18 -AV 1451 -PET data that provides an easy way to gain an overview of intensity and spatial distribution of pathological tau protein accumulation in an individual in comparison to a cohort of healthy controls. Methods Image data: PET scans were performed at the Department of Nuclear Medicine, University Hospital Cologne, Germany (Biograph mCT Flow 128 Edge scanner, Siemens, Knoxville, TN). Attenuation correction was based on a low dose CT, acquired prior to PET. 15 minutes of data acquisition in list mode started 90 minutes after application of 200 MBq F 18 -AV 1451. Scans were iteratively reconstructed on a 128 x 128 matrix using a 3 D OSEM algorithm of 4 iterations and 12 subsets (Smoothing: Gaussian filter of 5 mm FWHM). Avid Radiopharmaceuticals, Inc., Philadelphia, has provided a preexisting dataset of F 18 -AV 1451 PET scans of 19 healthy controls. Both datasets were spatially normalized to MNI space (norm cohort). Images were intensity standardized to the cerebellum according to AAL-atlas (Tzourio-Mazoyer et al. 2002). Algorithmic design of PTI (Figure 1) : Data are imported via a DICOM listener connected to a PACS. Files are converted to NIFTI format and spatially normalized to a mean image of the healthy controls using SPM 8 (Wellcome Trust Centre for Neuroimaging 2015) resulting in file dimensions of 79 x 95 x 68 isometric voxels. Images are intensity standardized to the mean cerebellar uptake. Then z-transformed voxelwise deviations from the norm cohort (3 <b>D</b> <b>deviation</b> maps) are calculated. 3 DSSPs are created from lateral, mesial and top and bottom views by creating maximum intensity projections of the first 10 voxels of brain-tissue from the respective viewing direction. A rainbow color table is applied for creation of RGB images from monochrome data. 3 DSSPs with two different z-value thresholds and axial slices superimposed on a MNI standard MRI are saved in bitmap format. TIP runs in a Matlab R 2015 a environment (The MathWorks, Inc., Natick, MA). Spatially normalized and intensity standardized images as well as 3 <b>D</b> <b>deviation</b> maps are stored in NIFTI format and are accessible for further research questions. Results The method requires only little user interaction and can be carried out by medical technologists in a standardized manner, potentially allowing the integration in daily routine work-up. Automated data processing proved to run stable without errors or interruptions in 50 test runs. Results obtained in 15 patients with different clinical diagnoses and suspected pathology were plausible and clearly correspond with the findings of unprocessed images as judged visually in transaxial, coronal and sagittal slices. Conclusions TIP is a helpful supportive tool for evaluating F 18 -AV 1451 -PETs in patients with suspected tau protein associated pathology. By providing an easy way for physicians and researchers to assess the spatial distribution and intensity of F 18 -AV 1451 uptake in comparison to healthy controls our tool simplifies the diagnostic process and automates the data preprocessing for further scientific use of the PET data...|$|E
40|$|Simons (1995) seminal work {{identified}} two different purposes how the performance measurement systems (PMS) {{of the company}} {{can be used by}} managers. Diagnostic PMS use means monitoring organizational outcomes and correct <b>ing</b> <b>deviations</b> from pre - set standards of performance. Interactive PMS use is associated with involvement of senior managers in the decision activities of their subordinates and thus stimulat ing learning and search for new opportunities of the future growth. The question is whether more diagno stically (or interactively) used PMSs mirror in the financial outcomes of the company, specifically in its returns and sales growth. To answer the question, the survey was carried out in companies domiciled in the Czech Republic. Applying exploratory facto r analysis, the level of diagnostic and interactive PMS use in the company was distilled. Consecutively, the company 's financial indicators of ROA, ROE, ROS and Sales growth were regressed on the identified factors of diagnostic and interactive PMS use. Th e results show that only diagnostic PMS use significantly influences ROE indicator and explains about 8 % of its fluctuations...|$|R
3000|$|... b(j)) categories. The {{coefficients}} {{of these variables}} are equivalent to the occupational labour market’s specific deviations from average matching productivity. Finally, the model contains interaction dummies for the yearly and occupational labour market–specific <b>deviations</b> <b>d</b> [...]...|$|R
5000|$|For a given sample , the {{sampling}} deviation of the estimator [...] is defined aswhere [...] is the expected {{value of the}} estimator. Note that {{the sampling}} <b>deviation,</b> <b>d,</b> depends {{not only on the}} estimator, but also on the sample.|$|R
40|$|The {{objective}} of the novel statistical distribution independent transfer policy model (SDITPM) {{is to reduce the}} service roundtrip time (RTT) in object-based computing effectively. As a result it shortens program execution and helps time-critical applications meet predefined deadlines effectively. The SDITPM achieves its objective through load balancing, which is a natural outcome of guided object/agent mobility. Applications running on the Internet are object-based, and the cognate objects interact in a client/server relationship over logical TCP (Transmission Control Protocol) channels. Service RTT is the interval between the point when a client has made a service request and the moment when it has received the result correctly. If the host of a logical server is congested, the average service RTT would be prolonged by delays of various origins, including queuing, context switching, and retransmissions. If the server can migrate from its congested host to a less busy node, the service RTT is automatically reduced due to the effect of load balancing, provided that the object mobility is properly guided. The SDITPM guides the object migration process by leveraging primary metrics (e. g. server's queue length). From every primary metric the corresponding secondary ones are derived for the sake of proportional (P), derivative (D), and integral (I) controls. The statistical leveraging operation {{is the responsibility of the}} M 3 RT module in the SDITPM framework, which treats every primary metric as a waveform. The secondary metrics to be derived are: (a) the "current estimated mean over the last estimated value" ratio for proportional (P) control, (b) the "current estimated rate of change" for derivative (D) control, and (c) the P and <b>D</b> <b>deviation</b> errors (DE) for integral (I) control. In every transfer policy decision making cycle the SDITPM combines the P, I, and D controls selectively to compute the overall transfer probability (TP 0) that decides if object migration should occur. An affirmative transfer policy decision by SDITPM is the first guidance to a successful object migration, which must be substantiated later by the location policy that provides the necessary second guidance of pinpointing the suitable host destination. The motivation of the SDITPM is to effect sound transfer policy decisions. Department of Computin...|$|E
3000|$|... m are the {{optimized}} D and d values, respectively. It {{is normal}} that smaller lateral scale {{results in a}} shorter mode wavelength λ due to more localized geometrical confinement. A slight difference lies in the dependences of mode wavelength λ on D and d. The weakly sub-linear change with D resembles conventional pillar cavities and {{can be explained by}} waveguide dispersion [24]. The super-linear dependence on d might be related to the existence of air apertures. Anyway, D and d have influences on λ to the similar extent in both modes O and A. It stays within 1.55 [*]±[*] 0.05  μm as the lateral sizes deviate by up to ± 10 %. As to the Q factor, its degradation with D and <b>d</b> <b>deviations,</b> caused by deviated effective incident angle of light on the DBRs [10], is as much as that with thickness deviations. In a more quantitative view, ± 5 % change in D or d keeps mode O almost of no degradation and mode A over 105 in Q factor. Again, mode O seems a little more robust than mode A, since ± 8 % deviation in lateral dimensions can keep its Q factor over 104. For a typical diameter of ~ 200  nm, the lateral size precision of ± 5 – 10 % means an error within ± 10 – 20  nm. This degree of controllability has been already available in the state-of-the-art nanotechnology [25]. The robustness against the uncertainty of the fabrication process implies the high technical feasibility to fabricate high-quality nanopillar cavity at 1.55 -μm telecommunication band.|$|R
40|$|Inline {{three-dimensional}} {{measurements are}} a growing part of optical inspection. Considering increasing production capacities and economic aspects, dynamic measurements under motion are inescapable. Using {{a sequence of}} different pattern, like it is generally done in fringe projection systems, relative movements of the measurement object {{with respect to the}} 3 d sensor between the images of one pattern sequence have to be compensated. Based on the application of fully automated optical inspection of circuit boards at an assembly line, the knowledge of the relative speed of movement between the measurement object and the 3 d sensor system should be used inside the algorithms of motion compensation. Optimally, this relative speed is constant over the whole measurement process and consists of only one motion direction to avoid sensor vibrations. The quantified evaluation of this two assumptions and the error impact on the 3 d accuracy are content of the research project described by this paper. For our experiments we use a glass etalon with non-transparent circles and transmitted light. Focused on the circle borders, {{this is one of the}} most reliable methods to determine subpixel positions using a couple of searching rays. The intersection point of all rays characterize the center of each circle. Based on these circle centers determined with a precision of approximately 1 = 50 pixel, the motion vector between two images could be calculated and compared with the input motion vector. Overall, the results are used to optimize the weight distribution of the 3 d sensor head and reduce non-uniformly vibrations. Finally, there exists a dynamic 3 d measurement system with an error of motion vectors about 4 micrometer. Based on this outcome, simulations result in a 3 <b>d</b> standard <b>deviation</b> at planar object regions of 6 micrometers. The same system yields a 3 <b>d</b> standard <b>deviation</b> of 9 µm without the optimization of weight distribution...|$|R
40|$|In this paper, we {{consider}} a uniformly ergodic Markov process (Xn) n[greater-or-equal, slanted] 0 valued in a measurable subset E of Rd with the unique invariant measure, where the density f is unknown. We establish the large deviation estimations for the nonparametric kernel density estimator in L 1 (Rd,dx) and for, and the asymptotic optimality in the Bahadur sense. These generalize the known {{results in the}} i. i. <b>d.</b> case. Large <b>deviations</b> Kernel density estimator Donsker-Varadhan entropy Uniformly ergodic Markov process Bahadur efficiency...|$|R
40|$|Antarctic krill {{are known}} to have strong {{swimming}} capabilities, but direct observations of the speed and direction of krill-swarm movement within their natural environment are rare. We identified and examined 4060 swarms within the main flow of the Antarctic Circumpolar Current (Scotia Sea) using a combination of an EK 60 echosounder, a 153. 6 kHz acoustic Doppler current profiler, and ground-truthing nets. Net displacement magnitude (m) and net angle of <b>deviation</b> (<b>d)</b> were determined by vector subtraction from the background flow immediately below them. Values were compared against control data sets in which swarms were absent. With greater background flow, m became increasingly lower than predicted, which suggests that drag influences swarm movement. The characteristics of the flow regime influenced swarm behavior, given that both m and d varied according to the direction of background flow. Furthermore, multiple-regression analysis indicated that swarm area, the vicinity of the sea-ice edge, and salinity had a significant influence on m, with levels of displacement being greatest in larger swarms and in low-salinity regions close to the ice edge. The ice edge is a key environment for Antarctic krill and swarm behavior may assist in retaining this location. Only fluorescence was found to have a significant influence on <b>d,</b> with <b>deviations</b> being greatest in regions of highest fluorescence. This agrees with laboratory observations of krill turning more frequently within food patches. We demonstrate {{that it is possible to}} measure instantaneous movement patterns in Antarctic krill swarms and, at large scales, these movements are consistent with current understanding of responses of krill to local stimuli such as sea-ice and patches of food...|$|R
3000|$|..., as {{a measure}} of network {{connection}} quality. For ease of analysis, BR and OR are aggregated into a single indicator, the unsatisfied user ratio (UUR), computed as UUR=BR + OR(1 −BR). From the operator perspective, important measures are: c) the HO Ratio (HOR), defined by the ratio between the number of HOs and carried calls, {{as a measure}} of network signaling load, and <b>d)</b> the average <b>deviation</b> from the default value of margins and transmit power in cells and adjacencies of the scenario. Most of these statistics are available on a cell basis.|$|R
40|$|Epistatic effects {{involving}} genic {{combinations of}} fixed and non fixed genes are shown {{to contribute to}} the genotypic mean of any population. These effects define specific additive x additive and additive x dominant epistatic components. As such components are not estimable, their relative importance cannot be assessed. These epistatic effects can cause bias in the estimates of the additive and dominance components to which they are confounded. The magnitude of the bias depends on the relative values of the epistatic effects, comparatively to <b>deviations</b> <b>d</b> and h, type of prevailing epistasis and direction of dominance...|$|R
40|$|A {{number of}} recent works have {{suggested}} that the period-luminosity (PL) relation for the Large Magellanic Cloud (LMC) Cepheids exhibits a controversial nonlinear feature with a break period at 10 days. Therefore, the aim of this Research Note is to test the linearity/nonlinearity of the PL relations for the LMC Cepheids in BVIcJHKs band, {{as well as in the}} Wesenheit functions. We show that simply comparing the long and short period slopes, together with their associate <b>d</b> standard <b>deviations,</b> leads to a strictly larger error rate than applying rigorous statistical tests such as the F-test. We applied various statistical tests to the current published LMC Cepheid data. These statistical tests include the F-test, the testimator test, and the Schwarz information criterion (SIC) method. The results from these statistical tests strongly suggest that the LMC PL relation is nonlinear in BVIcJH band but linear in the Ks band and in the Wesenheit functions. Using the properties of period-color relations at maximum light and multi-phase relations, we believe that the nonlinear PL relation is not caused by extinction errors. Comment: 6 pages, 5 figures and 2 tables, A&A accepte...|$|R
40|$|The {{objectives}} {{of this paper}} were to validate an improved model to describe failure to hatch by using data obtained from two hatches {{of a line of}} chickens and to examine the effect of hatch on the distribution for time of failure of an embryo to survive incubation. Breakout analysis of 11, 254 eggs that failed to hatch was used to characterize the distribution for time of failure to survive and the probability of failure to hatch. The distribution for time of failure to survive was modeled by a diphasic Weibull distribution, corresponding to the two phases of increased embryonic mortality during incubation. Distribution parameters for time of failure to survive were estimated by maximum likelihood and minimum Hellinger distance. Goodness-of-fit statistics validated the appropriateness of the diphasic Weibull distribution. Overall, the proportion of infertility was 0. 213, and the proportion of embryonic mortality by the end of incubation was 0. 086. Among embryos that suffered mortality during incubation, the proportion that died during Phase 1 was 0. 77; therefore, 0. 23 died during Phase 2. For Phase 1, mean time of mortality was 2. 6 <b>d,</b> and standard <b>deviation</b> was 3. 3 d. For Phase 2, mean time was 17. 4 <b>d,</b> and standard <b>deviation</b> was 2. 0 d. Time of mortality was distributed differently in the two hatches; this difference occurred mostly during Phase 1. Failure rates of the two hatches were different during the first 3 d of incubation. The model is useful to assess probability of failure to hatch and the distribution for time of failure to survive during incubation...|$|R
40|$|We give a new {{lower bound}} {{on the length}} of the minimal Steiner tree with a given {{topology}} joining given terminals in Euclidean space, in terms of toroidal images. The lower bound is equal to the length when the topology is full. We use the lower bound to prove bounds on the “error ” e in the length of an approximate Steiner tree, in terms of the maximum <b>deviation</b> <b>d</b> of an interior angle of the tree from 120 ◦. Such bounds are useful for validating algorithms computing minimal Steiner trees. In addition we give a number of examples illustrating features of the relationship between e and d, and make a conjecture which, if true, would somewhat strengthen our bounds on the error...|$|R
40|$|This {{paper is}} {{concerned}} with the characterization of the relationship between topology and traffic dynamics. We use a model of network generation that allows the transition from random to scale free networks. Specifically, we consider three different topological types of network: random, scale-free with γ = 3, scale-free with γ = 2. By using a novel LRD traffic generator, we observe best performance, in terms of transmission rates and delivered packets, in the case of random networks. We show that, even if scale-free networks are characterized by shorter characteristic-path- length (the lower the exponent, the lower the path-length), they show worst performances in terms of communication. We conjecture this could be explained in terms of changes in the load distribution, defined here as the number of shortest paths going through a given vertex. In fact, that distribu- tion is characterized by (i) a decreasing mean (ii) an increas- <b>ing</b> standard <b>deviation,</b> as the networks becomes scale-free (especially scale-free networks with low exponents). The use of a degree-independent server also discriminates against a scale-free structure. As a result, since the model is un- controlled, most packets will go through the same vertices, favoring the onset of congestion. Comment: 4 pages, 4 figures, included in conference proceedings ISCAS 2005, Kobe Japa...|$|R
5000|$|On {{the other}} hand, when n is small, the prior {{information}} is still {{relevant to the}} decision problem and affects the estimate. To see the relative weight of the prior information, assume that a=b; in this case each measurement brings in 1 new bit of information; the formula above shows that the prior information has the same weight as a+b bits of the new information. In applications, one often knows very little about fine details of the prior distribution; in particular, {{there is no reason}} to assume that it coincides with B(a,b) exactly. In such a case, one possible interpretation of this calculation is: [...] "there is a non-pathological prior distribution with the mean value 0.5 and the standard <b>deviation</b> <b>d</b> which gives the weight of prior information equal to 1/(4d2)-1 bits of new information." ...|$|R
40|$|An {{irregular}} contour of {{the medial}} femoral condyle (MFC) on {{magnetic resonance imaging}} (MRI) appears to indicate the severity of medial-type knee osteoarthritis (OA). The {{purpose of this study}} was to establish a system to enable objective assessments of OA knee severity using newly developed software that semi-automatically measures irregularity of the MFC. (1) We evaluated 48 patients aged 50 – 83  years with 55 knees of medial-type OA. The following scores were recorded: Lysholm score, visual analogue scale (VAS) and the Japanese Knee Osteoarthritis Measure (JKOM). MFC irregularity was automatically calculated by newly programmed computer software. Four parameters for condyle irregularity were calculated: (a) the average thickness of the contour (ATC), (b) the ratio of the upper surface length to the lower surface length of the contour (RUL), (c) average squared thickness of the contour (ASTC) and (<b>d)</b> standard <b>deviation</b> of the contour thickness (SDC). (2) Nine knees that underwent total knee arthroplasty were further analysed histopathologically and compared with irregularity score. Statistically, the RUL and SDC were significantly correlated with the Lysholm score, VAS and JKOM, with good reliability. Histological examinations showed that an irregular contour reflected the density of cystic lesions formed in subchondral bone. An irregularity of MFC on MRI is correlated with OA disease severity clinically and histopathologically. The new computer software is useful to objectively assess OA disease severity...|$|R
40|$|The {{structures}} of the natural zeolite garronite, NaCa 2. 5 Al 6 Si 10 O 32. 13 H 2 O, from Goble, Oregon, and Fara Vicentina, Italy, have been determined and refined by X-ray powder diffraction methods {{with the use of}} the full-profile Rietveld technique. The framework topology has the gismondine structure type (GIS), although the topological symmetry I 4 (1) /amd is lowered to I 4 BARm 2. Cell dimensions are Goble, a = 9. 9266 (2), c = 10. 3031 (3) angstrom; Fara Vicentina, a = 9. 8712 (2), c = 10. 2987 (3) angstrom. The space group assumed in the refinements accounts for the presence in the garronite powder pattern of reflections violating the <b>d</b> glide extinctions. <b>Deviation</b> from the maximum framework symmetry can be explained in terms of cation and H 2 O arrangements in the zeolitic cavities...|$|R
40|$|This paper {{examines}} {{the limits of}} transmission electron tomography reconstruction methods for a nanocomposite object composed of many closely packed nanoparticles. Two commonly used reconstruction methods in TEM tomography were examined and compared, and the sources of various artefacts were explored. Common visualization methods were investigated, and the resulting 2 ̆ 01 cinterpretation artefacts 2 ̆ 01 <b>d</b> (i. e., <b>deviations</b> from 2 ̆ 01 cactual 2 ̆ 01 d particle sizes and shapes arising from the visualization) were determined. Setting a known or estimated nanoparticle volume fraction as a criterion for thresholding does not in fact give a good visualization. Unexpected effects associated with common built-in image filtering methods were also found. Ultimately, this work set out to establish the common problems and pitfalls associated with electron beam tomographic reconstruction and visualization of samples consisting of closely spaced nanoparticlesPeer reviewed: YesNRC publication: Ye...|$|R
40|$|An {{optimized}} end pole {{compensation scheme}} for the 1. 8 Tesla, 25 -pole wiggler W 20 /SRRC was studied by using 2 D magnetostatic code OPERA- 2 <b>d.</b> The maximum <b>deviation</b> of the dipole steering for this wiggler can be minimized either by optimizing {{the sizes of}} the bias magnets or by optimizing the easy-axis rotating angles of the permanent rotators. After optimization, the dipole steering is within 100 Gauss-cm in the operating gap range so that a passive compensation scheme is possible even for an insertion device configured with very strong on-axis magnetic field strength. I. INTRODUCTION There are usually end correctors located {{at both ends of}} the insertion device to compensate the residual dipole steering due to the fringe magnetic field contributions as well as the magnetic field errors, which can be characterized by the first integral of the magnetic field (I 1) through the entire magnetic structure. Basically, the design goal of the end correction scheme could be guided in two directi [...] ...|$|R
40|$|Ichthyoplankton {{surveys were}} {{conducted}} in shelf and slope waters of the northern Gulf of Mexico during the months of May–September in 2005 and 2006 to investigate the potential role of this region as spawning and nursery habitat of sailfish (Istiophorus platypterus). During the two-year study, 2426 sailfish larvae were collected, {{ranging in size from}} 2. 0 to 24. 3 mm standard length. Mean density for all neuston net collections (n= 288) combined was 1. 5 sailfish per 1000 m 2, and maximum density was observed within frontal features created by hydrodynamic convergence (2. 3 sailfish per 1000 m 2). Sagittal otoliths were extracted from 1330 larvae, and otolith microstructure analysis indicated that the sailfish ranged in age from 4 to 24 days after hatching (mean= 10. 5 <b>d,</b> standard <b>deviation</b> [SD]= 3. 2 d). Instantaneous growth coefficients (g) among survey periods (n= 5) ranged from 0. 113 to 0. 127, and growth peaked during July 2005 collections when density within frontal features was highest. Daily instantaneous mortality rates (Z) ranged from 0. 228 to 0. 381, and Z was indexed to instantaneous weight-specific growth (G) to assess stage-specific production potential of larval cohorts. Ratios of G to Z were greater than 1. 0 for all but one cohort examined, indicating that cohorts were gaining biomass during the majority of months investigated. Stage-specific production potential, in combination with catch rates and densities of larvae, indicates that the Gulf of Mexico likely represents important spawning and nursery habitat for sailfish...|$|R
40|$|The {{present study}} focused on the {{application}} of the Anaerobic Digestion Model 1 on the methane production from acidified sorghum extract generated from a hydrogen producing bioreactor in a two-stage anaerobic process. The kinetic parameters for hydrogen and volatile fatty acids consumption were estimated through fitting of the model equations to the data obtained from batch experiments. The simulation of the continuous reactor performance at all HRTs tested (20, 15, and 10. d) was very satisfactory. Specifically, the largest deviation of the theoretical predictions against the experimental data was 12 % for the methane production rate at the HRT of 20. <b>d</b> while the <b>deviation</b> values for the 15 and 10. d HRT were 1. 9 % and 1. 1 %, respectively. The model predictions regarding pH, methane percentage in the gas phase and COD removal were in very good agreement with the experimental data with a deviation less than 5 % for all steady states. Therefore, the ADM 1 is a valuable tool for process design {{in the case of a}} two-stage anaerobic process as well. © 2011 Elsevier Ltd...|$|R
40|$|This paper {{examines}} the e ect of macroeconomic variable volatility on implied and realized asset price level volatilities in the U. S. using monthly data from 1986 - 2008. Two approaches are taken: An autoregressive distributed lag model us- <b>ing</b> rolling standard <b>deviations</b> and a GARCH model. The S&P 500 's volatility {{is used as}} a proxy for historical (actual) volatility and the VIX {{is used as a}} proxy for implied volatility. For the distributed lag model, each linear regression tests granger causality (using Newey-West robust standard errors) of a single macroeconomic variable by in- corporating lagged values (as determined by comparing Bayesian Information Criteria of both the constructed macroeconomic variable and the dependent asset volatility variable). Capacity utilization, PPI, and employment volatility are found to be sig- ni cant for predicting S&P volatility, while PPI and M 2 volatility are signi cant for the VIX. For the GARCH regressions, terms of trade, employment, and capacity uti- lization volatility are statistically signi cant. Forecasts are then constructed using those variables shown to be granger casual, but a two-sided t-test rejects the null hypothesis that forecast errors are zero in every case. Honors thesis, Department of Economic...|$|R
40|$|Using {{surface and}} aerological {{meteorological}} observations obtained at the Xisha Automatic Weather Station and three moored buoys along the continental slope, {{characteristics of the}} synoptic-scale disturbances over the northern South China Sea (NSCS) are extensively studied. The power spectra of surface and aerological observations suggest a synoptic feature with a pronounced energy peak at a period of 5 - 8 d and a weak peak at 3 - 4 <b>d.</b> The standard <b>deviation</b> of the synoptic temperature component derived at Xisha Station from 1976 to 2011 indicates that the strongest variability normally exists in August all through the whole troposphere. At the interannual scale, {{it is found that}} El Nio {{plays an important role in}} regulating the synoptic disturbances of atmosphere. The vertical synoptic disturbances have a double active peak following El Nio condition. The first peak usually occurs during the mature phase of El Nio, and the second one occurs in the summer of decay year. Comparing with the summer of developing years, the summer of the decaying year of El Nio has more active and stronger synoptic disturbances, especially for the 5 - 8 d period variations...|$|R
40|$|This paper {{develops}} a multi-industry growth {{model in which}} firms require external funds to conduct productivity-enhancing R&D. The cost of research is industry-specific. The tightness of financing constraints depends {{on the level of}} financial development and on industry characteristics. Over time, a financially constrained economy may converge to the growth path of a frictionless economy, so long as an industry with the fastest expanding technological frontier does not permanently fall behind due to low R&D. The model’s industry dynamics map into a differences-in-differences regression, in which industry growth depends on the interaction between financial development and industry level R&D intensity. Economic growth;Economic models;External sector;Industrial sector;Production;Productivity;r & d, r & d intensity, equation, r & d intensive industries, standard errors, survey, r & d spending, correlations, correlation, statistical significance, research spending, research activity, statistics, r & d-intensive industries, r & d investment, prediction, predictions, r & d expenditures, empirical validity, r & d expenditure, outliers, amount of r & d, research expenditures, standard deviations, counting, faces of r & d, research labs, research and development, research lab, industry r & d, total r & d spending, functional form, empirical specification, r & d investments, number of researchers, cross-country variation, absorptive capacity, r & d share, financial statistics, industry research, r & <b>d</b> activity, standard <b>deviation,</b> verifiability...|$|R
40|$|A {{new global}} {{analysis}} of methane {{lines in the}} 900 - 4800 cm^- 1 region has been performed thanks to new experimental data for both line positions and intensities. This implies three of the 12 CH 4 polyads, namely the dyad (940 - 1850 cm^- 1, 2 vibrational levels, 2 sublevels), the pentad (2150 - 3350 cm^- 1, 5 vibrational levels, 9 sublevels) and the octad (3550 - 4800 cm^- 1, 8 vibrational levels, 24 sublevels) {{and some of the}} associated hot bands. New FTIR spectra of the pentad and octad regions have been recorded with a very high resolution (better than 0. 001 cm^- 1 instrumental bandwidth, unapodized) at 78 K using the Bruker IFS 125 HR Zrich prototype (ZP 2001) spectrometer 1. New intensity measurements were performed in the whole region at the Kitt Peak National Observatory. We also used previously recorded high-resolution Raman spectra 2. The effective Hamiltonian was expanded up to order 6 for the ground state, 6 for the dyad, 5 for the pentad and 5 for the octad. We obtain global root mean square <b>deviations</b> <b>d</b> RMS for line positions = 1. 4 œ 10 ^- 4 cm^- 1 for the dyad, 6. 0 œ 10 ^- 4 cm^- 1 for the pentad and 3. 3 œ 10 ^- 3 cm^- 1 for the octad. This analysis represents a large improvement over the previous one 3 with d RMS = 0. 041 cm^- 1 for the octad system...|$|R
40|$|The {{concentration}} addition (CA) and {{the independent}} action (IA) models are widely used for predicting mixture toxicity based on its composition and individual component dose–response profiles. However, the prediction based on these models may be inaccurate due to interaction among mixture components. In this work, the nature and prevalence of non-additive effects were explored for binary, ternary and quaternary mixtures composed of hydrophobic organic compounds (HOCs). The toxicity of each individual component and mixture was determined using the Vibrio fischeri bioluminescence inhibition assay. For each combination of chemicals specified by the 2 n factorial design, the percent deviation of the predicted toxic effect from the measured value was used to characterize mixtures as synergistic (positive deviation) and antagonistic (negative deviation). An arbitrary classification scheme was proposed based on the magnitude of <b>deviation</b> (<b>d)</b> as: additive (⩽ 10 %, class-I) and moderately (10 50 %, class-IV) antagonistic/synergistic. Naphthalene, n-butanol, o-xylene, catechol and p-cresol led to synergism in mixtures while 1, 2, 4 -trimethylbenzene and 1, 3 -dimethylnaphthalene contributed to antagonism. Most of the mixtures depicted additive or antagonistic effect. Synergism was prominent {{in some of the}} mixtures, such as, pulp and paper, textile dyes, and a mixture composed of polynuclear aromatic hydrocarbons. The organic chemical industry mixture depicted the highest abundance of antagonism and least synergism. Mixture toxicity was found to depend on partition coefficient, molecular connectivity index and relative concentration of the components. © Elsevie...|$|R
30|$|Figure 8 (b) {{gives the}} local time and {{seasonal}} variations of percent deviations between IRI-URSI foF 2 (solid circle) and IRI-CCIR foF 2 (open circle) and the measured foF 2 during solar maximum of 1990 for low magnetic activity. Figure 8 (b) indicates clearly that both IRI options for critical frequency of F 2 -layer {{appear to be}} less accurate for equatorial region in Africa. Again, in January the deviation curve shows a relatively sharp increase in foF 2 with typical value near 125 % for URSI model, indicating that IRI-URSI option overpredicts foF 2 data by that significant amount at sunrise. The URSI model percent <b>deviation,</b> <b>d</b> foF 2 shows strong seasonal changes with largest value found in December solstice (0 – 124 %), and lowest value occurred in June solstice (3 – 65 %) with equinoxes (April: 0 – 70 %, October: 0 – 67 %) lying between the solstices extreme, whereas CCIR model indicates largest value in June solstice (July: 2 – 76 %), December solstice (January: 9 – 70 %), and March equinox (April: 0 – 71 %), with the smallest value seen in September equinox (October: 0 – 67 %). On average, absolute deviation of modeled foF 2 from observational data ranging from 0 – 80 % and ∼ 3 – 70 % for URSI and CCIR model, respectively for 1990 high solar activity year. Putting Figs. 8 (a) and 8 (b) together, we infer that overall deviations during solar minimum and maximum years are comparable for URSI option, but are marginally difference for CCIR model.|$|R
