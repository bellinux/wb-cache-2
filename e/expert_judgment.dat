1119|530|Public
25|$|Blink: The Power of Thinking Without Thinking (2005) is Malcolm Gladwell's second book. It {{presents}} {{in popular}} science format research from psychology and behavioral economics on the adaptive unconscious: mental processes that work rapidly and automatically from relatively little information. It considers both {{the strengths of}} the adaptive unconscious, for example in <b>expert</b> <b>judgment,</b> and its pitfalls, such as stereotypes.|$|E
2500|$|Table SPM-2 lists {{recent trends}} along with {{certainty}} levels for the trend having actually occurred, {{for a human}} contribution to the trend, and for the trend occurring in the future. In relation to changes (including increased hurricane intensity) where the certainty of a human contribution is stated as [...] "more likely than not" [...] footnote f to table SPM-2 notes [...] "Magnitude of anthropogenic contributions not assessed. Attribution for these phenomena based on <b>expert</b> <b>judgment</b> rather than formal attribution studies." ...|$|E
2500|$|This treaty {{provision}} is broadly {{in line with}} the United States Copyright Act and the Act's case law, which protects compilations of data whose [...] "selection and arrangement" [...] is sufficiently original. See [...] ("compilation" [...] as defined by the United States Copyright Act includes compilations of data). The standard for such originality is fairly low; for example, business listings have been found to meet this standard when deciding which companies should be listed and categorizing those companies required some kind of <b>expert</b> <b>judgment.</b> See Key Publ'ns, Inc. v. Chinatown Today Pub. Enters., 945 F.2d 509 (2d Cir. 1991) (applying Feist). As such, implementation of this treaty would not overrule Feist.|$|E
40|$|Previous {{research}} on the arts, entertainment, and other cultural objects has found, at most, a weak link between <b>expert</b> <b>judgments</b> of aesthetic excellence and audience ap-peal to nonexpert consumers. However, this tendency for audience appeal only weakly to reflect <b>expert</b> <b>judgments</b> of excellence {{raises the question of}} how this fragile relation-ship might be mediated by audience judgments of excel-lence. As the first study to examine the potential inter-vening role of audience judgments, the present article investigates the links between <b>expert</b> <b>judgments,</b> audience judgments, and audience appeal in an illustrative case based on 200 recordings of the song “My Funny Valen-tine. ” The results support a scenario in which audience ap-peal is weakly related to <b>expert</b> <b>judgments</b> through the hitherto neglected intervening role of audience judgments so as to suggest refinements in our approaches to mar-keting entertainment, the arts, or other cultural offerings, as well as various consumer services, durables, or nondurables...|$|R
40|$|<b>Expert</b> <b>judgments</b> {{have been}} worse {{than those of the}} {{simplest}} statistical models in virtually all domains that have been studied” (Camerer and Johnson, 1991) “In nearly every study of experts carried out within the judgment and decision-making approach, experience {{has been shown to be}} unrelated to the empirical accuracy of <b>expert</b> <b>judgments...</b>|$|R
40|$|<b>Expert</b> <b>judgments</b> {{are often}} used to {{estimate}} likelihood values in a security risk analysis. These judgments are subjective and their correctness rely on the competence, training, and experience of the experts. Thus, {{there is a need}} to validate the correctness of the estimates obtained from <b>expert</b> <b>judgments.</b> In this paper we report on experiences from a security risk analysis where indicators were used to validate likelihood estimates obtained from <b>expert</b> <b>judgments.</b> The experiences build on data collected during the analysis and on semi-structured interviews with the client experts who participated in the analysis. Oppdragsgiver: Research Council of Norwa...|$|R
2500|$|Although {{environmental}} groups have challenged {{a number of}} incidental take permits in court, judges typically defer to the <b>expert</b> <b>judgment</b> of the FWS. [...] Sierra Club v. Babbitt {{was one of the}} few cases where the plaintiffs won. The Chevron deference rule (see Chevron U.S.A., Inc. v. Natural Resources Defense Council, Inc.), in which the Supreme Court holds that courts should defer to agency interpretations of such statutes unless they are unreasonable, was used in this case. The environmental statutes were clear and unambiguous. However, the FWS interpretation of the laws were not reasonable. This sets the stage for regulatory federal agencies being held accountable for noncompliance of environmental statutes and irrational decision making.|$|E
2500|$|But {{how does}} Deleuze square his pessimistic diagnoses with his ethical naturalism? Deleuze claims that {{standards}} of value are internal or immanent: to live well is to fully express one's power, {{to go to}} the limits of one's potential, rather than to judge what exists by non-empirical, transcendent standards. Modern society still suppresses difference and alienates persons from what they can do. To affirm reality, which is a flux of change and difference, we must overturn established identities and so become all that we can become—though we cannot know what that is in advance. The pinnacle of Deleuzean practice, then, is creativity. [...] "Herein, perhaps, lies the secret: to bring into existence and not to judge. If it is so disgusting to judge, it is not because everything is of equal value, but on the contrary because what has value can be made or distinguished only by defying judgment. What <b>expert</b> <b>judgment,</b> in art, could ever bear on the work to come?" ...|$|E
2500|$|Researcher, Andrew Tutt, {{argues that}} {{algorithms}} should be overseen by a specialist regulatory agency, similar to FDA. His academic work {{emphasizes that the}} rise of increasingly complex algorithms calls for the {{need to think about}} the effects of algorithms today. Due to the nature and complexity of algorithms, it will prove to be difficult to hold algorithms accountable under criminal law. Tutt recognizes that while some algorithms will be beneficial to help meet technological demand, others should not be used or sold if they fail to meet safety requirements. Thus, for Tutt, algorithms will require [...] "closer forms of federal uniformity, <b>expert</b> <b>judgment,</b> political independence, and pre-market review to prevent the introduction of unacceptably dangerous algorithms into the market". The issue of algorithmic accountability (the responsibility of algorithm designers to provide evidence of potential or realised harms) is of particular relevance in the field of dynamic and non-linearly programmed systems, e.g. artificial neural networks, deep learning, and genetic algorithms (see Explainable AI).|$|E
30|$|Mangasarian 1965 first {{employed}} {{linear programming}} in classification prediction. For credit scoring, Vladimir et al. 2002 constructed a quadratic programming model for individual credit that included <b>expert</b> <b>judgments.</b> The numerical experiments {{showed that the}} evaluation model incorporating <b>expert</b> <b>judgments</b> can improve model performance. Kou et al. 2005 proposed a classification model using multi-criteria linear programming to discover behavior patterns of credit card applicants.|$|R
40|$|Accounts of {{arguments}} from expert opinion {{take it for}} granted that <b>expert</b> <b>judgments</b> are reliable, and so an argument that proceeds from premises about what an expert judges to a conclusion that the expert is probably right is a strong argument. In my (2013), I considered a potential justification for this assumption, namely, that <b>expert</b> <b>judgments</b> {{are more likely to be}} true than novice judgments, and discussed empirical evidence suggesting that <b>expert</b> <b>judgments</b> are not more reliable than novice judgments or even chance. In this paper, I consider another potential justification for this assumption, namely, that <b>expert</b> <b>judgments</b> are not influenced by the kinds of cognitive biases novice judgments are influenced by, and discuss empirical evidence suggesting that experts are vulnerable to pretty much the same kinds of cognitive biases as novices. If this is correct, then the basic assumption at the core of accounts {{of arguments}} from expert opinion remains unjustified...|$|R
40|$|Most {{models of}} {{aggregating}} <b>expert</b> <b>judgments</b> {{assume that there}} is available some infor-mation characterizing the experts. This information may be incorporated into hierarchical uncertainty models (second-order models). However, very often {{we do not know}} anything about experts or it is difficult to evaluate their quality. In this case, beliefs to experts may be in the interval [0, 1] and the resulting assessments become to be non-informative. More-over, attempts to assign some weights or beliefs to experts were not crowned with success because the behavior of experts may be distinguished in different circumstances. Therefore, this paper proposes to estimate <b>expert</b> <b>judgments</b> instead of <b>experts</b> themselves and studies how to assign interval probabilities of <b>expert</b> <b>judgments</b> by using the multinomial model...|$|R
60|$|Yes, as I {{look back}} upon it, a man must be greatly a philosopher {{to survive the}} continual impact of such brutish {{experiences}} through the years and years. I am such a philosopher. I have endured eight years of their torment, and now, in the end, failing {{to get rid of}} me in all other ways, they have invoked the machinery of state to put a rope around my neck and shut off my breath by the weight of my body. Oh, I know how the experts give <b>expert</b> <b>judgment</b> that the fall through the trap breaks the victim's neck. And the victims, like Shakespeare's traveller, never return to testify to the contrary. But we who have lived in the stir know of the cases that are hushed in the prison crypts, where the victim's necks are not broken.|$|E
5000|$|EXCALIBUR (website) {{software}} for processing <b>expert</b> <b>judgment</b> data with the classical model. Freely available.|$|E
5000|$|Tools: <b>Expert</b> <b>Judgment</b> Collections, Alternative Analysis, Publishing {{estimating}} data, Project management software implementation, Bottom up estimating ...|$|E
40|$|RGMM, ???? ??????? ?????????? ????????? ?????????? ?????????? ??????????? ?????? ?? ???? ? ?????????? ??????? ?????? ????????? ? ?????????? ????????? ???????????? ??????? ?????????? ?????? ?????? ????????? ?? ????? ??????? ??????. ???????? ???????????? ??????? ??? ?????????? ????????? ????????? ??????? ?????? ????????? (?????? ?????????) ???? ????? ?????????? ?????????? ??????????? ??????. ?????????? ????????? ?????????, ?? ?????????? ?????? ???????? ???????? ??????, ??? ? ??????????? ???????? ?????? ?????????, ????????? ?? ????? ?????????? ?????????? ???????????, ?? ??????????? ????????, ?? ???????????????? ?????????? ??????????????. The {{pairwise}} comparison method {{is used to}} solve poorly structured problems of decision making, for the calculation of relative weights of decision alternatives in terms of quality characteristic (decision criterion) {{on the basis of}} <b>expert</b> <b>judgments</b> of alternatives. In this paper, a method for estimating the stability of local weights of decision alternatives is developed. This method is used when local weights are calculated {{on the basis of the}} RGMM. The developed method includes: an estimation of the stability of the local ranking of decision alternatives to changes in <b>expert</b> {{pairwise comparison}} <b>judgments</b> and an estimation of the stability of consistency of <b>expert</b> pairwise comparison <b>judgments</b> to a change of a single judgment. The formulas are devised for calculating the stability intervals of <b>expert</b> pairwise comparison <b>judgments</b> as to changes of local ranking of decision alternatives. Stability intervals are proposed for finding critical elements of a decision-making problem. These critical elements are <b>expert</b> <b>judgments</b> that are sensitive to changes of a local ranking of alternatives and the most inconsistent <b>expert</b> <b>judgments.</b> ?????????? ????? ?????????? ???????????? ????????? ????? ??????????? ??????? ?? ???????????? ?????????????? ?? ?????? ?????? ?????? ????????? RGMM, ?????????? ?????????? ???????????? ?????????? ???????????? ??????????? ??????? ? ?????????? ? ?????????? ??????? ?????? ????????? ? ?????????? ???????????? ??????????????? ????????? ?????????? ?????? ?????? ????????? ? ????????? ????????? ??????. ???????? ????????? ??????? ??? ?????????? ???????????? ????????? ??????? ?????? ????????? (?????? ?????????) ?????????? ????????? ?????????? ???????????? ??????????? ???????. ????????? ????????? ????????????, ??????????? ????? ??????????? ???????? ??????, ??????? ???????? ??????????? ???????? ?????? ?????????, ??????????????? ? ?????????? ?????????? ???????????? ???????????, ? ??????????? ????????, ???????????????? ?????????? ??????????????????...|$|R
5000|$|Some authors {{argue that}} the key process taking place in {{decision}} conferencing is the behavioral aggregation of <b>expert</b> <b>judgments</b> ...|$|R
40|$|If {{we define}} “good taste” as that {{prescribed}} by professional experts {{in a particular}} cultural field and ask whether ordinary consumers (non-experts or members of the mass audience) have “good taste,” the evidence from previous studies suggests {{that the relationship between}} <b>expert</b> <b>judgments</b> and popular appeal to ordinary consumers is significantly but only weakly positive and is therefore consistent with a phenomenon of “little taste. ” Possible explanations stem from the consideration of a variable that might mediate and thereby weaken the relationship between <b>expert</b> <b>judgments</b> and popular appeal—namely, ordinary evaluations, in which non-expert consumers assess the excellence (rather than the enjoyability) of a cultural offering. An earlier experimental study of musical performances showed that ordinary evaluations did intervene between <b>expert</b> <b>judgments</b> and popular appeal to college students so that, in this sense, ordinary consumers did display aspects of “good taste”. New data on over 200 motion pictures corroborate this finding in another cultural context, with actual audience members, and through the use of real-world as opposed to experimental observations. Copyright Springer Science + Business Media, Inc. 2005 <b>expert</b> <b>judgments,</b> ordinary evaluations, popular appeal, professional critics, mass audiences, motion pictures, consumer tastes,...|$|R
5000|$|Tools: <b>Expert</b> <b>judgment</b> collection, {{analogous}} estimating, parametric estimating, Bottom up Estimation, Two-Point estimation, Three-point estimation, reserve analysis ...|$|E
50|$|The best {{argument}} for validation of <b>expert</b> <b>judgment</b> is the <b>expert</b> <b>judgment</b> data itself. Whereas the pre-2006 data contains wide variations in numbers of experts {{and numbers of}} calibration variables, the 33 independent professionally contracted post-2006 elicitations are more uniform in design, better resourced, better documented and better lend themselves to aggregate presentation. The data comprise in total 320 experts. Figure 1 shows the distribution of experts over the number of assessed calibration variables.|$|E
5000|$|Whether the TSA's <b>expert</b> <b>judgment</b> to {{withhold}} SSI from civil litigants and their attorneys constitutes [...] "actions committed to agency discretion by law" ...|$|E
30|$|Existing {{indicators}} are either quantitative when they reflect quantities, {{that can be}} actually measured on the final running system, or qualitative if they results from subjective <b>expert</b> <b>judgments.</b>|$|R
40|$|Abstract—Expert {{judgments}} {{are often used}} to estimate likelihood values in a security risk analysis. These {{judgments are}} subjective and their correctness rely on the competence, training, and experience of the experts. Thus, {{there is a need}} to validate the correctness of the estimates obtained from <b>expert</b> <b>judgments.</b> In this paper we report on experiences from a security risk analysis where indicators were used to validate likelihood estimates obtained from <b>expert</b> <b>judgments.</b> The experiences build on data collected during the analysis and on semi-structured interviews with the client experts who participated in the analysis. Keywords-security risk analysis; expert judgment; indicator I...|$|R
40|$|<b>Expert</b> <b>judgments</b> are a {{necessary}} part of environmental management. Typically, experts are defined by their qualifications, track record, professional standing, and experience. We outline the limitations of conventional definitions of expertise and describe how these requirements can sometimes exclude people with useful knowledge. The frailties and biases in <b>expert</b> <b>judgments</b> can interact with the social status afforded to <b>experts</b> to produce <b>judgments</b> that are both unassailable and wrong. Several approaches may improve the rigor of expert judgments; they include widening the set of experiences and skills involved in deliberations, employing structured elicitation, and making experts more accountable through testing and training. We outline the most serious impediments to the routine deployment of these tools, and suggest protocols that would overcome these hurdles. © 2011 Wiley Periodicals, Inc...|$|R
5000|$|Stein {{was awarded}} the University Medal, the highest honor of the Case Western Reserve University, for his {{leadership}} and <b>expert</b> <b>judgment</b> in guiding the University through the turbulent sixties.|$|E
5000|$|Burke regards {{failure to}} use a model (instead over-relying on <b>expert</b> <b>judgment)</b> {{as a type of}} model risk. Derman {{describes}} various types of model risk that arise from using a model: ...|$|E
5000|$|<b>Expert</b> <b>Judgment</b> (EJ) denotes a {{wide variety}} of {{techniques}} ranging from a single undocumented opinion, through preference surveys, to formal elicitation with external validation of expert probability assessments. Recent books are ...|$|E
40|$|The classification, modeling, and {{quantification}} {{of human}} errors in routine chemical analysis are described. Classifications include commission errors (mistakes and violations) and omission errors (lapses and slips) in different scenarios at different {{steps of the}} chemical analysis. A Swiss cheese model is used to characterize error interaction with a laboratory quality system. The quantification of human errors in chemical analysis, based on <b>expert</b> <b>judgments,</b> i. e. on the expert(s) knowledge and experience, is applied. A Monte Carlo simulation of the <b>expert</b> <b>judgments</b> {{was used to determine}} the distributions of the error quantification scores (scores of likelihood and severity, and scores of effectiveness of a laboratory quality system against the errors). Residual risk of human error after the error reduction by the laboratory quality system and consequences of this risk for quality and measurement uncertainty of chemical analytical results are discussed. Examples are provided using <b>expert</b> <b>judgments</b> on human errors in pH measurement of groundwater, multi-residue analysis of pesticides in fruits and vegetables, and elemental analysis of geological samples by inductively coupled plasma mass spectrometry...|$|R
30|$|Our {{contribution}} {{is a set}} of three robustness indicators, tailored for cloud-based systems, which require minimal knowledge of the systems and do not involve <b>expert</b> <b>judgments.</b> In addition, we provide Trio, an experimental tool to compute them on cloud-topologies.|$|R
5000|$|Jung Jin Lee, Paul B Kantor. A Study of Probabilistic Information Retrieval Systems in the Case of Inconsistent <b>Expert</b> <b>Judgments.</b> Journal of the American Society for Information Science (1986-1998). New York: Apr 1991. Vol. 42, Iss. 3; p. 166.|$|R
5000|$|..... <b>expert</b> <b>judgment</b> {{is not a}} {{substitute}} for definitive scientific research. Nor is it {{a substitute}} for careful deliberative expert reviews of the literature of the sort undertaken by the IPCC. However, its use within such review processes could enable a better expression of the diversity of <b>expert</b> <b>judgment</b> and allow more formal expression of expert judgments, which are not adequately reflected, in the existing literature. It can also provide insights for policy makers and research planners while research to produce more definitive results is ongoing. It is for these reasons that Moss and Schneider have argued that such elicitations should become a standard input to the IPCC assessment process ...|$|E
50|$|Since {{experts are}} invoked when {{quantities}} of interest are uncertain, {{the goal of}} structured <b>expert</b> <b>judgment</b> is a defensible quantification of uncertainty. Confronted with uncertainty, society at large will always harken to prophets, oracles, pundits, blue ribbon panels, crowd wisdom reputed to have performed well in the past. Scientists and engineers, in contrast, are typically averse to any methodology which eschews empirical validation. Most invocations of <b>expert</b> <b>judgment</b> do not attempt any form of validation, as if the predicate “expert” were validation enough. The classical model’s emphasis on validation is its distinguishing feature. Virtually all validation data with real experts and real applications (as opposed to academic exercises) has been generated by practitioners with the classical model.|$|E
5000|$|... 25 of the 33 {{studies have}} at least one, and usually two or more experts whose {{statistical}} accuracy is acceptable. Simply identifying those experts and relying on them {{would be a big}} improvement over un-validated <b>expert</b> <b>judgment</b> (spotting good performers without measuring performance is a fool’s errand.|$|E
50|$|To {{increase}} the reliability in <b>expert</b> <b>judgments,</b> the literature provides criteria {{and procedures for}} eliciting this information while {{reducing the risk of}} obtaining biased judgments (e.g., Cooke and Goossens, Ayyub, Garthwaite et al., Goossens et al., Kynn, Hallowell and Gambatese, and Cardenas et al.|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedThis thesis models {{the interaction of}} nonlinear relationships based upon gathered <b>expert</b> <b>judgments.</b> The model developed reproduces {{a portion of the}} military expert's mission assignment decision-making process. Specifically, this thesis illustrates a method of combining the influences of EXPERIENCE, LOGISTICS, PREPARATION TIME, CONTINUOUS OPERATIONS, MISSION, ENEMY, TERRAIN TYPE, VISIBILITY, ENGAGEMENT RANGES and TRAFFICABILITY with varying brigade task organizations in order to identify the most mission ready brigade based upon <b>expert</b> military <b>judgment</b> for use within a theater level simulation. The model produced by this study uses the Analytic Hierarchy Process (AHP) to obtain <b>expert</b> military <b>judgments</b> through relative scale pairwise comparison techniques and to recreate the results of those judgments. [URL] United States Marine Corp...|$|R
40|$|This study {{compares the}} results of two {{nationwide}} land degradation assessments for Senegal. The first approach is based on <b>expert</b> <b>judgments,</b> the second on trend analyses of Rainfall Use Efficiency (RUE). The comparison yields some interpretable results for extreme negative RUE slope values with maximum degrees of land degradation as indicated by experts, yet, overall, the correlation between both approaches proves to be low and without clear sign. As RUE trend analysis suffers from several inherent methodological problems and <b>expert</b> <b>judgments</b> {{are found to be}} consistent we consider expert opinions as the preferable option for a nation-wide land degradation assessment in Senegal. Yet, we argue that methodological improvements would increase the explanatory power of the expert approach and consolidate its position in a policy making framework. </p...|$|R
