259|951|Public
50|$|This {{test can}} be used to {{determine}} whether two independent samples were selected from populations having the same distribution; a similar nonparametric test used on <b>dependent</b> <b>samples</b> is the Wilcoxon signed-rank test.|$|E
5000|$|The Wilcoxon signed-rank test is a {{non-parametric}} {{statistical hypothesis}} test used when comparing two related samples, matched samples, or repeated measurements {{on a single}} sample to assess whether their population mean ranks differ (i.e. it is a paired difference test). It {{can be used as}} an alternative to the paired Student's t-test, t-test for matched pairs, or the t-test for <b>dependent</b> <b>samples</b> when the population cannot be assumed to be normally distributed. A Wilcoxon signed-rank test is a nonparametric test that can be used to determine whether two <b>dependent</b> <b>samples</b> were selected from populations having the same distribution.|$|E
50|$|The Mann-Whitney U test / Wilcoxon rank-sum test is not {{the same}} as the Wilcoxon signed-rank test, {{although}} both are nonparametric and involve summation of ranks. The Mann-Whitney U test is applied to independent samples. The Wilcoxon signed-rank test is applied to matched or <b>dependent</b> <b>samples.</b>|$|E
30|$|Pre- and {{postoperative}} measurements were compared using <b>dependent</b> <b>sample</b> t test for continuous data. p values of < 0.05 were considered statistically significant.|$|R
30|$|The {{proof is}} {{analogous}} to that of Theorem  3 in [8] except {{that we need to}} use the following Lemma  3.1 for the <b>dependent</b> <b>sampling</b> setting to replace Lemma  2 in [8].|$|R
40|$|License, which permits {{unrestricted}} use, distribution, {{and reproduction}} in any medium, provided the original work is properly cited. The variable sampling rate system is encountered in many applications. When the speed information {{is derived from}} the position marks along the trajectory, one would have a speed <b>dependent</b> <b>sampling</b> rate system. The conventional fixed or multisampling rate system theory may not work in these cases because the system dynamics include the uncertainties which resulted from the variable sampling rate. This paper derived a convenient expression for the speed <b>dependent</b> <b>sampling</b> rate system. The varying sampling rate effect is then translated into multiplicative uncertainties to the system. The design then uses the popula...|$|R
5000|$|The {{implementation}} of logical conditions in R can provide text elements for the dynamic report {{depended on the}} statistical analysis.The following text is as stan The Wilcoxon Sign test was applied as statistical comparison of the average of two <b>dependent</b> <b>samples</b> above. [...] In this case the the calculated P-value was 0.56 and hence greater than the significance (0.05 by default). This implies that [...] "H0: {{there is no difference}} between the [...] results in data1 and data2" [...] must be accepted. Depending on the R results (here 0.56) the text fragments are determined by logical conditions in the R-script. If the P-value was 0.45, which is lower than the significance (0.05 by default). An other appropriate text fragment is inserted in the dynamic report. By this workflow the replacement of the input data of the statistical or numerical analysis in R creates a reproducible report which the same methodology.|$|E
50|$|A {{government}} {{experiment to}} test ventilatory threshold was held between November and December 2004. Subjects included 32 physically active males (age: 22.3; TV: 180.5; TM: 75.5 kg; VO2max: 57.1 mL/kg/min) encountered a continuous test of increasing loads on a treadmill, cardiorespiratory and other variables were observed using ECG (recording of the electrical {{activity of the}} heart) and gas analyzer. During the test, {{subjects were asked to}} point at a scale from 6 to 20 reflecting their feeling of discomfort. The RPE threshold was recorded as constant value of 12-13. Averages of ventilatory and RPE threshold were conveyed by parameters that were monitored and then compared by using t-test for <b>dependent</b> <b>samples.</b> No significant difference was found between mean values of ventilatory and RPE threshold, when they were expressed by parameters such as: speed, load, heart rate, absolute and relative oxygen consumption. The conclusion of this experiment was: the fixed value (12-13) of RPE scale may be used to detect the exercise intensity that corresponds to ventilatory threshold.|$|E
50|$|ANOVA on ranks {{means that}} a {{standard}} analysis of variance is calculated on the rank-transformed data. Conducting factorial ANOVA on the ranks of original scores has also been suggested. However, Monte Carlo studies, and subsequent asymptotic studies found that the rank transformation is inappropriate for testing interaction effects in a 4x3 and a 2x2x2 factorial design. As the number of effects (i.e., main, interaction) become non-null, and as {{the magnitude of the}} non-null effects increase, there is an increase in Type I error, resulting in a complete failure of the statistic with as high as a 100% probability of making a false positive decision. Similarly, {{it was found that the}} rank transformation increasingly fails in the two <b>dependent</b> <b>samples</b> layout as the correlation between pretest and posttest scores increase. It was also discovered that the Type I error rate problem was exacerbated in the context of Analysis of Covariance, particularly as the correlation between the covariate and the dependent variable increased.|$|E
40|$|Let X 1, X 2, [...] ., Xn be identically {{distributed}} possibly dependent {{random variables}} with finite pth absolute moment assumed {{without loss of generality}} {{to be equal}} to 1. Denote the order statistics by X 1 :n, X 2 :n, [...] ., Xn:n. Bounds are derived for E(Xn:n) when it is assumed that the Xi's are (i) arbitrarily dependent and (ii) independent. The effect of assuming a symmetric common distribution for the Xi's is discussed. Analogous bounds are described for the expected range of the sample. Bounds on expectations of general linear combinations of order statistics are described in the independent case. maximal dependence order statistics maximum of <b>dependent</b> <b>sample</b> range of <b>dependent</b> <b>sample...</b>|$|R
40|$|Existing {{methods for}} the {{estimation}} of stable distribution parameters, such as those based on sample quantiles, sample characteristic functions or maximum likelihood generally assume an independent sample. Little {{attention has been paid}} to estimation from a <b>dependent</b> <b>sample.</b> In this paper, a method for {{the estimation of}} stable distribution parameters from a <b>dependent</b> <b>sample</b> is proposed based on the sample quantiles. The estimates are shown to be asymptotically normal. The asymptotic variance is calculated for stable moving average processes. Simulations from stable moving average processes are used to demonstrate these estimators. Comment: 18 pages, 1 figure. Most of this paper is included in chapter 3 of my PhD thesis, which is yet to be submitte...|$|R
40|$|The {{variable}} {{sampling rate}} system is encountered in many applications. When the speed information {{is derived from}} the position marks along the trajectory, one would have a speed <b>dependent</b> <b>sampling</b> rate system. The conventional fixed or multisampling rate system theory may not work in these cases because the system dynamics include the uncertainties which resulted from the variable sampling rate. This paper derived a convenient expression for the speed <b>dependent</b> <b>sampling</b> rate system. The varying sampling rate effect is then translated into multiplicative uncertainties to the system. The design then uses the popular μ-synthesis process to achieve a robust performance controller design. The implementation on a BLDC motor demonstrates the effectiveness of the design approach...|$|R
30|$|We {{consider}} the moving least-squares (MLS) method by the regression learning framework {{under the assumption}} that the sampling process satisfies the α-mixing condition. We conduct the rigorous error analysis by using the probability inequalities for the <b>dependent</b> <b>samples</b> in the error estimates. When the <b>dependent</b> <b>samples</b> satisfy an exponential α-mixing, we derive the satisfactory learning rate and error bound of the algorithm.|$|E
40|$|An Excel Macro {{was created}} to provide {{researchers}} with an easy to use resource in order to calculate the two <b>dependent</b> <b>samples</b> maximum test as provided in Maggio and Sawilowsky (2014), which permits conducting both the two <b>dependent</b> <b>samples</b> t-test and Wilcoxon signed-ranks test on the same data while eliminating concerns related to Type I error inflation and choice of statistical tests...|$|E
40|$|A web-based Shiny {{application}} {{written in}} R statistical language {{was developed and}} deployed online to calculate a new two <b>dependent</b> <b>samples</b> maximum test as presented in Maggio and Sawilowsky (2014 b). The maximum test allows researchers to conduct both the <b>dependent</b> <b>samples</b> t-test and Wilcoxon signed-ranks tests on same data without raising concerns associated with Type I error inflation and choice of statistical tests (Maggio and Sawilowsky, 2014 a). The maximum test in R statistical language provides a friendly user interface...|$|E
3000|$|Under the {{assumption}} (A 2) and other conditions, Masry [6] gave the asymptotic normality for the density estimator under a mixing <b>dependent</b> <b>sample</b> and Roussas [13] obtained the asymptotic normality for the kernel density estimator under an association sample. Unlike the mixing case, association and negatively associated random variables [...]...|$|R
3000|$|... “Pre” and “post” {{assessments}} included {{measures of}} anthropometrics, cardiorespiratory capacity, and inflammatory markers interleukin 6 (IL- 6), interleukin 8 (IL- 8), tumor necrosis factor alpha (TNFα) and C-reactive protein (CRP). Descriptive statistics, effect size (d), and <b>dependent</b> <b>sample</b> ‘t’ tests for all outcome measures {{were calculated for}} the YE group.|$|R
40|$|We {{study the}} {{distributions}} of Y-concomitants of the X-order statistics {{for a special}} <b>dependent</b> <b>sample</b> (Xi,Yi), i= 1, [...] .,n. The dependence among the sample {{is due to the}} Xi's, which are assumed to be distributed as equally-correlated multivariate normal. The finite-sample and asymptotic distributions of concomitants are derived under this setup. ...|$|R
40|$|Object {{recognition}} and segmentation of objects {{is a complex}} task. Our goal is to develop an algorithm that can recognize and segment wound objects in images. We attempt to solve the object {{recognition and}} segmentation problem by using a hypothesis optimization framework. This method optimizes the object segmentation by assigning objective function values to the object segmentation hypotheses. The optimization algorithm is a genetic algorithm. The objective function relies on textural and shape properties, and the textural properties relies on classification of superpixel-segments and superpixel-edges within wound images. Superpixel-segments and superpixel-edges within the same image are <b>dependent</b> <b>samples.</b> We use combined hyperparameter and feature selection methods to train classification models, and we evaluate the impact of <b>dependent</b> <b>samples</b> on these methods. To our knowledge, no study has evaluated model-selection methods when the data contains known groups of <b>dependent</b> <b>samples.</b> Our results confirm that <b>dependent</b> <b>samples</b> results in biased error estimates. Biased error estimates can cause suboptimal feature and hyperparameter selections, and therefore reduce the classification performance. Finally, we obtain promising results by using hypothesis optimization to solve object recognition and segmentation of wounds. These results are important because of the flexible nature of hypothesis optimization; they demonstrate that hypothesis optimization is a strong candidate for general-purpose machine-learnable object recognition and segmentation...|$|E
40|$|Detecting {{the number}} of signals in a given number of obser-vations, or order detection, {{is one of the}} key issues in many signal {{processing}} problems. Information theoretic criteria are widely used to estimate the order. In many applications, data does not follow the independently and identically distributed (i. i. d.) sampling assumption. Previous approaches address <b>dependent</b> <b>samples</b> by downsampling the dataset so that exist-ing order detection methods can be used. By downsampling the data, the sample size is decreased so that the accuracy of the order estimation is degraded. In this paper, we introduce two linear mixture models with <b>dependent</b> <b>samples.</b> The like-lihood for each model is developed based on the entire data set and used in an information theoretic framework to improve the order estimation performance for <b>dependent</b> <b>samples.</b> Ex-perimental results show performance improvement using this new method. Index Terms — Order detection, Entropy rate, MDL cri-teria...|$|E
30|$|Evaluation of the moka pot {{experiments}} {{was performed}} by pairwise comparison of the aluminum concentrations. On the one hand, samples from the individual coffee preparations were compared between brands (independent samples), {{and on the other}} hand for each brand the potential differences in aluminum concentration (<b>dependent</b> <b>samples)</b> after the individual coffee preparations were compared. Analysis of possible differences between the brands was performed using the t test for independent samples. The t test for <b>dependent</b> <b>samples</b> was used to analyze the possible differences between the different coffee preparations from individual brands.|$|E
40|$|We {{define a}} data {{dependent}} permutation complexity for a hypothesis set H, {{which is similar}} to a Rademacher complexity or maximum discrepancy. The crucial difference is that it is based on permutation (<b>dependent)</b> <b>sampling.</b> We prove a uniform bound on the generalization error, as well as a concentration result which means that the permutation estimate can be efficiently estimated. ...|$|R
40|$|In this paper, I {{proof that}} Importance Sampling {{estimates}} based on <b>dependent</b> <b>sample</b> sets are consistent under certain conditions. This {{can be used}} to reduce variance in Bayesian Models with factorizing likelihoods, using sample sets that are much larger than the number of likelihood evaluations, a technique dubbed Sample Inflation. I evaluate Sample Inflation on a toy Gaussian problem and two Mixture Models...|$|R
40|$|This paper mainly {{focuses on}} the least square {{regression}} problem for the -mixing and -mixing processes. The standard bound assumption for output data is abandoned and the learning algorithm is implemented with <b>samples</b> drawn from <b>dependent</b> <b>sampling</b> process with a more general output data condition. Capacity independent error bounds and learning rates are deduced {{by means of the}} integral operator technique...|$|R
40|$|There are surveys {{carried out}} {{repeatedly}} {{on the same}} set of units. In connection with such research, {{we are talking about the}} <b>dependent</b> <b>samples.</b> The aim and the contribution of this diploma thesis is a summary of available methods for analyzing data from <b>dependent</b> <b>samples</b> for both continuous and discrete variables. The Czech literature has been devoted to this topic only marginally. The theoretical part is divided into two main parts for two waves and for more waves of exploration, which are further divided according to the type of reference variable. The third part is devoted to the application of the theoretical information in a market research...|$|E
3000|$|... are not {{integrable}} in R. So, {{there are}} some difficulties in investigating the kernel density estimator under these <b>dependent</b> <b>samples.</b> Meanwhile, the nonparametric estimation and nonparametric tests for association and negatively associated random variables {{can be found in}} Prakasa Rao [25].|$|E
40|$|ABSTRACT | The manovacuometer is a simple, {{quick and}} {{non-invasive}} test which measures the maximal respiratory pressures (MRS). Guidelines recommend {{the use of}} a digital manovacuometer due to its high accuracy. The {{purpose of this study was}} to assess the test-retest reliability and concur-rent validity of a digital manovacuometer in measuring the maximal inspiratory and expiratory pressures (MIP/MEP) and nasal inspiratory pressure while sniffing (SNIP). A total of 30 healthy subjects were assessed (20 – 30 years old) using the UFMG and MicroRPM ® (Micro Medical, UK) digital manovacu-ometers. To assess reliability, Intraclass Correlation Coefficient (ICC) and Student’s t test it was used for <b>dependent</b> <b>samples.</b> For the validity assessment, the following were used: Pearson correlation, Student’s t test for <b>dependent</b> <b>samples,</b> linear regres-sion and the Bland-Altman method. The level of significanc...|$|E
40|$|International audienceA {{solution}} to the problem of stabilizing nonlinear systems with input with a constant pointwise delay and state- <b>dependent</b> <b>sampling</b> is proposed. It relies on a recursive construction of the sampling instants and on a recent variant of the classical reduction model approach. State feedbacks without distributed terms are obtained. A lower bound on the maximal allowable delay is determined via a Lyapunov- Krasovskii analysis...|$|R
40|$|We {{show how}} to test {{hypotheses}} for coefficient alpha {{in three different}} situations: Hypothesis tests of whether coefficient alpha equals a prespecified value, Hypothesis tests involving two statistically independent sample alphas as may arise when testing the equality of coefficient alpha across groups, Hypothesis tests involving two statistically <b>dependent</b> <b>sample</b> alphas as may arise when testing the equality of alpha across time, or when testing the equality of alpha for two test scores within the same sample. Coefficient alpha, Hypothesis testing, Structural equation modeling...|$|R
40|$|Outcome <b>dependent</b> <b>sampling</b> {{designs are}} {{commonly}} used in economics, market research and epidemiological studies. Case-control sampling design {{is a classic example}} of outcome <b>dependent</b> <b>sampling,</b> where exposure information is collected on subjects conditional on their disease status. In many situations, the outcome under consideration may have multiple categories instead of a simple dichotomization. For example, in a case-control study, there may be disease sub-classification among the “cases” based on progression of the disease, or in terms of other histological and morphological characteristics of the disease. In this note, we investigate the issue of fitting prospective multivariate generalized linear models to such multiple-category outcome data, ignoring the retrospective nature of the sampling design. We first provide a set of necessary and sufficient conditions for the link functions that will allow for equivalence of prospective and retrospective inference for the parameters of interest. We show that for categorical outcomes, prospective-retrospective equivalence does not hold beyond the generalized multinomial logit link. We then derive an approximate expression for the bias incurred when link functions outside this class are used. We illustrate the extent of bias through a real data example, based on the ongoing Prostate, Lung, Colorectal and Ovarian (PLCO) cancer screening trial by the National Cancer Institute...|$|R
40|$|This {{article is}} {{dedicated}} to the estimation of Wasserstein distances and Wasserstein costs between two distinct continuous distributions F and G on R. The estimator is based on the order statistics of (possibly <b>dependent)</b> <b>samples</b> of F resp. G. We prove the consistency and the asymptotic normality of our estimators. ...|$|E
40|$|Here is an ado {{file and}} a help file for calculating {{asymptotic}} and exact tests of symmetry for NxN contingency tables from <b>dependent</b> <b>samples.</b> This test {{is known in}} genetics as the TDT test. "symmetry" performs symmetry and marginal homogeneity tests on square NxN tables {{where there is a}} 1 to 1 matching of cases and controls (non-independence). ...|$|E
40|$|There {{are several}} {{questions}} which {{need to be}} addressed in this work: • Optimal sample size. What is the optimal number of pieces in each sample? Which parameters may have an effect on optimal sample size? • Effect of <b>dependent</b> <b>samples.</b> What is the effect of using a given population of data of finite size (e. g. 900...|$|E
40|$|In {{evaluation}} of the operating characteristics of mixed sampling plans (a known), the probability function, Fn(x), {{of the difference between}} the extreme value and the mean is important. Because it is an n multiple integral where n is the first sample size of a mixed <b>dependent</b> <b>sampling</b> plan, the normal way of evaluating this probability is the recursive quadrature method. Using this method, as the first sample size increases, the amount of computation increases exponentially, so that this probability function becomes a bottleneck in computation. This thesis presents a Taylor-expansion method for handling the probability function for large first sample sizes with satisfactory precision, for an arbitrary given x. In such a way, the {{evaluation of}} the dependent mixed plans is extended to a larger first sample size than previously available in the literature. Theoretically, the computational method presented here could be used for evaluating mixed plans of very large first sample size. Technically, however, this method is limited, by the computer memory resources. Using the computational approach described above, a FORTRAN program that can compute the operating characteristics of mixed plans for first sample sizes of up to 50 was developed. A series of mixed <b>dependent</b> <b>sampling</b> plans were then evaluated with an accuracy of seven decimal places...|$|R
40|$|Most {{calorimeter}} systems used in {{particle physics}} experiments are longitudinally subdivided into several compartments. The intercalibration of these compartments is highly non-trivial, {{as a result}} of two effects: 1) The dependence of the calorimeter response on the type of showering particle, and on its energy. 2) The depth <b>dependent</b> <b>sampling</b> fraction of showers. In this talk, I will illustrate the problems arising from this with examples from the scientific literature. I will also review some of the methods applied in practice to calibrate segmented calorimeters, and discuss the (lack of) merit of these methods...|$|R
40|$|In {{quantum field}} theory it is {{generally}} known that the energy density may be negative at a given point in spacetime. A number of papers have shown {{that there is a}} restriction on this energy density which is called a quantum inequality (QI). A QI is the lower bound to the "weighted average" of the energy density at a given point integrated over a time <b>dependent</b> <b>sampling</b> function. In this paper we give an example of a sampling function {{for which there is no}} QI. Comment: Corrected typo in Eq. 2. ...|$|R
