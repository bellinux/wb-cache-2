6|7052|Public
5000|$|Generic type {{which can}} be {{instantiated}} with any base <b>floating</b> <b>point</b> <b>type.</b>|$|E
5000|$|A googol is {{approximately}} 70! (factorial of 70). Using an integral, binary numeral system, one would need 333 bits {{to represent a}} googol, i.e., 1 googol ≈ 2332.19280949, or exactly [...] However, a googol is well within the maximum bounds of an IEEE 754 double-precision <b>floating</b> <b>point</b> <b>type.</b>|$|E
5000|$|C99 adds a new [...] keyword (and [...] {{convenience}} macro) {{that provides}} support for complex numbers. Any <b>floating</b> <b>point</b> <b>type</b> {{can be modified}} with , and is then defined {{as a pair of}} floating point numbers. Note that C99 and C++ do not implement complex numbers in a code-compatible way - the latter instead provides the class [...]|$|E
5000|$|Charm is a {{strongly}} typed language, but does allow some implicit conversions between numeric and <b>floating</b> <b>point</b> <b>types.</b> The following basic variable types are supported: ...|$|R
5000|$|There {{were several}} <b>floating</b> <b>point</b> <b>types,</b> {{including}} [...] (the 4-byte 754 representation) [...] (the 8-byte IEEE 754 representation), [...] (a 10-byte IEEE 754 representation used mostly internally by numeric coprocessors) and [...] (a 6-byte representation).|$|R
50|$|In both languages, {{the number}} of {{operations}} that can be performed on the advanced numeric types are limited compared to the built-in IEEE 754 <b>floating</b> <b>point</b> <b>types.</b> For instance, none of the arbitrary-size types support square root or logarithms.|$|R
5000|$|The {{standard}} constants real {{shorts and}} real lengths {{can be used}} to determine how many 'short's and 'long's can be usefully prefixed to 'short real' and 'long real'. The actually size of the 'short real', 'real' and 'long real' is available as constants short max real, max real and long max real etc. With the constants short small real, small real and long small real available for each type's machine epsilon. declarations of single precision often are not honored The value of [...] "n" [...] is provided by the SELECTED_REAL_KIND intrinsic function. ALGOL 68G's run time option --precision [...] "number" [...] can set precision for long long reals to the required [...] "number" [...] significant digits. The standard constants long long real width and 'long long max real {{can be used to}} determine actual precision. These IEEE floating-point types will be introduced in the next COBOL standard. Same size as 'double' on many implementations. Swift supports 80-bit extended precision <b>floating</b> <b>point</b> <b>type,</b> equivalent to long double in C languages.|$|E
40|$|Modern {{computing}} {{has adopted}} the <b>floating</b> <b>point</b> <b>type</b> as a default {{way to describe}} computations with real numbers. Thanks to dedicated hardware support, such computations are efficient on modern architectures, even in double pre-cision. However, rigorous reasoning about the resulting pro-grams remains difficult. This is {{in part due to}} a large gap bet-ween the finite floating point representation and the infinite-precision real-number semantics that serves as the deve-lopers ’ mental model. Because programming languages do not provide support for estimating errors, some computa-tions in practice are performed more and some less precisely than needed. We present a library solution for rigorous arithmetic com-putation. Our numerical data type library tracks a (double) floating point value, but also a guaranteed upper bound on the error between this value and the ideal value that would be computed in the real-value semantics. Our implemen-tation involves a set of linear approximations based on an extension of affine arithmetic. The derived approximations cover most of the standard mathematical operations, inclu-ding trigonometric functions, and are more comprehensive than any publicly available ones. Moreover, while interval arithmetic rapidly yields overly pessimistic estimates, our approach remains precise for several computational tasks of interest. We evaluate the library on a number of examples from numerical analysis and physical simulations. We found it to be a useful tool for gaining confidence in the correctness of the computation...|$|E
40|$|The {{data type}} bigfloat is the high-precision <b>floating</b> <b>point</b> <b>type</b> of LEDA. A bigfloat {{is a number}} of the form s. 2 "e where s and e are integers. s is called the {{significant}} or mantissa and e is called the exponent. Arithmetic on bigfloats is governed by two parameters: the mantissa length and the rounding mode. Both parameters can either be set globally or for a single operation. The arithmetic on bigfloats works as follows: first the exact result of an operation is computed and then the mantissa is rounded to the prescribed number of digits as dictated by the rounding mode. The available rounding modes are TONEAREST (round to the nearest representable number), TOPINF (round upwards), TONINF (round downwards), TOZERO (round towards zero), TOINF (round away from zero) and EXACT. The latter mode only applies to addition, substraction and multiplication. In this mode the presicion parameter is ignored and no rounding takes place. Since the exponents of bigfloats are arbitrary integers (type integer) arithmetic operations never underflow or overflow. However, exceptions (division by zero, square root of a negative number) may occur. They are handled according of the IEEE floating point standard, e. g. 5 / 0 evaluates to #infinity#, - 5 / 0 evaluates to -#infinity#, + #infinity# + 5 evaluates to +#infinity# and 0 / 0 evaluates to NaN (=not a number). This report is structured as follows. Section 2 defines the bigfloat through its manual page and the remaining sections contain the implementation. The implementation is split into files bigfloat. h and bigfloat. c. (orig.) SIGLEAvailable from TIB Hannover: RR 1912 (96 - 1 - 002) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekBundesministerium fuer Bildung, Wissenschaft, Forschung und Technologie, Bonn (Germany) DEGerman...|$|E
40|$|Abstract—We {{present a}} C++ library for {{constructing}} and analyzing general hidden Markov models. The library {{consists of a}} number of template classes and generic functions, parameterized with the precision of <b>floating</b> <b>point</b> <b>types</b> and different types of hardware acceleration. Keywords-HMMlib, Hidden Markov Models, SSE, OpenMP I...|$|R
50|$|The {{first four}} integer {{parameters}} are passed in registers eax, ebx, ecx and edx. <b>Floating</b> <b>point</b> parameters are {{passed on the}} <b>floating</b> <b>point</b> stack - registers st0, st1, st2, st3, st4, st5 and st6. Structure parameters are always passed on the stack. Additional parameters are passed on the stack after registers are exhausted. Integer values are returned in eax, pointers in edx and <b>floating</b> <b>point</b> <b>types</b> in st0.|$|R
5000|$|Numeric {{classes for}} <b>floating</b> <b>point</b> <b>types</b> are also different. Haskell's classes Fractional, RealFrac, RealFloat and Floating are not defined. Haskell's class Real defines toRational while Frege's defines (/):class Real (Num r) => r where -- classname precedes context --- the {{division}} operator (/) :: r -> r -> r ...|$|R
50|$|<b>Floating</b> <b>point</b> data <b>types</b> {{are valid}} data types for {{assignment}} from a probed process, but {{cannot be used}} for <b>floating</b> <b>point</b> mathematical operations.|$|R
5000|$|C has a less strict {{model of}} <b>floating</b> <b>point</b> <b>types</b> than Pascal. In C, {{integers}} may be implicitly converted to <b>floating</b> <b>point</b> numbers, {{and vice versa}} (though possible precision loss may be flagged by warnings). In Pascal, integers may be implicitly converted to , but conversion of [...] to [...] (where information may be lost) must be done explicitly via the functions [...] and , which truncate or round off the fraction, respectively.|$|R
5000|$|As another example, a {{three-dimensional}} Vector composite type {{that uses the}} <b>floating</b> <b>point</b> data <b>type</b> could be created with:struct Vector { float x; float y; float z;}; ...|$|R
40|$|We {{propose a}} scheme for reduced-precision {{representation}} of <b>floating</b> <b>point</b> {{data on a}} continuum between IEEE- 754 <b>floating</b> <b>point</b> <b>types.</b> Our scheme {{enables the use of}} lower precision formats for a reduction in storage space requirements and data transfer volume. We describe how our scheme can be accelerated using existing hardware vector units on a general-purpose processor (GPP). Exploiting native vector hardware allows us to support reduced precision <b>floating</b> <b>point</b> with low overhead. We demonstrate that supporting reduced precision in the compiler as opposed to using a library approach can yield a low overhead solution for GPPs...|$|R
50|$|The {{subset of}} C {{includes}} all common C language features necessary to describe complex algorithms. Like many embedded C compilers, <b>floating</b> <b>point</b> data <b>types</b> were omitted. <b>Floating</b> <b>point</b> arithmetic is supported through external libraries {{that are very}} efficient.|$|R
40|$|Complex signal {{processing}} algorithms are specified in <b>floating</b> <b>point</b> precision. When their hardware implementation requires fixed <b>point</b> precision, <b>type</b> refinement is needed. The paper presents a methodology and design environment for this quantization process. The method uses independent strategies for fixing MSB and LSB weights of fixed point signals. It enables short design cycles {{by combining the}} strengths of both analytical and simulation based methods. 1 Introduction Modern {{signal processing}} ASICs, such as integrated cable modems and wireless multimedia terminals, are specified with algorithms in <b>floating</b> <b>point</b> precision. Often, the architectural style with which these algorithms are implemented is precision-limited, and relies on a fixed point representation. This requires a designer to translate <b>floating</b> <b>point</b> <b>types</b> into fixed <b>point</b> <b>types,</b> using a refinement strategy. For each refined <b>floating</b> <b>point</b> number, a fixed point characteristic (including fractional and integral wordle [...] ...|$|R
50|$|As BDL {{is meant}} for Hardware synthesis, the {{complete}} ANSI-C syntax is not supported. The principal unsupported operations are: (i) <b>Floating</b> <b>point</b> data <b>types</b> (ii) Sizeof operator (iii) unions and (iv) Recursive functions.|$|R
50|$|Version 1.0 and 1.1 {{both have}} common (CM) and common lite (CL) profiles, the {{difference}} {{being that the}} common lite profile only supports fixed-point instead of <b>floating</b> <b>point</b> data <b>type</b> support, whereas common supports both.|$|R
40|$|This Stanford CS Education {{document}} {{tries to}} summarize all the basic {{features of the}} C language. The coverage is pretty quick, so it is most appropriate as review or for someone with some programming background in another language. Topics include variables, int <b>types,</b> <b>floating</b> <b>point</b> <b>types,</b> promotion, truncation, operators, control structures (if, while, for), functions, value parameters, reference parameters, structs, pointers, arrays, the preprocessor, and the standard C library functions. The most recent version is always maintained at its Stanford CS Education Library UR...|$|R
5000|$|It {{follows that}} an object {{definition}} {{can be extended}} by imposing data typing: a representation format, a default value, and legal operations (rules) and restrictions ("Division by zero {{is not to be}} tolerated!") are all potentially involved in defining an attribute, or conversely, may be spoken of as attributes of that object's type. A JPEG file is not decoded by the same operations (however similar they may be - these are all graphics data formats) as a PNG or BMP file, nor is a <b>floating</b> <b>point</b> <b>typed</b> number operated upon by the rules applied to typed long integers.|$|R
5000|$|... 4. SheerPower {{utilizes}} a [...] "Perfect Precision Math Package" [...] http://www.sp4gl.com/index_perfect.htmlx {{for which}} the patent 7149765 [...] "Apparatus and method for precision binary numbers and numerical operations" [...] is applied to. This precision math eliminates the rounding off errors experienced when using a <b>floating</b> <b>point</b> data <b>type.</b>|$|R
5000|$|Many {{processors}} have instruction {{sets that}} support such an operation on primitive types.Some machines have signed integers {{based on a}} sign-and-magnitude or one's complement representation (see signed number representations), both of which allow a differentiated positive and negative zero. This does not violate trichotomy {{as long as a}} consistent total order is adopted: either −0 = +0 or −0 < +0 is valid. Common <b>floating</b> <b>point</b> <b>types,</b> however, have an exception to trichotomy: there is a special value [...] "NaN" [...] (Not a Number) such that x < NaN, x > NaN, and x = NaN are all false for all floating-point values x (including NaN itself).|$|R
5000|$|Real: Ideally an EXPRESS {{real value}} is {{unlimited}} in accuracy and size. But in practise a real value {{is represented by}} a <b>floating</b> <b>point</b> value of <b>type</b> double.|$|R
5000|$|<b>Floating</b> <b>point</b> data <b>types,</b> usually {{represent}} {{values as}} high-precision fractional values (rational numbers, mathematically), but are sometimes misleadingly called reals (evocative of mathematical real numbers). They usually have predefined limits on both their maximum values and their precision. Output of {{these values are}} often represented in a decimal number format.|$|R
40|$|This paper {{addresses}} {{the challenges of}} System-on-Chip designs using High-Level Synthesis (HLS). HLS tools convert algorithms designed in C into hardware modules. This approach is a practical choice for developing complex applications. Nevertheless, certain hardware considerations are required when writing C applications for HLS tools. Hence, in order to demonstrate the fundamental hardware design concepts, a case studyis presented. Fast Fourier Transform (FFT) implementation in ANSI C is examined in order to explore the important design issues such as concurrency, data recurrences and memory accesses {{that need to be}} resolved before generating the hardware using HLS tools. There are additional language constraints {{that need to be addressed}} including use of pointers, recursion and <b>floating</b> <b>point</b> <b>types...</b>|$|R
5000|$|HINT is {{intended}} to be [...] "scalable" [...] to run on any size computer, from small serial systems to highly parallel supercomputers.The person using the HINT benchmark can use any <b>floating</b> <b>point</b> or integer <b>type.</b>|$|R
50|$|The Precision field usually {{specifies}} {{a maximum}} {{limit on the}} output, depending on the particular formatting <b>type.</b> For <b>floating</b> <b>point</b> numeric <b>types,</b> it specifies the number of digits {{to the right of}} the decimal point that the output should be rounded. For the string type, it limits the number of characters that should be output, after which the string is truncated.|$|R
40|$|Numerous {{applications}} in communication and multimedia domains show significant data-level parallelism (DLP). The amount of DLP varies between {{applications in}} the same domain or even within a single application. Most architectures support a single vector-, SIMD-width {{which may not be}} optimal. This may cause performance and energy inefficiency. We propose the use of multiple (heterogeneous) vector-widths to better serve applications with varying DLP. The SHAVE (Streaming Hybrid Architecture Vector Engine) VLIW vector processor shown in Figure 1 is an example of such an architecture. SHAVE is a unique VLIW processor that provides hardware support for native 32 -bit (short) and 128 -bit (long) vector operations. Vector arithmetic unit (VAU) supports 128 -bit vector arithmetic of 8 / 16 / 32 -bit integer and 16 / 32 -bit <b>floating</b> <b>point</b> <b>types.</b> Scalar arithmetic unit (SAU) supports 32 -bit vector arithmetic of 8 / 16 -bit integer and 16 -bit <b>floating</b> <b>point</b> <b>types.</b> The moviCompile compiler is an LLVM based commercial compiler targeting code generation for SHAVE processor family. The moviCompile compiler is capable of SIMD code generation for 128 -bit (long) and 64 -bit vector operations. This work focuses on compiler backend support for 32 -bit (short) vector operations. More specifically, this work aims to generate SIMD code for short vector types (e. g. 4 x i 8, 2 x i 16, 2 x f 16) that can be executed on 32 -bit SAU next to the 128 / 64 -bit SIMD code. As a result, moviCompile is able to generate heterogeneous assembly code consisting of both short and long vector SIMD operations. Currently, we are testing the compiler using TSVC (Test Suite for Vectorizing Compilers) and intend to measure the performance improvements...|$|R
40|$|Scientific {{applications}} {{rely heavily}} on <b>floating</b> <b>point</b> data <b>types.</b> <b>Floating</b> <b>point</b> operations are complex and require complicated hardware that is both area and power intensive. The emergence of massively parallel architectures like Rigel creates new challenges and poses new questions with respect to <b>floating</b> <b>point</b> support. The massively parallel aspect of Rigel places great emphasis on area efficient, low power designs. At the same time, Rigel is a general purpose accelerator and must provide high performance for a wide class of applications. This thesis presents an analysis of various <b>floating</b> <b>point</b> unit (FPU) components with respect to Rigel, and attempts to present a candidate design of an FPU that balances performance, area, and power and is suitable for massively parallel architectures like Rigel...|$|R
40|$|Abstract. Recently, the {{appearance}} of very large (3 – 10 M gate) FPGAs with embedded arithmetic units has {{opened the door to}} the possibility of <b>floating</b> <b>point</b> computation on these devices. While previous researchers have described peak performance or kernel matrix operations, there is as yet relatively little experience with mapping an application-specific <b>floating</b> <b>point</b> loop onto FPGAs. In this work, we port a supercomputer application benchmark onto Xilinx Virtex II and Virtex II Pro FPGAs and compare performance with three Pentium IV Xeon microprocessors. Our results show that this application-specific pipeline, with 12 multiply, 10 add/subtract, one divide, and two compare modules of single precision <b>floating</b> <b>point</b> data <b>type,</b> shows speed up of 10. 37 ×. We analyze the tradeoffs between hardware and software to characterize the algorithms that will perform well on current and future FPGA architectures. ...|$|R
40|$|Abstract. As {{is typical}} in {{evolutionary}} algorithms, fitness evaluation in GP takes {{the majority of}} the computational effort. In this paper we demonstrate the use of the Graphics Processing Unit (GPU) to accelerate the evaluation of individuals. We show that for both binary and <b>floating</b> <b>point</b> based data <b>types,</b> it is possible to get speed increases of several hundred times over a typical CPU implementation. This allows for evaluation of many thousands of fitness cases, and hence should enable more ambitious solutions to be evolved using GP...|$|R
40|$|Among {{the metric}} space {{indexing}} methods, AESA {{is known to}} produce the lowest query costs {{in terms of the}} number of distance computations. However, its quadratic construction cost and space consumption makes it infeasiblefor large dataseis. There have been some work on reducing the space requirements of AESA. Instead of keeping all the distances between objects, LAESA appoints a subset of the database as pivots, keeping only the distances between objects and pivots. Kvp uses the idea of prioritizing the pivots based on their distances to objects, only keeping pivot distances that it evaluates as promising. FQA discretizes the distances using a fixed amount of bits per distance instead of using system's <b>floating</b> <b>point</b> <b>types.</b> Varying the number of bits to produce a performance-space trade-off was also studied in Kvp. Recently, BAESA has been proposed based on the same idea, but using different distance ranges for each pivot. The t-spanner based indexing structure compacts the distance matrix by introducing an approximation factor that makes the pivots less effective. In this work, we show that the Kvp prioritization is oriented toward symmetric distance distributions. We offer a new method that evaluates the effectiveness of pivots in a better fashion by making use of the overall distance distribution. We also simulate the performance of our method combined with distance discretization. Our results show that our approach is able to offer very good space-performance trade-offs compared to AESA and tree-based methods. © 2008 IEEE...|$|R
40|$|We {{consider}} some unusual phenomena {{that appear in}} long consecutive calculations due to a new sort of round-off errors of Pentium <b>floating</b> <b>point</b> division bug <b>type.</b> Mathematical model of these round-off errors is proposed and new phenomena in numerical simulations of dynamical systems (differential equations) and other numerical procedures are discussed. It is shown also that these new phenomena could appear not only with Pentium <b>floating</b> <b>point</b> division bugs, but in a more general situation. 1 Introduction When solving differential equations or other dynamical systems on a computer, we inevitably run into perturbations caused by various types of discretizations. The first of them is time discretization, that is, the transition from differential equations or flows to difference equations or maps. The second type is space discretization, that is, the transition from continuous phase space to a discrete lattice. In numerical simulations these discretizations correspond to round-off errors [...] . ...|$|R
40|$|This {{dissertation}} {{reports on}} {{studies of the}} application of lexicographic graph searches to solve problems in FPGA detailed routing. Our contributions include the derivation of iteration limits for scalar implementations of negotiation congestion for standard <b>floating</b> <b>point</b> <b>types</b> and the identification of pathological cases for path choice. In {{the study of the}} routability-driven detailed FPGA routing problem, we show universal detailed routability is NP-complete based on a related proof by Lee and Wong. We describe the design of a lexicographic composition operator of totally-ordered monoids as path cost metrics and show its optimality under an adapted A* search. Our new router, CornNC, based on lexicographic composition of congestion and wirelength, established a new minimum track count for the FPGA Place and Route Challenge. For the problem of long-path timing-driven FPGA detailed routing, we show that long-path budgeted detailed routability is NP-complete by reduction to universal detailed routability. We generalise the lexicographic composition to any finite length and verify its optimality under A* search. The application of the timing budget solution of Ghiasi et al. is used to solve the long-path timing budget problem for FPGA connections. Our delay-clamped spiral lexicographic composition design, SpiralRoute, ensures connection based budgets are always met, thus achieves timing closure when it successfully routes. For 113 test routing instances derived from standard benchmarks, SpiralRoute found 13 routable instances with timing closure that were unroutable by a scalar negotiated congestion router and achieved timing closure in another 27 cases when the scalar router did not, at the expense of increased runtime. We also study techniques to improve SpiralRoute runtimes, including a data structure of a trie augmented by data stacks for minimum element retrieval, and the technique of step tomonoid elimination in reducing the retrieval depth in a trie of stacks structure...|$|R
