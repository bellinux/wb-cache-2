44|30|Public
25|$|The game map was an {{estimated}} five {{times larger than}} that of Xenoblade Chronicles, which proved problematic especially during the <b>debugging</b> <b>phase.</b> The open world was proving so problematic {{that at one point}} the team were considering scrapping it completely. The hexagonal map structure, with unlockable information points, was designed {{to solve the problem of}} players making their way through large fields. The restrictive nature of the main story quests was designed so players could be eased into the expansive nature of Mira. The choice of Los Angeles as the model for the game's hub city was inspired by Takahashi's liking for the city; his initial idea of modelling the hub after New York City was scrapped due to budgetary constraints in creating the necessary tall buildings. Creating the environment of NLA proved problematic due to the console's memory limitations, and the team worked especially hard on decreasing load times and ensuring collision detection worked properly.|$|E
500|$|After its {{appearance}} at the 2014 Paris Games Week, Tabata announced that XVs release window was [...] "roughly decided", with the company recruiting new staff {{to work on the}} game's master build. While it did not show at E3 2015, it did appear at that year's Gamescom, which Tabata said was the beginning of its official marketing campaign leading up to further announcements. As part of the marketing, the game was distanced from the Fabula Nova Crystallis subseries, as it would have placed symbolic limits upon their target audience. The game's minimal showing at the event caused a negative reaction from fans, with Tabata stating that Square Enix underestimated the importance of the event. Between May and November 2015, the publicity was deliberately kept low-key so staff could focus on completing the pre-beta version of the game. Once done, XV became playable in its entirety, enabling developers to get a full view of the product for the final stages of development and testing. Production on the beta version and progression to the <b>debugging</b> <b>phase</b> began in 2016. The master version, the version of the game that would be played at release, was completed in August, although further technical polishing took place after this.|$|E
50|$|The game map was an {{estimated}} five {{times larger than}} that of Xenoblade Chronicles, which proved problematic especially during the <b>debugging</b> <b>phase.</b> The open world was proving so problematic {{that at one point}} the team were considering scrapping it completely. The hexagonal map structure, with unlockable information points, was designed {{to solve the problem of}} players making their way through large fields. The restrictive nature of the main story quests was designed so players could be eased into the expansive nature of Mira. The choice of Los Angeles as the model for the game's hub city was inspired by Takahashi's liking for the city; his initial idea of modelling the hub after New York City was scrapped due to budgetary constraints in creating the necessary tall buildings. Creating the environment of NLA proved problematic due to the console's memory limitations, and the team worked especially hard on decreasing load times and ensuring collision detection worked properly.|$|E
40|$|CHR {{is a very}} {{versatile}} {{programming language}} that allows programmers to declaratively specify constraint solvers. An {{important part of the}} development of such solvers is in their testing and <b>debugging</b> <b>phases.</b> Current CHR implementations support those phases by offering tracing facilities with limited information. In this report, we propose a new trace for CHR which contains enough information to analyze any aspects of execution at some useful abstract level, common to several implementations. ...|$|R
40|$|Abstract—We {{report on}} the {{behavior}} of developers working with a live coding environment, which provides information about a program’s execution immediately after each change to the source code. The live coding environment we used shows information about each individual source code line, e. g., changed variable values or truth values of conditions. In comparison to developers working in a non-live environment, those working live found and fixed bugs they introduced significantly faster. Further, working live encouraged developers to switch between editing and <b>debugging</b> <b>phases</b> more frequently. I...|$|R
40|$|ISBN : 978 - 3 - 642 - 41010 - 9 International audienceAssertion-Based Verification {{is widely}} gaining acceptance. It {{makes use of}} assertions, which are formal {{expressions}} of the expected specification or requirements. Writing assertions concurrently with the design can bring significant benefits to both the design and verification processes for digital circuits. From the concrete perspective of an industrial development flow, inserting synthesized assertion monitors and associated debug infrastructures in an FPGA-based environment can improve the <b>debugging</b> <b>phases</b> in many application domains. This paper advocates this approach, through {{the presentation of the}} validation of an industrial HDLC controller IP using synthesizable property monitors, and draws conclusions from these experiments...|$|R
5000|$|After its {{appearance}} at the 2014 Paris Games Week, Tabata announced that XVs release window was [...] "roughly decided", with the company recruiting new staff {{to work on the}} game's master build. While it did not show at E3 2015, it did appear at that year's Gamescom, which Tabata said was the beginning of its official marketing campaign leading up to further announcements. As part of the marketing, the game was distanced from the Fabula Nova Crystallis subseries, as it would have placed symbolic limits upon their target audience. The game's minimal showing at the event caused a negative reaction from fans, with Tabata stating that Square Enix underestimated the importance of the event. Between May and November 2015, the publicity was deliberately kept low-key so staff could focus on completing the pre-beta version of the game. Once done, XV became playable in its entirety, enabling developers to get a full view of the product for the final stages of development and testing. Production on the beta version and progression to the <b>debugging</b> <b>phase</b> began in 2016. The master version, the version of the game that would be played at release, was completed in August, although further technical polishing took place after this.|$|E
30|$|Bohrbugs are {{permanent}} design faults and are almost deterministic in nature. They {{can be identified}} easily and weeded out during the testing and <b>debugging</b> <b>phase</b> of the software life cycle.|$|E
40|$|Ankara : Department of Industrial Engineering and the Institute of Engineering and Science of Bilkent University, 1996. Thesis (Master's) [...] Bilkent University, 1996. Includes bibliographical {{references}} leaves 97 - 100. Бог {{the last}} few deccides, the computer systems have replaced the man power in many important areas of life. Financial markets, production and service systems are controlled either fully by the computers or by the managers who extensively use the computers. Therefore, a major failure of a computerized system iriciy {{end up with a}} catastrophe that may affect a hirge number of people adversely, for this reason, reliability of computer systems attracts tlie attention of the researchers and practitioners. Software {{is one of the major}} component of a computer system. Therefore, in this thesis, we study the stochastic nature of the failure process of a software during the <b>debugging</b> <b>phase.</b> We propose a mathematical model for the software fciilure process which may also lead to prediction of future reliability. Different from many other available software relicibility models, we particularly try to incorporate the structural properties of a softwcire such as the number ol its different instruction paths cuid their logical complexities. The <b>debugging</b> <b>phase</b> is assumed to be composed of successive test sessions. Distribution functions of the length of a test session and the remaining number ol faults cifter a test session are computed. A stopping condition lor the <b>debugging</b> <b>phase</b> is proposed. Dayanık, SavaşM. S...|$|E
40|$|International audienceCHR {{is a very}} {{versatile}} {{programming language}} that allows programmers to declaratively specify constraint solvers. An {{important part of the}} development of such solvers is in their testing and <b>debugging</b> <b>phases.</b> Current CHR implementations support those phases by offering tracing facilities with limited information. In this paper, we propose a new trace for CHR which contains enough information to analyze any aspects of CHR^ execution at some general abstract level. This approach is based on the idea of generic trace. Such a trace is formally defined {{as an extension of the}} ω_r^ semantics. It is currently prototyped in a SWI Prolog based CHR implementation...|$|R
30|$|Likewise, {{the design}} process and {{construction}} activities were not a linear process. Participants engaged in extensive brainstorming sessions to consider ways of merging proposed dream toys and to share ideas between projects. Paper and craft prototypes varied widely both between {{groups as well as}} between members of the same group. In some cases, participants even began constructing a toy only to discover a day or two into the process that their design had fatal flaws! While the large number of abandoned designs and false starts could be perceived as lost time and resources, these prototyping and <b>debugging</b> <b>phases</b> foreground necessary design considerations—such as the importance of units when engaging in measurement—and highlight the large space of design possibilities.|$|R
40|$|Assertion Based Design, {{and more}} specifically, Assertion Based Verification (ABV) is quickly gaining wide {{acceptance}} {{in the design}} community. Assertions are mainly targeted at functional verification during the design and verification phases. In this paper, we concentrate {{on the use of}} assertions in post-fabrication silicon debug. We develop tools that effi-ciently generate the checkers from assertions, for their inclu-sion in the <b>debug</b> <b>phase.</b> We also detail how a checker gen-erator {{can be used as a}} means of circuit design for certain portions of self test circuits, and more generally the design of monitoring circuits. Efficient subset partitioning of check-ers for a dedicated fixed-size reprogrammable logic area is developed for efficient use of dedicated debug hardware. 1...|$|R
40|$|With the {{development}} of multicore hardware, concurrent, parallel and multicore software are becoming increasingly popular. Software companies are spending {{a huge amount of}} time and resources to nd and debug the bugs. Among all types of software bugs, concurrency bugs are also important and troublesome. This type of bugs is increasingly becoming an issue particularly due to the growing prevalence of multicore hardware. In this position paper, we propose a model for monitoring and debugging Starvation bugs as a type of concurrency bugs in multicore software. The model is composed into three phases: monitoring, detecting and debugging. The monitoring phase can support detecting phase by storing collected data from the system execution. The detecting phase can support <b>debugging</b> <b>phase</b> by comparing the stored data with starvation bug's properties, and the <b>debugging</b> <b>phase</b> can help in reproducing and removing the Starvation bug from multicore software. Our intention is that our model is the basis for developing tool(s) to enable solving Starvation bugs in software for multicore platforms...|$|E
40|$|Abstract: Traceability {{has been}} held as an {{important}} factor in testing activities as well as modeldriven development. Vertical traceability affords us opportunities to improve manageability from models and test cases to a code in testing and <b>debugging</b> <b>phase.</b> This paper represents a vertical test method which connects a system test level and an integration test level in testing stage by using UML. An experiment how traceability works to effectively focus on error spots has been included by using concrete examples of tracing from models to the code...|$|E
40|$|ABSTRACT Debugging multi-agent systems (which are concurrent, distributed,and {{consist of}} complex components) is difficult, yet crucial. We {{propose that the}} {{debugging}} process can be improved by followingan agent-oriented design methodology, and then using the design artifacts in the <b>debugging</b> <b>phase.</b> We present an example of thisscheme which uses interaction protocols to debug agent interaction. Interaction protocols are specified using AUML and are translatedto Petri nets. The debugger uses the Petri nets to monitor conversations and to provide precise and informative error messages whenprotocols aren't correctly followed by the agents...|$|E
40|$|In {{the last}} years, the {{importance}} of locating people and objects and communicating with them in real time has become a common occurrence in every day life. Nowadays, {{the state of the}} art of location systems for indoor environments has not a dominant technology as instead occurs in location systems for outdoor environments, where GPS is the dominant technology. In fact, each location technology for indoor environments presents a set of features that do not allow their use in the overall application scenarios, but due its characteristics, it can well coexist with other similar technologies, without being dominant and more adopted than the others indoor location systems. In this context, the European project SELECT studies the opportunity of collecting all these different features in an innovative system which can be used in a large number of application scenarios. The goal of this project is to realize a wireless system, where a network of fixed readers able to query one or more tags attached to objects to be located. The SELECT consortium is composed of European institutions and companies, including Datalogic S. p. A. and CNIT, which deal with software and firmware development of the baseband receiving section of the readers, whose function is to acquire and process the information received from generic tagged objects. Since the SELECT project has an highly innovative content, one of the key stages of the system design is represented by the <b>debug</b> <b>phase.</b> This work aims to study and develop tools and techniques that allow to perform the <b>debug</b> <b>phase</b> of the firmware of the baseband receiving section of the readers...|$|R
40|$|SCIRun is a {{scientific}} programming environment {{that allows the}} interactive construction, debugging, and steering of large-scale scientific computations. We review related systems and introduce a taxonomy that explores different computational steering solutions. Considering these approaches, we discuss why a tightly integrated problem solving environment, such as SCIRun, simplifies the design and <b>debugging</b> <b>phases</b> of computational science applications and how such an environment aids in the scientific discovery process. I. Introduction Since the introduction of computers, scientists and engineers have attempted to harness their power to simulate complex physical phenomena. Today, the computer is an almost universal tool used {{in a wide range}} of scientific and engineering domains. Computational science and engineering is the field that has grown out of the widespread use of computers to numerically simulate the physical phenomena associated with many problems in science and engineering. [...] ...|$|R
40|$|Within the {{computer}} science community, it {{is a well-known}} fact that the cost to correct an error in a computer system increases dramatically as the system life cycle progresses. The cost of correcting an error increases by orders of magnitude as the system moves from the development stages of analysis and design, to become most expensive during the maintenance and operation phase. Formal specification and prototyping help to eliminate many of these errors {{in the very early}} stages of a project before any production-level code has been written. Formal methods are used to diminish ambiguity, incompleteness, and inconsistency in a system. When used early in the system development process, they can reveal design flaws that otherwise might be discovered only during the costly testing and <b>debugging</b> <b>phases.</b> Even when using a formal specification one is left with the problem of validating the specification against the informal requirements...|$|R
40|$|Abstract. Floating-point {{arithmetic}} is {{an important}} source of errors in programs because of the loss of precision arising during a computation. Unfortunately, this arithmetic is not intuitive (e. g. many elementary operations are not associative, inversible, etc.) making the <b>debugging</b> <b>phase</b> very difficult and empiric. This article introduces a new kind of program transformation in order to automatically improve the accuracy of floating-point computations. We use P. Cousot and R. Cousot’s framework for semantics program transformation and we propose an offline transformation. This technique was implemented, and the first experimental results are presented. ...|$|E
40|$|Debugging multi-agent systems (which are concurrent, distributed, and {{consist of}} complex components) is difficult, yet crucial. We {{propose that the}} {{debugging}} process can be improved by following an agent-oriented design methodology, and then using the design artifacts in the <b>debugging</b> <b>phase.</b> We present {{an example of this}} scheme which uses interaction protocols to debug agent interaction. Interaction protocols are specified using AUML and are translated to Petri nets. The debugger uses the Petri nets to monitor conversations and to provide precise and informative error messages when protocols aren&# 039;t correctly followed by the agents...|$|E
40|$|In {{this paper}} using the main feature of our {{proposed}} Model in its inflection point, we propose a softwarereliability growth model, which relatively {{early in the}} testing and <b>debugging</b> <b>phase,</b> provides accurateparameters estimation, gives a very good failure behavior prediction and enable software developers topredict when to conclude testing, release the software and avoid over testing in order to cut the cost duringthe development {{and the maintenance of}} the software. Two real world experimental data previouslyanalyzed have been used to compare our proposed Early Estimation Logistic Model effectiveness withseveral pre-existing models...|$|E
40|$|In this paper, {{we present}} a low-cost, on-chip clock jitter digital {{measurement}} scheme for high performance microprocessors. It enables in situ jitter measurement during the test or <b>debug</b> <b>phase.</b> It provides very high measurement resolution and accuracy, despite the possible presence of power supply noise (representing {{a major source of}} clock jitter), at low area and power costs. The achieved resolution is scalable with technology node and can in principle be increased as much as desired, at low additional costs in terms of area overhead and power consumption. We show that, for the case of high performance microprocessors employing ring oscillators (ROs) to measure process parameter variations (PPVs), our jitter measurement scheme can be implemented by reusing part of such ROs, thus allowing to measure clock jitter with a very limited cost increase compared with PPV measurement only, and with no impact on parameter variation measurement resolution...|$|R
40|$|Abstract – We {{present a}} novel low cost scheme for the on-die {{measurement}} of either clock jitter, or process parameter variations. By re-using and properly modifying the Ring Oscillators (ROs) {{that are currently}} widely employed for process parameter variation measurement in high performance microprocessors, our proposed scheme can be easily set in either the process parameter variation measurement mode, or the clock jitter measurement mode, by acting on an external control signal. This way, during the test or <b>debug</b> <b>phase,</b> clock jitter can also be measured at negligible area and power costs with respect to process parameter variation measurement only. Our scheme is scalable in the provided clock jitter measurement resolution, while allowing the same process parameter variation measurement resolution as the currently employed RO based schemes. Moreover, due to its allowing both process parameter variation and clock jitter measurements, our scheme features accurate clock jitter measurement despite the possible presence of significant process parameter variations...|$|R
40|$|This report {{presents}} the results of the UNIFY Service Provider DevOps activities. First, we present the final definition and assessment of the concept. SP-DevOps is realized by a combination of various functional components facilitating integrated service verification, efficient and programmable observability, and automated troubleshooting processes. Our assessment shows that SP-DevOps can help providers to reach a medium level of DevOps maturity and allows significant reduction in OPEX. Second, we focus on the evaluation of the proposed SP-DevOps components. The set of tools proposed supports ops and devs across all stages, with a focus on the deployment, operation and <b>debugging</b> <b>phases,</b> and allows to activate automated processes for operating NFV environments. Finally, we present use-cases and our demonstrators for selected process implementions, which allowed the functional validation of SP-DevOps. Comment: This is the public deliverable D 4. 3 of the EU FP 7 UNIFY project (ICT- 619609) - "Updated concept and evaluation results for SP-DevOps". Original Deliverable published at [URL]...|$|R
40|$|After {{completing}} the design phase, the VME modules for the Low Level RF Control (LLRF) of the Rapid Cycling Synchrotron of J-PARC [1] {{are now in}} the production and <b>debugging</b> <b>phase.</b> First all modules are tested for basic functionality, for example dual harmonic signal generation. Then sets of modules are connected together to check higher-level functions and feedback. Finally, the LLRF modules are interfaced to high voltage components like amplifier and cavities. We present the results of the tests conducted so far, the test methods and test functions on several levels. This way we simulate beam operation working conditions and gain experience in controlling the parameters...|$|E
40|$|Consideration of {{the nature}} of the 'debugging' process applied to a new complex system during the initial period of its life. During this period {{failures}} and errors are corrected as they occur, with resulting improvement in the subsequent performance of the system. One mathematical idealization of this process leads to the assumption that system failure rate is decreasing with time. In practice, the <b>debugging</b> <b>phase</b> is considered completed when the failure rate reaches an equilibrium or constant value. Models are formulated for this phenomenon. Maximum likelihood estimates are obtained for relevant failure rate functions and for the end of the debugging period. A conservative upper confidence bound on the stable failure rate is obtained...|$|E
40|$|Session 1 A: Testing and Characterization of Embedded SoftwareInternational audienceIncreasing {{complexity}} {{in both the}} software and the underlying hardware, and ever tighter time-to-market pressures {{are some of the}} key challenges faced when designing multimedia embedded systems. Optimizing the <b>debugging</b> <b>phase</b> can help to reduce development time significantly. A powerful approach used extensively during this phase is the analysis of execution traces. However, huge trace volumes make manual trace analysis unmanageable. In such situations, Data Mining can help by automatically discovering interesting patterns in large amounts of data. In this paper, we are interested in discovering periodic behaviors in multimedia applications. Therefore, we propose a new pattern mining approach for automatically discovering all periodic patterns occurring in a multimedia application execution trace...|$|E
40|$|With {{the recent}} rapid {{increase}} in interactive web applications that employ back-end database services, an SQL injection attack {{has become one of}} the most serious security threats. The SQL injection attack allows an attacker to access the underlying database, execute arbitrary commands at intent, and receive a dynamically generated output, such as HTML web pages. In this paper, we present our technique, Sania, for detecting SQL injection vulnerabilities in web applications during the development and <b>debugging</b> <b>phases.</b> Sania intercepts the SQL queries between a web application and a database, and automatically generates elaborate attacks according to the syntax and semantics of the potentially vulnerable spots in the SQL queries. In addition, Sania compares the parse trees of the intended SQL query and those resulting after an attack to assess the safety of these spots. We evaluated our technique using real-world web applications and found that our solution is efficient in comparison with a popular web application vulnerabilities scanner. We also found vulnerability in a product that was just about to be released. ...|$|R
40|$|Measuring {{the values}} of {{discrete}} components frequently takes place during the test or <b>debug</b> <b>phase</b> of Printed Circuit Boards (PCB). This operation requires tools {{that are based on}} some access type. The shrinking geometries constrain the straightforward use of tools based on physical access. One of the aims of the IEEE 1149. 4 Std. is to facilitate those on-board measurements. This infrastructure relies on electronic access that includes high quality analog buses and a set of electronic switches, which enable to completely isolate a component under characterization, e. g. by injecting a known current and measuring the voltage across it. During this process, the infrastructure switches have a negative impact in the measurement accuracy. This paper analyses the measurement of one resistor in two situations: connected between a pin and ground and between two pins. The infrastructure switches that affect the measurement quality are identified and the upper limit of its systematic error is characterized. When the systematic error is completely defined then it is possible to remove its negative effect from the final result. info:eu-repo/semantics/publishedVersio...|$|R
5000|$|Together {{with the}} {{qualitative}} benefits mentioned above major [...] "cost improvements" [...] {{can be reached}} as the avoidance and earlier detection of errors will {{reduce the amount of}} resources needed for <b>debugging</b> in later <b>phases</b> of the project.|$|R
40|$|Abstract—Test {{prioritization}} techniques select {{test cases}} that maximize the confidence on the correctness {{of the system}} when the resources for quality assurance (QA) are limited. In {{the event of a}} test failing, the fault {{at the root of the}} failure has to be localized, adding an extra debugging cost that has to be taken into account as well. However, test suites that are prioritized for failure detection can reduce the amount of useful information for fault localization. This deteriorates the quality of the diagnosis provided, making the subsequent <b>debugging</b> <b>phase</b> more expensive, and defeating the purpose of the test cost minimization. In this paper we introduce a new test case prioritization approach that maximizes the improvement of the diagnostic information per test. Our approach minimizes the loss of di-agnostic quality in the prioritized test suite. When considerin...|$|E
40|$|Abstract—Event {{detection}} {{plays an}} important role in wireless sensor network (WSN) applications such as battlefield surveil-lance and habitat monitoring. However, effective approaches for specifying events in a sensor network remain a challenge. In this paper we present MEDAL, a formal event description language. MEDAL is a modified Petri net which provides a more compact formal language than its predecessor SNEDL. As a system analysis tool, MEDAL can capture the structural, spatial, and temporal properties of a complex event detection system, which can be used to assist system designers in identifying inconsistencies and potential problems. MEDAL can also perform case-specific analyses that can make the <b>debugging</b> <b>phase</b> easier. We present a case study as an example illustrating the features and effectiveness of MEDAL. We also describe an approach for simultaneous detection of multiple events in a single WSN. I...|$|E
40|$|Abstract Property {{checkers}} formally prove {{whether a}} given property holds for a circuit or system. In case of invalidity {{one or more}} counter-examples for the property are generated. Then {{the next step is}} debugging, a very time consuming task in complex designs that still lacks tool support. Often, the <b>debugging</b> <b>phase</b> is carried out using an ordinary simulator and requires a lot of manual work. In this paper we present an efficient automatic debugging approach for property checking to support the designer and verification engineer to easily identify the fault location. The technique fully supports hierarchical descriptions and by this enables diagnosis results at the source code level. Single and multiple errors are considered, while no assumption on a specific error type is needed. Debugging information is generated for the circuit and the property, as each of them can be erroneous. ...|$|E
40|$|Phase locked loops are {{incorporated}} into almost every large-scale mixed signal and digital system on chip (SOC). Various types of PLL architectures exist including fully analogue, fully digital, semi-digital, and software based. Currently {{the most commonly used}} PLL architecture for SOC environments and chipset applications is the Charge-Pump (CP) semi-digital type. This architecture is commonly used for clock synthesis applications, such as the supply of a high frequency on-chip clock, which is derived from a low frequency board level clock. In addition, CP-PLL architectures are now frequently used for demanding RF (Radio Frequency) synthesis, and data synchronization applications. On chip system blocks that rely on correct PLL operation may include third party IP cores, ADCs, DACs and user defined logic (UDL). Basically, any on-chip function that requires a stable clock will be reliant on correct PLL operation. As a direct consequence it is essential that the PLL function is reliably verified during both the design and <b>debug</b> <b>phase</b> and through production testing. This chapter focuses on test approaches related to embedded CP-PLLs used for the purpose of clock generation for SOC. However, methods discussed will generally apply to CP-PLLs used for other applications...|$|R
40|$|This {{document}} is the Accepted Manuscript {{version of the}} following article: Martin Oma???a, Daniele Rossi, Daniele Giaffreda, Cecilia Metra, T. M. Mak, Asifur Rahman, and Simon Tam, ???Low-Cost On-Chip Clock Jitter Measurement Scheme???, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, Vol. 23 (3) : 435 - 443, April 2014, DOI: [URL] ?? 2014 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. In this paper, we present a low-cost, on-chip clock jitter digital measurement scheme for high performance microprocessors. It enables in situ jitter measurement during the test or <b>debug</b> <b>phase.</b> It provides very high measurement resolution and accuracy, despite the possible presence of power supply noise (representing {{a major source of}} clock jitter), at low area and power costs. The achieved resolution is scalable with technology node and can in principle be increased as much as desired, at low additional costs in terms of area overhead and power consumption. We show that, for the case of high performance microprocessors employing ring oscillators (ROs) to measure process parameter variations (PPVs), our jitter measurement scheme can be implemented by reusing part of such ROs, thus allowing to measure clock jitter with a very limited cost increase compared with PPV measurement only, and with no impact on parameter variation measurement resolution...|$|R
40|$|The present thesis {{work has}} been carried out in the {{framework}} of the CMS collaboration, one of the experiment designed to study the physics of the proton-proton collisions at the Large Hadron Collider (LHC) at CERN. Experimentation at CMS (and at ATLAS) led to the discovery of a new particle in 2012 which has been identified as the Higgs boson, the missing brick of the Standard Model of the fundamental interactions. All the experiments at LHC are upgrading their detectors in order to fulfill the continuous increment of the LHC luminosity and the consequent increment of the per collision event rate. The CMS upgrade project foresees, inter alia, the production of a new pixel detector (CMS Phase 1 Pixel Upgrade) to be commissioned at the beginning of 2017. Crucial part of the upgrade is the new readout chip (ROC) for the silicon sensor, psi 46 digV 2 respin, designed at the Paul Scherrer Institute (PSI) with a 250 nm CMOS technology. This chip represents {{the state of the art}} in the readout electronics for the silicon detectors. The thesis concerns the study and the development of test procedures for this new readout chip. Thanks to a long stay at PSI, I could provide an important contribution to the <b>debug</b> <b>phases</b> of the first version of the ROC and TBM, the chip that handles the various ROCs in the pixel module, and to the development of the software used by the whole collaboration for the ROC and module testing. This experience allowed me to be the expert for the installation and commissioning of the ROC readout system in all the production centres in Italy. Furthermore, I managed the ROC wafers test from the early project phases. The ROCs are produced on silicon wafers and undergo various processes before being assembled on the modules, e. g., metal deposition on the pixel pads, thinning and dicing. These processes lead mechanical and thermal stresses that can damage the chips. The ROC wafers test has thus been performed following the same procedure before and after the processing. In order to minimize the failing ROCs fraction mounted on the modules. It has been measured that the processing introduces a 5. 2...|$|R
