11|48|Public
50|$|The first censuses in Uganda {{were taken}} in 1911, 1921 and 1931. It was done in a rather {{primitive}} way. The <b>enumeration</b> <b>unit</b> was 'huts' and not individuals. More scientific censuses were taken 1948 and 1959 where the <b>enumeration</b> <b>unit</b> was persons. The census was however divided into two separate enumerations, one for Africans, {{and one for the}} non-African population.The censuses during independence 1969, 1980, 1991 were taken jointly for all races. The censuses 1980 and 1991 included housing information and in addition a larger questionnaire for a sample of the population. However, the questionnaires for the 1980 were lost and only provisional figures are available from this census.|$|E
5000|$|A dasymetric map is an {{alternative}} to a choropleth map. As with a choropleth map, data are collected by enumeration units. But instead of mapping the data so that the region appears uniform, ancillary information is used to model internal distribution of the phenomenon. For example, population density will be much lower in forested area than urbanized area, so in a common operation, land cover data (forest, water, grassland, urbanization) {{may be used to}} model the distribution of population reported by census <b>enumeration</b> <b>unit</b> such as a tract or county.|$|E
50|$|A {{proportional}} symbol map {{is better}} than choropleth maps for showing raw data totals. A proportional symbols map uses symbols that are proportional the data that they are representing with point locations. These symbols can be true points or conceptual points. True points represent real objects or {{the exact location of}} a tangible object. This could be an oil well or fire hydrant. A conceptual point represents the center of the <b>enumeration</b> <b>unit,</b> such as a corn field. The raw data on proportional symbol maps go {{hand in hand with the}} data shown on choropleth maps.|$|E
50|$|Choropleth mapping is {{commonly}} used to show data for counties, states, or other <b>enumeration</b> <b>units.</b> Data collected for choropleth maps is usually grouped into separate classes based on attributes {{or other forms of}} classification. The classes are given a specific color or shading based on their values and what they are trying to portray. Choropleth maps are most effective when the data or classes change abruptly at each enumerated boundary.|$|R
50|$|In spatial analysis, the {{boundary}} {{problem has been}} discussed along with the modifiable areal unit problem (MAUP) inasmuch as MAUP {{is associated with the}} arbitrary geographic unit and the unit is defined by {{the boundary}} (Rogerson 2006). For administrative purposes, data for policy indicators are usually aggregated within larger <b>units</b> (or <b>enumeration</b> <b>units)</b> such as census tracts, school districts, municipalities and counties. The artificial units serve the purposes of taxation and service provision. For example, municipalities can effectively respond to the need of the public in their jurisdictions. However, in such spatially aggregated units, spatial variations of detailed social variables cannot be identified. The problem is noted when the average degree of a variable and its unequal distribution over space are measured (BESR 2002).|$|R
40|$|Commemorating the 100 th Anniversary of the Association of American Geographers 1904 – 2004 Even though Gehlke and Biehl (1934) {{discovered}} {{certain aspects}} of the modifiable areal unit problem (MAUP), the term MAUP was not coined formally until Openshaw and Taylor (1979) evaluated systematically the variability of correlation values when different boundaries systems were used in the analysis. The problem is called “the modifiable areal unit” because the boundaries of many geographical units are often demarcated artificially, and thus can be changed. For example, administrative boundaries, political districts, and census <b>enumeration</b> <b>units</b> are all subject to be redrawn. When data are gathered according to different boundary definitions, different data sets are generated. Analyzing these data sets will likely provide inconsistent results. This is the essence of the MAUP. link_to_subscribed_fulltex...|$|R
40|$|The {{accuracy}} of maps based on aggregate data {{is dependent upon}} (a) {{the extent to which}} aggregate values calculated for each <b>enumeration</b> <b>unit</b> are representative of the entire unit and, (b) the data classification system used to assign these values to classes. Size and compactness of these units as well as the variability of the distribution mapped are important factors in determining the {{accuracy of}} aggregate values calculated for the units. In this study, the individual and relative importance of these <b>enumeration</b> <b>unit</b> and surface characteristics are examined. Analysis indicates that all three variables exert a significant influence on accuracy of aggregate values with surface variation accounting for the greatest portion of the varia tion in accuracy...|$|E
40|$|Abstract: The cartogram, or value-by-area map, is {{a popular}} {{technique}} for cartographically representing social data. Such maps visually equalize a basemap prior to mapping a social variable by adjusting the size of each <b>enumeration</b> <b>unit</b> by a second, related variable. However, to scale the basemap units according to an equalizing variable, cartograms must distort the shape and/or topology of the original geography. Such compromises reduce {{the effectiveness of the}} visualization for elemental and general map-reading tasks. Here we describe a new kind of representation, termed a value-by-alpha map, which visually equalizes the basemap by adjusting the alpha channel, rather than the size, of each <b>enumeration</b> <b>unit.</b> Although not without its own limitations, the value-by-alpha map is able to circumvent the compromise inherent to the cartogram form, perfectly equalizing the basemap while preserving both shape and topology...|$|E
40|$|This project compares three GIS {{techniques}} that estimate populations who are potentially affected by environmental contamination in Portland, Oregon. All three GIS techniques utilize polygon containment to estimate populations potentially exposed to pollutants based on block level census data. In this study, multiple buffer distances at half-mile increments were used across all three techniques. Circular Euclidean distance buffers surrounding a known contaminated property, such as documented brownfields, approximate the contamination zone. Accurate estimates for populations exposed to harmful environmental conditions {{could provide a}} better understanding of environmental justice issues. The specific research questions were: 1) Are the population estimates sensitive to the GIS techniques used? 2) Are higher proportions of minority populations closer to brownfields than the general population? The three population estimation techniques are: the centroid containment method, areal apportionment method, and vector-based dasymetric mapping. Centroid containment method assigns the population of a census <b>enumeration</b> <b>unit</b> to its centroid. If the centroid falls within the contamination zone, then the total population of the census unit is counted as being exposed to the pollutant. Areal apportionment method estimates exposed populations by calculating the population based on its proportional size for the area of the census <b>enumeration</b> <b>unit</b> that falls within the contamination zone. Vector-based dasymetric mapping redistributes the population into areas that correspond more directly to resident populations within a census <b>enumeration</b> <b>unit.</b> Building volumes are used to dis-aggregate the census enumeration blocks into smaller spatial units that account for three-dimensional space and more realistically resemble the populations who may reside in the exposure zones. Using the population estimates derived from these methods, I examined if minority populations were disproportionately distributed near brownfields. For question 1, I compared the population estimates from the three methods at various buffer distances to ascertain the reliability of each of their calculations. After the sensitivity analysis was performed, I used a half-mile radius around brownfields as the potential risk area and compared the percentage of minorities in the risk area to the percentage of the total population in the risk area {{for each of the three}} GIS techniques. The results indicate all three techniques produced comparable population estimates across the various distance intervals. Consequently, this allowed for a fair comparison across all three GIS methods. In terms of environmental justice the results reported all three estimation techniques to have similar results. As the techniques got more advance, the results showed less inequality for minorities; however, the margin for this difference was only one to two sites depending on which methods are being compared. Also, the vector-based dasymetric method presented a lower percentage of minorities in the risk area when compared to the other two methods, because dasymetric mapping is designed to estimate were people are more likely concentrated. The research from this project presented how GIS could be used to investigate environmental justice, and how different spatial analysis methods could result in uncertainty with population distribution...|$|E
50|$|In Japan {{urbanized}} areas {{are defined as}} contiguous areas of densely inhabited districts (DIDs) using census <b>enumeration</b> districts as <b>units</b> with a density requirement of 4000 /km2.|$|R
40|$|We {{report on}} an {{empirical}} study investigating the effectiveness {{and efficiency of}} spatial inference making with contiguous (value-by-area) cartograms, compared to informational equivalent choropleth maps, combined with graduated circles. We find significant differences in people's inference-making performance dependent on the map type. Overall, {{results suggest that the}} choropleth map with graduated circles is more effective and more efficient than the cartogram for the analysis of population census data. However, map effectiveness and efficiency also significantly depends on the inference task complexity, and more surprisingly, on the shape characteristics of the depicted <b>enumeration</b> <b>units.</b> For simple tasks, cartograms seem as effective and efficient as the more traditional mapping method. For complex inference questions, inference performance with cartograms is significantly dependent on whether regular or irregular zones are distorted. As we know still very little about the perception and cognition of cartograms, we hope to shed new light for this intriguing mapping method with this empirical study...|$|R
40|$|Population {{data are}} {{generally}} provided by state census organisations at the predefined census <b>enumeration</b> <b>units.</b> However, these datasets very are often required at userdefined spatial units {{that differ from}} the census output levels. A number of population estimation techniques {{have been developed to}} address these problems. This article is one of those attempts aimed at improving county level population estimates by using spatial disaggregation models with support of buildings characteristic, derived from national topographic database, and average area of a flat. The experimental gridded population surface was created for Opatów county, sparsely populated rural region located in Central Poland. The method relies on geolocation of population counts in buildings, taking into account the building volume and structural building type and then aggregation the people total in 1 km quadrilateral grid. The overall quality of population distribution surface expressed by the mean of RMSE equals 9 persons, and the MAE equals 0. 01. We also discovered that nearly 20 % of total county area is unpopulated and 80 % of people lived on 33 % of the county territory...|$|R
40|$|ABSTRACT. Establishing {{biological}} reserves or “hot spots ” for endan-gered {{and threatened}} species {{is critical to}} support real-world species reg-ulatory and management problems. Geographic data {{on the distribution of}} endangered and threatened species can be used to improve ongoing ef-forts for species conservation in the United States. At present no spatial database exists which maps out the location of endangered species for the US. However, spatial descriptions do exist for the habitat associated with all endangered species, but in a form not readily suitable to use in a ge-ographic information system (GIS). In our study, the principal challenge was extracting spatial data describing these critical habitats for 472 species from over 1000 pages of the Federal Register. In addition, an appropriate database schema was designed to accommodate the different tiers of infor-mation associated with the species along with the confidence of designation; the interpreted location data was geo-referenced to the county <b>enumeration</b> <b>unit</b> producing a spatial database of endangered species for the whole of US. The significance of these critical habitat designations, database scheme and methodologies will be discussed...|$|E
40|$|The {{sampling}} design adopted {{for the study}} was a two-stage self-weighting design in which the first stage units were Census Enumerator Sub-Districts (ESDs) or their equivalents and the second stage were households. The sampling frame was drawn up {{on the basis of}} small clearly demarcated area units, each with a population estimate. In the sample design chosen, the area stage units were selected with probability proportional to size, based on the census population. Systematic sampling was used throughout, that is, sampling at a fixed interval in a list of ESDs, beginning at a randomly selected starting point. To ensure that the racial and geographic breakdown approximated the national population distribution, the area stage units (ESDs) were listed by statistical region and within the statistical region by urban or rural. Within these sub-statistical regions, the ESDs were then listed in order of percentage African. The sampling interval for the selection of the ESDs was obtained by dividing the 1991 ensus population of 38 120 853 by the 360 clusters to be selected. This yielded 105 800. Starting at a randomly selected point, every 105 800 th person down the cluster list was selected. In the second sampling stage the unit of analysis was the household. In selected ESD a listing or enumeration of households was carried out by means of a field operation. From the households listed in an ESD a sample of households was selected by systematic sampling. Even though the ultimate <b>enumeration</b> <b>unit</b> was the household, in most cases "stands" were used as enumeration units. However, when a stand was chosen as the <b>enumeration</b> <b>unit</b> all households on that stand had to be interviewed. The latest census population data, however, was available only for 1991. An assumption on population growth was thus made to obtain an approximation of the population size for 1993, the year of the survey. The sampling interval {{at the level of the}} household was determined in the following way: Based on the decision to have a take of 125 individuals on average pe cluster (i. e. assuming five members per household to give an average cluster size of 25 households), the interval of households to be selected was determined as the census population divided by 118. 1, i. e. allowing for population growth since the census...|$|E
40|$|This map is {{inspired}} by the 2013 subsidized housing unit density dataset published on MetroGIS datafinder catalog. It improves upon the spatial resolution of the original dataset, which aggregates raw count data by census track <b>enumeration</b> <b>unit,</b> by aggregating normalized density data (# of subsidized rental unit / total # of rental unit) at the census block group level. And with the availability of individual addresses of properties that contain subsidized rental units, the density value distribution is further restricted to individual land use parcels at a sub-census-block-group spatial level. This map {{is intended to be}} used as a full-page colored insert map for any research paper/report/book chapter that include a discussion on publicly subsidized rental housing in the Minneapolis - St. Paul area. It is a way to spatially visualize the original statistical data, and a separate data table that shows the names, addresses, and the number of subsidized unit contained for every properties that have subsidized rental units can accompany this map to provide more precise statistical information. Choropleth map showing the density of publicly-funded subsidized rental housing units in twin cities metro area (not including participants of section 8) ...|$|E
5000|$|In OCaml, {{immutable}} characters, immutable integer numbers, mutable floating-point numbers, immutable tuples, immutable <b>enumerations</b> —including immutable <b>units,</b> immutable booleans, immutable lists, immutable optionals—, immutable {{exceptions are}} immutable formatting strings are value types, while arrays, immutable strings, byte strings and dictionaries—including references—are value types., ...|$|R
40|$|Abstract. The {{power of}} human vision to {{synthesize}} information and recognize pattern {{is fundamental to}} the success of visualization as a scientific method. This same power can mislead investigators who use visualization to explore georeferenced data-if data reliability is not addressed directly in the visualization process. Here, we apply an integrated cognitive-semiotic approach to devise and test three methods for depicting reliability of georeferenced health data. The first method makes use of adjacent maps, one for data and one for reliability. This form of paired representation is compared to two methods in which data and reliability are spatially coincident (on a single map). A novel method for coincident visually separable depiction of data and data reliability on mortality maps (using a color fill to represent data and a texture overlay to represent reliability) is found to be effective in allowing map users to recognize unreliable data without interfering with their ability to notice clusters and characterize patterns in mortality rates. A coincident visually integral depiction (using color characteristics to represent both data and reliability) is found to inhibit perception of clusters that contain some <b>enumeration</b> <b>units</b> with unreliable data, and to make it difficult for users to consider data and reliability independently...|$|R
40|$|Includes bibliographical {{references}} (pages 81 - 84) Conventional correlation techniques {{frequently used}} by geographers prove inadequate for deriving measures of association {{which account for}} the areal extent of phenomena occupying space. Two equal-area techniques for determining the degree of areal association between two choroplethic distributions are proposed and examined. The first technique {{is based on the}} division of a region into equal-area groups according to the actual area measurements for the various <b>enumeration</b> units: <b>enumeration</b> <b>units</b> themselves serve as the integrant units. The second technique is based on a regular dot grid network divided into equal areas: each grid point (dot) defining an equal-area square. The network is superposed over the region and grid points are assigned values by choroplethic allocation from each of the enumerated units. Both techniques were evaluated using ten variables obtained from census tracts for the San Fernando Valley. The procedures for each method involved the ranking of data values and the grouping of tracts or grids into quartile, sextile, octile, and decile divisions. These equal-area groups were in turn compared using area semblance matrix. Corner and diagonal weighted matrices representing the possible variable pairs for both techniques were evaluated, and measures of areal association derived. The diagonally weighted grid technique proved a more satisfactory means of deriving measures of areal association than the initial area technique...|$|R
30|$|For a very {{large-scale}} integration (VLSI) implementation, the enumeration scheme {{becomes the}} bottleneck of sphere decoding algorithm {{as discussed in}} [14, 15, 17]. The enumeration must be efficiently implemented, so “one-node-per-cycle architecture” {{is one of the}} promising structures for the hardware implementation. Our proposed sphere decoder basically is a variant of the method proposed in [14]. The authors proposed a new architecture which consists of two entities: (1) metric computation unit (MCU) and (2) metric <b>enumeration</b> <b>unit</b> (MEU). These two components perform in a parallel manner to handle the forward search and the backward search separately. In our proposed method, the enumeration scheme, i.e., MEU and the MCU can work in the same way. The search will always perform the successive interference cancellation to obtain the nulling-cancelling points, once the unvisited node is accessed. In the meanwhile, the MEU can be used to find the remaining surviving nodes after the above process. Furthermore, the latency requirement of “one-node-per-cycle” architecture will not be a problem in our case. This is because the second term in the modified probabilistic tree pruning is not real-time calculation as described in Eq. (14). Only one additional cycle is needed to obtain the tight radius in each detection layer. The reduction of the number of the visited nodes is more significant than the few additional cycles. Additionally, the statistical pruning we used in the paper is also pre-computed to avoid that the radius obtained by the successive interference cancellation is too large in some extreme cases. In [14], the authors modified the SE enumeration scheme to achieve the critical path reduction. However, the modified one is not strictly compatible with the “one-node-per-cycle” architecture. Due to the complexity reduction and a few additional cycles required, the modified scheme is still working under the architecture. This situation is quite similar to the one we discussed above.|$|E
40|$|The household’ is most {{commonly}} used as unit of analysis in household surveys and as <b>enumeration</b> <b>unit</b> during census data collection. Relationships are mostly indicated with regards to ‘household head’ or ‘acting household head’. The way in which ‘the household’ is defined in these surveys has long been criticised by anthropologists and sociologists as unable to adequately capture {{the complexities of the}} social units within which people arrange themselves. However the problem often doesn’t simply lie with the definition but the assumptions made when designing a questionnaire and taking the survey from the design phase into the implementation phase. The aim {{of this paper is to}} reflect on lessons learnt from a household financial well-being survey with regards to defining ‘the household’ and operationalising the definition in a heterogeneous, complex society such as South Africa. In a follow-up study group interviews were conducted with fieldworkers who administered the survey, and willing respondents were re-interviewed, collecting qualitative information about those with familial and/or financial links. It was found that: 1. 	There was already confusion at the survey design stage as to exactly what the unit of analysis is. 2. 	This confusion translated into lack of clarity in terms of operationalising the working definition given to fieldworkers. 3. 	Fieldworkers therefore reverted back to what they know, namely ‘family’ or simply ‘who is there’. 4. 	The design and layout of the questionnaire did not easily allow for the capturing of more complex units and understanding the relationships between the individual members. 5. 	This affected not only the accuracy and reliability of the demographic data collected but may also have an impact on the financial well-being data collected. Considering the type of definition to be used for the household as unit of analysis is simply the starting point. Much more consideration needs to be given by survey designers to the implementation of concepts in the field instead of trying to find ‘the perfect’ definition...|$|E
40|$|Statistical {{integration}} is advanced {{as a means}} of making data meet particular needs and ad value to the whole statistical data collection and management system. At the data collection level it can produce significant benefits by reducing the cost of statistical collection and the burden placed on respondents, whilst also increasing the value of outputs in terms of achieving consistency and accuracy. At the data processing level, integration enables the benefits of common technology, analytical methods, tools and processes to be fully exploited. Data from different sources and different times can be consolidated to allow for richer databases to be developed and meaningful comparative analysis and interpretation of results to be achieved. Integration implies that common statistical frames, definitions and classification are promoted and used in all statistical surveys to achieve harmony in <b>enumeration</b> <b>units</b> such as enterprise, holdings or individual. However, flexibility which may be necessary at times is lost. For instance agricultural data may require zonal based sample frames than household agricultural holdings. The periods for data collection may be agricultural seasons rather than calendar months. Thus integrating general household survey data with agricultural data may be complicated. This paper examines the issues of statistical integration from the African perspective and discusses the challenges, opportunities and the desirability of pursuing and achieving such integrated systems. Please purchase PDF Split-Merge on www. verypdf. com to remove this watermark. 1...|$|R
40|$|Population {{figures are}} usually {{collected}} by national statistical institutes at small <b>enumeration</b> <b>units</b> (e. g. census tracts or building units). However, still for {{many countries in}} Europe, data are distributed at coarser geographical units like municipalities. This level of resolution is insufficient for analysis in many fields. In addition, the heterogeneity {{of the size of}} the geographical units causes great distortions in analysis, i. e. the Modifiable Areal Unit Problem (MAUP) (Openshaw 1984). Dasymetric mapping techniques have long been applied world-wide to derive finer (and MAUP-free) depictions of the population distribution. These techniques disaggregate population figures reported at coarse source zones into a finer set of zones using ancillary geographical data. Previous attempts to map the European population at high resolution have used CORINE Land Cover (CLC) as the main source of ancillary data. In this article we test new geographical datasets to produce an updated and improved European population grid map. It is tested whether using more detailed ancillary data in the dasymetric mapping significantly yields higher accuracies. As final outcome of this cartographic exercise, a European population grid map for the reference year of 2006, with a spatial resolution of 100 x 100 meters, is presented and validated against reference data. Resident population reported at commune level, a refined version of CLC and information on the soil sealing degree are used as the main inputs to produce the final map. JRC. H. 8 -Sustainability Assessmen...|$|R
40|$|Ambient {{noise is}} a subtle form of {{pollution}} in large urban areas, degrading human health and well-being. In Europe, directives require that urban environmental noise be measured and mapped for the main periods of the daily cycle. Subsequent analyses of human exposure to noise in those periods is usually conducted using resident (i. e., nighttime) population from the census and assuming constant densities within the <b>enumeration</b> <b>units.</b> However, population distribution and densities vary considerably from night to day in metropolitan areas, and disregard for that process results in gross misestimation of exposure to ambient noise in the daytime period. This study considers the spatio-temporal variation of population distribution in assessing exposure to ambient noise in a major urban area, the city of Lisbon, Portugal. Detailed and compatible day- and nighttime population distribution maps were used, developed by means of "intelligent dasymetric mapping". After categorizing noise levels in existing maps in each period, classified according to current legislation, human exposure to ambient noise was assessed with temporally matching population surfaces. Population exposure to noise in 2000 and 2009 was compared and further analyzed in regards to main source of noise, i. e. road traffic vs. aircraft [...] Results show that human exposure to noise shifts substantially in time and space, with {{a significant increase in}} exposed population from the nighttime to daytime period, especially in the higher noise levels. This is due to the combined effects of the daily variation of noise patterns and population distribution...|$|R
40|$|What I am {{interested}} in exploring in this Final Project is the correlation between increased restrictions on and punishment for marijuana use and increased problematic alcohol use at a population level. The population I {{am interested}} in is those living in the United States where both marijuana law enforcement and problematic alcohol use are prevalent. My question is: do these geographies intersect? This research {{is based on the}} idea that there is an ecological relationship between substance use and social control. It is my thought that law enforcement restricting the use of one substance will perturb ecological fabric and lead to increased use of another. Are consequences and complications from problematic alcohol use higher in places where marijuana prohibition is more forcefully enforced? I address this question at a national and state level with various indicies and <b>enumeration</b> <b>units.</b> The correlation between alcohol and marijuana at the level of individual human physiology has been discussed for over 100 years. Cannabis was commonly listed as a treatment for delirium tremens in standard medical texts (Edes 1887, Potter 1895) and manuals (Lilly 1898, Merck 1899, Parke Davis 1909) at the turn of the 19 th century in the United States. In the 1950 s, reports were published showing the benefits of a synthetic THC compound in the treatment of alcoholics’ withdrawal syndromes. In the 1970 s, several reports discussed the possibility of cannabis substitution therapy for the treatment of alcoholics. One article in the journal Nature observed that when the use of marijuana became more regular for a user, “the use o...|$|R
40|$|Includes Figures, Tables, Maps, Appendices and Bibliography. Dasymetric maps {{have been}} shown to be an {{improvement}} over more common mapping techniques in terms of spatial and attribute accuracy. This research is a step towards evaluating dasymetric maps in terms of their effectiveness in communicating information to map readers. The effectiveness of a map depends, in part, on the visual complexity of the pattern it presents to the map reader. Previous research has evaluated the visual complexity of maps using objective measures, but such measures have not been applied to dasymetric maps. In this study a set of classed binary dasymetric population density maps was constructed for testing. As complexity is widely recognized as multifaceted, the maps were evaluated using a variety of objective measures. Several results from previous research with choropleth maps were confirmed. In particular, this study found a) there is considerable redundancy among complexity measures, b) that map complexity is affected profoundly by the data classification method, and c) that generally speaking complexity increases as the number of class increases. A finding unique to binary dasymetric maps is that complexity increases as the percentage of ancillary features increases. Another part of the study compared a set of choropleth test maps with dasymetric maps. Four out of the six complexity measures indicate that dasymetric maps are more complex than choropleth maps. In particular, dasymetric maps create a more fragmented pattern with a greater disparity in the size of <b>enumeration</b> <b>units</b> as well. It remains for future research to determine if the differences in complexity found here have significant implications for map use...|$|R
40|$|In spatial epidemiology, disease {{incidence}} and demographic data are commonly summarized within larger regions such as administrative units because of privacy concerns. As a consequence, analyses using these aggregated data {{are subject to}} the Modifiable Areal Unit Problem (MAUP) as the geographical manifestation of ecological fallacy. In this study, we create small area disease estimates through dasymetric refinement, and investigate the effects on predictive epidemiological models. We perform a binary dasymetric refinement of municipality-aggregated dog tumor incidence counts in Switzerland for the year 2008 using residential land as a limiting ancillary variable. This refinement is expected {{to improve the quality of}} spatial data originally aggregated within arbitrary administrative units by deconstructing them into discontinuous subregions that better reflect the underlying population distribution. To shed light on effects of this refinement, we compare a predictive statistical model that uses unrefined administrative units with one that uses dasymetrically refined spatial units. Model diagnostics and spatial distributions of model residuals are assessed to evaluate the model performances in different regions. In particular, we explore changes in the spatial autocorrelation of the model residuals due to spatial refinement of the <b>enumeration</b> <b>units</b> in a selected mountainous region, where the rugged topography induces great shifts of the analytical units i. e., residential land. Such spatial data quality refinement results in a more realistic estimation of the population distribution within administrative units, and thus, in a more accurate modeling of dog tumor incidence patterns. Our results emphasize the benefits of implementing a dasymetric modeling framework in veterinary spatial epidemiology...|$|R
40|$|Robinsonian {{matrices}} {{arise in}} the classical seriation problem and {{play an important role}} in many applications where unsorted sim- ilarity (or dissimilarity) information must be reordered. We present a new polynomial time algorithm to recognize Robinsonian matrices based on a new characterization of Robinsonian matrices in terms of straight <b>enumerations</b> of <b>unit</b> interval graphs. The algorithm is simple and is based essentially on lexicographic breadth-rst search (Lex-BFS), using a divide-and-conquer strategy. When applied to a nonnegative symmetric n n matrix with m nonzero entries and given as a weighted adjacency list, it runs in O(d(n + m)) time, where d is the depth of the recursion tree, which is at most the number of distinct nonzero entries of A...|$|R
40|$|During {{the past}} three decades a large body of {{research}} has investigated the problem of specifying class intervals for choropleth maps. This work, however, has focused almost exclusively on placing observations in quasi-continuous data distributions into ordinal bins along the number line. All <b>enumeration</b> <b>units</b> that fall into each bin are then assigned an areal symbol that is used to create the choropleth map. The geographical characteristics of the data are only indirectly considered by such approaches to classification. In this article, we design, implement, and evaluate a new approach to classification that places class-interval selection into a multicriteria framework. In this framework, we consider not only number–line relationships, but also the area covered by each class, the fragmentation of the resulting classifications, {{and the degree to which}} they are spatially autocorrelated. This task is accomplished through the use of a genetic algorithm that creates optimal classifications with respect to multiple criteria. These results can be evaluated and a selection of one or more classifications can be made based on the goals of the cartographer. An interactive software tool to support classification decisions is also designed and described. [ABSTRACT FROM AUTHOR]; Copyright of Annals of the Association of American Geographers is the property of Taylor 2 ̆ 6 Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder 2 ̆ 7 s express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts. ...|$|R
40|$|BackgroundTransforming {{spatial data}} from one scale to another is a {{challenge}} in geographic analysis. As part of a larger, primary study to determine a possible association between travel barriers to pediatric cancer facilities and adolescent cancer mortality across the United States, we examined methods to estimate mortality within zones at varying distances from these facilities: (1) geographic centroid assignment, (2) population-weighted centroid assignment, (3) simple areal weighting, (4) combined population and areal weighting, and (5) geostatistical areal interpolation. For the primary study, we used county mortality counts from the National Center for Health Statistics (NCHS) and population data by census tract for the United States to estimate zone mortality. In this paper, to evaluate the five mortality estimation methods, we employed address-level mortality data {{from the state of}} Georgia in conjunction with census data. Our objective here is to identify the simplest method that returns accurate mortality estimates. ResultsThe distribution of Georgia county adolescent cancer mortality counts mirrors the Poisson distribution of the NCHS counts for the U. S. Likewise, zone value patterns, along with the error measures of hierarchy and fit, are similar for the state and the nation. Therefore, Georgia data are suitable for methods testing. The mean absolute value arithmetic differences between the observed counts for Georgia and the five methods were 5. 50, 5. 00, 4. 17, 2. 74, and 3. 43, respectively. Comparing the methods through paired t-tests of absolute value arithmetic differences showed no statistical difference among the methods. However, we found a strong positive correlation (r 20 = 200. 63) between estimated Georgia mortality rates and combined weighting rates at zone level. Most importantly, Bland 22 ̆ 0 ac 2 ̆ 01 cAltman plots indicated acceptable agreement between paired arithmetic differences of Georgia rates and combined population and areal weighting rates. ConclusionsThis research contributes to the literature on areal interpolation, demonstrating that combined population and areal weighting, compared to other tested methods, returns the most accurate estimates of mortality in transforming small counts by county to aggregated counts for large, non-standard study zones. This conceptually simple cartographic method should be of interest to public health practitioners and researchers limited to analysis of data for relatively large <b>enumeration</b> <b>units.</b> 2017 - 08 - 07 T 00 : 00 : 00 Z 28784135 PMC 554748...|$|R
40|$|Includes Tables, Figures, Maps and Bibliography. Most {{research}} studies examining Polish urban patterns {{at the national}} scale consider only one approach to quantify changes in the urbanization levels: measurement based on demographic data or measurement based on land-cover. With few exceptions, research on urbanization process utilizing multiple datasets in the individual voivodeships is limited. By applying the urban life cycle model using both demographic data, measured using <b>enumeration</b> <b>units</b> derived from Poland's administrative system, and land-cover data, derived from satellite imagery, this research can have a unique contribution to the Polish urbanization literature. Furthermore, {{the results of this}} work also have potential implications for urban planning and management at the provincial level by providing a more comprehensive approach to studying the spatial variation of the urban patterns of communes in a voivodeship. The goal of this research is to characterize the urbanization level of Lower Silesian Voivodeship between 1990 and 2010 using both demographic and land-cover data and apply the urban life cycle model to describe urban change within the province. My hypothesis is that a better understanding of urbanization patterns and trends in the province will result by examining urban change at multiple spatial scales and with multiple data sets. Specifically, I addressed the following research questions to evaluate my hypothesis: 1) what are the patterns and trends of urban change in Lower Silesia at each of the three administrative levels; 2) what is the potential spatial, temporal and typological variability in urbanization patterns within the region based on the two datasets; and 3) how does the inclusion of land-cover information better inform the identification of the stage of the urban life cycle model. Specifically, the urban life cycle model used to categorize the urban development is discussed, and previous urbanization studies in the context of population change as well as land-cover change are reviewed. The Background section introduces the Polish administrative system, which varies considerably from the system in the United States, and describes the study site of Lower Silesian Voivodeship. The Methodology section discusses the analyses needed to address each research question. Specifically, the datasets used in the research are discussed in detail, along with the methods used to quantify urban patterns. The Results of the analysis are then presented. The Discussion section synthesizes the results in order to assess variability of urban change in Lower Silesian Voivodeship at multiple scales and to evaluate the inclusion of land-cover information in the urban life cycle model. Finally, the Conclusion section highlights the major findings of the work...|$|R
40|$|Abstract Background Assessments of {{environmental}} exposure and health risks that utilize Geographic Information Systems (GIS) often make simplifying assumptions when using: (a) {{one or more}} discrete buffer distances to define the spatial extent of impacted regions, and (b) aggregated demographic data {{at the level of}} census <b>enumeration</b> <b>units</b> to derive the characteristics of the potentially exposed population. A case-study of school children in Orange County, Florida, is used to demonstrate how these limitations can be overcome by the application of cumulative distribution functions (CDFs) and individual geocoded locations. Exposure potential for 159, 923 school children was determined at the childrens' home residences and at school locations by determining the distance to the nearest gasoline station, stationary air pollution source, and industrial facility listed in the Toxic Release Inventory (TRI). Errors and biases introduced by the use of discrete buffer distances and data aggregation were examined. Results The use of discrete buffers distances in proximity-based exposure analysis introduced substantial bias in terms of determining the potentially exposed population, and the results are strongly dependent on the choice of buffer distance(s). Comparisons of exposure potential between home and school locations indicated that different buffer distances yield different results and contradictory conclusions. The use of a CDF provided a much more meaningful representation and is not based on the a-priori assumption that any particular distance is more relevant than another. The use of individual geocoded locations also provided a more accurate characterization of the exposed population and allowed for more reliable comparisons among sub-groups. In the comparison of children's home residences and school locations, the use of data aggregated at the census block group and tract level introduced variability as well as bias, leading to incorrect conclusions as to whether exposure potential was higher at school or at home. Conclusion The use of CDFs in distance-based environmental exposure assessment provides more robust results than the use of discrete buffer distances. Unless specific circumstances warrant the use of discrete buffer distances, their applcation should be discouraged in favor of CDFs. The use of aggregated data at the census tract or block group level introduces substantial bias in environmental exposure assessment, which can be reduced through individual geocoding. The use of aggregation should be minimized when individual-level data are available. Existing GIS analysis techniques are well suited to determine CDFs as well as reliably geocode large datasets, and computational issues do not present a barrier for their more widespread use in environmental exposure and risk assessment. </p...|$|R
30|$|We {{explored}} the antibacterial activity of {{a mixture of}} gentamicin and colloidal-gold particles (average diameter, 15 nm) toward E. coli К 12, by using the agar-well-diffusion method, <b>enumeration</b> of colony-forming <b>units</b> (CFUs), and turbidimetry. Gentamicin was chosen {{on the basis of}} the following reasons. First, as an aminoglycoside antibiotic, gentamicin is of unquestionable practical interest. Being a mixture of gentamicins C 1, C 2, and C 1 a, it is bacteriostatic to many gram-positive and gram-negative microorganisms, including E. coli Proteus Salmonella, and penicillin-resistant Staphylococcus strains. The mechanism of gentamicin action is linked to disruption of ribosomal synthesis of protein, and microbial resistance to gentamicin develops fairly slowly. Gentamicin is a major agent used to treat severe purulent infection, especially that caused by a resistant gram-negative flora. As a broad-spectrum antibiotic, gentamicin is often prescribed for patients with mixed infection and also when the infecting agent has not been identified. Sometimes gentamicin is effective when other antibiotics display insufficient activity [50].|$|R
30|$|The {{bacterial}} {{action of}} gentamicin {{and that of}} a mixture of gentamicin and 15 -nm colloidal-gold particles onEscherichia coli K 12 was examined by the agar-well-diffusion method, <b>enumeration</b> of colony-forming <b>units,</b> and turbidimetry. Addition of gentamicin to colloidal gold changed the gold color and extinction spectrum. Within the experimental errors, {{there were no significant}} differences in antibacterial activity between pure gentamicin and its mixture with gold nanoparticles (NPs). Atomic absorption spectroscopy showed that upon application of the gentamicin-particle mixture, there were no gold NPs in the zone of bacterial-growth suppression in agar. Yet, free NPs diffused into the agar. These facts are in conflict with the earlier findings indicating an enhancement of the bacterial activity of similar gentamicin–gold nanoparticle mixtures. The possible causes for these discrepancies are discussed, and the suggestion is made that a necessary condition for enhancement of antibacterial activity is the preparation of stable conjugates of NPs coated with the antibiotic molecules.|$|R
40|$|The {{bacterial}} {{action of}} gentamicin {{and that of}} a mixture of gentamicin and 15 -nm colloidal-gold particles onEscherichia coliK 12 was examined by the agar-well-diffusion method, <b>enumeration</b> of colony-forming <b>units,</b> and turbidimetry. Addition of gentamicin to colloidal gold changed the gold color and extinction spectrum. Within the experimental errors, {{there were no significant}} differences in antibacterial activity between pure gentamicin and its mixture with gold nanoparticles (NPs). Atomic absorption spectroscopy showed that upon application of the gentamicin-particle mixture, there were no gold NPs in the zone of bacterial-growth suppression in agar. Yet, free NPs diffused into the agar. These facts are in conflict with the earlier findings indicating an enhancement of the bacterial activity of similar gentamicin–gold nanoparticle mixtures. The possible causes for these discrepancies are discussed, and the suggestion is made that a necessary condition for enhancement of antibacterial activity is the preparation of stable conjugates of NPs coated with the antibiotic molecules...|$|R
40|$|The {{fluorescent}} cell-counting {{technique was}} applied to the <b>enumeration</b> of cell-infecting <b>units</b> of respiratory syncytial (RS) virus in human fetal diploid (HFD) cover-slip cell cultures; it was a sensitive, precise, and rapid assay method. Approximately 2 hr was required for maximal adsorption of RS virus to HFD cell monolayers. However, about 15 % of the infectious virus in the inoculum remained unadsorbed; this percentage was not significantly reduced even when the adsorption period was extended to 5 hr. A linear relationship between virus concentration and the number of fluorescent cells existed over a range of 1. 2 log 10 units. Variation of the mean of replicate determinations in a single experiment was approximately 7. 5 %. The distribution of single infected HFD cells on cover-slip cell cultures corresponded with the calculated frequencies of the Poisson distribution. The Chi square test for the extent of fit was calculated for several experiments, and the value of P was never less than 0. 5. The addition of immune serum after virus adsorption effectively inhibited the development of detectable levels of viral antigen in secondarily infected cells...|$|R
40|$|Moraxella catarrhalis is a {{major cause}} of {{infectious}} exacerbations of chronic obstructive lung disease. Adhesion of this pathogen to epithelial cells is critical for its pathogenicity. Although much work has been done on identifying surface molecules of M. catarrhalis as adhesins, several adhesion assays were used in these studies which has never been validated or compared to each other. In the present study, we have examined the capacity of M. catarrhalis to adhere to different human epithelial cells. By using the two most commonly used adhesion assays based on the <b>enumeration</b> of colony-forming <b>units</b> or on the counting of adherent bacteria per epithelial cell by light microscopy, we identified significant limitations of both methods. These arose either from differences in strain-specific adhesion pattern on the epithelial cell surface or the dependence on the state of confluence of the epithelial cell layer. We developed a new fluorescence-based adhesion assay and compared our results to the two conventional methods. We demonstrated that the fluorescence- based adhesion assay offers a reliable and convenient method for the quantification of M. catarrhalis adhesion to confluent epithelial cell monolayers...|$|R
