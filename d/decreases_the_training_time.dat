7|10000|Public
40|$|Abstract: Problem statement: Artificial Immune Recognition System (AIRS) is {{most popular}} and {{effective}} immune inspired classifier. Resource competition is one stage of AIRS. Resource competition is done {{based on the number}} of allocated resources. AIRS uses a linear method to allocate resources. The linear resource allocation increases the training time of classifier. Approach: In this study, a new nonlinear resource allocation method is proposed to make AIRS more efficient. New algorithm, AIRS with proposed nonlinear method, is tested on benchmark datasets from UCI machine learning repository. Results: Based on the results of experiments, using proposed nonlinear resource allocation method <b>decreases</b> <b>the</b> <b>training</b> <b>time</b> and number of memory cells and doesn’t reduce the accuracy of AIRS. Conclusion: The proposed classifier is an efficient and effective classifier...|$|E
40|$|Problem statement: Artificial Immune Recognition System (AIRS) is {{most popular}} and&# 13; {{effective}} immune inspired classifier. Resource competition is one stage of AIRS. Resource&# 13; competition is done {{based on the}} number of allocated resources. AIRS uses a linear method to allocate&# 13; resources. The linear resource allocation increases the training time of classifier. Approach: In this&# 13; study, a new nonlinear resource allocation method is proposed to make AIRS more efficient. New&# 13; algorithm, AIRS with proposed nonlinear method, is tested on benchmark datasets from UCI machine&# 13; learning repository. Results: Based on the results of experiments, using proposed nonlinear resource&# 13; allocation method <b>decreases</b> <b>the</b> <b>training</b> <b>time</b> and number of memory cells and doesn't reduce the&# 13; accuracy of AIRS. Conclusion: The proposed classifier is an efficient and effective classifier...|$|E
40|$|In {{classification}} tasks {{it may be}} wise {{to combine}} observations from di!erent sources. Not only it <b>decreases</b> <b>the</b> <b>training</b> <b>time</b> {{but it can also}} increase the robustness and the performance of the classi"cation. Combining is often done by just (weighted) averaging of the outputs of the di!erent classi"ers. Using equal weights for all classi"ers then results in the mean combination rule. This works very well in practice, but the combination strategy lacks a fundamental basis as it cannot readily be derived from the joint probabilities. This contrasts with the product combination rule which can be obtained from the joint probability under the assumption of independency. In this paper we will show di!erences and similarities between this mean combination rule and the product combination rule in theory and in practice. # 2000 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved...|$|E
30|$|The {{method of}} rough set {{reduction}} provided an {{efficient way to}} reduce the number of attributes, as well as the complexity of the information expression system. It has <b>decreased</b> <b>the</b> <b>training</b> <b>time</b> of HMM after the reduction of redundant information.|$|R
40|$|<b>The</b> long <b>training</b> <b>time</b> is a {{big problem}} that block the {{application}} of brain computer interface. This paper solve this problem using the existing dataset of many subjects. By analyzing the differences in EEG signal among different users, removing these differences as much as possible and exploits the common points of different individual, the proposed method can correct the data in existing dataset to a new dataset close to the target data. Then using the ensemble method, we combine the model of the existing dataset to a final model. The result shows that the proposed method can <b>decrease</b> <b>the</b> <b>training</b> <b>time</b> greatly while <b>the</b> recognize accuracy can also meet the need...|$|R
40|$|The {{application}} of multi-objective evolutionary computation techniques to the genetic programming of classifiers {{has the potential}} to both improve <b>the</b> accuracy and <b>decrease</b> <b>the</b> <b>training</b> <b>time</b> of <b>the</b> classifiers. The performance of two such algorithms are investigated on the even 6 -parity problem and the Wisconsin Breast Cancer, Iris and Wine data sets from the UCI repository. The first method explores the addition of an explicit size objective as a parsimony enforcement technique. The second represents a program¿s classification accuracy on each class as a separate objective. Both techniques give a lower error rate with less computational cost than was achieved using a standard GP with the same parameters...|$|R
40|$|In this paper, {{we propose}} the new Ball Ranking Machines (BRMs) {{to address the}} {{supervised}} ranking problems. In previous work, supervised ranking methods have been successfully applied in various information retrieval tasks. Among these methodologies, the Ranking Support Vector Machines (Rank SVMs) are well investigated. However, one major fact limiting their applications is that Ranking SVMs need optimize a margin-based objective function over all possible document pairs within all queries on the training set. In consequence, Ranking SVMs need select {{a large number of}} support vectors among a huge number of support vector candidates. This paper introduces a new model of of Ranking SVMs and develops an efficient approximation algorithm, which <b>decreases</b> <b>the</b> <b>training</b> <b>time</b> and generates much fewer support vectors. Empirical studies on synthetic data and content-based image/video retrieval data show that our method is comparable to Ranking SVMs in accuracy, but use much fewer ranking support vectors and significantly less training time. ...|$|E
40|$|Abstract—An {{artificial}} {{neural network}} (ANN) is proposed to predict the input impedance of a broadband antenna {{as a function of}} its geometric parameters. The input resistance of the antenna is first parameterized by a Gaussian model, and the ANN is constructed to approximate the nonlinear relationship between the antenna geometry and the model parameters. Introducing the model simplifies the ANN and <b>decreases</b> <b>the</b> <b>training</b> <b>time.</b> The reactance of the antenna is then constructed by the Hilbert transform from the resistance found by the neuromodel. A hybrid gradient descent and particle swarm optimization method is used to train the neural network. As an example, an ANN is constructed for a loop antenna with three tuning arms. The antenna structure is then optimized for broadband operation via a genetic algorithm that uses input impedance estimates provided by the trained ANN in place of brute-force electromagnetic computations. It is found that the required number of electromagnetic computations in training the ANN is ten times lower than that needed during the antenna optimization process, resulting in significant time savings. Index Terms—Artificial neural network, broadband antenna...|$|E
40|$|AbstractFeed Forward Artificial Neural Networks are {{the most}} widely used models to explain the {{information}} processing mechanism of the brain. Network topology {{plays a key role in}} the performance of the feed forward neural networks. Recently, the small- world network topology has been shown to meet the properties of the real life networks. Therefore, in this study, we consider a feed forward artificial neural network with small-world topology and analyze its performance on classifying the epilepsy. In order to obtain the small-world network, we follow the Watts-Strogatz approach. An EEG dataset taken from healthy and epileptic patients is used to test the performance of the network. We also consider different numbers of neurons in each layer of the network. By comparing the performance of small-world and regular feed forward artificial neural networks, it is shown that the Watts-Strogatz small-world network topology improves the learning performance and <b>decreases</b> <b>the</b> <b>training</b> <b>time.</b> To our knowledge, this is the first attempt to use small-world topology in a feed forward artificial neural network to classify the epileptic case...|$|E
40|$|This paper {{presents}} a new method for pre-training neural networks that can <b>decrease</b> <b>the</b> total <b>training</b> <b>time</b> for a neural network {{while maintaining the}} final performance, which motivates its use on deep neural networks. By partitioning <b>the</b> <b>training</b> task in multiple training subtasks with sub-models, which can be performed independently and in parallel, it is shown {{that the size of}} the sub-models reduces almost quadratically with the number of subtasks created, quickly scaling down the sub-models used for the pre-training. The sub-models are then merged to provide a pre-trained initial set of weights for the original model. The proposed method is independent of the other aspects of <b>the</b> <b>training,</b> such as architecture of <b>the</b> neural network, <b>training</b> method, and objective, making it compatible {{with a wide range of}} existing approaches. The speedup without loss of performance is validated experimentally on MNIST and on CIFAR 10 data sets, also showing that even performing the subtasks sequentially can <b>decrease</b> <b>the</b> <b>training</b> <b>time.</b> Moreover, we show that larger models may present higher speedups and conjecture about the benefits of the method in distributed learning systems. Comment: Figure 2 b has lower quality due to file size constraint...|$|R
40|$|This paper {{presents}} {{a new approach}} to selecting the initial seed set using stratified sampling strategy in bootstrapping-based semi-supervised learning for semantic relation classification. First, <b>the</b> <b>training</b> data is partitioned into several strata according to relation types/subtypes, then relation instances are randomly sampled from each stratum to form the initial seed set. We also investigate different augmentation strategies in iteratively adding reliable instances to the labeled set, and find that the bootstrapping procedure may stop at a reasonable point to significantly <b>decrease</b> <b>the</b> <b>training</b> <b>time</b> without degrading too much in performance. Experiments on the ACE RDC 2003 and 2004 corpora show the stratified sampling strategy contributes more than the bootstrapping procedure itself. This suggests that a proper sampling strategy is critical in semi-supervised learning. ...|$|R
40|$|The maze {{traversal}} problem (finding {{the shortest}} {{distance to the}} goal from any position in a maze) has been an interesting challenge in computational intelligence. Recent work {{has shown that the}} cellular simultaneous recurrent neural network (CSRN) can solve this problem for simple mazes. This thesis focuses on exploiting relevant information about the maze to improve learning and <b>decrease</b> <b>the</b> <b>training</b> <b>time</b> for <b>the</b> CSRN to solve mazes. Appropriate variables are identified to create useful clusters using relevant information. The CSRN was next modified to allow for an additional external input. With this additional input, several methods were tested and results show that clustering the mazes improves the overall learning of the traversal problem for the CSRN. Comment: 29 pages, 15 figures, Undergraduate Honors Thesi...|$|R
40|$|Vulnerability {{assessment}} in power systems is important {{so as to}} determine how vulnerable a power system in case of any unforeseen catastrophic events. This paper presents the application of Radial Basis Function Neural Network (RBFNN) for vulnerability assessment of power system incorporating a new proposed feature extraction method named as the Neural Network Weight Extraction (NNWE) for dimensionality reduction of input data. The performance of the RBFNN is compared with the Multi Layer Perceptron Neural Network (MLPNN) so as {{to evaluate the effectiveness}} of the RBFNN in assessing the vulnerability of a power system based on the indices, power system loss and possible loss of load. In this study, vulnerability analysis simulations were carried out on the IEEE 300 bus test system using the Power System Analysis Toolbox and the development of neural network models were implemented in MATLAB version 7. Test results prove that the RBFNN give better vulnerability assessment performance than the multilayer perceptron neural network in terms of accuracy and training time. The proposed feature extraction method <b>decreases</b> <b>the</b> <b>training</b> <b>time</b> drastically from hours to less than seconds, this bound to influence the vulnerability classification and increase the speed of convergence. It is also concluded that the reduction in error is achieved by using PSL as an output variable of ANN, in all the cases the error of RBFNN output by PSL is less than 4. 87 % which is well within tolerable limits. </font...|$|E
40|$|This {{research}} {{presents an}} intrusion detection method for network datasets using Minimax Probability Machines (MPM) and genetic algorithm. The minimax probability machines can achieve the comparative performance with the Support Vector Machine (SVM). To do more accurate data classification and <b>decrease</b> <b>the</b> <b>training</b> <b>time</b> of classifier, {{we present a}} genetic feature optimized method for minimax probability machines. Genetic algorithm is used to optimize the feature so as to generate newly features to boost minimax probability machines do more accurate classification and need less <b>training</b> <b>time.</b> A new classifier model based on minimax probability machines with genetic feature optimized is proposed and is applied to intrusion detection in this paper. The experimental {{results show that the}} classification method with genetic feature optimized has better performance than the traditional learning method...|$|R
40|$|Feature {{discretization}} (FD) techniques often yield {{adequate and}} compact {{representations of the}} data, suitable for machine learning and pattern recognition problems. These representations usually <b>decrease</b> <b>the</b> <b>training</b> <b>time,</b> yielding higher classification accuracy while allowing for humans to better understand and visualize the data, {{as compared to the}} use of the original features. This paper proposes two new FD techniques. The first one is based on the well-known Linde-Buzo-Gray quantization algorithm, coupled with a relevance criterion, being able perform unsupervised, supervised, or semi-supervised discretization. The second technique works in supervised mode, being based on the maximization of the mutual information between each discrete feature and the class label. Our experimental results on standard benchmark datasets show that these techniques scale up to high-dimensional data, attaining in many cases better accuracy than existing unsupervised and supervised FD approaches, while using fewer discretization intervals...|$|R
40|$|Web page {{prefetching}} {{has been}} used efficiently to reduce the access latency problem of the Internet, its success mainly relies on the accuracy of Web page prediction. As powerful sequential learning models, Conditional Random Fields (CRFs) have been used successfully to improve the Web page prediction accuracy when {{the total number of}} unique Web pages is small. However, because <b>the</b> <b>training</b> complexity of CRFs is quadratic to the number of labels, when applied to a website with a large number of unique pages, <b>the</b> <b>training</b> of CRFs may become very slow and even intractable. In this paper, we <b>decrease</b> <b>the</b> <b>training</b> <b>time</b> and computational resource requirements of CRFs training by integrating error correcting output coding (ECOC) method. Moreover, since the performance of ECOC-based methods crucially depends on the ECOC code matrix in use, we employ a coding method, Search Coding, to design the code matrix of good quality. ...|$|R
40|$|The {{artificial}} {{neural network}} {{is one of the}} interesting techniques that have been advantageously used to deal with modeling problems. In this study, the computing with {{artificial neural network}} (CANN) is proposed. The model is applied to modulate the information processing of one-dimensional task. We aim to integrate a new method which is based on a new coding approach of generating the input-output mapping. The latter is based on increasing the neuron unit in the last layer. Accordingly, to show the efficiency of the approach under study, a comparison is made between the proposed method of generating the input-output set and the conventional method. The results illustrated that the increasing of the neuron units, in the last layer, allows to find the optimal network’s parameters that fit with the mapping data. Moreover, it permits to <b>decrease</b> <b>the</b> <b>training</b> <b>time,</b> during <b>the</b> computation process, which avoids the use of computers with high memory usage...|$|R
40|$|Recently machine learning-based Intrusion Detection systems (IDs) {{have been}} {{subjected}} to extensive researches because they can detect both misuse and anomaly. Most of existing IDs use all features in the network packet to look for known intrusive patterns. In this paper a new hybrid model RSC-PGP (Rough Set Classification- Parallel Genetic Programming) is presented {{to address the problem of}} identifying important features in building an intrusion detection system, increase the convergence speed and <b>decrease</b> <b>the</b> <b>training</b> <b>time</b> of RSC. Tests are done on KDD- 99 data used for The Third International Knowledge Discovery and Data Mining Tools Competition. Results showed that the proposed model gives better and robust representation of rules as it was able to select features resulting in great data reduction, time reduction and error reduction in detecting new attacks. Empirical results reveal that Genetic Programming (GP) based techniques could play a major role in developing IDs which are light weight and accurate when compared to some of the conventional intrusion detection systems based on machine learning paradigms. Key words...|$|R
40|$|Deep neural {{networks}} (DNNs) have recently yielded strong results {{on a range}} of applications. Training these DNNs using a cluster of commodity machines is a promising approach since <b>training</b> is <b>time</b> consuming and compute-intensive. Furthermore, putting DNN tasks into containers of clusters would enable broader and easier deployment of DNN-based algorithms. Toward this end, this paper addresses the problem of scheduling DNN tasks in the containerized cluster environment. Efficiently scheduling data-parallel computation jobs like DNN over containerized clusters is critical for job performance, system throughput, and resource utilization. It becomes even more challenging with the complex workloads. We propose a scheduling method called Deep Learning Task Allocation Priority (DLTAP) which performs scheduling decisions in a distributed manner, and each of scheduling decisions takes aggregation degree of parameter sever task and worker task into account, in particularly, to reduce cross-node network transmission traffic and, correspondingly, <b>decrease</b> <b>the</b> DNN <b>training</b> <b>time.</b> We evaluate <b>the</b> DLTAP scheduling method using a state-of-the-art distributed DNN training framework on 3 benchmarks. The results show that the proposed method can averagely reduce 12 % cross-node network traffic, and <b>decrease</b> <b>the</b> DNN <b>training</b> <b>time</b> even with <b>the</b> cluster of low-end servers...|$|R
30|$|A novel data-driven, soft sensor {{based on}} support vector {{regression}} (SVR) integrated with a data compression technique {{was developed to}} predict the product quality for the hydrodesulfurization (HDS) process. A wide range of experimental data was taken from a HDS setup to <b>train</b> and test <b>the</b> SVR model. Hyper-parameter tuning {{is one of the}} main challenges to improve predictive accuracy of the SVR model. Therefore, a hybrid approach using a combination of genetic algorithm (GA) and sequential quadratic programming (SQP) methods (GA–SQP) was developed. Performance of different optimization algorithms including GA–SQP, GA, pattern search (PS), and grid search (GS) indicated that the best average absolute relative error (AARE), squared correlation coefficient (R 2), and computation time (CT) (AARE =  0.0745, R 2  =  0.997 and CT =  56  s) was accomplished by the hybrid algorithm. Moreover, to reduce the CT and improve the accuracy of the SVR model, the vector quantization (VQ) technique was used. The results also showed that the VQ technique can <b>decrease</b> <b>the</b> <b>training</b> <b>time</b> and improve prediction performance of the SVR model. The proposed method can provide a robust, soft sensor in a wide range of sulfur contents with good accuracy.|$|R
40|$|Abstract: Recently machine learning-based {{intrusion}} detection approaches {{have been subjected}} to extensive researches because they can detect both misuse and anomaly. In this paper, rough set classification (RSC), a modern learning algorithm, is used to rank the features extracted for detecting intrusions and generate {{intrusion detection}} models. Feature ranking is a very critical step when building the model. RSC performs feature ranking before generating rules, and converts the feature ranking to minimal hitting set problem addressed by using genetic algorithm (GA). This is done in classical approaches using Support Vector Machine (SVM) by executing many iterations, each of which removes one useless feature. Compared with those methods, our method can avoid many iterations. In addition, a hybrid genetic algorithm is proposed to increase the convergence speed and <b>decrease</b> <b>the</b> <b>training</b> <b>time</b> of RSC. <b>The</b> models generated by RSC take the form of “IF-THEN ” rules, which have the advantage of explication. Tests and comparison of RSC with SVM on DARPA benchmark data showed that for Probe and DoS attacks both RSC and SVM yielded highly accurate results (greater than 99 % accuracy on testing set) ...|$|R
40|$|As {{the size}} of data sets used to build {{classifiers}} steadily increases, training a linear model efficiently with limited memory becomes essential. Several techniques {{deal with this problem}} by loading blocks of data from disk one at a time, but usually take a considerable number of iterations to converge to a reasonable model. Even the best block minimization techniques [1] require many block loads since they treat all training examples uniformly. As disk I/O is expensive, reducing the amount of disk access can dramatically <b>decrease</b> <b>the</b> <b>training</b> <b>time.</b> This paper introduces a selective block minimization (SBM) algorithm, a block minimization method that makes use of selective sampling. At each step, SBM updates the model using data consisting of two parts: (1) new data loaded from disk and (2) a set of informative samples already in memory from previous steps. We prove that, by updating the linear model in the dual form, the proposed method fully utilizes the data in memory and converges to a globally optimal solution on the entire data. Experiments show that the SBM algorithm dramatically reduces the number of blocks loaded from disk and consequently obtains an accurate and stable model quickly on both binary and multi-class classification...|$|R
40|$|Abstract — This paper {{proposes a}} {{combination}} of methodologies based on a recent development –called Extreme Learning Machine (ELM) – <b>decreasing</b> drastically <b>the</b> <b>training</b> <b>time</b> of nonlinear models. Variable selection is beforehand performed on the original dataset, using the Partial Least Squares (PLS) and a projection based on Nonparametric Noise Estimation (NNE), to ensure proper results by the ELM method. Then, after the network is first created using the original ELM, {{the selection of the}} most relevant nodes is performed by using a Least Angle Regression (LARS) ranking of the nodes and a Leave-One-Out estimation of the performances, leading to an Optimally-Pruned ELM (OP-ELM). Finally, the prediction accuracy of the global methodology is demonstrated using th...|$|R
40|$|Specialists {{are expecting}} the {{knowledge}} of location will trigger yet another revolution in mobile services. Location-base services (LBS) have attracted many researchers and enterprises {{and one of the}} key aspects of LBS is positioning technology. Considering a wider and more complex field - ubiquitous computing, location is the fundamental element. This work focuses on some aspects of the new techniques of terrestrial positioning systems. Wireless LAN {{is one of the most}} popular systems used for positioning for indoor environments and public places. We have investigated the trilateration and fingerprinting approaches and the results showed the advantages of fingerprinting. A novel method to generate the fingerprints database based on Universal Kriging (UK) was developed, which can not only significantly <b>decrease</b> <b>the</b> <b>training</b> <b>time,</b> but also increase the accuracy of estimates. In mobile phone positioning systems, most techniques suffer from the non-line-of-sight (NLOS) propagation. We investigated the specifics of NLOS error, and proposed a method to mitigate the errors. Furthermore, a new algorithm named WSMM (wireless signal map matching) was discussed. Simulations and experiments verified the idea, and the accuracy of positioning can be improved greatly. Since fingerprinting technique can utilize rather than suffer from the NLOS propagation, it was also applied in mobile phone positioning system. Experiments showed both the deterministic approach and probabilistic approach can provide better results comparing with other techniques in suburban area. To achieve a robust positioning system and provide more useful information of the user, multisensor combination and data fusion are necessary. As the first step of future research, a mulitsensor synchronization system was developed. This system can promisingly achieve synchronization with error less than 0. 4 ms, which is suitable for most land applications. Hence the main findings of this thesis are: (1) a novel method of yielding fingerprint database for both wireless LAN (WLAN) and mobile phone systems when using the fingerprinting technique for positioning; (2) a database method to mitigate NLOS error for mobile phone positioning systems; (3) a low cost synchronization system for integration of multiple sensors...|$|R
40|$|It {{remains unclear}} whether {{probabilistic}} category {{learning in the}} feedback-based weather prediction task (FB-WPT) can be mediated by a non-declarative or procedural learning system. To address this issue, we compared <b>the</b> effects of <b>training</b> <b>time</b> and verbal working memory, which influence the declarative learning system but not the non declarative learning system, in the FB and paired associate (PA) WPTs, as the PA task recruits a declarative learning system. The results of Experiment 1 showed that the optimal accuracy in the PA condition was significantly <b>decreased</b> when <b>the</b> <b>training</b> <b>time</b> was reduced from 7 to 3 s, but this did not occur in the FB condition, although shortened <b>training</b> <b>time</b> impaired <b>the</b> acquisition of explicit knowledge in both conditions. The results of Experiment 2 showed that the concurrent working memory task impaired the optimal accuracy and the acquisition of explicit knowledge in the PA condition but did not influence the optimal accuracy or the acquisition of self-insight knowledge in the FB condition. The apparent dissociation results between the FB and PA conditions suggested that a non-declarative or procedural learning system {{is involved in the}} FB-WPT and provided new evidence for the multiple-systems theory of human category learning...|$|R
40|$|In this work, {{we propose}} several online methods {{to build a}} {{learning}} curriculum from a given set of target-task-specific training tasks in order to speed up reinforcement learning (RL). These methods can <b>decrease</b> <b>the</b> total <b>training</b> <b>time</b> needed by an RL agent compared to <b>training</b> on <b>the</b> target task from scratch. Unlike traditional transfer learning, we consider creating a sequence from several training tasks {{in order to provide}} the most benefit in terms of reducing <b>the</b> total <b>time</b> to <b>train.</b> Our methods utilize the learning trajectory of the agent on the curriculum tasks seen so far to decide which tasks to train on next. An attractive feature of our methods is that they are weakly coupled to the choice of the RL algorithm as well as the transfer learning method. Further, when there is domain information available, our methods can incorporate such knowledge to further speed up the learning. We experimentally show that these methods can be used to obtain suitable learning curricula that speed up <b>the</b> overall <b>training</b> <b>time</b> on two different domains. Comment: 12 pages and 4 figures More experiments added to the previous versio...|$|R
40|$|Abstract. A new SVM (Support Vector Machine) classifier-combination model, {{based on}} Hierarchical Partition approach, for {{enterprise}} credit assessment is proposed in this paper. Enterprise credit assessment {{is essentially an}} ordinal classification (or ranking), and the popular multi-classification technique does not deal with the ordinal information in the sample data. Hierarchical Partition approach makes use of the ordinal characteristics to simplify the structure of classifiers, {{as well as to}} <b>decrease</b> <b>the</b> <b>training</b> and testing <b>time.</b> It {{can also be used to}} solve the problems where the imbalance sample distribution and different loss-costs among different ranks present. Experimental results show that the generalization ability of the new model using Hierarchical Partition approach are better than that of neural network model and traditional 1 -vs- 1 combined SVM model...|$|R
40|$|Support Vector Machines (SVM) are the {{classifiers}} {{which were}} originally designed for binary classification. The classification applications can solve multi-class problems. Decision-tree-based {{support vector machine}} which combines support vector machines and decision tree {{can be an effective}} way for solving multi-class problems. This method can <b>decrease</b> <b>the</b> <b>training</b> and testing <b>time,</b> increasing the efficiency of the system. The different ways to construct the binary trees divides the data set into two subsets from root to the leaf until every subset consists of only one class. The construction order of binary tree has great influence on the classification performance. In this paper we are studying an algorithm, Tree structured multiclass SVM, which has been used for classifying data. This paper proposes the decision tree based algorithm to construct multiclass intrusion detection system...|$|R
40|$|Recent {{years have}} {{demonstrated}} that using random feature maps can significantly <b>decrease</b> <b>the</b> <b>training</b> and testing <b>times</b> of kernel-based algorithms without significantly lowering their accuracy. Regrettably, because random features are target-agnostic, typically thousands of such features are necessary to achieve acceptable accuracies. In this work, we consider the problem of learning {{a small number of}} explicit polynomial features. Our approach, named Tensor Machines, finds a parsimonious set of features by optimizing over the hypothesis class introduced by Kar and Karnick for random feature maps in a target-specific manner. Exploiting a natural connection between polynomials and tensors, we provide bounds on the generalization error of Tensor Machines. Empirically, Tensor Machines behave favorably on several real-world datasets compared to other state-of-the-art techniques for learning polynomial features, and deliver significantly more parsimonious models. Comment: 19 pages, 4 color figures, 2 tables. Submitted to ECML 201...|$|R
40|$|Conference Name: 1 st International Conference on High Performance Structures and Materials Engineering. Conference Address: Beijing, PEOPLES R CHINA. Time:MAY 05 - 06, 2011. A new SVM (Support Vector Machine) classifier-combination model, {{based on}} Hierarchical Partition approach, for {{enterprise}} credit assessment is proposed in this paper. Enterprise credit assessment {{is essentially an}} ordinal classification (or ranking), and the popular multi-classification technique does not deal with the ordinal information in the sample data. Hierarchical Partition approach makes use of the ordinal characteristics to simplify the structure of classifiers, {{as well as to}} <b>decrease</b> <b>the</b> <b>training</b> and testing <b>time.</b> It {{can also be used to}} solve the problems where the imbalance sample distribution and different loss-costs among different ranks present. Experimental results show that the generalization ability of the new model using Hierarchical Partition approach are better than that of neural network model and traditional 1 -vs- 1 combined SVM model...|$|R
40|$|As one of {{the most}} {{successful}} application of image analysis and understanding, face recognition has recently received significant attention, especially during the past few years. There are at least two reasons for this trend the first is the wide range of commercial and law enforcement applications and the second is the availability of feasible technologies after 30 years of research. The problem of machine recognition of human faces continues to attract researchers from disciplines such as image processing, pattern recognition, neural networks, computer vision, computer graphics, and psychology. The strong need for user-friendly systems that can secure our assets and protect our privacy without losing our identity in a sea of numbers is obvious. Although very reliable methods of biometric personal identification exist, for example, fingerprint analysis and retinal or iris scans, these methods depend on the cooperation of the participants, whereas a personal identification system based on analysis of frontal or profile images of the face is often effective without the participant’s cooperation or knowledge. The three categories of face recognition are face detection, face identification and face verification. Face Detection means extract the face from total image of the person. Face identification means the input to the system is an unknown face, and the system reports back the determined identity from a database of known individuals. Face verification means the system needs to confirm or reject the claimed identity of the input. My thesis was face verification in static images. Here a static image means the images which are not in motion. The eigenvectors based face verification algorithm gave the results on face verification in static images based upon the eigenvectors and neural network backpropagation algorithm. Eigen vectors are used for give the geometrical information about the faces. First we take 10 images for each person in same angle with different expressions and apply principle component analysis. Here we consider image dimension as 48 x 48 then we get 48 eigenvalues. Out of 48 eigenvalues we consider only 10 highest eigenvaues corresponding eigenvectors. These eigenvectors are given as input to the neural network for training. Here we used backpropagation algorithm for <b>training</b> <b>the</b> neural network. After completion of training we give an image which is in different angle for testing purpose. Here we check the verification rate (the rate at which legitimate users is granted access) and false acceptance rate (the rate at which imposters are granted access). Here neural network take more <b>time</b> for <b>training</b> purpose. <b>The</b> proposed algorithm gives the results on face verification in static images based upon the eigenvectors and neural network modified backpropagation algorithm. In modified backpropagation algorithm momentum term is added for <b>decrease</b> <b>the</b> <b>training</b> <b>time.</b> Here for using the modified backpropagation algorithm verification rate also slightly increased and false acceptance rate also slightly decreased...|$|R
40|$|Combining several {{classifiers}} {{has become}} a very active subdiscipline {{in the field of}} pattern recognition. For years, pattern recognition community has focused on seeking optimal learning algorithms able to produce very accurate classifiers. However, empirical experience proved that is is often much easier finding several relatively good classifiers than only finding one single very accurate predictor. The advantages of combining classifiers instead of single classifier schemes are twofold: it helps reducing the computational requirements by using simpler models, and it can improve the classification skills. It is commonly admitted that classifiers need to be complementary in order to improve their performances by aggregation. This complementarity is usually termed as diversity in classifier combination community. Although diversity is a very intuitive concept, explicitly using diversity measures for creating classifier ensembles is not as successful as expected. In this thesis, we propose an information theoretic framework for combining classifiers. In particular, we prove by means of information theoretic tools that diversity between classifiers is not sufficient to guarantee optimal classifier combination. In fact, we show that diversity and accuracies of the individual classifiers are generally contradictory: two very accurate classifiers cannot be diverse, and inversely, two very diverse classifiers will necessarily have poor classification skills. In order to tackle this contradiction, we propose a information theoretic score (ITS) that fixes a trade-off between these two quantities. A first possible application is to consider this new score as a selection criterion for extracting a good ensemble in a predefined pool of classifiers. We also propose an ensemble creation technique based on AdaBoost, by taking into account the information theoretic score for iteratively selecting the classifiers. As an illustration of efficient classifier combination technique, we propose several algorithms for building ensembles of Support Vector Machines (SVM). Support Vector Machines {{are one of the most}} popular discriminative approaches of pattern recognition and are often considered as state-of-the-art in binary classification. However these classifiers present one severe drawback when facing a very large number of training examples: they become computationally expensive to train. This problem can be addressed by decomposing the learning into several classification tasks with lower computational requirements. We propose to train several parallel SVM on subsets of <b>the</b> complete <b>training</b> set. We develop several algorithms for designing efficient ensembles of SVM by taking into account our information theoretic score. The second part of this thesis concentrates on human face detection, which appears to be a very challenging binary pattern recognition task. In this work, we focus on two main aspects: feature extraction and how to apply classifier combination techniques to face detection systems. We introduce new geometrical filters called anisotropic Gaussian filters, that are very efficient to model face appearance. Finally we propose a parallel mixture of boosted classifier for reducing the false positive rate and <b>decreasing</b> <b>the</b> <b>training</b> <b>time,</b> while keeping <b>the</b> testing time unchanged. The complete face detection system is evaluated on several datasets, showing that it compares favorably to state-of-the-art techniques...|$|R
40|$|Intrusion {{detection}} is {{a promising}} {{area of research}} in the domain of security with the rapid development of internet in everyday life. Many intrusion detection systems (IDS) employ a sole classifier algorithm for classifying network traffic as normal or abnormal. Due to the large amount of data, these sole classifier models fail to achieve a high attack detection rate with reduced false alarm rate. However by applying dimensionality reduction, data can be efficiently reduced to an optimal set of attributes without loss of information and then classified accurately using a multi class modeling technique for identifying the different network attacks. In this paper, we propose an intrusion detection model using chi-square feature selection and multi class support vector machine (SVM). A parameter tuning technique is adopted for optimization of Radial Basis Function kernel parameter namely gamma represented by ‘ϒ’ and over fitting constant ‘C’. These are the two important parameters required for the SVM model. The main idea behind this model is to construct a multi class SVM which has not been adopted for IDS so far to <b>decrease</b> <b>the</b> <b>training</b> and testing <b>time</b> and increase the individual classification accuracy of the network attacks. The investigational results on NSL-KDD dataset which is an enhanced version of KDDCup 1999 dataset shows that our proposed approach results in a better detection rate and reduced false alarm rate. An experimentation on the computational <b>time</b> required for <b>training</b> and testing is also carried out for usage in time critical applications...|$|R
40|$|Today, the {{increasing}} complexity, performance requirements {{and cost of}} current (and future) applications in society is transversal {{to a wide range}} of activities, from science to industry. The scale of the data from Web growth and advances in sensor data collection technology have been rapidly increasing the magnitude and complexity of tasks that Machine Learning (ML) algorithms have to solve. This growth is driving the need to extend the applicability of existing ML algorithms to larger datasets and to devise parallel algorithms that scale well with the volume of data or, in other words, can handle “Big Data”. In this Thesis, we partly contribute to solving this problem, by making use of two complementary components: a body of novel ML algorithms and a set of high-performance ML parallel implementations for adaptive multi-core machines. In the first component, a new adaptive step size technique that enhances the convergence of Restricted Boltzmann Machines (RBMs), thereby effectively <b>decreasing</b> <b>the</b> <b>training</b> <b>time</b> of Deep Belief Networks (DBNs), is presented. Also, a novel Semi-Supervised Non-Negative Matrix Factorization (SSNMF) algorithm, aiming at extracting the most discriminating characteristics of each class, while reducing substantially the overall time required for generating the models, is proposed. In addition, a novel Incremental Hypersphere Classifier (IHC) with built-in multi-class support, which is able to accommodate memory and computational restrictions while providing good classification performance, is presented. This highly-scalable algorithm can update models and classify new data in real-time as well as handle concept drift scenarios. Moreover, since it keeps the samples that are near the decision frontier while removing noisy and less relevant ones, it can select a representative subset of the data for applying more sophisticated algorithms in a fraction of the time required for the complete dataset. A learning framework (IHC-SVM), encompassing the IHC and Support Vector Machine (SVM) algorithms is validated in a real-world case study of protein membership prediction. Overall the resulting system proved to be able to excel the baseline SVM (with an F-measure of 96. 39 %) using only a subset of the data (ca. 50 %) and demonstrated its capacity to deal with the everyday dynamic changes of real-world biological databases. In another direction, and motivated by the need to deal with missing data often occurring in large-scale data, a novel solution, designated by Neural Selective Input Model (NSIM), is proposed. The method empowers Neural Networks (NNs) with the ability to handle Missing Values (MVs) and excels single imputation techniques while offering better or similar classification performance than the state-of-the-art multiple imputation methods. With the new methodology we have successfully addressed a real-world case study of bankruptcy prediction in a large dataset of French companies, with results (F-measure of 95. 70 %) that are superior to previous approaches. The backbone of the second component of this Thesis is a Graphics Processing Unit (GPU) computational framework, named GPU Machine Learning Library (GPUMLib), which aims at providing the building blocks for developing high-performance GPU parallel ML software, promote cooperation within the field and contribute to the development of innovative applications. The rationale consists of taking advantage of the GPU high-throughput parallel architecture to expand the scalability of supervised, semi-supervised and unsupervised ML algorithms. Since its release, GPUMLib, now with over 2, 000 downloads, has benefited researchers worldwide. New GPU parallel implementations of the Back-Propagation (BP) and Multiple Back-Propagation (MBP) supervised algorithms, integrating the NSIM, are presented, providing significant speedups (up to 180 ×). In particular, these implementations played an important role for the detection of Ventricular Arrhythmias (VAs) (with a sensitivity of 98. 07 %) that improved previous work, by reducing the computation time from weeks to hours. In this line, an Autonomous Training System (ATS) is designed to automatically find GPU high-quality solutions. In the unsupervised verge, a GPU parallel implementation of the CD–k algorithm, which boosts considerably the RBMs and DBNs training speed, is presented, achieving speedups up to 46 ×. Additionally, new GPU parallel implementations of the Non-Negative Matrix Factorization (NMF) algorithm are presented, yielding speedups up to 706 ×. Both unsupervised implementations are tested in benchmarks and in real datasets. Overall, this Thesis contributes with adaptive multi-core machines for exploring “Big Data”, which – as we hope – will have a positive impact in solving otherwise intractable ML problems. Tese de doutoramento em Engenharia Informática, apresentada à Faculdade de Ciências e Tecnologia da Universidade de Coimbr...|$|R
40|$|In all pattern {{recognition}} systems, increasing the recognition speed and {{improvement of the}} recognition accuracy are two important goals. However, these items usually perform against each other, when the former is improved, <b>the</b> latter is <b>decreased,</b> and vice versa. In this thesis, {{the focus is on}} both items; <b>decreasing</b> <b>the</b> overall processing time and increasing the system accuracy. To such an aim, <b>the</b> number of <b>training</b> samples is decreased by proposing a technique for dataset size reduction that leads to <b>decrease</b> of <b>the</b> training/testing time. Also, the number of features is decreased by proposing a new technique for dimensionality reduction. It <b>decreases</b> <b>the</b> <b>training</b> and testing <b>time,</b> and by deleting less important features, it increases the system accuracy, too. The existing dataset size reduction algorithms, usually remove samples near to the centers of classes, or support vector samples between different classes. However, the former samples include valuable information about the class characteristics, and are important to make system model. The latter samples are important for evaluating system efficiency and adjustment of system parameters. The proposed dataset size reduction method employs Modified Frequency Diagram technique to create a template for each class. Then, a similarity value is calculated for each pattern. Thereafter, the samples in each class are rearranged based on their similarity values. Consequently, <b>the</b> number of <b>training</b> samples is reduced by Sieving technique. As a result, the training/testing time is decreased. In other part of this study, the number of extracted features is decreased by proposing a new method, which is, analyzing the one-dimensional and two-dimensional spectrum diagrams of standard deviation and minimum to maximum distributions for initial feature vector elements. In recent years, the attractive nature of Optical Character Recognition (OCR) has caused the researchers to develop various algorithms for recognizing different alphabets. Target performance for an OCR system is to recognize at least five characters per second with 99. 9...|$|R
30|$|There are six {{different}} dimensions of parallelization for neural networks introduced by Nordstrom et al. [7], each with increasing communication costs. In <b>the</b> simplest dimension, <b>training</b> session parallelism, there exists virtually no communication costs. Each node {{in the system}} gets an entire copy of the dataset and model, each initialized with different weights. Each of <b>the</b> nodes <b>trains</b> a model and at <b>the</b> end of <b>training,</b> <b>the</b> model that performs the highest is selected. This technique, although rather trivial in concept, can be useful because training neural networks have the propensity to get stuck in local minima. The multiple, initialized models aim to widen the search to hopefully find a global minimum [8]. Another advantage to this approach is its parallelism is unbound, i.e. the increase in nodes and <b>the</b> <b>decrease</b> in <b>training</b> <b>time</b> (for that many different models) is nearly perfectly linear. <b>The</b> next dimension, <b>training</b> example parallelism, also known as Data Parallelism, splits <b>the</b> <b>training</b> data onto multiple nodes. Each node then computes on a smaller data size, reducing <b>training</b> <b>time.</b> This approach is examined in more detail in the coming sections and is the parallelization paradigm used in this paper’s work. The final dimension that is suitable for a cluster computer environment is Node Parallelism, {{which is similar to}} model parallelism. In model parallelism, the neural network model is divided up and distributed across multiple nodes where each node only trains its portion of the model. For completeness, layer parallelism, weight parallelism, and bit parallelism are the remaining three dimensions. Node parallelism utilizes pipelining to increase the throughput of <b>the</b> <b>training</b> instances being calculated through the network (either during forwardpropagation or backpropagation). Weight parallelism refers to the simultaneous calculations of all neurons in a given layer. Lastly, Bit parallelism is the bit level parallelization and Nordstorm et al. states it is often taken for granted. However, these three dimensions are outside the scope of this paper and are generally addressed, to some degree, by the optimizations in the neural network libraries used and the optimizations in the underlying computer operating system.|$|R
