6066|4558|Public
5|$|The {{second major}} {{instrument}} was the MKF-6M multispectral camera, which carried out Earth-resources observations. An improved {{form of a}} camera first tested on Soyuz 22, the camera captured an area of 165220kilometres with <b>each</b> <b>image,</b> down to a resolution of 20metres. <b>Each</b> <b>image</b> was captured simultaneously in six bands in 1200-frame cassettes, which required regular replacement due to the fogging effects of radiation. Salyut 6 also featured a KATE-140 stereoscopic topographic mapping camera with a focal length of 140millimetres, which captured images of 450450kilometres with a resolution of 50metres in the visible and infrared spectra, which could be operated either remotely or by the resident crews. The photographic capabilities of the station were, therefore, extensive, and the Soviet Ministry of Agriculture had planted a number of specifically selected crops at test sites to examine {{the capabilities of the}} cameras.|$|E
5|$|Three {{volumes of}} albums titled Maria-sama ga Miteru: Haru Image Album {{containing}} image songs and background music tracks were released between April and September 2005. <b>Each</b> <b>image</b> album {{was assigned to}} one of the three families of roses; the songs were sung by the voice actors of the anime series. A vocal album titled Christmas Album was released in December 2008. Shueisha produced 12 drama CDs between January 14, 2004 and December 14, 2007, and Frontier Works produced three additional drama CDs between July 24, 2009 and July 22, 2010; the CDs use the same voice actors from the anime series. The drama CDs are based on the stories in the novels. The fifth and tenth drama CDs by Shueisha were released in limited edition versions each with a slipcase and a pair of character mini-figures.|$|E
5|$|Tombaugh's {{task was}} to {{systematically}} capture sections {{of the night sky}} in pairs of images. <b>Each</b> <b>image</b> in a pair was taken two weeks apart. He then placed both images of each section in a machine called a blink comparator, which by exchanging images quickly created a time lapse illusion of the movement of any planetary body. To reduce the chances that a faster-moving (and thus closer) object be mistaken for the new planet, Tombaugh imaged each region near its opposition point, 180 degrees from the Sun, where the apparent retrograde motion for objects beyond Earth's orbit is at its strongest. He also took a third image as a control to eliminate any false results caused by defects in an individual plate. Tombaugh decided to image the entire zodiac, rather than focus on those regions suggested by Lowell.|$|E
50|$|It {{has built}} in GPS with GLONASS, {{that can be}} {{selected}} to write to <b>each</b> <b>image's</b> Exif data.|$|R
30|$|After normalization, we must {{combine these}} five factors for <b>each</b> sub <b>image</b> {{to produce an}} overall Importance Measure (IM) for <b>each</b> sub <b>image.</b>|$|R
3000|$|... (8) Projecting the 3 D genome {{model to}} {{generate}} 60 projection <b>images</b> for <b>each</b> genome <b>image</b> {{according to the}} 60 equivalent icosahedral orientations of the capsid. The cross coefficients between <b>each</b> genome <b>image</b> and the 60 projection images are calculated here. The orientation of the projection <b>image</b> best-matched with <b>each</b> genome <b>image</b> is assigned the corresponding orientation to the genome image for further analysis.|$|R
25|$|The {{following}} images {{illustrate the}} differences. The top note in <b>each</b> <b>image</b> is a web note; the bottom note is a sheet fed note.|$|E
25|$|In 2014, Padma et al. used {{combined}} wavelet statistical texture {{features to}} segment and classify AD benign and malignant tumor slices. Zhang et al. found kernel {{support vector machine}} decision tree had 80% classification accuracy, with an average computation time of 0.022s for <b>each</b> <b>image</b> classification.|$|E
25|$|With color {{motion picture}} film, {{information}} about {{the color of the}} light at <b>each</b> <b>image</b> point is also captured. This is done by analyzing the visible spectrum of color into several regions (normally three, commonly referred to by their dominant colors: red, green and blue) and recording each region separately.|$|E
3000|$|... 12 [*]=[*] 4, we {{were able}} to recover the low-quality version of <b>each</b> secret <b>image</b> in the second secret group, [Lena, Pepper], and the {{lossless}} version of <b>each</b> secret <b>image</b> in the first secret group, [House, Cameraman], respectively.|$|R
5000|$|The 150 mph {{paintings}} {{depart from}} images of individuals jumping from New York's World Trade Center during the September 11 attacks. <b>Each</b> <b>image’s</b> human subject {{has been removed}} leaving the architecture itself as sole [...] "witness and unintentional memorial." ...|$|R
30|$|In Section 3, {{we present}} how the region-tree {{structure}} assists in the illumination process of a 2 D drawing. Its advantage {{stems from the}} possibility of dealing individually, and in a consistent way, <b>each</b> <b>images</b> region allowing also process them in parallel.|$|R
25|$|The GIF Specification allows <b>each</b> <b>image</b> {{within the}} logical screen of a GIF file to specify {{that it is}} interlaced; i.e., that {{the order of the}} raster lines in its data block is not sequential. This allows a partial display of the image that can be {{recognized}} before the full image is painted.|$|E
25|$|Chaplot et al. was {{the first}} to use Discrete Wavelet Transform (DWT) {{coefficients}} to detect pathological brains. Maitra and Chatterjee employed the Slantlet transform, which is an improved version of DWT. Their feature vector of <b>each</b> <b>image</b> is created by considering the magnitudes of Slantlet transform outputs corresponding to six spatial positions chosen according to a specific logic.|$|E
25|$|Before DEFLATE is applied, {{the data}} is {{transformed}} via a prediction method: a single filter method {{is used for the}} entire image, while for <b>each</b> <b>image</b> line, a filter type is chosen to transform the data to make it more efficiently compressible. The filter type used for a scanline is prepended to the scanline to enable inline decompression.|$|E
50|$|It was {{not until}} 2004 that the first rapid {{acquisition}} and quantification method for creating parametric maps was invented. This new acquisition method performs 8 acquisitions at 4 different excitation delays, giving 8 values to estimate T1, T2, PD and M0 for <b>each</b> <b>images</b> voxel.|$|R
3000|$|The {{registers}} used {{between the}} column accumulators are synchronized {{at the end}} of each row; so their clock enable depends on the image topology. In our case, corners have been filled with zeros before dividing the image. Therefore, the size of <b>each</b> <b>image's</b> component is [...]...|$|R
30|$|PBoW [39] model: The pyramid BoW (PBoW) {{model is}} that the BoW model {{incorporates}} with the spatial pyramid which could learn the spatial information of cloud <b>images.</b> We divide <b>each</b> cloud <b>image</b> into three levels, i.e., 1, 2, and 4, which results in 1, 4, and 16 cells, respectively. Thus, for <b>each</b> cloud <b>image,</b> it contains a total of 21 cells. The PBoW model also represents cloud images as histograms based on each cell. Herein, the codebook is obtained {{in the same way}} as BoW. Hence, the histogram for <b>each</b> cloud <b>image</b> is 29400 dimensionality.|$|R
25|$|The poem Light encapsulates {{those qualities}} of {{personal}} vision that characterise Diesendorf's poetry. She brought to insular Australia her experiences of Europe, memories of music, art and gaiety, tempered by loss, deprivation and courage. In one short poem she {{touches on the}} love of her dying husband, her childhood poverty and the death of her baby brother, transforming <b>each</b> <b>image</b> with the radiance of light into something beautiful.|$|E
25|$|When {{he wants}} to insert something, he {{overlaps}} the images... Jean-Luc separates the images he's editing from the editing console. He arranges the screen perpendicular to the console so that there's nothing {{between him and the}} image. He has to turn away to make the edit. He decides edits very quickly. After he's seen all the footage, he uses small thumbnails from photocopied or printed images of each scene, and makes books, gluing <b>each</b> <b>image</b> to a page.|$|E
25|$|Since <b>each</b> <b>image</b> block {{requires}} its own {{local color}} table, a GIF file having lots of image blocks {{can be very}} large, limiting the usefulness of full-color GIFs. Additionally, not all GIF rendering programs handle tiled or layered images correctly. Many rendering programs interpret tiles or layers as animation frames and display them in sequence as an endless animation with most web browsers automatically displaying the frames with a delay time of 0.1 seconds or more.|$|E
3000|$|... 22 [*]=[*] 5, we {{were able}} to recover the low-quality version of <b>each</b> secret <b>image</b> in the third secret group, and the {{lossless}} version of <b>each</b> secret <b>image</b> in the first and second secret groups, respectively. Finally, for the scenario where we received all six stego JPEG codes, because r [...]...|$|R
30|$|USPS dataset The USPS dataset is a {{handwritten}} digit dataset, which contains two parts: the training set with 7291 samples, {{and the test}} set with 2007 samples. In this experiment, we randomly selected 7000 images of the 10 letters. Thus, there are 700 <b>images</b> in <b>each</b> category. The size of <b>each</b> <b>images</b> is 16  ×  16 pixels.|$|R
30|$|Select {{source image}} {{and find the}} largest human region in <b>each</b> source <b>image.</b>|$|R
25|$|To {{implement}} the ideas described in previous sections, one {{need to know}} how to derive a computationally efficient invariant representation of an image. Such unique representation for <b>each</b> <b>image</b> can be characterized as it appears by a set of one-dimensional probability distributions (empirical distributions of the dot-products between image and a set of templates stored during unsupervised learning). These probability distributions in their turn can be described by either histograms or a set of statistical moments of it, as it will be shown below.|$|E
25|$|The format {{supports}} up to 8 {{bits per pixel}} for <b>each</b> <b>image,</b> {{allowing a}} single image to reference its own palette of up to 256 different colors chosen from the 24-bit RGB color space. It also supports animations and allows a separate palette of up to 256 colors for each frame. These palette limitations make GIF less suitable for reproducing color photographs and other images with color gradients, but it is well-suited for simpler images such as graphics or logos with solid areas of color.|$|E
25|$|Fourier {{transformation}} is also {{useful as a}} compact representation of a signal. For example, JPEG compression uses {{a variant of the}} Fourier transformation (discrete cosine transform) of small square pieces of a digital image. The Fourier components of each square are rounded to lower arithmetic precision, and weak components are eliminated entirely, so that the remaining components can be stored very compactly. In image reconstruction, <b>each</b> <b>image</b> square is reassembled from the preserved approximate Fourier-transformed components, which are then inverse-transformed to produce an approximation of the original image.|$|E
50|$|The group {{consists}} of five members. <b>Each</b> member's <b>image</b> {{is represented by}} a fruit.|$|R
40|$|In {{computer}} graphics and virtual reality, multi-spectral and panoramic images are needed. To get a panoramic multi-spectral images, {{the approach to}} use a filter transmitting spectral bands are spatially varying is proposed. A filter is attached to an 8 -bit black/white camera. Then take images (there is no parallax). Each scene points are taken many times in different spectral band. After that mosaic images. Here, this image mosaicing have problems which different from traditional RGB-image mosaicing. First, illumination and spectral response greatly effect images. Second, because of using spatially varying filter, value of the scene point of <b>each</b> <b>images</b> are different depend on the position on the <b>each</b> <b>images.</b> Last, if put one upon another in a little shifted, the multi-spectral-data are shifted greatly. So exact mosaicing are needed. This research propose the way of making rotational panoramas using images these are taken by camera which the filter is attached, and the way of mosaicing which is faster and more accurate. Acknowledgemen...|$|R
3000|$|... (4) Projecting {{the masked}} capsid density map {{according}} to the icosahedral orientation of <b>each</b> particle <b>image.</b>|$|R
25|$|Lawley {{was photographed}} by Kenneth Willardt for a fine art exhibition, Size Does Matter. The show was on exhibit from November to December 2013 at the 588 Gallery in Chelsea. Lawley was photographed nude with various animals,including rabbits, an owl, an octopus and tarantulas. <b>Each</b> <b>image</b> {{in the exhibit}} had a QR code. When the QR code was scanned with a cell phone, added {{movement}} in the image was visible {{on the cell phone}} screen. In addition to the images in the gallery, a billboard with Lawley flanked by bunnies was placed on the West Side Highway in Manhattan.|$|E
25|$|Gerber {{is used in}} PCB {{fabrication}} data. PCBs {{are designed}} on a specialized electronic design automation (EDA) or a computer-aided design (CAD) system. The CAD systems output PCB fabrication data to allow fabrication of the board. This data typically contains a Gerber file for <b>each</b> <b>image</b> layer (copper layers, solder mask, legend or silk...). Gerber is also the standard image input format for all bare board fabrication equipment needing image data, such as photoplotters, legend printers, direct imagers or automated optical inspection (AOI) machines and for viewing reference images in different departments. For assembly the fabrication data contains the solder paste layers and the central locations of components to create the stencil and place and bond the components.|$|E
25|$|During encoding, {{participants}} are typically exposed to 1–10 visual patterns while {{connected to a}} brain imaging device. As the subject encodes the visual patterns researchers are able to directly view the activation of areas involved in visual memory encoding. During recall subjects again need to have all visual stimuli removed {{by means of a}} dark room or blindfolding to avoid interfering activation of other visual areas in the brain. Subjects are asked to recall <b>each</b> <b>image</b> clearly in their mind's eye. While recalling the images researchers are able view the areas activated by the visual memory task. Comparing the control 'baseline' state to the activated areas during the visual memory task allows researchers to view which areas are used during visual memory.|$|E
40|$|This paper {{presents}} a demosaicing approach which combines the Bayer patterns of multiple overlapping images. Multiple {{images of the}} same subject are taken, where the camera is free to pan, tilt, and rotate around its optical axis. The images are spatially registered and a Bayer pattern mosaic is created by combining <b>each</b> <b>image’s</b> Bayer pattern. In the region of overlap, <b>each</b> additional <b>image</b> “fills in ” the gaps in the Bayer pattern for each color channel, creating a completely filled Bayer mosaic. The presented method is implemented in graphics hardware, which provides hardware acceleration. Results are shown in which increased color channel resolution is achieved in the overlapping regions of multiple images. 1...|$|R
5000|$|... {{overall size}} of <b>each</b> boot <b>image</b> can be {{controlled}} to fit within network or removable disk limits ...|$|R
5000|$|... #Caption: Navigational {{image for}} The Sacrifice, with <b>each</b> {{character}} <b>image</b> linking to {{one part of}} the comic.|$|R
