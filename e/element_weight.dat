19|407|Public
5000|$|RM(r, m) codes can be decoded using {{majority}} logic decoding. The {{basic idea}} of majority logic decoding isto build several checksums for each received code word element. Since {{each of the}} different checksums must {{all have the same}} value (i.e. the value of the message word <b>element</b> <b>weight),</b> we can use a majority logic decoding to decipherthe value of the message word element. Once each order of the polynomial is decoded, the received word is modifiedaccordingly by removing the corresponding codewords weighted by the decoded message contributions, up to the present stage.So for a rth order RM code, we have to decode iteratively r+1, times before we arrive at the finalreceived code-word. Also, the values of the message bits are calculated through this scheme; finally we can calculatethe codeword by multiplying the message word (just decoded) with the generator matrix.|$|E
40|$|The method {{involves}} distributing amplitude adjustment between active transmitter and/or receiver modules and a {{high frequency}} supply network such that a predefined portion of selected antenna <b>element</b> <b>weight</b> is taken by the high frequency supply network and an amplitude adjustment member. The supply network and the transmitter and/or receiver modules are designed such that a transfer function is provided by a functional element, where {{the distribution of the}} function corresponds to the portion of the <b>element</b> <b>weight...</b>|$|E
40|$|International audienceWe are {{interested}} in aircraft trajectories seen as stochastic processes. These processes evolve in an unknown atmospheric random environnment. As several aircraft parameters are unknown, such as true airspeed (TAS) and wind, we have to estimate them. To this end, we suggest to use ensemble weather forecasts, which give different scenarios for the atmosphere, with a system of trajectory predictions. In this way, we evaluate the likelihood of each element and we construct a random weather environment organized by the <b>element</b> <b>weight.</b> To get this result, we use sequential Monte Carlo methods (SMC) in the special context of random environment. We propose to use particle Markov chain Monte Carlo method (pMCMC) to estimate the aircraft parameters...|$|E
40|$|This work gives a {{methodology}} for analyzing matroids with random <b>element</b> <b>weights,</b> with {{emphasis placed on}} independent, exponentially distributed <b>element</b> <b>weights.</b> The minimum <b>weight</b> basic <b>element</b> in such a structure is shown to be an absorbing state in a Markov chain, while the distribution of weight of the minimum <b>weight</b> <b>element</b> is shown to be of phase-type. We then present two sided bounds for matroids with NBUE distributed weights, {{as well as for}} weights with bounded positive hazard rates. We illustrate our method using the transversal matroid to solve stochastic assignment problems. (Author) (kr) Naval Postgraduate School Research Council. [URL] Direct FundingN...|$|R
40|$|AbstractIn {{this paper}} a general {{bottleneck}} combinatorial optimization problem with uncertain <b>element</b> <b>weights</b> modeled by fuzzy intervals is considered. A possibilistic formalization {{of the problem}} and solution concepts in this setting, which lead to compute robust solutions under fuzzy weights, are given. Some algorithms for finding a solution according to the introduced concepts and evaluating optimality of solutions and elements are provided. These algorithms are polynomial for bottleneck combinatorial optimization problems with uncertain <b>element</b> <b>weights,</b> if their deterministic counterparts are polynomially solvable...|$|R
3000|$|P(θ) is voltage element pattern. The {{number of}} {{radiators}} is denoted as N and {{is set to}} 100. Decision variables are complex <b>element</b> <b>weights</b> A [...]...|$|R
40|$|Optic Properties on AgGaSe 2 Polycristal Fabrication. Polycristal AgGaSe 2, is {{compound}} (I-III-VI 2) a semiconductoras {{basic material}} for thin film for solar cell. Polycristal was succesfully grown using Bridgmann Method, heated onsequential temperature treatment until 850 °C and cooled down slowly until room temperature. Results observed were inthe form of ingot (bars) with {{more or less}} 3 cm length and 13 mm in diameter. By using X-Ray Fefraction, compositionobtained of each <b>element</b> (<b>weight</b> %) was Ag = 29, 3996 %, Ga = 36, 8123 % and Se = 30, 29 % while using X-RayDifraction lattice parameter obtained/calculated a = 4, 4112 Å, c = 8, 8854 Å, and c/a = 2, 01426...|$|E
40|$|This report studies {{clamping}} {{phenomena in}} placed revetments on the Dutch dikes. One {{of the reasons}} of placing stones is that {{the stability of the}} single stones increases. Pull–out tests on single stones show a strength up to 10 times the single stone weight. In present standardized design methods for placed revetments there is no significant contribution of clamping to the design stability of the single stone. The physical model study is based on a structural beam model. The beam consists of loose column or block elements on an elastic foundation. The joined elements act as a structure due to the normal (clamping) force from the gravity component along the dike slope. The elements are lifted by wave pressure differences, which occur during run-back and slamming of the waves on the relatively smooth slope. In analytical models it can be demonstrated that upward pressure up to say 2. 5 times the <b>element</b> <b>weight</b> can be taken. For concentrated pressure peaks even higher clamping factors are found. The clamping factor is defined as the pressure load divided by the <b>element</b> <b>weight,</b> and can be interpreted as a factor of increased strength of the revetment. One of the most important threats for utilisation of these results is the uncertainty regarding the actual presence of normal force in the top layer of the revetment. Another thing is that there are always loose elements. Failure of these elements should not necessary lead to progressive failure of the top layer. This opens possibilities for utilising the clamping strength for resistance gainst extreme wave loads in the design of revetments. Report is in Dutch...|$|E
40|$|Deterministic design {{methods are}} {{commonly}} used to determine preliminary breakwater designs. Partial safety factors take into account previous experiences and provide a robust preliminary design. However, local circumstances can prove to differ considerably compared to average design conditions and stochastic variations in breakwater strength parameters are commonly neglected. With new armouring techniques, such as Core-loc armouring, the uncertainties about the armour strength are relatively large. Design guidelines include a safety factor, but often an additional safety margin is applied in the final design of the armouring to ensure stability. This can result in structure strengths more, or less, than locally required. The economic optimum geometry with the lowest costs is possibly not achieved. These costs consist of the initial construction cost, the collapse damage cost and the economic damage cost due to downtime. To include the damage cost or risk (= failure probability x economic consequence) of breakwater collapse and functional failure, a probabilistic approach {{can be used to}} determine the failure probabilities. In Veracruz, Mexico, the port authority of the Port of Veracruz investigates the feasibility of a large port extension next to the existing port of Veracruz. In the preliminary layout a Core-loc armoured breakwater is anticipated to provide shelter at a container terminal and quay location. Deterministic design methods result in an <b>element</b> <b>weight</b> of 18. 7 t (8. 5 m 3). Two construction methods are evaluated: a water-based and a land-based construction method, with crest heights of 3 m +SWL and 11 m +SWL respectively. In this deterministic evaluation the economic consequences of functional failure are not taken into account, but both alternatives fulfilled the harbour tranquillity restrictions by the port authority: a maximum downtime of 5 %. The water-based construction method is elected as the best construction method, due to lower construction costs of 110. 7 $ million. The deterministic breakwater design is optimised with a probabilistic method for the most important parameters: the weight of the Core-loc elements and the breakwater crest height. And a progressive deterioration over time of the strength of the Core-loc armour is taken into account. The probabilities of collapse and functional failure of the breakwater and the economic consequences of failure are determined for 56 combinations of <b>element</b> <b>weight</b> and crest height. The probability of collapse is composed of two failure mechanisms: the Core-loc armour and the toe structure. The probabilities of failure and economic consequences are time dependent, due to the sea level rise, the deterioration of the breakwater armour and the economic development of the port over the lifetime of 50 years. Therefore, all alternatives have different probabilities of failure for each year. Discounting all costs to a single year the economic optimal design geometry over the total lifetime proves to have a Core-loc <b>element</b> <b>weight</b> of 30. 8 t (14 m 3) and a crest height of 7 m +SWL. The construction costs of this geometry are 153 $ million. A crest height of 7 m +SWL complies with an allowable downtime of approximately 0. 2 %. The downtime costs are of considerable more influence than estimated by the port authority. Also the consequences of a breakwater collapse result in a 65 % heavier <b>element</b> <b>weight.</b> The discounted total costs over the lifetime of the breakwater are 219 $ million for the probabilistic design and 468 $ million for the deterministic design. The collapse costs and downtime costs have a significant influence on the total costs over the lifetime and therefore on the economic optimal geometry of the breakwater. A more robust design than deterministically derived can reduce the total cost over the lifetime by almost 50 %. Civil Engineering and Geoscience...|$|E
40|$|Abstract. Considering {{the model}} of {{computing}} under uncertainty where <b>element</b> <b>weights</b> are uncertain but can be obtained at a cost by query operations, we study the problem of identifying a cheapest (minimum-weight) set among a given collection of feasible sets using a minimum number of queries of <b>element</b> <b>weights.</b> For the general case we present an algorithm that makes at most d · OPT + d queries, where d is the maximum cardinality of any given set and OPT is the optimal number of queries needed to identify a cheapest set. For the minimum multi-cut problem in trees with d terminal pairs, we give an algorithm that makes at most d ·OPT + 1 queries. For the problem of computing a minimum-weight base of a given matroid, we give an algorithm that makes at most 2 ·OPT queries, generalizing a known result for the minimum spanning tree problem. For each of our algorithms we give matching lower bounds. ...|$|R
2500|$|Recent {{enhancements}} to {{the classification}} tree method include the prioritized test case generation: It {{is possible to}} assign <b>weights</b> to the <b>elements</b> of the classification tree in terms of occurrence and error probability or risk. These weights are then used during test case generation to prioritize test cases. Statistical testing is also available (e.g. for wear and fatigue [...] tests) by interpreting the <b>element</b> <b>weights</b> as a discrete probability distribution.|$|R
40|$|Considering {{the model}} of {{computing}} under uncertainty where <b>element</b> <b>weights</b> are uncertain but can be obtained at a cost by query operations, we study the problem of identifying a cheapest (minimum-weight) set among a given collection of feasible sets using a minimum number of queries of <b>element</b> <b>weights.</b> For the general case we present an algorithm that makes at most d·OPT+d queries, where d is the maximum cardinality of any given set and OPT is the optimal number of queries needed to identify a cheapest set. For the minimum multi-cut problem in trees with d terminal pairs, we give an algorithm that makes at most d·OPT+ 1 queries. For the problem of computing a minimum-weight base of a given matroid, we give an algorithm that makes at most 2 ·OPT queries, generalizing a known result for the minimum spanning tree problem. For each of our algorithms we give matching lower bounds. Peer-reviewedPost-print 39 th International Symposium, MFCS 2014, Budapest, Hungary, August 25 - 29, 2014. In Proceedings, Part I...|$|R
40|$|AbstractSafety {{accidents}} {{in the coal}} mine occurred frequently, that how to reduce them became an important national task, which the hazards identification and the risk forecast work in the coal mine system can solve. In the process of risk forecast in the coal mine system, considering characteristics that system risk is different in different period, the IDO (identification, difference, opposition) change rule of the set pair which has <b>element</b> <b>weight</b> is analyzed, and {{on the basis of}} which, the system risk forecast model based on GSPA-MARKOV is put forward. The application example shows that the risk state in the coal mine system is forecasted by the transition probability and the ergodicity in the model, which embodies fully dynamic, predictable and so on, thus it provides a new method to determine the risk state in the coal mine system...|$|E
40|$|ACCESS RESTRICTED TO THE UNIVERSITY OF MISSOURI AT AUTHOR'S REQUEST. ] Carbon {{nanotube}} (CNTs) are allotropes {{of carbon}} with a cylindrical nanostructure. Very {{different from other}} materials, CNTs show a great value for electronics, thermal conductivity, optics and other fields of science and technology. In particular, due to their extraordinary tensile strength and elastic modulus, carbon nanotubes have huge applications in various structural materials. My academic research is focus on creating a new CNT structure and test its mechanical properties. The secondary growth CNTs grown on the primary CNTs is {{the direction of the}} experiment research. CVD and ALD coating methods are used to produce the secondary growth CNT forest on two different substrates. In the analysis setups, SEM with EDS mapping and TEM indicate the secondary CNT structure. TGA and Raman Spectra are used to analyze the <b>element,</b> <b>weight</b> percent, and the functionalization of CNTs. Nano indentation calculates the mechanical properties of the secondary CNTs such as loading strain, unloading stiffness and the elastic potential energy. After the mechanical analysis and the microscopy observation, the secondary branched structure is successfully found and the functionalization, stiffness and the elastic energy of the CNT forests have enhanced after secondary growth. The single-walled and double-walled CNTs have also been found after secondary growth...|$|E
40|$|In this study, {{we propose}} a new {{multilevel}} optimisation process which responds to these requirements. The original optimization problem is decomposed {{into a system}} level and several element levels, aiming at the minimization of <b>element</b> <b>weight</b> under inequality constraints. Bounds of these constraints are given by the system level optimisation. In order {{to speed up the}} process and avoid local minima–which is met in most multilevel optimisation schemes- we replace the system constraints by a surrogate model. Last, a penalty term added in the "element level" objective allows us to force the equality of some design variables among all elements. The surrogate is incrementally built along system/element level loops from local optimisation results. For this reason, we choose to use Support Vector Regression methodology, which is more suitable to incremental training and data compression than more classical non-linear regression schemes such as neural networks are. The proposed multilevel strategy is tested on the classical ten bar truss problem; the benchmark structure is subject to several load cases and its weight is minimized under both mechanical and geometrical (equality in detailed variables) constraints. We obtain good results in terms of objective value, convergence time and detailed variables continuity. The implementation of our scheme on a full-size industrial problem: dimensioning of stiffened panels of a fuselage is under development...|$|E
40|$|Abstract — In {{this paper}} a general {{bottleneck}} combinatorial optimization problem with uncertain <b>element</b> <b>weights</b> modeled by fuzzy intervals is considered. A rigorous possibilistic formalization {{of the problem}} and solution concepts in this setting that lead to finding robust solutions under fuzzy weights are given. Some algorithms for finding a solution according to the introduced concepts and evaluating optimality of solutions and elements are provided. Keywords — Bottleneck combinatorial optimization, Interval, Fuzzy interval, Fuzzy optimization and design, Possibility theory...|$|R
40|$|In {{the work}} there are {{presented}} {{the results of}} studying the filtering <b>elements</b> <b>weight</b> changes when heated. For all the variants of filtering elements at the primary stages of heating there is characteristic increasing their linear dimensions due to their thermal expansion. It was established that using hydrolyzed ethyl silicate as a binder, as well as water solution of liquid glass with addition of aluminum powder in the refractory material permits to obtain filtering elements without high-temperature solid-phase sintering...|$|R
40|$|This paper {{deals with}} a general {{combinatorial}} optimization problem in which closed intervals and fuzzy intervals model uncertain <b>element</b> <b>weights.</b> The notion of a deviation interval is introduced, which allows us to characterize the optimality and the robustness of solutions and elements. The problem of computing deviation intervals is addressed and some new complexity results in this field are provided. Possibility theory is then applied to generalize a deviation interval and a solution concept to fuzzy ones. Minmax regret Interval Possibility theory Combinatorial optimization...|$|R
40|$|For {{studying}} {{the law of}} crack propagation around a gas drilling borehole, an experimental study about coal with a cavity under uniaxial compression was carried out, with the digital speckle correlation method capturing the images of coal failure. A sequence of coal failure images and the full-field strain of failure were obtained. The strain softening characteristic was shown by the curve. A method of curve dividing—named fitting-damaging—was proposed, combining the least square fitting residual norm and damage fraction. By this method, the five stages and four key points of a stress-strain curve were defined. Then, the full-field stress was inverted {{by means of the}} theory of elasticity and the adjacent <b>element</b> <b>weight</b> sharing model. The results show that σci was 30. 28 – 41. 71 percent of σf and σcd was 83. 08 – 87. 34 percent of σf, calculated by the fitting-damaging method, agreeing with former research. The results of stress inversion showed that under a low stress level (0. 15 σf < σ < 0. 5 σf), microdamage evolving into plastic failure later was formed around the cavity. Under a high stress level (0. 5 σf < σ < 0. 85 σf), the region of stress concentration suddenly crazed and formed a brittle crack. When σ ≥ 0. 85 σf, the crack was developing, crack lines were connecting with each other, and the coal finally failed. The outcome of the stress inversion was completely concomitant with the images of crack propagation. Additionally, the stress around the cavity was able to be calculated accurately...|$|E
40|$|Includes bibliographical {{references}} (page 67) In {{this thesis}} a method, using a {{branch and bound}} algorithm, is described for obtaining optimal phase settings for jammer cancellation with an electronically scanning array (ESA) whose only variable parameters are element phases which vary in discrete steps. For {{the purpose of this}} study it is assumed that the interference environment is known. Therefore, no estimation of environmental noise input will be required. The signal to interference plus noise ratio (SIR) for a linear array with an adaptive complex <b>element</b> <b>weight</b> vector was given in the literature. In particular the optimum weight vector that maximizes the SIR when the weights are unconstrained has been derived. This solution is not valid when the values allowed for the components of the weight vector are restricted to a discrete set. In particular, in an ESA the only values available are those with constant magnitude and discrete phases determined by the number of phase shifter bits. Methods of continuous analysis cannot produce a result for this discrete problem and an exhaustive search is impractical in most circumstances. In contrast to the exhaustive search method, the branch and bound technique needs to examine only a small subset of all possible discrete weight vectors in order to arrive at the best SIR. The method has been tested and the jammer cancellation results in the test cases have been encouraging. As a minimum, the approach described provides upper bounds for adaptation performance for electronically scanned arrays that are controlled with digital phase shifters...|$|E
40|$|We {{study the}} Online Budgeted Maximum Coverage (OBMC) problem. Subsets of a {{weighted}} ground set U arrive one by one, where each set has a cost. The online algorithm has {{to select a}} collection of sets, under the constraint that their cost is at most a given budget. Upon arrival of a set the algorithm must decide whether to accept or to reject the arriving set, and it may also drop previously accepted sets (preemption). Rejecting or dropping a set is irrevocable. The goal is to maximize the total weight of the elements covered by the sets in the chosen collection. We present a deterministic 4 /(1 -r) -competitive algorithm for OBMC, where r is the maximum ratio between {{the cost of a}} set and the total budget. Building on that algorithm, we then present a randomized O(1) -competitive algorithm for OBMC. On the other hand, we show that the competitive ratio of any deterministic online algorithm is Omega(1 /(sqrt{ 1 -r})). We also give a deterministic O(Delta) -competitive algorithm, where Delta is the maximum weight of a set (given that the minimum <b>element</b> <b>weight</b> is 1), and if the total weight of all elements, w(U), is known in advance, we show that a slight modification of that algorithm is O(min{Delta,sqrt{w(U) }}) -competitive. A matching lower bound of Omega(min{Delta,sqrt{w(U) }}) is also given. Previous to the present work, only the unit cost version of OBMC was studied under the online setting, giving a 4 -competitive algorithm [Saha, Getoor, 2009]. Finally, our results, including the lower bounds, apply to Removable Online Knapsack which is the preemptive version of the Online Knapsack problem...|$|E
40|$|Spearman’s footrule and Kendall’s tau are two well {{established}} distances between rankings. They, however, {{fail to take}} into account concepts crucial to evaluating a result set in information retrieval: element relevance and positional information. That is, changing the rank of a highly-relevant document should result in a higher penalty than changing the rank of an irrelevant document; a similar logic holds for the top versus the bottom of the result ordering. In this work, we extend both of these metrics to those with position and <b>element</b> <b>weights,</b> and show that a variant of the Diaconis–Graham inequality still holds — the generalized two measures remain within a constant factor of each other for all permutations. We continue by extending the <b>element</b> <b>weights</b> into a distance metric between elements. For example, in search evaluation, swapping the order of two nearly duplicate results should result in little penalty, even if these two are highly relevant and appear {{at the top of the}} list. We extend the distance measures to this more general case and show that they remain within a constant factor of each other. We conclude by conducting simple experiments on web search data with the proposed measures. Our experiments show that the weighted generalizations are more robust and consistent with each other than their unweighted counterparts...|$|R
50|$|The {{acceptance}} of the law allowed tables of <b>element</b> equivalent <b>weights</b> to be drawn up. These equivalent weights were widely used by chemists in the 19th century.|$|R
40|$|We here {{present some}} recent {{developments}} of MadGraph/MadEvent since the latest published version, 4. 0. These developments include: Jet matching with Pythia parton showers for both Standard Model and Beyond the Standard Model processes, decay chain functionality, decay width calculation and decay simulation, process generation for the Grid, a package for calculation of quarkonium amplitudes, calculation of Matrix <b>Element</b> <b>weights</b> for experimental events, automatic dipole subtraction for next-to-leading order calculations, and a package for automatic calculation of Feynman rules and model files from the Lagrangian of any New Physics model. Anglai...|$|R
40|$|This thesis {{presents}} a methodology to determine failure criteria of building insulation {{materials in the}} event of a fire that is specific to each typology of insulation material used. This methodology is based on material characterisation and assessment of fire performance of the most common insulation materials used in construction. Current methodologies give a single failure criterion independent of the nature of the material – this can lead to uneven requirements when addressing materials of different characteristics. At present, fire safety codes establish that performance of different materials or assemblies is assumed to be “equivalent” when subject to the same test, where attainment of the unique failure criteria occurs after a required minimum time. Nevertheless, when using extremely different materials this may not be actually the case. Building performance is currently defined in a quantitative way with respect to factors such as energy usage (i. e. global thermal transmittance), <b>element</b> <b>weight</b> (i. e. thickness and mass), space utilisation and cost of application. In the case of fire performance, only a threshold value is required, therefore a quantitative performance assessment is not conducted. As a result, the drivers are those associated with the variables that can be quantified, whereas the thresholds merely need to be met without any alternative for a better performance. This work opens the door to a performance-based-design methodology that takes into account fire performance as an optimisation variable for the building design, to be used with all other quantifiable variables. An added advantage is that the numerical tool required embraces a low level of complexity. As a result, the possibility for any insulation product to achieve quantifiable and acceptable fire safety levels for required energy efficiency targets is established. As a final remark, an application of the performance assessment methodology that introduces fire safety as a quantifiable variable is presented...|$|E
40|$|A common garden {{experiment}} with juvenile herring (Clupea harengus L.) {{was carried out}} to investigate the influence of salinity on different phenotypic traits commonly used to investigate population structure in this species. Local Atlantic spring spawning herring from western Norway was crossed with Baltic spring spawning herring to create hybrid offspring. Purebred Atlantic spring spawning offspring was also produced. The offspring of both crosses were randomly assigned and co- reared in tanks with two different salinities (16 and 35 psu). The fish were kept in the respective salinities from fertilization to final sampling 20 months later. Samples from both salinity groups were collected at day 187, 279 and 614 days post hatching (DPH). The mean (± SD) vertebrae count was higher in the purebred (56. 4 ± 0. 6) group than in the hybrid (55. 9 ± 0. 4) group, while it was not different between the two different salinities. Otolith shape developed over time from being more circular to being more oblong as the fish grew from 9 to 21 cm in total length. The otolith shape was more circular in the hybrid group, which indicate genetic effects on overall otolith shape. The ambient environment and genetic origin influenced the otolith chemistry. As an example, the otolith strontium levels (mg <b>element</b> <b>weight</b> / kg otolith weight) {{were higher in the}} 35 psu salinity, with a mean of 978 mg/kg vs 803 mg/kg in the 16 psu salinity. The purebreds within each salinity, also had a higher mean otolith strontium level of 997 mg/kg vs 876 mg/kg in the hybrids. The purebreds showed a lower survival than the hybrids, in the 16 psu salinity, before 187 DPH. The results of this study can be used to understand how the ambient salinity in the nursing areas affect the phenotype of herring and by this contributing {{to a better understanding of}} the complex population structure in herring stocks...|$|E
40|$|In {{coordination}} with a Technical Advisory Committee (TAC) consisting of County Engineers and Iowa DOT representatives, the Iowa DOT has proposed {{to develop a}} set of standards for a single span prefabricated bridge system for use on the local road system. The purpose of the bridge system is to improve bridge construction, accelerate project delivery, improve worker safety, be cost effective, reduce impacts to the travelling public by reducing traffic disruptions and the duration of detours, and allow local forces to construct the bridges. HDR Inc. was selected by the Iowa DOT to perform the initial concept screening of the bridge system. This Final Report summarizes the initial conceptual effort to investigate potential systems, make recommendations for a preferred system and propose initial details to be tested in the laboratory in Phase 2 of the project. The prefabricated bridge components were {{to be based on}} the following preliminary criteria set forth by the TAC. The criteria were to be verified and/ or modified as part of the conceptual development. - 24 ’ and 30 ’ roadway widths - Skews of 0 o, 15 o, and 30 o - Span lengths of 30 ’ – 70 ’ in 10 ’ increments using precast concrete beams - Voided box beams could be considered - Limit precast <b>element</b> <b>weight</b> to 45, 000 pounds for movement and placement of beams - Beams could be joined transversely with threaded rods - Abutment concepts may included precast as well as an option for cast-in-place abutments with pile foundations In addition to the above criteria, there was an interest to use a single-width prefabricated bridge component to simplify fabrication as well as a desire to utilize non-prestressed concrete systems where possible to allow for precasting of the beam modules by local forces or local precast plants. The SL- 1 modular steel bridge rail was identified for use with this single span prefabricated bridge system...|$|E
40|$|This paper {{presents}} a method {{developed by the}} author to assess student design projects. The method involves identifying assessment <b>elements,</b> <b>weighting</b> the <b>elements,</b> and grading the work as if had only one author. Data supplied by the students is used to identify the relative work put into each element. This allows individual grades to be calculated in a fair and consistent means. Sufficient background, context, and detail is given to allow anyone to use the reported method. In over eight years of use, no significant problems have been detected with this method. 1...|$|R
40|$|In addition, the sirloin which I threw overboard, {{instead of}} {{drifting}} {{off into the}} void, didn’t {{seem to want to}} leave the rocket and revolved about it, a second artificial satellite, which produced a brief eclipse of the sun every eleven minutes and four seconds. To calm my nerves I calculated till evening the components of its trajectory, as well as the orbital perturbation caused by the presence of the lost wrench. I figured out that for the next six million years the sirloin, rotating about the ship in circular path, would lead the wrench, then catch up with it from behind and pass it again. – The Star Diaries, Stanislaw Lem. In this chapter, we will introduce a powerful technique for “structure ” approximation. The basic idea is to perform a search by assigning <b>elements</b> <b>weights,</b> and picking the elements according to their <b>weights.</b> <b>Elements</b> <b>weight</b> indicates their importance. By repeatedly picking elements according to their weights, and updating the weights of objected that are being neglected (i. e., they are more important than the current weights indicate), we end up with a structure that has some desired properties. We will demonstrate this technique for two problems. In the first problem, we will compute a spanning tree of points that has low stabbing number. In the second problem, we will show how th...|$|R
40|$|We {{present the}} XML-based Partition Testing (XPT) ap-proach for the {{automatic}} generation of XML instances from a XML Schema. The approach {{is inspired by}} the well-known Category Partition method for black-box testing. The gen-erated instances {{can be used for}} inter-operability testing of applications that expect in input conforming XML in-stances, as well as for other interesting purposes, such as database population, XML Schema benchmarking, web ser-vices testing, and so on. The implementation of XPT in a prototype tool called TAXI is described. To limit the num-ber of generated instances, TAXI also incorporates practi-cal strategies for handling <b>element</b> <b>weights</b> and type values...|$|R
40|$|Nowadays {{the most}} {{important}} sectors in the strategy {{for the future of}} the port of Rotterdam are container handling, chemicals and distribution. To offer these sectors the opportunity to grow and renew, space is needed. By means of the construction of Maasvlakte II this space can be given. This land reclamation is planned to be located in the area between the Euro-Maasgeul in the north and the current Maasvlakte in the west and the extended demarcation line in the south. The planned extension will be done in two phases. The first one includes 700 ha with a length of breakwater of 2. 7 km, whose construction is planned to start in 2006 / 07 and the second phase, which includes 300 ha more, with a length of breakwater of 1. 3 km is planned to start in 2013 / 2023. In this study the cross-section of the breakwater, which protects the new area of the land reclamation, is analysed. Classical deterministic design could provide a preliminary geometry for the breakwater, but the dimensions of it are too big and also the costs. Therefore a probabilistic optimization could be made in order to check if a reduction or growth, in the geometry, can provide an economical optimum geometry with a substantial save. First a classical deterministic design is made. The most important elements of the crosssection are determined with the classical formulas and design guidelines. The following elements are analysed: - The armour layer - The toe - The secondary armour - The core - The filter system to establish the supporting bottom material - The crest height When the dimensions are given for all the elements, the geometry of the breakwater is established for the deterministic design. Afterwards the construction costs are determined for the breakwater solution. The deterministic design results in an <b>element</b> <b>weight</b> of 18. 8 tons (6. 6 m 3) and crest height of NAP+ 18 m. Economic consequences of the different failure mechanisms are not taken into account. The crest height is normally dependent on the construction method. In that case, the construction method does not produce a sensitive reduction in the breakwater geometry because the security level required in the determination of the crest height is too restrictive. After the deterministic design, the probabilistic optimization takes place [...] Hydraulic EngineeringCivil Engineering and Geoscience...|$|E
40|$|Along 3, 260 -km {{coastline}} of Vietnam, {{there is}} a system of about 2, 700 -km sea dike. This system has an important role {{for the development of}} Vietnam. It protects densely concentrated population and industrial areas. However, in the storm season, there is usually some collapse in the sea dike system due to the strong impacts of wind wave and high sea water level. One of the reasons, which cause the dikes collapse, is that the toe protection loses its stability. The present research deals with the study of toe protection for sea dikes and revetments in Vietnam. In the present research, two dimension physical model tests were carried out to study the toe protection for sea dikes and revetments in Vietnam using typical data of water depth, wave climate and slope of revetments, etc. Experiments were carried out in a regular wave flume of the Laboratory of Fluid Mechanics Delft University of Technology. Wave-induced kinetic energy, which causes the movement of stones at the toe, was determined from the obtained velocities at different locations in front of the slope. Moreover, the study proposed a new concept on relative turbulent energy intensity to compare fluctuation between points and an assumption on an equivalent wave based on wave-induced kinetic energy equivalence to determine the stone size for bottom protection. The results of the experiments proved that the seabed (sand) would be totally eroded and lowered by strong wave-induced kinetic energy at nodes positions, the net flows and movement of nodes due to water level changes during a storm. The seabed is still eroded even if it was lowered. As a result, the toe protection loses its stability and the slope slides afterwards. Moreover, the study concluded that the equivalent wave height is equal to Hmax = (1 +KR) H, where H is incoming wave height and KR is the reflection coefficient. Finally, an equation, which was also based on wave-induced kinetic energy equivalence, has been provided giving a good reliability to use for determining the stone size for bottom protection. For the safety of the sea dikes, the study recommended that, for the Vietnamese specification, there should be a requirement on slope (revetment) stability calculation in case {{there is a}} scour hole in front of the slope. A mattress made of available material like bamboo can be applied to keep the scour hole away from the toe. For preliminary design, the stones, which keep the mattress stable, can be determined by modified Shields curve with Hmax. Finally, the study recommended that the approach for determining the stone size based on the equivalent wave should be developed and applied in scale model research. A crest height of 7 m +SWL complies with an allowable downtime of approximately 0. 2 %. The downtime costs are of considerable more influence than estimated by the port authority. Also the consequences of a breakwater collapse result in a 65 % heavier <b>element</b> <b>weight.</b> The discounted total costs over the lifetime of the breakwater are 219 $ million for the probabilistic design and 468 $ million for the deterministic design. The collapse costs and downtime costs have a significant influence on the total costs over the lifetime and therefore on the economic optimal geometry of the breakwater. A more robust design than deterministically derived can reduce the total cost over the lifetime by almost 50 %. Civil Engineering and Geoscience...|$|E
40|$|This thesis {{presents}} a methodology to determine failure criteria of building insulation {{materials in the}} event of a fire that is specific to each typology of insulation material used. This methodology is based on material characterisation and assessment of fire performance of the most common insulation materials used in construction. Current methodologies give a single failure criterion independent of the nature of the material – this can lead to uneven requirements when addressing materials of different characteristics. At present, fire safety codes establish that performance of different materials or assemblies is assumed to be “equivalent” when subject to the same test, where attainment of the unique failure criteria occurs after a required minimum time. Nevertheless, when using extremely different materials this may not be actually the case. Building performance is currently defined in a quantitative way with respect to factors such as energy usage (i. e. global thermal transmittance), <b>element</b> <b>weight</b> (i. e. thickness and mass), space utilisation and cost of application. In the case of fire performance, only a threshold value is required, therefore a quantitative performance assessment is not conducted. As a result, the drivers are those associated with the variables that can be quantified, whereas the thresholds merely need to be met without any alternative for a better performance. Factors such as shortage of fuel for energy generation and determination in reducing carbon dioxide emissions to the atmosphere pose a scenario in which a decrease of energy consumption is required to allow sustainable development. Since a large proportion of the energy is used in buildings, plans for improving their energy performance have been strongly encouraged worldwide. During the last decade, energy efficiency has become the main driver for building design, leading to significant changes in construction typologies and materials. Therefore, achieving quantifiable improvements in insulation capacity has become a strong driver in building design. The main consequence of this required higher energy performance is the need for increased levels of insulation materials in buildings. Among the most common insulation materials, rigid polymeric (plastic) materials have demonstrated great potential for the built environment. These materials appear to optimise all current requirements in a scenario where cost, space usage and thermal conductivity are the only quantifiable and thus optimisable design variables. An analysis of the hazards introduced by higher levels of insulation materials in buildings is presented, accompanied by an assessment of the limitations of standard fire testing as a mechanism to quantify their fire performance. Common specific failure modes associated to fire and insulation materials in construction are highlighted, leading to a redefinition of the failure criteria. A methodology is then developed to quantitatively assess fire performance. The objective is therefore to structure a testing procedure that enables quantitative comparison between the fire performance of very different materials. An extensive testing plan serves as baseline to define this methodology and determine the failure criteria for the studied insulation materials, which correspond to rigid polyisocyanurate (PIR), rigid phenolic foam (PF), expanded polystyrene (EPS) and stone wool (SW). These materials are identified as the most common insulators used in construction over recent decades. The identification of a failure criterion requires a fundamental understanding of the thermal degradation processes associated to the different materials. This understanding can only be achieved by studying the relevant materials at the fundamental material scale and the results acquired by increasing length scales until all fire related parameters are incorporated. The testing plan is therefore defined in a way such that different scales are revised, with the objective of isolating different levels of complexity and failure modes. For that purpose, experiments are sequentially based on thermogravimetry, differential scanning calorimetry, bench-scale flammability, combustibility and one-dimensional thermal analysis, intermediate-scale thermal analysis and real-scale tests. The established methodology takes into account the phenomena that govern the behaviour of insulation products in fire, i. e. heat transfer by conduction linked to thermal degradation processes triggered by transfer and accumulation of energy. The current state-of-the-art allows for a fairly consistent analysis or definition of the thermal degradation processes that can include reactions in the solid phase as well as gas phase and heat and mass transfer processes. While this might be a desirable approach in some cases, this infers a high level of complexity and great uncertainty in the definition of input parameters, becoming a tedious task where compensation errors play an important role. Thus, the aim of this work is to establish the critical failure modes that determine the performance of the materials in their application. The methodology presented demonstrates that for the definition of the failure criteria of insulation materials, only a simple heat transfer model that assumes inert conditions in the solid phase is required. This enables a much simpler approach to be followed. This work opens the door to a performance-based-design methodology that takes into account fire performance as an optimisation variable for the building design, to be used with all other quantifiable variables. An added advantage is that the numerical tool required embraces a low level of complexity. As a result, the possibility for any insulation product to achieve quantifiable and acceptable fire safety levels for required energy efficiency targets is established. As a final remark, an application of the performance assessment methodology that introduces fire safety as a quantifiable variable is presented...|$|E
40|$|AbstractThe {{differential}} quadrature element method (DQEM) {{has been}} proposed. The <b>element</b> <b>weighting</b> coefficient matrices are {{generated by the}} differential quadrature (DQ) or generic differential quadrature (GDQ). By using the DQ or GDQ technique and the mapping procedure the governing differential or partial differential equations, the transition conditions of two adjacent elements and the boundary conditions can be discretized. A global algebraic equation system {{can be obtained by}} assembling all of the discretized equations. This method can convert a generic engineering or scientific problem having an arbitrary domain configuration into a computer algorithm. The DQEM irregular element torsion analysis model is developed...|$|R
40|$|For Laplace {{operator}} in one space dimension, we propose to formulate the heuristic finite volume method {{with the help}} of mixed Petrov-Galerkin finite <b>elements.</b> <b>Weighting</b> functions for gradient discretization are parameterized by some universal function. We propose for this function a compatibility interpolation condition and we prove that such a condition is equivalent to the inf-sup property when studying stability of the numerical scheme. In the case of stable scheme and under two distinct hypotheses concerning the regularity of the solution, we demonstrate convergence of the finite volume method in appropriate Hilbert spaces and with optimal order of accuracy. Comment: 27 page...|$|R
50|$|Supervised {{dictionary}} learning exploits {{both the}} structure underlying the input {{data and the}} labels for optimizing the dictionary elements. For example, a supervised dictionary learning technique applied dictionary learning on classification problems by jointly optimizing the dictionary <b>elements,</b> <b>weights</b> for representing data points, and parameters of the classifier based on the input data. In particular, a minimization problem is formulated, where the objective function consists of the classification error, the representation error, an L1 regularization on the representing weights for each data point (to enable sparse representation of data), and an L2 regularization on {{the parameters of the}} classifier.|$|R
