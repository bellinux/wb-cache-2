1|23|Public
40|$|The {{design of}} {{meaningful}} student activities, such as lab exercises and assignments, is a core element of computer graphics pedagogy. Here, we briefly describe our efforts towards making {{the process of}} defining and structuring computer graphics activities more explicit. We focus on four main activity categories that are building blocks for practical course design: Independent, Iterative, Incremental and Integrative. These "Four I's" of computer graphics activity provide the fundamental ingredients for explicitly defining the design of activity-oriented computer graphics courses {{with the potential to}} deliver significant artefacts that may, for example, constitute a portfolio of work for assessment or presentation to employers. The categorisations are intended as the first steps towards more clearly structuring and communicating <b>exercise</b> <b>specifications</b> in collaborative course development settings...|$|E
40|$|This thesis {{presents}} a methodology and {{a tool for}} automated assessment of programming exercises, {{with the purpose of}} reducing the workload of teachers. Our aim is for the tool to provide accurate and useful assessment given an <b>exercise</b> <b>specification.</b> Using the tool could allow teachers to spend more time helping students. The tool, implemented in Haskell, is intended to be used by teachers through a command line interface and targets a subset of Java. Assessment is achieved by using semantic and behavioural analysis. Semantic analysis consists of normalisation and prefix trees, while behavioural analysis consists of testing including integrated shrinking. The presented tool is evaluated using a data set from the course TDA 450 at Chalmers University of Technology. The tool managed to classify 60 % of the solutions as either correct or incorrect with no false positives. The result shows {{that it is possible to}} automatically assess student solutions and suggests that more solutions can be classified given further development...|$|R
40|$|A formal {{specification}} animator executes and interprets traces on a specification. Similar to software testing, an-imation can only show {{the presence of}} errors, never their absence. However, animation is a powerful means of find-ing errors, {{and it is important}} that we adequately <b>exercise</b> a <b>specification</b> when we animate it. This paper outlines a systematic approach to the anima-tion of {{formal specification}}s. We demonstrate the method on a small example, and then discuss its application to a non-trivial, system-level specification. Our aim is to pro-vide a method for planned, documented and maintainable animation of specifications, so that we can achieve a high level of coverage, evaluate the adequacy of the animation, and repeat the process at a later time. 1...|$|R
40|$|The {{expansion}} of medical technology in hospitals is commonly asserted to {{be a result}} of the preferences of medical doctors translated into organizational policies as a result of professional dominance in health care organizations. This paper examines the theoretical and empirical bases for hypotheses of professional dominance and the utility of these hypotheses in explaining hospital decisions to adopt new medical technologies. The analysis, which is based on 5 years of data collection including 378 personal interviews at 25 U. S. hospitals, indicates that appropriate application of the concept requires specification of the type of physician exercising influence and of the hospital decision systems within which it is <b>exercised.</b> <b>Specification</b> is needed because neither physicians nor hospitals are unitary categories when considered in relation to technology adoptions. In this paper, four categories of physicians are identified: community generalists, community specialists, referral specialists and hospital-based specialists. Members of these categories exhibit different skills and interests, different relationships to hospitalstechnologies, and differential access to the resources of organization influence including two unrelated to professional dominance. To understand the exercise of physician influence, it is further useful to differentiate three decision systems which review and pass judgement on different types of hospital technologies. They are: the medical-individualistic, the fiscal-managerial and the strategic-institutional. The three decision systems make decisions in accord with different values and goals and display different decision structures and dynamics. Ironically, the physicians who most clearly possess the resources of influence associated with professional dominance are centrally involved in only one of the three systems. They play only minor roles in the two which make the most far reaching and costly technological decisions. ...|$|R
40|$|Finite state transductions {{have been}} shown to be quite useful in a number of areas; however, it is still the case that it is often {{difficult}} to express certain kinds of transductions without resorting to a state and transition view. INR was developed to explore this problem, and several applications of transduction were studied as <b>exercises</b> in <b>specification</b> during INR 2 ̆ 019 s development. The specification of the NYSIIS phonetic encoding function (developed for the New York State Identification and Intelligence System) provides a clear example of many important ideas. An INR specification for NYSIIS is provided, which is syntactically simlar to the prose description and from which INR can directly produce the 149 state subsequential transducer. Peer reviewed: YesNRC publication: Ye...|$|R
40|$|Verification of the {{functional}} correctness of VHDL specifications {{is one of}} the primary and most time consuming task of design. However, it must necessarily be an incomplete task since it is impossible to completely <b>exercise</b> the <b>specification</b> by exhaustively applying all input patterns. The paper aims at presenting a two-step strategy based on symbolic analysis of the VHDL specification, using a behavioral fault model. First, we generate a reduced number of functional test vectors for each process of the specification which allow complete code statement coverage and bit coverage, allowing the identification of possible redundancies in the VHDL process. Then, through the definition of a controllability measure, we verify if these functional test vectors {{can be applied to the}} process inputs when it is interconnected to other processes. If this is not the case, the analysis of the non-applicable inputs provides identification of possible code redundancies and design errors. Experimental r [...] ...|$|R
40|$|Abstract—Paxos, Viewstamped Replication, and Zab are {{replication}} protocols for high-availability in asynchronous environments with crash failures. Claims {{have been}} made about their similarities and differences. But how does one determine whether two protocols are the same, and if not, how significant are the differences? We address these questions using refinement mappings. Protocols are expressed as succinct specifications that are progressively refined to executable implementations. Doing so enables a principled understanding of the correctness of design decisions for implementing the protocols. Additionally, differences that {{have a significant impact}} on performance are surfaced by this <b>exercise.</b> Index Terms—Systems <b>specification</b> methodology, distributed systems, reliability Ç...|$|R
40|$|The Regression Kink (RK) {{design is}} an {{increasingly}} popular empirical method, {{with more than}} 20 studies circulated using RK in the last 5 years since the initial circulation of Card, Lee, Pei and Weber (2012). We document empirically that these estimates, which typically use local linear regression, are highly sensitive to curvature in the underlying relationship between the outcome and the assignment variable. As an alternative inference procedure, motivated by randomization inference, we propose that researchers construct a distribution of placebo estimates in regions without a policy kink. We apply our procedure to three empirical RK applications - two administrative UI datasets with true policy kinks and the 1980 Census, which has no policy kinks - and we find that statistical significance based on conventional p-values may be spurious. In contrast, our permutation test reinforces the asymptotic inference results of a recent Regression Discontinuity study and a Difference-in-Difference study. Finally, we propose estimating RK models with a modified cubic splines framework and test the performance of different estimators in a simulation <b>exercise.</b> Cubic <b>specifications</b> - in particular recently proposed robust estimators (Calonico, Cattaneo and Titiunik 2014) - yield short interval lengths with good coverage rates...|$|R
40|$|Abstract—Service Level Agreements (SLAs) {{are used}} to specify the {{negotiated}} conditions between the provider and the consumer of services. In this paper we present a stepwise method to identify and categorize a set of test requirements that represent the potential situations that can be <b>exercised</b> regarding the <b>specification</b> of each isolated guarantee term of an SLA. This identification is addressed by means of devising a set of coverage levels that allow grading the thoroughness of the tests. The utilization of these test requirements would focus on twofold objectives: (1) the generation of a test suite that allows exercising the situations described in the test requirements and (2) the support for the derivation of a monitoring plan that checks the compliance of these requirements at runtime. The approach is illustrated over an eHealth case study...|$|R
40|$|Abstract — Traditional {{coverage}} metrics {{measure the}} ability of test cases to fully exercise the source code. The assumption is that fully covered code will better test the correct behavior of the software. Unfortunately, {{there is no guarantee}} that the software will meet its specification. In this paper, we present a new coverage metric which measures {{the ability of}} the test cases to fully <b>exercise</b> the <b>specification</b> of the software. Our approach allows the developer to iteratively develop and elaborate the test suite and the specification together. We infer a specification from the execution of the software for the given test cases. If the test cases are inadequate, the specification will be inaccurate, and if the code is incorrect, the inferred specification will be incorrect. The tester can then refine the test suite or repair the software to improve the resulting specification. To evaluate this approach we applied it to three Java classes and their associated test suites. For each of the three classes, our method revealed incorrect and missing invariants in our specification, which implied limitations in our test suite. We were subsequently able to improve the test suite (and specification) by adding appropriate test cases. After applying our method several times, all incorrect invariants were removed, and our final test suite was more complete than the original. I...|$|R
40|$|Traditional {{coverage}} metrics {{measure the}} ability of test cases to fully exercise the source code. The assumption is that fully covered code will better test the correct behavior of the software. Unfortunately, {{there is no guarantee}} that the software will meet its specification. In this paper, we present a new coverage metric which measures {{the ability of}} the test cases to fully <b>exercise</b> the <b>specification</b> of the software. However, a specification is not required. Our approach allows the developer to iteratively develop and elaborate the test suite and the specification together. Using a dynamic invariant detector, we infer a specification from the execution of the software for the given test cases. If the test cases are inadequate, the specification will be inaccurate, and if the code is incorrect, the inferred specification will be incorrect. The tester can then refine the test suite or repair the software to improve the resulting specification. To evaluate this approach we applied it to three Java classes and their associated test suites. For each of the three classes, our method revealed incorrect and missing invariants in our specification, which implied limitations in our test suite. We were subsequently able to improve the test suite (and specification) by adding appropriate test cases. After applying our method several times, all incorrect invariants were removed, and our final test suite was more complete than the original...|$|R
40|$|Teaching {{students}} {{to read and write}} specifications is difficult. It is even more diffcult to motivate specifications [...] to convince students of the value of specifications and make students eager to use them. This paper describes the Groupthink <b>specification</b> <b>exercise.</b> Groupthink is a fun group activity, in the style of a game show, that teaches students about specifications (the difficulty of writing them, techniques for getting them right, and criteria for evaluating them), teamwork, and communication. Specifications are not used as an end in themselves, but are motivated to students as a means to solving realistic problems that involve understanding system behavior. Students enjoy the activity, and it improves their ability to read and write specifications. The two-hour, low-prep activity is self-contained, scales from classes of ten to hundreds of students, and is freely available to other instructors...|$|R
40|$|Test the {{applicability}} of the ORE standard in a realistic scholarly setting-thesis description, submission and publication. ● Demonstrate the advantages of the ORE approach in complex object publication, by combining it with existing web-standards compliant technologies. ● Provide examples to fully <b>exercise</b> the ORE <b>specifications</b> in order to provide validation and future direction. TheOREM will contain two main strands. Firstly, we will create a small corpus of ideal born-digital theses based on real theses and describe these as completely as possible using ORE. Secondly, we will define a realistic scholarly scenario in which such theses might be handled, and implement demonstrators for each component system in the scenario in order to show the capabilities and limitations of ORE. Summary 1. This proposal is submitted by the University of Cambridge, seeking funds from the JISC for projec...|$|R
40|$|Formal {{methods can}} {{significantly}} {{assist in the}} design and modelling of safety-critical systems. However, formal methods are frequently criticised as being unusable through being too complex and requiring expert knowledge to use. We assert that to make formal methods usable they must be able to be presented in a manner which is readily interpretable. However, we must ensure that the inferences which may be drawn from such a presentation are correct with respect to the formal semantics. Concurrent systems in which communication occurs between asynchronously operating agents are widely used in safety-critical applications. Unfortunately designing and understanding such systems is made difficult by the interactions between the various concurrent agents. We present an <b>exercise</b> in the <b>specification</b> and modelling of a safety-critical multiprocessing system fragment. This serves to illustrate three issues which are crucial to the design and modelling of a safety-critical system. The [...] ...|$|R
40|$|This paper {{presents}} and discusses the LOTOS specification of a real-time parallel kernel. The {{purpose of this}} <b>specification</b> <b>exercise</b> has been to evaluate LOTOS with respect to its capabilities to model real-time features with a realistic industrial product. LOTOS was used to produce the formal specification of TRANS-RTXC, which is a real-time parallel kernel developed by Intelligent Systems international. This paper shows that although timing constraints cannot be explicitly represented in LOTOS, the language is suitable for the specification of co-ordination of real-time tasks, which is the main functionality of the real-time kernel. This paper also discusses the validation process of the kernel specification {{and the role of}} tools in this validation process. We believe that our experience (use of structuring techniques, use of validation methods and tools, etc) is valuable for designers who want to apply formal models in their design or analysis tasks...|$|R
40|$|The {{process and}} results of using formal methods to specify the Lights Out Ground Operations System (LOGOS) are {{presented}} in this paper. LOGOS is a prototype multi-agent system developed to demonstrate the feasibility of providing autonomy to satellite ground operations functions at NASA Goddard Space Flight Center (GSFC). Following the initial implementation of LOGOS, the development team decided to use formal methods to check for race conditions, deadlocks and omissions. The <b>specification</b> <b>exercise</b> revealed several omissions as well as race conditions. After completing the specification, the team concluded that certain tools {{would have made the}} specification process easier. This paper gives a sample specification of two of the agents in the LOGOS system and examples of omissions and race conditions found. 1. Introduction Until recently, space missions have been operated manually from ground control centers. The high costs of satellite operations has prompted NASA and other funding [...] ...|$|R
40|$|Abstract. Teaching {{students}} {{to read and write}} specifications is difficult. It is even more difficult to motivate specifications — to convince students of the value of specifications and make students eager to use them. The Groupthink <b>specification</b> <b>exercise</b> aims to fulfill all these goals. Groupthink is a fun group activity, in the style of a game show, that teaches students about teamwork, communication, and <b>specifications.</b> This <b>exercise</b> teaches students {{how difficult it is to}} write an effective specification (determining what needs to be specified, making the choices, and capturing those choices), techniques for getting them right, and criteria for evaluating them. It also gives students practice in doing so, in a fun environment that is conducive to learning. Specifications are used not as an end in themselves, but as a means to solving realistic problems that involve understanding system behavior. Students enjoy the activity, and it improves their ability to read and write specifications. The two-hour, low-prep activity is self-contained, scales from classes of ten to hundreds of students, and can be split into 2 one-hoursessionsorintegratedintoanexistingcurriculum. Itisfreely available from the autho...|$|R
40|$|The {{blackboard}} {{architecture is}} a complex, though powerful, model of problem-solving, and opinions vary {{as to its}} interpretation. The use of formal specifications for blackboard systems appears warranted by their complexity, their application in real-time and safety critical domains, {{and because of the}} informality of the construct itself. This paper describes the Z specification of a blackboard framework, the aims, and the methods by which it was executed. At present, the specification is only a top-level one (and occupies over 100 A 4 pages, including proofs and explanatory text) : this has allowed concentration on the interpretation of the architecture, and has allowed the formal proof of a number of properties which have, hitherto, had 'folklore' status. The <b>specification</b> <b>exercise</b> revealed a number of areas in which further work was required. The blackboard specification is {{one of a number of}} Z specifications of AI architectures that we have undertaken: the problem areas first identified in the blackboard specification have reappeared in the others, and we suggest ways of solving the problems which are, perhaps, of general utility...|$|R
40|$|A {{study has}} {{reviewed}} {{the variety of}} mine inertisation systems available in Australia and their technical <b>specifications.</b> <b>Exercises</b> which involved “evaluation or auditing ” of selected mines as {{to the ability to}} deliver inert gases generated from inertisation units to high priority underground fire locations have been undertaken in a number of mines. These exercises have been built around the use of the fire simulation computer program VENTGRAPH and modelling of fire scenarios in selected different mine layouts. A coding system has been developed from these audit exercises. Designs have been developed to allow delivery of high volumes of inert gases down mine bore holes. A section of the paper has examined considerations presented by the layouts of underground mines developed from surface extraction pits. Inertisation and dilution issues in mine openings create complex situations. Mains headings present a complex ventilation network with often numerous parallel headings, hundreds of cut-throughs and a variety of ventilation control devices. In these complex systems the additional interference from a fire means maintaining control of the movement of inert gas is more difficult than elsewhere in the mine. Some illustrations of these issues are given. Mine fires and heatings are recognised across the world as a major hazard issue. New approaches allowing improvement in understanding their use of inertisation techniques have been examined. The outcome of the project is that the mining industry is in an improved position in their understanding of mine fires, use of inertisation and the use of modern advances to preplan for the handling of possible emergency incidents...|$|R
40|$|Under the (weak) {{assumption}} of a Markovian underlying price process, an alternative and intuitive {{characterization of the}} early exercise premium is proposed. This new representation involves the first passage time density of the underlying spot price to the exercise boundary and is simply based on the observation that the discounted early exercise premium must be a martingale under the “risk-neutral ” measure. The Markov property ensures analytical tractability since it enables the decomposition of the joint density between the first hitting time and the underlying asset price through the convolution of their marginal densities. The analytical pricing solution proposed for American options is automatically consistent with the “value-matching ” condition, is valid for any parameterization of the exercise boundary, and is shown to possess appropriate asymptotic properties. More important, such new valuation frame-work can be easily transposed from the standard geometric Brownian motion assumption to more general Markovian asset price processes, which can accommodate stochastic volatility and/or sto-chastic interest rates. The optimal stopping time density is shown to satisfy a non-linear but one-dimensional integral equation. Using the algorithm suggested by Park and Schuurmann (1976), the first hitting time density of a geometric Brownian motion is obtained for any (time-dependent) specification of the early exercise boundary and tight lower bounds follow {{for the price of}} an American option. Several <b>exercise</b> boundary parametric <b>specifications</b> are tested and it is shown that, with only one para-meter and at a higher computational speed, it is possible to achieve an accuracy comparable to a 15, 000 -step binomial tree. The extension to alternative Markovian diffusion processes is left for future research...|$|R
40|$|Automatic Dependent Surveillance-Broadcast (ADS-B) is a {{surveillance}} system placed in aircraft that periodically transmits state vector estimates {{and other information}} to air traffic control centers and other nearby aircraft (and may also receive traffic and weather information from various entities). The state vector estimates are derived from navigation avionics and are transmitted via a common communications channel, which means that ADS-B is highly dependent on aircraft navigation and communication systems. ADS-B also requires ground stations to receive information from aircraft. As {{a result of this}} complex architecture, the ADS-B system is prone to various failure modes. A systematic and comprehensive performance monitoring system is required to ensure the safe use of ADS-B data for air traffic control operations. It is vital that such monitoring systems are in place before a global ADS-B implementation date is mandated by the International Civil Aviation Organization. A number of air navigation service providers and regulators have developed ADS-B performance monitoring methods without a standardized guideline for system specifications. These include Airservices Australia, the U. S. Federal Aviation Administration, EUROCONTROL, the Civil Aviation Authority of Singapore and the Civil Aviation Department of Hong Kong. This paper presents a holistic set of system specifications for ADS-B monitoring systems. In particular, the paper analyzes the ADS-B infrastructure, conducts a systematic review of existing ADS-B monitoring systems, classifies the system characteristics to identify gaps, and derives a set of specifications for developing ADS-B monitoring systems. The paper also assesses the compliance of existing ADS-B monitoring systems against the proposed specifications using a mapping <b>exercise.</b> The system <b>specifications</b> serve as a foundation or minimum requirements for air navigation service providers and original equipment manufacturers to develop systematic and comprehensive ADS-B monitoring systems...|$|R
40|$|Abstract. Teaching {{students}} {{to read and write}} specifications is difficult. It is even more difficult to motivate specifications [...] to convince students of the value of specifications and make students eager to use them. The Groupthink <b>specification</b> <b>exercise</b> aims to fulfill all these goals. Groupthink is a fun group activity, in the style of a game show, that teaches students about teamwork, communication, and <b>specifications.</b> This <b>exercise</b> teaches students {{how difficult it is to}} write an effective specification (determining what needs to be specified, making the choices, and capturing those choices), techniques for getting them right, and criteria for evaluating them. It also gives students practice in doing so, in a fun environment that is conducive to learning. Specifications are used not as an end in themselves, but as a means to solving realistic problems that involve understanding system behavior. Students enjoy the activity, and it improves their ability to read and write specifications. The two-hour, low-prep activity is self-contained, scales from classes of ten to hundreds of students, and can be split into 2 one-hour sessions or integrated into an existing curriculum. It is freely available from the author (mernst@csail. mit. edu), complete with lecture slides, handouts, a scoring spreadsheet, and optional software. Instructors outside MIT have successfully used the materials. 1 Introduction Specification (along with related verification activities such as testing) is crit-ical to the success of any real software system. However, many students view specification as a dry, tedious, and impractical topic. One problem is that many undergraduate programming classes fail to integrate specification into the curriculum in a realistic way. Writing a specificationfor the purpose of being graded takes specifications out of the context in which they are used, so students gain little appreciation for their utility. Addition-ally, typical class assignments are simple, so techniques that are crucial for more complex software may not be cost-effective. Students learn the (incorrect!) lessonthat specification is pointless...|$|R
40|$|Title of Document: DESIGN AND ANALYSIS OF AN AUTOMATED ASSEMBLY PROCESS FOR MANUFACTURING PAINT BRUSH KNOTS Aleksandr Borisovich Gorbashev, M. S. 2011 Mechanical Engineering Directed By: Chandrasekhar Thamire 			 Department of Mechanical Engineering Manufacturing {{process for}} paint brushes {{requires}} handling and assembling of flexible and delicate filaments {{and can be}} cumbersome in manual assembly processes. Common issues resulting from such manual assembly process include variations in filament density, deviations in filament straightness, and issues due to right and left handed bias in assembly operations, resulting in poor quality of the end products. Coupled with operator fatigue and health problems, these issues provide an excellent motivation for refining the process. The primary objectives {{of this study were}} to develop an assembly system that will 1) increase product quality, and 2) improve the production rate. The secondary objective was to develop a set of design guidelines for handling flexible elements such as synthetic filaments within provided housings. 	 In order to develop the automated assembly process, needs analysis and product design <b>specification</b> <b>exercises</b> were performed first, followed by functional decomposition of the process at the first level. Designs for individual subsystems were developed next using functional decomposition at lower levels, concept generation, concept evaluation, feasibility testing, testing for design parameters, design through solid modeling, strength analysis, concept testing using physical prototypes and subsystem refinement. In order to assess the response of filament assemblies when subjected to external loading and moving relative to the housings, experiments were designed and conducted. For a range of factors, tests were conducted to establish limits of pulling force required to displace filament bundles within the housings. Correlations relating filament motion to applied loading were developed for a variety of housing geometries and material types. Design guideline related to motion of filaments within housings was developed. In light of the testing performed, design guidelines for development of gripper-plates used for gripping of bulk filament bundles were also established. It is expected that these guidelines will be useful in the manufacturing automation industry, involving manufacture of toothbrushes, hair brushes and fiber-optic elements. Upon successful completion of the feasibility tests, full-scale prototypes using the final concepts of subsystems were fabricated. Tests were conducted to determine the reliability of the process and quality of the brush knots. Results indicate that the quality of the brushes was much higher than the traditional hand-made brushes and that the productivity would nearly double. Upon delivery of the system to the company sponsoring this research, it is expected that the system developed would be able to produce up to 3 million brushes per year...|$|R
40|$|In Chapter 1, I {{study the}} {{efficiency}} properties of competitive search equilibria in economies with informational asymmetries. Employers and workers are both risk-neutral and ex-ante homogeneous. I characterize an equilibrium where employers post contracts and workers direct their search towards them. When a match is formed, the disutility of labor is drawn randomly and observed privately by the worker. An employment contract is an incentive-compatible mechanism that satisfies a participation constraint on the worker's side. I first {{show that in}} a static setting the competitive search equilibrium is constrained efficient, that is, it cannot be Pareto improved by a Social Planner {{subject to the same}} informational and participation constraints faced by the decentralized economy. I then show that in a dynamic setting, on the contrary, the equilibrium can be constrained inefficient. The crucial difference between the static and the dynamic environment is that the worker's outside option is exogenously given in the former, while in the latter it is endogenously determined as the equilibrium continuation utility of unemployed workers. Inefficiency arises because the worker's outside option affects the ex-ante cost of information revelation, generating a novel externality which is not internalized by competitive search. (cont.) In Chapter 2, I explore whether match-specific heterogeneity, with or without full information, can amplify the responsiveness of unemployment rate and market tightness to productivity shocks. On the contrary, I show that heterogeneity can dampen the response of market tightness to productivity, once one calibrates the model to match two main facts: the finding rate and the finding rate elasticity to market tightness. First, I show a theoretical result for the steady state analysis in the extreme case of no aggegate shock. Then, I report the calibration <b>exercise</b> for alternative <b>specification</b> of the idiosyncratic shocks distribution. Chapter 3 is the product of joint work with Daron Acemoglu and constructs a model of non-balanced economic growth. The main economic force is the combination of differences in factor proportions and capital deepening. Capital deepening tends to increase the relative output of the sector with a greater capital share (despite the equilibrium reallocation of capital and labor away from that sector). We first illustrate this force using a general two-sector model. We then investigate it further using a class of models with constant elasticity of substitution between two sectors and Cobb-Douglas production functions in each sector. (cont.) In this class of models, non-balanced growth is shown to be consistent with an asymptotic equilibrium with constant interest rate and capital share in national income. We investigate whether for realistic parameter values, the model generates transitional dynamics that are consistent with both the more rapid growth of some sectors in the economy and aggregate balanced growth facts. Finally, we construct and analyze a model of "non-balanced endogenous growth," which extends the main results of the paper to an economy with endogenous and directed technical change. This model shows that non-balanced technological progress will generally be an equilibrium phenomenon. by Veronica Guerrieri. Thesis (Ph. D.) [...] Massachusetts Institute of Technology, Dept. of Economics, 2006. Includes bibliographical references...|$|R

