0|10000|Public
40|$|This paper {{presents}} a new coarse-grained thread pipelining execution model for exploiting coarse-grained parallelism from <b>general-purpose</b> <b>application</b> <b>programs</b> in shared-memory multiprocessor systems. Based on the fine-grained thread pipelining model {{proposed for the}} superthreaded architecture [7], this new model allows concurrent execution of loop iterations with run-time data dependence checking and control speculation. These features allow the parallelization {{of a variety of}} program constructs that cannot be parallelized with existing run-time schemes. The pipelined execution of loop iterations results in lower parallelization overhead than in other existing techniques. The performance of this coarse-grained thread pipelining model was evaluated using some real applications and a synthetic benchmark. With a sufficiently large grain size compared to the parallelization overhead, significant speedups are possible. The synthetic benchmark provides a means for estimating the performance [...] ...|$|R
40|$|This paper {{presents}} a new parallelization model, called coarse-grained thread pipelining, for exploiting speculative coarse-grained parallelism from <b>general-purpose</b> <b>application</b> <b>programs</b> in shared-memory multiprocessor systems. This parallelization model, {{which is based}} on the fine-grained thread pipelining model proposed for the superthreaded architecture [11, 12], allows concurrent execution of loop iterations in a pipelined fashion with run-time data-dependence checking and control speculation. The speculative execution combined with the run-time dependence checking allows the parallelization of a variety of program constructs that cannot be parallelized with existing run-time parallelization algorithms. The pipelined execution of loop iterations in this new technique results in lower parallelization overhead than in other existing techniques. We evaluated the performance of this new model using some real applications and a synthetic benchmark. These experiments show that pr [...] ...|$|R
40|$|The common {{single-threaded}} execution model limits processors to exploiting {{only the}} relatively small amount of instruction-level parallelism available in <b>application</b> <b>programs.</b> The superthreaded processor, on the other hand, is a concurrent multithreaded architecture (CMA) that can exploit the multiple granularities of parallelism available in <b>general-purpose</b> <b>application</b> <b>programs.</b> Unlike other CMAs that rely primarily on hardware for run-time dependence detection and speculation, the superthreaded processor combines compiler-directed thread-level speculation of control and data dependences with run-time data dependence verification hardware. This hybrid of a superscalar processor and a multiprocessor-ona-chip can utilize many of the existing compiler techniques used in traditional parallelizing compilers developed for multiprocessors. Additional unique compiler techniques, such as the conversion of data speculation into control speculation, are also introduced to generate the superthreaded code and to enhance the parallelism between threads. A detailed execution-driven simulator is {{used to evaluate the}} performance potential of this new architecture. It is found that a superthreaded processor can achieve good performance on complex <b>application</b> <b>programs</b> through this close coupling of compile-time and run-time information...|$|R
5000|$|AutoBench 1.1 - {{single-threaded}} {{code for}} automotive, industrial, and <b>general-purpose</b> <b>applications</b> ...|$|R
40|$|The Spallation Neutron Source (SNS) {{is using}} a Java based {{framework}} for <b>application</b> <b>program</b> development. The framework, called XAL, {{is designed to provide}} an accelerator physics programming interface to the accelerator. Much of the underlying connections to the EPICS control system are hidden from the user. Use of this framework allows writing <b>general-purpose</b> <b>applications</b> that can be applied to various parts of the accelerator. Also the accelerator structure is initiated from a database, so introduction of new beamline devices or signal modifications are immediately available for all XAL applications. An on-line model is included in this framework for quick beam tracking. The overall framework is described, and example applications are shown. ...|$|R
5000|$|ASTM B 491: Standard Specification for Aluminum and Aluminum-Alloy Extruded Round Tubes for <b>General-Purpose</b> <b>Applications</b> ...|$|R
5000|$|The {{choice of}} whether to {{purchase}} an industry-specific <b>application</b> or a <b>general-purpose</b> <b>application</b> is often very difficult. Concerns over a custom-built application or one designed for a specific industry include: ...|$|R
5|$|During development, Intel, HP, and {{industry}} analysts predicted that IA-64 would dominate in servers, workstations, and high-end desktops, and eventually supplant RISC and complex instruction set computing (CISC) architectures for all <b>general-purpose</b> <b>applications.</b>|$|R
2500|$|Commands Unix {{makes little}} {{distinction}} between commands (user-level programs) for system {{operation and maintenance}} (e.g. cron), commands of general utility (e.g. grep), and more <b>general-purpose</b> <b>applications</b> such as the text formatting and typesetting package. Nonetheless, some major categories are: ...|$|R
40|$|GPGPU: Modern {{graphics}} processing {{units used}} as general purpose architectures. • NVIDIA CUDA architecture and model [1]. Objective: Simplify the encoding of parallel, <b>general-purpose</b> <b>applications,</b> on heterogeneous systems with GPUs devices. • Fermi: The NVIDIA’s latest generation of CUDA architecture [2]. Presents an improved double precisio...|$|R
40|$|Video content becomes {{increasingly}} important in WWW applications as the emerging global cyber infrastructure develops. Improvements in hardware (e. g., fast CPUs, graphics chips, inexpensive mass storage, inexpensive video cameras, cell phones), software (e. g., video editors, video player extensions to web browsers and other <b>general-purpose</b> <b>applications,</b> web development environments), and networkin...|$|R
40|$|Abstract: In {{order to}} {{investigate}} the impact of dynamic hardware reconfiguration on <b>general-purpose</b> <b>applications,</b> we present a superscalar micro-architecture that includes a variable number of execution units. The set of execution units available is updated dynamically according to the run-time behaviour of the application. Performance is evaluated using software simulation of the proposed micro-architecture while running real-world applications of the SPEC 2000 benchmark suite. ...|$|R
40|$|The use of {{reconfiguration}} {{of field}} programmable gate arrays (FPGAs) {{to improve the}} area efficiency of a class of FPGA circuits is reported. The applicability of the technique to a large class of <b>general-purpose</b> <b>applications</b> is established by identifying the key characteristics of suitable circuits. The {{development of a new}} design methodology that allows the technique to be reliably deployed with repeatable results is described...|$|R
40|$|Traditionally, {{the terms}} “low power ” and “programmable logic ” {{have not been}} used in the same context. However, the advent of the zero-power CPLD has {{transformed}} the discussion, as this technology brings the many advantages of programmable logic to designers of low-power electronic products. Now, in addition to the CPLD’s demonstrated ability to excel in <b>general-purpose</b> <b>applications,</b> zero-power CPLDs can reduce overall power consumption i...|$|R
40|$|Initially {{introduced}} as special-purpose accelerators for games and graphics code, graphics processing units (GPUs) {{have emerged as}} widely-used high-performance parallel computing platforms. GPUs traditionally provided only softwaremanaged local memories (or scratchpads) instead of demandfetched caches. Increasingly, however, GPUs are being used in broader application domains where memory access patterns are both harder to analyze and harder to manage in software-controlled caches. In response, GPU vendors have included sizable demand-fetched caches in recent chip designs. Nonetheless, several problems remain. First, since these hardware caches are quite new and highly-configurable, {{it can be difficult}} to know when and how to use them; they sometimes degrade performance instead of improving it. Second, since GPU programming is quite distinct from <b>general-purpose</b> <b>programming,</b> <b>application</b> programmers do not yet have solid intuition about which memory reference patterns are amenable to demand-fetched caches. In response, this paper characterizes application performance on GPUs with caches and provides a taxonomy for reasoning about different types of access patterns and locality. Based on this taxonomy, we present an algorithm which can be automated and applied at compile-time to identify an application’s memory access patterns and to use that information to intelligently configure cache usage to improve application performance. Experiments on real GPU systems show that our algorithm reliably predicts when GPU caches will help or hurt performance. Compared to always passively turning caches on, our method can increase the average benefit of caches from 5. 8 % to 18. 0 % for applications that have significant performance sensitivity to caching...|$|R
40|$|Much {{work has}} been done to reduce the energy {{consumption}} of the processor or memory using adaptation algorithms for general-purpose systems. This paper develops new adaptation algo-rithms that combine the benefits of multiple time scales of adaptation and joint processor-memory adaptation to save more energy than previous algorithms for <b>general-purpose</b> <b>applications.</b> Specifi-cally, our final algorithm for joint adaptation of processor and memory has the following attributes that have not previously been available for general-purpose adaptations. First, the algorithm can trade off a specified amount of performance for energy savings. In contrast, previous work on processor adaptation has focused on saving energy without “much ” performance loss – our work not only allows more energy savings but also provides a performance guarantee. Second, previous processor adaptation algorithms for <b>general-purpose</b> <b>applications</b> adapt at either a fine or coarse time scale. The new algorithm allows adaptation at both time scales, exploiting both short term and long term variability. Third, previous work has considered processor and memory adaptation separately. Our algorithm is the first to jointly adapt both processor and memory, and shows that such joint adaptation can provide significant energy savings over adapting either component alone...|$|R
40|$|Researchers have {{proposed}} {{the use of}} adaptation to reduce the energy consumption of different hardware components, such as the processor, memory, disk, and display for <b>general-purpose</b> <b>applications.</b> Previous algorithms to control these adaptations, however, have focused on a single component. This work takes {{the first step toward}} developing algorithms that can jointly control adaptations in multiple interacting components for <b>general-purpose</b> <b>applications,</b> with the goal of minimizing the total energy consumed within a specified performance loss. Specifically, we develop a joint-adaptation algorithm for processor and memory adaptations. We identify two properties that enable per-component algorithms to be easily used in a cross-component context—the algorithms’ performance impact must be guaranteed and composable. We then modify a current processor and a memory algorithm to obey these properties. This allows the cross-component problem to be reduced to determine an appropriate (energy-optimal) allocation of the target performance loss (slack) between the two components. We develop such an optimal slack allocation algorithm that exploits the above properties. The result is an efficient cross-component adaptation framework that minimizes the total energy of the processor and memory without exceeding the target performance loss, while substantially leveraging current per-component algorithms. Our experiments show tha...|$|R
50|$|IDAPI {{stands for}} Integrated Database <b>Application</b> <b>Program</b> Interface or Independent Database <b>Application</b> <b>Program</b> Interface. It was {{originally}} {{a component of}} the Paradox relational database management system. It is now the <b>application</b> <b>program</b> interface of the BDE or Borland Database Engine.|$|R
50|$|The Netscape Server <b>Application</b> <b>Programming</b> Interface (NSAPI) is an <b>application</b> <b>programming</b> {{interface}} for extending server software, typically {{web server}} software.|$|R
50|$|The Generic Security Service <b>Application</b> <b>Program</b> Interface (GSSAPI, also GSS-API) is an <b>application</b> <b>programming</b> {{interface}} {{for programs}} to access security services.|$|R
5000|$|The same <b>application</b> <b>programming</b> {{interface}} (API), so <b>application</b> <b>programs</b> {{could be}} transferred among PCP, MFT and MVT without even needing re-compilation.|$|R
40|$|The {{advances}} in the mobile phone technology have enabled such devices to be <b>programmed</b> to run <b>general-purpose</b> <b>applications</b> using a special mobile edition of the Java programming language. Java {{is designed to be}} a heterogeneous programming language targeting different platforms. Such ability when coupled with the provision of high-speed mobile Internet access would open the door for a new breed of distributed mobile applications. This paper explores the limitations of this technology and addresses the consideration that must be taken when designing and developing such applications...|$|R
50|$|The {{control block}} acted as the <b>Application</b> <b>programming</b> {{interface}} between Logical IOCS and the <b>application</b> <b>program</b> and usually was defined within (and resided within) the <b>application</b> <b>program</b> itself. The addresses of I/O subroutines would be resolved during a linkedit phase after compilation or else dynamically inserted at OPEN time.|$|R
2500|$|The {{operating}} system provides an interface between an <b>application</b> <b>program</b> {{and the computer}} hardware, so that an <b>application</b> <b>program</b> can interact with the hardware only by obeying rules and procedures programmed into the {{operating system}}. [...] The operating system is also a set of services which simplify development and execution of <b>application</b> <b>programs.</b> Executing an <b>application</b> <b>program</b> involves {{the creation of a}} process by the operating system kernel which assigns memory space and other resources, establishes a priority for the process in multi-tasking systems, loads program binary code into memory, and initiates execution of the <b>application</b> <b>program</b> which then interacts with the user and with hardware devices.|$|R
5000|$|In computing, Server <b>Application</b> <b>Programming</b> Interface (SAPI) is {{the direct}} module {{interface}} to web servers {{such as the}} Apache HTTP Server, Microsoft IIS, and Oracle iPlanet Web Server. Microsoft uses the term Internet Server <b>Application</b> <b>Programming</b> Interface (ISAPI), and the defunct Netscape web server used the term Netscape Server <b>Application</b> <b>Programming</b> Interface (NSAPI) for the same purpose. [...] In other words, SAPI is an <b>application</b> <b>programming</b> interface (API) provided by the web server to help other developers in extending the web server capabilities.|$|R
40|$|DE 102009037436 A 1 UPAB: 20110307 NOVELTY - The method {{involves}} initializing an <b>application</b> <b>program</b> on a device, {{where the}} <b>application</b> <b>program</b> {{is provided with}} authentication information. The <b>application</b> <b>program</b> is authenticated (120) against an input device by authentication information. The input device is authenticated (130) opposite to a computer. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for accessing a device on a computer of a computer network. USE - Method for accessing a device on a computer of a computer network. ADVANTAGE - The method involves initializing an <b>application</b> <b>program</b> on a device, where the <b>application</b> <b>program</b> is provided with authentication information, and hence ensures simple device accessing method...|$|R
2500|$|Historically, CPUs {{have used}} hardware-managed caches but the earlier GPUs only {{provided}} software-managed local memories. However, as GPUs are being increasingly used for <b>general-purpose</b> <b>applications,</b> state-of-the-art GPUs are being designed with [...] hardware-managed multi-level caches [...] which {{have helped the}} GPUs to move towards mainstream computing. For example, GeForce 200 series GT200 architecture GPUs did not feature an L2 cache, the Fermi GPU has 768KB last-level cache, the Kepler GPU has 1536KB last-level cache, the Maxwell GPU has 2048KB last-level cache and the Pascal GPU has 4096 KB last-level cache.|$|R
40|$|We {{consider}} {{speeding up}} <b>general-purpose</b> <b>applications</b> with hardware accelerators. Traditionally hardware accelerators are tediously hand-crafted to achieve top performance. ASC (A Stream Compiler) simplifies exploration of hardware accelerators by transforming the hardware design task into a software design process using only 'gcc' and 'make' {{to obtain a}} hardware netlist. ASC enables programmers to customize hardware accelerators at three levels of abstraction: the architecture level, the functional block level, and the bit level. All three customizations are based on one uniform representation: a single C++ program with custom types and operators for each level of abstraction...|$|R
40|$|Program {{instructions}} that consume and produce small operands can be executed in hardware circuitry {{of less than}} full size. We compare different proposed models of accounting for the usefulness of bit-positions in operands, using a run-time profiling tool, both to observe and summarize operand values, and to reconstruct and analyze the program's data-flow graph to discover useless bits. We find that under aggressive models, {{the average number of}} useful bits per integer operand is as low as 10, not only in kernels but also in <b>general-purpose</b> <b>applications</b> from SPEC 95. ...|$|R
5000|$|Historically, CPUs {{have used}} hardware-managed caches but the earlier GPUs only {{provided}} software-managed local memories. However, as GPUs are being increasingly used for <b>general-purpose</b> <b>applications,</b> state-of-the-art GPUs are being designed with hardware-managed multi-level caches [...] which {{have helped the}} GPUs to move towards mainstream computing. For example, GeForce 200 series GT200 architecture GPUs did not feature an L2 cache, the Fermi GPU has 768 KB last-level cache, the Kepler GPU has 1536 KB last-level cache, the Maxwell GPU has 2048 KB last-level cache and the Pascal GPU has 4096 KB last-level cache.|$|R
40|$|An often {{overlooked}} area of graphics {{is the ability}} of <b>application</b> <b>programs</b> to create graphical images. Many programs exist which allow creation interactively, but few offer the same ability for noninteractive <b>application</b> <b>programs.</b> By allowing an <b>application</b> <b>program</b> to create graphical images more user friendly programs may be created by programmers. Department of Computer ScienceThesis (M. S. ...|$|R
40|$|Application Portable Parallel Library (APPL) {{computer}} program is subroutine-based message-passing software library {{intended to provide}} consistent interface to variety of multiprocessor computers on market today. Minimizes effort needed to move <b>application</b> <b>program</b> from one computer to another. User develops <b>application</b> <b>program</b> once and then easily moves <b>application</b> <b>program</b> from parallel computer on which created to another parallel computer. ("Parallel computer" also include heterogeneous collection of networked computers). Written in C language with one FORTRAN 77 subroutine for UNIX-based computers and callable from <b>application</b> <b>programs</b> written in C language or FORTRAN 77...|$|R
40|$|This paper {{presents}} a graphical programming tool, S-PNGEN that utilizes an extended form of S-System Petri net (SSPN) to express in graphical form <b>application</b> <b>programs</b> written for microcontrollers. The tool allows SSPN-like graphical <b>application</b> <b>program</b> to be constructed easily {{with a built-in}} diagram editor. The SSPN <b>application</b> <b>program</b> constructed is internally represented by directed graphs, which is then parsed using context-free diagram and string grammar to check for correct diagram and sentence syntax. Upon successful parsing, the tool automatically translates the SSPN-represented <b>application</b> <b>program</b> into assembly code for a target microcontroller...|$|R
40|$|To {{efficiently}} examine volumetric {{data sets}} from CT or MRI scans good volume rendering applications are needed. This thesis describes {{the design and}} implementation of an <b>application</b> <b>programming</b> interface (API) to be used when developing volume-rendering applications. A complete <b>application</b> <b>programming</b> interface has been designed. The interface is designed so that it makes writing <b>application</b> <b>programs</b> containing volume rendering fast and easy. The interface also makes created <b>application</b> <b>programs</b> hardware independent. Volume rendering using 3 d-textures is implemented on Windows and Unix platforms. Rendering performance has been compared between different graphics hardware...|$|R
40|$|The {{capabilities}} of generic <b>application</b> <b>programs</b> (wordprocessors, spreadsheets, databases etc.) are not widely known in schools. In this paper, {{the results of}} an empirical study in which the use of <b>application</b> <b>programs</b> by a sample of class-teachers in an ordinary municipal primary school in Finland and by a respective sample of the classteachers in a teacher training school are reported. In addition, the development of the level of mastery of <b>application</b> <b>programs</b> plus the deve-lopment {{of the impact of the}} <b>application</b> <b>programs</b> in teaching in a school with a strong IT-orientation are reported...|$|R
40|$|As {{computing}} power of GPU increases dramatically, the GPU {{is widely used}} for <b>general-purpose</b> parallel <b>applications</b> as well as graphics applications. Especially, programmers using the GPU can easily create multiple threads {{with the help of}} APIs provided by GPU vendors. In GPU architecture, threads are grouped into a warp to run on the SIMD pipeline, leading to high performance. However, computational resources of GPU are not fully utilized in executing <b>general-purpose</b> <b>applications</b> due to control-flow instructions, resulting in performance degradation. To improve the GPU performance, several warp formations for handling branch divergence due to control-flow instructions have been proposed. In this work, we analyze the GPU performance according to warp formations with real GPU hardware configuration. Our simulation results show that the warp formation providing high hardware utilization does not guarantee high performance if hardware resources are not fully supported. Therefore, hardware configuration should be considered together with hardware utilization to improve the GPU performance by using warp formation...|$|R
