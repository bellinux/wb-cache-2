5|61|Public
50|$|SETLL {{causes the}} <b>index</b> <b>pointer</b> for the file named in Factor 2 to be {{positioned}} at the location specified by the value in Factor 1.|$|E
50|$|The first 32 {{registers}} {{are often}} used for address manipulation. The remaining 32 registers are used for floating point operations. Because there are four sets of eight byte boundary registers (32-63) most floating point operations are done using only registers without any memory access. Behalf of the first 32 registers one register pair {{is defined as the}} program counter, another pair as the stack pointer, and one more pair as an <b>index</b> <b>pointer</b> for internal operations. There is no dedicated accumulator — any general register can be used for ALU results because the register file is designed to allow up to two read and one write operations for the first 32 registers and up to eight read and one write operations for the remaining 32 registers at the same time. Any pair of registers {{can be used as a}} 16-bit index register.|$|E
40|$|The JOLT (TM) {{commercial}} microcomputer, {{based on}} the MOS Technology 6502 processor chip, for use in Omega navigation system is evaluated. A computer program was prepared in hand-assembled code to demonstrate receiver operation. The processor provides binary processing with interrupts enabled, a carriage return is given to initialize the teleprinter, and a jump is performed to enter the program loop to wait for an interrupt. The program loop operates continuously testing the interrupt flag. The interrupt routine reads the receiver status word and determines whether the current time-slot is the A slot. If so, the interrupt flag, {{which is also the}} data <b>index</b> <b>pointer,</b> is reset to zero. The status word is stored in the status buffer. If the time-slot is not A, the interrupt flag/pointer is incremented by one to index the phase and status to the proper buffer words for later use by the print routine...|$|E
3000|$|... 2) We {{improve the}} method of {{verification}} proofs generating {{which is based on}} the version groups and chained keys. In the verification, verification tags and proofs can be gained by <b>index</b> <b>pointers</b> corresponding to the storage relationship in the version groups and chained keys.|$|R
5000|$|... end of {{valid data}} (<b>index</b> or <b>pointer),</b> or {{amount of data}} {{currently}} in the buffer (integer) ...|$|R
40|$|A brief {{review of}} the Zlib {{development}} is given. Emphasized is the Zlib nerve system which uses the One-Step <b>Index</b> <b>Pointers</b> (OSIP’S) for efficient computation and flexible use of the Truncated Power Series Algebra (TPSA). Also emphasized is the treatment of parameterized maps with an object-oriented language (e. g. C++). A parameterized map can be a Vector Power Series (Vps) or a Lie generator represented by an exponent of a Truncated Power Series (Tps) of which each coefficient is an object of truncated power series...|$|R
40|$|A {{taxonomy}} {{that uses}} 22 attributes to characterize Cprogram overflows {{was used to}} construct 291 small Cprogram test cases {{that can be used}} to diagnostically determine the basic capabilities of static and dynamic analysis buffer overflow detection tools. Attributes in the taxonomy include the buffer location (e. g. stack, heap, data region, BSS, shared memory) ; scope difference between buffer allocation and access ; <b>index,</b> <b>pointer,</b> and alias complexity when addressing buffer elements; complexity of the control flow and loop structure surrounding the overflow ; type of container the buffer is within (e. g. structure, union, array) ; whether the overflow is caused by a signed/unsigned type error ; the overflow magnitude and direction ; and whether the overflow is discrete or continuous. As an example, the 291 test cases were used to measure the detection, false alarm, and confusion rates of five static analysis tools. They reveal specific strengths and limitations of tools and suggest directions for improvements...|$|E
40|$|Graduation date: 1966 A {{method is}} {{presented}} for enlarging read-outs from a Victoreen Model 575 Radocon, an ionization chamber type of radiation survey instrument. The read-out enlargement {{is accomplished by}} taking the signal across the Radocon indicating meter, amplifying it, and using the amplified signal to drive a projection meter. The face and back of the projection meter are made of transparent Lucite so that the meter may {{be placed in a}} standard lantern slide projector, and an enlarged image of the meter scale and <b>index</b> <b>pointer</b> projected on a screen or wall. A field-effect transistor used as a source-follower provides the amplifier with an input impedance of over 100 megohms so that the current flowing through the Radocon indicating meter is not reduced when the amplifier input is connected in parallel with it. An emitter-coupled differential amplifier takes its input from the source of the field-effect transistor; its output drives the projection meter. The differential amplifier provides the gain necessary to accurately drive the projection meter. Through the property of common-mode rejection, the drift in output due to variations in temperature is reduced. In operation, the input of the amplifier is connected in parallel with the Radocon indicating meter. The polarity is arranged so that the voltage on the gate of the field-effect transistor goes from zero to plus 0. 1 volt as the Radocon indicating meter goes from zero to full scale. The amplified signal appears as a voltage difference between the collectors of the two transistors in the differential amplifier. The projection meter, placed between the collectors, provides the final read-out. This method of read-out enlargement makes it possible to accurately display radiation measurements to a large audience. The read-out enlarger, used with the Radocon Model 575, {{can be used in a}} large classroom to demonstrate various physical laws which involve changes in radiation intensity. The same method may also be used to enlarge read-outs from other types of instruments. An accurate enlargement may be obtained without altering the circuit of the instrument or affecting its read-out...|$|E
40|$|Zlib is a {{numerical}} library for Truncated Power Series Al-gebra (TPSA) and Lie Algebra for application to nonlin-ear analysis of single particle dynamics. The first version {{was developed in}} 1990 {{with the use of}} the One-Step <b>Index</b> <b>Pointers</b> (OSIP’s). The OSIP’s form the Zlib nerve that of-fers optimal computation and allow order grading as well as flexible initialization of the global number of variables for the TPSA. While the OSIP’s are still kept for minimum in-dex passing to achieve efficient computation, Zlib has been being upgraded to allow flexible and gradable local num-ber of variables in each C++ object of the Truncated Power Series (Tps) class. Possible applications using Zlib are dis-cussed. ...|$|R
40|$|In particular, {{we address}} the problem of each peer {{choosing}} the k best pointers to store in addition to its <b>index</b> <b>pointers</b> to minimize the average query lookup times. We focus on two popular P 2 P systems, namely Pastry and Chord: we exploit the inherent structure of these systems to develop efficient, scalable algorithms for optimally choosing the k additional pointers. Simulations with Chord and Pastry demonstrate that our algorithms are very effective in reducing the lookup times significantly. Our approach is particularly useful for applications such as name services in mobile environments or location services, where we can expect a low churn rate for peers and a relatively higher churn rate for objects. I...|$|R
40|$|Several {{important}} database reorganization techniques move tuples in a {{table from}} one location {{to another in}} a single pass. For example, the Mariposa distributed database system frequently moves or copies tables between sites. However, moving a table generally invalidates the pointers contained in its secondary indices. Because index reconstruction is extremely resource-intensive, table movement has been considered a expensive operation. In this paper, we present simple, efficient mechanisms for translating <b>index</b> <b>pointers.</b> We also demonstrate their effectiveness using performance measurements of an implementation in Mariposa. Use of these mechanisms will enable parallel and distributed systems like Mariposa to move tables more freely, pro viding many more options for performance-enhancing reorganizations of the database. 1...|$|R
25|$|Coming {{into the}} market in the early 1880s, the index {{typewriter}} uses a pointer or stylus to choose a letter from an <b>index.</b> The <b>pointer</b> is mechanically linked so that the letter chosen could then be printed, most often by the activation of a lever.|$|R
40|$|Abstract — Virtualization {{is being}} widely adopted in today’s {{computing}} systems. Its unique security advantages in isolating and introspecting commodity OSes as virtual machines (VMs) have enabled {{a wide spectrum}} of applications. However, a common, fundamental assumption is the presence of a trustworthy hypervisor. Unfortunately, the large code base of commodity hypervisors and recent successful hypervisor attacks (e. g., VM escape) seriously question the validity of this assumption. In this paper, we present HyperSafe, a lightweight approach that endows existing Type-I bare-metal hypervisors with a unique self-protection capability to provide lifetime controlflow integrity. Specifically, we propose two key techniques. The first one – non-bypassable memory lockdown – reliably protects the hypervisor’s code and static data from being compromised even in the presence of exploitable memory corruption bugs (e. g., buffer overflows), therefore successfully providing hypervisor code integrity. The second one – restricted <b>pointer</b> <b>indexing</b> – introduces one layer of indirection to convert the control data into <b>pointer</b> <b>indexes.</b> These <b>pointer</b> <b>indexes</b> are restricted such that the corresponding call/return targets strictly follow the hypervisor control flow graph, hence expanding protection to control-flow integrity. We have built a prototype and used it to protect two open-source Type-I hypervisors: BitVisor and Xen. The experimental results with synthetic hypervisor exploits and benchmarking programs show HyperSafe can reliably enable the hypervisor self-protection and provide the integrity guarantee with a small performance overhead. I...|$|R
40|$|The {{objects of}} {{attention}} can be located anywhere along the causal link {{from the source}} of stimuli to the final output of the vision system. As causes, they attract and control attention, and as products, they constitute targets of analysis and explicit comments. Stimulus-driven <b>indexing</b> creates <b>pointers</b> that support fast and frugal cognition...|$|R
40|$|Abstract:- Textual {{substitution}} methods, {{often called}} dictionary methods or Lempel-Ziv methods, after the important work of Lempel and Ziv, are one-dimensional compression methods that maintain a constantly changing dictionary of strings to adaptively compress {{a stream of}} characters by replacing common substrings with <b>indices</b> (<b>pointers)</b> into a dictionary. Lempel and Ziv proved that the proposed schemes were practical as well as asymptotically optimal for a general source model. Two-dimensional (i. e. images) applications of textual substitution methods have been widely studied in the past. Those applications involve first {{the application of a}} linearization strategy to the input data, and then the encoding of the resulting mono-dimensional vector using LZ type one-dimensional methods. More recent strategies blend textual substitution methods with Vector Quantization. In this paper we discuss the textual substitution methods for image compression, with particular attention to the AVQ class of algorithms, and review recent advances in the field...|$|R
40|$|This paper {{describes}} {{an effort to}} extend the LempelZiv algorithm to a practical universal lossy compression algorithm. It {{is based on the}} idea of approximate string matching with a rate-distortion (R Γ D) criterion, and is addressed within the framework of vector quantization (VQ) [4]. A practical one pass algorithm for VQ codebook construction and adaptation for individual signals is developed which assumes no prior knowledge of the source statistics and involves no iteration. We call this technique rate-distortion Lempel-Ziv (RDLZ). As {{in the case of the}} Lempel-Ziv algorithm, the encoded bit stream consists of codebook (dictionary) updates as well as <b>indices</b> (<b>pointers)</b> to the codebook. The idea of "trading" bits for distortion in modifying the codebook will be introduced. Experimental results show that, for Gaussian sources as well as real images, RDLZ performs comparably, sometimes favorably, to static codebook VQ trained on the corresponding sources or images. 1. INTRODUCTION [...] ...|$|R
40|$|Textual {{substitution}} methods, {{often called}} dictionary methods or Lempel-Ziv methods, after the important work of Lempel and Ziv, are one-dimensional compression methods that maintain a constantly changing dictionary of strings to adaptively compress {{a stream of}} characters by replacing common substrings with <b>indices</b> (<b>pointers)</b> into a dictionary. Lempel and Ziv proved that the proposed schemes were practical as well as asymptotically optimal for a general source model. Two-dimensional (i. e. images) applications of textual substitution methods have been widely studied in the past. Those applications involve first {{the application of a}} linearization strategy to the input data, and then the encoding of the resulting monodimensional vector using LZ type one-dimensional methods. More recent strategies blend textual substitution methods with Vector Quantization. In this paper we discuss the textual substitution methods for image compression, with particular attention to the AVQ class of algorithms, and review recent advances in the field...|$|R
30|$|Athalye et al. [7] {{presented}} generic architectures for {{the implementation}} of the Sequential Importance Resampling Filter (SIRF). The proposed architecture is based on using dual-port memory. The memory stores the addresses of the particles in its upper half, while the sampled particles are stored in {{the lower half of the}} memory. The idea is that the resampling unit returns the set of <b>indexes</b> (<b>pointers)</b> of the replicated particles instead of the particles themselves. Using index addressing alone does not ensure that the scheme with the single memory will work correctly. They used other memories to store the indexes of the replicated particles and the discarded particles. The size of overall used memory is 4 M: 2 M depth dual-port memory to store the addresses and particles state vectors, M depth memory to store the replicated particles indexes and M depth FIFO to store the discarded particle indexes. They proposed two architectures to implement the SIRF using systematic resampling (SR) and using residual systematic resampling (RSR) algorithms.|$|R
40|$|Large {{inverted}} {{indices are}} by now {{common in the}} construction of web-scale search engines. For faster access, inverted indices are indexed internally so {{that it is possible to}} skip quickly over unnecessary documents. The classical approach to skipping dictates that a skip should be positioned every √ f document pointers, where f is the overall number of documents where the term appears. We argue that due to the growing size of the web more refined techniques are necessary, and describe how to embed a compressed perfect skip list in an inverted list. We provide statistical models that explain the empirical distribution of the skip data we observe in our experiments, and use them to devise good compression techniques that allow us to limit the waste in space, so that the resulting data structure increases the overall index size by just a few percents, still making it possible to <b>index</b> <b>pointers</b> with a rather fine granularity. ...|$|R
5000|$|In-place {{can have}} {{slightly}} different meanings. In its strictest form, the algorithm {{can only have}} a constant amount of extra space, counting everything including function calls and pointers. However, this form is very limited as simply having an index to a length n array requires O(log n) bits. More broadly, in-place means that the algorithm does not use extra space for manipulating the input but may require a small though nonconstant extra space for its operation. Usually, this space is O(log n), though sometimes anything in o(n) is allowed. Note that space complexity also has varied choices in {{whether or not to}} count the index lengths as part of the space used. Often, the space complexity is given {{in terms of the number}} of <b>indices</b> or <b>pointers</b> needed, ignoring their length. In this article, we refer to total space complexity (DSPACE), counting pointer lengths. Therefore, the space requirements here have an extra log n factor compared to an analysis that ignores the length of <b>indices</b> and <b>pointers.</b>|$|R
40|$|This paper {{presents}} {{a new approach}} to dependence testing in the presence of nonlinear and non-closed array <b>index</b> expressions and <b>pointer</b> references. The chains of recurrences formalism and algebra is used to analyze the recurrence relations of induction variables, and for constructing recurrence forms of array <b>index</b> expressions and <b>pointer</b> references. We use these recurrence forms to determine if the array and pointer references are free of dependences in a loop nest. Our recurrence formulation enhances the accuracy of standard dependence algorithms such as the extreme value test and range test. Because the recurrence forms are easily converted to closed forms (when they exist), induction variable substitution and array recovery can be delayed until after the loop is analyzed...|$|R
30|$|Data owners’ {{outsourced}} data on cloud {{data storage}} servers by the deduplication technique can reduce {{not only their}} own storage cost but also cloud’s. This paradigm also introduces new security {{issues such as the}} potential threat of data lost or corrupted. Data integrity verification is utilized to safeguard these data integrity. However, the cloud deduplication storage only focuses on file/chunk level to store one copy of the same data hosted by different data owners, and is not concerned with the same part of different data, e.g., a series of version files. We propose an integrity verification algorithm of different version files. The algorithm establishes the generic storage model of different version control methods to improve the universality of data verification. Then, the methods of verification tags and proofs generating are improved based on the <b>index</b> <b>pointers</b> corresponding to the storage relationship in the version groups and chained keys. Finally, the random diffusion extraction based on the random data sampling in the version group is proposed to improve the verification efficiency. The results of theoretical and experimental analysis indicate that the algorithm can achieve fast and large-scale verification for different version data.|$|R
50|$|Due to the {{inconsistent}} {{usage of}} CUE data definition, the additional requirement that CUE chunk names {{be stored in}} an additional LABL chunk, along with the inherent 32-bit limitation of the CUE chunk <b>pointer</b> <b>index,</b> the RF64 format also defines a new R64M marker chunk.|$|R
50|$|For {{its first}} use in combat, during the 1862 Peninsula Campaign, the Rogers train {{substituted}} a new telegraph instrument, the Beardslee Patent Magneto-Electric Field Telegraph Machine, invented by George W. Beardslee of New York. This instrument required no battery, using instead a hand-cranked generator, {{but it was}} also based on a dial indicator. The Beardslee telegraph was housed in a wooden chest with handles and weighed about 100 pounds. It had two significant technical deficiencies, however. Its generator could not produce enough electricity to transmit signals more than about 5 to 8 miles. More seriously, there was a tendency for the sending and receiving <b>index</b> <b>pointers</b> to get out of synchronization, hopelessly garbling transmitted messages. Broken machines had to be sent back to New York City for repair. It was these deficiencies that led Myer to his decision to use traditional Morse key technology and attempt to hire trained telegraphers, a decision that led Secretary Stanton to dismiss him as chief signal officer. All of the telegraph train assets of the Signal Corps were turned over to the Military Telegraph Service, but they did not use the Beardslee telegraph due to its unreliability. At their peak of usage in 1863, there were 30 telegraph trains in the field.|$|R
40|$|Formulas are {{presented}} for the recursive generation of four-body integrals {{in which the}} integrand consists of arbitrary integer powers (>= - 1) of all the interparticle distances r_ij, multiplied by an exponential containing an arbitrary linear combination of all the r_ij. These integrals are generalizations of those encountered using Hylleraas basis functions, and include all that are needed to make energy computations on the Li atom and other four-body systems with a fully exponentially correlated Slater-type basis of arbitrary quantum numbers. The only quantities needed to start the recursion are the basic four-body integral first evaluated by Fromm and Hill, plus some easily evaluated three-body "boundary" integrals. The computational labor in constructing integral sets for practical computations is less than when the integrals are generated using explicit formulas obtained by differentiating the basic integral with respect to its parameters. Computations are facilitated by using a symbolic algebra program (MAPLE) to compute array <b>index</b> <b>pointers</b> and present syntactically correct FORTRAN source code as output; in this way {{it is possible to}} obtain error-free high-speed evaluations with minimal effort. The work can be checked by verifying sum rules the integrals must satisfy. Comment: 10 pages, no figures, accepted by Phys. Rev. A (January 2009...|$|R
40|$|Many {{database}} reorganization techniques move tuples in a {{table from}} one location {{to another in}} a single pass. For example, distributed database systems move or copy tables between sites to optimize data placement. However, such systems typically drop and then rebuild the secondary indices defined over the table being moved. There are two primary reasons for this. First, moving a table invalidates any physical tuple pointers contained in its secondary indices (e. g., in the leaves of a tree). Second, changes in tuple or page size can cause index tuples on the remote site to be repacked onto pages {{in a way that}} degrades the clustering imposed by the structure (e. g., in the upper levels of an R-tree). The cost of rebuilding secondary indices is largely why table movement has been considered a expensive operation. This, in turn, means that data layout optimization has been considered expensive as well. In this paper, we present a simple, efficient mechanism for translating <b>index</b> <b>pointers</b> as well as an approach to preserving internal index clustering. By exploiting the structure of the original index, we can recycle its important properties and produce a usable index on the remote site without the expense of building one from scratch. We also demonstrate the effectiveness of these mechanisms using performance measurements of an implementation in the Mariposa distributed data manager. 1...|$|R
40|$|Pointers are an {{important}} programming concept. They are used explicitely or implicitly in many programming languages. In particular, the semantics of object-oriented programming languages rely on pointers. We introduce a semantics for pointer structures. Pointers are seen as <b>indexes</b> and <b>pointer</b> fields are functions from these indexes to values. Using this semantics we turn all pointer operations into simple assignments and then we use refinement calculus techniques to construct a pointer-manipulating program that checks {{whether or not a}} single linked list has a loop. We also introduce an induction principle on pointer structures in order to reduce complexity of the proofs. 1...|$|R
40|$|This paper reports our {{implementation}} of the Virtual Corpus approach to deriving ngram statistics for ngrams of any length from large-scale corpora based on the suffix array data structure. In order to enable the VC to accommodate corpora with a vocabulary of different size, we first convert corpus tokens into integer codes. To accelerate the processing, we employ a bucket-radixsort for sorting the VC <b>indices</b> (or <b>pointers,</b> each of which represents a sequence of corpus tokens from its position {{to the end of}} the corpus). The time complexity of the sorting algorithm is of O(N log n N) in token code comparisons. ...|$|R
50|$|In Sami shamanism, the noaidi - the Sami shaman - {{used the}} drum {{to get into}} a trance, or to obtain {{information}} from the future or about other places. The drum was held in one hand, and operated with the other hand. While the noaidi was in trance, his free spirit left his body to visit the spiritual world or other places. When used for divination purposes, the drum was used together with a drum hammer and an vuorbi (<b>index</b> or <b>pointer)</b> made of brass or horn. Answers could be interpreted from where the vuorbi stopped on the membrane, and at which symbols.|$|R
5000|$|INDX [...] - [...] <b>Index</b> {{create a}} <b>pointer</b> (copy {{descriptor}}) from a base (MOM) descriptorNXLN [...] - [...] Index and load name (resulting in an indexed descriptor)NXLV [...] - [...] Index and load value (resulting in a data value)EVAL [...] - [...] Evaluate descriptor (follow address chain until data word or another descriptor found) ...|$|R
5000|$|Linked lists {{have several}} {{advantages}} over dynamic arrays. Insertion or deletion of an element {{at a specific}} point of a list, assuming that we have <b>indexed</b> a <b>pointer</b> to the node (before the one to be removed, or before the insertion point) already, is a constant-time operation (otherwise without this reference it is O(n)), whereas insertion in a dynamic array at random locations will require moving half of the elements on average, and all the elements in the worst case. While one can [...] "delete" [...] an element from an array in constant time by somehow marking its slot as [...] "vacant", this causes fragmentation that impedes the performance of iteration.|$|R
50|$|The ST7 has six registers: the accumulator, X and Y <b>index</b> registers, stack <b>pointer,</b> program counter, and {{condition}} code register. Also, double-indirect addressing allows the zero page of RAM {{to serve as}} additional registers. An unusual but useful feature is that an interrupt pushes four of these registers on the stack (A and X {{as well as the}} usual PC and CC), and interrupt return restores them.|$|R
2500|$|Linked lists {{have several}} {{advantages}} over dynamic arrays. Insertion or deletion of an element {{at a specific}} point of a list, assuming that we have <b>indexed</b> a <b>pointer</b> to the node (before the one to be removed, or before the insertion point) already, is a constant-time operation (otherwise without this reference it is O(n)), whereas insertion in a dynamic array at random locations will require moving half of the elements on average, and all the elements in the worst case. [...] While one can [...] "delete" [...] an element from an array in constant time by somehow marking its slot as [...] "vacant", this causes fragmentation that impedes the performance of iteration.|$|R
50|$|The {{base level}} MIX format has four files: a {{metadata}} file, an index file, a status file, and some set of message data files. The metadata file contains base-level data {{applicable to the}} entire mailbox; i.e., the UID validity, last assigned UID, and list of keywords. The <b>index</b> file contains <b>pointers</b> to each unexpunged message in the message data files, along with flags, size, and IMAP internaldate data. The status file contains per-message flags and keywords.|$|R
50|$|The data {{is present}} in {{arbitrary}} order, but the logical ordering is specified by the index. The data rows may be spread throughout the table regardless {{of the value of}} the indexed column or expression. The non-clustered index tree contains the index keys in sorted order, with the leaf level of the <b>index</b> containing the <b>pointer</b> to the record (page and the row number in the data page in page-organized engines; row offset in file-organized engines).|$|R
50|$|While a pointer {{contains}} {{the address of}} the item to which it refers, a handle is an abstraction of a reference which is managed externally; its opacity allows the referent to be relocated in memory by the system without invalidating the handle, which is impossible with pointers. The extra layer of indirection also increases the control that the managing system has over the operations performed on the referent. Typically the handle is an <b>index</b> or a <b>pointer</b> into a global array of tombstones.|$|R
