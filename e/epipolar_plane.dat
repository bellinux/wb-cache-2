69|16|Public
50|$|As an {{alternative}} visualization, consider the points X, OL & OR that form a plane called the <b>epipolar</b> <b>plane.</b> The <b>epipolar</b> <b>plane</b> intersects each camera's image plane where it forms lines—the epipolar lines. All epipolar planes and epipolar lines intersect the epipole {{regardless of where}} X is located.|$|E
40|$|General variational {{framework}} for inverse problems on ray space Specialization to arbitrary convex data terms and spatial regularizers possible Currently implemented are denoising, disparity map regularization, inpainting, multi-label segmentation 4 D Light Field Parametrization and <b>Epipolar</b> <b>Plane</b> Images (EPIs...|$|E
40|$|The method seeking correspondences in <b>Epipolar</b> <b>Plane</b> Images (EPI) is presented. The {{principal}} idea is {{to employ}} dense sequence {{to get more information}} which could guide the correspondence algorithm. Theoretically a simple technique finding homogeneous straight lines in EPI suffices to establish correspondences...|$|E
40|$|Classical {{multiview}} geometry assumes pinhole (central) cameras. All rays of {{a central}} camera intersect in a projection center. In this work we introduce oblique cameras. Their rays do not intersect at all. For such cameras in certain configurations we elaborate a theory that generalizes classical <b>epipolar</b> geometry. <b>Epipolar</b> <b>planes</b> are replaced by double ruled quadrics and epipoles disappear. The rays in double ruled quadrics can be parameterized by one parameter along a line. Therefore, the correspondences between oblique images {{can also be found}} separately in one-parametric subsets of oblique images, analogically to finding correspondences within corresponding epipolar lines. Subsets, in which the correspondences can be solved separately from the rest, are called visibility closures. We give a configuration of rays in projectire space that forms two different oblique cameras with visibility closures being double ruled quadrics. We also show that other arrangements of non-central cameras, which appeared in the literature, have double ruled visibility clo- sures...|$|R
40|$|We {{study the}} problem of {{estimating}} the epipolar geometry from apparent contours of smooth curved surfaces with affine camera models. Since apparent contours are viewpoint dependent, the only true image correspondences are projections of the frontier points, i. e., surface points whose tangent planes are also their <b>epipolar</b> <b>planes.</b> However, frontier points are unknown a priori and must be estimated simultaneously with epipolar geometry. Previous approaches to this problem adopt local greedy search methods which are sensitive to initialization, and may get trapped in local minima. We propose the first algorithm that guarantees global optimality for this problem. We first reformulate the problem using a separable form {{that allows us to}} search effectively in a 2 D space, instead of on a 5 D hypersphere in the classical formulation. Next, in a branch-andbound algorithm we introduce a novel lower bounding function through interval matrix analysis. Experimental results on both synthetic and real scenes demonstrate that the proposed method is able to quickly obtain the optimal solution. 1...|$|R
40|$|Epipolar {{geometry}} of a stereopair {{can be expressed}} either in 3 D, as the relative orientation (i. e. translation and rotation) of two bundles of optical rays in case of calibrated cameras or, in case of unclalibrated cameras, in 2 D as {{the position of the}} epipoles on the image planes and a projective transformation that maps points in one image to corresponding epipolar lines on the other. The typical coplanarity equation describes the first case; the Fundamental matrix describes the second. It has also been proven in the Computer Vision literature that 2 D epipolar geometry imposes two independent constraints on the parameters of camera interior orientation. In this contribution these constraints are expressed directly in 3 D Euclidean space by imposing the equality of the dihedral angle of <b>epipolar</b> <b>planes</b> defined by the optical axes of the two cameras or by suitably chosen corresponding epipolar lines. By means of these constraints, new closed form algorithms are proposed for the estimation of a variable or common camera constant value given the fundamental matrix and the principal point position of a stereopair...|$|R
40|$|General {{segmentation}} framework using substitutable classifier and optimization components Ray space features extend common classifiers {{to overcome}} problems of single view classification Variational multi-label optimization framework for consistent multi-label assignment on 4 D ray space 4 D Light Field Parametrization and <b>Epipolar</b> <b>Plane</b> Images (EPIs...|$|E
40|$|This paper {{presents}} {{a method for}} efficiently rendering a sequence of regularly spaced perspectives of a scene. The algorithm presented, called Multiple Viewpoint Rendering (MVR), exploits perspective coherence by rendering <b>epipolar</b> <b>plane</b> images. The computer graphics camera geometry is constrained to produce <b>epipolar</b> <b>plane</b> images with linear features. Transformation and shading operations can be performed once per image sequence instead of once per view. Geometric position, color, and texture coordinates can be computed using linear interpolation. Both view independent and dependent shading algorithms are supported. Both one and two-dimensional grids of perspectives can be rendered. Details of a hardware-accelerated implementation of a multiple viewpoint polygon renderer are given. 1 Introduction Most of the time, photography captures information about the world as seen from a single viewpoint. A still camera's single lens records a scene as it appears at one instant, while a movie cam [...] ...|$|E
40|$|We {{present a}} new {{benchmark}} database to compare and evaluate existing and upcoming algorithms which are tailored to light field processing. The data is characterised by a dense {{sampling of the}} light fields, which best fits current plenoptic cameras and is a characteristic property not found in current multi-view stereo benchmarks. It allows to treat the disparity space as a continuous space, and enables algorithms based on <b>epipolar</b> <b>plane</b> image analysis without having to refocus first. All datasets provide ground truth depth {{for at least the}} center view, while some have additional segmentation data available. Part of the light fields are computer graphics generated, the rest are acquired with a gantry, with ground truth depth established by a previous scanning of the imaged objects using a structured light scanner. In addition, we provide source code for an extensive evaluation of a number of previously published stereo, <b>epipolar</b> <b>plane</b> image analysis and segmentation algorithms on the database. 1...|$|E
40|$|In this paper, {{we propose}} {{a method for}} {{generating}} arbitrary view image by interpolating images between three cameras using epipolar geometry. Projective geometry has recently {{been used in the}} field of computer vision, because projective geometry can be easily determined compararing with Euclidean geometry. In the proposed method, three input camera images are rectified so that the vertical and horizontal directions can be completely aligned to the <b>epipolar</b> <b>planes</b> between the cameras. This rectification provides Projective Voxel Space (PVS), in which the three axes are aligned with the direction of camera’s projection. Such alignment simplifies the procedure for projection and back projection between the 3 D space and the image planes. First, we apply shape-from-silhouette with taking advantage of PVS. The consistency of color value between the images is evaluated for final determination of the object surface voxel. Therefore, consistent matching in three images is estimated and images can be interpolated from the matching information. Synthesized images are based on 3 D shape in PVS, so the occlusion of the object is reproduced in the generated images, however it requires only weak calibration...|$|R
40|$|In this paper, {{we propose}} a new method for 3 D {{reconstruction}} from three cameras {{based on the}} projective geometry. If the subject is just synthesizing images from new viewpoint, 3 D shape reconstruction in Euclidean space is not required, and projective reconstruction gives enough information to synthesize new viewpoint images. This means that full calibration is not needed but recovery of epipolar geometry between input cameras is sufficient. In the proposed method, three input camera images are rectified so that the vertical and horizontal directions can be completely aligned to the <b>epipolar</b> <b>planes</b> between the cameras. This rectification provides Projective Voxel Space(PVS), in which the three axis is aligned with the camera’s projection direction. Such alignement simplifies the procedure for projection and back projection between the 3 D space and the image planes. Taking advantage of this PVS, we apply shape-from-silhouette in the PVS to acquire bounding space of the object. The consistency of color value between the projected pixel in the camera images is evaluated for final determination of the object surface voxel. ...|$|R
40|$|This article {{presents}} a methodology for reconstruction of 3 D faces {{which is based}} on stereoscopic images of the scene using active and passive surface reconstruction [...] A sequence of gray patterns is generated, which are projected onto the scene and their projection recorded by a pair of stereo cameras [...] The images are rectified to make coincident their <b>epipolar</b> <b>planes</b> and so to generate a stereo map of the scene [...] An algorithm for stereo matching is applied, whose result is a bijective mapping between subsets of the pixels of the images [...] A particular connected subset of the images (e. g. the face) is selected by a segmentation algorithm [...] The stereo mapping is applied to such a subset and enables the triangulation of the two image readings therefore rendering the (x;y; z) points of the face, which in turn allow the reconstruction of the triangular mesh of the face [...] Since the surface might have holes, bilateral filters are applied to have the holes filled [...] The algorithms are tested in real conditions and we evaluate their performance with virtual datasets [...] Our results show a good reconstruction of the faces and an improvement of the results of passive system...|$|R
40|$|This paper {{presents}} {{a novel approach}} to automatic traffic monitoring using 2 D spatiotemporal images. A TV camera is mounted above a highway to monitor the traffic through two slice windows, and a panoramic view image and an <b>epipolar</b> <b>plane</b> image are formed for each lane. Our real-time vision system for automatic traffic monitoring, VISATRAM, is an inexpensive system with a PC 486 and a frame grabber. The system can not only count vehicles and estimate their speeds, but also classify them using 3 D measurements. The system has been tested with real road images under various light conditions, including shadows in daytime and lights at night. Keywords: Intelligent vehicle/highway system, traffic monitoring, spatio-temporal image, <b>epipolar</b> <b>plane</b> image, panoramic view image * The author is currently on a leave in the Computer Science Department, University of Massachusetts at Amherst, MA 01003. Email: zhu@cs. umass. edu, or zhuzhg@mail. tsinghua. edu. cn. This work was supported by China Advance [...] ...|$|E
40|$|Abstract. Geometric {{constraints}} have {{an essential}} significance for point correspondence in computer vision systems. Traditional epipolar constraint in bi-ocular system faces two main problems: threshold-setting and corresponding ambiguities. This paper describes a collinear <b>epipolar</b> <b>plane</b> model and proposes a novel criterion for bi-ocular corresponding which allows setting a uniform threshold. Furthermore, it proposes the concept tri-correspondence units, proves their specificity against ambiguities, and discusses {{the merging of}} them...|$|E
40|$|We {{present the}} method seeking correspondences in a dense {{rectified}} image sequence, {{considered as a}} set of <b>Epipolar</b> <b>Plane</b> Images (EPI). The main idea is to employ dense sequence to get more information which could guide the correspondence algorithm. A set of EPIs has properties that are favorable for evaluating the quality of correspondence pair (correspondence cost). Information contained in image data is used directly, no features are detected. Our spatio-temporal volume analysis approach aims at accuracy and density of the correspondences established...|$|E
40|$|International audienceThis paper {{presents}} a geometric approach to recognizing smooth objects from their outlines. We define a signature function that associates feature vectors with objects and baselines connecting pairs of possible viewpoints. Feature vectors, {{which can be}} projective, affine, or Euclidean, are computed using the planes that pass through a fixed baseline and are also tangent to the object's surface. In the proposed framework, matching a test outline {{to a set of}} training outlines is equivalent to finding intersections in feature space between the images of the training and the test signature functions. The paper presents experimental results for the case of internally calibrated perspective cameras, where the feature vectors are angles between <b>epipolar</b> tangent <b>planes...</b>|$|R
40|$|This paper {{presents}} a geometric approach to recognizing smooth objects from their outlines. We define a signature function that associates feature vectors to lines in space, considered as baselines connecting pairs of possible viewpoints. The feature vector of a baseline is computed using the <b>planes</b> from its <b>epipolar</b> pencil {{that are also}} tangent to the object's surface. This approach encompasses projective, affine, and Euclidean features, depending on the desired camera model. Given a training set of outlines acquired by a camera undergoing an unknown motion, we build a representation of its signature function, and a single test outline from a new viewpoint gives another function corresponding to all baselines passing through its camera center. The task of matching the test outline to the training sequence then becomes equivalent to finding an intersection of {{the images of the}} training and the test signature functions. The paper demonstrates experimental results for the case of internally calibrated perspective cameras, where the feature vectors are angles between <b>epipolar</b> tangent <b>planes...</b>|$|R
40|$|Abstract — We {{present a}} method for {{detecting}} motion regions in video sequences observed by a moving camera, {{in the presence of}} strong parallax due to static 3 D structures. The proposed method classifies each image pixel into planar background, parallax or motion regions by sequentially applying 2 D planar homographies, the epipolar constraint and a novel geometric constraint, called “structure consistency constraint”. The structure consistency constraint is the main contribution of this paper, and is derived from the relative camera poses in three consecutive frames and is implemented within the “Plane+Parallax ” framework. Unlike previous planar-parallax constraints proposed in the literature, the structure consistency constraint does not require the reference plane to be constant across multiple views. It directly measures the inconsistency between the projective structures from the same point under camera motion and reference plane change. The structure consistency constraint is capable of detecting moving objects followed by a moving camera in the same direction, a so called degenerate configuration where the epipolar constraint fails. We demonstrate the effectiveness and robustness of our method with experimental results on real-world video sequences. Index Terms — Motion detection, multiple view geometry, <b>epipolar</b> constraint, <b>plane</b> plus parallax I...|$|R
40|$|In this paper, {{we propose}} a depth map {{estimation}} algorithm, based on <b>Epipolar</b> <b>Plane</b> Image (EPI) line extraction, that {{is able to}} correctly handle partially occluded objects in wide baseline camera setups. Furthermore, we introduce a descriptor matching technique to reduce the negative influence of inaccurate color correction and similarly textured objects on the depth maps. A visual comparison between an existing EPI-line extraction algorithm and our method is provided, showing that our method provides more accurate and consistent depth maps in most cases. SCOPUS: cp. pinfo:eu-repo/semantics/publishe...|$|E
40|$|In {{this paper}} we model {{the problem of}} {{structure}} from motion as the range estimation with known motion First, we approximate the motion within a reasonable time interval as a 30 translation and thus some image transformations are applied to convert an arbitrary motion to a ID translation Second we have avoided the feature extraction and correspondence problems by analyzing the <b>epipolar</b> <b>plane</b> image in the Fourier domain Experimental results with real scene images on campus have shown the eflciency and robustness of the approach...|$|E
40|$|International audienceWe {{consider}} {{the synthesis of}} intermediate views of an object captured by two widely spaced and calibrated cameras. This problem is challenging because foreshortening effects and occlusions induce {{significant differences between the}} reference images when the cameras are far apart. That makes the association or disappearance/appearance of their pixels difficult to estimate. Our main contribution lies in disambiguating this illposed problem by making the interpolated views consistent with a plausible transformation of the object silhouette between the reference views. This plausible transformation is derived from an object-specific prior that consists of a nonlinear shape manifold learned from multiple previous observations of this object by the two reference cameras. The prior is used to estimate how the epipolar silhouette segments observed in the reference views evolve between those views. This information directly supports the definition of epipolar silhouette segments in the intermediate views, and the synthesis of textures in those segments. It permits to reconstruct the <b>Epipolar</b> <b>Plane</b> Images (EPIs) and the continuum of views associated with the <b>Epipolar</b> <b>Plane</b> Image Volume, obtained by aggregating the EPIs. Experiments on synthetic and natural images show that our method preserves the object topology in intermediate views and deals effectively with the selfoccluded regions and the severe foreshortening effect associated with wide-baseline camera configurations...|$|E
40|$|International audienceThis {{paper is}} {{about the problem of}} {{structure}} and motion recovery from two views of a rigid scene. Especially, we deal with the case of scenes containing planes, i. e. there are sets of coplanar points. Coplanarity is a strong constraint for both structure recovery and motion estimation. Most existing works do only exploit one of the two aspects, or, if both, then in a sub-optimal manner. A typical example is to estimate motion (epipolar geometry) using raw point correspondences, to perform a 3 D reconstruction and then to fit planes and maybe correct 3 D point positions to make them coplanar. In this paper we present an approach to estimate camera motion and piecewise planar structure simultaneously and optimally: the result is the estimation of camera motion and 3 D structure, that minimizes reprojection error while satisfying the piecewise planarity. The estimation problem is minimally parameterized using 2 D entities-epipoles, <b>epipolar</b> transformation, <b>plane</b> homographies and image points-subsequently deriving the corresponding 3 D entities is trivial. Experimental results show that the reconstruction is of clearly superior quality compared to traditional methods based only on points, even if the scene is not perfectly piecewise planar...|$|R
40|$|Abstract—We {{present a}} method for {{detecting}} motion regions in video sequences observed by a moving camera {{in the presence of}} a strong parallax due to static 3 D structures. The proposed method classifies each image pixel into planar background, parallax, or motion regions by sequentially applying 2 D planar homographies, the epipolar constraint, and a novel geometric constraint called the “structure consistency constraint. ” The structure consistency constraint, being the main contribution of this paper, is derived from the relative camera poses in three consecutive frames and is implemented within the “Plane þ Parallax ” framework. Unlike previous planar-parallax constraints proposed in the literature, the structure consistency constraint does not require the reference plane to be constant across multiple views. It directly measures the inconsistency between the projective structures from the same point under camera motion and reference plane change. The structure consistency constraint is capable of detecting moving objects followed by a moving camera in the same direction, a so-called degenerate configuration where the epipolar constraint fails. We demonstrate the effectiveness and robustness of our method with experimental results of real-world video sequences. Index Terms—Motion detection, multiple-view geometry, <b>epipolar</b> constraint, <b>Plane</b> þ Parallax. ...|$|R
40|$|This {{paper is}} {{about the problem of}} {{structure}} and motion recovery from two views of a rigid scene. Especially, we deal with the case of scenes containing planes, i. e. there are sets of coplanar points. Coplanarity is a strong constraint for both structure recovery and motion estimation. Most existing works do only exploit one of the two aspects, or, if both, then in a sub-optimal manner. A typical example is to estimate motion (epipolar geometry) using raw point correspondences, to perform a 3 D reconstruction and then to fit planes and maybe correct 3 D point positions to make them coplanar. In this paper, we present an approach to estimate camera motion and piecewise planar structure simultaneously and optimally: the result is the estimation of camera motion and 3 D structure, that minimizes reprojection error, while satisfying the piecewise planarity. The estimation problem is minimally parameterized using 2 D entities – epipoles, <b>epipolar</b> transformation, <b>plane</b> homographies and image points – subsequently deriving the corresponding 3 D entities is trivial. Experimental results show that the reconstruction is of clearly superior quality compared to traditional methods based only on points, even if the scene is not perfectly piecewise planar. 1...|$|R
40|$|Abstract—We {{develop a}} {{continuous}} {{framework for the}} analysis of 4 D light fields, and describe novel variational methods for disparity reconstruction as well as spatial and angular super-resolution. Disparity maps are estimated locally using <b>epipolar</b> <b>plane</b> image analysis without the need for expensive matching cost minimization. The method works fast and with inherent subpixel accuracy, since no discretization of the disparity space is necessary. In a variational framework, we employ the disparity maps to generate super-resolved novel views of a scene, which corresponds to increasing the sampling rate of the 4 D light field in spatial as well as angular direction. In contrast to previous work, we formulate the problem of view synthesis as a continuous inverse problem, which allows us to correctly take into account foreshortening effects caused by scene geometry transformations. All optimization problems are solved with state-of-the-art convex relaxation techniques. We test our algorithms on a number of real-world examples as well as our new benchmark dataset for lightfields, and compare results to a multiview stereo method. The proposed method is both faster as well as more accurate. Data sets and source code are provided online for additional evaluation. Index Terms—Light fields, <b>epipolar</b> <b>plane</b> images, 3 D reconstruction, super-resolution, view interpolation, variational methods...|$|E
40|$|Vehicles {{parked on}} streets make traffic {{problems}} {{in urban areas}} worse. We propose a method to detect those street-parking vehicles, using <b>epipolar</b> <b>plane</b> images (EPIs) acquired by a line scan camera, for traffic census. Our method is based on calculating {{the distance between the}} vehicles and the camera from the slope of the feature paths in an EPI. The feature paths are extracted in an EPI by the Hough transformation. The experimental results show that our proposed method can detect vehicles almost at 70 % rate of detection...|$|E
40|$|Ray-space {{interpolation}} {{is one of}} the key {{technologies to}} generate virtual viewpoint images, typically in <b>epipolar</b> <b>plane</b> images (EPI), to realize free viewpoint television (FTV). In this paper, a novel Radon transform based ray-space interpolation algorithm (RTI) is proposed to generate virtual viewpoint images in the EPI. In the proposed RTI, feature points of each EPI are first extracted to form the corresponding feature <b>epipolar</b> <b>plane</b> image (FEPI). Radon transform is then applied to each FEPI to detect the candidate interpolation direction set. Then corresponding pixels in neighboring real view rows for each pixel to be interpolated are found by some block matching based interpolation method, in which the smoothness property of the disparity field and the correlation among neighboring EPIs are explored. Possible occlusion regions are processed by the proposed one-sided interpolation. Finally, to solve the problem that the cameras are not equally spaced, a novel ray-space based spacing correction algorithm is proposed to correct the EPI such that pixels corresponding to the same scene point will form a straight line. Experimental results suggest that the proposed RTI and the spacing correction algorithm can achieve good performance. Moreover, compare with traditional methods, our proposed algorithm can generate good interpolation result even if apriori depth range is not known. © 2013 3 D Display Research Center and Springer-Verlag Berlin Heidelberg...|$|E
40|$|The notable {{improvements}} {{on performance}} and low cost of digital cameras and GPS/IMU devices have caused MMSs (Mobile Mapping Systems) to be gradually {{becoming one of}} the most important devices for mapping highway and railway networks, generating and updating road navigation data and constructing urban 3 D models over the last 20 years. Moreover, the demands for large scale visual street-level image database construction by the internet giants such as Google and Microsoft have made the further rapid development of this technology. As {{one of the most important}} sensors, the omni-directional cameras are being commonly utilized on many MMSs to collect panoramic images for 3 D close range photogrammetry and fusion with 3 D laser point clouds since these cameras could record much visual information of the real environment in one image at field view angle of 360 ° in longitude direction and 180 ° in latitude direction. This paper addresses the problem of panoramic epipolar image generation for 3 D modelling and mapping by stereoscopic viewing. These panoramic images are captured with Point Grey’s Ladybug 3 mounted on the top of Mitsubishi MMS-X 220 at 2 m intervals along the streets in urban environment. Onboard GPS/IMU, speedometer and post sequence image analysis technology such as bundle adjustment provided high accuracy position and attitude data for these panoramic images and laser data, this makes it possible to construct the epipolar geometric relationship between any two adjacent panoramic images and then the panoramic epipolar images could be generated. Three kinds of projection planes: sphere, cylinder and flat plane are selected as the <b>epipolar</b> images’ <b>planes.</b> In final we select the flat plane and use its effective parts (middle parts of base line’s two sides) for epipolar image generation. The corresponding geometric relations and results will be presented in this paper...|$|R
40|$|This paper {{presents}} a low-cost 3 D surface scanner, {{composed of two}} fixed web cameras and a hand-held planar laser beam. Setup pre-calibration provides interior orientations of the cameras and their scaled relative orientation. Our calibration algorithm, based on bundle adjustment, uses image pairs of a chessboard, whose nodes are identified automatically and referred to the ground points. For scanning, synchronized image pairs are continuously recorded from each location of the static cameras as the laser source is slowly moved by hand; each pair thus records {{a profile of the}} 3 D surface intersected by the laser <b>plane.</b> <b>Epipolar</b> resampling reduces the search for point correspondences to finding the intersections of homologous epipolar lines with the recorded laser profile. After a smoothing operation, peaks are identified as the maxima of Gaussian curves fitted to the gray-value data along the epipolar lines; the final identification of peaks involves information from the neighbourhood of the initial estimation. An innovative aspect is that the photogrammetric triangulation of 3 D points gains in robustness by enforcing extra geometric constraints. Thus, all points of a profile must lie on a laser plane, whose coefficients are involved as unknowns in the 3 D reconstruction adjustment. This allows identifying blunders in peak detection; for epipolar lines with more peaks, only points which, when reconstructed, satisfy a distance threshold from the laser plane participate in the final 3 D data set. Furthermore, the object is placed in a corner (the equations of its two planes in the setup system are found automatically by prior scanning), which is intersected by the laser plane in two lines. Their points are identified and constrained to simultaneously satisfy both the corresponding plane equation and the equation of the laser plane. Usin...|$|R
30|$|From the model, a {{plane is}} formed when Ol, W, and Or are connected. This plane {{is called the}} <b>epipolar</b> <b>plane.</b> If we know wl, we can find wr by searching along a line lr[*]=[*]er[*]×[*]wr. This line is called the epipolar line. From the epipolar line, lr[*]=[*]er[*]×[*]wr[*]=[*][er][*]×[*]wr, where [er] is the cross product, and {{because we know that}} wr is mapping to wl, we get the {{relation}} wr[*]=[*]H wl. H is a 3 [*]×[*] 3 homography matrix of rank 3 that describes the mapping between two points. By combining both equations, we get lr[*]=[*][er][*]×[*]H wl[*]=[*]F wl, where F[*]=[*][er][*]×[*]H and is called the fundamental matrix [21].|$|E
40|$|This paper {{proposes a}} novel notion of <b>Epipolar</b> <b>Plane</b> Range Image (EPRI). A line-scanning laser range sensor {{is mounted on}} the side of our data {{acquisition}} vehicle, repeating drawing horizontal scanning lines. Laminating the line range data along time axis, we can follow temporal continuity of horizontal cross section of the geometry seen from the scanner, hence can estimate the motion of the vehicle. Applying these information to another vertical-line-scanning range data we can align the position of the scanning lines, and can efficiently get correct 3 D geometric model of the real urban space, even if the vehicle travels in arbitrary speed without any external devices as GPS...|$|E
40|$|International audienceThis paper {{presents}} {{a novel approach}} for light field editing. The problem of propagating an edit from a single view to the remaining light field is solved by a structure tensor driven diffusion on the <b>epipolar</b> <b>plane</b> images. The proposed method is shown to be useful for two applications: light field inpainting and recolorization. While the light field recolorization is obtained with a straightforward diffusion, the inpainting application is particularly challenging, as the structure tensors accounting for disparities are unknown under the occluding mask. We address this issue with a disparity inpainting {{by means of an}} interpolation constrained by superpixel boundaries. Results on synthetic and real light field images demonstrate the effectiveness of the proposed method...|$|E
40|$|Abstract. While multi-view stereo {{reconstruction}} of Lambertian sur-faces is nowadays highly robust, reconstruction methods based on corre-spondence search usually fail {{in the presence}} of ambiguous information, like in the case of partially reflecting and transparent surfaces. On the <b>epipolar</b> <b>plane</b> images of a 4 D light field, however, surfaces like these give rise to overlaid patterns of oriented lines. We show that these can be identified and analyzed quickly and accurately with higher order struc-ture tensors. The resulting method can reconstruct with high precision both the geometry of the surface as well as the geometry of the reflected or transmitted object. Accuracy and feasibility are shown on both ray-traced synthetic scenes and real-world data recorded by our gantry. ...|$|E
40|$|This paper {{presents}} a systematic approach to automatically construct the 3 D natural scene from video sequences. The dense layered depth maps {{are derived from}} image sequences captured by a vibrated camera with only approximately known motion. The approach consists of (1) image stabilization by motion filtering and (2) depth estimation by spatio-temporal texture analysis. The two stage method not only generalized the so called panoramic image method and <b>epipolar</b> <b>plane</b> image method to handle the image sequence vibrations due to the un-controllable fluctuation of the camera, but also bypasses the feature extraction and matching problems encountered in stereo or visual motion. Our approach allows automatic modeling of the real environment {{for inclusion in the}} VR representation...|$|E
30|$|The {{calibration}} {{process in}} a stereo vision system consists of calculating {{the parameters of the}} system both internal and external, such as the pixel size, focal length, and image size. External parameters define the orientation and position of the cameras in 3 D space. In an orthogonal stereo system or fixed system, the calibration is well defined using Zhang’s calibration algorithm [7]. The output of the calibration is used in the rectification process. Rectification is used to transform the left and right images to be parallel to the <b>epipolar</b> <b>plane</b> and co-linear to the baseline [8]. This transformation simplifies the next process, which is the correspondence, where the search across the scanning line becomes 1 D instead of 2 D.|$|E
40|$|We {{present an}} {{algorithm}} for finding correspondences in <b>epipolar</b> <b>plane</b> images (EPIs). An EPI is a 2 -dimensional spatio-temporal image {{obtained from a}} dense image sequence that is rectified so that each scene point is projected to the same row in all frames. Scenes with opaque Lambertian surfaces without occlusions are assumed. The approach is based on finding lines with similar intensities in an EPI for each image row separately, by dynamic programming. We focus on the correspondence accuracy. The high accuracy is enabled by a wide base line as in a stereo and by more data available. However, the matching is easier than in stereo as the displacement between neighboring frames is very small. No feature extraction is used, the algorithm is purely signal-based...|$|E
