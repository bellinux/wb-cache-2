97|341|Public
2500|$|Saccadic {{suppression}} {{is responsible}} for maintaining a continuous, stable, visual world by reducing visual sensitivity to events occurring before, during and after a saccade. The more complex the background on which the stimulus is presented, the larger the suppression. The increase in saccadic suppression {{can lead to a}} decrease in detection of change in the visual field. Saccadic suppression can be linked to the phenomenon of change blindness, in which individuals lack the ability to detect small or large changes within an environment without the aid of directed attention. [...] There are two types of saccadic suppression, with the first concerning the detection of light spot flashes during saccades. The lower the spatial frequency, meaning less objects in the visual field, of the light flashes the stronger the saccadic suppression. With a less number of items in the visual field it is easier for saccadic suppression to occur. The higher the spatial frequency, which involves more complex objects within the visual field, the less likely there will be saccadic suppression. The second type concerns the detection of <b>image</b> <b>displacement</b> or a change in an image during eye movement. A displacement between saccades would not be detected because the movement of the eye eliminates the signal for the detection of the displacement. The location of the target forms an image feature that is used by the visual system to establish space constancy, which maintains the location of an object in space. Target blanking is used to study visual stability, space constancy and the content of information transferred across saccades. Blanking a target after a saccade eliminates an attempt to make a second saccade, resulting in the visual system failing to find the target information. Stability and constancy across the saccades is broken, as a result, image displacements become easier to detect when blanking occurs during or right after the saccade.|$|E
5000|$|... #Caption: Diagram {{describing}} {{relationship of}} <b>image</b> <b>displacement</b> to depth with stereoscopic images, assuming flat co-planar images.|$|E
50|$|Saccadic {{suppression}} of <b>image</b> <b>displacement</b> (SSID), is the phenomenon in visual perception where the brain selectively blocks visual processing during eye movements {{in such a}} way that large changes in object location in the visual scene during a saccade or blink are not detected.|$|E
40|$|This paper {{examines}} the robustness of correspondence-based approaches to structure from motion. Unlike earlier studies {{it is proven}} in an algorithm-independent way, that small absolute errors in <b>image</b> <b>displacements</b> cause absolute errors in rotational motion parameters significant enough to lead to large relative errors in the determination of environmental depth. Even if the motion parameters are known exactly, through sophisticated navigation systems, small errors in <b>image</b> <b>displacements</b> still lead to large errors in depth for environmental points whose distance from the camera is greater than a few multiples of the total translation in depth of the camera...|$|R
40|$|In this paper, {{we propose}} a novel method for {{measuring}} the coma aberrations of lithographic projection optics based on relative <b>image</b> <b>displacements</b> at multiple illumination settings. The measurement accuracy of coma can be improved because the phase-shifting gratings are {{more sensitive to the}} aberrations than the binary gratings used in the TAMIS technique, and the impact of distortion on <b>displacements</b> of aerial <b>image</b> can be eliminated when the relative <b>image</b> <b>displacements</b> are measured. The PROLITH simulation results show that, the measurement accuracy of coma increases by more than 25 % under conventional illumination, and the measurement accuracy of primary coma increases by more than 20 % under annular illumination, compared with the TAMIS technique. (c) 2007 Optical Society of America...|$|R
40|$|This paper {{examines}} the robustness of correspondence-based approaches to structure from motion. Unlike earJier studies {{it is proven}} in an algorithmindependent way, that small absolute errors in <b>image</b> <b>displacements</b> cause absolute errors in rotational motion parameters significant eno ugh to lead to large relative errors in the determination of environmental depth. Even if the exact motion parameters are known a priori small errors in <b>image</b> <b>displacements</b> still lead to large errors in depth for environmental points whose distance from the camera is greater than a few multiples of the total trans-]ation in depth of the camera. In order to clarify the many issues on robustness that are raised in this paper, a new depth determination algorithm is developed and applied to dynamic image sequences of natural outdoor scen e...|$|R
50|$|Using {{the device}} pictured on the right, Ziat et al. (2010) {{demonstrated}} a phenomenon {{akin to the}} saccadic suppression of <b>image</b> <b>displacement</b> (Bridgeman et al., 1975) in the tactile system. Under certain conditions participants failed to detect that dots had changed location as they moved their fingers over the tactile display.|$|E
5000|$|When humans {{turn the}} head {{from left to}} right, the image {{projected}} on the retinas moves in the direction opposite to the head movement. Without the head turning, such an <b>image</b> <b>displacement</b> would appear as something moving; but when it is correlated with the turning of the head, no movement of the environment is seen. However, what if the image were to move in coordination with the head movement, but the extent of that movement were less (or more) than would be usual for the head movement in question? Would the anomaly be noticed? ...|$|E
50|$|In-body image {{stabilization}} requires the lens {{to have a}} larger output image circle because the sensor is moved during exposure and thus uses a larger part of the image. Compared to lens movements in optical {{image stabilization}} systems the sensor movements are quite large, so the effectiveness {{is limited by the}} maximum range of sensor movement, where a typical modern optically stabilized lens has greater freedom. The required sensor movement (both speed and range) increase with the focal length of the lens being used, making sensor-shift technology less suited for very long telephoto lenses, especially when using slower shutter speeds because the available motion range of the sensor quickly becomes insufficient to cope with the increasing <b>image</b> <b>displacement.</b>|$|E
5000|$|<b>Image</b> {{centroid}} <b>displacements</b> due to microlensing by the Ellis wormhole ...|$|R
5000|$|... == Affine {{homography}} == When {{the image}} region {{in which the}} homography is computed is small or the image has been acquired with a large focal length, an affine homography is a more appropriate model of <b>image</b> <b>displacements.</b> An affine homography is a special type of a general homography whose last row is fixed to ...|$|R
50|$|Sequences of ordered images {{allow the}} {{estimation}} of motion as either instantaneous image velocities or discrete <b>image</b> <b>displacements.</b> Fleet and Weiss provide a tutorial introduction to gradient based optical flow.John L. Barron, David J. Fleet, and Steven Beauchemin provide a performance analysis {{of a number of}} optical flow techniques. It emphasizes the accuracy and density of measurements.|$|R
5000|$|A saccade is a fast eye motion, {{and because}} it is a motion that is {{optimised}} for speed, there is inevitable blurring of the image on the retina, as the retina is sweeping the visual field. Blurred retinal images are not of much use, and the eye has a mechanism that [...] "cuts off" [...] the processing of retinal images when it becomes blurred. Humans become effectively blind during a saccade. This phenomenon is called saccadic masking or saccadic suppression. There are two major types of saccadic masking, flash suppression (the inability to see a flash of light during a saccade) and saccadic suppression of <b>image</b> <b>displacement</b> (characterized by the inability to perceive whether a target has moved or not during a saccade).|$|E
5000|$|E. fasciata is {{a highly}} {{successful}} ambush predator. In the course of evolution, it has specialized in preying on fast flying insects, such as flies and bees. One reason for this preference may be that flying insects serve as nutritious food, which {{is important in the}} spring when there is a limited food supply. Adult females often perch on flowers, where they wait to prey on honeybees. Insect prey can be captured upon landing, or even during flight, due to the fast strike of E. fasciata and its ability to rotate its head and the two powerful raptorial forelegs more than 90Â° laterally, without moving the rest of its body. E. fasciata shows no evidence of being cannibalistic. Distinct rocking and jerking movements are executed, which not only serve as camouflage in moving vegetation, but also facilitate spatial vision with the aid of motion parallax or retinal <b>image</b> <b>displacement.</b>|$|E
5000|$|Wallach and Kravitz {{devised a}} {{mechanical}} apparatus that enabled head movements to cause displacements {{of an image}} by any desired percentage {{of the extent of}} that head movement, and discovered that subjects could detect deviations of as little as 2% from the normal degree of displacement. This showed that a highly accurate compensating process corrects for the <b>image</b> <b>displacement</b> that normally accompanies a head movement, thus yielding an appearance of stability. Wallach called this process constancy of visual direction (CVD), and he noted with interest that it could be easily modified through perceptual adaptation. To demonstrate this, Wallach & Kravitz set the apparatus so that during head movements the visual image moved by 150% of what would be the normal displacement, and had subjects turn their heads back and forth watching this altered displacement for 10 minutes. After this brief adaptation period, subjects were shown an objectively stationary target as they turned their heads. They reported that it no longer appeared motionless, but swung {{back and forth in the}} direction opposite the movements that had occurred during adaptation, In order to make the target appear stationary, the apparatus had to be set so that the target actually moved by about 14% in the same direction it had moved during the adaptation period. The CVD process that correlates head movements and image shifts had been modified by exposure to an abnormal stimulus condition. (The adaptation of the CVD process was temporary and dissipated after a few minutes.) ...|$|E
40|$|As a {{critical}} dimension shrinks, the degradation in image quality caused by wavefront aberrations of projection optics in lithographic tools becomes a serious problem. It {{is necessary to}} establish a technique for a fast and accurate in situ aberration measurement. We introduce what {{we believe to be}} a novel technique for characterizing the aberrations of projection optics by using an alternating phase-shifting mask. The even aberrations, such as spherical aberration and astigmatism, and the odd aberrations, such as coma, are extracted from focus shifts and <b>image</b> <b>displacements</b> of the phase-shifted pattern, respectively. The focus shifts and the <b>image</b> <b>displacements</b> are measured by a transmission image sensor. The simulation results show that, compared with the accuracy of the previous straightforward measurement technique, the accuracy of the coma measurement increases by more than 30 % and the accuracy of the spherical-aberration measurement increases by approximately 20 %. (c) 2006 Optical Society of America...|$|R
40|$|Subjects were {{required}} to perceptually judge the location of flash targets presented {{at the time of}} a saccade at various positions scattered two-dimensionally on a dimly illuminated structured background. The saccade-contingent n&localization was shown only in the direction parallel to the saccade, and not in the direction perpendicular to the saccade. In addition, the mislocalization uuder the âilluminated background â condition was different in several respects from that observed when targets were presented in the dark. It was suggested that the mislocalization is successfully explained by assuming three physiological and cognitive processes: a sluggish activity of the extraretinal eye position signal, visual cues from the visible background, and selective inattention to <b>image</b> <b>displacements.</b> Saccade Visual localization Visual stability Eye position signal Under normal illumination, <b>image</b> <b>displacements</b> on the retina, which are caused by saccadic eye movements, do not bring about an apparent displacement of the corresponding perceived object. A predominant explanation for this visual stability, first suggested by Helmholt...|$|R
40|$|This paper {{addresses}} {{the problem of}} fully-automatic localization and segmentation of 3 D intervertebral discs (IVDs) from MR images. Our method contains two steps, where we first localize {{the center of each}} IVD, and then segment IVDs by classifying image pixels around each disc center as foreground (disc) or background. The disc localization is done by estimating the <b>image</b> <b>displacements</b> from a set of randomly sampled 3 D image patches to the disc center. The <b>image</b> <b>displacements</b> are estimated by jointly optimizing the training and test displacement values in a data-driven way, where we take into consideration both the training data and the geometric constraint on the test image. After the disc centers are localized, we segment the discs by classifying image pixels around disc centers as background or foreground. The classification is done in a similar data-driven approach as we used for localization, but in this segmentation case we are aiming to estimate the foreground/background probability of each pixel instead of the <b>image</b> <b>displacements.</b> In addition, an extra neighborhood smooth constraint is introduced to enforce the local smoothness of the label field. Our method is validated on 3 D T 2 -weighted turbo spin echo MR images of 35 patients from two different studies. Experiments show that compared to state of the art, our method achieves better or comparable results. Specifically, we achieve for localization a mean error of 1. 6 - 2. 0 mm, and for segmentation a mean Dice metric of 85 %- 88 % and a mean surface distance of 1. 3 - 1. 4 mm...|$|R
5000|$|Saccadic {{suppression}} {{is responsible}} for maintaining a continuous, stable, visual world by reducing visual sensitivity to events occurring before, during and after a saccade. The more complex the background on which the stimulus is presented, the larger the suppression. The increase in saccadic suppression {{can lead to a}} decrease in detection of change in the visual field. Saccadic suppression can be linked to the phenomenon of change blindness, in which individuals lack the ability to detect small or large changes within an environment without the aid of directed attention. [...] There are two types of saccadic suppression, with the first concerning the detection of light spot flashes during saccades. The lower the spatial frequency, meaning less objects in the visual field, of the light flashes the stronger the saccadic suppression. With a less number of items in the visual field it is easier for saccadic suppression to occur. The higher the spatial frequency, which involves more complex objects within the visual field, the less likely there will be saccadic suppression. The second type concerns the detection of <b>image</b> <b>displacement</b> or a change in an image during eye movement. A displacement between saccades would not be detected because the movement of the eye eliminates the signal for the detection of the displacement. The location of the target forms an image feature that is used by the visual system to establish space constancy, which maintains the location of an object in space. Target blanking is used to study visual stability, space constancy and the content of information transferred across saccades. Blanking a target after a saccade eliminates an attempt to make a second saccade, resulting in the visual system failing to find the target information. Stability and constancy across the saccades is broken, as a result, image displacements become easier to detect when blanking occurs during or right after the saccade.|$|E
40|$|When the eye {{changes its}} position, {{the image of}} the scene moves across the retina. Despite such an obvious <b>image</b> <b>displacement,</b> the visual world appears stable in daily life. In the case of saccadic eye movements, the visual system is thought to use extra-retinal signals {{accompanying}} saccades to maintain perceptual stability (Morrone an...|$|E
40|$|Measurement {{requirements}} and candidates for measuring crystal growth in space are described, emphasizing holographic instrumentation. Existing instrumentation {{planned for the}} IML- 1 Spaceflight is described along with advanced concepts for future application which incorporate diode lasers, fiber optics, and holographic optical elements. Particle <b>image</b> <b>displacement</b> velocimetry in crystal growth chambers is described...|$|E
40|$|Fundus images provide high {{optical gain}} for eye {{movement}} tracking, i. e. large <b>image</b> <b>displacements</b> {{occur as a}} result of small eye rotations. Subpixel registration techniques can provide resolution better than 1 arc minute using images acquired with a CCD camera. Ocular torsion may also be estimated, with a precision of approximately 0. 1 degree. This talk will discuss the software algorithms used to attain this performance...|$|R
50|$|In {{addition}} to the processes compensating for <b>image</b> <b>displacements</b> during head rotation, Wallach and various collaborators examined other sorts of compensations related to perceptual stability during bodily movement, including displacements caused by nodding and by eye movements, the changing orientation of objects as one walks past, optical expansion caused by moving forward, displacement in a dimension unrelated to the physical movement, and movement-correlated alterations in form perception.|$|R
40|$|Context. To {{evaluate}} {{site quality}} {{and to develop}} multi-conjugative adaptive optics systems for future large solar telescopes, characterization of contributions to seeing from heights up to at least 12 km above the telescope is needed. Aims. We describe a method for evaluating contributions to seeing from different layers along the line-of-sight to the Sun. The method is based on Shack Hartmann wavefront sensor data recorded over a large field-of-view with solar granulation and uses only measurements of differential <b>image</b> <b>displacements</b> from individual exposures, such that the measurements are not degraded by residual tip-tilt errors. Methods. The covariance of differential <b>image</b> <b>displacements</b> at variable field angles provides {{a natural extension of}} the work of Sarazin and Roddier to include measurements that are also sensitive to the height distribution of seeing. By extending the numerical calculations of Fried to include differential <b>image</b> <b>displacements</b> at distances much smaller and much larger than the subaperture diameter, the wavefront sensor data can be fitted to a well-defined model of seeing. The resulting least-squares fit problem can be solved with conventional methods. The method is tested with simple simulations and applied to wavefront data from the Swedish 1 -m Solar Telescope on La Palma, Spain. Results. We show that good inversions are possible with 9 â 10 layers, three of which are within the first 1. 5 km, and a maximum distance of 16 â 30 km, but with poor height resolution in the range 10 â 30 km. Conclusions. We conclude that the proposed method allows good measurements when Fried's parameter r 0 is larger than about 7. 5 cm for the ground layer and that these measurements should provide valuable information for site selection and multi-conjugate development for the future European Solar Telescope. A major limitation is the large field of view presently used for wavefront sensing, leading to uncomfortably large uncertainties in r 0 at 30 km distance...|$|R
40|$|The first International Microgravity Laboratory (IML- 1), {{scheduled}} for spaceflight in early 1992 includes a crystal-growth-from-solution experiment which {{is equipped with}} an array of optical diagnostics instrumentation which includes transmission and reflection holography, tomography, schlieren, and particle <b>image</b> <b>displacement</b> velocimetry. During the course of preparation for this spaceflight experiment we have performed both experimentation and analysis for each of these diagnostics. In this paper we describe the work performed in the development of holographic particle <b>image</b> <b>displacement</b> velocimetry for microgravity application which will be employed primarily to observe and quantify minute convective currents in the Spacelab environment and also to measure the value of g. Additionally, the experiment offers a unique opportunity to examine physical phenomena which are normally negligible and not observable. A preliminary analysis of the motion of particles in fluid was performed and supporting experiments were carried out. The results of the analysis and the experiments are reported...|$|E
40|$|Abstract. <b>Image</b> <b>displacement</b> fieldsâoptical flow fields, stereo {{disparity}} fields, {{normal flow}} fieldsâdue to rigid motion possess a global geometric structure which {{is independent of}} the scene in view. Motion vectors of certain lengths and directions are constrained to lie on the imaging surface at particular loci whose location and form depends solely on the 3 D motion parameters. If optical flow fields or stereo disparity fields are considered, then equal vectors are shown to lie on conic sections. Similarly, for normal motion fields, equal vectors lie within regions whose boundaries also constitute conics. By studying various properties of these curves and regions and their relationships, a characterization {{of the structure of}} rigid motion fields is given. The goal {{of this paper is to}} introduce a concept underlying the global structure of <b>image</b> <b>displacement</b> fields. This concept gives rise to various constraints that could form the basis of algorithms for the recovery of visual information from multiple views. 1...|$|E
40|$|An {{apparatus}} {{is described}} for indicating the instantaneous angular deflection {{of an object}} about a selected axis without mechanical contact with the object. Light from a light source is transmitted through a flat refractor to a converging lens which focuses the light through another flat refractor onto a differential photocell. The first flat refractor {{is attached to the}} object such that when the object is deflected about the selected axis the refractor is also deflected about that axis. The two flat refractors are identical and they are placed an equal distance from the converging lens as are the light source and the photocell. The output of the photocell which is a function of <b>image</b> <b>displacement</b> is fed to a high gain amplifier that drives a galvanometer which rotates the second flat refractor. The second refractor is rotated so that the <b>image</b> <b>displacement</b> is very nearly zero making the galvanometer current a measure of the deflection of the object about the selected axis...|$|E
40|$|Abstract. Phase {{correlation}} {{techniques have}} been used in image registration to estimate <b>image</b> <b>displacements.</b> These techniques have been also used to estimate optical ow by applying it locally. In this work a di erent phase correlation-based method is proposed to deal with a deformation/translation motion model, instead of the pure translations that the basic phase correlation technique can estimate. Some experimentals results are also presented to show the accuracy of the motion paramenters estimated {{and the use of the}} phase correlation to estimate optical ow...|$|R
30|$|Hartmann spherical {{aberration}} automatic detection system consists of <b>image</b> measurement, <b>displacement</b> control, and measurement. The area array CCD {{is used as}} the image sensor for image measurement.|$|R
30|$|Optical motion {{analysis}} requires {{the estimation of}} the position and orientation (pose) of an object across image sequences. Through the identification of common object features in successive <b>images,</b> <b>displacement</b> data can be âtrackedâ over time. However, accurate quantification of whole-body pose can be a difficult problem to solve since {{the human body is}} an extremely complex, highly articulated, self-occluding and only partially rigid entity [8 â 10]. To make this process more tractable, the structure of the human body is usually simplified as a series of rigid bodies connected by frictionless rotational joints.|$|R
40|$|Also cross-refernced as CAR-TR- 732) 	<b>Image</b> <b>displacement</b> fieldsoptical flow fields, stereo {{disparity}} fields, {{normal flow}} fieldsdue to rigid motion possess a global geometric structure which {{is independent of}} the scene in view. Motion vectors of certain lengths and directions are constraine d to lie on the imaging surface at particular loci whose location and form depends solely on the 3 D motion parameters. If optical flow fields or stereo disparity fields are considered, then equal vectors are shown to lie on conic sections. Similarly, for normal motion fields, equal vectors lie within regions whose boundaries also constitute conics. By studying various properties of these curves and regions and their relationships, a characterization {{of the structure of}} rigid motion fields is given. The go al {{of this paper is to}} introduce a concept underlying the global structure of <b>image</b> <b>displacement</b> fields. This concept gives rise to various constraints that could form the basis of algorithms for the recovery of visual information from multiple views...|$|E
40|$|A {{previous}} investigation in our laboratory showed that virtual environment users rely on image velocity errors rather than <b>image</b> <b>displacement</b> errors to sense head-tracking latency. In that study, {{the effect of}} <b>image</b> <b>displacement</b> error may have been suppressed because its peak amplitudes were associated with higher head velocity, which is thought to suppress visual motion sensitivity. To determine whether diminished motion sensitivity could {{play a role in}} user discrimination of latency, we investigated whether subjective perception of image motion comparable to that caused by latency is impaired by concurrent horizontal or vertical head movement. Eight participants monocularly viewed a checkerboard pattern in a head-mounted display that was oscillated from side-to-side with half peak-to-peak amplitudes from 0 Â° to 5. 64 Â°. Perceptual sensitivity to image motion amplitude was reduced by almost half, during 30 Â° repetitive horizontal head movements at 0. 4 Hz. Accordingly, the reduced sensitivity to motion during head movement appears to be a phenomenon that modulates users â tolerance of erroneous image motion caused by virtual environment latency...|$|E
40|$|The time {{course of}} visual mislocalization {{produced}} by a rapid retinal <b>image</b> <b>displacement</b> was examined in moving-background and saccadic eye movement experiments. In both experiments, the target for localization task and its background scene were dichoptically presented: they were presented separately to the different eyes. The error curves of mislocalization shown in the dichoptic viewing condition {{were the same as}} those in monocular viewing (in the moving-background experiment) and binocular viewing conditions (in the saccadic eye movement experiment), indicating that in both experiments the neural interaction responsible for generating mislocalization took place at a site after the lateral geniculate nucleus in the visual system, not at the retinal level. Two possible explanations for mislocalization, one neurophysiological nd the other cognitive, were proposed. Furthermore, it was established that the error curves of mislocaHzation are substantially different between the moving-background and the saccadic eye movement experiments: in the saccadic eye movement experiment, the error curves changed with the actual target position, but not in the moving-background experiment. This was interpreted as showing that the basic mechanism for misiocalization is not the same between the two experimental situations. Visual localization Retinal <b>image</b> <b>displacement</b> Saccade Visual integration Dichoptic viewin...|$|E
40|$|In {{this paper}} we {{investigate}} physics-based plane beam model, frequently used in mechanical and civil engineering, to track large non-linear deformations in images. Such models do not only contribute to robust and precise tracking, {{in the presence}} of clutter and partial occlusions, but also allow to compute the forces that produce observed deformations. We verify the correctness of the recovered forces by using them in a simulation and compare the results to the original <b>image</b> <b>displacements.</b> We apply this method to track deformations of the pole vault, the rat whiskers and the car antenna...|$|R
50|$|The sensors {{are based}} on an image {{correlation}} method. The Sensor takes subsequent pictures from the surface being measured and compares the <b>images</b> for <b>displacement.</b> Resolutions down to 1 nm are possible.|$|R
40|$|A {{high-speed}} panoramic {{visual stimulation}} device is introduced which is suitable to analyse visual interneurons during stimulation with rapid <b>image</b> <b>displacements</b> as experienced by fast moving animals. The responses of an identified motion sensitive neuron {{in the visual}} system of the blowfly to behaviourally generated image sequences are very complex and hard to predict from the established input circuitry of the neuron. This finding suggests that the computational significance of visual interneurons can only be assessed if they are characterised not only by conventional stimuli as are often used for systems analysis, but also by behaviourally relevant input. ...|$|R
