116|1|Public
25|$|The <b>keyspace</b> {{partitioning}} and {{overlay network}} components are described below {{with the goal}} of capturing the principal ideas common to most DHTs; many designs differ in the details.|$|E
25|$|Both {{consistent}} hashing and rendezvous hashing {{have the}} essential property that removal or addition of one node changes only {{the set of}} keys owned by the nodes with adjacent IDs, and leaves all other nodes unaffected. Contrast this with a traditional hash table in which addition or removal of one bucket causes nearly the entire <b>keyspace</b> to be remapped. Since any change in ownership typically corresponds to bandwidth-intensive movement of objects stored in the DHT from one node to another, minimizing such reorganization is required to efficiently support high rates of churn (node arrival and failure).|$|E
2500|$|The {{structure}} of a DHT can be decomposed into several main components. [...] The foundation is an abstract <b>keyspace,</b> such as the set of 160-bit strings. A <b>keyspace</b> partitioning scheme splits ownership of this <b>keyspace</b> among the participating nodes. An overlay network then connects the nodes, allowing them to find the owner of any given key in the <b>keyspace.</b>|$|E
50|$|The 3-subset variant relaxes the {{restriction}} for the key-spaces to be independent, {{by moving the}} intersecting parts of the <b>keyspaces</b> into a subset, which contains the keybits common between the two key-spaces.|$|R
2500|$|Once these {{components}} are in place, a typical {{use of the}} DHT for storage and retrieval might proceed as follows. Suppose the <b>keyspace</b> is the set of 160-bit strings. To index a file with given [...] and [...] in the DHT, the SHA-1 hash of [...] is generated, producing a 160-bit key , and a message [...] is sent to any node participating in the DHT. The message is forwarded from node to node through the overlay network until it reaches the single node responsible for key [...] as specified by the <b>keyspace</b> partitioning. That node then stores the key and the data. Any other client can then retrieve {{the contents of the}} file by again hashing [...] to produce [...] and asking any DHT node to find the data associated with [...] with a message [...] The message will again be routed through the overlay to the node responsible for , which will reply with the stored [...]|$|E
2500|$|Redundancy can {{be added}} to improve reliability. [...] The [...] key pair can be stored in more than one node {{corresponding}} to the key. Usually, rather than selecting just one node, real world DHT algorithms select [...] suitable nodes, with [...] being an implementation-specific parameter of the DHT. In some DHT designs, nodes agree to handle a certain <b>keyspace</b> range, the size of which may be chosen dynamically, rather than hard-coded.|$|E
2500|$|For example, the Chord DHT uses {{consistent}} hashing, which treats keys as {{points on}} a circle, and [...] is the distance traveling clockwise {{around the circle}} from [...] to [...] Thus, the circular <b>keyspace</b> is split into contiguous segments whose endpoints are the node identifiers. If [...] and [...] are two adjacent IDs, with a shorter clockwise distance from [...] to , then the node with ID [...] owns all the keys that fall between [...] and [...]|$|E
2500|$|All DHT topologies {{share some}} {{variant of the}} most {{essential}} property: for any key , each node either has a node ID that owns [...] or has a link to a node whose node ID is closer to , {{in terms of the}} <b>keyspace</b> distance defined above. It is then easy to route a message to the owner of any key [...] using the following greedy algorithm (that is not necessarily globally optimal): at each step, forward the message to the neighbor whose ID is closest to [...] When there is no such neighbor, then we must have arrived at the closest node, which is the owner of [...] as defined above. This style of routing is sometimes called key-based routing.|$|E
2500|$|Some {{advanced}} DHTs like Kademlia perform iterative lookups {{through the}} DHT first {{in order to}} select a set of suitable nodes and send [...] messages only to those nodes, thus drastically reducing useless traffic, since published messages are only sent to nodes that seem suitable for storing the key and iterative lookups cover just a small set of nodes rather than the entire DHT, reducing useless forwarding. In such DHTs, forwarding of [...] messages may only occur {{as part of a}} self-healing algorithm: if a target node receives a [...] message, but believes that [...] is out of its handled range and a closer node (in terms of DHT <b>keyspace)</b> is known, the message is forwarded to that node. Otherwise, data are indexed locally. This leads to a somewhat self-balancing DHT behavior. Of course, such an algorithm requires nodes to publish their presence data in the DHT so the iterative lookups can be performed.|$|E
5000|$|The {{structure}} of a DHT can be decomposed into several main components. [...] The foundation is an abstract <b>keyspace,</b> such as the set of 160-bit strings. A <b>keyspace</b> partitioning scheme splits ownership of this <b>keyspace</b> among the participating nodes. An overlay network then connects the nodes, allowing them to find the owner of any given key in the <b>keyspace.</b>|$|E
50|$|A <b>keyspace</b> {{may contain}} column {{families}} or super columns. Each super column contains {{one or more}} column family, each column family contains at least one column. The <b>keyspace</b> is the highest abstraction in a distributed data store.|$|E
50|$|A <b>keyspace</b> (or key space) in a NoSQL {{data store}} is {{an object that}} holds {{together}} all column families of a design. It is the outermost grouping of the data in the data store. It resembles the schema concept in Relational database management systems. Generally, there is one <b>keyspace</b> per application.|$|E
5000|$|... #Caption: A <b>keyspace</b> example with {{a number}} of column families.|$|E
50|$|Furthermore, even if {{dedicated}} hardware or FPGA implementations made {{it possible}} to test the required 2.35 trillion keys per second needed to scan the entire <b>keyspace</b> in two minutes, the key could simply be replaced more often. Furthermore, the parity bytes could be replaced by real key bytes, increasing the <b>keyspace</b> to 64 bits, which is 65,536 times as large.|$|E
5000|$|Below {{an example}} of <b>keyspace</b> creation, {{including}} a column family in CQL 3.0:CREATE <b>KEYSPACE</b> MyKeySpace WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 };USE MyKeySpace;CREATE COLUMNFAMILY MyColumns (id text, Last text, First text, PRIMARY KEY(id));INSERT INTO MyColumns (id, Last, First) VALUES ('1', 'Doe', 'John');SELECT * FROM MyColumns;Which gives: id | first | last----+-------+------ 1 | John | Doe(1 rows) ...|$|E
5000|$|Hashcat offers {{multiple}} attack modes {{for obtaining}} effective and complex coverage over a hash's <b>keyspace.</b> These modes are: ...|$|E
50|$|However, 48 bits, even if small by today's standards, is a {{significant}} amount of <b>keyspace</b> to search through. For most practical applications, one would want to break the key faster than it is changed, and as the key changes at a minimum of every 120 seconds, this would require scanning through on average at least half the <b>keyspace</b> in that period of time. As an implementation taking 1 µs for each try (i.e., testing a million keys per second; about what libdvbcsa can do on a single core of a modern x86 processor) would require 8.9 years to scan the entire <b>keyspace,</b> this makes a brute force approach impractical for decrypting the data in real time, even with a highly parallel implementation.|$|E
50|$|A super {{column is}} part of a <b>keyspace</b> (data model) {{together}} with other super columns and column families, and columns.|$|E
5000|$|A {{high-level}} {{object oriented}} interface to Cassandra: It is mainly {{inspired by the}} Cassandra-java-client. The API is defined in the <b>Keyspace</b> interface.|$|E
5000|$|Brute-force <b>keyspace</b> search {{has broken}} some {{real-world}} ciphers and applications, including single-DES (see EFF DES cracker), 40-bit [...] "export-strength" [...] cryptography, and the DVD Content Scrambling System.|$|E
50|$|The <b>keyspace</b> {{partitioning}} and {{overlay network}} components are described below {{with the goal}} of capturing the principal ideas common to most DHTs; many designs differ in the details.|$|E
5000|$|Higher order {{correlation}} attacks can be {{more powerful}} than single order correlation attacks, however this effect is subject to a [...] "law of limiting returns". The table below shows {{a measure of the}} computational cost for various attacks on a keystream generator consisting of eight 8-bit LFSRs combined by a single Boolean function. Understanding the calculation of cost is relatively straightforward: the leftmost term of the sum represents the size of the <b>keyspace</b> for the correlated generators, and the rightmost term represents the size of the <b>keyspace</b> for the remaining generators.|$|E
5000|$|A {{column of}} a {{distributed}} data store is a NoSQL {{object of the}} lowest level in a <b>keyspace.</b> It is a tuple (a key-value pair) consisting of three elements: ...|$|E
5000|$|As an example, an {{implementation}} of a simple distributed hashtable over Cassandra is listed. /** * Insert a new value keyed by key * @param key Key for the value * @param value the String value to insert */ public void insert(final String key, final String value) throws Exception { execute(new Command (...) { public Void execute(final <b>Keyspace</b> ks) throws Exception { ks.insert(key, createColumnPath(COLUMN_NAME), bytes(value)); return null; } }); } /** * Get a string value. * @return The string value; null if no value exists for the given key. */ public String get(final String key) throws Exception { return execute(new Command (...) { public String execute(final <b>Keyspace</b> ks) throws Exception { try { return string(ks.getColumn(key, createColumnPath(COLUMN_NAME)).getValue (...) [...] ); } catch (NotFoundException e) { return null; } } }); } /** * Delete a key from cassandra */ public void delete(final String key) throws Exception { execute(new Command (...) { public Void execute(final <b>Keyspace</b> ks) throws Exception { ks.remove(key, createColumnPath(COLUMN_NAME)); return null; } }); } ...|$|E
50|$|Another {{very simple}} {{implementation}} technique, usable when the keys {{are restricted to}} a narrow range of integers, is direct addressing into an array: the value for a given key k is stored at the array cell Ak, or {{if there is no}} binding for k then the cell stores a special sentinel value that indicates the absence of a binding. As well as being simple, this technique is fast: each dictionary operation takes constant time. However, the space requirement for this structure is the size of the entire <b>keyspace,</b> making it impractical unless the <b>keyspace</b> is small.|$|E
50|$|Because Bitcask keeps all keys {{in memory}} at all times, the system must have enough memory {{to contain the}} entire <b>keyspace</b> in {{addition}} to other operational components and the operating system's file-system buffers.|$|E
5000|$|When {{the number}} of weak keys {{is known to be}} very small (in {{comparison}} {{to the size of the}} <b>keyspace),</b> generating a key uniformly at random ensures that the probability of it being weak is a (known) very small number.|$|E
5000|$|The <b>keyspace</b> {{has similar}} {{importance}} like a schema has in a database. In {{contrast to the}} schema, however, it does not stipulate any concrete structure, like it is known in the entity-relationship model used widely in the relational data models. For instance, {{the contents of the}} <b>keyspace</b> can be column families, each having different number of columns, or even different columns. So, the column families that somehow relate to the row concept in relational databases do not stipulate any fixed structure. The only point that is the same with a schema is that it also contains a number of [...] "objects", which are tables in RDBMS systems and here column families or super columns.|$|E
50|$|PAST is a {{distributed}} {{file system}} layered {{on top of}} Pastry. A file is stored into the system by computing the hash of its filename. Then Pastry routes {{the contents of the}} file to the node in the circular <b>keyspace</b> closest to the hash obtained from the filename. This node will then send copies of the file to the k nodes nearest the actual key, most of which are likely to be leaf nodes of this node and thus directly reachable. Retrieval of data is accomplished by rehashing the file name and routing a request for the data over Pastry to the proper place in the <b>keyspace.</b> The request can be fulfilled by any of the k nodes that have copies of the data. This accomplishes both data redundancy and load distribution. Since adjacent nodes in the <b>keyspace</b> are geographically diverse the odds that all k of them will go offline at the same time is very small. More importantly, since the Pastry routing protocol seeks to minimize the distance traveled, the nearest node to the machine that made the request (according to the metric) {{is likely to be the}} one that responds with the data.|$|E
5000|$|As an example, we show {{a number}} of column {{families}} in a <b>keyspace.</b> The [...] keyword defines how the column comparison is made. In the example, the UTF-8 standard has been selected. Other ways of comparison exist, such as , , , [...] [...] 0.01 ...|$|E
50|$|If a key {{were eight}} bits (one byte) long, the <b>keyspace</b> {{would consist of}} 28 or 256 {{possible}} keys. Advanced Encryption Standard (AES) can use a symmetric key of 256 bits, resulting in a key space containing 2256 (or 1.1579 × 1077) possible keys.|$|E
50|$|The {{consistent}} hashing concept {{also applies}} {{to the design of}} distributed hash tables (DHTs). DHTs use consistent hashing to partition a <b>keyspace</b> among a distributed set of nodes, and additionally provide an overlay network that connects nodes such that the node responsible for any key can be efficiently located.|$|E
50|$|To {{mitigate}} {{this problem}} for IPv6, RFC 4193 specifies a large (40-bit, {{which has a}} <b>keyspace</b> of about a trillion) unique Global ID to be pseudo-randomly generated by each organization using Unique Global Addresses. It is very unlikely that two network addresses generated in this way will be the same.|$|E
5000|$|This {{searches}} the entire 56-bit DES <b>keyspace</b> and returns [...] "1" [...] if it probably finds a matching key. In practice, several plaintexts {{are required to}} confirm the key, as two different keys can result {{in one or more}} matching plaintext-ciphertext pairs. If no key is found, it returns 0.|$|E
5000|$|The goal {{of having}} a 'flat' <b>keyspace</b> (i.e., all keys equally strong) is always a cipher design goal. As {{in the case of}} DES, {{sometimes}} a small number of weak keys is acceptable, provided that they are all identified or identifiable. An algorithm that has unknown weak keys does not inspire much trust [...]|$|E
5000|$|The {{conclusion}} of the paper describing the project was [...] "We have demonstrated that a brute-force search of DES <b>keyspace</b> is not only possible, but is also becoming practical for even modestly funded groups. RSA's prize for the find was US$10,000; {{it is safe to}} say that DES is inadequate for protecting data of any greater value." ...|$|E
