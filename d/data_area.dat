186|6910|Public
25|$|Because {{these first}} two FAT entries store special values, {{there are no}} data {{clusters}} 0 or 1. The first data cluster (after the root directory if FAT12/FAT16) is cluster 2, and cluster 2 is by definition the first cluster in the <b>data</b> <b>area.</b>|$|E
2500|$|A volume's <b>data</b> <b>area</b> {{is divided}} up into identically sized clusters, small blocks of {{contiguous}} space. Cluster sizes {{vary depending on}} the type of FAT file system being used and the size of the partition; typically cluster sizes lie somewhere between [...] and [...]|$|E
2500|$|To load a system, a 24-byte <b>data</b> <b>area</b> {{is loaded}} into main storage {{from the first}} block of the {{selected}} IPL device at location 0 and {{the second and third}} 8 byte data areas, which are Read IPL-type CCWs, are initiated from the [...] "implied" [...] CCW, and this channel program continuation causes the first portion of the system loading software to be loaded elsewhere in main storage. The first 8 byte <b>data</b> <b>area</b> contains a PSW which, when fetched {{at the conclusion of the}} IPL, causes the CPU to branch to the bootstrap loader (called [...] "IPL Text") at the main storage address where it was just loaded. The IPL Text is then executed, and eventually the operating system's nucleus is loaded and is branched to, after which normal OS operations commences.|$|E
5000|$|In {{the later}} {{instances}} of the OS, support was added for Format 1 CCWs, and, hence, {{for access to}} <b>data</b> <b>areas</b> which are [...] "above the line" [...] (Format 0 CCWs may only access <b>data</b> <b>areas</b> which are [...] "below the line").|$|R
5000|$|EXCPVR may be viewed, historically, as a V=R (i.e., Virtual=Real) {{version of}} EXCP. However, EXCPVR is not {{restricted}} to V=R applications. Indeed, EXCPVR may refer to non-V=R <b>data</b> <b>areas</b> provided such <b>data</b> <b>areas</b> are [...] "fixed" [...] and the channel command words which reference such <b>data</b> <b>areas</b> have been translated from virtual to real addresses by the programmer using the LRA Load Real Address [...] privileged instruction, as the data channels deal only with real addresses,not virtual addresses. EXCPVR was first introduced in SVS and was continued in MVS/370.|$|R
5000|$|A DVD-ROM contains, {{besides the}} main-data, {{additional}} <b>data</b> <b>areas.</b> CSS stores there: ...|$|R
2500|$|A CD is {{made from}} [...] thick, {{polycarbonate}} plastic and weighs 15–20grams. From the center outward, components are: the center spindle hole (15mm), the first-transition area (clamping ring), the clamping area (stacking ring), the second-transition area (mirror band), the program (<b>data)</b> <b>area,</b> and the rim. The inner program area occupies a radius from 25 to 58mm.|$|E
2500|$|A buffer {{overflow}} {{occurring in the}} heap <b>data</b> <b>area</b> {{is referred to as}} a heap overflow and is exploitable in a manner different from that of stack-based overflows. [...] Memory on the heap is dynamically allocated by the application at run-time and typically contains program data. [...] Exploitation is performed by corrupting this data in specific ways to cause the application to overwrite internal structures such as linked list pointers. [...] The canonical heap overflow technique overwrites dynamic memory allocation linkage (such as malloc meta data) and uses the resulting pointer exchange to overwrite a program function pointer.|$|E
5000|$|Additionally, the PS/2 {{introduced}} a new software <b>data</b> <b>area</b> known as the Extended BIOS <b>Data</b> <b>Area</b> (EBDA). Its primary use was to add a new buffer area for the dedicated mouse port. This also required making a change to the [...] "traditional" [...] BIOS <b>Data</b> <b>Area</b> (BDA) which was then required {{to point to the}} base address of the EBDA.|$|E
5000|$|The I or Input specs are next, and {{describe}} the <b>data</b> <b>areas</b> within files. RPG II permits redefinition of <b>data</b> <b>areas</b> so that a field named FLDA might occupy the same area as an array AR that contains 8 elements of 1 character each. Non-record <b>areas</b> such as <b>data</b> structures can be described. Depending on {{the values of the}} input record, indicators may be conditioned.|$|R
5000|$|...JRN & *JRNRCV: Journal and journal {{receiver}} (used to journal {{changes to}} files, <b>data</b> <b>areas,</b> and stream files).|$|R
5000|$|Maps it (with the [...] "linear" [...] target) {{onto the}} <b>data</b> <b>areas</b> of the PVs the logical volume belongs to.|$|R
50|$|The <b>data</b> <b>area</b> {{begins with}} a set of one or more volume descriptors, {{terminated}} with a volume descriptor set terminator. Collectively the volume descriptor set acts as a header for the <b>data</b> <b>area,</b> describing its content (similar to the BIOS parameter block used by FAT, HPFS and NTFS formatted disks).|$|E
5000|$|...DTAARA: <b>Data</b> <b>area</b> (small bits {{of storage}} used to store tiny items of data for fast access).|$|E
5000|$|XBDA (EDR-DOS 7.01.07 and DRMK only) : Configures the {{relocation}} of the Extended BIOS <b>data</b> <b>area</b> (XBDA) ...|$|E
50|$|It {{is not a}} {{good idea}} to back up an Ingres {{database}} with an OS dump of the database's <b>data</b> <b>areas.</b>|$|R
40|$|Abstract—Real-time systems need time-predictable architec-tures {{to support}} static {{worst-case}} execution time (WCET) analy-sis. One architectural feature, the data cache, {{is hard to}} analyze when different <b>data</b> <b>areas</b> (e. g., heap allocated and stack allocated data) share the same cache. This sharing leads to less precise results of the cache analysis part of the WCET analysis. Splitting the data cache for different <b>data</b> <b>areas</b> enables composable <b>data</b> cache analysis. The WCET analysis tool can analyze the accesses to these different <b>data</b> <b>areas</b> independently. In this paper we present the design and implementation of a cache for stack allocated data. Our port of the LLVM C++ compiler supports {{the management of the}} stack cache. The combination of stack cache instructions and the hardware implementation of the stack cache is a further step towards time-predictable architectures. I...|$|R
40|$|Abstract Caches are {{essential}} {{to bridge the gap}} between the high latency main memory and the fast processor pipeline. Standard processor architectures implement two first-level caches to avoid a structural hazard in the pipeline: an instruction cache and a data cache. For tight worst-case execution times it is important to classify memory accesses as either cache hit or cache miss. The addresses of instruction fetches are known statically and static cache hit/miss classification is possible for the instruction cache. The access to data that is cached in the data cache is harder to predict statically. Several different <b>data</b> <b>areas,</b> such as stack, global data, and heap allocated data, share the same cache. Some addresses are known statically, other addresses are only known at runtime. With a standard cache organization all those different <b>data</b> <b>areas</b> must be considered by worst-case execution time analysis. In this paper we propose to split the data cache for the different <b>data</b> <b>areas.</b> <b>Data</b> cache analysis can be performed individually for the different areas. Access to an unknown address in the heap does not destroy the abstract cache state for other <b>data</b> <b>areas.</b> Furthermore, we propose to use a small, highly associative cache for the heap area. We designed and implemented a static analysis for this cache, and integrated it into a worst-case execution time analysis tool...|$|R
50|$|In disk drives, each {{physical}} {{sector is}} made up of three basic parts, the sector header, the <b>data</b> <b>area</b> and the error-correcting code (ECC). The sector header contains information used by the drive and controller; this information includes sync bytes, address identification, flaw flag and header parity bytes. The header may also include an alternate address to be used if the <b>data</b> <b>area</b> is undependable. The address identification is used to ensure that the mechanics of the drive have positioned the read/write head over the correct location. The <b>data</b> <b>area</b> contains the recorded user data, while the ECC field contains codes based on the data field, which are used to check and possibly correct errors that may have been introduced into the data.|$|E
50|$|Constellation 3D {{estimated}} that {{a card with}} a <b>data</b> <b>area</b> of 25 mm could hold 10 gigabytes of data.|$|E
50|$|The {{middle of}} the display is called the <b>data</b> <b>area.</b> It {{consists}} of one input-capable field and one output field per line.|$|E
40|$|Traditionally shared <b>data</b> <b>areas</b> {{have been}} used to {{efficiently}} communicate between embedded system tasks, such as periodically executing control system tasks. Such implementations are highly sensitive to the execution order of tasks, i. e., they depend on a static timeline. SAE AADL supports a port communication model that ensures deterministic processing of signal streams. In this paper we discuss an analytical framework that allows us to optimize such port-based communication by generating a runtime executive that utilizes shared <b>data</b> <b>areas</b> where appropriate, while ensuring the timing semantic assumed by the control application...|$|R
50|$|However, such {{differentiation}} introduces additional design complexity. For example, {{deciding how}} to define the different <b>data</b> flow <b>areas,</b> and how to handle event passing between different <b>data</b> flow <b>areas.</b>|$|R
40|$|In striped {{disk array}} systems, the {{independency}} of disks for prefetching {{is more important}} than parallelism under high concurrency of accesses, based on which strip prefetching with low read cost has more ability to improve the performance of RAID. However, it indiscriminately fetching all the involved strips limits its applicability. To solve this problem, we propose a Locality-aware Strip Prefetching Scheme (LSP), where it keeps track of the users&rsquo; accesses and identifies the hot <b>data</b> <b>areas.</b> Only those strips located in the hot <b>data</b> <b>areas</b> will be prefetched and each prefetching request fetches one strip. LSP has several advantages. First, LSP adapts to the evolving workloads in an online and self-tuning fashion and satisfies the striped disk array systems due to the low read cost in each prefetching request. Second, LSP fully exploits the spatial locality in users&rsquo; accesses. Here, the spatial locality is more general, which includes multiple simple patterns, such as loop references, sequentiality, reverse references, and other locality-awarded patterns. Third, LSP discriminates the hot <b>data</b> <b>areas</b> from the cold <b>data</b> <b>areas</b> when prefetching, which significantly alleviates the waste of disk bandwidth, optimizes the cache utilization and improves the prefetching accuracy. We have implemented the prototype of LSP algorithm in Linux kernel 2. 6. 18. The experimental results show that LSP outperforms SP and Sequential prefetching (SEQP) by up to 22. 4 % and 24. 1 % in terms of the average response time, and by up to 1. 5 times and 2. 3 times in terms of throughput, respectively...|$|R
5000|$|Primrose, Entering Incomplete Map <b>Data</b> <b>Area,</b> Three Lotus Flowers for Acts of Defiance - 2015 - Mama Quilla Productions / Theatre 503 ...|$|E
5000|$|Data formats {{for data}} exchange. Digital message bitstrings are exchanged. The bitstrings are divided in fields and each field carries {{information}} {{relevant to the}} protocol. Conceptually the bitstring {{is divided into two}} parts called the header area and the <b>data</b> <b>area.</b> The actual message is stored in the <b>data</b> <b>area,</b> so the header area contains the fields with more relevance to the protocol. Bitstrings longer than the maximum transmission unit (MTU) are divided in pieces of appropriate size.|$|E
50|$|Data Offset: (8 bits): The {{offset from}} the start of the packet's DCCP header to the start of its {{application}} <b>data</b> <b>area,</b> in 32-bit words.|$|E
25|$|Address space layout {{randomization}} (ASLR) is {{a computer}} security feature which involves arranging the positions of key <b>data</b> <b>areas,</b> usually including {{the base of the}} executable and position of libraries, heap, and stack, randomly in a process' address space.|$|R
40|$|A {{system which}} predicts {{halftone}} reproduction curves for film-developer combinations, {{based on the}} combined optical and chemical modulation transfer function for that combination, has been tested using a 100 cycle/inch contact line screen. Density in fine detail is predicted and used to determine fractional <b>area</b> <b>data.</b> Predicted fractional <b>area</b> <b>data</b> was found to lie within 95 percent confidence intervals of experimental fractional <b>area</b> <b>data...</b>|$|R
50|$|There {{are three}} <b>data</b> <b>areas</b> on a GD-ROM disc. The first is in {{conventional}} CD format, and usually contains an audio track {{with a warning}} that the disc is for use on a Dreamcast, and can damage CD players. These vary by region.|$|R
50|$|The <b>data</b> <b>area</b> is {{the largest}} {{component}} of the file system, using {{the majority of the}} space. It is where the actual file and directory data are stored.|$|E
5000|$|The <b>data</b> <b>area</b> of {{the disc}} usually {{includes}} DRM-restricted {{copies of the}} audio content, for which a player only exists on the dominant PC operating system, Microsoft Windows.|$|E
50|$|A good {{analogy is}} to {{consider}} a packet {{to be like a}} letter: the header is like the envelope, and the <b>data</b> <b>area</b> is whatever the person puts inside the envelope.|$|E
40|$|Abstract. Future {{embedded}} systems {{are expected to}} use chip-multiprocessors to provide the execution power for increasingly demanding applications. Multi-processors increase {{the pressure on the}} memory bandwidth and processor local caching is mandatory. However, data caches are known to be very hard to inte-grate into the worst-case execution time (WCET) analysis. We tackle this issue from the computer architecture side: provide a data cache organization that en-ables tight WCET analysis. Similar to the cache splitting between instruction and data, we argue to split the data cache for different <b>data</b> <b>areas.</b> In this paper we show cache simulation results for the split-cache organization, propose the mod-ularization of the data cache analysis for the different <b>data</b> <b>areas,</b> and evaluate the implementation costs in a prototype chip-multiprocessor system. ...|$|R
50|$|A: National census {{authority}} data agglomeration <b>data</b> (land <b>area</b> or population).|$|R
5000|$|Finding {{the right}} <b>data</b> (subject <b>area,</b> {{historical}} reach, breadth, level of detail, etc.) ...|$|R
