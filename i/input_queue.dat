77|319|Public
5000|$|... after {{successful}} JCL validation, job {{is moved}} to <b>input</b> <b>queue,</b> waiting for job execution ...|$|E
50|$|As the {{complexity}} of these chips increases, the cost also increases. These processors are relatively costlier than their counterparts without the prefetch <b>input</b> <b>queue.</b>|$|E
5000|$|... {{according}} to Environment tag, a job {{can be moved}} from the <b>input</b> <b>queue</b> to the routing queue, waiting for another JEM cluster which will fetch and execute it ...|$|E
40|$|Many {{proposals}} of <b>input</b> <b>queuing</b> {{cell switch}} architectures have recently {{appeared in the}} literature. Some have even found application in commercial very high speed IP routers. In this paper we discuss {{the pros and cons}} of <b>input</b> and output <b>queuing</b> switch architectures, we provide a taxonomy of scheduling algorithms for <b>input</b> <b>queuing</b> switches, and we present comparative performance results for some of the recent proposals of <b>input</b> <b>queuing</b> cell switches. Performance is measured in terms of cell loss probability, as well as average, standard deviation, and 99 -th quantile of the cell delay with different traffic patterns. The complexity of the algorithms, and the amount of control information to be exchanged inside the switch for their execution, are also discussed...|$|R
40|$|Abstract — Advanced <b>input</b> <b>queuing</b> is an attractive, {{promising}} architecture for high-speed ATM switches, {{because it}} combines the low cost of <b>input</b> <b>queuing</b> {{with the high}} performance of output queuing. The need for scalable schedulers for advanced <b>input</b> <b>queuing</b> switch architectures {{has led to the}} development of efficient distributed scheduling algorithms. We introduce a new distributed scheduling algorithm, FIRM, which provides improved performance characteristics over alternative distributed algorithms. FIRM achieves saturation throughput 1 with lower delay than the most efficient alternative (up to 50 % at high load). Furthermore, it provides improved fairness (it approximates FCFS) and tighter service guarantee than others. FIRM provides a basis for a class of distributed scheduling algorithms, many of which provide even more improved performance characteristics. I...|$|R
40|$|<b>Input</b> <b>queued</b> and {{combined}} input/output queued architectures have recently come {{to play a}} major role in the design of high performance switches and routers for packet networks. These architectures must be controlled by a packet scheduling algorithm, which solves contentions in the transfer of data units to switch outputs. Several scheduling algorithms were proposed in the literature for switches operating on xed-size data units. In this paper we consider the case of packet switches, i. e., devices operating on variable-size data units at their interfaces, but internally operating on xed-size data units, and we propose novel extensions of known scheduling algorithms for <b>input</b> <b>queued</b> {{and combined}} input/output queued architectures. We show by simulation that, in the case of packet switches, <b>input</b> <b>queued</b> and combined input/output queued architectures can provide performance advantages over output queued architectures. 1 Background: <b>Input</b> vs. Output <b>queued</b> Switches A key compon [...] ...|$|R
5000|$|When a {{page fault}} occurs, [...] "anticipatory paging" [...] systems {{will not only}} bring in the {{referenced}} page, but also the next few consecutive pages (analogous to a prefetch <b>input</b> <b>queue</b> in a CPU).|$|E
50|$|In {{operating}} systems, {{processes are}} loaded into memory, {{and wait for}} their turn to be executed by the central processing unit (CPU). CPU scheduling manages process states and decides when a process will be executed next by using the <b>input</b> <b>queue.</b>|$|E
50|$|Generally in {{applications}} like prefetch <b>input</b> <b>queue,</b> M/M/1 Model is popularly {{used because}} of limited use of queue features. In this model {{in accordance with}} microprocessors, the user takes {{the role of the}} execution unit and server is the bus interface unit.|$|E
40|$|Abstract. <b>Input</b> <b>queued</b> and {{combined}} input/output queued switchingarchitectures must {{be controlled by}} a schedulingalgorithm, which solves contention in the transfer of data units to switch outputs. We {{consider the case of}} packet switches (or routers), i. e., devices operatingon variable-size data units at their interfaces, assumingthat they internally operate on fixed-size data units, and we propose novel extensions of known schedulingalgorithms for <b>input</b> <b>queued</b> {{and combined}} input/output queued architectures. We show by simulation that such architectures can provide performance advantages over traditional output queued architectures. ...|$|R
40|$|<b>Input</b> <b>queuing</b> switch {{architectures}} must {{be controlled}} by a scheduling algorithm, which solves contentions in the trans-fer of data units from inputs to outputs. Several scheduling algorithms were proposed in the literature for switches op-erating on fixed-size data units. In this paper we {{consider the case of}} packet switches, i. e., devices operating on variable-size data units at their interfaces, but internally operating on fixed-size data units, and we propose novel extensions of known scheduling algorithms. We show that, in the case of packet switches, <b>input</b> <b>queuing</b> architectures can provide advantages over output <b>queuing</b> architectures. 1 <b>Input</b> vs. Output <b>Queueing</b> In the recent past, significant research efforts were de-voted by both the academic and the industrial commu...|$|R
40|$|Abstract- This paper {{examines}} the delay performance of multicast switches with multiple <b>input</b> <b>queues</b> per <b>input</b> buffer. Under {{the assumptions of}} a Poisson uniform traffic pattern, random packet assigning policy, and random packet scheduling policy, we derive the packet delay and service time under different fanouts. To verify this analysis, extensive simulations are conducted with various fanouts, the numbers of queues, and packet arrival rates. It is shown that the theoretical results agree with the simulation results well. The analysis proves {{that it is possible}} to predict how much the packet delay could be decreased through introducing more <b>input</b> <b>queues</b> per <b>input</b> buffer...|$|R
50|$|The Actor model might usefully be {{described}} as a specialised kind of process-oriented system in which the message-passing model is restricted to the simple fixed case of one infinite <b>input</b> <b>queue</b> per process (i.e. actor), to which any other process can send messages.|$|E
50|$|Synchronous <b>input</b> <b>queue</b> (SIQ): if a GUI {{application}} was not servicing its window messages, the entire GUI system could get stuck and a reboot was required. This problem was considerably reduced with later Warp 3 fixpacks and refined by Warp 4, by taking {{control over the}} application after it had not responded for several seconds.|$|E
50|$|Pipelining {{was brought}} to the {{forefront}} of computing architecture design during the 1960s due to the need for faster and more efficient computing. Pipelining is the broader concept and most modern processors load their instructions some clock cycles before they execute them. This is achieved by pre-loading machine code from memory into a prefetch <b>input</b> <b>queue.</b>|$|E
40|$|This paper {{concentrates}} on obtaining uniform weighted round robin schedules for <b>input</b> <b>queued</b> packet switches. The desired schedules are uniform {{in the sense}} that each connection is serviced at regularly spaced time slots, where the spacing is proportional to the inverse of the guaranteed data rate. Suitable applications include ATM networks as well as satellite switched TDMA systems that provide per packet delay guarantees. Three heuristic algorithms are proposed to obtain such schedules under the constraints imposed by the unit speedup of <b>input</b> <b>queued</b> switches. Numerical experiments indicate that the algorithms have remarkable performance in finding uniform schedules...|$|R
40|$|Abstmct — We {{present a}} new scheduler, the two-dimenswnal {{round-robin}} (2 DRR) scheduler, that provides high throughput and fair access in a packet switch that uses multiple <b>input</b> <b>queues.</b> We consider an architecture {{in which each}} input port maintains a separate queue for each output. In an. V x. V switch, our scheduler determines which of the queues in the total of. Y 2 <b>input</b> <b>queues</b> are served during each time slot. We demonstrate the fairness properties of the 2 DRR scheduler and compare its performance {{with that of the}} <b>input</b> and output <b>queueing</b> configurations showing that our scheme achieves the same saturation throughput as output queueing. The 2 DRR scheduler can be implemented using simple logic components thereby allowing a very high-speed implementation...|$|R
40|$|A switch, or server, serves n <b>input</b> <b>queues,</b> {{processing}} messages {{arriving at}} these queues {{to a single}} output channel. At each time slot the switch can process a single message {{from one of the}} queues. The goal of a switching policy is to minimize the size of the buffers at the <b>input</b> <b>queues</b> that maintain the messages that have not yet been processed. This is a typical on-line setting in which decisions are made based on the current state without knowledge of future events. This general scenario models multiplexing tasks in various systems such as communication networks, cable modem systems, and traffic control. Traditionally, researchers analyzed the performance of a given policy assuming some distribution on the arrival rates of messages at the <b>input</b> <b>queues,</b> or by assuming that the service rate is at least the aggregate of all the input rates. We use competitive analysis to analyze switching service policies, thus avoiding any prior assumptions on the input. Specifically, we show O(log n) -competitive switching policies for the problem and demonstrate matching lower bounds...|$|R
5000|$|The {{technique}} of self-modifying code can be problematic on a pipelined processor. In this technique, {{one of the}} effects of a program is to modify its own upcoming instructions. If the processor has an instruction cache, the original instruction may already have been copied into a prefetch <b>input</b> <b>queue</b> and the modification will not take effect.|$|E
50|$|In networking, packets {{are the key}} {{foundation}} for scheduling. There are {{many different types of}} packet travelling around network core every day, and they are treated totally different. For example, voice and video packets have higher priority than normal packets. In order to manage and distribute packet effectively, network devices also use <b>input</b> <b>queue</b> to determine which packet will be transmitted first.|$|E
5000|$|XM uses a {{prioritized}} product backlog as {{the primary}} work <b>input</b> <b>queue,</b> where work is visualized in an open area generally on a single team Kanban Board. Every XM team has a Scrum Master and also a Product Owner, who together with the team help to ensure that Agile/Lean principles are followed. In XM the Scrum Master has some critical responsibilities, including to: ...|$|E
40|$|Introduction Recently,ma y service disciplinesha ve been {{proposed}} to providequa% 1 y of service (QoS) gua) ] tees in a integraz] services network [5] [...] [8]. Most of theseae gorithms {{were designed to}} be useda the output ports of a output queued (OQ) switch. UnfortunaHzz, OQ switchesha ve serious scaous problem beca%A the ae vaFH%B] t in memorybaory]%T is much slowertha the ae ae]fi 1 fi t intrazz] 6 %zB 2 speed. Consequently, <b>input</b> <b>queueing</b> isuna voidaBA in buildinga lald ca pal y switch. It is well knowntha <b>input</b> <b>queueing</b> su#ers from heafifi%%] 6 %B blocking which limits thema]H um throughput to a out 0. 586 (under uniformtrao aa sumption) [9]. Itca be improved toaHHATF h 100 % throughput if cellsal delivered from input ports to output portsba]% onma%T ummaH hing [10]. However, the high computaz] 62 z complexity of currently known a 2 T% 1] 6 fifi prohibitsmahi ummaH hing from being used ina high-speed switch. Another aother h to improve the performaTB of a <b>input</b> <b>queued</b> (IQ) Manuscrip...|$|R
40|$|Many {{researchers}} had evaluated the throughput and delay performance of virtual output queued (VOQ) packet switches using iterative weighted/un-weighted scheduling algorithms. Prof. Nick Mckeown from Stanford University had evolved with excellent iterative maximal matching (i-slip) scheme which provides throughput near to 100 %. Prof. Kim had suggested multiple <b>input</b> <b>queued</b> architecture which also {{provide more than}} 90 % throughput for less number of <b>input</b> <b>queues</b> per port. (In VOQ N queues per port are used). Our attempt is to use MIQ architecture and evaluate delay, throughput performance with i-slip algorithm for scheduling. While evaluating performance we had used Bernoulli’s and Bursty (ON-OFF) traffic models...|$|R
40|$|Abstract—We {{consider}} {{the problem of}} scheduling multicast traffic in a buffered crossbar switch with multiple <b>input</b> <b>queues</b> at each <b>input</b> port. In this paper, we design and investigate a series of combinations of queuing policies and scheduling algorithms and report the simulation result. It is shown that {{a small number of}} <b>input</b> <b>queues</b> at each <b>input</b> port can dramatically improve the performance under burst multicast traffic in buffered crossbar switches. Under this architecture, it is feasible to design simple queuing policies and scheduling algorithms for high speed switches while keeping high performance and small size of buffer within crossbar. Index terms—scheduling; multicast; buffered crossbar switch I...|$|R
50|$|Most modern {{processors}} {{load the}} machine code before they execute it, {{which means that}} if an instruction that is too near the instruction pointer is modified, the processor will not notice, but instead execute the code {{as it was before}} it was modified. See prefetch <b>input</b> <b>queue</b> (PIQ). PC processors must handle self-modifying code correctly for backwards compatibility reasons but they are far from efficient at doing so.|$|E
50|$|Fetching the {{instruction}} opcodes from program memory {{well in advance}} is known as prefetching and it is served by using prefetch <b>input</b> <b>queue</b> (PIQ).The pre-fetched instructions are stored in data structure - namely a queue. The fetching of opcodes well in advance, prior to their need for execution increases the overall efficiency of the processor boosting its speed. The processor no longer has {{to wait for the}} memory access operations for the subsequent instruction opcode to complete. This architecture was prominently used in the Intel 8086 microprocessor.|$|E
50|$|In {{computer}} science, an <b>input</b> <b>queue</b> is {{a collection}} of processes in storage that are waiting to be brought into memory to run a program. Input queues are mainly used in Operating System Scheduling which is a technique for distributing resources among processes. Input queues, not only apply to operating systems (OS), but may also be applied to scheduling inside networking devices. The purpose of scheduling is to ensure resources are being distributed fairly and effectively; therefore, it improves the performance of the system.|$|E
40|$|High speed {{variable}} length packet switches often use a cell switching core. For such purposes, an <b>input</b> <b>queueing</b> structure has an advantage since it imposes minimal bandwidth requirements on cell buffering memories; {{this leads to}} superior scalability of the switches. The authors consider <b>input</b> <b>queueing</b> switches in which all cells arriving at an <b>input</b> are <b>queued</b> in a single first-come-first-served queue. It is well known that, for such a simple arrangement, the maximum switch throughput {{can be obtained by}} a saturation analysis (i. e [...] each queue is assumed to be infinitely backlogged and then the switch throughput is computed). The authors establish that this saturation throughput also provides a sufficient condition for stochastic stability of the <b>input</b> <b>queues.</b> It is assumed that the cell arrival process at each input is Bernoulli. Each input belongs to one of two priority classes; during output contention resolution, the head-of-the-line cell from a high priority input is given preference. The saturation throughputs of the high and low priority inputs can be computed. It is proved that if the arrival rate at each input is less than the saturation throughput then the queue lengths are stochastically stable. The major contribution of the paper is that it provides an analytical approach for such a problem; the technique can be adapted for more eneral problems...|$|R
40|$|Abstract — <b>Input</b> <b>queued</b> {{switches}} exploiting buffered crossbars (CICQ switches) {{are widely}} considered very promising architectures that outperform <b>input</b> <b>queued</b> (IQ) switches with bufferless switching fabrics {{both in terms}} of architectural scalability and performance. Indeed the problem of scheduling packets for transfer through the switching fabric is significantly simplified by the presence of internal buffers in the crossbar, which makes possible the adoption of efficient, simple and fully distributed scheduling algorithms. In this paper we study the throughput performance of CICQ switches supporting multicast traffic, showing that, similarly to IQ architectures, also CICQ switches with arbitrarily large number of ports may suffer of significant throughput degradation under “pathological ” multicast traffic patterns. Despite of th...|$|R
40|$|Abstract — Many {{researchers}} had evaluated the throughput and delay performance of virtual output queued (VOQ) packet switches using iterative weighted/un-weighted scheduling algorithms. Prof. Nick Mckeown from Stanford University had evolved with excellent iterative maximal matching (i-slip) scheme which provides throughput near to 100 %. Prof. Kim had suggested multiple <b>input</b> <b>queued</b> architecture which also {{provide more than}} 90 % throughput for less number of <b>input</b> <b>queues</b> per port. (In VOQ N queues per port are used). Our attempt is to use MIQ architecture and evaluate delay, throughput performance with i-slip algorithm for scheduling. While evaluating performance we had used Bernoulli’s and Bursty (ON-OFF) traffic models. Keywords- Network communications; Packet-switching networks;routing protocols; Sequencing and scheduling. I...|$|R
5000|$|... cbreak mode {{is one of}} two character-at-a-time modes. (Stephen R. Bourne jokingly {{referred}} to it [...] as a [...] "half-cooked" [...] and therefore [...] "rare" [...] mode.) The line discipline performs no line editing, and the control sequences for line editing functions are treated as normal character input. Applications programs reading from the terminal receive characters immediately, as soon as they are available in the <b>input</b> <b>queue</b> to be read. However, the [...] "interrupt" [...] and [...] "quit" [...] control characters, as well as modem flow control characters, are still handled specially and stripped from the input stream.|$|E
50|$|To enter {{protected}} mode, the Global Descriptor Table (GDT) {{must first}} be created {{with a minimum of}} three entries: a null descriptor, a code segment descriptor and data segment descriptor. In an IBM-compatible machine, the A20 line (21st address line) also must be enabled to allow the use of all the address lines so that the CPU can access beyond 1 megabyte of memory (Only the first 20 are allowed to be used after power-up, to guarantee compatibility with older software written for the Intel 8088-based IBM PC and PC/XT models). After performing those two steps, the PE bit must be set in the CR0 register and a far jump must be made to clear the prefetch <b>input</b> <b>queue.</b>|$|E
3000|$|... ➢ Max Packet Number is {{the largest}} number of packets on the <b>input</b> <b>queue.</b> When the number of packets at the <b>input</b> <b>queue</b> is equal to this threshold, the switch also changes to its normal {{operating}} state.|$|E
40|$|AbstractUnder the {{systolic}} communication model, {{each cell}} (or processor) {{in a parallel}} processing system can operate directly on data residing at the cell's <b>input</b> <b>queues</b> and move computed results directly to the cell's output queues. Incoming and outgoing data need not be stored in the cell's local memory, if not required by the computation. By avoiding these local memory accesses, systolic communication can achieve high efficiency when executing many systolic algorithms. Though efficient, systolic communication may lead to deadlocks at run time if data arriving at a cell's <b>input</b> <b>queues</b> are improperly ordered. This paper describes {{the nature of this}} deadlock problem, gives an abstract formulation of the problem, and provides a deadlock avoidance strategy...|$|R
40|$|This paper {{presents}} a performance study of input buffering in Asynchronous Transfer Mode switches. The main performance metrics are average cell wait-ing time and output port contention under {{a variety of}} load conditions and operating environments. Re-sults show that as the percent utilization increases, the average cell wait time increases. Analysis of out-put port contention shows that it increases with the increase in utilization, and the number of conflicts increases with both the utilization and the <b>input</b> <b>queues.</b> The results are presented in a series of curves that present the number of cells delivered during output conflicts versus percent utilization. The soft-ware developed offers several benefits: it provides a flexible approach to investigate performance of <b>input</b> <b>queueing</b> {{under a variety of}} conditions. The simula-tion duration, interarrival distribution, number of <b>input</b> <b>queues</b> and maximum queue size can all be varied simply by editing the initialization file; no recompiling of source code is required. It allows for modification and extensibility with only minor soft-ware changes. It does not contain operating-specific function calls; thus, it can be used without changes on standard DOS-based PCs or UNIX-based systems...|$|R
40|$|As {{we saw in}} class, a {{simplified}} formulation for computing the saturation throughput of a 2 x 2 <b>input</b> <b>queued</b> switch (single <b>queue</b> per <b>input,</b> i. e. without VOQ's) is the following. The head-ofline cells of the two <b>input</b> <b>queues</b> may have the following destination combinations: (i) " 00 ", meaning that they are both destined to output 0; or (ii) " 01 ", meaning the first is destined to output 0 {{and the other is}} destined to output 1; or (iii) " 10 "; or (iv) " 11 ". If we assume that the switch is saturated, the queues are always non-empty. If we assume that arrivals are i. i. d. (independent identically distributed) with uniformly distributed destinations, then the probability of each of the above combinations is 0. 25, and it is independent of the combination in the previous time slot and of the service decision made in the previous time slot. Combinations 01 and 10 yield 2 outgoing cells per time slot for the two outputs (average throughput 1. 0 per output), while combinations 00 and 11 yield only 1 outgoing cell per time slot for both outputs (average throughput 0. 5 per output). Hence, the average (saturation) throughput per output over a long time window will be: 0. 25 * 0. 5 + 0. 25 * 1. 0 + 0. 25 * 1. 0 + 0. 25 * 0. 5 = 0. 75. Using the same technique, calculate the saturation throughput of a 3 x 3 <b>input</b> <b>queued</b> switch (single <b>queue</b> per <b>input).</b> The head-of-line cells of the three <b>input</b> <b>queues</b> may form 2...|$|R
