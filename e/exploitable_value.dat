1|9|Public
40|$|AbstractIn {{the current}} study, {{the action of}} two {{bacteria}} capable of producing biosurfactants and oxidizing iron (Fe) and sulfur (S), namely Bacillus pumilus SKC- 2 and Alicyclobacillus ferrooxydans SKC/SAA- 2, was investigated {{with respect to their}} ability in possessing dual-function as either bio-collector or depressant for the development of sulfide bioflotation processes. Both bacterial strains were able to produce high amounts of biosurfactants interacted with pyrite that had an important role in their adhesion on the surface of pyrite as well as the change of pyrite surface properties. Over the course of the experiments, the pH of the solutions gradually decreased to ∼ 3, indicating the active oxidation of pyrite minerals by bacteria. The growth of both bacterial strains resulted in the generation of biosurfactants as represented by the decrease of the surface tension of the solutions and the increase of the contact angle of the pyrite surfaces as a function of time. However, the contact angle of pyrite surfaces gradually decreased after 5 days of incubation until the experiments terminated on 30 days. Scanning electron microscopy equipped with energy dispersive X-ray spectroscopy (SEM-EDS) and Fourier transform Infrared (FTIR) analyses also confirmed the role of both bacterial strains in changing the pyrite surface properties to be more hydrophobic or more hydrophilic depending on the time of incubation. These results indicate that the changes of pyrite surface properties are clearly as the results of bacterial action, likely serving as both bio-collector or bio-frother and depressant that would be very applicable for flotation processes. These results increase our knowledge on the interactions in pyrite-bacteria complexes and could potentially be a very useful result with real <b>exploitable</b> <b>value</b> for those working on sulfide bioflotation processes...|$|E
2500|$|If {{there is}} a {{constant}} pressure outside of the vessel which is equal to the starting pressure , the positive work of the outer pressure reduces the <b>exploitable</b> energy (negative <b>value).</b> This adds a term to the equation above: ...|$|R
50|$|The {{authentication}} (Au) metric {{describes the}} number of times that an attacker must authenticate to a target to exploit it. It does not include (for example) authentication to a network in order to gain access. For locally <b>exploitable</b> vulnerabilities, this <b>value</b> should only be set to Single or Multiple if further authentication is required after initial access.|$|R
5000|$|Biometric Tokenization is {{the process}} of substituting a stored {{biometric}} template with a non-sensitive equivalent, called a token, that lacks extrinsic or <b>exploitable</b> meaning or <b>value.</b> The process combines the biometrics with public-key cryptography to enable the use of a stored biometric template (e.g., fingerprint image on a mobile or desktop device) for secure or strong authentication to applications or other systems without presenting the template in its original, replicable form.|$|R
40|$|In principle, an {{exhaust system}} {{influences}} the maximum output and {{characteristic of a}} two-stroke combustion engine. The {{first part of the}} contribution deals with the possibility of transformation in a maximum output and a range of <b>exploitable</b> speed <b>values</b> by means of the change of exhaust manifold length in exhaust pipe section. This knowledge covers an output curve variability of an engine operating speed range. The second part of this contribution deals with the combustion product 2 ̆ 7 s temperature influence in an exhaust system for a maximum engine output and a range of exploitable speed. By increasing the combustion product 2 ̆ 7 s temperature in an exhaust system, the maximum engine output is reduced overall. There is the transfer of engine output to higher engine speed because the exhaust manifold length is short theoretically. That is why it is necessary to provide the optimum value of combustion products temperature in an exhaust system and so achieve maximum values of outgoing parameters...|$|R
40|$|This {{thesis is}} an {{investigation}} of the tragic form in relation to Horkheimer and Adorno’s Dialectic of Enlightenment. Horkheimer and Adorno claim that the culture industry has appropriated the tragic form for its own purposes and rendered it a {{part of the process of}} the conversion of the individual to <b>exploitable</b> circuits of <b>value.</b> Enlightenment Tragedy is a type of tragedy that avoids their critique, and in fact offers the reader tools to resist the culture industry in a de-reifying moment. This thesis investigates Shakespeare’s Troilus and Cressida, and Flannery O’Connor’s “Good Country People” and “Revelation” and locates the characteristics that mark them as instances of this tragedy...|$|R
40|$|Abstract — This paper {{presents}} a comprehensive characterization {{study on the}} <b>exploitable</b> memory <b>value</b> reuse present in programs. We compare three reuse schemes: store value reuse, loaded value reuse, and macro data reuse [12], [13]. Macro data reuse, enabled by macro data loads, capitalizes on under-utilized cache port bandwidth and makes use of the spatial locality found in port-wide macro data. Using a generalized memory value reuse table (MVRT) model, we present the results of (1) per program reuse analysis, (2) per data size analysis, (3) per region analysis, (4) per MVRT size analysis, and (4) estimating the impact of ISA and machine widths. The macro data load mechanism is shown to open up significantly more loaded value reuse instances compared with previous loaded value reuse proposals: over 75 % (SPEC 2 k integer), 23 % (SPEC 2 k floating-point), and 139 % (MiBench) more load-to-load forwarding opportunities using a 64 -entry MVRT. We also perform a quantitative study using a realistic processor model and show that over 35 % of L 1 cache accesses in the SPEC 2 k integer and MiBench programs can be eliminated, resulting in a related energy reduction of 27 % and 31 % on average, respectively. I...|$|R
40|$|Nature builds {{resilience}} through storages {{of energy}} {{and one of the}} most iconic elements encompassing this capacity is represented by forests. Biomass in general and forests in particular are known to be one of the vast global reservoirs of carbon and energy and are described as forms of renewable natural capital that managed until the second half of the ninetieth century to globally sustain human communities as the major source of energy. Forests actively contribute to the functionality of a territorial system through a series of attributes such as water retention capacity, soil and slope stability, primary production capacity, all termed ecosystem services, benefits that communities take for granted. In more remote rural areas around the world, such as the Carpathian Mara River watershed, local communities still rely on forestry wood as the main supplier of thermal energy. The dependency relation also represents the community’s level of perception towards natural resources, the landmarks and image of the entire Land of Maramureș as „the wood civilisation”, for example, being a cultural construct build through this energetic resource. In a time of pressure over energetic resources and following the premises of Emergy Theory and methodology, in this paper we propose an alternative approach in assessing natural resources in order to better understand that the <b>exploitable</b> energetic <b>value</b> of forests comes second when compared with the value of the provided ecosystem services...|$|R
5000|$|Tokenization, {{when applied}} to data security, {{is the process of}} substituting a {{sensitive}} data element with a non-sensitive equivalent, referred to as a token, that has no extrinsic or <b>exploitable</b> meaning or <b>value.</b> The token is a reference (i.e. identifier) that maps back to the sensitive data through a tokenization system. The mapping from original data to a token uses methods which render tokens infeasible to reverse {{in the absence of the}} tokenization system, for example using tokens created from random numbers. The tokenization system must be secured and validated using security best practices applicable to sensitive data protection, secure storage, audit, authentication and authorization. The tokenization system provides data processing applications with the authority and interfaces to request tokens, or detokenize back to sensitive data. [...] The security and risk reduction benefits of tokenization require that the tokenization system is logically isolated and segmented from data processing systems and applications that previously processed or stored sensitive data replaced by tokens. Only the tokenization system can tokenize data to create tokens, or detokenize back to redeem sensitive data under strict security controls. The token generation method must be proven to have the property that there is no feasible means through direct attack, cryptanalysis, side channel analysis, token mapping table exposure or brute force techniques to reverse tokens back to live data.|$|R

