5|74|Public
40|$|In this paper, we {{investigate}} {{the notion that}} there may be alternate methods, beyond typical rectilinear interpolations such as Bilinear Interpolation, that have a greater suitability for use in visual/image preprocessors for Artificial Neural Networks. We present a novel method for down-sampling image data in preparation for a <b>Feed-Forward</b> <b>Perceptron</b> system assisted by a neural usefulness metric, inspired by those common to pruning algorithms. This new method achieves greater accuracy compared to the same system using by Bilinear Interpolation, and has a reduced computational time...|$|E
40|$|We present {{empirical}} {{justification of}} why logistic regression may acceptably approximate, using {{the number of}} currently vocalizing interlocutors, the probabilities returned by a time-invariant, conditionally independent model of turn-taking. The resulting parametric model with 3 degrees of freedom is shown to be identical to an infinite-range Ising antiferromagnet, with slow connections, in an external field; it is suitable for undifferentiated-participant scenarios. In differentiatedparticipant scenarios, untying parameters results in an infinite-range spin glass whose degrees of freedom scale as {{the square of the}} number of participants; it offers an efficient representation of participant-pair synchrony. We discuss the implications of model parametrization and of the thermodynamic and <b>feed-forward</b> <b>perceptron</b> formalisms for easily quantifying aspects of conversational dynamics. ...|$|E
40|$|Karyotyping is {{a common}} method in cytogenetics. Automatic {{classification}} of the chromosomes within the microscopic images {{is the first step}} in designing an automatic karyotyping system. This is a difficult task especially if the chromosome is highly curved within the image. This paper introduces a new Wavelet Transform based Linear Discriminant Analysis based feature vector for discriminating both normal and automatically straightened chromosomes in group E. A three layer <b>feed-forward</b> <b>perceptron</b> neural network, which is trained by means of the backpropagation algorithm, is used to classify the input chromosome into one of the three classes in the group E. When tested on a data set of 303 highly curved chromosomes after automatically straightening by a previously reported method by the authors of current article [1] an average correct classification rate of 99. 3 % was obtained. 1...|$|E
50|$|Research on {{comparable}} devices {{was also}} {{being done in}} other places such as SRI, and many researchers had big expectations on what they could do. The initial excitement became somewhat reduced, though, when in 1969 Marvin Minsky and Seymour Papert published the book “Perceptrons” with a mathematical proof about the limitations of two-layer <b>feed-forward</b> <b>perceptrons</b> as well as unproven claims about the difficulty of training multi-layer perceptrons. The book's only proven result -- that linear functions cannot model non-linear ones -- was trivial but the book had nevertheless a pronounced effect on research funding and, consequently, the community.|$|R
5000|$|What {{the book}} does prove {{is that in}} three-layered <b>feed-forward</b> <b>perceptrons</b> (with a {{so-called}} [...] "hidden" [...] or [...] "intermediary" [...] layer), {{it is not possible}} to compute some predicates unless at least one of the neurons in the first layer of neurons (the [...] "intermediary" [...] layer) is connected with a non-null weight to each and every input. This was contrary to a hope held by some researchers in relying mostly on networks with a few layers of [...] "local" [...] neurons, each one connected only to a small number of inputs. A feed-forward machine with [...] "local" [...] neurons is much easier to build and use than a larger, fully connected neural network, so researchers at the time concentrated on these instead of on more complicated models.|$|R
40|$|AbstractIt {{is shown}} that the general {{approximation}} property of <b>feed-forward</b> multilayer <b>perceptron</b> networks can be achieved in networks where the number of nodes in each layer is bounded, {{but the number of}} layers grows to infinity. This is the case provided the node function is twice continuously differentiable and not linear...|$|R
40|$|An {{algorithm}} {{to extract}} representations from <b>feed-forward</b> <b>perceptron</b> networks (threshold) is outlined. The representation {{is based on}} polytopic decision regions in the input space{ and is exact not an approximation. Multiple example networks are analyzed. Some consequences of the representation are magnitudes of complexity for network analysis algorithms in general, complexity bounds on comparing the error of symbolic systems with perceptron networks, as well as implications for generalization and learning. 1 Introduction Despite being prolic models, feed-forward neural networks have been ubiquitously criticized for having a low degree of comprehensibility. A neural network could be performing a task it was trained for perfectly, but to its users it remains a black-box, its internal workings being completely opaque. There are multiple reasons for this opacity: First, since neural networks consists of a large quantity of interconnected processing units which continuously pass informat [...] ...|$|E
40|$|In this paper, {{we propose}} a neural tree classifier, called the convex {{objective}} function neural tree (COF-NT), {{which has a}} specialized perceptron at each node. The specialized perceptron is a single layer <b>feed-forward</b> <b>perceptron</b> which calculates the errors before the neuron's non-linear activation function instead of after them. Thus, the network parameters are independent of non-linear activation functions, and subsequently, the objective function is a convex objective function. The solution can be easily obtained by solving a system of linear equations which will require less computational power than conventional iterative methods. During the training, the proposed neural tree classifier divides the training set into smaller subsets by adding new levels to the tree. Each child perceptron takes forward the task of training done by its parent perceptron on the superset of this subset. Thus, the training is done {{by a number of}} single layer perceptrons (each perceptron carrying forward the work done by its ancestors) that reach the global minima in a finite number of steps. The proposed algorithm has been tested on available benchmark datasets and the results are promising in terms of classification accuracy and training time. © 2015 Elsevier B. V. All rights reserved...|$|E
40|$|We {{describe}} {{how to solve}} linear-non-separable problems using simple <b>feed-forward</b> <b>perceptrons</b> without hidden layers, and a biologically motivated topologically distributed encoding for input data. We point out why neural networks have advantages compared to classic mathematical algorithms without loosing performance. The Iris-dataset from Fisher [3] is analyzed as a practical example. Keywords: Classification, perceptrons, topologically distributed encoding, Iris dataset 2 Introduction In this paper we examine the performance of neural classification networks dealing with real world problems. We show that neural networks can provide results comparable to mathematical methods (c. f. [10]). But in contrast to mathematical methods neural networks do not require applicability preconditions, which in most cases are not fulfilled. To clarify this we will give two examples of well known analytic methods. The Bayesian decision theory has two severe drawbacks: the a-priori-probabilities of all [...] ...|$|R
40|$|The {{fields of}} neural {{computation}} and artificial neural networks have developed {{much in the}} last decades. Most of the works in these fields focus on implementing and/or learning discrete functions or behavior. However, technical, physical, and also cognitive processes evolve continuously in time. This cannot be described directly with standard architectures of artificial neural networks such as multi-layer <b>feed-forward</b> <b>perceptrons.</b> Therefore, in this paper, we will argue that neural networks modeling continuous time are needed explicitly for this purpose, because with them the synthesis and analysis of continuous and possibly periodic processes in time are possible (e. g. for robot behavior) besides computing discrete classification functions (e. g. for logical reasoning). We will relate possible neural network architectures with (hybrid) automata models that allow to express continuous processes. Comment: 16 pages, 10 figures. This paper is an extended version of a contribution presented at KI 2009 Workshop Complex Cognitio...|$|R
40|$|Arti cial neural {{networks}} {{are suitable for}} the prediction of chaotic time series. A modi ed back-propagation algorithm with neuron splitting is used to train <b>feed-forward</b> multilayer <b>perceptron</b> networks for prediction. There are two ways of parallelizing: distributing the training set for batch learning or distribute the vector-matrix-operations for on-line training. Three implementation are compaired: PVM on a workstation cluster and Parix and the new PVM/Parix onaTransputer system. Results {{about the quality of}} forecasting an examplary time series and speedups of the parallel programs are presented. ...|$|R
40|$|An Artificial Neural Network (ANN) {{simulation}} model {{is used to}} study the effect of operational condition on the efficiency of a solvent extraction process. The model developed {{in this study was}} solely based on the concept of machine learning than complex mass and energy balances. The input parameters of the model were chosen to be operational conditions of the chemical process whereas, efficiency of the system was selected as the output parameter. A <b>feed-forward</b> Multi-Layer <b>Perceptron</b> Neural Network was successfully applied to capture the relationship between inputs and output parameters...|$|R
40|$|Random cost {{simulations}} {{were introduced}} {{as a method}} to investigate optimization prob-lems in systems with con icting constraints. Here I study the approach {{in connection with the}} training of a <b>feed-forward</b> multilayer <b>perceptron,</b> as used in high energy physics ap-plications. It is suggested to use random cost simulations for generating a set of selected con gurations. On each of those nal minimization may then be performed by a standard algorithm. For the training example at hand many almost degenerate local minima are thus found. Some eort is spent to discuss whether they lead to equivalent classications of the data. ...|$|R
40|$|In this study, an Artificial Neural Network (ANN) {{approach}} is utilized {{to perform a}} parametric study {{on the process of}} extraction of lubricants from heavy petroleum cuts. To train the model, we used field data collected from an industrial plant. Operational conditions of feed and solvent flow rate, Temperature of streams and mixing rate were considered as the input to the model, whereas the flow rate of the main product was considered as the output of the ANN model. A <b>feed-forward</b> Multi-Layer <b>Perceptron</b> Neural Network was successfully applied to capture the relationship between inputs and output parameters. Comment: 7 pages, 2 figure...|$|R
40|$|This paper {{presents}} a Learning Classifier System (LCS) where each classifier condition {{is represented by}} a <b>feed-forward</b> multi-layered <b>perceptron</b> (MLP) network. Adaptive behavior is realized {{through the use of}} self-adaptive parameters and neural constructivism, providing the system with a flexible knowledge representation. The approach allows for the evolution of networks of appropriate complexity to solve a continuous maze environment, here using either discrete-valued actions, continuous-valued actions, or continuous-valued actions of continuous duration. In each case, it is shown that the neural LCS employed is capable of developing optimal solutions to the reinforcement learning task presented in this paper...|$|R
40|$|We discuss an {{unsupervised}} learning method which {{is driven by}} an information theoretic based criterion. Information theoretic based learning has been examined by several authors Linsker [2, 3], Bell and Sejnowski [5], Deco and Obradovic [1], and Viola et al [6]. The method we discuss differs from previous work {{in that it is}} extensible to a <b>feed-forward</b> multi-layer <b>perceptron</b> with an arbitrary number of layers and makes no assumption about the underlying PDF of the input space. We show a simple unsupervised method by which multi-dimensional signals can be nonlinearly transformed onto a maximum entropy feature space resulting in statistically independent features. 1. ...|$|R
40|$|ABSTRACT: An {{artificial}} {{neural network}} {{has been used to}} determine the volume flux and rejections of Ca 2 +, Na+ and Cl¯, as a function of transmembrane pressure and concentrations of Ca 2 +, polyethyleneimine, and polyacrylic acid in water softening by nanofiltration process in presence of polyelectrolytes. The <b>feed-forward</b> multi-layer <b>perceptron</b> {{artificial neural network}} including an eight-neuron hidden layer has the least error in modeling this non-linear process. The overall agreement between the artificial neural network results and experimental data is very good for both the volume flux and rejections, because the maximum values of normalized bias and error are- 0. 01122 and 1. 0737 respectively...|$|R
40|$|Two connectionist {{models are}} {{reported}} that mimicked the defining {{features of the}} double dissociation between phonological and surface dyslexia in word reading. One model was a <b>feed-forward,</b> three-layer <b>perceptron,</b> and the other included recurrent connections. Neither model contained an architectural separation of sublexical and lexical processes, nor of phonological and semantic processes. Analyses showed that the double dissociation was simulated because the control parameter input gain shifted the models between conjunctive and componential modes of processing. The dissociation was not simulated by any kind of damage to separate system components. The models {{are discussed in the}} context of current accounts of surface and phonological dyslexia...|$|R
40|$|Hybrid connectionist/HMM systems model time {{using both}} a Markov chain and through {{properties}} of a connectionist network. In this paper, {{we discuss the}} nature of the time dependence currently employed in our systems using recurrent networks (RNs) and <b>feed-forward</b> multi-layer <b>perceptrons</b> (MLPs). In particular, we introduce local recurrences into an MLP to produce an enhanced input representation. This {{is in the form of}} an adaptive gamma filter and incorporates an automatic approach for learning temporal dependencies. We have experimented on a speaker-independent phone recognition task using the TIMIT database. Results using the gamma filtered input representation have shown improvement over the baseline MLP system. Improvements have been obtained through merging the baseline and gamma filter models...|$|R
40|$|An hybrid array, {{constituted}} by five chemically modified tin oxide thin films and a humidity sensor are {{the input of}} a <b>feed-forward</b> two layers <b>perceptron</b> net to identify and quantify individual concentrations of methane and ethyl alcohol mixtures in a variable humidity environment. The network always identifies which gas is present, no matter to humidity, and predicts the gas concentration with a percentage full scale error equal to 8 %...|$|R
40|$|AbstractService-oriented {{architecture}} (SOA) is {{an approach}} for constructing distributed business-driven frameworks. Reliability of service- oriented systems (SOS) {{relies on the}} Web services used and in addition Internet associations. Predicting reliability of Web service has turned into an imperative exploration issue. In this paper Hidden Markov Model (HMM) and Artificial Neural Network (ANN) are used to model the Web service failure model and predict Web services reliability. The forward-backward estimation-maximization algorithm is used to estimate the modeling parameters for HMM. Different methods of ANN like <b>feed-forward,</b> multilayer <b>perceptron,</b> and radial basis function neural networks are applied to predict the reliability. The accuracy for each model is calculated and the performance parameters of models are compared by considering Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) metrics...|$|R
40|$|An {{artificial}} {{neural network}} (ANN) approach was used to obtain a simulation model to predict the rotating disc contactor (RDC) performance during the extraction of aromatic hydrocarbons from lube oil cuts, to produce a lubricating base oil using furfural as solvent. The field data used for training the ANN model was obtained from a lubricating oil production company. The input parameters of the ANN model were the volumetric flow rates of feed and solvent, the temperatures of feed and solvent, and the disc rotation rate. The output parameters were the volumetric flow rate of the raffinate phase and the extraction yield. In this study, a <b>feed-forward</b> multi-layer <b>perceptron</b> neural network was successfully used to demonstrate the complex relationship between the mentioned input and output parameters...|$|R
40|$|As {{potential}} candidates for explaining human cognition, connectionist models of sentence processing must demonstrate {{their ability to}} behave systematically, gen-eralizing from a small training set. It has recently been shown that simple recur-rent networks and, to a greater extent, echo-state networks possess some ability to generalize in artificial language learning tasks. We investigate this capacity for a recently introduced model that consists of separately trained modules: a recursive self-organizing module for learning temporal context representations and a <b>feed-forward</b> two-layer <b>perceptron</b> module for next-word prediction. We show that the performance of this architecture is comparable with echo-state networks. Taken to-gether, these results weaken the critisism of connectionist approaches, showing that various general recursive connectionist architectures share the potential of behaving systematically. Key words: recurrent neural network, self-organization, next word prediction, systematicit...|$|R
40|$|This paper applies {{forensic}} data mining {{to determine the}} true status of employees and thereafter provide useful evidences for proper administration of administrative rules in a Typical Nigerian Teaching Service. The conventional technique of personnel audit was studied and a new technique for personnel audit was modeled using Artificial Neural Networks and Decision Tree algorithms. Atwo-layer classifier architecture was modeled. The outcome of the experiment proved that Radial Basis Function Artificial Neural Network is better than <b>Feed-forward</b> Multilayer <b>Perceptron</b> in modeling of appointment and promotion audit in layer 1 while Logitboost Multiclass Alternating Decision Tree in Layer 2 is best in modeling suspicious appointment audit and abnormal promotion audit among the tested Decision Trees. The evidential rules derived from the decision trees for determining the suspicious appointment and abnormal promotion were also presented...|$|R
30|$|According to Anderson [1], {{the most}} common used neural network for {{prediction}} tasks is the feed-forward network that {{has at least one}} hidden layer of neurons. In this network, each layer comprises neurons that receive weighted inputs from the preceding layer and send weighted outputs to the neurons in the succeeding layer. There is no feedback connections. The most known representatives of <b>feed-forward</b> networks are <b>perceptrons</b> and their modified version called RBF neural network (RBFNN) [10].|$|R
40|$|Artificial Neural Networks usually {{provide better}} {{approaches}} to problems than they request: pattern classification; pattern recognition; pattern association; identification; resistance to noise; approximation of functions and machine learning. From {{the accuracy of}} results of previous works [1][3][2], {{it is believed that}} multilayer perceptrons can be applied perfectly in the prediction of secondary structures of proteins. A common classifier of neural networks which is usually applied to the prediction of secondary structures of proteins is the <b>Feed-Forward</b> MLP (MultiLayer <b>Perceptron),</b> wit...|$|R
40|$|<b>Feed-forward</b> {{multi-layer}} <b>perceptrons</b> (MLP) and recurrent {{neural networks}} (RNN) fed with {{different sets of}} acoustic features are proposed for computing the presence and absence of speech in continuous speech signal in presence of various levels of background noise. Detailed performance evaluations on voice activity detection (VAD) are reported using the Aurora 2, Aurora 3 and TIMIT corpora. It is shown that the best results are obtained with an RNN fed by the acoustic features used for automatic speech recognition (ASR) augmented by specific features. Detailed evaluations are also proposed for ASR using Aurora 2 and the German, Italian and Spanish portions of the test set of the Aurora 3 corpus. The highest word error rate (WER) reduction (16. 9 %) is obtained when the only-noise presence probability is used to modify the phone posterior probabilities used for speech decoding. 1...|$|R
30|$|Back Propagation (BP) ANNs {{are more}} common than {{any other kind of}} network (Al Hazza and Adesta 2013) {{to be found in the}} {{relevant}} literature. Although this kind of models may be approached in many ways pertaining to the characteristics of the training procedure used, the problem of selecting the most suitable scheme is not addressed satisfactorily by the researchers. In this paper, an attempt is made to test several training BP algorithms to test their characteristics and suitability for the at hand problem. <b>Feed-forward</b> <b>perceptrons</b> are trained with three different back propagation algorithms, namely the adaptive back propagation algorithm of the steepest descent, the back propagation Levenberg–Marquardt algorithm and the back propagation Bayesian algorithm. This way, several models with different characteristics are built and tested and the optimum is selected. The analysis results indicate that the proposed model can be used to predict surface roughness in end milling with a less that 10  % error, even for tests with cutting conditions that were not used in the training of the system. Furthermore, Radial Basis Function (RBF) neural networks are tested with the same problem and the results are compared to the previous ones. A comparative study indicates the advantages and disadvantages of each approach. Furthermore, the models are used for the prediction of surface roughness in milling. The best models in terms of their performance are stored and can be used for the prediction of surface roughness of random input data, confined of course in the extremes of the input data used in the training of the models, but also 3 D surfaces are produced for the a priori determination and selection of optimum cutting conditions, based on experimental results and simulation output. It can be concluded that the proposed novel models prove to be successful, resulting in reliable predictions, therefore providing a possible way to avoid time- and money-consuming experiments.|$|R
40|$|AbstractIn this study, a {{comparative}} approach was made between {{artificial neural network}} (ANN) and response surface methodology (RSM) to predict the mass transfer parameters of osmotic dehydration of papaya. The effects of process variables such as temperature, osmotic solution concentration and agitation speed on water loss, weight reduction, and solid gain during osmotic dehydration were investigated using a three-level three-factor Box-Behnken experimental design. Same design was utilized to train a <b>feed-forward</b> multilayered <b>perceptron</b> (MLP) ANN with back-propagation algorithm. The predictive capabilities of the two methodologies were compared in terms of {{root mean square error}} (RMSE), mean absolute error (MAE), standard error of prediction (SEP), model predictive error (MPE), chi square statistic (χ 2), and coefficient of determination (R 2) based on the validation data set. The results showed that properly trained ANN model is found to be more accurate in prediction as compared to RSM model...|$|R
40|$|Artificial neural {{networks}} {{are suitable for}} the prediction of chaotic time series. A modified back-propagation algorithm with neuron splitting is used to train <b>feed-forward</b> multilayer <b>perceptron</b> networks for prediction. There are two ways of parallelizing: distributing the training set for batch learning or distribute the vector-matrix-operations for on-line training. Three implementation are compaired: PVM on a workstation cluster and Parix and the new PVM/Parix on a Transputer system. Results {{about the quality of}} forecasting an examplary time series and speedups of the parallel programs are presented. 1 Introduction Artificial {{neural networks}} (ANN) are suitable for the prediction of chaotic time series. They can approximate any function after an amount of training. Especially for prediction ANNs are an alternative to the classical methods. Today ANNs are already applied to the calculation of the demand for electrical power and to the forecasting of economic data [1], [2], [3]. ANNs [...] ...|$|R
40|$|Correct {{estimation}} of the scour around vertical piles in the field exposed to oscillatory waves {{is very important for}} many offshore structures and coastal engineering projects. Conventional predictive formulas for the geometric properties of scour hole, however, are not able to provide sufficiently accurate results. Artificial Neural Networks (ANNs) are simplified mathematical representation of the human brain. Three-layer normal feed-forward ANN is a powerful tool for input-output mapping and has been widely used in civil engineering problems. In this article the ANNs approach is used to predict the geometric properties of the scour around vertical pile. Two different ANNs including multilayer perceptron (with four different learning rules) and radial basis functions neural networks are used for this purpose. The results show that a three-layer normal <b>feed-forward</b> multilayer <b>perceptron</b> with quick propagation (QP) learning rule can predict the scour hole properties successfully. Key Words: ANN, Scour hole, Multilayer perceptron, Quick propagation...|$|R
40|$|In this study, a {{comparative}} approach was made between {{artificial neural network}} (ANN) and response surface methodology (RSM) to predict the mass transfer parameters of osmotic dehydration of papaya. The effects of process variables such as temperature, osmotic solution concentration and agitation speed on water loss, weight reduction, and solid gain during osmotic dehydration were investigated using a three-level three-factor Box-Behnken experimental design. Same design was utilized to train a <b>feed-forward</b> multilayered <b>perceptron</b> (MLP) ANN with back-propagation algorithm. The predictive capabilities of the two methodologies were compared in terms of {{root mean square error}} (RMSE), mean absolute error (MAE), standard error of prediction (SEP), model predictive error (MPE), chi square statistic (χ 2), and coefficient of determination (R 2) based on the validation data set. The results showed that properly trained ANN model is found to be more accurate in prediction as compared to RSM model...|$|R
40|$|This work aims to {{simulate}} potential scenarios in Rainfall-Runoff (R-R) transformation at daily scale, mainly perceived {{for the control}} and management of water resources, using <b>feed-forward</b> multilayer <b>perceptrons</b> (MLP) and, subsequently, Jordan Recurrent Neural Networks (JNN). R-R transformation {{is one of the}} most complex issue in hydrological environment due to high temporal and spatial variability, very strong and non linear interconnections among variables: a good challenge for Artificial Neural Networks (ANN). Abilities and limitations of MLP and JNN models have been investigated, especially focusing on drought periods where water resources management and control are particulary needed. The study compares the results of the two networks typologies to outputs from a conceptual linear model and then to physical context of two small Ligurian catchments. It also demonstrates the remarkable improvement obtained with the JNN approach especially when rainfall memory effect is employed as an additional input. 1...|$|R
40|$|The {{impact of}} reduced weight and output {{precision}} on the back-propagation training algorithm [Wer 74, RHW 86] is experimentally determined for a <b>feed-forward</b> multilayer <b>perceptron.</b> In contrast with previous such studies, {{the network is}} large with over 20, 000 weights, and is trained with a large, real-world data set of over 130, 000 patterns to perform a difficult task, that of phoneme classification for a continuous speech recognition system. The results indicate that 16 b weight values are sufficient to achieve training and classification results comparable to 32 b floating point, provided that weight and bias values are scaled separately, and that rounding rather than truncation is employed to reduce the precision of intermediary values. Output precision {{can be reduced to}} 8 bits without significant effects on performance. 1 Introduction Research into artificial neural networks (ANNs) is hindered by their extensive computational demands. However, these algorithms exhibit massiv [...] ...|$|R
40|$|This paper {{provides}} {{a brief overview}} on document analysis and recognition area, highlighting main steps and modules {{that are used to}} build recognition systems of the mentioned type. We underline basic workflow of such system down to the problem of single character recognition problem and highlighting possibilities and ways for artificial neural networks usage. Further we are conductinga formal comparison of abilities of printed characters recognition between two well known types of second generation neural networks, namely <b>feed-forward</b> back-propagation multilayer <b>perceptron</b> (MLP) and Kohonen self-organizing features map (SOM) ...|$|R
40|$|Downscaling global weather {{prediction}} model outputs to individual locations or local scales {{is a common}} practice for operational weather forecast in order to correct the model outputs at subgrid scales. This paper presents an empirical-statistical downscaling method for precipitation prediction which uses a <b>feed-forward</b> multilayer <b>perceptron</b> (MLP) neural network. The MLP architecture was optimized by considering physical bases that determine the circulation of atmospheric variables. Downscaled precipitation was then used as inputs to the super tank model (runoff model) for flood prediction. The case study was conducted for the Thu Bon River Basin, located in Central Vietnam. Study {{results showed that the}} precipitation predicted by MLP outperformed that directly obtained from model outputs or downscaled using multiple linear regression. Consequently, flood forecast based on the downscaled precipitation was very encouraging. It has demonstrated as a robust technology, simple to implement, reliable, and universal application for flood prediction through the combination of downscaling model and super tank model...|$|R
