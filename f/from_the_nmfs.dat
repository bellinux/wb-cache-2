15|10000|Public
25|$|Reeves, RR, Hofman, RJ, Silber, GK, and D Wilkinson (1996). Acoustic {{deterrence}} {{of harmful}} marine mammal-fishery interactions. Proceedings of a workshop held in Seattle, Washington, 20–22 March 1996. U.S. Department of Commerce, NOAA Technical Memorandum, NMFS-OPR-10 (unpublished). 70 pp. Available <b>from</b> <b>the</b> <b>NMFS</b> Office of Protected Resources, 1335 East/ West Highway, Silver Springs, MD. 20910, USA.|$|E
50|$|According to data {{compiled}} by the National Marine Fisheries Service, since 1983, the landings from U.S. fishing vessels of Atlantic wolffish as bycatch has declined 95%, landing 64.7 metric tons (mt) in 2007. In 1950, when the NMFS started collecting their data, 1,098 mt of Atlantic wolffish were landed, worth $137,008. In 1970, 271.2 mt; the landings peaked in 1983 at 1,207 mt, bringing in $455,291; rapidly depleting again and by 1990 the landings were down to 400 mt, and by 2002, 154 mt. The last available data <b>from</b> <b>the</b> <b>NMFS</b> were in 2007 at 64.7 mt, worth a total of $100,341.|$|E
50|$|The total {{project cost}} for the project is {{estimated}} at $83 million. According to the implementation agreement, CalAm will pay an amount equivalent to the estimated cost of buttressing the dam, or approximately $49 million. The Conservancy, with assistance <b>from</b> <b>the</b> <b>NMFS,</b> will secure the additional $34 million from state, federal, and private foundation sources. Construction of the project {{is expected to take}} three years - activities will be restricted to approximately April to November to avoid the rainy season and impact to migrating steelhead. During years two and three of construction, the Carmel River and San Clemente Creek will be diverted around the reservoir and dam site, and the reservoir will be emptied.|$|E
3000|$|In a {{previous}} study [18], we showed that <b>the</b> <b>NMF</b> components promise a higher representation and localization property {{compared to the other}} MD techniques. Therefore, <b>the</b> features extracted <b>from</b> <b>the</b> <b>NMF</b> component represent <b>the</b> TFM with a high-time and-frequency localization.|$|R
30|$|Pre-processing. All sound {{files are}} re-sampled to 16 kHz and treated as mono signal. The signals are {{analyzed}} by STFT with a Hann window, and a window size of 1024, leading to 513 frequency bins ranging from 0 – 8 kHz. The constant coefficient is removed <b>from</b> <b>the</b> <b>NMF</b> analysis and added for reconstruction in post-processing.|$|R
40|$|In this letter, {{we propose}} a new {{identification}} criterion that guarantees {{the recovery of}} the low-rank latent factors in the nonnegative matrix factorization (NMF) model, under mild conditions. Specifically, using the proposed criterion, it suffices to identify the latent factors if the rows of one factor are sufficiently scattered over the nonnegative orthant, while no structural assumption is imposed on the other factor except being full-rank. This is by far the mildest condition under which the latent factors are provably identifiable <b>from</b> <b>the</b> <b>NMF</b> model...|$|R
40|$|NOAA- 75091704 - 1, a {{microfilm}} {{of selected}} publications <b>from</b> <b>the</b> <b>NMFS</b> FPC bibliography, {{is for sale}} separately by National Technical Information Service as pt. 4 of the above title. Item 609 -C- 1 PB 245 345. Includes summary statement, NMFS FPC bibliography and selected abstracts. Contract DOC 3. 35189. Mode of access: Internet...|$|E
40|$|The {{focus of}} this chapter is on the Eastern Bering Sea (EBS) region. The Aleutian Islands region (Chapter 1 A) and the Bogoslof Island area (Chapter 1 B) are {{presented}} as separate sections. Changes in the input data The 2006 NMFS summer bottom-trawl survey (BTS) abundance at age estimates were computed and included for this assessment. Additionally, a time series of estimates was created that included two additional north-west strata. Previously, only the standard 6 strata were included since the additional strata were not covered in 1982 - 1984 and 1986. Another change to the BTS data included a correction on the assumed standard errors for the annual abundance estimates from 1982 - 1998. Estimates of pollock biomass from near the surface down to 3 m above the bottom {{were added to the}} assessment based on the 2006 echo-integration trawl (EIT) survey. Age composition estimates were derived from the population-at-length estimates using the 2006 BTS age-length key. Observer data for age and size composition and average weight-at-age were evaluated for the 2005 fishery and were included in the analyses. The catch-at-age data were recompiled for 1991 - 2005 to reflect a minor change in the timing of stratification (one stratum running till the end of May instead of the end of June). Total pollock catch for 2005 was estimated <b>from</b> <b>the</b> <b>NMFS</b> Alaska Region data. Th...|$|E
40|$|Relative to the November {{edition of}} last year's GOA SAFE report, the {{following}} substantive {{changes have been}} made in the Pacific cod stock assessment. Changes in the Input Data 1) Catch data for 2005 were updated, and preliminary catch data for 2006 were incorporated. 2) Size composition data from the 2005 commercial fisheries were updated, and preliminary size composition data from the 2006 commercial fisheries were incorporated. 3) Age composition data from the 2005 GOA bottom trawl survey were incorporated. 4) Parameters governing the length-at-age and weight-at-length relationships were re-estimated based on all available data <b>from</b> <b>the</b> <b>NMFS</b> bottom trawl survey time series. Changes in the Assessment Model Now that a sufficient number of age data are available to estimate parameters of the length-at-age schedule outside the model, the values of these parameters have been set equal to their point estimates rather than estimated internally in the stock assessment model (as was already the case with parameters governing the weight-at-length relationship). No other substantive changes were made to the assessment model. Changes in Assessment Results 1) The projected 2007 female spawning biomass is 127, 000 t, up about 9 % from last year=s projection for 2006. 2) The projected 2007 age 3 + biomass is 375, 000 t, up about 15 % from last year=s projection fo...|$|E
40|$|In this paper, a novel {{method for}} facial {{representation}} called Spatially Confined Non-Negative Matrix Factorization (SFNMF) is presented. SFNMF aims to extract more spatially confined, parts-based representation <b>from</b> <b>the</b> <b>NMF</b> based representation by merely removing non-prominent region, and focalize on the salient feature. SFNMF derived a significant set of basis which allows a non-subtractive representation {{of images and}} these bases manifest localized features. Experimental results are presented to compare SFNMF with NMF and Local NMF. Advantageous of SFNMF is demonstrated when SFNMF achieves highest verification rate among the other...|$|R
40|$|We {{introduce}} {{a novel approach}} for noise-robust feature extraction in speech recognition, based on non-negative matrix factorization (NMF). While NMF has previously been used for speech denoising and speaker separation, we directly extract time-varying features <b>from</b> <b>the</b> <b>NMF</b> output. To this end we extend basic unsupervised NMF to a hybrid supervised/unsupervised algorithm. We present a Dynamic Bayesian Network (DBN) architecture that can exploit these features in a Tandem manner together with the maximum likelihood phoneme estimate of a bidirectional long short-term memory (BLSTM) recurrent neural network. We show that addition of NMF features to spelling recognition systems can increase word accuracy by up to 7 % absolute in a noisy car environment...|$|R
40|$|In this paper, we {{test the}} use of Nonnegative Matrix Fac-torization (NMF) for feature {{extraction}} {{in the context of}} audio classification. NMF calculates a decomposition of the spectrogram into nonnegative factors and has been successfully applied to audio source separation. Thus, {{it has the potential to}} be robust to noise disturbances when used for feature calculation. We then introduce two fea-ture sets directly derived <b>from</b> <b>the</b> <b>NMF</b> decomposition. Experiments performed on an 8 -class speaker recognition task with Support Vector Machines show that the pro-posed representations convey complementary information to the baseline MFCC features. Indeed, {{the use of}} only the NMF-based descriptors lead to similar results as the refer-ence features, and the combination of these representations yields a significant improvement of the obtained accuracy. ...|$|R
40|$|Abstract. —In 1998, the {{barndoor}} skate Dipturus laevis {{was reported to}} have been locally extirpated in parts of its northern range and to be potentially on the brink of extinction. Managers were faced with assessing the species with virtually no information other than a limited number of individuals observed in annual groundfish surveys. Since that time, a number of the primary life history parameters have been estimated, but the population dynamics of the species remain largely unexplored. In this study, we use information from the National Marine Fisheries Service (NMFS) annual groundfish surveys to investigate two critical components of {{barndoor skate}} population dynamics: the relationship of recruitment to spawner abundance and the maximum population growth rate. A strong stock–recruitment relationship was found in the fall survey data, suggesting that recruitment is closely tied to spawner abundance. The Ricker and Beverton–Holt stock–recruit models were fitted to the survey data, and estimates of the slope at the origin was generated. These parameters provided an estimate of the maximum annual reproductive rate, which was then converted to an estimate of the instantaneous maximum population growth rate of 0. 37 – 0. 38 per year. A second analysis was also conducted using a Leslie matrix and data <b>from</b> <b>the</b> <b>NMFS</b> survey. Observed rates of population change were used to estimate early life history parameters and incorporate density dependence into the density-independent framework of a Leslie matrix demographic model. From this method, the instantaneous maximum populatio...|$|E
40|$|Natural {{resource}} agencies worldwide {{must develop}} species recovery plans that specify threats, propose targets required for recovery, {{and evaluate the}} extent to which habitat alteration and restoration may influence species decline and recovery. To evaluate the impacts of proposed habitat alterations on species of conservation concern, standardized protocols may be adopted even when supporting data are scarce. For example, a habitat matrix was developed by the National Marine Fisheries Service (NMFS) to guide consultations under the Endangered Species Act for actions that may affect the functioning of the freshwater habitat used by several federally listed salmonid species. The habitat matrix has also been advocated as a tool for recovery planning by agencies apart <b>from</b> <b>the</b> <b>NMFS,</b> who could use it to define the habitat conditions assumed to be necessary for salmonid population viability and hence recovery. This use of the habitat matrix in a recovery context has not been evaluated, and, despite its widespread use as a regulatory tool, the empirical relationships between many of the habitat matrix variables and salmonid populations remain unexplored. By amassing data on habitat assessments and trends in fish abundance, we empirically evaluate the relationship between habitat matrix scores and salmonid population metrics. We found that abundance trends for populations of three species of threatened and endangered salmonids (chinook, coho, and steelhead) were unrelated to these habitat matrix assessments. This study reveals the danger of assuming quantitative relationships between habitat and organism and cautions against co-opting protocols from the regulatory realm for recovery planning for endangered species...|$|E
40|$|Abstract only. With the {{publication}} of the NOAA Draft Policy on Catch Shares, which encourages US Management Councils to adopt Catch Share Programs (yet another new name for LAPs, ITQs, or IFQs) there will likely be more deliberations on such programs. With changes mandated in the revised Magnuson-Stevens Act and the suggestions made in several Congressional studies, this will require {{a closer look at the}} required elements of operational protocols to address excessive shares in plan amendments which institute quota shares. The emphasis on the word operational is intentional. The purpose of this paper is to introduce some of the topics which will need to be addressed. An important issue will be the difference between, and the requirements to address, market power issues and what may be called management objective excessive shares. It will be necessary to define a conceptually clear process between the desired policy objectives and the requirement to define excessive shares in terms of a percentage of quota shares owned (or controlled) by a single entity. With respect to market power this should be related to standard Justice Department norms related to the ability to increase market price. Another important issue will be the need to define a point where there is an implicit or explicit hand off <b>from</b> <b>the</b> <b>NMFS</b> Regional Offices and the Justice Department when intervention is required on market power issues. Two of the criteria for determining this point will be the differences in their legal mandates and types of in-house expertise...|$|E
40|$|This paper {{presents}} an {{improvement of the}} classical Non-negative Matrix Factorization (NMF) approach, for dealing with local representations of image objects. NMF, when applied to global data representations such as faces presents a high ability to represent local features of the original data in an unsupervised way. However, when applied to local representations NMF generates redundant basis. This work implements an improvement on <b>the</b> original <b>NMF</b> approach by incorporating prior knowledge {{in the form of}} a weight matrix extracted <b>from</b> <b>the</b> training data. A detailed mathematical description of the inclusion of this weight matrix is provided, and results demonstrating its advantages are included. Furthermore, <b>the</b> original <b>NMF</b> approach lacks a hierarchy of the elements of the estimated basis. A technique to determine an ordered set of discriminant basis is also presented. Finally, the effectiveness of the weighted approach with respect to the classical one is experimentally compared. This is done by implementing a clustering algorithm that automatically extracts object parts <b>from</b> <b>the</b> <b>NMF</b> representation of an image database corresponding to newspapers. 1...|$|R
3000|$|... [...]. The regularization terms due to two gamma bases are {{additionally}} considered. Different <b>from</b> <b>the</b> Bayesian <b>NMF</b> (BNMF) [15], BGS-NMF conducts group sparse learning {{which does}} not only characterize the within-segment harmonic information but also represent the across-segment rhythmic regularity. Sparse sets of basis vectors are further determined for sparse representation. Basically, BGS-NMF follows a general objective function. By applying different hyperparameter values [...]...|$|R
40|$|Abstract. Separating {{multiple}} music {{sources from}} a single channel mix-ture is a challenging problem. We present {{a new approach to}} this problem based on non-negative matrix factorization (NMF) and note classifica-tion, assuming that the instruments used to play the sound signals are known a priori. The spectrogram of the mixture signal is first decomposed into building components (musical notes) using an <b>NMF</b> algorithm. <b>The</b> Mel frequency cepstrum coefficients (MFCCs) of both the decomposed components and the signals in the training dataset are extracted. The mean squared errors (MSEs) between the MFCC feature space of the decomposed music component and those of the training signals are used as the similarity measures for the decomposed music notes. The notes are then labelled to the corresponding type of instruments by the K nearest neighbors (K-NN) classification algorithm based on the MSEs. Finally, the source signals are reconstructed <b>from</b> <b>the</b> classified notes and the weighting matrices obtained <b>from</b> <b>the</b> <b>NMF</b> algorithm. Simulations are provided to show the performance of the proposed system...|$|R
40|$|The TRAC was {{established}} in 1998 to peer review assessments of transboundary resources in the Georges Bank area and thus {{to ensure that the}} management efforts of both Canada and USA, pursued either independently or cooperatively, are founded on a common understanding of resource status. Prior to 2003, scientists from both countries had participated in each other’s peer review of the Gulf of Maine herring assessments but {{there has not been a}} joint peer review meeting. During the 10 - 14 February 2003 meeting, the TRAC considered the assessment framework for the Gulf of Maine herring assessments. At that meeting, consensus was reached on how to deal with the stock complex and management units. It was deemed necessary to undertake an evaluation of the entire complex with subsequent consideration of the individual components. Evaluation of the relative proportions of the biomass between the inshore Gulf of Maine and Georges Bank should be considered to give guidance for the individual components. It was also established that surveys indices <b>from</b> <b>the</b> <b>NMFS</b> bottom trawl survey would be divided into and treated as two distinct time periods pre- 1985 and 1985 present. Despite considerable investigation of model formulations for the assessment, a number of issues remained to be resolved. It was noted that to verify and compare models, new and revised data need to be considered. Thus a continuation of the hydroacoustic survey is likely to elucidate trends in biomass. Improved age determinations for older fish should give a better indication of the total mortality in the stock...|$|E
40|$|In March the Council adopted {{for public}} review {{a list of}} {{groundfish}} stocks to be assessed next year (Agenda Item F. 3. b, Attachment 1), which {{will be used to}} decide the harvest specifications and management measures for 2009 and 2010 groundfish fisheries. As part of that decision, the Council announced that the yelloweye rockfish assessment, which is tentatively scheduled to be an updated assessment, may ultimately be scheduled as a full assessment. At this meeting, the Council should consider advice <b>from</b> <b>the</b> <b>NMFS</b> science centers, advisory bodies, and the public before deciding the list of groundfish stock assessments to be assessed next year. Dr. Elizabeth Clarke, Division Director at the NMFS Northwest Fisheries Science Center (NWFSC), proposed a schedule of 2007 Stock Assessment Review (STAR) panels (Agenda Item F. 3. b, Attachment 2) in her briefing to the Council. The Council is tasked to give guidance to the NWFSC on the proposed 2007 STAR Panel schedule after receiving advice from NMFS science centers, advisory bodies, and the public. The Council also adopted for public review a draft stock assessment Terms of Reference (Agenda Item F. 3. c, Attachment 1) with a request for focused input from the public and council advisory bodies on the concept of a more formal role for representatives from the Groundfish Management Team (GMT) and Groundfish Advisory Subpanel (GAP) at future STAR Panels. Dr. Martin Dorn, the SSC’s Groundfish Subcommittee chair, is scheduled to report on the draft Stock Assessment Terms of Reference and may provide a new revised Terms of Reference as a supplemental attachment. The Council is tasked at this meeting with final adoption of a list of groundfish stocks to be assessed next year, including full and updated assessments; a schedule of STAR Panels to review new full assessments (the Scientific and Statistical Committee will review updated assessments) ...|$|E
40|$|Under the 1994 {{amendments}} to the Marine Mammal Protection Act, the National Marine Fisheries Service (NMFS) and the U. S. Fish and Wildlife Service (USFWS) were required to produce stock assessment reports for all marine mammal stocks in waters within the U. S. Exclusive Economic Zone. This document contains the stock assessment reports for the U. S. Pacific marine mammal stocks under NMFS jurisdiction. Marine mammal species which are under the management jurisdiction of the USFWS {{are not included in}} this report. A separate report containing background, guidelines for preparation, and. a summary of all stock assessment reports is available <b>from</b> <b>the</b> <b>NMFS</b> Office of Protected Resources. This report was prepared by staff of the Southwest Fisheries Science Center, NMFS and the Alaska Fisheries Science Center, NMFS. The information presented here was compiled primarily from published sources, but additional unpublished information was included where it contributed to the assessments. The authors wish to thanks the members of the Pacific Scientific Review Group for their valuable contributions and constructive criticism: Hannah Bernard, Robin Brown, Mark Fraker, Doyle Hanan, John Heyning, Steve Jeffries, Katherine Ralls, Michael Scott, and Terry Wright. Their comments greatly improved the quality of these reports, We also thanks the Marine Mammal Commission, The Humane Society of the United States, The Marine Mammal Center, The Center for Marine Conservation, and Friends of the Sea Otter for their careful reviews and thoughtful comments. Special thanks to Paul Wade of the Office of Protected Resources for his exhaustive review and comments, which greatly enhanced the consistency and technical quality of the reports. Any ommissions or errors are the sole responsibility of the authors. This is a working document and individual stock assessment reports will be updated as new information becomes available and as changes to marine mammal stocks and fisheries occur; therefore, each stock assessment report is intended to be a stand alone document. The authors solicit any new information or comments which would improve future stock assessment reports. This is Southwest Fisheries Science Center Technical Memorandum NOAA-TM-NMFS-SWFSC- 219, July 1995. 11...|$|E
30|$|We can {{see from}} this figure that as the block size increases, {{the ratio of the}} public pixels increases. When the block size is 100 × 100, the {{algorithm}} with the new blocking strategy obtains the best robustness performance (the ratio of the public pixels is over 89 %). It is worth noting that the robustness performance in the block size of 40 × 40 is better than that of 60 × 60, which can be explained as follows. Though the larger block size can increase the ratio of public pixels for the rotational robustness improvement, in the testing we have applied the same rank for NMF decomposition {{in such a way that}} the bigger the block size is, the more feature information extracted <b>from</b> <b>the</b> <b>NMF</b> processing operation will be lost. As a result, the uniqueness will be reduced. Therefore, a rational block size is a trade-off between the robustness and uniqueness. For NMF-based hashing algorithm, we propose to apply the blocks of size 100 × 100 for hashing.|$|R
40|$|Variations of the ionospheric Total Electron Content (TEC) over China are {{investigated}} {{using the}} TEC {{data obtained from}} China Crustal Movement Observation Network in the year 2004. The results show a single-peak occurred in post-noon for the diurnal variation and two peaks exit around two equinox points, respectively, for the seasonal variation. Overall, the values of TEC increased <b>from</b> <b>the</b> north {{to the south of}} China. There were small but clear longitudinal differences in both sides of the longitudes with zero magnetic declination. The intensity of the day-to-day variation of TEC was not a monotonic change along the latitudes. It was usually weaker in the middle of China than that in the north or south. Comparing with the maximum F-layer electron density (<b>NmF</b> 2) derived <b>from</b> <b>the</b> ionosonde stations in China, it is found that the day-to-day variation of TEC was less significant than that of NmF 2, and that the northern crest of the equatorial anomaly identified <b>from</b> <b>the</b> <b>NmF</b> 2 data can reach Guangzhou-region. While, the TEC crest was hardly observed in the same location. This is probably caused by the tilt of topside ionosphere near the northern anomaly crest region at lower latitudes...|$|R
40|$|Nonnegative Matrix Factorization (NMF) {{has been}} proven to be {{effective}} in text mining. However, since NMF is a well-known unsupervised components analysis tech-nique, <b>the</b> existing <b>NMF</b> method can not deal with prior constraints, which are beneficial to clustering or classifi-cation tasks. In this paper, we address the text clustering problem via a novel strategy, called Pairwise Constraints-guided Non-negative Matrix Factorization (PCNMF for short). Differing <b>from</b> <b>the</b> traditional <b>NMF</b> method, <b>the</b> pro-posed method can capture the available abundance prior constraints in original space, which result in more effec-tive for clustering or information retrieval. Therefore, PC-NMF enforces the discriminative capability in the reduced space. Utilizing the appropriate transformation, PCNMF represents as a new optimization problem, which can be ef-ficiently solved by an iterative approach. The cluster mem-bership of each document can be easily determined as <b>the</b> standard <b>NMF.</b> Empirical studies based on Benchmark doc-ument corpus demonstrate appealing results. ...|$|R
40|$|The United States District Court for the District of Massachusetts {{recently}} {{approved the}} continuing {{construction of a}} 310 million tunnel and sewage outfall project in Boston Harbor. The {{court held that the}} Environmental Protection Agency (EPA), the U. S. Army Corps of Engineers, and the National Marine Fisheries Service (NMFS), did not violate the Endangered Species Act (ESA), by approving the construction of the outfall tunnel. The court specifically found that the evidence failed to show any harm to endangered species, that the environmental impact statements issued by the EPA were sufficient to establish compliance with the requirements of the ESA, and that the ESA does not require a biological opinion <b>from</b> <b>the</b> <b>NMFS</b> prior to the commencement of any construction. Judge Mazzone was the presiding judge, having overseen the outfall project since the court had ordered the cleanup of Boston Harbor in 1985, pursuant to the Clean Water Act. This case provides an example of the judicial confusion over the relationship between the EPA in approving such projects and the NMFS as the agency in charge of implementing the ESA. Judge Mazzone found that halting construction while the NMFS completed its biological opinion would be an 2 ̆ 2 exaltation of form over substance. 2 ̆ 2 Clearly, his position varies substantially from that of Judge Celebrezze, who wrote the circuit court opinion in the famous case of Hill v. Tennessee Valley Authority. In that case, the court stated that the procedural and substantive requirements of the ESA should be enforced strictly because an error on the side of permissiveness could possibly result in the extinction of a species. The major substantive issue presented in this case is whether the EPA has complied with the mandate of the ESA by allowing construction to proceed without a final biological opinion from the Department of Commerce. It is surprising that the Browner court believed that such compliance was unnecessary when the consequences of proceeding in such a manner are so significant. Vast public resources are invested in these kinds of projects which require federal funding and approval. These funds are at risk if a biological opinion is issued after the onset of construction which requires the discontinuation of the project because it would endanger the existence of a species. In addition, an error in judgment can be irreversible when dealing with a species that is already at risk of extinction. In many cases there are no second chances. Therefore, the protections that Congress has given to endangered species, through the procedures set forth in the Endangered Species Act, should be strictly complied with, even when it appears that following such procedure is merely exalting form over substance. By focusing on the procedural and substantive requirements of the ESA, and exploring the various judicial interpretations of those requirements, this Note will explain why the Browner court erred in its decision. It is imperative that the judicial system mandate strict compliance with the ESAs interagency cooperation requirements. By requiring federal agencies to cooperate under set procedures, Congress affirmatively established a system of checks and balances, affording endangered species the best possible protection. Strict compliance with these procedures allows the nation to grow and develop with minimal economic loss, while providing endangered species the protection that scientific and technological advancements can offer...|$|E
40|$|Thesis (Master's) [...] University of Washington, 2013 The {{excess of}} {{anthropogenic}} carbon dioxide (CO 2) produced since {{the industrial revolution}} is being absorbed by the oceans through the carbon cycle. Atmospheric carbon dioxide has increased about 40 % since the preindustrial era, and the oceans have absorbed {{more than a third}} of these emissions. This has led to the release of H+ ions via seawater carbonate chemistry, and hence to a reduction in ocean pH, that is, ocean acidification. Ocean pH has been reduced by roughly 0. 1 units, which is equivalent to an increase in H+ of roughly 30 %, and about a 16 % decrease in. Corrosive waters, the waters below the CaCO 3 saturation horizon, are predicted to reach shallower depths more in the Northeast Pacific Ocean than in any other ocean basin. The saturation horizon is projected to reach the surface of the North Pacific Ocean during this century, and some regions of the Bering Sea are predicted to become carbon shell corrosive seasonally by the middle of this century, which will expose a wide range of North Pacific species, including Bristol Bay Red king crab, to corrosive waters. Bristol Bay Red king crab has been one of the most valuable fished stocks in the US. It is managed by the State of Alaska under federal guidelines defined in the Fishery Management Plan (FMP) for crab in the Bering Sea and Aleutian Islands. Current management rules are designed to handle short-term fluctuations in stock abundance mainly due to exploitation. The impact of ocean acidification on red king crab is predicted to lead to long-term changes to stock abundance, and for which management is currently unprepared. This thesis explores the impact of ocean acidification on recruitment and yield of Bristol Bay red king crab under a range of ocean acidification scenarios and management strategies. The management strategies include setting the exploitation rate for the directed fishery to that under the overfishing limit (OFL) rule, applying constant exploitation rates, and setting exploitation rates that maximize catch and discounted profit. Trends in recruitment to the first size-class in the stock assessment model are estimated using a pre-recruit model in which survival is parameterized based on experimental results <b>from</b> <b>the</b> <b>NMFS</b> Kodiak laboratory. Exploitation rates are estimated, and time series (2000 - 2100) for MMB, catch, and discounted profit are projected, for each management strategy for three levels of variable fishery costs and for the economic discount factor. The catch, biomass, and discounted profit equilibrate at non-zero values for the no-OA scenario, but are driven to zero for all exploitation rates in the OA scenarios. Lower constant exploitation rates lead to a longer time before the biomass is driven close to zero, but the total discounted profit is highest at the highest exploitation rate for the three OA scenarios. The OFL control rule performs better than the constant exploitation rate strategies in terms of conserving the resource, because this rule closes the fishery at low biomass levels, which are also unprofitable. Estimated total discounted profits for the strategies which maximize catch and discounted profit are about the same for the base no-OA scenario, while the strategy that maximizes profit leads to slightly higher discounted profit and it depletes the stock below the biomass threshold sooner than the strategy which maximizes catch. Catches are the same for the strategies which maximize catch for no-OA scenario, and are higher for the strategy which maximizes catch for the OA scenarios. Higher discount rates lead to higher biomasses and catches, and the fishery is closed earlier for higher costs (food, fuel, and bait costs) for the OA scenarios when exploitation rates are selected to maximize profit...|$|E
40|$|In the Anadromous Fish Appendix of the US Army Corp of Engineers (USACE) Environmental Impact Statement on the Lower Snake River Hydrosystem Alternatives for {{recovery}} of Snake River salmon and steelhead (hereafter {{referred to as}} 2 ̆ 2 A-Fish 2 ̆ 2), the National Marine Fisheries Service (NMFS) suggested that transportation effectiveness of spring/summer chinook may have improved markedly in recent years. The NMFS conclusion was based on estimates of 2 ̆ 7 D 2 ̆ 7 -values (the differential delayed survival rate between transported fish and fish that migrated in-river) for 1994 - 1995 (NMFS, 1999). NMFS suggested, if 2 ̆ 7 D 2 ̆ 7 is high (estimated in A-Fish at 0. 8) and extra mortality of in-river and transported smolts is unrelated to the hydropower system, transportation options may meet recovery standards as well or better than natural river options. NMFS also suggested that further studies could reduce the uncertainty about true values of 2 ̆ 7 D 2 ̆ 7 and provide greater confidence {{to make a decision}} on the alternative management action needed to recover listed Snake River salmon and steelhead. In this analysis, we demonstrate that the evidence is compatible {{with a wide range of}} ‘D’ values, but only a small portion of this distribution is as high as the A-Fish estimate. We also present evidence that the extra mortality of in-river fish is related to the hydrosystem. We analyzed a suite of plausible assumptions used in the calculation of 2 ̆ 7 D 2 ̆ 7. Based on our analysis of the 1994 - 1996 PIT-tag data, there is a wide range of possible 2 ̆ 7 D 2 ̆ 7 -values. The NMFS 2 ̆ 7 estimate falls at the upper end of this distribution (90 th – 95 th percentiles). Alternative 2 ̆ 7 D 2 ̆ 7 -values, based on what we believe to be more reasonable assumptions, were closer to 0. 48. Because 2 ̆ 7 D 2 ̆ 7 is a modeled value (and not a measurement, as implied in the A-Fish), it is very sensitive to the suite of assumptions made and how the data are grouped. 2 ̆ 7 D 2 ̆ 7 estimates were most sensitive to: (1) whether or not fish that were transported from downstream collection/transport sites (Lower Monumental (LMO) and McNary (MCN) dams) were included in the group of fish used to estimate transport smolt to adult return rates (SAR); and (2) how reach survival rate estimates were extrapolated down to Bonneville Dam (BON). In 1994 the ‘D’-value estimated using four collection projects was much lower than two collection projects. However, in 1995 and 1996 the difference in ‘D’ using two and four collection projects was not as dramatic as in 1994. Therefore, the estimated high ‘D’-values are mainly driven by this single assumption for one year. Based on past and proposed future transportation operations, it is unclear why fish transported at the lower two projects were excluded <b>from</b> <b>the</b> <b>NMFS</b> analysis. Transported fish are subjected to stress, injury, and crowding at the collection projects. In addition, the physiological state of fish may be poorly synchronized with the time of saltwater entry for transported fish. These factors could explain the higher delayed mortality experienced by transported fish as suggested by a consistently estimated ‘D’ value that is less than 1. We disagree with the NMFS assertion that “ongoing direct experiments that contrast the return rates of tagged fish that pass through the hydrosystem versus the return rates of transported fish can resolve this question in a clear and unambiguous manner”. While a few components of the 2 ̆ 7 D 2 ̆ 7 -value estimate are measurable, the sensitivity analysis highlights differences in assumptionsand uncertainties that are not likely resolvable in the near term. In addition, low numbers of returning adults and small numbers of smolts for wild spring/summer chinook salmon may hamper reducing the uncertainty in estimates for reach survival rates and SARs for a non-detected group. Therefore, data are unlikely to perfect our understanding of 2 ̆ 7 D 2 ̆ 7 or eliminate the uncertainty in the most influential assumptions. The hypothesis of extra or delayed mortality due to hydrosystem passage has an empirical basis, as well as biological rationale. Based on recent PIT tag data we also found evidence that delayed mortality of both in-river and transported smolts was related to hydropower. More specifically, the evidence suggests that, at least for collected and bypassed smolts, there is a difference between the patterns of direct passage survival rates and SARs. Smolts first detected and transported from the downstream projects (LMO and MCN) had lower SARs than smolts collected and transported from higher up in the system. Similarly (as reported in the A-Fish), SARs of in-river smolts decreased as the number of times the fish were collected and bypassed increased. These pieces of information provide evidence that the Snake River spring summer chinook extra mortality is related to the juvenile migration hydrosystem experience. Based on results from life-cycle modeling (Marmorek and Peters 1998 b), transport based management options lead to a high likelihood of recovery only when ‘D’ is high and the source of extra mortality is not related to the experience during hydrosystem passage. However, when extra mortality is hydrosystem related (which our analysis supports), the natural river options are still the most likely management action to recover these stocks, even if ‘D’ is high (which our analysis does not support). Simply studying ‘D’, if that were possible, without determining the source of extra mortality, yields little additional insight into effects of the different management actions on Snake River spring/summer chinook recovery. Given the dangerously low level ofthese populations, we do not believe it is prudent to make management decision on the configuration and operation of the Snake and Columbia hydrosystem for the next 5 - 20 years (i. e. delaying a decision preserves status quo configuration), based solely on one optimistic assumption about the effectiveness of past and current hydrosystem operations...|$|E
40|$|Non-negative matrix {{factorization}} (NMF) {{has been}} widely used for challenging single-channel audio source separation tasks. However, inference in NMF-based models relies on iterative inference methods, typically formulated as multiplicative updates. We propose ”deep NMF”, a novel non-negative deep network architecture which results <b>from</b> unfolding <b>the</b> <b>NMF</b> it-erations and untying its parameters. This architecture can be discriminatively trained for optimal separation performance. To optimize its non-negative parameters, we show how {{a new form of}} back-propagation, based on multiplicative updates, can be used to preserve non-negativity, without the need for constrained optimization. We show on a challenging speech separation task that deep NMF improves in terms of accuracy upon NMF and is competitive with conventional sigmoid deep neural networks, while requiring a tenth of the number of parameters...|$|R
40|$|Nonnegative matrix {{factorization}} (NMF), {{which aims}} at obtaining the nonnegative low-dimensional representation of data, has received wide attention. To obtain more effective nonnegative discriminant bases <b>from</b> <b>the</b> original <b>NMF,</b> in this paper, a novel method called nonnegative discriminant matrix factorization (NDMF) is proposed for image classification. NDMF integrates the nonnegative constraint, orthogonality, and discriminant {{information in the}} objective function. NDMF considers the incoherent information of both factors in standard NMF and is proposed to enhance the discriminant ability of the learned base matrix. NDMF projects the low-dimensional representation of the subspace of the base matrix to regularize <b>the</b> <b>NMF</b> for discriminant subspace learning. Based on the Euclidean distance metric and the generalized Kullback-Leibler (KL) divergence, two kinds of iterative algorithms are presented to solve the optimization problem. The between-and within-class scatter matrices are divided into positive and negative parts for the update rules and the proofs of the convergence are also presented. Extensive experimental results demonstrate {{the effectiveness of the}} proposed method in comparison with <b>the</b> state-of-the-art discriminant <b>NMF</b> algorithms. </p...|$|R
40|$|NMF and PLSI are two {{state-of-the-art}} {{unsupervised learning}} models in data mining, {{and both are}} widely used in many applications. References have shown <b>the</b> equivalence between <b>NMF</b> and PLSI under some conditions. However, a new issue arises here: why can they result in different solutions since they are equivalent? or in other words, their algorithm differences are not studied intensively yet. In this note, we explicitly give the algorithm differences between PLSI and NMF. Importantly, we find that even if starting <b>from</b> <b>the</b> same initializations, <b>NMF</b> and PLSI may converge to different local solutions, and the differences between them are born in the additional constraints in PLSI though NMF and PLSI optimize the same objective function...|$|R
40|$|Nonnegative matrix {{factorization}} (NMF) is {{a useful}} dimension reduction method that has been investigated and applied in various areas. NMF is considered for high-dimensional data in which each element has a nonnegative value, and it provides a low-rank approximation formed by factors whose elements are also nonnegative. The nonnegativity constraints imposed on the low-rank factors not only enable natural interpretation but also reveal the hidden structure of data. Extending <b>the</b> benefits of <b>NMF</b> to multidimensional arrays, nonnegative tensor factorization (NTF) {{has been shown to}} be successful in analyzing complicated data sets. Despite <b>the</b> success, <b>NMF</b> and NTF have been actively developed only in the recent decade, and algorithmic strategies for computing NMF and NTF have not been fully studied. In this thesis, computational challenges regarding NMF, NTF, and related least squares problems are addressed. First, efficient algorithms of NMF and NTF are investigated based on a connection <b>from</b> <b>the</b> <b>NMF</b> and <b>the</b> NTF problems to the nonnegativity-constrained least squares (NLS) problems. A key strategy is to observe typical structure of the NLS problems arising in <b>the</b> <b>NMF</b> and <b>the</b> NTF computation and design a fast algorithm utilizing the structure. We propose an accelerated block principal pivoting method to solve the NLS problems, thereby significantly speeding up <b>the</b> <b>NMF</b> and NTF computation. Implementation results with synthetic and real-world data sets validate the efficiency of the proposed method. In addition, a theoretical result on the classical active-set method for rank-deficient NLS problems is presented. Although the block principal pivoting method appears generally more efficient than the active-set method for the NLS problems, it is not applicable for rank-deficient cases. We show that the active-set method with a proper starting vector can actually solve the rank-deficient NLS problems without ever running into rank-deficient least squares problems during iterations. Going beyond the NLS problems, it is presented that a block principal pivoting strategy can also be applied to the l 1 -regularized linear regression. The l 1 -regularized linear regression, also known as the Lasso, has been very popular due to its ability to promote sparse solutions. Solving this problem is difficult because the l 1 -regularization term is not differentiable. A block principal pivoting method and its variant, which overcome a limitation of previous active-set methods, are proposed for this problem with successful experimental results. Finally, a group-sparsity regularization method for NMF is presented. A recent challenge in data analysis for science and engineering is that data are often represented in a structured way. In particular, many data mining tasks have to deal with group-structured prior information, where features or data items are organized into groups. Motivated by an observation that features or data items that belong to a group are expected to share the same sparsity pattern in their latent factor representations, We propose mixed-norm regularization to promote group-level sparsity. Efficient convex optimization methods for dealing with the regularization terms are presented along with computational comparisons between them. Application examples of the proposed method in factor recovery, semi-supervised clustering, and multilingual text analysis are presented. PhDCommittee Chair: Park, Haesun; Committee Member: Gray, Alexander; Committee Member: Lebanon, Guy; Committee Member: Monteiro, Renato; Committee Member: Zha, Hongyua...|$|R
40|$|We {{investigate}} a semi-automated identification of technical problems occurred by armed forces weapon systems during mission of war. The proposed methodology {{is based on}} a semantic analysis of textual information in reports from soldiers (war logs). Latent semantic indexing (LSI) with non-negative matrix factorization (NMF) as technique from multivariate analysis and linear algebra is used to extract hidden semantic textual patterns <b>from</b> <b>the</b> reports. <b>NMF</b> factorizes <b>the</b> term-by-war log matrix - that consists of weighted term frequencies – into two non-negative matrices. This enables natural parts-based representation of the report information and it leads to an easy evaluation by human experts because human brain also uses parts-based representation. For an improved research and technology planning, the identified technical problems are a valuable source of information. A case study extracts technical problems from military logs of the Afghanistan war. Results are compared to a manual analysis written by journalists of "Der Spiegel"...|$|R
40|$|Nonnegative matrix {{factorization}} (NMF) is {{a popular}} tool for analyzing the latent structure of nonnegative data. For a positive pairwise similarity matrix, symmetric NMF (SNMF) and weighted NMF (WNMF) {{can be used to}} cluster the data. However, both of them are not very efficient for the ill-structured pairwise similarity matrix. In this paper, a novel model, called relationship matrix nonnegative decomposition (RMND), is proposed to discover the latent clustering structure <b>from</b> <b>the</b> pairwise similarity matrix. The RMND model is derived <b>from</b> <b>the</b> nonlinear <b>NMF</b> algorithm. RMND decomposes a pairwise similarity matrix into a product of three low rank nonnegative matrices. The pairwise similarity matrix is represented as a transformation of a positive semidefinite matrix which pops out the latent clustering structure. We develop a learning procedure based on multiplicative update rules and steepest descent method to calculate the nonnegative solution of RMND. Experimental results in four different databases show that the proposed RMND approach achieves higher clustering accuracy...|$|R
30|$|Minimum average RSSI threshold: this {{information}} is gathered by <b>the</b> <b>NMF.</b> By using <b>the</b> <b>NMF,</b> we automatically include RSSI information from duplicate packets received from this neighbour.|$|R
40|$|In this paper, a novel graph-preserving sparse non-negative matrix {{factorization}} (GSNMF) {{algorithm is}} proposed for facial expression recognition. The GSNMF algorithm is derived <b>from</b> <b>the</b> original <b>NMF</b> algorithm by exploiting both sparse and graph-preserving properties. The latter may contain the class information of the samples. Therefore, GSNMF can be conducted as an unsupervised or a supervised dimension reduction method. A sparse representation of the facial images is obtained by minimizing the l 1 -norm of the basis images. Furthermore, according to graph embedding theory, the neighborhood of the samples is preserved by retaining the graph structure in the mapped space. The GSNMF decomposition transforms the high-dimensional facial expression images into a locality-preserving subspace with sparse representation. To guarantee convergence, we use the projected gradient method to calculate the non-negative solution of GSNMF. Experiments are conducted on the JAFFE database and the Cohn-Kanade database with not-occluded and partially occluded facial images. The {{results show that the}} GSNMF algorithm provides better facial representations and achieves higher recognition rates than NMF. Moreover, GSNMF is also more robust to partial occlusions than other tested methods...|$|R
3000|$|For <b>the</b> <b>NMF</b> window length, {{we chose}} T= 10 frames, which offered a good balance between {{dictionary}} complexity and ASR performance. The length of <b>the</b> <b>NMF</b> R matrix initialization filter that functions as an {{upper bound on}} the reverberation time the update algorithm can handle was set to T [...]...|$|R
