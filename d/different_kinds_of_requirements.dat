16|10000|Public
40|$|This is {{an outline}} of a {{conceptual}} framework for architecting complex software. The framework identifies multiple independent structures of software that support <b>different</b> <b>kinds</b> <b>of</b> <b>requirements,</b> making possible to partition requirements so that each group can be supported by a different architectural structure. Architectural views are separated from architectural structures {{making it easier to}} define different processes and allocate different development stages for design and description of architectural structures and views. 1. 1 Keywords Requirements, software architecture, architectural views, structures of software 2...|$|E
40|$|Modern XML-based {{technologies}} as well {{as current}} busi- ness trends enable a move to service-oriented computing, in which functionality previously offered by application soft- ware, is now offered by the infrastructure. We argue that this enforces {{a new approach to}} requirements engineering for services, and that two <b>different</b> <b>kinds</b> <b>of</b> <b>requirements</b> engi- neering will emerge: requirements engineering for service consumers and requirements engineering for service provi- ders. For the latter kind, we propose service blueprinting, which originated in marketing, as a good source to develop a requirements engineering approach to serviced-oriented computing...|$|E
40|$|The {{determination}} of the motor and battery specification {{is one of the}} major challenges in the electric vehicle designs, specially considering the <b>different</b> <b>kinds</b> <b>of</b> <b>requirements</b> and specifications in different designs. This paper discusses about a method of simulating a vehicle in given terrains and driver models for extracting the torque, power and battery usage; and hence get the most optimized set of parameters for the given design requirement. The simulation takes the driver as a fuzzy model and the overall vehicle as a non-linear model for calculating the motor and battery requirements according to the terrain, driver commands and traffic condition...|$|E
40|$|Cluster {{computing}} {{and multimedia}} {{are two different}} classes of applications emerging in recent general computing development. However, such application poses <b>different</b> <b>kind</b> <b>of</b> <b>requirement</b> on processing power where traditional time-sharing process schedulers could not deliver. Processing power delivery in such scheduler is not guaranteed neither in terms of amount nor timing. To cope with this, a process scheduler with guaranteed service has to be employed. In this paper, we will present {{the design of a}} guaranteed process scheduler by adapting the concept of contractual computing. Department of Electronic and Information EngineeringRefereed conference pape...|$|R
40|$|In {{this paper}} we {{identify}} several {{areas in which}} the satisfaction <b>of</b> security <b>requirements</b> can affect the cost and performance of a system, and describe {{what is known about}} tradeoffs in these areas. We also show where these tradeoffs appear in the life cycle of a system, and show how they are affected by <b>different</b> <b>kinds</b> <b>of</b> security <b>requirements...</b>|$|R
40|$|This paper {{introduces}} a new <b>kind</b> <b>of</b> Java-server, the Smart Data Server (SDS). It offers the accessing of data with function-calls to SDS-networks. This allows {{the building of}} lean clients with the shifting of functionality to the SDS-network. The server builds the middle-tier in a multi-tier architecture (Client - SDS-Network - Database). It offers a frame for easy-to-build environment-independent SDS-modules with multiple functionality. It is modular and easily scalable for <b>different</b> <b>kinds</b> <b>of</b> <b>requirement.</b> 1. Introduction The Internet has witnessed some major steps of evolution in its history. The biggest step was carried out {{with the introduction of}} the World Wide Web (WWW). To access data via the WWW, a web browser asks an HTTP server for a document. This document may either be a static document that never changes its contends or may be generated dynamically, using CGI-Scripts. With CGI a user can transfer data to the server. The server uses CGI programs to manage the data a [...] ...|$|R
40|$|Abstract — Safety {{systems in}} nuclear {{industry}} must conform to an increasing set of regulatory requirements. These requirements are scattered throughout multiple documents expressing {{different levels of}} requirements or <b>different</b> <b>kinds</b> <b>of</b> <b>requirements.</b> Consequently, when licensees want to extract the set of regulations related to a specific concern, they lack explicit traces between all regulation documents and mostly get lost while attempting to compare two different regulatory corpora. This paper presents the regulatory landscape {{in the context of}} digital Instrumentation and Command systems in nuclear power plants. To cope with this complexity, we define and discuss challenges toward an approach based on information retrieval techniques to first narrow the regulatory research space into themes and then assist the recovery of these traceability links...|$|E
40|$|Scheduling theory {{generally}} {{assumes that}} real-time systems are mostly composed of activities with hard real-time requirements. Many systems are built today by composing different applications or {{components in the}} same system, leading to a mixture of many <b>different</b> <b>kinds</b> <b>of</b> <b>requirements</b> with small parts of the system having hard real-time requirements and other larger parts with requirements for more flexible scheduling and for quality of service. Hard real-time scheduling techniques are extremely pessimistic for {{the latter part of}} the application, and consequently it is necessary to use techniques that let the system resources be fully utilized to achieve the highest possible quality. This paper presents a framework for a scheduling architecture that provides the ability to compose several applications or component...|$|E
40|$|Abstract — In complex {{industrial}} projects, {{textual information}} remains the main vector of {{information at the}} project level. Consequently, requirements are scattered throughout multiple documents expressing different levels of requirements and <b>different</b> <b>kinds</b> <b>of</b> <b>requirements.</b> Formalizing this information and tracing different relationships among documents and organizing this environment present a challenging question. Domain-specific modeling and traceability modeling are Model-Driven Engineering (MDE) techniques that could address various aspects of requirements formalization. Text-based high level requirements can be formalized as document concepts can be gathered and represented. Still, relationships cannot always be determined using sole MDE approaches and, as a consequence, relationships and traceability issue remains. Information retrieval (IR) approaches have already proved {{to work in an}} efficient way on large text corpora for requirements traceability analysis but do only consider similarity aspects of flatten documents, losing their organization and hierarchy. This paper aims to introduce how a combined use of both MDE and IR can lead to improved requirements organization and traceability while handling textual ambiguous requirements documents...|$|E
50|$|A {{variable}} geometry turbomachine {{is basically}} a combination of different fixed geometry turbomachines, which uses movable valves to optimize the efficiency at various service requirements. Generally, due to <b>different</b> <b>kinds</b> <b>of</b> service <b>requirements</b> many <b>different</b> <b>kinds</b> <b>of</b> pump (compressor) or turbine are used. And for a particular type <b>of</b> operating <b>requirements</b> we require specific type of pump or turbine, which will provide optimum conditions of operation. Thus a variable geometry turbomachine adjusts to different service requirements.|$|R
40|$|This paper {{describes}} a useful set of NLP tools {{which has been}} successfully applied to many <b>different</b> <b>kinds</b> <b>of</b> industrial <b>requirements</b> spanning muitiple domains and applications at Boeing. The tools can be combined to constitute a full-spectrum natural language system and can be customized for new domains relatively easily. To date, this array of formal and natural language processing technologies {{has been used to}} perform mass changes to legacy textual databases and to facilitate user interfacing to relational databases and software applications...|$|R
40|$|National audienceThis paper proposes an {{instrumented}} {{approach to}} integrate feature diagrams with UML models, via UML proﬁles and a Rational Software Architect plugin. The concrete contribution is the speciﬁcation {{of a new}} UML proﬁle based upon a meta-model synthesising existing feature diagrams semantics, and a Rational Software Architect (RSA) implementation. Our RSA implementation makes possible to link feature diagrams with UML model artefacts. Indeed, it allows traceability between feature models and other <b>different</b> <b>kinds</b> <b>of</b> models (<b>requirements,</b> class diagrams, sequences or activity diagrams, etc.) ...|$|R
40|$|Design and {{analysis}} of a dual-mode driven parallel XY micromanipulator for micro/nanomanipulations Hui Tang 1, Yangmin Li 1, 2 and Jiming Huang 1 This article presents a novel design of a flexure-based, piezoelectric actuated, completely decoupled, high-bandwidth, highresolution, and large stroke parallel XY micromanipulator with two amplification levers. The monolithic mechanism is featured with dual working modes, which meets <b>different</b> <b>kinds</b> <b>of</b> <b>requirements</b> in terms of high resolution and large workspace in micro/nano fields. In {{order to reduce the}} displacement loss, the modeling {{and analysis}} of bending motion of the levers are conducted; thereafter, compliance and stiffness modeling by employing the matrix method are established. Furthermore, the dynamics modeling and analysis via Lagrange equations are performed to improve the dynamic proper-ties of the mechanism. The simulation results of finite element analysis indicate that the cross-coupling between the two axes is kept to 1. 2 %; meanwhile, the natural frequency of the mechanism is about 700 Hz, and the amplifier ratio is approximately 2. 32. Both theoretical analysis and finite element analysis results well validate the performance of the proposed mechanism...|$|E
40|$|Abstract 1 Scheduling theory {{generally}} {{assumes that}} real-time systems are mostly composed of activities with hard real-time requirements. Many systems are built today by composing different applications or {{components in the}} same system, leading to a mixture of many <b>different</b> <b>kinds</b> <b>of</b> <b>requirements</b> with small parts of the system having hard real-time requirements and other larger parts with requirements for more flexible scheduling and for quality of service. Hard real-time scheduling techniques are extremely pessimistic for {{the latter part of}} the application, and consequently it is necessary to use techniques that let the system resources be fully utilized to achieve the highest possible quality. This paper presents a framework for a scheduling architecture that provides the ability to compose several applications or components into the system, and to flexibly schedule the available resources while guaranteeing hard real-time requirements. The framework (called FSF) is independent of the underlying implementation, and can run on different underlying scheduling strategies. It is based on establishing service contracts that represent the complex and flexible requirements of the applications, and which are managed by the underlying system to provide the required level of service...|$|E
40|$|Timed automata are {{a popular}} {{formalism}} to model real-time systems. They were introduced {{two decades ago}} to support formal verification. Since then they have also been used for other purposes and a large has been introduced {{to be able to}} deal with the many <b>different</b> <b>kinds</b> <b>of</b> <b>requirements</b> of real-time system. This paper presents a fairly comprehensive survey, comprised of eighty variants of timed automata. The paper classifies all these eighty variants of timed automata in an effort to determine current developments. It uses analysis techniques, formal properties, and decision problems to draw distinctions between different versions. Moreover, the paper discusses the challenges behind using a timed automata specification to derive an implementation of a working real-time system and presents some solutions. Finally, the paper lists and classifies forty tools supporting timed automata. The paper does not only discuss many variants and their supporting concepts (e. g., closure properties, decision problems), techniques (e. g., for analysis), and tools, but it also attempts to help the reader navigate the vast literature in the field, to highlight differences and similarities between variants, and to reveal research trends and promising avenues for future exploration...|$|E
40|$|As software-intensive systems {{become more}} pervasive, {{more and more}} safety-critical systems are being {{developed}} and deployed. Yet when most people think about safety <b>requirements,</b> they think <b>of</b> safety-critical functional <b>requirements,</b> which are requirements that have critical safety ramifications if not correctly implemented. However, there are actually four major classifications of safety-related requirements: (1) pure safety requirements, (2) safety-significant requirements, (3) safety constraints, and (4) requirements for safety systems. This paper describes a taxonomy <b>of</b> these <b>different</b> <b>kinds</b> <b>of</b> safetyrelated <b>requirements,</b> and clearly and briefly defines and describes each of the above categories <b>of</b> safety-related <b>requirements...</b>|$|R
40|$|In {{order to}} treat the problem of optimal growth of an aggregative economy, the {{goverment}} is introduced into the neo-classical growth model. The properties of the government's dynamic budget constraint are analyzed. Hence <b>different</b> <b>kinds</b> <b>of</b> government's policy <b>requirements</b> {{are taken into account}} Especially the effects of public investment are examined. A condition for dynamic stability is considered. ...|$|R
40|$|Systems Engineering (SE) {{controls}} {{a complex}} environment consisting of various collaborative subsystems. Each subsystem demands <b>different</b> <b>kind</b> <b>of</b> <b>requirements</b> and follows a specific strategy for its development process. Unifying and harmonizing {{the development process}} of all collaborative subsystems towards achieving the ultimate integrated system {{is one of the}} main challenges of SE. This work introduces a new approach towards having a generic SE unified process applicable to various environments. We suggest a service-oriented framework for SE process implemented using Web Services, and describe the process scenario in a machine-friendly abstract layer over the Development Process. This description layer choreographs collaborative subsystems and is implemented by a Web Services Choreography Description Language (WSCDL). It also covers Interface Management concerns of SE. In such an environment, as long as all services follow a unique framework for the SE process such as the one specified by the International Council on Systems Engineering (INCOSE), each phase of the process would then be an anonymous service implemented by a different vendor. As the result, an organization could easily customize its own specific development environment by editing this choreography layer according to its specific development policies, and then tailor its own desired development environment by choosing and integrating various services available on the Web. Source: Masters Abstracts International, Volume: 45 - 01, page: 0351. Thesis (M. Sc.) [...] University of Windsor (Canada), 2006...|$|R
40|$|We {{present a}} formal {{specification}} {{and analysis of}} a haemodialysis machine (HD machine) in Event-B using the Rodin Toolset. The medical device domain is a particularly complex multidisciplinary field involving disparate branches of engineering, biological and medical fields {{as well as a}} critical patient-machine interface. Requirements include safety properties, process steps, human-machine interfaces, timing constraints, dynamic control algorithms, and design features. Our aim is to demonstrate that the Event-B based modelling, verification and validation tools deal with the variety of requirements involved in a typical medical device. We utilise ProR for structuring and tracking requirements. We model the HD machine using iUML-B state-machines and class diagrams, and build a corresponding BMotion Studio visualisation. For verification, we use both theorem proving and model checking techniques. We validate the design of the system using (i) diagrams to aid the modelling of the sequential properties of the requirements, and (ii) ProB-based animation and visualisation tools to explore the system's behaviour. Some of the safety properties involve dynamic behaviour which is difficult to verify in Event-B. For these properties we use (iii) co-simulation tools to validate against a continuous model of the physical behaviour. We conclude that the Event-B based modelling tools are particularly rich in verification and validation techniques {{and with the help of}} supporting tools for requirements tracking, are able to address the <b>different</b> <b>kinds</b> <b>of</b> <b>requirements</b> in a medical device...|$|E
40|$|When {{training}} {{managers and}} instructional designers assess e-learning projects, {{it is well}} understood that the volatility {{of the subject matter}} will directly impact maintenance costs, {{for the simple reason that}} subject-matter areas with a high rate of change require more frequent content updates. There are well-known approaches using web technologies and modular design that can mitigate these costs when course content is anticipated to be volatile. Less well understood, however, is how changes to requirements impact lifecycle costs, and how technology and course design can be used to mitigate costs when this kind of change occurs. Changes to audience, course length, level of interactivity, hosting or delivery environment, assessment methodology, delivery language, branding, and course scope are common lifecycle events in today’s e-learning landscape. Typically, however, changes to requirements are seldom anticipated or designed for at the outset of an e-learning project. Instead, requirements are assumed to be static, with the result that if and when they do change, the costs can be high. This paper will present original research demonstrating that changes to e-learning requirements occur routinely over the product lifecycle, and it will examine some of the impact associated with different kinds of changes. An analogy will be drawn between content volatility (where future maintenance costs are routinely anticipated and mitigated) and changes to requirements, where such changes should also be (but seldom are) anticipated and mitigated. Quantitative survey results indicating the frequency of <b>different</b> <b>kinds</b> <b>of</b> <b>requirements</b> changes will be shown, and mitigation strategies will be presented, including the roles o...|$|E
40|$|In {{current trends}} the logos {{are playing a}} vital role in {{industrial}} and all commercial applications. Fundamentally the logo is defined as it’s a graphic entity which contains colors textures, shapes and text etc., which is organized in some special visible format. But unfortunately it is very difficult thing to save their brand logos from duplicates. In practical world there are several systems available for logo reorganization and detection with <b>different</b> <b>kinds</b> <b>of</b> <b>requirements.</b> Two dimensional global descriptors are used for logo matching and reorganization. The concept of Shape descriptors based on Shape context and the global descriptors are based on the logo contours. There is an algorithm which is implemented for logo detection is based on partial spatial context and spatial spectral saliency (SSS). The SSS is able to keep away from the confusion effect of background and also speed up the process of logo detection. All such methods are useful only when the logo is visible completely without noise and not subjected to change. We contribute, through this paper, to the design of a novel variation framework able to match and recognize multiple instances of multiple reference logos in image archives. Reference logos and test images are seen as constellations of local features (interest points, regions, etc.) and matched by minimizing an energy function mixing: 1) a fidelity term that measures the quality of feature matching, 2) a neighborhood criterion that captures feature co-occurrence geometry, and 3) a regularization term that controls the smoothness of the matching solution. We also introduce a detection/recognition procedure and study its theoretical consistency...|$|E
40|$|Abstract—In market-driven {{software}} product development, requirements that can potentially {{go into a}} product or a product release represent <b>different</b> <b>kinds</b> <b>of</b> investments. <b>Requirements</b> differ {{in the type of}} value that they provide and level of risk associated to investing in them. In this paper we investigate how business risk associated with different requirement types is considered by the decision makers and how it affects requirement selection decisions. The results of the conducted case study indicate that due to lacking methods for handling the requirements business risk, requirements with low level of risk are preferred over other type <b>of</b> <b>requirements</b> such as innovations and architectural improvements. Keywords-requirements selection; market driven software development; value; risk; case study I...|$|R
40|$|International audienceThis paper proposes an {{instrumented}} {{solution to}} integrate feature diagrams with UML models {{to be used}} as part of a general approach for designing software product lines and for product generation. The contribution is implemented in IBM Rational Software Architect (RSA). It is intended {{to be used in the}} context of large, complex and multi-domain projects, and at allowing model transformations to derive products. Our RSA implementation makes it possible to link feature diagrams with UML model artifacts. It allows traceability between feature models and other <b>different</b> <b>kinds</b> <b>of</b> models (<b>requirements,</b> class diagrams, sequence or activity diagrams, etc.). It is used in a project dedicated to create smart building optimization systems...|$|R
40|$|Combinatorial {{designs are}} very {{effective}} tools for managing keys in an infrastructure where power and memory {{are two major}} constraints. None of the present day wireless technologies takes the advantage of combinatorial designs. In this paper, we have proposed a general framework using combinatorial designs which will enable the participating devices to communicate securely among themselves with little memory and power overhead. The scheme caters for <b>different</b> <b>kinds</b> <b>of</b> user <b>requirements</b> and allows the designer to choose different combinatorial designs for different parts or levels of the network. This general framework will find application in all wireless radio technologies, typically WPANs and WLANs. This is a hitherto unexplored technique in wireless technologies...|$|R
40|$|The {{development}} of a software product line is seldom a green field task. Legacy systems exist that serve as an information source or that should be integrated into a product line. The information needed is usually elicited interactively with high involvement by the domain experts of the application domain. As domain experts have a high workload and are often unavailable, relying primarily on high expert involvement is a risk for the successful introduction of a product line engineering approach into an organization. This chapter presents an approach for the extraction of requirements from user documentation, which gives guidance on how to elicit knowledge from existing user documentation and how to transform information from this documentation into product line models. This approach is called the PuLSE-Framework for product line engineering. We describe the metamodel that {{is the basis of}} the approach, the extraction patterns that are derived from the metamodel, and the process that guides the application of the patterns and the derivation of information relevant for building a product line. This information can be features of legacy products, parts of use cases {{that can be used for}} product line analysis, <b>different</b> <b>kinds</b> <b>of</b> <b>requirements</b> and, most important forproduct line engineering, commonalities and variabilities among existing products. With thehelp of this information, a product line model with the product line requirements can be builtmuch faster and the workload of the domain experts is significantly reduced. We performedan initial validation of the approach in industrial case studies and in a controlled experiment...|$|E
40|$|Continuous {{technological}} development and increasing efficiency demands are driving products toward {{becoming more and}} more complex. For the aerospace industry - where the requirements for performance, safety and low environmental impact already are substantial - this means that more extensive quality assurance measures must be taken to ensure the fulfillment of the requirements of each individual component. However, to avoid that the work with quality improvement become too extensive and increase the product cost to unbearable levels it is necessary to have methods to prioritize and focus improvement efforts on the product features that matters most for fulfilling customer requirements. Therefore, the concept of Key Characteristics is used today, both in the aerospace and other industries; a term for those characteristics that {{have a significant impact on}} requirement compliance and whose outcomes at the same time are expected to vary considerably in manufacturing. The concept itself is similar among those who use it but the purpose of and methodology for identifying and managing Key Characteristics vary, even within the same industry. This thesis is therefore aimed to create a view of which factors that characterize an effective and efficient way for companies in the aerospace industry to work with Key Characteristics. The thesis involves a case study to create a framework for how companies within this industry work with Key Characteristics, a literature review to see which approaches are advocated by previous research and two benchmark studies to see examples of how Key Characteristics are used and handled in practice in industry. The results show that the work of Key Characteristics should meet three main criteria in order to be effective and efficient: • it must be clearly focused on the characteristics that have critical impact on customer requirements and at the same time considerable variation in production, • it should be initiated early in the product development process and then performed iteratively during the process of continuously reducing variation problems in manufacturing, and • it should identify Key Characteristics using both qualitative and quantitative tools to best capture all <b>different</b> <b>kinds</b> <b>of</b> <b>requirements</b> on the product. Finally a practical example is given of how the work with Key Characteristics should look like at GKN Sweden AB, the case study company in the aerospace industry, to effectively minimize the costs associated with production variation, and yet satisfy all customer requirements. Validerat; 20140811 (global_studentproject_submitter...|$|E
40|$|This thesis has two aims. The first {{aim is to}} set out an {{argument}} for social insurance {{in the form of}} compulsory income insurance in the event of sickness or unemployment, and to explore two lines of arguments for social insurance policies that are commonly associated with an active welfare state that seeks to prevent or reduce reliance on social insurance. The second aim is to outline and defend an account of legitimacy that takes moral autonomy seriously by making legitimacy partly dependent on our entrenched values and preferences. The first aim is relevant for articles I-VI. In article I it is argued that the extent to which behavioural responses to social insurance is seen as ethically problematic, it is primarily a problem that concerns the institution rather than the morality of the individual whose behaviour is influenced by social insurance. Thus, insofar as behavioural responses to social insurance are an ethical problem, it is a problem for political philosophy rather than individual ethics. In article II {{an argument}} for social insurance in the form of compulsory income insurance in the event of sickness or unemployment is presented, viz. the argument from autonomy. It is based on a concern for the protection of our identity according to what is called a “thick” conception of the person, which holds that our identities as separate persons are constituted by our central aims and commitments. It is also argued that contrary to what has been claimed by its opponents; social insurance needs not lead to the bad risks exploiting the good risks, or be head-on in conflict with individual freedom. Article III identifies normative issues that deserve attention in relation to in relation to a general introduction of prevention policies in social insurance and market insurance. It is argued that the importance of these issues suggests that arguments and distinctions drawn from moral and political philosophy should play a more prominent role both in the debate on the shift towards an active welfare state and the use of prevention policies in market insurance. Article IV is a response to comments from Professor David Buchanan initiated by article III. Article V explores what is called the argument from autonomy for reduced compensation rates in social insurance or making compensation from such insurance conditional on <b>different</b> <b>kinds</b> <b>of</b> <b>requirements</b> such as participation in rehabilitation or vocational training. It is argued that such policies are justified if they tend to ensure an adequate level of autonomy, where autonomy is understood {{in the sense of a}} “thick” conception of personal autonomy based on Norman Daniel’s extension of the principle of fair equality of opportunity. Article VI discusses the objection that arguments pertaining to the principle of fairness often are irrelevant since the principle of fairness is based on the acceptance of the relevant benefits. It is argued that this objection from non-acceptance fails because we can – and do – accept the benefits form such institutions on a practical level and this is enough to ground obligations pertaining fairness. The implications of this argument for policies associated with the active welfare state are explored, taking a reform of the Swedish sickness insurance as an example. The second aim is relevant for article VII. In article VII it is argued that an account of legitimacy should satisfy three conditions. The justification thesis and the legitimacy thesis are presented as accounts of justification and legitimacy respectively. It is argued that the proposed accounts satisfy these conditions. An account of political obligations is also given.   QC 20150121 </p...|$|E
40|$|Abstract: With rapid {{development}} of renewable generator, Application of storage plays {{important role in}} improving energy efficiency. At first <b>different</b> <b>kinds</b> <b>requirement</b> <b>of</b> storage is analysed with detail case in paper. The trend of storage for future is Prospect. Then demand of new energy for power system is analysed and calculated. A decomposition model for Energy storage for renewable power generation is established. Some suggestion will be given for energy storage for GuangDong power gird co. Ltd...|$|R
40|$|Abstract Executing test {{automation}} {{for large-scale}} software-intensive embedded systems {{requires a lot}} of hardware because tests must be executed for different hardware configurations. When there is not enough hardware for all <b>kind</b> <b>of</b> configurations, other solutions are developed to fill high test coverage with less hardware. Placing simulated configurations to the cloud makes the hardware usage more effective. The case company has developed a cloud-based testing service for embedded systems which are in this case 4 G base stations. This study investigated how well the service fulfils the target team’s needs for testing. Testers have a <b>different</b> <b>kind</b> <b>of</b> <b>requirements</b> for cloud-based testing. Requirements were split into four categories from the qualitative data which was collected by interviewing testers of the target team. Requirement categories are test environments, test automation development, test execution and partly-simulated system under tests, more commonly known as SUTs. Four tests were implemented to the cloud with Robot Framework which is a test automation tool for developing automated tests. An empirical data showed that executing cloud-based tests is not always so fast due to long waiting times of getting a test environment from the cloud. However, when test environments were received they have initialised automatically with default settings and required testing tools. That reduces testers’ workload because locally built test environments require a lot of manual work like maintaining test environments. Beside other research questions, this study investigated fault detection capability of partly-simulated SUTs. 34 Cloud-based tests with partly-simulated SUTs were executed. 26 Tests passed and 8 tests failed. The simulator’s software caused only one failure and tested software caused other seven failures. From these statistics, we can at least say that the partly-simulated SUT can find some faults. The study also investigated a specific fault which was not found in the simulator even though it was found in the real SUT, which was a clear disadvantage...|$|R
40|$|Abstract. The {{push toward}} {{business}} process automation has generated {{the need for}} integrating different enterprise applications involved in such processes. The typical approach to integration and to process automation {{is based on the}} use of adapters and message brokers. The need for adapters in Web services mainly comes from two sources: one is the heterogeneity at the higher levels of the interoperability stack, and the other is the high number of clients, each of which can support different interfaces and protocols, thereby generating the need for providing multiple interfaces to the same service. In this paper, we characterize the problem of adaptation of web services by identifying and classifying <b>different</b> <b>kinds</b> <b>of</b> adaptation <b>requirements.</b> Then, we focus on business protocol adapters, and we classify the different ways in which two protocols may differ. Next, we propose a methodology for developing adapters in Web services, based on the use of mismatch patterns and service composition technologies...|$|R
40|$|Issues in {{an issue}} {{tracking}} system contain <b>different</b> <b>kinds</b> <b>of</b> information like <b>requirements,</b> features, development tasks, bug reports, bug fixing tasks, refactoring tasks and so on. This information is generally accompanied by discussions or comments, which again are <b>different</b> <b>kinds</b> <b>of</b> information (e. g. social interaction, implementation ideas, stack traces or error messages). We propose to improve automatic categorization {{of this information}} and use the categorized data to support software engineering tasks. We want to obtain improvements in two different ways. Firstly, we want to obtain algorithmic improvements (e. g. natural language processing techniques) to retrieve and use categorized auxiliary data. Secondly we want to utilize multiple task-based categorizations to support different software engineering tasks...|$|R
40|$|In {{this paper}} we {{identify}} several {{areas in which}} the satisfaction <b>of</b> security <b>requirements</b> can affect the cost and performance of a system, and describe {{what is known about}} tradeoffs in these areas. We also show where these tradeoffs appear in the life cycle of a system, and show how they are affected by <b>different</b> <b>kinds</b> <b>of</b> security <b>requirements.</b> 1 Introduction When designing a system of any magnitude, it is necessary {{to keep in mind that}} it must satisfy a large number <b>of</b> <b>requirements,</b> some <b>of</b> which may be in conflict with each other. In some cases it may be necessary to trade off the different requirements against each other; in other cases it may be possible to identify other system features that may be traded off to resolve the conflicts between the two requirements. This is as true of security as it is of any other system requirement. Tradeoffs in security can be complicated by the fact that a system may have a number <b>of</b> different security <b>requirements</b> that lead to demands that are [...] ...|$|R
40|$|It is crucial, but hard, {{to predict}} the parts {{of a system that}} must be changed in the next release of a {{software}} system. Previous results show that professional and experienced developers largely underpredict the number of software entities subject to change in the next release, even though a thorough impact analysis driven by requirements is conducted. Thus, there is a strong need for understanding what <b>kind</b> <b>of</b> properties of a system make it more or less vulnerable to change. This empirical study analyzes change of C++ source code classes that occurred in two releases of the industrial object-oriented project PMR. The analysis is expected to result in better knowledge about the relationship between size and change-prone classes. The result, which is consistent over the two releases despite <b>different</b> <b>kinds</b> <b>of</b> new <b>requirements,</b> shows that the median size in the set of changed classes is significantly larger than in the set of unchanged classes. Our interpretation of the results is that large classes are more change-prone than small classes. This implies that large classes should be regarded as candidates for change regardless <b>of</b> the <b>kind</b> <b>of</b> new <b>requirements...</b>|$|R
40|$|Model {{checking}} is {{an automatic}} technique to verify whether {{a mathematical model}} M satisfies a given correctness requirement (or. specification) S. Usually, model M is given as a finite automaton or a pushdown automaton, and speci-fication S is given as a finite automaton (i. e., S is regular specification). Thus, the model checking problem {{can be reduced to}} the language-inclusion problem L(M) ⊆ L(S). Based on this approach, some model checkers have been success-fully implemented in industry, e. g., SPIN and MOPED. While many analysis problems such as identifying dead code and accesses to uninitialized variables can be captured as regular specifications, many others require inspection of the stack or matching of calls and returns, are context-free. Although the model checking problem for context-free specifications is undecidable, algorithmic so-lutions have been proposed for checking many <b>different</b> <b>kinds</b> <b>of</b> non-regular <b>requirements.</b> Especially, Alur and Madhusudan [2] have proposed visibly push-down automata (Vpas) as an adequate formalism for model checking context-free specifications. Different from pushdown automata, Vpas are closed under union, intersection, and complementation. The language-inclusion problem for Vpas is decidable in 2 Exptime. Our research concentrates on the following problems: – Study timed extension of Vpas and properties of this class of automata. – Study determinizability and complementability of some extensions of Vpas such as 2 -Vpas and visibly stack automata (Vsas). – Revisit superdeterministic pushdown automata and the language-inclusion problem L(A) ⊆ L(B), where A is an arbitrary PDA and B is a superdeter-ministic PDA. – Study language-inclusion problem for Vpas using a heuristic technique to reduce time complexity. ...|$|R
40|$|This report {{describes}} {{in more detail}} a project applying VR as a planning tool for the redevelopment of landscape. The aim {{of the project was}} to visualize redevelopment projects of two sites of the company Wismut GmbH, a former mining company. Applying VR technology, both the actual and the planning state of the landscape can be visualized interactively. The visualization of the planning state includes also the several processes (e. g., leveling and deposit) which will be carried out through the redevelopment. With the VR Geo system the engineers of Wismut were able to plan these two redevelopment projects in a powerful manner. They visualized the various planning states and discussed them with decision-maker. Furthermore the results were presented to people who are living in this area. They got a first impression of how their region will look in future. This report describes not just another VR application. It describes the realization of a project which provides the customer with a sy stem to generate, to change, and to present a landscape in different states. In fact the most difficult problem was to generate, i. e. to model, the landscape based upon the <b>different</b> <b>kind</b> <b>of</b> original data: hard copies of 2 D maps, a lot of textual information, color photos and some digital data (3 D point and line data of the dumps, grey scale aerial photo of the landscape). All the remaining has been modeled during the project: topology of the landscapes, streets, trees, bushes, buildings, dumps, railway tracks, fences, pits, churches, a location line for the trucks to level one dump, etc. The next difficulty was the <b>different</b> <b>kind</b> <b>of</b> user <b>requirements.</b> The system should be integrated into the daily work to support the engineers in planning redevelopment projects. Furthermore the redevelopment projects should be presented using a high end and a low end VR environment: high end VR workstation, VR interaction devices, stereo large screen projection, low end graphics workstation. Third, th e system should be able to produce on demand videos and color images based on the <b>requirements</b> <b>of</b> the customer. And fourth, the system should be open for future extensions. To sum up the VR Geo system enables the engineers of Wismut to visualize redevelopment projects in a scalable manner: snapshots of the planned landscape as color images, videos of a flight through the landscape, interim results for discussion on a low end workstation, and a highly immersive presentation using a high end VIZ system. All alternatives have been applied during the project described within this paper...|$|R
40|$|Marketing {{is having}} various {{components}} and functions for <b>different</b> <b>kind</b> <b>of</b> applications, <b>requirements</b> and objectives. Advertisement {{is one of}} the key components of marketing practise. The major challenges are facing by the advertising industry is the tremendous growth of the technology and its acceptance by the people with the same pace. Due to this fast paced advancement of technology, many new media vehicles have introduced and accepted by all. But for advertisers and academicians, it is still challenging and interesting to know the effectiveness of these media vehicles to achieve the advertising objectives. Sales revenue {{is one of the}} key performance indicators of the advertising campaign. This is an attempt to do a comparative study of the sales effects of traditional and contemporary media vehicles. This document is the dissertation report for MSc. Marketing course, 2014 - 15. It starts with the commentary of high levels of the topic which covers the general area of the subject and fast food industry of the Irish market. Then it covers the rationale of choosing this topic and what the gaps are, which is going to be filled with this research. In the literature review section is having collection of various literatures which have been reviewed to design and frame the research by broadening the knowledge through various advertising theories and comparison of the contemporary and traditional media vehicles. In later sections researcher has framed the thesis statement, objectives, sub-objectives and research questions. Based on these studies, philosophy and theoretical framing have been done. The type of philosophy, in this study is “epistemology” which discuss about the interdependency factors, key performance indicators and relationship between the various factors to achieve the advertising objectives. Further research and studies, says that there are two major actors viz. consumers and marketers. To know the points of view of both and get the deeper knowledge of the subjects researcher have adopted mixed method approach which includes quantitative, qualitative, primary and secondary research. The primary research has done for both consumers and marketers. Then analyse the results and findings got from primary research and secondary research analysis. The findings from primary research and secondary research analysis are discussed in the “discussion” section. This is also discussed about the advertising objectives can be related ultimately to sales goal and branding objectives. To execute this research everything was not very simple there were many limitations. Further researcher has highlighted that how these are taken care. Also discussed about the future possibility of this research...|$|R
