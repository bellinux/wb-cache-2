4|10000|Public
40|$|Impulse {{response}} {{data from}} the test of a payload fairing half shell model is analysed using six time domain system identification algorithms; Polyreference. <b>Double</b> <b>Least</b> <b>Squares,</b> Total Least Squares, Ibrahim Time Domain, Eigensystem Realization Algorithm and Smith Least Squares. The identified parameters obtained using each of the techniques are in good agreement with each other...|$|E
40|$|This paper {{estimates}} {{the impact of}} foreign direct investment (FDI) on economic growth in CEMAC countries. The basic theory is that of endogenous growth. The econometrical study {{is based on the}} work of Alaya, Nicet-Chenaf, and Rougier (2009) and Borensztein, De Gregorio, and Lee (1998). This model also includes the channels through which FDI influences growth. The study covers the period 1980 - 2010. All CEMAC countries are considered. To estimate the model, the author has used the method of <b>double</b> <b>least</b> <b>squares</b> and the generalised method moment. The results show that FDI affect growth in all CEMAC countries except Congo. The mean by which the realisation of their influence differs from one country to another. The main recommendation of the study is to promote economic and structural policies to modernise the economies of CEMAC...|$|E
40|$|Part 11 : Image ProcessingInternational audienceSparse {{decomposition}} {{has been}} widely used in numerous applications, such as image processing, pattern recognition, remote sensing and computational biology. Despite plenty of theoretical developments have been proposed, developing, implementing and analyzing novel fast sparse approximation algorithm is still an open problem. In this paper, a new pursuit algorithm <b>Double</b> <b>Least</b> <b>Squares</b> Pursuit (DLSP) is proposed for sparse decomposition. In this algorithm, {{the support of the}} solution is obtained by sorting the coefficients which are calculated by the first Least-Squares, and then the non-zero values over this support are detected by the second Least-Squares. The results of numerical experiment demonstrate the effectiveness of the proposed method, which is with less time complexity, more simple form, and gives close or even better performance compared to the classical Orthogonal Matching Pursuit (OMP) method...|$|E
40|$|Variable {{selection}} is fundamental in high-dimensional statistical modeling, including non- and semiparametric regression. However little {{work has been}} done for variable selection in a partially linear model. We propose and study a unified approach via <b>double</b> penalized <b>least</b> <b>squares,</b> retaining good features of both variable selection and model estimation in the framework of partially linear models. The proposed method is distinguished from others in that the penalty functions combine the ` 1 penalty coming from wavelet thresholding in the nonparametric component with the ` 1 penalty from the lasso in the parametric component. Simulations are used to investigate the per-formance of the proposed estimator in various settings, illustrating its effectiveness for simultaneous variable selection as well as estimation...|$|R
40|$|Abstract: Variable {{selection}} is fundamental in high-dimensional statistical modelling, including non-and semiparametric regression. However, little {{work has been}} done for variable selection in a partially linear model (PLM). We propose and study a unified approach via <b>double</b> penalized <b>least</b> <b>squares,</b> retaining good features of both variable selection and model estimation in the framework of PLM. The proposed method is distinguished from others in that the penalty functions combine the 1 penalty coming from wavelet thresholding in the non-parametric component with the 1 penalty from the lasso in the parametric component. Simulations are used to investigate the performances of the proposed estimator in various settings, illustrating its effectiveness for simultaneous variable selection as well as estimation. Key words: lasso; 1 penalty; partially linear model; variable selection; wavelet estimatio...|$|R
40|$|AbstractLinear Hall element's {{output voltage}} is {{directly}} proportional to its external magnetic field intensity, but usually, the filed intensity of some point around a magnet is not in linear relation with the distance between them. In {{order to get a}} good linear output for speed regulating handle which is based on linear Hall element, the author applies linear Hall element SS 495 A and describes layouts of different combination like double Hall elements with double magnets, single Hall element with <b>double</b> magnets. <b>Least</b> <b>square</b> fit is done to every layout through experiments and then relation curve of the rotation angle between output voltage of SS 495 A and magnets is obtained. Corresponding applicable situation is also given. Experiments show that output linearity of these two methods is good. These two layout methods can be easily operated and they are applicable for small-scale speed regulating handle...|$|R
40|$|One {{important}} current {{focus of}} modal identification is a reformulation of modal parameter estimation algorithms into a single, consistent mathematical formulation with a corresponding set of definitions and unifying concepts. Particularly, a matrix polynomial approach {{is used to}} unify the presentation with respect to current algorithms such as the least-squares complex exponential (LSCE), the polyreference time domain (PTD), Ibrahim time domain (ITD), eigensystem realization algorithm (ERA), rational fraction polynomial (RFP), polyreference frequency domain (PFD) and the complex mode indication function (CMIF) methods. Using this unified matrix polynomial approach (UMPA) allows {{a discussion of the}} similarities and differences of the commonly used methods. The use of least squares (LS), total least squares (TLS), <b>double</b> <b>least</b> <b>squares</b> (DLS) and singular value decomposition (SVD) methods is discussed in order to take advantage of redundant measurement data. Eigenvalue and SVD transformation methods are utilized to reduce the effective size of the resulting eigenvalue–eigenvector problem as well. � 1998 Academic Press Limited 1...|$|E
40|$|A {{satellite}} workshop of ICDAR 2005 In {{this paper}} a fast data driven learning-corpus building algorithm (FDDLCB) is proposed. The generic technique allows to build dynamically a representative and compact learning corpus for a connectionist model. The constructed dataset contains just a reduced number of patterns but sufficiently descriptive {{to characterize the}} different classes which should be separated. The method {{is based on a}} <b>double</b> <b>least</b> mean <b>squares</b> (LMS) error minimization mechanism trying to find the optimal boundaries of the different pattern classes. In the classical learning process the LMS is serving to minimize the error during the learning and this process is improved with a second one, as the new samples selection is also based on the idea to minimize the recognition error. Reinforcing the class boundaries where the recognition fails let us achieve a rapid and good generalization without any loss of occuracy. A modified version of the algorithm will be also presented. The experiments were performed on MNIST (Modified NIST) separated digit dataset. The encouraging result (98. 51 %) using just 1. 85 % of the available patterns from the original training dataset is comparable even with {{the state of the art}} techniques...|$|R
40|$|In {{this paper}} a fast data driven learning-corpus build-ing {{algorithm}} (FDDLCB) is proposed. The generic tech-nique allows to build dynamically a representative and com-pact learning corpus for a connectionist model. The con-structed dataset contains just a reduced number of pat-terns but sufficiently descriptive {{to characterize the}} differ-ent classes which should be separated. The method {{is based on a}} <b>double</b> <b>least</b> mean <b>squares</b> (LMS) error minimization mechanism trying to find the optimal boundaries of the dif-ferent pattern classes. In the classical learning process the LMS is serving to minimize the error during the learning and this process is improved with a second one, as the new samples selection is also based on the idea to minimize the recognition error. Reinforcing the class boundaries where the recognition fails let us achieve a rapid and good gener-alization without any loss of occuracy. A modified version of the algorithm will be also presented. The experiments were performed on MNIST 1 separated digit dataset. The encour-aging result (98. 51 %) using just 1. 85 % of the available pat-terns from the original training dataset is comparable even with {{the state of the art}} techniques. 1...|$|R
40|$|Abstract:Through {{theoretical}} derivation, some {{properties of}} the total <b>least</b> <b>squares</b> estimation are found. The total <b>least</b> <b>squares</b> estimation is the linear transformation of the <b>least</b> <b>squares</b> estimation, and the total <b>least</b> <b>squares</b> estimation is unbiased. The condition number of the total <b>least</b> <b>squares</b> estimation {{is greater than the}} <b>least</b> <b>squares</b> estimation, so the total <b>least</b> <b>squares</b> estimation is easier to be affected by the data error than the <b>least</b> <b>squares</b> estimation. Then through the further derivation, the relationships of solutions, residuals and unit weight variance estimations between the total <b>least</b> <b>squares</b> and the <b>least</b> <b>squares</b> are given...|$|R
40|$|AbstractIn this note, {{we present}} two {{results on the}} scaled total <b>least</b> <b>squares</b> problem. First, we discuss the {{relation}} between the scaled total <b>least</b> <b>squares</b> and the <b>least</b> <b>squares</b> problems. We derive an upper bound for the difference between the scaled total <b>least</b> <b>squares</b> solution and the <b>least</b> <b>squares</b> solution and establish a quantitative relation between the scaled total <b>least</b> <b>squares</b> residual and the <b>least</b> <b>squares</b> residual. Second, we give a perturbation analysis of the scaled total <b>least</b> <b>squares</b> problem. Numerical experiments in comparing our results with existing results are demonstrated...|$|R
30|$|In this section, we {{investigate}} widely used fuzzy regression methods of Fuzzy <b>Least</b> <b>Squares</b> (FLS), General Fuzzy <b>Least</b> <b>Squares</b> (GFLS), Sakawa–Yano (SY), Hojati–Bector–Smimou (HBS), Approximate-Distance Fuzzy <b>Least</b> <b>Squares</b> (ADFLS) and Interval-Distance Fuzzy <b>Least</b> <b>Squares</b> (IDFLS).|$|R
40|$|Abstract: This Paper {{describes}} {{an application of}} Structured Total <b>Least</b> <b>Squares</b> method to the interpolation of terrain data. We briefly review the ideas of <b>Least</b> <b>Squares,</b> Total <b>Least</b> <b>Squares,</b> and Structured Total <b>Least</b> <b>Squares.</b> We illustrate the use of Structured Total <b>Least</b> <b>Squares</b> in the approximation of terrain surfaces using a novel discrete surface, the Triangular Regular Network. The Structured Total <b>Least</b> <b>Squares</b> algorithm allows us to deal with data corrupted by noise in every coordinate (x; y; z) ...|$|R
50|$|In {{mathematical}} statistics, polynomial <b>least</b> <b>squares</b> {{refers to}} {{a broad range of}} statistical methods for estimating an underlying polynomial that describes observations. These methods include polynomial regression, curve fitting, linear regression, <b>least</b> <b>squares,</b> ordinary <b>least</b> <b>squares,</b> simple linear regression, linear <b>least</b> <b>squares,</b> approximation theory and method of moments. Polynomial <b>least</b> <b>squares</b> has applications in radar trackers, estimation theory, signal processing, statistics, and econometrics.|$|R
40|$|In {{this paper}} we {{investigate}} the finite sample risk performance of feasible generalised <b>least</b> <b>squares</b> estimators applied in models with serially correlated error terms. The risk {{functions of the}} ordinary <b>least</b> <b>squares,</b> generalised <b>least</b> <b>squares</b> and feasible generalised <b>least</b> <b>squares</b> estimators are derived under the asymmetric Linear-Exponential loss function. A numerical evaluation using simulation is {{used to compare the}} risk functions. Our numerical results show that the relative risk gains of the feasible generalised <b>least</b> <b>squares</b> estimators over the ordinary <b>least</b> <b>squares</b> estimator increases with higher loss asymmetry, particularly for larger serial correlation coefficients. [URL]...|$|R
40|$|We {{show that}} the {{generalized}} total <b>least</b> <b>squares</b> (GTLS) problem with a singular noise covariance matrix {{is equivalent to the}} restricted total <b>least</b> <b>squares</b> (RTLS) problem and propose a recursive method for its numerical solution. The method is based on the generalized inverse iteration. The estimation error covariance matrix and the estimated augmented correction are also characterized and computed recursively. The algorithm is cheap to compute and is suitable for online implementation. Simulation results in <b>least</b> <b>squares</b> (LS), data <b>least</b> <b>squares</b> (DLS), total <b>least</b> <b>squares</b> (TLS), and RTLS noise scenarios show fast convergence of the parameter estimates to their optimal values obtained by corresponding batch algorithms. Index Terms total <b>least</b> <b>squares</b> (TLS), generalized total <b>least</b> <b>squares</b> (GTLS), restricted total <b>least</b> <b>squares</b> (RTLS), recursive estimation, subspace tracking, system identification. I...|$|R
40|$|The {{behavior}} of the t test in small samples for coefficient significance in time-series regressions is examined after using the Prais-Winsten (PW) and Cochrane-Orcutt (CO) corrections for autocorrelation. Results are compared to ordinary <b>least</b> <b>squares</b> and generalized <b>least</b> <b>squares.</b> Key words: First-order autocorrelation generalized <b>least</b> <b>squares,</b> ordinary <b>least</b> <b>squares,</b> Prais-Winsten...|$|R
40|$|This article studies weighted, generalized, <b>least</b> <b>squares</b> estimators {{in simple}} linear {{regression}} with serially correlated errors. Closed-form expressions of weighted <b>least</b> <b>squares</b> estimators and variances are presented under some common stationary autocorrelation settings, a first-order autoregression and a first-order moving-average. These explicit expressions also have appealing applications, including an efficient weighted <b>least</b> <b>squares</b> computation method and a new sufficient and necessary condition on the equality of weighted <b>least</b> <b>squares</b> estimators and ordinary <b>least</b> <b>squares</b> estimators...|$|R
5000|$|Two-stage <b>least</b> <b>squares,</b> three-stage <b>least</b> <b>squares,</b> and {{seemingly}} unrelated regressions.|$|R
5000|$|Finding {{the tree}} and branch lengths {{minimizing}} the <b>least</b> <b>squares</b> residual is an NP-complete problem. However, for a given tree, the optimal branch lengths can be determined in [...] time for ordinary <b>least</b> <b>squares,</b> [...] time for weighted <b>least</b> <b>squares,</b> and [...] time for generalised <b>least</b> <b>squares</b> (given the inverse of the covariance matrix).|$|R
50|$|For details {{concerning}} nonlinear {{data modeling}} see <b>least</b> <b>squares</b> and non-linear <b>least</b> <b>squares.</b>|$|R
40|$|Preface Examples of the General Linear Model Introduction One-Sample Problem Simple Linear Regression Multiple Regression One-Way ANOVA First Discussion The Two-Way Nested Model Two-Way Crossed Model Analysis of Covariance Autoregression Discussion The Linear <b>Least</b> <b>Squares</b> Problem The Normal Equations The Geometry of <b>Least</b> <b>Squares</b> Reparameterization Gram-Schmidt Orthonormalization Estimability and <b>Least</b> <b>Squares</b> Estimators Assumptions for the Linear Mean Model Confounding, Identifiability, and Estimability Estimability and <b>Least</b> <b>Squares</b> Estimators...|$|R
40|$|AbstractThis article {{surveys the}} history, development, and {{applications}} of <b>least</b> <b>squares,</b> including ordinary, constrained, weighted, and total <b>least</b> <b>squares.</b> The presentation includes proofs {{of the basic}} theory, in particular, unitary factorizations and singular-value decompositions of matrices. Numerical examples with real data demonstrate {{how to set up}} and solve several types of problems of <b>least</b> <b>squares.</b> The bibliography lists comprehensive sources for more specialized aspects of <b>least</b> <b>squares...</b>|$|R
40|$|<b>Least</b> <b>squares</b> estimations {{have been}} used {{extensively}} in many applications system identification and signal prediction. These applications, the <b>least</b> <b>squares</b> estimators can usually be found by solving Toeplitz <b>least</b> <b>squares</b> problems. We present fast algorithms for solving the Toeplitz <b>least</b> <b>squares</b> problems. The algorithm is derived by using the displacement representation of the normal equations matrix. Numerical experiments show that these algorithms are efficient. published_or_final_versio...|$|R
40|$|The paper {{presents}} a comparison {{study for the}} performances of seven wind estimation algorithms for spaceborne scatterometers. These algorithms are weighted <b>least</b> <b>square</b> in log domain, maximum-likelihood, <b>least</b> <b>square</b> weighted <b>least</b> <b>square,</b> adjustable weighted <b>least</b> <b>square,</b> L 1 norm, and <b>least</b> wind speed <b>square</b> algorithms using radar scatterometer measurements. For each algorithm, the system performance simulation results are presented for the NASA scatterometer system planned to be launched in the 1990 's...|$|R
40|$|The {{performance}} of several algorithms for positioning a single sensor node based on distance estimates to {{it from a}} number of nodes at known positions (anchor nodes) is compared when the distance estimates are obtained from a measurement campaign. The distance estimates are based on timeof-arrival measurements done by ultrawideband devices in an indoor office environment. The compared algorithms are based on nonlinear <b>least</b> <b>squares,</b> <b>least</b> <b>squares</b> and total <b>least</b> <b>squares</b> after data preprocessing, and projection methods. No algorithm is uniformly best; however, <b>least</b> <b>squares</b> and total <b>least</b> <b>squares</b> after data preprocessing show higher mean squared errors in almost all cases, while the nonlinear <b>least</b> <b>squares</b> and projection methods have similar performance; projection methods performs slightly better. 1...|$|R
50|$|The {{discrete}} <b>least</b> <b>squares</b> meshless (DLSM) {{method is}} a newly introduced meshless method {{based on the}} <b>least</b> <b>squares</b> concept. The method {{is based on the}} minimization of a <b>least</b> <b>squares</b> functional defined as the weighted summation of the squared residual of the governing differential equation and its boundary conditions at nodal points used to discretize the domain and its boundaries. While most of the existing meshless methods need background cells for numerical integration, DLSM did not require numerical integration procedure due to the use of discrete <b>least</b> <b>squares</b> method to discretize the governing differential equation. A Moving <b>least</b> <b>squares</b> (MLS) approximation method is used to construct the shape function making the approach a fully <b>least</b> <b>squares</b> based approach.|$|R
40|$|In this correspondence, we {{introduce}} a minimax regret criteria to the <b>least</b> <b>squares</b> problems with bounded data uncertainties and solve it using semi-definite programming. We investigate a robust minimax <b>least</b> <b>squares</b> approach that minimizes a worst case difference regret. The regret {{is defined as}} the difference between a squared data error and the smallest attainable squared data error of a <b>least</b> <b>squares</b> estimator. We then propose a robust regularized <b>least</b> <b>squares</b> approach to the regularized <b>least</b> <b>squares</b> problem under data uncertainties by using a similar framework. We show that both unstructured and structured robust <b>least</b> <b>squares</b> problems and robust regularized <b>least</b> <b>squares</b> problem can be put in certain semi-definite programming forms. Through several simulations, we demonstrate the merits of the proposed algorithms with respect to the the well-known alternatives in the literature. Comment: Submitted to the IEEE Transactions on Signal Processin...|$|R
40|$|In {{previous}} work we introduced a construction to produce biorthogonal multiresolutions from given subdivisions. The approach involved estimating {{the solution to}} a <b>least</b> <b>squares</b> problem {{by means of a}} number of smaller <b>least</b> <b>squares</b> approximations on local portions of the data. In this work we use a result by Dahlquist, et. al. on the method of averages to make observational comparisons between this local <b>least</b> <b>squares</b> estimation and full <b>least</b> <b>squares</b> approximation. We have explored examples in two problem domains: data reduction and data approximation. We observe that, particularly for design matrices with a repetitive pattern of column entries, the <b>least</b> <b>squares</b> solution is often well estimated by local <b>least</b> <b>squares,</b> that the estimation rapidly improves with the size of the local <b>least</b> <b>squares</b> problems, and that the quality of the estimate is largely independent {{of the size of the}} full problem...|$|R
5000|$|The {{recursive}} <b>least</b> <b>squares</b> algorithm considers {{an online}} {{approach to the}} <b>least</b> <b>squares</b> problem. It can be shown that by initialising [...] and , {{the solution of the}} linear <b>least</b> <b>squares</b> problem given in the previous section can be computed by the following iteration: ...|$|R
40|$|It {{is shown}} that, in {{comparison}} to the results obtained from a conventional <b>least</b> <b>squares</b> approach, a total <b>least</b> <b>squares</b> solution leads to significant improvements in the geometry and appearance of images synthesised in a linear combination of views procedure. Use of the total <b>least</b> <b>squares</b> criterion is appropriate when errors on the control points are independently and identically distributed between the basis images and the target image being synthesised. When this is not be the case it is pointed out that the generalised total <b>least</b> <b>squares</b> procedure should be used. A synthetic object is used to evaluate the improvement in geometric accuracy obtained by use of the total <b>least</b> <b>squares</b> solution {{in comparison to}} a classical <b>least</b> <b>squares</b> method. Simulated and real images of laboratory test objects are similarly used to illustrate the improvement in appearance of images reconstructed by means of the total <b>least</b> <b>squares</b> procedure. ...|$|R
40|$|Critical random {{coefficient}} AR(1) {{processes are}} investigated where the random coefficient is binary, taking values - 1 and 1. Asymptotic behavior of <b>least</b> <b>squares</b> estimator for {{the mean of}} the random coefficient is discussed. Ordinary <b>least</b> <b>squares</b> estimator is shown to be consistent. Weighted <b>least</b> <b>squares</b> estimator turns out to be asymptotically normally distributed. This enables us to present a unified limit result for the weighted <b>least</b> <b>squares</b> estimator valid for the stationary, explosive and critical cases. Also, a test of criticality is discussed. Critical process Random coefficient AR(1) Test of criticality Weighted and ordinary <b>least</b> <b>squares...</b>|$|R
50|$|Total <b>Least</b> <b>Squares</b> DMD: Total <b>Least</b> <b>Squares</b> DMD is {{a recent}} {{modification}} of Exact DMD meant to address issues of robustness to measurement noise in the data. In, the authors interpret the Exact DMD as a regression problem that is solved using ordinary <b>least</b> <b>squares</b> (OLS), which assumes that the regressors are noise free. This assumption creates a bias in the DMD eigenvalues when it is applied to experimental data sets where all of the observations are noisy. Total <b>least</b> <b>squares</b> DMD replaces the OLS problem with a total <b>least</b> <b>squares</b> problem, which eliminates this bias.|$|R
40|$|M. Com. (Econometrics) The {{objective}} {{of this study is}} to evaluate different estimation techniques that can be used to estimate the coefficients of a model. The estimation techniques were applied to empirical data drawn from the South African economy. The Monte Carlo studies are unique in that data was statistically generated for the experiments. This approach was due to the fact that actual observations on economic variables contain several econometric problems, such as autocorrelation and MUlticollinearity, simultaneously. However, the approach in this study differs in that empirical data is used to evaluate the estimation techniques. The estimation techniques evaluated are : • Ordinary <b>least</b> <b>squares</b> method • Two stage <b>least</b> <b>squares</b> method • Limited information maximum likelihood method • Three stage <b>least</b> <b>squares</b> method • Full information maximum likelihood method. The estimates of the different coefficients are evaluated on the following criteria : • The bias of the estimates • The variance of the estimates • t-values of the estimates • The root mean square error. The ranking of the estimation techniques on the bias criterion is as follows : 1 Full information maximum likelihood method. 2 Ordinary <b>least</b> <b>squares</b> method 3 Three stage <b>least</b> <b>squares</b> method 4 Two stage <b>least</b> <b>squares</b> method 5 Limited information maximum likelihood method The ranking of the estimation techniques on the variance criterion is as follows : 1 Full information maximum likelihood method. 2 Ordinary <b>least</b> <b>squares</b> method 3 Three stage <b>least</b> <b>squares</b> method 4 Two stage <b>least</b> <b>squares</b> method 5 Limited information maximum. likelihood method All the estimation techniques performed poorly with regard to the statistical significance of the estimates. The ranking of the estimation techniques on the t-values of the estimates is thus as follows 1 Three stage <b>least</b> <b>squares</b> method 2 ordinary <b>least</b> <b>squares</b> method 3 Two stage <b>least</b> <b>squares</b> method and the limited information maximum likelihood method 4 Full information maximum likelihood method. The ranking of the estimation techniques on the root mean square error criterion is as follows : 1 Full information maximum likelihood method and the ordinary <b>least</b> <b>squares</b> method 2 Two stage <b>least</b> <b>squares</b> method 3 Limited information maximum likelihood method and the three stage <b>least</b> <b>squares</b> method The results achieved in this study are very similar to those of the Monte Carlo studies. The only exception is the ordinary <b>least</b> <b>squares</b> method that performed better on every criteria dealt with in this study. Though the full information maximum likelihood method performed the best on two of the criteria, its performance was extremely poor on the t-value criterion. The ordinary <b>least</b> <b>squares</b> method is shown, in this study, to be the most constant performer...|$|R
40|$|AbstractWe {{study the}} {{convergence}} of discrete and penalized <b>least</b> <b>squares</b> spherical splines in spaces with stable local bases. We derive a bound for error in the approximation of a sufficiently smooth function by the discrete and penalized <b>least</b> <b>squares</b> splines. The error bound for the discrete <b>least</b> <b>squares</b> splines is explicitly dependent on the mesh size of the underlying triangulation. The error bound for the penalized <b>least</b> <b>squares</b> splines additionally depends on the penalty parameter...|$|R
40|$|This paper {{focuses on}} the {{iterative}} identification problems for a class of Hammerstein nonlinear systems. By decomposing the system into two fictitious subsystems, a decomposition-based <b>least</b> <b>squares</b> iterative algorithm is presented for estimating the parameter vector in each subsystem. Moreover, a data filtering-based decomposition <b>least</b> <b>squares</b> iterative algorithm is proposed. The simulation {{results indicate that the}} data filtering-based <b>least</b> <b>squares</b> iterative algorithm can generate more accurate parameter estimates than the <b>least</b> <b>squares</b> iterative algorithm...|$|R
