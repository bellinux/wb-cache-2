286|2256|Public
50|$|The {{adjective}} near in {{the context}} of near sets is used to denote the fact that observed <b>feature</b> <b>value</b> differences of distinct objects are small enough to beconsidered indistinguishable, i.e., within some tolerance.|$|E
50|$|For {{continuous}} features, usually, some threshold <b>feature</b> <b>value</b> is selected, and {{the stump}} contains two leaves — for values below {{and above the}} threshold. However, rarely, multiple thresholds may be chosen and the stump therefore contains three or more leaves.|$|E
50|$|In {{addition}} to having certainty measures included in the representation, {{the representation of the}} corresponding feature values may itself be suitable for an averaging operation or not. Most feature representations can be averaged in practice, but only in certain cases can the resulting descriptor be given a correct interpretation in terms of a <b>feature</b> <b>value.</b> Such representations are referred to as averageable.|$|E
30|$|A {{standard}} approach in feature selection literature is to directly apply training and selection criteria on the <b>feature</b> <b>values.</b> However, when natural {{variability in the}} data is high and number of training samples are less, even minor changes in <b>feature</b> <b>values</b> would introduce errors in the bayes probability calculations. Classification methods such as SVM {{on the other hand}} try to get around this problem by normalising the <b>feature</b> <b>values</b> and by parametric training of the classifiers against several possible changes in <b>features</b> <b>values.</b> In classifier studies, this essentially shifts the focus from <b>feature</b> <b>values</b> to distance values. Instead of directly optimising the classifier parameters based on <b>feature</b> <b>values,</b> the distance functions itself is trained and optimised.|$|R
40|$|Zheng {{classification}} is a {{very important}} step in the diagnosis of traditional Chinese medicine (TCM). In clinical practice of TCM, <b>feature</b> <b>values</b> are often missing and incomplete cases. The performance of Zheng classification is strictly related to rates of missing <b>feature</b> <b>values.</b> Based on the pattern of the missing <b>feature</b> <b>values,</b> a new approach named local-validity is proposed to classify zheng classification with missing <b>feature</b> <b>values.</b> Firstly, the maximum submatrix for the given dataset is constructed and local-validity method finds subsets of cases for which all of the <b>feature</b> <b>values</b> are available. To reduce the computational scale and improve the classification accuracy, the method clusters subsets with similar patterns to form local-validity subsets. Finally, the proposed method trains a classifier for each local-validity subset and combines the outputs of individual classifiers to diagnose zheng classification. The proposed method is applied to the real liver cirrhosis dataset and three public datasets. Experimental results show that classification performance of local-validity method is superior to the widely used methods under missing <b>feature</b> <b>values...</b>|$|R
30|$|The <b>feature</b> <b>values</b> are ratios in {{the range}} [0, 1]. Thus, the <b>feature</b> <b>values</b> can be {{understood}} as confidence values. The final classification compares the <b>feature</b> <b>values</b> extracted from the background with those extracted from the input frame. If the feature outcomes are the same (whether abandoned or removed), the final result is the agreed outcome. Otherwise, the most confident outcome between them is chosen. This procedure avoids the unfeasible task of defining suitable thresholds while achieving high accuracy.|$|R
50|$|Depending on {{the type}} of the input feature, several {{variations}} are possible. For nominal features, one may build a stump which contains a leaf for each possible <b>feature</b> <b>value</b> or a stump with the two leaves, one of which corresponds to some chosen category, and the other leaf to all the other categories. For binary features these two schemes are identical. A missing value may be treated as a yet another category.|$|E
50|$|If a given {{class and}} <b>feature</b> <b>value</b> never occur {{together}} in the training data, then the frequency-based probability estimate will be zero. This is problematic because it will wipe out all information in the other probabilities when they are multiplied. Therefore, it is often desirable to incorporate a small-sample correction, called pseudocount, in all probability estimates such that no probability is ever set to be exactly zero. This way of regularizing naive Bayes is called Laplace smoothing when the pseudocount is one, and Lidstone smoothing in the general case.|$|E
50|$|Two {{examples}} of image are local edge orientation and local velocity in an image sequence. In {{the case of}} orientation, {{the value of this}} feature may be more or less undefined if more than one edge are present in the corresponding neighborhood. Local velocity is undefined if the corresponding image region does not contain any spatial variation. As a consequence of this observation, it may be relevant to use a feature representation which includes a measure of certainty or confidence related to the statement about the <b>feature</b> <b>value.</b> Otherwise, it is a typical situation that the same descriptor is used to represent feature values of low certainty and feature values close to zero, with a resulting ambiguity in the interpretation of this descriptor. Depending on the application, such an ambiguity {{may or may not be}} acceptable.|$|E
40|$|In many {{classification}} tasks {{training data}} have missing <b>feature</b> <b>values</b> {{that can be}} acquired at a cost. For building accurate predictive models, acquiring all missing values is often prohibitively expensive or unnecessary, while acquiring a random subset of <b>feature</b> <b>values</b> may not be most effective. The goal of active feature-value acquisition is to incrementally select <b>feature</b> <b>values</b> that are most costeffective for improving the model’s accuracy. We present an approach that acquires <b>feature</b> <b>values</b> for inducing a classification model based on an estimation of the expected improvement in model accuracy per unit cost. Experimental results demonstrate that our approach consistently reduces the cost of producing {{a model of a}} desired accuracy compared to random feature acquisitions. ...|$|R
40|$|International audienceThe goal {{of sound}} {{morphing}} by feature interpolation is to obtain sounds whose <b>values</b> of <b>features</b> are intermediate between {{those of the}} source and target sounds. In order to do this, {{we should be able}} to resynthesize sounds that present a set of predefined <b>feature</b> <b>values,</b> a notoriously difficult problem. In this work, we present morphing techniques to obtain hybrid musical instrument sounds whose <b>feature</b> <b>values</b> correspond as close as possible to the ideal interpolated <b>values.</b> When the <b>features</b> capture perceptually relevant information, the morphed sound whose features are interpolated is perceptually intermediate. The features we use are acoustic correlates of salient timbre dimensions derived from perceptual studies, such that sounds whose <b>feature</b> <b>values</b> are intermediate between two would be placed between them in the underlying timbre space. We measure the perceptual impact of the morphed sounds directly by the <b>feature</b> <b>values,</b> using them as an objective measure with which to evaluate the results. Thus we consider that the morphed sounds change perceptually linearly when the corresponding <b>feature</b> <b>values</b> vary linearly...|$|R
40|$|In {{orthodox}} feature theory, phonological output representations {{must contain}} redundant <b>feature</b> <b>values</b> {{in order to}} achieve phonetic interpretability. This assumption sits awkwardly with the current view that constraints on phonological well-formedness are expressed over output. One inevitable and undesirable result is that redundant <b>feature</b> <b>values</b> clutter up the statement o...|$|R
5000|$|Sound change {{includes}} any {{processes of}} language change that affect pronunciation (phonetic change) or sound system structures (phonological change). Sound change can consist of {{the replacement of}} one speech sound (or, more generally, one phonetic <b>feature</b> <b>value)</b> by another, {{the complete loss of}} the affected sound, or even the introduction of a new sound in a place where there had been none. Sound changes can be environmentally conditioned, meaning that the change only occurs in a defined sound environment, whereas in other environments the same speech sound is not affected by the change. The term [...] "sound change" [...] refers to diachronic changes—that is, irreversible changes in a language's sound system over time; [...] "alternation", on the other hand, refers to changes that happen synchronically (i.e. within the language of an individual speaker, depending on the neighboring sounds) and which do not change the language's underlying system (for example, the -s in the English plural can be pronounced differently depending on what sound it follows; this is a form of alternation, rather than sound change). However, since [...] "sound change" [...] can refer to the historical introduction of an alternation (such as post-vocalic /k/ in Tuscan—once k, but now h)—the label is inherently imprecise and often must be clarified as referring to phonetic change or restructuring.|$|E
50|$|One of {{the most}} {{critical}} and often time consuming elements of an LCS is the matching process. The first step in an LCS learning cycle takes a single training instance from the environment and passes it to P where matching takes place. In step two, every rule in P is now compared to the training instance to see which rules match (i.e. are contextually relevant to the current instance). In step three, any matching rules are moved to a match set M. A rule matches a training instance if all feature values specified in the rule condition are equivalent to the corresponding <b>feature</b> <b>value</b> in the training instance. For example, assuming the training instance is (001001 ~ 0), these rules would match: (###0## ~ 0), (00###1 ~ 0), (#01001 ~ 1), but these rules would not (1##### ~ 0), (000##1 ~ 0), (#0#1#0 ~ 1). Notice that in matching, the endpoint/action specified by the rule is not taken into consideration. As a result, the match set may contain classifiers that propose conflicting actions. In the fourth step, since we are performing supervised learning, M is divided into a correct set C and an incorrect set I. A matching rule goes into the correct set if it proposes the correct action (based on the known action of the training instance), otherwise it goes into I. In reinforcement learning LCS, an action set A would be formed here instead, since the correct action is not known.|$|E
40|$|Blocking in {{inflection}} {{occurs when}} a morphological exponent prevents the application of another exponent expressing the same <b>feature</b> <b>value,</b> thus barring the occurrence of multiple exponents of a single morphosyntactic <b>feature</b> <b>value.</b> In instances of extended exponence, more than one exponent in the same word realizes the same <b>feature</b> <b>value.</b> We provide a unified account of blocking and extended exponence that combines a realizational approach to inflection with Optimality Theory (Realization Optimality Theory), encoding morphological realization rules as ranked violable constraints. The markedness constraint *FEATURE SPLIT bars the realization of any morphosyntactic <b>feature</b> <b>value</b> {{by more than one}} exponent. If *FEATURE SPLIT ranks lower than two or more realization constraints expressing the same <b>feature</b> <b>value,</b> then we observe extended exponence. Otherwise, we find blocking of lower-ranked exponents. We show that Realization Optimality Theory is superior to various alternative approaches to blocking and extended morphological exponence...|$|E
5000|$|Accommodates {{missing data}} (i.e. missing <b>feature</b> <b>values</b> in {{training}} instances) ...|$|R
30|$|For each sample device or system: {{measure the}} desired <b>feature</b> <b>values.</b>|$|R
30|$|Joining the <b>feature</b> <b>values</b> {{over all}} frames {{consecutively}} yields the SF vector.|$|R
40|$|For facial {{expression}} analysis different-different approaches have been implemented. First thing preprocessing step {{is done and}} after that face detection algorithm applied for face detection. After face detection facial feature tracking or feature is extracted on the behalf of obtained <b>feature</b> <b>value.</b> Different-different classifier likes ANN, HMM, SVM is used to train the <b>feature</b> <b>value</b> obtained and then after testing is done to classify the classes to whom this <b>feature</b> <b>value</b> belongs. If high dimension <b>feature</b> <b>value</b> is obtained than PCA does work to reduce the dimension. Our approach is that apply Gabor filter on face to extract the feature. After feature extraction SVM is used to training and testing the <b>feature</b> <b>value</b> and finally similarity measure is evaluated to classify the classes to whom it belongs means it be from happy, sad, disgust, surprise, angry class. As many algorithm {{has been used for}} showing the Facial Expression of a Human. They have used different-different feature extraction method. Consideration of some similarity function has taken. Sometimes PCA, LDA was considered for dimensions reduction and for better accuracy...|$|E
30|$|The <b>feature</b> <b>value</b> is {{the ratio}} of patches that have similar {{internal}} and external regions.|$|E
40|$|Opinion rich web {{resources}} such as discussion forums, review sites and blogs which are bulky and are available in digital form. For the purpose of customer and business perspective, the task of scanning these reviews manually is computational burden. Hence, to process reviews automatically and summarizing them in suitable form is more efficient. The distinguished problem of producing opinion summary addresses is how to determine the mood, sentiment or opinion expressed in the review {{with respect to a}} numerical <b>feature</b> <b>value.</b> In this paper, {{the focus is on the}} main task of opinion mining called as opinion summarization. The extraction of product feature, technical <b>feature</b> <b>value</b> and opinion are critical for opinion summarization as they affect the performance significantly. The proposed approach consists of a software system in which mining of product feature, technical <b>feature</b> <b>value</b> and opinion is performed. The main motto of this software system is to recognize the technical <b>feature</b> <b>value</b> depending on which the reviews are summarized. This software is helpful for humans to understand the technical values expressed in the reviews...|$|E
5000|$|If {{the sum of}} the <b>feature</b> <b>values</b> is 2, {{the order}} is infinite, i.e., the {{notation}} represents a wallpaper group or a frieze group. Indeed, Conway's [...] "Magic Theorem" [...] indicates that the 17 wallpaper groups are exactly those with {{the sum of the}} <b>feature</b> <b>values</b> equal to 2. Otherwise, the order is 2 divided by the Euler characteristic.|$|R
30|$|In {{the most}} common cases of feature representations, within a {{consonant}} or vowel, each feature can take just one value. For example, a vowel is either [+[*]high] or [−[*]high], but not [+[*]high, −[*]high] or [−[*]high, +[*]high]. To account for the generalization, Hoard (1971 : 237) proposes a “principle of simultaneity,” according to which all <b>feature</b> <b>values</b> within a consonant or vowel must be simultaneously implementable. <b>Feature</b> <b>values</b> such as [+[*]high, −[*]high] and [−[*]high, +[*]high], which are called “contour features,” are ruled out {{because they have to}} be sequentially ordered, instead of being simultaneously implementable. Duanmu (1994) proposes a similar constraint called the No Contour Principle, which disallows contour <b>feature</b> <b>values.</b>|$|R
30|$|The second {{principle}} of the event horizon model that is relevant here is principle 5 : “When several events are similar accessing any specific event model is difficult.” In naturalistic activity, effective event boundaries tend to correspond to points in time when many features of the situation are changing (Newtson, Engquist, & Bois, 1977; Zacks, Kumar, Abrams, & Mehta, 2009; Zacks, Speer, & Reynolds, 2009). Effective segmentation reduces competition during memory retrieval by binding together intervals with similar <b>feature</b> <b>values,</b> and establishing boundaries when <b>feature</b> <b>values</b> are changing. Ineffective segmentation leads to event boundaries with similar <b>feature</b> <b>values</b> {{on either side of}} the boundary, exacerbating retrieval competition.|$|R
40|$|Abstract—The caoting {{layer of}} each {{industry}} structure detected by ultrasonic testing is one commonly used thickness detection technique. This article proposed a method which used the extreme value of resonant amplitude spectrum {{to extract the}} thickness <b>feature</b> <b>value,</b> it can solve the <b>feature</b> <b>value</b> extraction difficult question which caused by two situations of time domain signals aliasing when the coating layer thickness is quite small and signals weak when signal attenuates seriously. And when the extreme point distributed unevenly in the actual detection, some measurements to improve the <b>feature</b> <b>value</b> extraction precision are proposed, which increased the detection signal-to-noise rated effectively. Take the structure of steel shell bonding rubber coating layer as the detection object, the detection experiment is studied to confirm its validity...|$|E
3000|$|... ◦ Each <b>feature</b> <b>value</b> {{is mapped}} by {{descending}} order. A pseudo-Gaussian {{is produced by}} alternating below and above the mean.|$|E
3000|$|... ◦ Map each <b>feature</b> <b>value</b> of Cloud server to virtual model within {{specific}} range, {{which is}} defined in step 1.|$|E
5000|$|The orbifold Euler {{characteristic}} is 2 {{minus the}} sum of the <b>feature</b> <b>values,</b> assigned as follows: ...|$|R
40|$|Agreement {{features}} introduce greater complexity into agreement systems than {{is generally}} recognized. They may determine the agreement domain (Dargi) and certain combinations of <b>feature</b> <b>values</b> can rule out particular sentence types (Tsakhur). Feature interactions show {{three levels of}} complexity: just the target may be involved (German), or a computation of controller <b>feature</b> <b>values</b> may be required (Slovene), or computation may involve a covert feature (Miya) ...|$|R
40|$|In many {{data mining}} and machine {{learning}} tasks, datasets include instances that have missing <b>feature</b> <b>values</b> {{that can be}} acquired at a cost. However, both the acquisition cost and the usefulness {{with respect to the}} learning task may vary dramatically for different <b>feature</b> <b>values.</b> While this observation has inspired a number of approaches for active and cost-sensitive learning, most work in these areas has focused o...|$|R
30|$|The <b>feature</b> <b>value</b> of the {{periodic}} halftone detector {{is defined as}} the maximum value over all the bins of the histogram.|$|E
30|$|The {{waveform}} {{analysis of}} the abnormal fan and <b>feature</b> <b>value</b> of unit space are extracted, and the MD value is calculated by Eq. (6).|$|E
30|$|In {{order to}} {{characterize}} the waveform, extraction width and reference line set, and variation and abundance values are extracted as the <b>feature</b> <b>value.</b>|$|E
40|$|Content {{fingerprinting}} {{provides a}} compact representation of multimedia objects for copy detection. This paper analyzes the robustness of the ordinal ranking module frequently used in content fingerprinting {{by examining the}} changes in ranks as local variations are introduced in <b>feature</b> <b>values.</b> Closed-form expressions to measure such sensitivity are derived when <b>feature</b> <b>values</b> are jointly Gaussian-distributed. The results show that sensitivity depends {{on the strength of}} local variation, the total number of blocks, and the correlations among blockbased <b>feature</b> <b>values.</b> Experiments with both synthesized data and image data validate the analysis and provide interesting insights, inspiring an approach to reduce the sensitivity. Index Terms — content identification, content fingerprinting, block-based fingerprinting, ordinal ranking 1...|$|R
30|$|As the {{statistical}} studies, some <b>features</b> <b>values</b> vary from male to female and from age group to another.|$|R
40|$|Abstract—The work of {{coloring}} hand-drawn animation {{is done by}} manually specifying {{and painting}} each closed region in line drawings. To make this process more efficient, this research creates associations between closed regions in line drawings of adjacent frames, which need to be colored with the same color. <b>Feature</b> <b>values</b> are first computed from the shape of each closed region, and a cost of associating pairs of closed regions is computed from these <b>feature</b> <b>values.</b> The combination of associations that minimizes the total cost is then computed based on these costs. Three shape descriptors for computing <b>feature</b> <b>values</b> for each closed region are examined: ellipses, Fourier descriptors, and shape context; and the accuracy of making associations using each of them is studied. I...|$|R
