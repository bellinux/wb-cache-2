14|10000|Public
50|$|Océ {{received}} the ON DEMAND Best of Show Award for {{recognition of their}} design innovation with the VarioPrint 6250 digital duplex cut-sheet printer. The award recognized the printer as the world's fastest and most productive, operating at 250 <b>images</b> <b>per</b> <b>minute</b> (or 125 duplexed documents per minute).|$|E
50|$|DS-300 — DS-300 is {{an image}} scanner for stacked paper originals (A5 to A3 format) capable of {{scanning}} 300 <b>images</b> <b>per</b> <b>minute.</b> The scanners were chosen by the Russian Federal State Statistics Service to process {{the results of the}} 2002, 2006 and 2010 Russian Censuses. The device is also used by Rosobrnadzor during the exam.|$|E
50|$|There are {{two kinds}} of {{document}} feeders capable of two-sided (duplex) scanning: a reversing automatic document feeder or RADF scans one side of a page, then flips it and scans the other side. A duplexing automatic document feeder or DADF scans both sides in one pass. The advantage of the DADF is faster speed for two-sided originals. RADFs and DADFs are rated in <b>images</b> <b>per</b> <b>minute</b> (IPM), the number of sides they can scan each minute; this may depend upon the resolution being used (rather than the maximum resolution supported).|$|E
40|$|A major {{disadvantage}} of {{scanning probe microscopy}} is the slow speed of image acquisition, typically less than one <b>image</b> <b>per</b> <b>minute.</b> This paper describes three techniques {{that can be used}} to increase the speed of a conventional scanning probe microscope by greater than one hundred times. This is achieved by the combination of high-speed vertical positioning, sinusoidal scanning, and high-speed image acquisition. These techniques are simple, low-cost, and can be applied to many conventional microscopes without significant modification. Experimental results demonstrate an increased scan rate from 1 to 200 Hz. This reduces the acquisition time for a 200 x 200 resolution image from 3 min to 1 s...|$|R
40|$|OBJECTIVE: To {{develop an}} {{anatomic}} marker system (AMS) as an accurate, reliable method of thermal imaging data analysis, {{for use in}} cryotherapy research. DESIGN: Investigation of the accuracy of new thermal imaging technique. SETTING: Hospital orthopedic outpatient department in England. PARTICIPANTS: Consecutive sample of 9 patients referred to anterior knee pain clinic. INTERVENTIONS: Not applicable. MAIN OUTCOME MEASURES: Thermally inert markers were placed at specific anatomic locations, defining an area over the anterior knee of patients with anterior knee pain. A baseline thermal image was taken. Patients underwent a 3 -minute thermal washout of the affected knee. Thermal images were collected {{at a rate of}} 1 <b>image</b> <b>per</b> <b>minute</b> for a 20 -minute re-warming period. A Matlab (version 7. 0) program was written to digitize the marker positions and subsequently calculate the mean of the area over the anterior knee. Virtual markers were then defined as 15...|$|R
40|$|We report {{here for}} the first time {{observations}} of prominence velocities over a wide range of temperatures and with a high time cadence. Our study of ultraviolet movies of prominences reveals that multithermal features with speeds of 5 - 70 km/s perpendicular to the line of sight are common in the prominences which showed traceable motions. These speeds are noticeably higher than the typical speeds of 5 - 20 km/s observed in data from "quiet" prominences and are more typical of "activated" prominences in which speeds of up to 40 km/s have been reported. The observations were performed using five separate datasets taken by the Solar and Heliospheric Observatory's Coronal Diagnostic Spectrometer (SOHO/CDS) in its wide slit overlap program mode in lines from He approx. I, O approx. V, and Mg approx. IX and a separate prominence observation taken with both the Transition Region and Coronal Explorer (TRACE) in its 1216 and 1600 bands and in by the Swedish Vacuum Solar Telescope (SVST) at La Palma. The movies were taken with cadences > 1 <b>image</b> <b>per</b> <b>minute</b> and were made simultaneously or near-simultaneously in spectral lines formed at two or more temperatures...|$|R
40|$|Face {{recognition}} {{systems have}} been widely used in various security applications, for example in attendance system. The success of face recognition system relies on the trained face images as well as the face image captured that being recognized. Among the variables that determine the success of face recognition is face pose. Previous works showed that frontal face pose produced the best face recognition success rate. This paper proposes a face pose tracking subsystem that {{can be used as a}} filter so that only the frontal face pose that will be processed in the face recognition subsystem. The criteria for various face poses, i. e. frontal, tilted and turned, either left or right, have been formulated. Experimental results showed that the success rate of face recognition by implementing frontal face pose tracking can improve by 70. 5 %. However, it has trade off in reduced face image capture speed from 61 <b>images</b> <b>per</b> <b>minute</b> to 10 <b>images</b> <b>per</b> <b>minute...</b>|$|E
40|$|It is {{proposed}} a device {{for the study}} of the real-time aggregation of white thrombi in whole blood. This device allows extrapolating the 3 D-shape of platelets clotting within artificial capillaries, by simultaneously monitoring the electrical impedance between a pair of specifically designed gold electrodes and 2 D optical image of pixel luminance of the fluorescent labeled platelets. Up to 30 3 D <b>images</b> <b>per</b> <b>minute</b> have been obtained, for a process which requires few minutes before the aggregation of large thrombi structures, a noticeable result with respect to other 3 D-shape reconstruction methods. The contribution of this paper mainly consists in the study of an uncertainty model which, in our opinion, is of fundamental importance to provide a valuable metrological estimation of thrombus growth under flow conditions...|$|E
40|$|Calcium imaging is a {{technique}} for observing neuron activity {{as a series of}} images showing indicator fluorescence over time. Manually segmenting neurons is time-consuming, leading to research on automated calcium imaging segmentation (ACIS). We evaluated several deep learning models for ACIS on the Neurofinder competition datasets and report our best model: U-Net 2 DS, a fully convolutional network that operates on 2 D mean summary images. U-Net 2 DS requires minimal domain-specific pre/post-processing and parameter adjustment, and predictions are made on full 512 × 512 images at ≈ 9 K <b>images</b> <b>per</b> <b>minute.</b> It ranks third in the Neurofinder competition (F_ 1 = 0. 569) and is the best model to exclusively use deep learning. We also demonstrate useful segmentations on data from outside the competition. The model's simplicity, speed, and quality results make it a practical choice for ACIS and a strong baseline for more complex models in the future. Comment: Accepted to 3 rd Workshop on Deep Learning in Medical Image Analysis ([URL]...|$|E
50|$|The {{observatories}} continuously {{monitor the}} sky, taking {{a set of}} <b>images</b> approximately once <b>per</b> <b>minute,</b> gathering up to 100 gigabytes of data per night. By using the transit method, data collected from WASP {{can be used to}} measure the brightness of each star in each image, and small dips in brightness caused by large planets passing in front of their parent stars can be searched for.|$|R
30|$|In many {{real world}} target {{monitoring}} applications like hostile territorial target monitoring [2], {{because of the}} diversity in target characteristics, different types (modalities) of sensors like video, audio, temperature, etc., are required. Since targets are intrinsically of different importance or priority, it is natural to have different coverage requirement such as number of covering sensors and sampling rate of each sensor. Moreover, all the sensed data must be transmitted to the sink node for further processing. For example, in a surveillance system deployed for monitoring hostile territorial targets, sensitive targets like nuclear plants {{may need to be}} covered by several video sensors (from different angle) and geiger counter sensors (at different positions) and such sensors need to have a high sampling rate, e.g., one <b>image</b> <b>per</b> <b>minute,</b> in order to provide timely data; while other less important targets like a barrack may require only one video sensor with a lower sampling rate like one image every ten minutes but the image quality needs to be high; yet still other types of sensors such as magnetic sensors are needed to monitor other targets. All the sensed data need to be collected by a more powerful base station that performs some data fusion and further transmits the data to some remote destinations through its transceiver. Evidently, a prominent feature of such real world sensor networks is the heterogeneity: different targets are covered by different type and number of sensors running at possibly different sampling rates with various initial energy endowment.|$|R
40|$|During {{the week}} of the impacts of Comet Shoemaker-Levy 9 into Jupiter, we used a speckle imaging camera mounted on the Lick Observatory 3 meter Telescope to record a {{continuous}} stream of images of the planet. Because the speckle imaging technique compensates for atmospheric blurring, the resulting images were most likely the highest resolution of any taken from the ground. These images compliment the Hubble Space Telescope data by covering time periods when Hubble was not observing Jupiter. We collected full planet 1024 by 1024 pixel CCD <b>images</b> taken 20 <b>per</b> <b>minute</b> for 4 hours per night over 6 nights July 15 to 22. Only a portion of this raw data has been reduced to high resolution images to date...|$|R
40|$|Tropical {{cyclones}} (TCs) are weather {{systems with}} vast destructive power. Accurate location of their circulation centers, or "eyes", is thus important to forecasters. However, the eye fix process is often done manually in practice. While multiple factors are {{considered in the}} process, with subjective elements in these methods, forecasters could disagree. This paper describes a TC eye fix system that uses a novel motion field structure analysis method. It can handle TCs without well-defined structure that are partially out of the image. The systems also adapts user inputs and past results to improve its accuracy. Implemented on a commodity desktop computer, the system can process about 5 <b>images</b> <b>per</b> <b>minute,</b> giving an average error of about 0. 16 degrees in latitude/longitude on Mercator projected map for TCs that are completely inside the radar image. This is well within the relative error of about 0. 3 - 0. 4 degrees given by different TC warning centers. This TC eye fix system is useful in giving an objective TC center location in contrast to traditional manual analysis. © 2005 IEEE. published_or_final_versio...|$|E
40|$|We {{present a}} novel system for {{browsing}} {{through a very}} large set of images according to similarity. The images are dynamically placed on a 2 D canvas next to their nearest neighbors in a high-dimensional feature space. The layout and choice of images is generated on-the-fly during user interaction, reflecting the user's navigation tendencies and interests. This intuitive solution for image browsing provides a continuous experience of navigating through an infinite 2 D grid arranged by similarity. In contrast to common multidimensional embedding methods, our solution does not entail an upfront creation of a full global map. Image map generation is dynamic, fast and scalable, independent {{of the number of}} images in the dataset, and seamlessly supports online updates to the dataset. Thus, the technique is a viable solution for massive and constantly varying datasets consisting of millions of images. Evaluation of our approach shows that when using DynamicMaps, users viewed many more <b>images</b> <b>per</b> <b>minute</b> compared to a standard relevance feedback interface, suggesting that it supports more fluid and natural interaction that enables easier and faster movement in the image space. Most users preferred DynamicMaps, indicating it is more exploratory, better supports serendipitous browsing and more fun to use. Author Keywords Image browsing; Image search; similarity browsing...|$|E
40|$|To {{present an}} adapted Clinical Trial Processor (CTP) test set-up for receiving, anonymising and saving Digital Imaging and Communications in Medicine (DICOM) data using {{external}} {{input from the}} original database of an existing clinical study information system to guide the anonymisation process. Two methods are presented for an adapted CTP test set-up. In the first method, images are pushed from the Picture Archiving and Communication System (PACS) using the DICOM protocol through a local network. In the second method, images are transferred through the internet using the HTTPS protocol. In total 25, 000 images from 50 patients were moved from the PACS, anonymised and stored within roughly 2 h using the first method. In the second method, an average of 10 <b>images</b> <b>per</b> <b>minute</b> were transferred and processed over a residential connection. In both methods, no duplicated images were stored when previous images were retransferred. The anonymised images are stored in appropriate directories. The CTP can transfer and process DICOM images correctly in a very easy set-up providing a fast, secure and stable environment. The adapted CTP allows easy integration into {{an environment in which}} patient data are already included in an existing information system. Store DICOM images correctly in a very easy set-up in a fast, secure and stable environment Allows adaptation of the software to perform a certain task based on specific needs Allows easy integration into an existing environment Reduce the possibility of inappropriate anonymisation...|$|E
5000|$|Shortly after departure, the {{traveling}} twin sees the stay-at-home twin {{with no time}} delay. At arrival, the image in the ship screen shows the staying twin as he was 1 year after launch, because radio emitted from Earth 1 year after launch gets to the other star 4 years afterwards and meets the ship there. During this leg of the trip, {{the traveling}} twin sees his own clock advance 3 years and the clock in the screen advance 1 year, {{so it seems to}} advance at [...] the normal rate, just 20 <b>image</b> seconds <b>per</b> ship <b>minute.</b> This combines the effects of time dilation due to motion (by factor ε=0.6, five years on earth are 3 years on ship) and the effect of increasing light-time-delay (which grows from 0 to 4 years).|$|R
40|$|Folic acid, Vitamin B 9, is {{strongly}} advised {{as a supplement}} taken by pregnant woman to maintain {{the health of the}} embryo, and deficiency increases the risk of neural tube defects. However, a safe upper limit of folate to consume has not been established, and an excess of dietary folate may interfere with neurodevelopmental metabolism, increasing the risk of adverse outcomes, including autism spectrum disorder (ASD). It has been suggested that folate affects connectivity among neurons as the brain develops. Glutamate is important in the regulation of neural tissue development, as it is a common excitatory neurotransmitter that binds to synaptic membranes. Because it is so structurally similar to folic acid, it may compete for binding sites on neurons within developing tissues, affecting connectivity of neurons during embryonic brain development. This experiment tested the effects of adding excess folate, glutamate or both to cultures of developing neural tissue to determine whether embryonic neuronal behavior is altered. An inverted phase microscope with a heated stage was used to collect time-lapse images of a region of extending neurites with growth cones. After 30 <b>images</b> (one <b>per</b> <b>minute),</b> a known concentration of folate, glutamate or both was added to the dish and 30 more images were collected. Images were analyzed using ImageJ software, and alterations in exploratory behavior of the neurites with growth cones were recorded. The data suggests that excess glutamate can overcome an inhibition of area change <b>per</b> <b>minute</b> by excess folate, which authenticates a mechanism of inhibition of neural development by excess folate. This supports the hypothesis of underconnectivity during brain development leading to ASD...|$|R
2500|$|For {{commercial}} radiotelegraph licenses in the United States, the Federal Communications Commission specifies {{tests for}} Morse code proficiency in words <b>per</b> <b>minute</b> and in code groups <b>per</b> <b>minute.</b> The Commission specifies that a word is 5 characters long. [...] The Commission specifies Morse code test elements at 16 code groups <b>per</b> <b>minute,</b> 20 words <b>per</b> <b>minute,</b> 20 code groups <b>per</b> <b>minute,</b> and 25 words <b>per</b> <b>minute.</b> The word <b>per</b> <b>minute</b> rate would {{be close to}} the PARIS standard, and the code groups <b>per</b> <b>minute</b> would {{be close to the}} CODEX standard.|$|R
40|$|In cryo-electron {{microscopy}} {{and single}} particle analysis, data acquisition and image processing are generally {{carried out in}} sequential steps and computation of a three-dimensional reconstruction only begins once all the micrographs have been acquired. We are developing an integrated system for processing images of icosahedral particles during microscopy to provide reconstructed density maps in real-time at the highest possible resolution. The system is designed as a combination of pipelines to run in parallel on a computer cluster and analyzes micrographs as they are acquired, handling automatically all the processing steps from defocus estimation and particle picking to origin/orientation determination. An ab initio model is determined independently from the first micrographs collected, and new models are generated as more particles become available. As a proof of concept, we simulated data acquisition sessions using three sets of micrographs of good to excellent quality that were previously recorded from different icosahedral viruses. Results show that the processing of single micrographs can keep pace with an acquisition rate of about two <b>images</b> <b>per</b> <b>minute.</b> The reconstructed density map improves steadily during the image acquisition phase and its quality {{at the end of}} data collection is only moderately inferior to that obtained by expert users who processed semi-automatically all the micrographs after the acquisition. The current prototype demonstrates the advantages of integrating three-dimensional image processing with microscopy, which include an ability to monitor acquisition in terms of the final structure and to predict how much data and microscope resources are needed to achieve a desired resolution. © 2014 Elsevier Inc...|$|E
40|$|Reinforced {{concrete}} is {{a composite}} material consisting in a rebar embedded in a concrete matrix. The structural equilibrium depends {{of the quality of}} bond between the two materials as well as theintrinsic quality of each of the components. When the structure is submitted to heating, cracking can occur due to the difference in thermal expansion properties between the two materials. In the presentwork, a theoretical approach, correlating rebar expansion rate under a heating process, to the fracture properties of concrete is proposed. The validating test consists in a Double Cantilever concrete Beam, with an initial crack length a 0, a rebar embedded at a distance a 0 from the crack tip. The rebar is heated through an induction process and its expansion induces a crack opening displacement {{at the level of the}} concrete. The deformation of the concrete due to the rebar expansion induces at the level of the crack tip, a displacement as well as a stress field, leading to the crack propagation. In order to follow the development of the displacement field at the level of the crack front, a digital camera is used to capture pictures of that zone at various steps (two <b>images</b> <b>per</b> <b>minute)</b> of the heating process. The images are then analysed with image correlation techniques to obtain the displacement field as well as the strain field, and then the time of crack propagation. The critical crack opening displacement, the critical energy releaseand Stress Intensity Factor corresponding to the time of crack propagation, can be deduced with a good accuracy...|$|E
40|$|Abstract: Reinforced {{concrete}} is {{a composite}} material consisting in a rebar embedded in a concrete matrix. The structural equilibrium depends {{of the quality of}} bond between the two materials as well as the intrinsic quality of each of the components. When the structure is submitted to heating, cracking can occur due to the difference in thermal expansion properties between the two materials. In the present work, a theoretical approach, correlating rebar expansion rate under a heating process, to the fracture properties of concrete is proposed. The validating test consists in a Double Cantilever concrete Beam, with an initial crack length a 0, a rebar embedded at a distance a 0 from the crack tip. The rebar is heated through an induction process and its expansion induces a crack opening displacement {{at the level of the}} concrete. The deformation of the concrete due to the rebar expansion induces at the level of the crack tip, a displacement as well as a stress field, leading to the crack propagation. In order to follow the development of the displacement field at the level of the crack front, a digital camera is used to capture pictures of that zone at various steps (two <b>images</b> <b>per</b> <b>minute)</b> of the heating process. The images are then analysed with image correlation techniques to obtain the displacement field as well as the strain field, and then the time of crack propagation. The critical crack opening displacement, the critical energy release and Stress Intensity Factor corresponding to the time of crack propagation, can be deduced with a good accuracy. Key words Induction heating, rebar, crack opening displacement, images correlation, stress intensity factor, energ...|$|E
5000|$|In 2002, the {{discharge}} of Wilson Creek in its upper reaches ranged from [...] <b>per</b> <b>minute,</b> {{with an average of}} [...] <b>per</b> <b>minute.</b> In the creek's middle reaches, it ranged from 0 to [...] <b>per</b> <b>minute,</b> with an average of [...] <b>per</b> <b>minute.</b> In its lower reaches, {{the discharge}} ranged from [...] <b>per</b> <b>minute,</b> with an average of [...] <b>per</b> <b>minute.</b>|$|R
50|$|The Lackawanna River has a {{perennial}} flow. Nevertheless, it experiences low flow conditions during warm weather. The discharge {{of the river}} near Forest City was observed to range from 3904.83 to 69,568.83 gallons <b>per</b> <b>minute,</b> {{with an average of}} 35,584.83 gallons <b>per</b> <b>minute.</b> The river's discharge near Archbald ranged from 3976.60 to 323,158.40 gallons <b>per</b> <b>minute,</b> with an average of 97,130.90 gallons <b>per</b> <b>minute.</b> Below the Broadway Street Bridge, it ranged from 34,560 to 586,397 gallons <b>per</b> <b>minute</b> and averaged 222,732.46 gallons <b>per</b> <b>minute.</b> Near Coxton Road, the discharge averaged 266,478 gallons <b>per</b> <b>minute.</b>|$|R
5000|$|St. Francis - 18000 gal <b>per</b> <b>minute</b> at , or 6000 gal <b>per</b> <b>minute</b> at [...] and 6000 gal <b>per</b> <b>minute</b> at ...|$|R
40|$|Abstract Background Little {{objective}} evidence exists regarding {{what makes a}} good lecture. [*]Our purpose was to determine qualities of radiology review course lectures that are associated with positive audience evaluation. Methods 57 presentations from the Ottawa Resident Review Course (2012) were analyzed by a PGY 4 radiology resident blinded to the result of audience evaluation. Objective data extracted were: slides per minute, lines of text per text slide, words per text slide, cases per minute, <b>images</b> <b>per</b> <b>minute,</b> images per case, number of audience laughs, number of questions posed to the audience, number of summaries, inclusion of learning objectives, ending on time, use of pre/post-test and use of special effects. Mean audience evaluation scores for each talk from daily audience evaluations (up to 60 per talk) were standardized out of 100. Correlation coefficient was calculated between continuous variables and audience evaluation scores. Student T test was performed on categorical variables and audience evaluation scores. Results Strongest positive association with audience evaluation scores was for image quality (r[*]=[*] 0. 57) and number of times the audience laughed (r[*]=[*] 0. 3). Strongest negative association was between images per case and audience scores (r[*]=[*]- 0. 25). Talks with special effects were rated better (mean score 94. 3 vs. 87. 1, p[*]<[*] 0. 001). Talks with the highest image quality were rated better (mean score 94. 1 vs. 87. 5, p[*]<[*] 0. 001). Talks which contained a pre/post-test were rated better (mean score 92 vs. 87. 8, p[*]=[*] 0. 004). Conclusion Many factors go into making a great review course lecture. At the University of Ottawa Resident Review Course, high quality images, use of special effects, use of pre/post-test and humor were most strongly associated with high audience evaluation scores. High image volume per case may be negatively associated with audience evaluation scores...|$|E
40|$|Digital {{multimedia}} content is omnipresent on the Web; Google posted on August 2005, a total image size of 2; 187; 212; 422, Yahoo estimated that its index covered 1 : 5 billion of images at that time, while nowadays statistics show a continuous growth in these numbers (indicatively Flickr uploads amount {{to an average}} of about 3000 <b>images</b> <b>per</b> <b>minute).</b> Given such numbers, the availability of machine processable semantic descriptions for this content becomes a key factor for the realisation of applications of practical interest, perpetuating the challenge of what constitutes the multimedia community holy grail, i. e. the semantic gap between representations that can be automatically extracted and the underlying meaning. In the late 1970 s and early 1980 s, inuenced by the AI paradigm, the analysis and understanding of audiovisual content became a problem of achieving intelligent behaviour by simulating what humans know through computational means. Hence, the rst attempts towards knowledge-directed image analysis emerged. A period of explosive growth in approaches conditioned by knowledge followed: varying knowledge representation and reasoning schemes, in accordance with the contemporary AI assets, were proposed, and knowledge attempted to address all aspects involved, ranging from perceptual characteristics of the visual manifestations to control strategies. The broad and ambitious scope targeted by the use of knowledge, resulted in representations and reasoning mechanisms that exhibited high complexity and inexibility, while the lack of well-founded semantics further reduced e#cacy and interoperability. Research focus shifted to machine learning, which gained particular popularity as means for capturing knowledge that cannot be represented e ectively or explicitly. Recent analysis in multimedia has reached a point where detectors can be learned in a generic fashion for a signi cant number of conceptual entities. The obtained performance however exhibits versatile behaviour, reecting implications over the training set selection, similarities in visual manifestations of distinct conceptual entities, and appearance variations of the conceptual entities. A factor partially accountable for these limitations relates to the fact that machine learning techniques realise the transition from visual features to conceptual entities based solely on information regarding perceptual features. Hence, a signi cant part of the knowledge pertaining to the semantics underlying the interpretation is missed. The advent of the Semantic Web paved a new era in knowledge sharing, reuse and interoperability, by making formal semantics explicit and machine understandable rather than just machine processable. The multimedia community embraced the new technologies, utilising ontologies at rst in order to attach explicit meaning to the produced annotations (at the content and the media layers), and subsequently as means for assisting the very extraction of the annotations. The state of the art with respect to the latter approaches is characterised by particular features, among which the poor handling of uncertainty, the restricted utilisation of formal semantics and inference services, and by focus on representing associations between perceptual features and domain entities rather than logical relations between the domain entities or on modelling analysis aspects. This thesis addresses the problem of how enhanced semantic descriptions of visual content may be automatically derived through the utilisation of formal semantics and reasoning, and how the domain speci c descriptions can be transparently integrated with media related ones referring to the structure of the content. The central contributions of the thesis lie in: i) the de nition of a uni ed representation of the domain speci c knowledge required for the extraction of semantics and of the analysis speci c knowledge that implements the process of extraction, ii) the development of a formal reasoning framework that supports uncertainty handling for the purpose of the semantic integration and enrichment of initial descriptions deriving from di erent analysis systems, and iii) the de nition of an MPEG- 7 compliant ontology that formally captures the structure of {{multimedia content}} allowing for precise semantics and for serving as means for the de nition of mappings between the existing ontologies addressing multimedia content structural aspects. The rst refers to a uni ed ontology-based knowledge representation framework that allows one to model the process of extracting semantic descriptions in accordance to perceptual and conceptual aspects of the knowledge characterising the speci c domain. The use of ontologies for both knowledge components enhances the potential of sharing and reuse of the respective components, but most importantly enables the extensibility of the framework to other application domains and its sharing across di erent systems. More speci cally, semantic concepts {{in the context of the}} examined domain are de ned in an ontology, enriched with qualitative attributes (e. g., color homogeneity), low-level features (e. g., color model components distribution), object spatial relations, and multimedia processing methods (e. g., color clustering). The RDF(S) language has been used for the representation of the developed domain and analysis ontologies, while for the rules that determine how tools for multimedia analysis should be applied depending on concept attributes and low-level features, are expressed in F-Logic. The second part of the contribution refers to the development of a fuzzy DL-based reasoning framework in order to integrate image annotations at scene and region level, into a semantically consistent nal description, further enhanced by means of inference. The use of fuzzy DLs semantics allow to formally handle the uncertainty that charasterises multimedia analysis and understanding, while the use of DLs allows to bene t from the high expressivity and the e#cient reasoning algorithms in the management of the domain speci c semantics. The initial annotations forming the input may come from di erent modalities and analysis implementations, and their degrees can be re-adjusted using weights to specify the reliability of the corresponding analysis technique or modality. Finally, the third part tackles the engineering of a multimedia ontology, and more speci cally of one addressing aspects related to the structure and decomposition schemes of multimedia content. The existing MPEG- 7 based ontologies despite induced by the need for formal descriptions and precise semantics raise new interoperability issues as they build on di erent rationales and set to serve varying roles. The ontology developed within the context of this thesis re-engineers part of the MPEG- 7 speci cations to ensure precise semantics and transparency of meaning...|$|E
50|$|The {{discharge}} of Black Creek is 769.33 gallons <b>per</b> <b>minute</b> in its upper reaches. By the Hazleton High School, it increases to 2253.6 gallons <b>per</b> <b>minute</b> {{and by the}} Hazleton Wastewater Treatment Plant it increases to 5109 gallons <b>per</b> <b>minute.</b> Downstream of the wastewater treatment plant, the discharge is 22787.5 gallons per minute; at the bridge at Tomhicken it is 31227.33 gallons <b>per</b> <b>minute,</b> and at the State Route 3020 bridge it is 31824.17 gallons <b>per</b> <b>minute.</b> Above and below the Gowen Discharge, the creek's discharge is 32659.33 and 46529.5 gallons <b>per</b> <b>minute</b> respectively. Upstream of the State Route 3018 bridge, the discharge is 55684.17 gallons <b>per</b> <b>minute</b> and near the mouth it is 53938.56 gallons <b>per</b> <b>minute.</b>|$|R
40|$|To {{get around}} UAS {{limitations}} and propose a viable solution for wildlife monitoring, {{the development of}} new inventory methods is needed. However, most authors use the classic systematic transect method as data processing and statistics are easier. We thus created an application to process data from every type of flight plan and to help detect and compare observations on large datasets. WiMUAS is a small software compatible with the open-source QGIS© that allows the creation of visual maps compatible with geographical information systems based on telemetry data and payload parameters to estimate the covered area. The application also has a slider for animal detection that allows multiple observers to record and compare their results for accurate counts. We then tested it on data from a trial realized on savannah animal populations in Democratic Republic of Congo using the Falcon UAS. We created a new type of flight plan, a rosette-shaped design that can be covered in three flights,. and repeated it twice. More than 5000 images were collected during the six flights. Image projection gives an area of 12, 4 km 2 for the first trial and of 12, 1 km 2 for the second. The mean sampling rate for both test is 6, 1 %. Observers spotted buffaloes, hippos, warthogs and various antelopes with different success over an average rate of 8 <b>images</b> reviewed <b>per</b> <b>minute.</b> Resulting densities observed by the three observers are similar for each test (coefficient of variation 6, 9 and 8, 6 % respectively) but mean densities vary a lot between the two trials (23, 8 and 6, 5 animals/km 2 respectively) ...|$|R
50|$|Mental readers {{generally}} read {{at approximately}} 250 words <b>per</b> <b>minute.</b> Auditory readers read at approximately 450 words <b>per</b> <b>minute.</b> Visual readers read at approximately 700 words <b>per</b> <b>minute.</b>|$|R
5|$|Above {{the swamp}} on Twomile Run, the {{discharge}} of the stream is 511 gallons <b>per</b> <b>minute.</b> Upstream of Middle Branch Twomile Run, the discharge is 618 gallons <b>per</b> <b>minute</b> and downstream of Robbins Hollow, the discharge is 1402 gallons <b>per</b> <b>minute.</b> Downstream of the sampling site KC121, the discharge is 1893 gallons <b>per</b> <b>minute</b> and upstream of Huling Branch, the discharge is 2258 gallons <b>per</b> <b>minute.</b> The discharge {{at the mouth of}} Twomile Run ranges from 200 to 10,000 gallons <b>per</b> <b>minute,</b> which is between 0.6 and 2 percent of {{the discharge of}} Kettle Creek.|$|R
5000|$|The {{discharge}} of Pine Creek {{south of the}} community of Fountain is 2364.5 gallons <b>per</b> <b>minute.</b> The creek's discharge is 8706.47 gallons <b>per</b> <b>minute</b> at the Gap School Road bridge and at the Schwenks Road bridge, the discharge is 18807.06 gallons <b>per</b> <b>minute</b> [...] It is 20099.86 gallons <b>per</b> <b>minute</b> at the Pennsylvania Route 25 bridge near the community of Spring Glen and is 42252.4 gallons <b>per</b> <b>minute</b> at the end of State Route 4015. At the Michaels Food Products Bridge in Klingerstown, the discharge is 48122.62 gallons <b>per</b> <b>minute.</b>|$|R
5000|$|An upper {{boundary}} for {{the number}} of chest compressions was added at 120 <b>per</b> <b>minute,</b> making the current recommendation 100-120 <b>per</b> <b>minute.</b> The 2010 guidelines only stated 100+ <b>per</b> <b>minute.</b>|$|R
5000|$|Three {{models of}} {{printers}} were offered: a medium-speed printer running at 600 lines <b>per</b> <b>minute,</b> a high-speed printer running at 1,250 lines <b>per</b> <b>minute,</b> and a bill-printer running at 600 lines <b>per</b> <b>minute</b> on continuous forms and 800 lines <b>per</b> <b>minute</b> on card-stock. Like the card punches, the printers were fully buffered.|$|R
50|$|A {{variety of}} medium and {{high-speed}} drum printers could be supplied. Medium-speed printers printed at about 600 lines <b>per</b> <b>minute</b> using all available characters. The high-speed printers delivered 1080 lines <b>per</b> <b>minute</b> or 1000 lines <b>per</b> <b>minute</b> (depending on the model), printing all 64 characters per line with excellent print quality. A later model delivered up to 1150 lines <b>per</b> <b>minute.</b> When {{fitted with a}} drum having a 16-character set, the printing speed was 2,700 lines <b>per</b> <b>minute.</b>|$|R
50|$|The {{discharge}} of Lick Run {{at the mouth}} of Fork Run ranges from 1804.3 to 38028.21 gallons <b>per</b> <b>minute.</b> Between UNT 26087 and UNT 26085, the stream's discharge ranges between 2264.76 and 58079.29 gallons <b>per</b> <b>minute.</b> The stream's discharge above Flegals Run ranges from 7065.01 gallons <b>per</b> <b>minute</b> to 51556.74 gallons <b>per</b> <b>minute.</b> Near its mouth, the discharge is between 1760.18 and 59564.65 gallons <b>per</b> <b>minute.</b> The uppermost reaches of the creek nearly run dry oftentimes during the summer.|$|R
5000|$|Rate of fire: 10 rounds <b>per</b> <b>minute</b> maximum, 3 rounds <b>per</b> <b>minute</b> {{sustained}} ...|$|R
