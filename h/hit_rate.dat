1397|829|Public
25|$|CAD systems seek to {{highlight}} suspicious structures. Today's CAD systems cannot detect 100% of pathological changes. The <b>hit</b> <b>rate</b> (sensitivity) can {{be up to}} 90% depending on system and application.|$|E
25|$|The ReadyBoost {{algorithm}} was {{improved in}} Windows 7, resulting in better performance. One experiment showed reading of flash memory up to 5–10 {{times faster than}} Windows Vista due to higher <b>hit</b> <b>rate.</b>|$|E
25|$|Most bills do {{not receive}} any responses, or hits, but many bills receive two or more hits. The {{approximate}} <b>hit</b> <b>rate</b> is around 11.4%. Double- and triple-hitters are common, and bills with 4 or 5 hits are not unheard of. Almost daily, a bill receives its 6th hit. , the site record is held by a $1 bill with 15 entries (though no new hits have been reported since March 26, 2005).|$|E
40|$|In {{order to}} {{identify}} new scaffolds for drug discovery, surface plasmon resonance is frequently used to screen structurally diverse libraries. Usually, <b>hit</b> <b>rates</b> are low and identification processes are time consuming. Hence, approaches which improve <b>hit</b> <b>rates</b> and, thus, reduce the library size are required...|$|R
40|$|WWW caching necessitates {{advanced}} replacement {{policies that}} include sophisticated control logic and efficient contents management. This paper presents a constructive approach {{for the design}} and analysis of such advanced policies. Based on this approach, we develop a new caching policy, namely, PSSW. Trace-driven simulations show PSS-W outperforms most contemporary policies in both <b>hit</b> <b>rates</b> and byte <b>hit</b> <b>rates.</b> ...|$|R
30|$|The <b>hit</b> <b>rates</b> {{obtained}} {{during the}} simulations (byte <b>hit</b> <b>rates,</b> which we omit due to space limitations, follow an identical pattern, but with lower scores) show that SACS outperforms both LRU and LFU with a 10 % sample. SACS tops the two competing algorithms {{in every day}} of the simulation with an average of 1.5 % over LFU and 1.75 % over LRU (with a maximum daily difference of 3.2 % and 3.6 %, respectively). Similar results were obtained with a 10 % cache size, but with slightly higher overall <b>hit</b> <b>rates</b> for all the algorithms. The results confirm the ability of SACS to adapt to different situations and to obtain the most out of the information available to make replacement decisions. Overall, <b>hit</b> <b>rates</b> are 10 % lower than the ones obtained with a 100 % sample size, because sampling does not allow the algorithms to make a fully informed decision.|$|R
25|$|MIPS16 is an Application-Specific Extension for MIPS I {{through to}} V {{designed}} by LSI Logic and MIPS Technologies, announced on 21 October 1996 alongside its first implementation, the LSI Logic TinyRISC processor. MIPS16 was subsequently licensed by NEC Electronics, Philips Semiconductors, and Toshiba (among others); and implemented {{as an extension}} to the MIPS I, II, an III architectures. MIPS16 decreases the size of application by up to 40% by using 16-bit instructions instead of 32-bit instructions' and also improves power efficiency, the instruction cache <b>hit</b> <b>rate,</b> and is equivalent in performance to its base architecture. It is supported by hardware and software development tools from MIPS Technologies and other providers.|$|E
25|$|During Operation Allied Force, the NATO mission over Kosovo in 1999, the RAF {{contribution}} included 16 Panavia Tornados and 12 Harrier GR7s. On 27 April 1999, {{during a}} mission to attack a Serbian military depot, RAF Harriers came under heavy anti-aircraft fire, but did not suffer losses as a result. In April 1999, {{the rules of engagement}} were changed to allow Harriers to use GPS navigation and targeting during medium-altitude bombing missions. A total of 870 Harrier II sorties were carried out during the 78-day bombing campaign. The BBC reported the Harrier II had been achieving 80% direct <b>hit</b> <b>rate</b> during the conflict; a later Parliamentary Select Committee found that 24% of munitions expended in the theatre by all RAF aircraft had been precision weapons.|$|E
500|$|Princess Royal hit Derfflinger once, {{but only}} damaged two armour plates and caused a coal bunker to flood. She hit Blücher at least twice, {{including}} the shot that crippled her, {{out of a}} total of 271 [...] shells fired during the battle, a <b>hit</b> <b>rate</b> of only 0.7%. By way of contrast, her sister [...] made four hits out of 243 shells fired, a rate of 1.6%. She also fired two 13.5-inch shrapnel shells at the German airship L5 as its crew attempted to bomb the sinking Blücher, mistaking it for a British ship, despite the fact that the maximum elevation of those guns was only 20°. Princess Royal was not damaged during the battle.|$|E
40|$|Previous {{research}} suggests that defensiveness involves decreased perceptual sensitivity for intense or threatening stimuli, which is reflected in various perceptual and electroencephalographic changes. Expanding on this theme, Kline, Schwartz, and Dikman (1992) found that high defensive subjects evidence decreased perceptual acuity for a putative human pheromone, androstenone (AND), proposing an olfactory "perceptual defense" effect. Thus, the present study explored relationships between performance and electroencephalographic parameters in a visual "perceptual defense" paradigm, AND perception, and repressive-defensiveness. Except for in the spring, high-defensive subjects were in general less perceptually sensitive for AND. Excluding subjects run during the spring, detection accuracy for AND correlated negatively with identification thresholds for unpleasant words. AND perception tended to correlated positively with identification thresholds for sexual-taboo words. In general, highest identification thresholds obtained for neutral and sexual-taboo words, and lowest for pleasant and unpleasant words. Mean <b>hit</b> <b>rates</b> in a word detection task were 0. 19, 0. 17, 0. 24, 0. 44, 0. 65, and 0. 80 for six ascending durations (chance = 0. 17). For the shortest three durations (≤ 50. 1 msec), low defensive subjects had higher <b>hit</b> <b>rates</b> for neutral (NEU) versus unpleasant (UPLS), and sexual-taboo (SEX) words. In contrast, HD showed lowest <b>hit</b> <b>rates</b> for NEU and highest <b>hit</b> <b>rates</b> for SEX. At the longest three durations (≥ 66. 8 msec), LD showed lowest <b>hit</b> <b>rates</b> for NEU and UPLS, and HD had lower <b>hit</b> <b>rates</b> for pleasant words (PLS) and SEX than did LD. Confidence increased with duration, but no significant defensiveness or word category differences emerged. HD showed less alpha (8 - 13 Hz) in response to 50. 1 msec masked words at posterior leads for SEX, VPLS, and especially PLS relative to NEV, where LD showed essentially the opposite pattern. Alpha decreases for SEX correlated significantly with ≤ 50. 1 msec <b>hit</b> <b>rates</b> for SEX at 02, and correlated with ≤ 50. 1 <b>hit</b> <b>rates</b> for PLS and VPLS at 01. In response to 100. 2 msec duration masked words, all subjects showed less alpha during SEX than during NEV, especially posteriorly, which was somewhat right lateralized for HD. The results suggest that defensiveness may involve unconscious supersensitivity to emotional content that facilitates conscious subsensitivity to emotional content...|$|R
3000|$|As in Experiment 1, {{the bottom}} section of Table  1 shows the overall {{performance}} of the four scale groups. To examine whether {{the first and second}} phases of the experiment differed, we again conducted two 2 (phase 1 vs. phase 2)[*]×[*] 4 (scales) ANOVAs for <b>hit</b> <b>rates</b> and false alarm <b>rates.</b> For <b>hit</b> <b>rates,</b> phase revealed a significant main effect, F(1, 92)[*]=[*] 8.99, p[*]=[*]. 003, η 2 [...]...|$|R
50|$|The aim {{of virtual}} {{screening}} {{is to identify}} molecules of novel chemical structure that bind to the macromolecular target of interest. Thus, success of a virtual screen is {{defined in terms of}} finding interesting new scaffolds rather than the total number of hits. Interpretations of virtual screening accuracy should therefore be considered with caution. Low <b>hit</b> <b>rates</b> of interesting scaffolds are clearly preferable over high <b>hit</b> <b>rates</b> of already known scaffolds.|$|R
500|$|Rocket {{projectiles}} {{were developed}} during the Second World War. In the case of Coastal Command, {{they were to be}} used in A/S and as maritime strike weapons. For aircraft use there were two different types of head: a 60lb one with high explosive and a 25lb armour-piercing head of steel – known as the 'Rocket Spear'. Groups of four rockets were arranged on under-wing racks. Trials began in November 1942 and ended in February 1943 in respect of A/S. The firing range against U-boats was considered to be [...] or less and could be fired in pairs or all together in a single salvo. The first recorded success was No. 48 Squadron RAFs sinking of [...] on 4 June 1943. The rockets tended to follow the line of flight of the aircraft rather than the line of sight. Tests indicated a 30 percent <b>hit</b> <b>rate.</b> However, just one hit was lethal to a U-boat. Though effective against U-boats, the later DCs were favoured.|$|E
500|$|Xbox {{division}} head Phil Spencer {{had also}} hinted {{the possibility of}} adding support for games from the original Xbox. For the backwards compatibility team, after they completed the framework for Xbox 360 compatibility so that other engineering teams could take over, they turned {{to the question of}} compatibility with the original Xbox console. The program was started in November 2016, under the code name [...] "Fusion", and was led by software engineer Spencer Perreault. Perreault initially tried the same approaches as the team had done with [...] "Fission", but due to the differences in memory management sizes and chipset bit-rates, these initial tests failed. Instead, Perreault worked to bring [...] "Dolphin", a developer tool for the original Xbox, working to get its emulation correct. La Chapelle brought in a number of personal Xbox titles to test in Perreault's emulation, getting about a 10% [...] "hit rate" [...] on successes, though the variety of failures helped Perreault to identify common problems, and within a month, had improved the successful <b>hit</b> <b>rate</b> to about 90%. As with Xbox 360 backwards compatibility, the Fusion emulation enables Xbox games to be scaled to 1080p resolutions, work with Xbox One networking features, and can allow mixed-console System Link connection between all three generations of Xbox.|$|E
2500|$|The {{report was}} {{released}} by the Fraunhofer Ernst Mach Institut (EMI) and Wehrtechnische Dienststelle 91 (WTD91) on 19 April 2015. According to their 372-page report, the observed <b>hit</b> <b>rate</b> of the predominantly plastic weapon with the unsupported free-floating barrel drops down to a mere 7 percent at 100 meters when the temperature is 30°C (86°F) or above, whereas the Bundeswehr required a <b>hit</b> <b>rate</b> of 90 percent at that distance.|$|E
40|$|Cache <b>hit</b> <b>rates</b> play a {{significant}} role in program runtimes. It is possible to increase cache <b>hit</b> <b>rates</b> using various techniques. One such technique, presented in [1], is to group structure fields to maximize spatial locality. This report discusses the implementation of this idea under the Pegasus/CASH compiler framework. The Pegasus simulator was modified to produce appropriate profiling information, which was then analyzed using a weighted matching algorithm to produce improved structure field reordering recommendations. 1...|$|R
3000|$|...] {{estimates}} {{performance of}} an optimal unbiased observer), {{which are very}} similar to the observed <b>hit</b> <b>rates</b> reported in previous studies (Gregg & Samuel, 2008).|$|R
3000|$|At an SNR of 15 dB and a {{reverberation}} time RT 60 of 560 ms, the proposed algorithm (Setup V) achieved DOA <b>hit</b> <b>rates</b> of A [...]...|$|R
2500|$|Hyman {{analyzed}} {{these experiments}} and wrote they met most, {{but not all}} of the [...] "stringent standards" [...] of the joint communiqué. He expressed concerns with the randomization procedure, the reliability of which {{he was not able to}} confirm based on the data provided by Bem. Hyman further noted that although the overall <b>hit</b> <b>rate</b> of 32% was significant, the <b>hit</b> <b>rate</b> for static targets (pictures) was in fact insignificant (inconsistently with previous ganzfeld research). The overall significance of the experiments was solely due to dynamic targets (videos). In the hit rates regarding these dynamic targets, however, some interesting patterns were found that implied visual cues may have been leaked: ...|$|E
2500|$|Bem and Honorton (1994) {{investigated}} certain {{personality traits}} and characteristics as potential psi-conducive variables which they suggested {{play an important}} role in claimed ESP performance. According to parapsychologists these factors are thought to be positively correlated with increased scores in ganzfeld experiments, as compared to unselected participants. Traits and characteristics of subjects thought to increase the chance of obtaining a successful <b>hit</b> <b>rate</b> in a psi experiment include: ...|$|E
2500|$|An {{alternative}} to the ROC curve is the detection error tradeoff (DET) graph, which plots the false negative rate (missed detections) vs. the false positive rate (false alarms) on non-linearly transformed x- and y-axes. The transformation function is the quantile function of the normal distribution, i.e., the inverse of the cumulative normal distribution. It is, in fact, the same transformation as zROC, below, except that the complement of the <b>hit</b> <b>rate,</b> the miss rate or false negative rate, is used. This alternative spends more graph area on the region of interest. Most of the ROC area is of little interest; one primarily cares about the region tight against the y-axis and the top left corner – which, because of using miss rate instead of its complement, the <b>hit</b> <b>rate,</b> is the lower left corner in a DET plot. Furthermore, DET graphs have the useful property of linearity and a linear threshold behavior for normal distributions. The DET plot is used extensively in the automatic speaker recognition community, where the name DET was first used. The analysis of the ROC performance in graphs with this warping of the axes was used by psychologists in perception studies halfway through the 20th century, where this was dubbed [...] "double probability paper".|$|E
40|$|The speed {{gap between}} {{processors}} and memory continues to widen. This problem {{has led to}} an increased reliance on complex cache hierarchies. Caches are very eective for programs with near 100 % cache <b>hit</b> <b>rates,</b> but they fail on many important applications that do not exhibit sucient data locality. In the same vein, TLBs fail in their role of hiding the latency of virtual-to-physical address translation. A TLB typically contains between 64 and 512 entries, which leads to low TLB <b>hit</b> <b>rates</b> for applications with poor data locality and large working sets. We propos...|$|R
40|$|Abstract This paper {{examines}} {{the costs and}} potential benefits of long-term prefetching for content distribution. Incontrast with traditional short-term prefetching, in which caches use recent access history to predict and prefetch objects likely to be referenced in the near future, long-term prefetching uses long-term steady-state object access rates and update frequencies to identify objects to replicate to content distribution locations. Compared to demand caching, long-term prefetching increases network bandwidth and diskspace costs but may benefit a system by improving <b>hit</b> <b>rates.</b> Using analytic models and trace-based simulations, we examine several algorithms for selecting objects for long-term prefetching. We findthat although the web's Zipf-like object popularities makes it challenging to prefetch enough objects to significantly improve <b>hit</b> <b>rates,</b> systems can achieve significant benefits at modest costs by focusing theirattention on long-lived objects. 1 Introduction In spite of advances in web proxy caching techniques {{in the past few}} years, proxy cache <b>hit</b> <b>rates</b> havenot improved much. Even with unlimited cache space, passive caching suffers from uncacheable data, consistency misses for cached data and compulsory misses for new data. Prefetching attempts to overcomethese limitations of passive caching by proactively fetching content without waiting for client requests. Traditional short-term prefetching at clients uses recent access history to predict and prefetch objects likelyto be referenced in the near future and can considerably improve <b>hit</b> <b>rates</b> [8, 9, 18, 21, 32]. In this paper we examine a technique more appropriate for large proxies and content distribution networks(CDNs), namely long-term prefetching. Rather than basing prefetching decisions on the recent history of individual clients, long term prefetching seeks to increase <b>hit</b> <b>rates</b> by using global object access patterns toidentify a collection of valuable objects to replicate to caches and content distribution servers...|$|R
40|$|Recent {{studies have}} found that age is {{negatively}} associated with the accuracy of decoding emotional facial expressions; this effect of age was found for actors as well as for raters. Given that motivational differences and stereotypes may bias the attribution of emotion, the aim {{of the present study was}} to explore whether these age effects are due to response bias, that is, the unbalanced use of response categories. Thirty younger raters (19 – 30 years) and thirty older raters (65 – 81 years) viewed video clips of younger and older actors representing the same age ranges, and decoded their facial expressions. We computed both raw <b>hit</b> <b>rates</b> and bias-corrected <b>hit</b> <b>rates</b> to assess the influence of potential age-related response bias on decoding accuracy. Whereas raw <b>hit</b> <b>rates</b> indicated significant effects of both the actors’ and the raters’ ages on decoding accuracy for sadness, these age effects were no longer significant when response bias was corrected. Our results suggest that age effects on the accuracy of decoding facial expressions may be due, at least in part, to age-related response bias...|$|R
2500|$|In 2010, Lance Storm, Patrizio Tressoldi, and Lorenzo Di Risio {{analyzed}} 29 ganzfeld {{studies from}} 1997 to 2008. Of the 1,498 trials, 483 produced hits, corresponding to a <b>hit</b> <b>rate</b> of 32.2%. This <b>hit</b> <b>rate</b> is statistically significant with p < [...]001. Participants selected for personality traits and personal characteristics {{thought to be}} psi-conducive were found to perform significantly better than unselected participants in the ganzfeld condition. Hyman (2010) published a rebuttal to Storm et al. According to Hyman [...] "reliance on meta-analysis as the sole basis for justifying the claim that an anomaly exists and that the evidence for it is consistent and replicable is fallacious. It distorts what scientists mean by confirmatory evidence." [...] Hyman wrote the ganzfeld studies have not been independently replicated and have failed to produce evidence for psi. Storm et al. published a response to Hyman claiming the ganzfeld experimental design {{has proved to be}} consistent and reliable but parapsychology is a struggling discipline that has not received much attention so further research on the subject is necessary. Rouder et al. in 2013 wrote that critical evaluation of Storm et al.'s meta-analysis reveals no evidence for psi, no plausible mechanism and omitted replication failures. Storm et al. in reply showed several mistakes in Rouder et al.'s paper.|$|E
2500|$|After a 2-year musical hiatus (during {{which she}} {{appeared}} in six films), Lo released her 10th studio album, Process, on 10 June 2007, upon joining her new record company [...] "WOW Music". She is the first artist to release a USB flash memory digital album in Asia. Process contained the radio chart-topping hit [...] "Love to the Limit" [...] (愛到不能) and a Mandarin song, [...] "Run Slowly" [...] (走慢點) which was previously available for download at Candy's temporary website http://candylo.hk and saw a <b>hit</b> <b>rate</b> of more than 200,000 before being taken down.|$|E
2500|$|Sheldrake's The Sense of Being Stared At was {{published}} in 2003. The book explores telepathy, precognition, and the [...] "psychic staring effect". It reported on an experiment Sheldrake conducted where blindfolded subjects guessed whether persons were staring at them or at another target. Sheldrake reported subjects exhibiting a weak sense of being stared at, but no sense of not being stared at, and attributed the results to morphic resonance. Sheldrake reported a <b>hit</b> <b>rate</b> of 53.1%, describing two subjects as [...] "nearly always right, scoring way above chance levels".|$|E
3000|$|... for Setup V {{over all}} SNR (bottom panel in column (a)) and 55.3 % for Setup II (top panel in column (a)). The {{proposed}} algorithm (Setup V) results in high <b>hit</b> <b>rates</b> even at low SNR.|$|R
40|$|This paper {{presents}} LRU-SP, a size-adjusted and popularity-aware {{extension to}} Least Recently Used (LRU) for caching web objects. The standard LRU, focusing on recently used and equal sized objects, is {{not suitable for}} the web context because web objects vary dramatically in size and the recently accessed objects may possibly differ from popular ones. LRU-SP is built on two LRU extensions, namely Size-Adjusted LRU and Segmented LRU. As LRU-SP differentiates object size and access frequency, it can achieve higher <b>hit</b> <b>rates</b> and byte <b>hit</b> <b>rates.</b> Furthermore, an efficient implementation scheme is developed and trace-driven simulations are performed to compare LRUSP against Size-Adjusted LRU, Segmented LRU and LRV caching algorithms...|$|R
40|$|The World Wide Web {{is growing}} {{exponentially}} and already {{accounts for a}} big percentage of the traffic in the Internet. Often popular Web servers are overloaded, hot documents travel many times across the same congested links, and receivers experience slow response times. Cache <b>hit</b> <b>rates</b> can be significantly increased by having caches cooperate. In this report we extensively analyze the log entries of the Eurecom and other Squid caches [] {{in order to show}} what <b>hit</b> <b>rates</b> might be achieved with cooperating caches. We also discuss how to chose a parent cache out of several sibling caches based on ping and download round trip times. ...|$|R
2500|$|Hyman {{wrote the}} autoganzfeld {{experiments}} were flawed {{because they did}} not preclude the possibility of sensory leakage. In 2010, Lance Storm, Patrizio Tressoldi, and Lorenzo Di Risio analyzed 29 ganzfeld studies from 1997 to 2008. Of the 1,498 trials, 483 produced hits, corresponding to a <b>hit</b> <b>rate</b> of 32.2%. This <b>hit</b> <b>rate</b> is statistically significant with p < [...]001. Participants selected for personality traits and personal characteristics thought to be psi-conducive were found to perform significantly better than unselected participants in the ganzfeld condition. Hyman (2010) published a rebuttal to Storm et al. According to Hyman [...] "reliance on meta-analysis as the sole basis for justifying the claim that an anomaly exists and that the evidence for it is consistent and replicable is fallacious. It distorts what scientists mean by confirmatory evidence." [...] Hyman wrote the ganzfeld studies have not been independently replicated and have failed to produce evidence for telepathy. Storm et al. published a response to Hyman claiming the ganzfeld experimental design has proved to be consistent and reliable but parapsychology is a struggling discipline that has not received much attention so further research on the subject is necessary. Rouder et al. 2013 wrote that critical evaluation of Storm et al.'s meta-analysis reveals no evidence for telepathy, no plausible mechanism and omitted replication failures.|$|E
2500|$|Although Iron Dome {{has proven}} its {{effectiveness}} against rocket attacks, Defense Ministry officials are concerned {{it will not}} be able to handle more massive arsenals possessed by Hezbollah in Lebanon should a conflict arise. [...] Although in Operation Protective Edge it had a 90 percent <b>hit</b> <b>rate</b> against only rockets determined to be headed for populated areas, 735 intercepts were made at a cost of $70,000–$100,000 per interceptor; with an estimated 100,000 rockets possessed by Hezbollah, Iron Dome systems could be fiscally and physically overwhelmed by dozens of incoming salvos. [...] Directed energy weapons are being investigated as a complement to Iron Dome, and are prized for less costly defense capabilities provided both in terms of system cost and cost per shot. [...] Solid-state lasers worldwide have power levels ranging from 10–40kW; to destroy a rocket safely from [...] away, several low-power beams could coordinate and converge on one spot to burn through its outer shell and destroy it. [...] Because laser beams become distorted under fog or heavy cloud conditions, any laser would be used in conjunction with Iron Dome rather than as a replacement for it.|$|E
2500|$|On 26 July 1930, Bader was {{commissioned}} {{as a pilot}} officer into No. 23 Squadron RAF based at Kenley, Surrey. Flying Gloster Gamecocks and soon after, Bristol Bulldogs, Bader became a daredevil while training there, often flying illegal and dangerous stunts. While very fast for its time, the Bulldog had directional stability problems at low speeds, which made such stunts exceptionally dangerous. Strict orders were issued forbidding unauthorised aerobatics below [...] Douglas took this as an unnecessary safety rule rather than an order to be obeyed. After one training flight at the gunnery range, Bader achieved only a 38 percent <b>hit</b> <b>rate</b> on a target. Receiving jibes from a rival squadron (No. 25 Squadron RAF), Bader took off to perform aerobatics and show off his skill. It was against regulations, and seven out of 23 accidents caused by ignoring regulations had proven fatal. The CO of No. 25 Squadron remarked that he would order Bader to face a court-martial if Bader was in his unit. The COs of Bader's unit, Harry Day and Henry Wollett, gave the pilots more latitude, although Day encouraged them to recognise their own limits.|$|E
40|$|We study caching {{strategies}} for proxies that cache VBR encoded continuous media objects for highly interactive streaming applications. First, we develop {{a model for}} streaming VBR encoded continuous media objects. This model forms {{the basis for a}} stream admission control criterion and our study of caching strategies. We find that unlike conventional web caches, proxy caches for continuous media objects need to replicate or stripe objects to achieve high <b>hit</b> <b>rates.</b> We develop novel caching strategies that either implicitly or explicitly track the request pattern and cache (and replicate) objects accordingly. Our numerical results indicate that our caching strategies achieve significantly higher <b>hit</b> <b>rates</b> than caching without object replication...|$|R
25|$|David Jay Brown, who {{conducted}} {{some of the}} experiments for Sheldrake, states {{that one of the}} subjects who was reported as having the highest <b>hit</b> <b>rates</b> was {{under the influence of the}} drug MDMA (Ecstasy) during the trials.|$|R
3000|$|Figure 12 (a) {{shows the}} results for several SNR {{conditions}} from 20 to − 5 dB without reverberation. It is apparent that the proposed algorithm (Setup V) outperforms Setup II for all conditions, resulting in mean <b>hit</b> <b>rates</b> [...]...|$|R
