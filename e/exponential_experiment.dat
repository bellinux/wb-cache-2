3|68|Public
40|$|The {{theory of}} the pulsed neutron {{experiment}} and the 2 zone <b>exponential</b> <b>experiment</b> is developed in this thesis. The energy dependent diffusion approximation to the Boltzniann neutron transport equation is used throughout. A concise operator and vector foi^malism is introduced ii^ich permits a unified treatment {{of the variety of}} representations (multigroup, polynomial expansion, etc.) of the diffusion equation. Familiarity with this formalism is developed by first considering the well understood eigenvalue {{theory of the}} asymptotic pulsed neutron experiment. New material is presented in Chapter 3 in that the theory of the pre-asymptotic pulsed neutron experiment given assumes a finite and not an instantaneous pulse. Part of the contents of this chapter have been presented at the IAEA Symposium on Neutron Thermalization and Reactor Spectra, Ann Arhor (1967). In Chapter 4 the operator formalism allows the energy dependent theory of the <b>exponential</b> <b>experiment</b> to be treated in a manner which is new and amenable to numerical calculation. In both this chapter and the preceeding chapters the results of computations are compared with either experiment or other calculations. Finally, in order to complement an earlier discussion of the discrete eigenvalue spectra of the thermalization operator, a review of the properties of the continuous spectrum is given...|$|E
40|$|The neutron {{absorption}} of bulk granular graphite has been measured in a classical exponential diffusion experiment. Our first measurements of April 2002 implementing both exponential decay and pulsed die-away experiments {{and using the}} TUNL pulsed accelerator at Duke University as a neutron source indicated a capture cross section for graphite a striking factor of three lower than the measured value for carbon of 3. 4 millibarns. Therefore a new <b>exponential</b> <b>experiment</b> with an improved geometry enabling greater accuracy has been performed giving an apparent cross section for carbon {{in the form of}} bulk granular graphite of less than 0. 5 millibarns. This result confirms our first result and is also consistent with less than one part per million of boron in our graphite. The bulk density of the graphite is 1. 02 compared with the actual particle density of 1. 60 indicating a packing fraction of 0. 64 or a void fraction of 0. 36. We suspect that the apparent suppression of absorption in bulk graphite may be associated with the strong coherent diffraction of neutrons that dominates neutron transport in graphite. Coherent diffraction has never been taken into account in graphite reactor design and no neutron transport code including general use codes such as MCNP incorporate diffraction effects even though diffraction dominates many practical thermal neutron transport problems...|$|E
40|$|For {{the purpose}} of {{studying}} a heterogeneous multiplying medium, the author proposes dividing it {{into a number of}} homogeneous regions having the diffusion and absorption properties of the pure moderator. The fuel elements, represented by portions of active surfaces of zero thickness, constitute the separation surfaces of these sub-regions. Externally, the system is bounded by passive surfaces devoid of fissionable nuclei. The theory of diffusion involving several groups of neutrons is applied to each sub-region, while the productive and absorbing effects of fissionable materials are represented by the conditions on the active surfaces. To apply the method, it is necessary to know certain parameters of the behaviour of an active surface in a known flux. The moderator group constants are presumed to be known. The author shows that, theoretically, a single <b>exponential</b> <b>experiment,</b> carried out with {{a very small number of}} rods, should suffice to determine these parameters experimentally. The facility used for these experiments is a subcritical assembly; the fuel is uranium oxide containing 1. 8 % uranium 235; a water moderator is used. Measurements made for a series of different configurations confirm that the parameters sought depend solely on the nature of the fuel. The results are used to forecast the behaviour of a subcritical and a critical lattice. In the first case the calculations are verified directly by experiment; in the second, they are checked by comparison with the published results...|$|E
40|$|A {{characterization}} {{of a certain}} class of <b>exponential</b> <b>experiments,</b> so called E-ex-periments, is given. This allows us to give necessary and su±cient conditions for a sequence of experiments to converge to an E-experiment. The obtained results are valid for Gaussian shift experiments. Some asymptotic approximations using E-experiments are studied. ...|$|R
40|$|A {{multiple}} {{casting process}} {{was developed for}} the production of 0. 590 plus or minus 0. 005 -in. diam. by 12 -in. long solid right cylinders. The process involves vacuum induction melting in MgZrO/sub 3 / coated graphite crucibles and casting into coated graphite molds. Thirty-eight hundred fifty 90 wt. % U of 1. 5 wt. % enrichment- 10 wt. % Mo as-cast pins were produced for HNPF <b>Exponential</b> <b>Experiments.</b> (auth...|$|R
40|$|Basic physics {{parameters}} are being determined for lattices of slightly enriched fuel in moderators consisting of D/sub 2 /O [...] H/sub 2 /O mixtures of different concentrations. The principal effort {{was devoted to}} procurement and erection of equipment and materials needed for the critical experiments with D/ sub 2 /O in the moderator, the <b>exponential</b> <b>experiments,</b> the hot <b>exponential</b> <b>experiments,</b> and the neutron age experiments. The erection of these facilities and the D/sub 2 /O handling system was completed and checked out. The fuel rod preparation was completed, and all necessary hazard evaluations and linense applications were submitted. Two critical experiments with 4 %-enriched UO/sub 2 / fuel and H/sub 2 /O moderator (one clean and one poisoned with boric acid) were completed. In these cores the critical mass, critical buckling, thermal disadvantage factor, and cadmium ratio of U/sup 235 / were measured and the data are reported. The BPG computer code, which {{will be used to}} analyze the experiments with D/sub 2 /O in the moderator, was completed and checked out. The accuracy of the code was checked by computing a variety of H/sub 2 /O and D/sub 2 / O moderated critical experiments and applicable neutron age measurements and comparing results with those obtained by other standard calculational methods. Calculations supporting the planning and design of the experiments also continued. (auth...|$|R
40|$|Bayesian {{networks}} are not well-formulated for continuous variables. The majority of recent works dealing with Bayesian inference are restricted only to special types of continuous {{variables such as}} the conditional linear Gaussian model for Gaussian variables. In this context, an exact Bayesian inference algorithm for clusters of continuous variables which may be approximated by independent component analysis models is proposed. The complexity in memory space is linear and the overfitting problem is attenuated, while the inference time is still <b>exponential.</b> <b>Experiments</b> for multibiometric score fusion with quality estimates are conducted, and {{it is observed that}} the performances are satisfactory compared to some known fusion techniques...|$|R
40|$|<b>Exponential</b> <b>experiments</b> {{have been}} carried out to give the {{materials}} buckling of a number of near—homogeneous U 235 /aluminium alloy fuelled systems having fertile oxides intimately mixed with the BeO moderator. Relative fission rates of U 235, U 233, and Pu 239 were also measured in the equilibrium spectrum region of each assembly. Five assemblies having 5 w/o natural uranium oxide in BeO were investigated for a range of BeO/U 235 atomic ratios from 1500 : 1 to 5700 : 1. A similar range covering four assemblies was examined for 5 w/o thorium oxide in BeO. A comparison of the experimental results with diffusion theory calculations is included...|$|R
40|$|A {{literature}} search {{has been conducted}} to locate sources of neutronics data for light water moderated systems which contain thorium and/or uranium- 233. It is concluded that insufficient data is currently available to validate neutronics design methods for licensing the {sup 233 }UO{sub 2 }-ThO{sub 2 } fuel cycle in light water reactors. A summary of the neutronics data sources found is reported in this document. These sources include critical and <b>exponential</b> <b>experiments</b> with lattices of fuel rods containing {sup 233 }U + Th or {sup 235 }U + Th. A few experiments using homogeneous aqueous solutions of {sup 233 }UO{sub 2 }(NO{sub 3) {sub 2 } or {sup 233 }UO{sub 2 }F{sub 2 } are also included. The only critical lattice data using both {sup 233 }U + Th came from the LWBR program. All these experiments were zoned radially {{and in most cases}} axially also. Geometrically clean lattice critical data were measured for the CETR and TUPE programs. Both series used {sup 235 }UO{sub 2 }-ThO(sub 2 } pellets. A series of 21 <b>exponential</b> <b>experiments</b> using 3 % {sup 233 }UO{sub 2 } - 97 % ThO{sub 2 } fuel vibratory compacted to 92 % of theoretical density in Zircaloy- 2 tubing was performed at BNL using both unpoisoned and boric acid poisoned H{sub 2 }O moderator. For completeness, homogeneous systems are listed in which basic neutronics data have been measured. However, it is expected that most data concerning homogeneous systems will be applied to criticality safety problems rather than neutronics methods validation...|$|R
40|$|We {{propose a}} stable {{explicit}} numerical integration method based on matrix exponential operator that enables accurate large time stepping for transient analysis. We utilize Krylov subspace projection {{to reduce the}} computational complexity of matrix <b>exponential.</b> Numerical <b>experiments</b> show {{the advantages of the}} proposed method over backward Euler in terms of accuracy and performance. © 2011 IEEE. link_to_subscribed_fulltex...|$|R
3000|$|Figure 5 a {{shows the}} results {{obtained}} for the <b>exponential</b> arrivals <b>experiment</b> while Fig. 5 b shows results for the bursty arrivals experiment. It {{can be observed in}} Fig. 5 a that both RAP-IE and RAP-AllApps are able to generate the optimal plans at each decision stage while RAP-OneApp can generate the optimal plan in all stages except decision stages 7, 8, and 9. In most decision stages the application which causes the most reduction in the overall SV value has also the highest V [...]...|$|R
40|$|Abstract In this article, we derive {{and study}} {{symmetric}} <b>exponential</b> integrators. Numerical <b>experiments</b> are performed for the cubic Schrödinger equation and comparisons with classical exponential integrators and other geometric methods are also given. Some {{of the proposed}} methods preserve the L 2 -norm and/or {{the energy of the}} system...|$|R
40|$|We {{demonstrate}} {{the ability to}} control the spontaneous emission from a superconducting qubit coupled to a cavity. The time domain profile of the emitted photon is shaped into a symmetric truncated <b>exponential.</b> The <b>experiment</b> is enabled by a qubit coupled to a cavity, with a coupling strength that can be tuned in tens of nanoseconds while maintaining a constant dressed state emission frequency. Symmetrization of the photonic wave packet will enable use of photons as flying qubits for transfering the quantum state between atoms in distant cavities. Comment: 15 pages, 6 figure...|$|R
40|$|AbstractTo {{identify}} RNA motifs {{interacting with}} 5 S rRNA, a systematic evolution of ligands by <b>exponential</b> enrichment <b>experiment</b> was applied. Some {{of the resulting}} RNA aptamers contained a consensus sequence similar to the sequence in the loop region of helix 89 of 23 S rRNA. We show that the synthetic helix 89 RNA motif indeed interacted with 5 S rRNA and that the region around loop B of 5 S rRNA {{was involved in this}} interaction. These results suggest the presence of a novel RNA–RNA interaction between 23 S rRNA and 5 S rRNA which may {{play an important role in}} the ribosome function...|$|R
40|$|A hollow {{waveguide}} mid-infrared gas sensor {{operating from}} 1000 cm{sup - 1 } to 4000 cm{sup - 1 } has been developed, optimized, and its performance characterized by combining a FT-IR spectrometer with Ag/Ag-halide hollow core optical fibers. The hollow core waveguide simultaneously {{serves as a}} light guide and miniature gas cell. CH{sub 4 } was used as test analyte during <b>exponential</b> dilution <b>experiments</b> for accurate determination of the achievable limit of detection (LOD). It is shown that the optimized integration of an optical gas sensor module with FT-IR spectroscopy provides trace sensitivity at the few hundreds of parts-per-billion concentration range (ppb, v/v) for CH{sub 4 }...|$|R
40|$|Simulation {{schemes for}} {{probabilistic}} inference in Bayesian belief networks offer many advantages over exact algorithms; for example, these schemes have a linear and thus predictable runtime while exact algorithms have <b>exponential</b> runtime. <b>Experiments</b> {{have shown that}} likelihood weighting {{is one of the}} most promising simulation schemes. In this paper, we present a new simulation scheme that generates samples more evenly spread in the sample space than the likelihood weighting scheme. We show both theoretically and experimentally that the stratified scheme outperforms likelihood weighting in average runtime and error in estimates of beliefs. Comment: Appears in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (UAI 1994...|$|R
30|$|In [145], Fiedler et al. {{test the}} IQX {{hypothesis}} {{according to which}} QoE and QoS parameters are connected through an <b>exponential</b> relationship. Their <b>experiment</b> validates the IQX hypothesis for VoIP services, where PESQ-generated MOS is expressed {{as a function of}} packet loss, and reordering ratio caused by jitter. For web surfing, exponential mappings are shown to outperform a previously published logarithmic function.|$|R
40|$|The {{replication}} of phage-specific RNA duplexes in cells {{infected with the}} bacteriophage MS 2 has been studied by means of density labeling and CsCl equilibrium density sedimentation. It {{was found that the}} input parental RNA strand enters a duplex and that progeny duplexes appear soon afterwards, with approximately <b>exponential</b> kinetics. Chase <b>experiments</b> indicate that most of the parental and progeny duplexes are not conserved...|$|R
40|$|This paper studies {{boosting}} algorithms {{that make}} a single pass over a set of base classiers. We rst analyze a one-pass algorithm {{in the setting of}} boosting with diverse base classiers. Our guarantee {{is the same as the}} best proved for any boosting algo-rithm, but our one-pass algorithm is much faster than previous approaches. We next exhibit a random source of examples for which a picky variant of Ad-aBoost that skips poor base classiers can outperform the standard AdaBoost al-gorithm, which uses every base classier, by an <b>exponential</b> factor. <b>Experiments</b> with Reuters and synthetic data show that one-pass boosting can sub-stantially improve on the accuracy of Naive Bayes, and that picky boosting can sometimes lead to a further improvement in accuracy. ...|$|R
40|$|A {{relatively}} inexpensive reactor {{for the specific}} purpose of testing a sub-critical portion of another reactor under conditions that would exist during actual operation is discussed. It is concluded that an engineering tool for reactor development work that bridges the present gap between <b>exponential</b> and criticality <b>experiments</b> and the actual full scale operating reactor is feasible. An {{example of such a}} test reactor which would not entail development effort to ut into operation is depicted...|$|R
40|$|We present {{experimental}} results on resonant spin tunnelling {{in a single}} crystal of Mn$_{ 12 }$- 2 Cl benzoate with different concentration of dislocations. The time evolution of the magnetisation follows the stretched exponential over a few time decades. The values of parameters of stretched <b>exponential</b> deduced from <b>experiment</b> {{have been used to}} determine the concentration of dislocations before and after the cooling-annealing process, using the algorithm recently suggested by Garanin and Chudnovsky. Comment: 7 pages, 2 figure...|$|R
40|$|When use {{the image}} mutual {{information}} {{to assess the}} quality of reconstructed image in pseudo-thermal light ghost imaging, a negative exponential behavior {{with respect to the}} measurement number is observed. Based on information theory and a few simple and verifiable assumptions, semi-quantitative model of image mutual information under varying measurement numbers is established. It is the Gaussian characteristics of the bucket detector output probability distribution that leads to this negative <b>exponential</b> behavior. Designed <b>experiments</b> verify the model. Comment: 13 pages, 6 figure...|$|R
40|$|We {{investigate}} the electrostatic charging of an agitated bed of identical grains using simulations, mathematical modeling, and experiments. We simulate charging with a discrete-element model including electrical multipoles {{and find that}} infinitesimally small initial charges can grow exponentially rapidly. We propose a mathematical Turing model that defines conditions for exponential charging to occur and provides insights into the mechanisms involved. Finally, we confirm the predicted <b>exponential</b> growth in <b>experiments</b> using vibrated grains under microgravity, and we describe novel predicted spatiotemporal states that merit further study...|$|R
40|$|In {{this paper}} {{a variety of}} {{shrinkage}} methods for estimating unknown population parameters has been considered. Aprior distribution for the parameters around their natural origins has been postulated and the ordinary Bayes estimators are used in place of natural origins in the ordinary shrinkage estimators to obtain Bayesian shrinkage estimators. The results are applied {{to the problem of}} estimating the location and scale parameters and the reliability function of the two-parameter <b>exponential</b> distribution. Simulation <b>experiments</b> are used to study the performances of these estimators...|$|R
40|$|Boosted {{decision}} trees typically yield good accuracy, precision, and ROC area. However, {{because the}} outputs from boosting {{are not well}} calibrated posterior probabilities, boosting yields poor squared error and cross-entropy. We empirically demonstrate why AdaBoost predicts distorted probabilities and examine three calibration methods for correcting this distortion: Platt Scaling, Isotonic Regression, and Logistic Correction. We also experiment with boosting using log-loss {{instead of the usual}} <b>exponential</b> loss. <b>Experiments</b> show that Logistic Correction and boosting with log-loss work well when boosting weak models such as decision stumps, but yield poor performance when boosting more complex models such as full decision trees. Platt Scaling and Isotonic Regression, however, significantly improve the probabilities predicted by both boosted stumps and boosted trees. After calibration, boosted full decision trees predict better probabilities than other learning methods such as SVMs, neural nets, bagged decision trees, and KNNs, even after these methods are calibrated...|$|R
40|$|In {{condensed}} {{matter the}} formation of a muonium atom from a positive muon and an election is described usually with a first order kinetic equation which assumes that the process is random and that the charge distribution is uniform. According to this model the muon polarization function as a function of time should reduce to an <b>exponential</b> law. <b>Experiments</b> in superfluid helium demonstrates that this is incorrect. Our proposed technique allows to reconstruct the muonium formation rate function from the SR histogram in low transverse magnetic field without presupposing a particular theoretical form, i. e. with no parametrization. The technique is based on solving the integral equation of the first kind for the muon polarization function using the maximum likelihood method. The obtained results are of fundamental importance for the analysis of the charge kinetics in superfluid helium. Ó 2000 Elsevier Science B. V. All rights reserved...|$|R
40|$|A {{simulated}} annealing procedure with acceptance-probability control {{instead of the}} usual temperature control is proposed. The algorithm presented here has proved to be fully insensitive to initial parameters values, free of local-minima trapping problems, and shows superior convergence compared to adaptive-step classical {{simulated annealing}} with <b>exponential</b> cooling schedule. <b>Experiments</b> on computer generated synthetic data (with noise), closely resembling the optical constants of a metal, were performed to verify the effectiveness of the algorithm. The algorithm is then applied to parameter estimation of the model of optical constants of aluminum...|$|R
40|$|Stochastic variational {{inference}} for collapsed models {{has recently been}} successfully applied to large scale topic modelling. In this paper, we propose a stochastic collapsed {{variational inference}} algorithm in the sequential data setting. Our algorithm is applicable to both finite hidden Markov models and hierarchical Dirichlet process hidden Markov models, and to any datasets generated by emission distributions in the <b>exponential</b> family. Our <b>experiment</b> results on two discrete datasets show that our inference is both more efficient and more accurate than its uncollapsed version, stochastic variational inference. Comment: NIPS Workshop on Advances in Approximate Bayesian Inference, 201...|$|R
40|$|We {{provide a}} new {{analysis}} of an efficient margin-based algorithm for selective sampling in classification problems. Using the so-called Tsybakov low noise condition to parametrize the instance distribution, we show bounds on the convergence rate to the Bayes risk {{of both the}} fully supervised and the selective sampling versions of the basic algorithm. Our analysis reveals that, excluding logarithmic factors, the average risk of the selective sampler converges to the Bayes risk at rate N −(1 +α) (2 +α) / 2 (3 +α) where N denotes the number of queried labels, and α> 0 is the exponent in the low noise condition. For all α> √ 3 − 1 ≈ 0. 73 this convergence rate is asymptotically faster than the rate N −(1 +α) /(2 +α) achieved by the fully supervised {{version of the same}} classifier, which queries all labels, and for α → ∞ the two rates exhibit an <b>exponential</b> gap. <b>Experiments</b> on textual data reveal that simple variants of the proposed selective sampler perform much better than popular and similarly efficient competitors. ...|$|R
40|$|We {{report on}} both {{theoretical}} developments of and computational {{experience with the}} patchwork rejection technique as studied in [20] and [21]. The basic approach is due to Minh [13] who suggested a special sampling method for the gamma distribution. Its general objective is to rearrange the area below the density or histogram f(x) {{in the body of}} the distribution by certain point reflections such that variates may be generated efficiently within a large center interval. This is carried out via uniform hat functions combined with minorizing rectangles for immediate acceptance of one transformed uniform deviate. The remaining tails of f(x) are covered by <b>exponential</b> functions. <b>Experiments</b> show that patchwork rejection algorithms are in general faster than its competitors at the cost of higher set [...] up times. Categories and Subject Descriptors: G. 3 [Probability and Statistics]: Random number generation; G. 3 [Probability and Statistics]: Statistical Software; I. 6. 1 [Simulation and Modeling]: Simulation Theory General Terms: Random Variate Transformations, Algorithms, Stochastic Simulation Additional Key Words and Phrases: Random variate generation, patchwork rejection, sampling techniques, unimodal distributions...|$|R
40|$|Pattern {{recognition}} problems span a {{broad range}} of applications, where each ap-plication has its own tolerance on classification error. The varying levels of risk associ-ated with many pattern recognition applications indicate the need for a versatile algorithm with the ability to measure its own reliability. In this work, the supervised incremental learning algorithm Learn++ [1, 2], which exploits the synergistic power of an ensemble of classifiers, is further developed to add the capability of assessing its own confidence. Estimation of the true generalization performance of the classifier as well as the confi-dences on classification of individual data instances is investigated separately. Several confidence estimation techniques are explored such as majority voting, variance based confidence estimation, and the weighted <b>exponential</b> method. <b>Experiments</b> for incorporating confidence estimation techniques for evaluating the confidence of decisions made by Learn++ produced promising results, with weighted exponential based confidence estimation providing the best performance. The objective of the confidence experiments was to evaluate the algorithm's ability to assess the confi-dence in its own decisions. In addition to the ability of Learn++ to assess its own confi...|$|R
40|$|Participants in two {{experiments}} moved a mouse-like {{device to}} the right to move a cursor on a computer screen to a target position. The cursor was invisible during motion but reappeared {{at the end of each}} movement. The relationship between the amplitudes of the cursor movement and the mouse movement was <b>exponential</b> in <b>Experiment</b> 1 and logarithmic in Experiment 2 for two groups of participants, while it was linear for the control groups in both experiments. The results of both experiments indicate that participants adjusted well to the external transformation by developing an internal model that approximated the inverse of the external transformation. We introduce a method to determine the locus of the internal model. It indicates that the internal model works at a processing level that either preceded specification of movement amplitude, or had become part of movement amplitude specification. Limited awareness of the nonlinear mouse–cursor relationship and the fact that a working-memory task had little effect on performance suggest that the internal model is modular and not dependent on high-level cognitive processe...|$|R
40|$|This work {{presents}} an approach for the modeling and numerical optimization of ball joints within a Marker-less Motion Capture (MoCap) framework. In skeleton based approaches, kinematic chains {{are commonly used}} to model 1 DoF revolute joints. A 3 DoF joint (e. g. a shoulder or hip) is consequently modeled by concatenating three consecutive 1 DoF revolute joints. Obviously such a representation is not optimal and singularities can occur. Therefore, we propose to model 3 DoF joints with spherical joints or ball joints using the representation of a twist and its exponential mapping (known from 1 DoF revolute joints). The exact modeling and numerical optimization of ball joints requires additionally the adjoint transform and the logarithm of the <b>exponential</b> mapping. <b>Experiments</b> with simulated and real data demonstrate that ball joints can better represent arbitrary rotations than the concatenation of 3 revolute joints. Moreover, we demonstrate that the 3 revolute joints representation {{is very similar to}} the Euler angles representation and has the same limitations in terms of singularities. 1...|$|R
30|$|The Markov state number must be finite {{and not too}} large; how to map {{the vast}} number of {{coefficients}} with limited states has great meaning. The method we give is to map states with the coefficients according to various presupposed function models. Those functions are just envelopes of discrete matching probabilities for each state. The matching probability is defined as follows: suppose the number of all coefficients is M {{and the number of}} coefficients corresponding to a specific state is K, then the matching probability for this state is K/M. The simplest model is the average function, i.e., mapping the states evenly with the coefficients according to a fixed ratio determined by the number of state N. Though the average function model can avoid matching too many outbound coefficients to the same state and each state stands for coefficients evenly, it fails to employ thoroughly the regularity of coefficient distribution analyzed above. Therefore, besides the average function, we considered other models like absolute linear function, quadratic function, Gaussian function, and <b>exponential</b> function. <b>Experiment</b> results are given in next section, and the comparison will demonstrate which function model reaches our target best.|$|R
40|$|In many {{industrial}} applications of big data, the Jaccard Similarity Computation {{has been widely}} {{used to measure the}} distance between two profiles or sets respectively owned by two users. Yet, one semi-honest user with unpredictable knowledge may also deduce the private or sensitive information (e. g., the existence of a single element in the original sets) of the other user via the shared similarity. In this paper, we aim at solving the privacy issues in Jaccard similarity computation with strict differential privacy guarantees. To achieve this, we first define the Conditional ϵ-DPSO, a relaxed differential privacy definition regarding set operations, and prove that the MinHash-based Jaccard Similarity Computation (MH-JSC) satisfies this definition. Then for achieving strict differential privacy in MH-JSC, we propose the PrivMin algorithm, which consists of two private operations: 1) the Private MinHash Value Generation that works by introducing the Exponential noise to the generation of MinHash signature. 2) the Randomized MinHashing Steps Selection that works by adopting Randomized Response technique to privately select several steps within the MinHashing phase that are deployed with the <b>Exponential</b> mechanism. <b>Experiments</b> on real datasets demonstrate that the proposed PrivMin algorithm can successfully retain the utility of the computed similarity while preserving privacy. Comment: 27 pages, 6 figures, 4 table...|$|R
40|$|In certain <b>exponential</b> {{absorption}} <b>experiments,</b> notably {{measurements of}} cross sections by transmission, {{it is important}} to achieve minimum statistical error in a limited time, or to minimize the counting time required to measure the absorption coefficient with a preassigned accuracy. The conditions required to attain these ends, i. e., the geometry for optimum transmission, and the best apportionment of counting times among the incident and transmitted beams and background, have been investigated {{for a wide range of}} relative backgrounds (0. 001 to 0. 01), and for two geometries: (1) Beam area fixed, absorber thickness alone is varied, and (2) Beam area and absorber thickness are both disposable parameters, while the total amount of absorber intercepting the beam remains fixed. In both cases the incident flux density and the background rate are assumed constant. The optimum transmissions are shown to be, in general, considerably smaller than those commonly used in absorption experiments. Thus, in Case 1, a useful rule is to employ a transmission of about 0. 1 for low backgrounds, 0. 2 for moderate backgrounds, and 0. 3 for high backgrounds. The following have also been determined: (a) minimum statistical error for a given total counting time, (b) statistical error and the best distribution of counting times for nonoptimum geometry, and (c) sensitivity of the accuracy or total counting time to deviations from optimum transmission. Work performed at the Oak Ridge National Laboratory. "Date Declassified: July 13, 1948. "Includes bibliographical references. In certain <b>exponential</b> absorption <b>experiments,</b> notably measurements of cross sections by transmission, {{it is important to}} achieve minimum statistical error in a limited time, or to minimize the counting time required to measure the absorption coefficient with a preassigned accuracy. The conditions required to attain these ends, i. e., the geometry for optimum transmission, and the best apportionment of counting times among the incident and transmitted beams and background, have been investigated for a wide range of relative backgrounds (0. 001 to 0. 01), and for two geometries: (1) Beam area fixed, absorber thickness alone is varied, and (2) Beam area and absorber thickness are both disposable parameters, while the total amount of absorber intercepting the beam remains fixed. In both cases the incident flux density and the background rate are assumed constant. The optimum transmissions are shown to be, in general, considerably smaller than those commonly used in absorption experiments. Thus, in Case 1, a useful rule is to employ a transmission of about 0. 1 for low backgrounds, 0. 2 for moderate backgrounds, and 0. 3 for high backgrounds. The following have also been determined: (a) minimum statistical error for a given total counting time, (b) statistical error and the best distribution of counting times for nonoptimum geometry, and (c) sensitivity of the accuracy or total counting time to deviations from optimum transmission. Mode of access: Internet...|$|R
