43|3|Public
25|$|H. Rogers, 1967. Theory of {{recursive}} {{functions and}} <b>effective</b> <b>computability.</b> McGraw-Hill.|$|E
2500|$|Rogers, H. [...] The Theory of Recursive Functions and <b>Effective</b> <b>Computability,</b> MIT Press. , ...|$|E
2500|$|H. Rogers, Jr., 1967. The Theory of Recursive Functions and <b>Effective</b> <b>Computability,</b> {{second edition}} 1987, MIT Press. [...] (paperback), ...|$|E
40|$|We define instantiational and {{algorithmic}} completeness for {{a formal}} language. We show that, {{in the presence of}} Church's Thesis, an alternative interpretation of Goedelian incompleteness is that Peano Arithmetic is instantiationally complete, but algorithmically incomplete. We then postulate a Provability Thesis that links Peano Arithmetic and <b>effective</b> algorithmic <b>computability,</b> just as Church's Thesis links Recursive Arithmetic and <b>effective</b> instantiational <b>computability.</b> Comment: 18 pages; an HTML version is available at [URL]...|$|R
2500|$|Minsky expands his [...] "...idea of an algorithm—an {{effective}} procedure..." [...] {{in chapter}} 5.1 <b>Computability,</b> <b>Effective</b> Procedures and Algorithms. Infinite machines.|$|R
40|$|A {{function}} f is continuous iff the pre-image f − 1 [V] of any {{open set}} V is open again. Dual to this topological property, f is called open iff the image f[U] of any open set U is open again. Several classical Open Mapping Theorems in Analysis provide {{a variety of}} sufficient conditions for openness. By the Main Theorem of Recursive Analysis, computable real functions are necessarily continuous. In fact they admit a well-known characterization {{in terms of the}} mapping V↦ → f − 1 [V] being effective: Given a list of open rational balls exhausting V, a Turing Machine can generate an according list for f − 1 [V]. Analogously, effective openness requires the mapping U↦ → f[U] on open real subsets to be effective. The present work reveals important and rich classes of functions to be effectively open and thus applicable in the foundations of solid modeling to computations on regular sets. We also address the general relation between <b>effective</b> openness and <b>computability</b> of functions. ...|$|R
2500|$|Hartley Rogers, Jr., Theory of Recursive Functions and <b>Effective</b> <b>Computability</b> McGraw-Hill (1967) [...] (describes {{recursive}} ordinals and the Church–Kleene ordinal) ...|$|E
2500|$|Before {{the precise}} {{definition}} of computable function, mathematicians often used the informal term effectively calculable. [...] This term has since {{come to be}} identified with the computable functions. Note that the <b>effective</b> <b>computability</b> of these functions does not imply that they can be efficiently computed (i.e. computed within a reasonable amount of time). In fact, for some effectively calculable functions it can be shown that any algorithm that computes them will be very inefficient {{in the sense that the}} running time of the algorithm increases exponentially (or even superexponentially) with the length of the input. The fields of feasible computability and computational complexity study functions that can be computed efficiently.|$|E
5000|$|Hartley Rogers Jr., The Theory of Recursive Functions and <b>Effective</b> <b>Computability,</b> MIT Press, [...] (paperback), [...] (textbook) ...|$|E
5000|$|... "It is our {{object to}} {{describe}} a primitive device, {{to be called a}} Q-machine, which arrives at <b>effective</b> <b>computability</b> via arithmetic rather than via logic. Its three operations are keeping tally, comparing non-negative integers, and transferring" [...] (Melzak (1961) p. 281) ...|$|E
50|$|Several {{independent}} {{efforts to}} give a formal characterization of effective calculability led {{to a variety of}} proposed definitions (general recursion, Turing machines, λ-calculus) that later were shown to be equivalent. The notion captured by these definitions is known as recursive or <b>effective</b> <b>computability.</b>|$|E
5000|$|A {{hypothesis}} {{leading to}} a natural law?: In late 1936 Alan Turings paper (also proving that the Entscheidungsproblem is unsolvable) was delivered orally, but had not yet appeared in print. On the other hand, Emil Posts 1936 paper had appeared and was certified independent of Turings work. Post strongly disagreed with Churchs [...] "identification" [...] of <b>effective</b> <b>computability</b> with the λ-calculus and recursion, stating: ...|$|E
50|$|Before {{the precise}} {{definition}} of computable function, mathematicians often used the informal term effectively calculable. This term has since {{come to be}} identified with the computable functions. Note that the <b>effective</b> <b>computability</b> of these functions does not imply that they can be efficiently computed (i.e. computed within a reasonable amount of time). In fact, for some effectively calculable functions it can be shown that any algorithm that computes them will be very inefficient {{in the sense that the}} running time of the algorithm increases exponentially (or even superexponentially) with the length of the input. The fields of feasible computability and computational complexity study functions that can be computed efficiently.|$|E
5000|$|In his 1967 Theory of Recursive Functions and <b>Effective</b> <b>Computability</b> Hartley Rogers' characterizes [...] "algorithm" [...] roughly as [...] "a {{clerical}} (i.e., deterministic, bookkeeping) procedure [...] [...] [...] {{applied to}} [...] [...] [...] symbolic inputs and which will eventually yield, for each such input, a corresponding symbolic output"(p. 1). He {{then goes on}} to describe the notion [...] "in approximate and intuitive terms" [...] as having 10 [...] "features", 5 of which he asserts that [...] "virtually all mathematicians would agree to" [...] (p. 2). The remaining 5 he asserts [...] "are less obvious than *1 to *5 and about which we might find less general agreement" [...] (p. 3).|$|E
50|$|Type free λ-calculus treats {{functions}} as rules {{and does not}} differentiate functions and the objects which they are applied to, meaning λ-calculus is type free. A by-product of type free λ-calculus is an <b>effective</b> <b>computability</b> equivalent to general recursion and Turing machines. The set of λ-terms {{can be considered a}} functional topology in which a function space can be embedded, meaning λ mappings within the space X are such that λ:X → X. Introduced November 1969, Dana Scott's untyped set theoretic model constructed a proper topology for any λ-calculus model whose function space is limited to continuous functions. The result of a Scott continuous λ-calculus topology is a function space built upon a programming semantic allowing fixed point combinatorics, such as the Y combinator, and data types. By 1971, λ-calculus was equipped to define any sequential computation and could be easily adapted to parallel computations. The reducibility of all computations to λ-calculus allows these λ-topological properties to become adopted by all programming languages.|$|E
40|$|There is an {{intensive}} discussion nowadays {{about the meaning}} of <b>effective</b> <b>computability,</b> with implications to the status and provability of the Church–Turing Thesis (CTT). I begin by reviewing what has become the dominant account of the way Turing and Church viewed, in 1936, <b>effective</b> <b>computability.</b> According to this account, to which I refer as the Gandy–Sieg account, Turing and Church aimed to characterize the functions that can be computed by a human computer. In addition, Turing provided a highly convincing argument for CTT by analyzing the processes carried out by a human computer. I then contend that if the Gandy–Sieg account is correct, then the notion of <b>effective</b> <b>computability</b> has changed after 1936. Today computer scientists view <b>effective</b> <b>computability</b> in terms of finite machine computation. My contention is supported by the current formulations of CTT, which always refer to machine computation, and by the current argumentation for CTT, which is different from the main arguments advanced by Turing and Church. I finally turn to discuss Robin Gandy’s characterization of machine computation. I suggest that there is an ambiguity regarding the types of machines Gandy was postulating. I offer three interpretations, which differ in their scope and limitations, and conclude that none provides the basis for claiming that Gandy characterized finite machine computation...|$|E
40|$|Abstract. We {{show the}} {{existence}} and <b>effective</b> <b>computability</b> of op-timal winning strategies for request-response games in case {{the quality of a}} play is measured by the limit superior of the mean accumulated waiting times between requests and their responses. 1991 Mathematics Subject Classification. 68 Q 45. 1...|$|E
40|$|Inspired by Quantum Mechanics, we {{reformulate}} Hilbert’s tenth {{problem in}} the domain of integer arithmetics into either a problem involving a set of infinitely coupled differential equations or a problem involving a Shrödinger propagator with some appropriate kernel. Either way, Mathematics and Physics could be combined for Hilbert’s tenth problem and for the notion of <b>effective</b> <b>computability.</b> ...|$|E
40|$|Olszewski {{claims that}} the Church-Turing thesis {{can be used in}} an {{argument}} against platonism in philosophy of mathematics. The key step of his argument employs an example of a supposedly effectively computable but not Turing-computable function. I argue that the process he describes is not an effective computation, and that the argument relies on the illegitimate conflation of <b>effective</b> <b>computability</b> with there being a way to find out...|$|E
40|$|We {{show the}} {{existence}} and <b>effective</b> <b>computability</b> of optimal winning strategies for request-response games in case {{the quality of a}} play is measured by the limit superior of the mean accumulated waiting times between requests and their responses. Comment: The present paper is a revised version with simplified proofs of results announced in the conference paper of the same name presented at ATVA 2008, which in turn extended results of the third author's dissertatio...|$|E
40|$|Inspired by Quantum Mechanics, we {{reformulate}} the Hilbert’s tenth {{problem in}} the domain of integer arithmetics into a problem involving a set of coupled differential equations. Analytical and numerical studies of the differential equations will either themselves settle and/or be of crucial assistance for some physical implementation of an adiabatic quantum algorithm to determine the existence of solutions for the particular Diophantine equation in question. Either way, Mathematics and Physics could be combined for the Hilbert’s tenth problem and for the notion of <b>effective</b> <b>computability...</b>|$|E
40|$|Computability Theory: An Introduction {{provides}} information {{pertinent to the}} major concepts, constructions, and theorems of the elementary theory of computability of recursive functions. This book provides mathematical evidence for {{the validity of the}} Church-Turing thesis. Organized into six chapters, this book begins with an overview of the concept of effective process so that a clear understanding of the <b>effective</b> <b>computability</b> of partial and total functions is obtained. This text then introduces a formal development of the equivalence of Turing machine computability, enumerability, and decid...|$|E
40|$|We {{propose a}} quantum {{algorithm}} for the classically non-computable Hilbert’s tenth problem, which ultimately {{links to the}} Turing halting problem. Quantum continuous variables and quantum adiabatic evolution are employed for an implementation. Also discussed are a method for the time estimation for the adiabatic evolution, and a comparison with more the well-known quantum computation employing {{a finite number of}} qubits. Provided certain hamiltonian and its ground state can be physically constructed according to the algorithm, the notion of <b>effective</b> <b>computability</b> is extended beyond the Church-Turing thesis of classical computability...|$|E
40|$|These five {{lectures on}} undecidability {{were given to}} {{students}} with a good level in mathematics but with no special knowledge on logic. The first conference presents the formalization of mathematics with a short historical survey, the language of first order predicates and the axioms of set theory. The second and third lectures explain the incompleteness phenomena from the Hilbert program until Gödel's theorems with a presentation of the sequent calculus of Gentzen. The fourth talk deepens model theory reasoning {{in the case of}} the continuum hypothesis, and the last conference gives examples of <b>effective</b> <b>computability</b> results...|$|E
40|$|The {{distinction}} between {{games of chance}} and games of skill is not well-defined at present. We in-troduce the concept of chanciness for non-deterministic two-player zero-sum games with sequential moves and perfect information. chanciness quantifies how strongly a game is influenced by chance events. We demonstrate that the relative influence of chance on game outcomes varies with the skill of the playing agents. Therefore, we assign a chanciness value to {{the combination of a}} game and a specific set of players. The <b>effective</b> <b>computability</b> of chanciness is demonstrated for exemplary games. Key words: games of chance, chance and skill, automatic evaluation, computer aided game invent-ing 1...|$|E
40|$|The Lambda Calculus is {{a formal}} system, {{originally}} {{intended as a}} tool in the foundation of mathematics, but mainly used to study the concepts of algorithm and <b>effective</b> <b>computability.</b> Recently, the Lambda Calculus and related systems acquire attention from Computer Science for another reason too: several important programming language concepts can be explained elegantly and can be studied successfully {{in the framework of}} the Lambda Calculi. We show this mainly by means of examples. We address ourselves to interested computer scientists who have no prior knowledge of the Lambda Calculus. The concepts discussed include: parameterization, definitions, recursion, elementary and composite data types, typing, abstract types, control of visibility and life-time, and modules. 1...|$|E
40|$|The {{problem of}} {{experiment}} design {{is defined as}} an information system consisting of information source, measurement unit, environmental disturbances, data handling and storage, and the mathematical analysis and usage of data. Based on today's concept of <b>effective</b> <b>computability,</b> general guidelines for {{the definition of the}} relevant information content in data classes are derived. The lack of a universally applicable information theory and corresponding mathematical or system structure is restricting the solvable problem classes to a small set. It is expected that a new relativity theory of information, generally described by a universal algebra of relations will lead to new mathematical models and system structures capable of modeling any well defined practical problem isomorphic to an equivalence relation at any corresponding level of abstractness...|$|E
40|$|AbstractThis paper {{presents}} a methodology for testing a logic program containing function symbols and built-in predicates forsafetyandeffective computability. Safety is the property that {{the set of}} answers for a given query is finite. A related issue is whether the evaluation strategy can effectively compute all answers and terminate. We consider these problems {{under the assumption that}} queries are evaluated using a fair bottom-up fixpoint computation. We also model the use of function symbols, to construct complex terms such as lists, and arithmetic operators, by considerating Datalog programs with infinite base relations over whichfiniteness constraintsandmonotonicity constraintsare imposed. One of the main results of this paper is a recursive algorithm,check–clique, to test the safety and <b>effective</b> <b>computability</b> of predicates in arbitrarily complex cliques in the predicate connection graph. This algorithm takes certain procedures as parameters, and its applicability can be strengthened by making these procedures more sophisticated. We specify the properties required of these procedures precisely, and present a formal proof of correctness for the algorithmcheck–clique. This work can be seen as providing a framework for testing safety and <b>effective</b> <b>computability</b> of recursive programs, in some ways analogous to thecapture rulesframework of Ullman. A second important contribution is a framework for analyzing programs that are produced by theMagic Setstransformation utilizingcheck–cliqueto analyze recursive cliques. The transformed program unfortunately often has a clique structure that combines several cliques of the original program. Given the complexity of algorithmcheck–clique, {{it is important to keep}} cliques as small as possible. We deal with this problem by considering cliques in an intermediate program, called theadorned program, produced by the Magic Sets transformation. The clique structure of the adorned program is similar to that of the original program, and by showing how to analyze the transformed program in terms of the cliques in the adorned program, we avoid the potentially expensive analysis of the cliques in the transformed program...|$|E
40|$|Abstract. In {{this text}} {{we will discuss}} {{different}} forms of randomness in Natural Sciences and present some recent results relating them. In fi-nite processes, randomness differs in various theoretical context, or, to put it otherwise, there is no unifying notion of finite time randomness. In particular, we will introduce, classical (dynamical), quantum and al-gorithmic randomness. In physics, differing probabilities, {{as a measure of}} randomness, evidentiate the differences between the various notions. Yet, asymptotically, one is universal: Martin-Löf randomness provides a clearly defined and robust notion of randomness for infinite sequences of numbers. And this is based on recursion theory, that is the theory of <b>effective</b> <b>computability.</b> As a recurring issue, the question will be raised of what randomenss means in biology, phylogenesis in particular. Finally, hints will be given towards a thesis, relating finite time randomness and time irreversibility in physical processes 1. ...|$|E
40|$|In {{as much as}} {{physical}} theories are formalizable, set theory provides a framework for theoretical physics. Four speculations about the relevance of set theoretical modeling for physics are presented: the role of transcendental set theory (i) in chaos theory, (ii) for paradoxical decompositions of solid threedimensional objects, (iii) {{in the theory of}} <b>effective</b> <b>computability</b> (Church-Turing thesis) related to the possible "solution of supertasks", and (iv) for weak solutions. Several approaches to set theory and their advantages and disadvantages for physical applications are discussed: Cantorian "naive" (i. e., non-axiomatic) set theory, constructivism and operationalism. In the author's opinion, an attitude of "suspended attention" (a term borrowed from psychoanalysis) seems most promising for progress. Physical and set theoretical entities must be operationalized wherever possible. At the same time, physicists should be open to "bizarre" or "mindboggling" new formalisms, which need not [...] ...|$|E
40|$|The primary {{notion of}} <b>effective</b> <b>computability</b> is that {{provided}} by Turing machines (or equivalently {{any of the}} other common models of computation). We denote the partial function computed by the eth Turing machine in some standard list by # e. When these machines are equipped with an "oracle" for a subset A of the natural numbers #, i. e. an external procedure that answers questions of the form "is n in A", they define the basic notion of relative computability or Turing reducibility (from Turing (1939)). We say that A is computable from (or recursive in) B if there is a Turing machine which, when equipped with an oracle for B, computes (the characteristic function of) A, i. e. for some e, # B e = A. We denote this relation by A # T<F 1...|$|E
40|$|Some of {{the more}} {{differential}} aspects of the nascent field of computational topology are introduced and treated in considerable depth. Relevant categories based upon stratified geometric objects are proposed, and fundamental problems are identified and discussed {{in the context of}} both differential topology and computer science. New results on the triangulation of objects in the computational differential categories are proven, and evaluated from the perspective of <b>effective</b> <b>computability</b> (algorithmic solvability). In addition, the elements of innovative, effectively computable approaches for analyzing and obtaining computer generated representations of geometric objects based upon singularity/stratification theory and obstruction theory are formulated. New methods for characterizing complicated intersection sets are proven using differential analysis and homology theory. Also included are brief descriptions of several implementation aspects of some of the approaches described, as well as applications of the results in such areas as virtual sculpting, virtual surgery, modeling of heterogeneous biomaterials, and high speed visualizations...|$|E
40|$|AbstractUpward-closed sets of integer vectors {{enjoy the}} merit of having {{a finite number of}} minimal {{elements}}, which is behind the decidability of a number of Petri net related problems. In general, however, such a finite set of minimal elements may not be effectively computable. In this paper, we develop a unified strategy for computing the sizes of the minimal elements of certain upward-closed sets associated with Petri nets. Our approach can be regarded as a refinement of a previous work by Valk and Jantzen (in which a necessary and sufficient condition for <b>effective</b> <b>computability</b> of the set was given), in the sense that complexity bounds now become available provided that a bound can be placed on the size of a witness for a key query. The sizes of several upward-closed sets that arise in the theory of Petri nets as well as in backward-reachability analysis in automated verification are derived in this paper, improving upon previous decidability results shown in the literature...|$|E
40|$|Computation {{based on}} the {{principles}} of Quantum Mechanics [1] has been shown to offer better performances over classical computation, ranging from the square-root improvement in an unstructured search [2] to the exponential gain in the factorisation of integers [3]. However superior in reducing the complexity of hard computation, these quantum algorithms and all the others discovered so far are only applicable to the classically computable functions. That leaves untouched the class of classically noncomputable functions, such as the halting problem for Turing machines [4]. It is in fact widely believed that quantum computation cannot offer anything new about computability [5]. Contrary to this, I show that quantum computation can indeed compute the noncomputables, provided certain hamiltonian and its ground state can be physically constructed. I present a quantum algorithm for the classically noncomputable Hilbert’s tenth problem [6]. This algorithm can ultimately solve the halting problem for Turing machines in the computation of partial recursive functions. With this result, the notion of <b>effective</b> <b>computability</b> is extended beyond th...|$|E
40|$|Inasmuch as {{physical}} theories are formalizable, set theory {{provides a framework}} for theoretical physics. Four speculations about the relevance of set theoretical modeling for physics are presented: the role of transcendental set theory (i) hr chaos theory, (ii) for paradoxical decompositions of solid three-dimensional objects, (iii) {{in the theory of}} <b>effective</b> <b>computability</b> (Church-Turhrg thesis) related to the possible "solution of supertasks," and (iv) for weak solutions. Several approaches to set theory and their advantages and disadvatages for" physical applications are discussed: Cantorian "naive" (i. e., nonaxiomatic) set theory, contructivism, and operationalism, hr the arrthor's ophrion, an attitude of "suspended attention" (a term borrowed from psychoanalysis) seems most promising for progress. Physical and set theoretical entities must be operationalized wherever possible. At the same thne, physicists shouM be open to "bizarre" or "mindboggling" new formalisms, which treed not be operationalizable or testable at the thne of their " creation, but which may successfully lead to novel fields of phenomenology and technology...|$|E
40|$|We study {{a notion}} of translatability between classes of schemas when {{restrictions}} are placed on interpretations. Unlike the usual notion of translatability, our translatability lets the translation depend on the interpretation. We consider five classes of schemas. Three of these have been extensively studied in the literature: flow-chart, flow-chart with counters, and recursive. The two others are defined herein; {{the first of which}} is a class of “maximal power” and is equivalent to similarly motivated classes of other investigators; while the second is, in some sense, a nontrivial class of “minimal power”. Our main results specify restrictions on interpretations that will allow the translatability of a class into a class, not being translatable into over all (or unrestricted) interpretations. Additional results specify restrictions on interpretations under which is not translatable into; the proofs of these clarify the mechanisms in the main results. Last, we consider the notion of <b>effective</b> <b>computability</b> in algebraic structures {{in the light of the}} main results...|$|E
