906|10000|Public
5|$|Dworkin and MacKinnon, however, {{continued}} to discuss civil rights litigation {{as a possible}} approach to combating pornography. In the fall of 1983, MacKinnon secured a one-semester appointment for Dworkin at the University of Minnesota, to teach a course in literature for the Women's Studies program and co-teach (with MacKinnon) an interdepartmental course on pornography, where they <b>hashed</b> out details of a civil rights approach. With encouragement from community activists in south Minneapolis, the Minneapolis city government hired Dworkin and MacKinnon to draft an antipornography civil rights ordinance as {{an amendment to the}} Minneapolis city civil rights ordinance.|$|E
5|$|The delegates {{voted to}} address the {{platform}} before deciding on nominees, and debate began when they reconvened at 8:45p.m. Many fights and compromises had been <b>hashed</b> out in the Resolutions Committee already, but the delegates insisted on debating several provisions. On many planks, there was widespread agreement among the delegates. On the monetary issue, the platform declared that all money, whether metal or paper, should be issued by the government, not by banks (as was common for paper money at the time). They also called for the unlimited coinage of silver and the repayment of the national debt in bonds, rather than gold dollars. Other planks of the platform called for a graduated income tax, laws to mandate safe working conditions in factories, the regulation of interstate commerce, {{and an end to}} child and convict labor; all of these were familiar parts of Greenback platforms from earlier elections, and provoked no serious dissent.|$|E
25|$|Operation Magic Carpet (Yemen) {{began in}} June 1949 {{and ended in}} September 1950. Part of the {{operation}} happened during the 1947–48 Civil War in Mandatory Palestine and the 1948 Arab–Israeli War (May 15, 1948 – March 10, 1949). The operation was planned by the American Jewish Joint Distribution Committee. The plan was for the Jews from all over Yemen {{to make their way}} to the Aden area. Specifically, the Jews were to arrive in <b>Hashed</b> Camp and live there until they could be airlifted to Israel. <b>Hashed</b> was an old British military camp in the desert, about a mile away from the city of Sheikh Othman. The operation took longer than was originally planned. Over the course of the operation, hundreds of migrants died in <b>Hashed</b> Camp, as well as on the plane rides to Israel. By September 1950, almost 50,000 Jews had been successfully airlifted to the newly formed state of Israel.|$|E
5000|$|... uint32_t jenkins_one_at_a_time_hash(const uint8_t* key, size_t length) { size_t i = 0; uint32_t <b>hash</b> = 0; while (i != length) { <b>hash</b> += keyi++; <b>hash</b> += <b>hash</b> << 10; <b>hash</b> ^= <b>hash</b> >> 6; } <b>hash</b> += <b>hash</b> << 3; <b>hash</b> ^= <b>hash</b> >> 11; <b>hash</b> += <b>hash</b> << 15; return hash;} ...|$|R
5000|$|A <b>hash</b> tree is a tree of <b>hashes</b> {{in which}} the leaves are <b>hashes</b> of data blocks in, for instance, a file or set of files. Nodes further up in the tree are the <b>hashes</b> of their {{respective}} children. For example, in the picture <b>hash</b> 0 {{is the result of}} <b>hashing</b> the concatenation of <b>hash</b> 0-0 and <b>hash</b> 0-1. That is, <b>hash</b> 0 = <b>hash(</b> [...] <b>hash</b> 0-0 + <b>hash</b> 0-1 [...] ) where + denotes concatenation.|$|R
5000|$|... #Caption: An {{example of}} a binary <b>hash</b> tree. <b>Hashes</b> 0-0 and 0-1 are the <b>hash</b> values of data blocks L1 and L2, respectively, and <b>hash</b> 0 is the <b>hash</b> of the {{concatenation}} of <b>hashes</b> 0-0 and 0-1.|$|R
25|$|On July 2, 2015, Plex {{revealed}} the machine hosting their blog and forums had been compromised. Personal information like IP addresses, forum private messages, email addresses and <b>hashed</b> and salted password have been accessed. This access was gained via a 0-day vulnerability in their forums software.|$|E
25|$|The {{steering}} committee behind the {{proposal for a}} draft bill for the commission organised a national conference that saw about 200+ people from all religious backgrounds attending it. There they <b>hashed</b> out the framework for a commission that could advise the relevant parties on the many interfaith issues that arise in pluralistic Malaysia such as conversion from Islam to another faith, which is deemed as apostasy in Malaysia.|$|E
25|$|Variants D and E {{create an}} ad-hoc {{peer-to-peer}} network to push and pull payloads over the wider Internet. This {{aspect of the}} virus is heavily obfuscated in code and not fully understood, but has been observed to use large-scale UDP scanning {{to build up a}} peer list of infected hosts and TCP for subsequent transfers of signed payloads. To make analysis more difficult, port numbers for connections are <b>hashed</b> from the IP address of each peer.|$|E
5000|$|For example, {{the class}} Employee might {{implement}} its <b>hash</b> function by composing the <b>hashes</b> of its members:public class Employee { int employeeId; String name; Department dept; // other methods {{would be in}} here @Override public int hashCode (...) { int <b>hash</b> = 1; <b>hash</b> = <b>hash</b> * 17 + employeeId; <b>hash</b> = <b>hash</b> * 31 + name.hashCode (...) <b>hash</b> = <b>hash</b> * 13 + (dept == null ? 0 : dept.hashCode (...) [...] ); return hash; }} ...|$|R
50|$|Binary <b>hash</b> {{chains are}} {{commonly}} used in association with a <b>hash</b> tree. A Binary <b>hash</b> chain takes two <b>hash</b> values as inputs, concatenates them and applies a <b>hash</b> function to the result, thereby producing a third <b>hash</b> value.|$|R
50|$|Multiplicative <b>hashing</b> is {{a simple}} type of <b>hash</b> {{function}} often used by teachers introducing students to <b>hash</b> tables. Multiplicative <b>hash</b> functions are simple and fast, but have higher collision rates in <b>hash</b> tables than more sophisticated <b>hash</b> functions.|$|R
25|$|The {{best way}} {{to speed up the}} baby-step giant-step {{algorithm}} is to use an efficient table lookup scheme. The best in this case is a hash table. The hashing is done on the second component, and to perform the check in step 1 of the main loop, γ is <b>hashed</b> and the resulting memory address checked. Since hash tables can retrieve and add elements in O(1) time (constant time), this does not slow down the overall baby-step giant-step algorithm.|$|E
25|$|Filename {{searches}} {{are implemented}} using keywords. The filename {{is divided into}} its constituent words. Each of these keywords is <b>hashed</b> and stored in the network, together with the corresponding filename and file hash. A search involves choosing one of the keywords, contacting the node with an ID closest to that keyword hash, and retrieving the list of filenames that contain the keyword. Since every filename in the list has its hash attached, the chosen file can then be obtained in the normal way.|$|E
25|$|The stolen passwords, {{which were}} <b>hashed</b> (i.e. just a {{checksum}} was stored, allowing testing whether a given password {{is the correct}} one), were cracked and posted on a Russian password forum later on that day. By the morning of June 6, passwords for thousands of accounts were available online in plain text. Graham Cluley of the internet security firm Sophos warned that the leaked passwords {{could be in the}} possession of criminals by 6 June. LinkedIn said, in an official statement, that they would email all its members with security instructions and instructions on how they could reset their passwords.|$|E
5000|$|Murmur3_32(key, len, seed) // Note: In this version, all integer {{arithmetic}} {{is performed}} with unsigned 32 bit integers. // In {{the case of}} overflow, the result is constrained by the application of modulo [...] arithmetic. [...] c1 &larr; 0xcc9e2d51 c2 &larr; 0x1b873593 r1 &larr; 15 r2 &larr; 13 m &larr; 5 n &larr; 0xe6546b64 [...] <b>hash</b> &larr; seed [...] for each fourByteChunk of key k &larr; fourByteChunk [...] k &larr; k &times; c1 k &larr; (k ROL r1) k &larr; k &times; c2 [...] <b>hash</b> &larr; <b>hash</b> XOR k <b>hash</b> &larr; (<b>hash</b> ROL r2) <b>hash</b> &larr; <b>hash</b> &times; m + n [...] with any remainingBytesInKey remainingBytes &larr; SwapToLittleEndian(remainingBytesInKey) // Note: Endian swapping is only necessary on big-endian machines. // The purpose is to place the meaningful digits towards {{the low end of}} the value, // so that these digits have the greatest potential to affect the low range digits // in the subsequent multiplication. Consider that locating the meaningful digits // in the high range would produce a greater effect upon the high digits of the // multiplication, and notably, that such high digits are likely to be discarded // by the modulo arithmetic under overflow. We don't want that. [...] remainingBytes &larr; remainingBytes &times; c1 remainingBytes &larr; (remainingBytes ROL r1) remainingBytes &larr; remainingBytes &times; c2 [...] <b>hash</b> &larr; <b>hash</b> XOR remainingBytes [...] <b>hash</b> &larr; <b>hash</b> XOR len [...] <b>hash</b> &larr; <b>hash</b> XOR (<b>hash</b> >> 16) <b>hash</b> &larr; <b>hash</b> &times; 0x85ebca6b <b>hash</b> &larr; <b>hash</b> XOR (<b>hash</b> >> 13) <b>hash</b> &larr; <b>hash</b> &times; 0xc2b2ae35 <b>hash</b> &larr; <b>hash</b> XOR (<b>hash</b> >> 16) ...|$|R
50|$|Often, an {{additional}} <b>hash</b> of the <b>hash</b> list itself (a top <b>hash,</b> also called root <b>hash</b> or master <b>hash)</b> is used. Before downloading a file on a p2p network, {{in most cases}} the top <b>hash</b> is acquired from a trusted source, for instance a friend or a web site that is known to have good recommendations of files to download. When the top <b>hash</b> is available, the <b>hash</b> list can be received from any non-trusted source, like any peer in the p2p network. Then the received <b>hash</b> list is checked against the trusted top <b>hash,</b> and if the <b>hash</b> list is damaged or fake, another <b>hash</b> list from another source will be tried until the program finds one that matches the top <b>hash.</b>|$|R
40|$|Abstract — Sensors {{and other}} small devices that {{periodically}} transmit relatively small packets of information motivate the study of <b>hash</b> chains with small domains and ranges. <b>Hash</b> chain based protocols work using deferred disclosure and it is often assumed their <b>hash</b> functions are one-way, hence essentially unbreakable. However small domains and ranges make <b>hash</b> functions much weaker. If a deterministic <b>hash</b> function’s domain and range are the same and both are very small, then {{it may not be}} possible for the <b>hash</b> function to be one-way. In fact, <b>hash</b> chains with size-constrained domains and ranges are likely to cycle quickly. This paper proposes a solution to this challenge- the general <b>hash</b> chain construction. A general <b>hash</b> chain uses several subsequent <b>hash</b> elements at once as input to produce each output <b>hash</b> element. General <b>hash</b> chains have the following properties: (1) repeated <b>hash</b> elements do not necessarily indicate cycles in the <b>hash</b> chain, (2) subsequent elements of these <b>hash</b> chains do not have exponentially diminishing ranges. This makes general <b>hash</b> chains alluring for devices with size constraints on their domains and ranges. I...|$|R
25|$|Digital Guardian allows {{businesses}} to host and manage on premise or choose managed security programs. When deployed as a managed service, Digital Guardian does not collect the actual data itself. Rather, it aggregates the metadata about the files and documents and watches for patterns of activity. Metadata is encrypted, <b>hashed</b> and digitally signed before being transferred to Digital Guardian's hosting facilities via FIPS 140-2 certified messaging protocol. Digital Guardian then provides updated analytics, alerts and reports. Administrators can continuously monitor data, application and system access and usage, whether end users are online, offline or in virtual environments. Organizations can apply specific risk-based policy controls {{to adhere to}} data governance and compliance rules.|$|E
25|$|Typically, {{humans are}} asked to choose a password, {{sometimes}} guided by suggestions or restricted {{by a set of}} rules, when creating a new account for a computer system or Internet Web site. Only rough estimates of strength are possible, since humans tend to follow patterns in such tasks, and those patterns can usually assist an attacker. In addition, lists of commonly chosen passwords are widely available for use by password guessing programs. Such lists include the numerous online dictionaries for various human languages, breached databases of plaintext and <b>hashed</b> passwords from various online business and social accounts, along with other common passwords. All items in such lists are considered weak, as are passwords that are simple modifications of them. For some decades, investigations of passwords on multi-user computer systems have shown that 40% or more are readily guessed using only computer programs, and more can be found when information about a particular user is taken into account during the attack.|$|E
500|$|But by the multiplicative {{form of the}} Chernoff bound, {{when the}} load factor is bounded away from one, the {{probability}} that a block of length [...] contains at least [...] <b>hashed</b> values is exponentially small as a function of , ...|$|E
50|$|People {{who write}} {{complete}} <b>hash</b> table implementations choose a specific <b>hash</b> function—such as a Jenkins <b>hash</b> or Zobrist hashing—and independently choose a hash-table collision resolution scheme—such ascoalesced hashing,cuckoo <b>hashing,</b> orhopscotch <b>hashing.</b>|$|R
40|$|In this paper, {{we present}} a new {{construction}} for strong separating <b>hash</b> families by using hypergraphs and obtain some optimal separating <b>hash</b> families. We also improve some previously known bounds of separating <b>hash</b> families. Comment: <b>Hash</b> family, separating <b>hash</b> family, strong separating <b>hash</b> family, hypergrap...|$|R
50|$|In {{the top of}} a <b>hash</b> tree {{there is}} a top <b>hash</b> (or root <b>hash</b> or master <b>hash).</b> Before {{downloading}} a file on a p2p network, in most cases the top <b>hash</b> is acquired from a trusted source, for instance a friend or a web site that is known to have good recommendations of files to download. When the top <b>hash</b> is available, the <b>hash</b> tree can be received from any non-trusted source, like any peer in the p2p network. Then, the received <b>hash</b> tree is checked against the trusted top <b>hash,</b> and if the <b>hash</b> tree is damaged or fake, another <b>hash</b> tree from another source will be tried until the program finds one that matches the top <b>hash.</b>|$|R
500|$|... causing this sum to be {{bounded by}} a {{constant}} independent of. It {{is also possible}} to perform the same analysis using Stirling's approximation instead of the Chernoff bound to estimate {{the probability that a}} block contains exactly [...] <b>hashed</b> values.|$|E
500|$|The {{film was}} criticized for having an unoriginal story and unlikable characters. Brandon Ciampaglia of IGN {{described}} it as [...] "yet another stupid horror film" [...] {{and gave it a}} score of 4/10. Ryan Turek of ComingSoon.net criticized the story, characters, and lack of suspense, but found the film more entertaining than the original, with better acting than it deserved. He added that despite the movie's low budget, [...] " [...] has <b>hashed</b> together a fine-looking film that’s technically competent." [...] Tristan Sinns of Dread Central awarded it two out of five stars, criticizing it for not featuring the mythological Boogeyman and having unsympathetic characters but praising the death scenes as [...] "rather creative." ...|$|E
500|$|A digital {{forensic}} investigation commonly {{consists of}} 3 stages: acquisition or imaging of exhibits, analysis, and reporting. Ideally acquisition involves capturing {{an image of}} the computer's volatile memory (RAM) and creating an exact sector level duplicate (or [...] "forensic duplicate") of the media, often using a write blocking device to prevent modification of the original. However, the growth in size of storage media and developments such as cloud computing [...] have led to more use of 'live' acquisitions whereby a 'logical' copy of the data is acquired rather than a complete image of the physical storage device. Both acquired image (or logical copy) and original media/data are <b>hashed</b> (using an algorithm such as SHA-1 or MD5) and the values compared to verify the copy is accurate.|$|E
3000|$|Furthermore, we plan {{to explore}} the {{variations}} of the current FJLT <b>hashing.</b> Similar to the NMF-based <b>hashing</b> approach (referred as NMF-NMF-SQ <b>hashing</b> in [16]) where the <b>hash</b> {{is based on a}} two-stage application of NMF, we can modify the proposed FJLT <b>hashing</b> into a two-stage FJLT-based <b>hashing</b> approach by introducing a second stage of FJLT as follows. Treat the intermediate <b>hash</b> [...]...|$|R
40|$|ISBN 9780819475046 International audiencePerceptual <b>hashing</b> has to {{deal with}} the {{constraints}} of robustness, accuracy and security. After modeling the process of <b>hash</b> extraction and the properties involved in this process, two diﬀerent security threats are studied, namely the disclosure of the secret feature space and the tampering of the <b>hash.</b> Two diﬀerent approaches for performing robust <b>hashing</b> are presented: Random-Based <b>Hash</b> (RBH) where the security is achieved using a random pro jection matrix and Content-Based <b>Hash</b> (CBH) were the security relies on the diﬃculty to tamper the <b>hash.</b> As for digital watermarking, diﬀerent security setups are also devised: the Batch <b>Hash</b> Attack, the Group <b>Hash</b> Attack, the Unique <b>Hash</b> Attack and the Sensitivity Attack. A theoretical analysis of the information leakage in the context of Random-Based <b>Hash</b> is proposed. Finally, practical attacks are presented: (1) Minor Component Analysis is used to estimate the secret projection of Random-Based <b>Hashes</b> and (2) Salient point tampering is used to tamper the <b>hash</b> of Content-Based <b>Hashes</b> systems...|$|R
50|$|Double <b>hashing</b> is a {{computer}} programming technique used in <b>hash</b> tables to resolve <b>hash</b> collisions, in cases when two different values to be searched for produce the same <b>hash</b> key. It is a popular collision-resolution technique in open-addressed <b>hash</b> tables. Double <b>hashing</b> is implemented in many popular libraries.|$|R
500|$|... {{provide a}} {{randomized}} algorithm called signature sort {{that allows for}} linear time sorting of sets of up to [...] items at a time, for any constant [...] As in the algorithm of Kirkpatrick and Reisch, they perform range reduction using {{a representation of the}} keys as numbers in base for a careful choice of [...] Their range reduction algorithm replaces each digit by a signature, which is a <b>hashed</b> value with [...] bits such that different digit values have different signatures. If [...] is sufficiently small, the numbers formed by this replacement process will be significantly smaller than the original keys, allowing the non-conservative packed sorting algorithm of [...] to sort the replaced numbers in linear time. From the sorted list of replaced numbers, it is possible to form a compressed trie of the keys in linear time, and the children of each node in the trie may be sorted recursively using only keys of size , after which a tree traversal produces the sorted order of the items.|$|E
500|$|For most {{applications}} of hashing, {{it is necessary}} to compute the hash function for each value every time that it is <b>hashed,</b> rather than once when its object is created. In such applications, random or pseudorandom numbers cannot be used as hash values, because then different objects with the same value would have different hashes. And cryptographic hash functions (which are designed to be computationally indistinguishable from truly random functions) are usually too slow to be used in hash tables. Instead, other methods for constructing hash functions have been devised. These methods compute the hash function quickly, and can be proven to work well with linear probing. In particular, linear probing has been analyzed from the framework of -independent hashing, a class of hash functions that are initialized from a small random seed and that are equally likely to map any -tuple of distinct keys to any -tuple of indexes. The parameter [...] {{can be thought of as}} a measure of hash function quality: the larger [...] is, the more time it will take to compute the hash function but it will behave more similarly to completely random functions.|$|E
500|$|Several reviewers re-watched {{the episode}} {{after the end}} of the series. Keith DeCandido {{reviewed}} the episode for Tor.com in July 2011. He highlighted the appearance of Vincent Schiavelli, saying that he [...] "totally owns every scene he’s in". He thought that the situation which left La Forge in charge of the Enterprise was [...] "horribly contrived" [...] and said [...] "Picard doesn’t even give a good excuse for going down to the planet beyond the script calling for it". He gave the episode a score of six out of ten, summing up that it was a [...] "fun, enjoyable, diverting episode". Zack Handlen reviewed the episode in May 2010 for The A.V. Club. He criticised the episode, saying that there [...] "are all kinds of problems, the biggest being that the episode doesn't really have a third act, but the moral superiority of the crew is on full display, and it's frustrating." [...] He summed up the moral story played out in this episode, saying that [...] "On TOS, Kirk, Spock, and McCoy would've <b>hashed</b> out the appeal of an unbeatable weapon as well as its drawbacks. Here, we're all supposed to know that violence begets violence, and that's it." [...] He gave the episode a grade of C+.|$|E
50|$|In {{order to}} meet the {{requirements}} of speed generally demanded by a computer game, files are indexed in a <b>hash</b> table using a quick, low-collision <b>hashing</b> algorithm. The index of a specific file within the <b>hash</b> table is the <b>hash</b> of the lowercased filename modulo the size of the <b>hash</b> table, allowing for quick verification of a file's existence within the archive. If multiple files within the archive have the same <b>hash,</b> colliding entries will follow each other in increasing index order (forming a colliding <b>hash</b> cluster). In order to identify the exact entry for the requested file within a colliding <b>hash</b> cluster, each <b>hash</b> table entry stores 2 additional <b>hashes</b> of the lowercased filename, each using the same <b>hashing</b> algorithm but with a different seed value, as well as a locale code and platform code. The end of a colliding <b>hash</b> cluster is detected either by encountering an empty <b>hash</b> table entry or by traversing the entire <b>hash</b> table (including the modulo loopback) back to the initial <b>hash</b> table index.|$|R
30|$|<b>Hashing</b> time: calculating <b>hash</b> {{value is}} the main step of the query. The <b>hashing</b> time means the time {{consumed}} by applying <b>hash</b> functions.|$|R
40|$|An {{undergraduate}} thesis {{pertaining to}} the viability of fractal images as visual <b>hashes</b> in cryptographic processes. Fractal visual <b>hashes</b> {{have the potential to}} replace traditional hexadecimal <b>hashes</b> for SSH applications with the goal of increasing user recognition of identities of remote computers. Rather than rely on human users manually comparing two hexadecimal <b>hashes</b> as part of SSH's public-private key encryption process, users would compare fractal images generated from the original hexadecimal <b>hashes</b> to determine if the keys match. A visual <b>hash</b> comparison and differentiation game was developed that evaluated the ability of users to differentiate between two images visually. Users were presented with pairs of either flag <b>hashes,</b> t-flag <b>hashes,</b> identicon <b>hashes,</b> hexadecimal <b>hashes,</b> or fractal <b>hashes</b> and were asked to decide if the images were the same or different. The <b>hashes</b> generated were created in a way so that <b>hashes</b> being compared were very similar in their visual characteristics {{to increase the number of}} images that were hard to distinguish. The data collected was analyzed through Bayesian analysis wherein the user input and the <b>hash</b> details were evaluated to the find the highest probability effective entropy and user error of the system. The fractal visual <b>hash</b> achieved the highest amount of effective entropy among the tested <b>hash</b> types and provided a foundation for further research to be conducted on visual <b>hash</b> systems and the validity of fractal <b>hashes...</b>|$|R
