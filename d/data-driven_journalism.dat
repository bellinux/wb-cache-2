33|1|Public
5000|$|... #Subtitle level 3: <b>Data-driven</b> <b>journalism</b> and {{the value}} of trust ...|$|E
5000|$|There is {{a growing}} list of {{examples}} how <b>data-driven</b> <b>journalism</b> can be applied: ...|$|E
5000|$|Computer {{assisted}} {{reporting and}} <b>data-driven</b> <b>journalism,</b> where journalists {{make use of}} large databases to produce stories.|$|E
40|$|Taking {{advantage}} of {{the growing number of}} data sets released by governments, American news organizations now often publish news applications, such as online databases or interactive maps. As a consequence, many residents of American cities can get constantly informed about each crime, environmental pollution or new commercial service that occur in their city or neighborhood. Although some researchers now study how such artifacts and <b>data-driven</b> techniques affect <b>journalism,</b> very little is known about how citizens actually make sense of such information. Yet, what are the various types of collectives emerging from the access to this structured and localized information? How a group of individuals actually emerge as a "public" attributing collective meanings to seemingly scattered single occurrences? We investigate an online initiative carried out by the Los Angeles Times based on the reporting and mapping of every homicide committed in Los Angeles county in the last four years. Each crime is systemically presented on a webpage which is then open to discussion. This operation, which has attracted around 30, 000 comments since its beginning, seemed to us as a good opportunity to investigate how collective judgments are emerging around criminal occurrences. To perform a systematic and exhaustive analysis of this large textual corpus, we developed and made use of dedicated lexicometric and machine learning tools. Stressing the importance of ecology in the process of providing collective meanings to single occurrences, our claim is that emerging publics are strongly shaped by the characteristics of each neighborhood...|$|R
50|$|<b>Data-driven</b> <b>journalism</b> has an even wider approach. At {{the core}} the process {{builds on the}} growing {{availability}} of open data that is freely available online and analyzed with open source tools. <b>Data-driven</b> <b>journalism</b> strives to reach new levels of service for the public, helping consumers, managers, politicians to understand patterns and {{make decisions based on}} the findings. As such, data driven journalism might help to put journalists into a role relevant for society in a new way.|$|E
5000|$|<b>Data-driven</b> <b>journalism</b> (extending {{the focus}} of data {{investigation}} to a workflow from data analysis to data visualization to storytelling based on the findings) ...|$|E
50|$|In 2001 Paulos {{taught a}} course on {{quantitative}} literacy for journalists at the Columbia University School of Journalism. The course stimulated further programs at Columbia and elsewhere in precision and <b>data-driven</b> <b>journalism.</b>|$|E
50|$|In many {{investigations}} {{the data}} {{that can be found}} might have omissions or is misleading. As one layer of <b>data-driven</b> <b>journalism</b> a critical examination of the data quality is important. In other cases the data might not be public or is not in the right format for further analysis, e.g. is only available in a PDF. Here the process of <b>data-driven</b> <b>journalism</b> can turn into stories about data quality or refusals to provide the data by institutions. As the practice as a whole is in early development steps, examinations of data sources, data sets, data quality and data format are therefore an equally important part of this work.|$|E
5000|$|As {{projects}} like the MP Expense Scandal (2009) and the 2013 {{release of the}} [...] "Offshore leaks" [...] demonstrate, <b>data-driven</b> <b>journalism</b> can assume an investigative role, dealing with [...] "not-so open" [...] aka secret data on occasion.|$|E
50|$|Onigbinde {{has always}} been a big believer in <b>data-driven</b> <b>journalism</b> and participated a health journalism project as part of the Knight Innovation Fellowship of the International Center for Journalists. In 2014, BudgIT {{launched}} Tracka, a project tracking tool.|$|E
50|$|Harry Joe Enten is {{a senior}} {{political}} writer and analyst and for the website FiveThirtyEight. Enten was described by the Columbia Journalism Review as being {{of a new generation}} of political journalists, focusing on <b>data-driven</b> <b>journalism</b> instead of reporting from the campaign trail.|$|E
50|$|Data journalism {{trainer and}} writer Paul Bradshaw {{describes}} {{the process of}} <b>data-driven</b> <b>journalism</b> in a similar manner: data must be found, which may require specialized skills like MySQL or Python, then interrogated, for which understanding of jargon and statistics is necessary, and finally visualized and mashed {{with the aid of}} open source tools.|$|E
50|$|<b>Data-driven</b> <b>journalism</b> is {{a process}} whereby journalists build stories using {{numerical}} data or databases as a primary material. In contrast, database journalism is an organizational structure for content. It focuses on the constitution and maintenance of the database upon which web or mobile applications can be built, and from which journalists can extract data to carry out data-driven stories.|$|E
50|$|According to {{information}} architect and multimedia journalist Mirko Lorenz, <b>data-driven</b> <b>journalism</b> {{is primarily a}} workflow that consists of the following elements: digging deep into data by scraping, cleansing and structuring it, filtering by mining for specific information, visualizing and making a story. This process can be extended to provide information results that cater to individual interests and the broader public.|$|E
5000|$|Miller is {{a leading}} figure in {{innovation}} in journalism, especially in transparency, trust and <b>data-driven</b> <b>journalism.</b> [...] He delivered the U.S. Army Creekmore Lecture in 2007, and has taught at the University of Southern California, Columbia University, Stanford University, [...] the University of California at Berkeley and the College of Charleston. He spent a year at Stanford University as a Knight Fellow, studying transparency and new models of journalism.|$|E
50|$|In 1989, Dedman {{received}} the Pulitzer Prize for Investigative Reporting for The Color of Money, his {{series of articles}} in 1988 in Bill Kovach's Atlanta Journal-Constitution on racial discrimination by banks and other mortgage lenders in middle-income black neighborhoods. In addition to raising awareness of redlining of minority areas, and leading Congress to expand disclosure of data allowing analysis of racial patterns in mortgage data, The Color of Money was an influential early example of computer-assisted reporting, data journalism, and <b>data-driven</b> <b>journalism.</b>|$|E
50|$|In {{the context}} of <b>data-driven</b> <b>journalism,</b> the extent of such tracking, such as {{collecting}} user data or any other information {{that could be used}} for marketing reasons or other uses beyond the control of the user, should be viewed as problematic. One newer, non-intrusive option to measure usage is a lightweight tracker called PixelPing. The tracker is the result of a project by ProPublica and DocumentCloud. There is a corresponding service to collect the data. The software is open source and can be downloaded via GitHub.|$|E
5000|$|<b>Data-driven</b> <b>journalism,</b> often {{shortened}} to [...] "ddj", {{is a term}} in use since 2009, {{to describe}} a journalistic process based on analyzing and filtering large data sets {{for the purpose of}} creating a news story. Main drivers for this process are newly available resources such as open source software, open access publishing and open data. This approach to journalism builds on older practices, most notably on CAR (acronym for [...] "computer-assisted reporting") a label used mainly in the US for decades. Other labels for partially similar approaches are [...] "precision journalism", based on a book by Philipp Meyer, published in 1972, where he advocated the use of techniques from social sciences in researching stories.|$|E
50|$|In October 2014, BT {{simultaneously}} refreshed its newspaper, website, tablet and smartphone apps. One {{big change}} - applied to both its print and digital products - was {{in how it}} organises the news. Previously organised by geographical markets, BT's news is now categorised by industry sectors. The paper {{said that this was}} to underline its sharpened focus on business. Sections include: Companies & Markets, Real Estate, Banking & Finance, Energy & Commodities, Technology, Consumer, Transport and Government & Economy. Another important change for the BT website was the shift to responsive web design, which ensures that the site is readable and easy to navigate on any screen. Other key changes include an emphasis on visual journalism and <b>data-driven</b> <b>journalism,</b> and the introduction of three new blogs on the website. The 2014 redesign also won BT an Award of Excellence from the Society of News Design in March 2015.|$|E
40|$|This article gives a brief {{review of}} the history of data journalism, as well as the {{prerequisites}} for its appearance. The authors describe the advantages of employing data-journalism skills in the newsrooms. Finally, the article provides a review of <b>data-driven</b> <b>journalism</b> projects all over the world, state of art 2014...|$|E
40|$|The diploma thesis titled Transformation of Journalistic Practices Relating to Advent of Data Journalism {{concerns}} {{the current situation}} in Czech data journalism and its devel- opment. It examines how the newsroom, the journalists and the readers cope with new technology, big data and the related advent of data journalism. The theoretical part of the text explains the relationship between classical journalism and data-driven journal- ism and compares the so-called "narrative" and "interactive" approaches as forms of <b>data-driven</b> <b>journalism.</b> Further it gives {{a deeper understanding of}} the relation between data journalism and the new concept of objectivity and it deals with the cultural, social and technical preconditions of data journalism. The thesis also describes the historical aspects of the topic comparing computer-assisted reporting to current practice. Sub- sequent chapters deal with the visualizations, infographics, amateur data journalism and open data which all play a key role in <b>data-driven</b> <b>journalism.</b> The practical part intro- duces the Czech data team. It was originally established within the publishing house Economia and then moved to the office of Czech Radio (Český rozhlas). The final parts discuss the cooperation model of the Czech data team, the use of visualization in their projects, [...] ...|$|E
40|$|This paper {{presents}} the initial {{results of a}} two-year research project, Data Journalism Work Practices, which focuses on newsrooms in Finland, the UK and the US. Data journalism or <b>data-driven</b> <b>journalism</b> has been defined simply as journalism based on large data sets (Rogers 2011; Bounegru et al. 2012.) According to our ongoing research on data journalism work methods, we can claim {{this has been an}} oversimplification. Based on six interviews of leading Finnish, American and British data journalists we can claim that there are already at least three different models for organizing data journalism work practices, and two main streams of data journalism, not just one...|$|E
40|$|This paper {{deals with}} the transformations that have {{occurred}} in news journalism worldwide in the early 21 st century. I argue that they havebeen the most significant changes to the profession for 100 years, and the challenges facing the news media industry in responding to them are substantial, as are those facing journalism education. This argument is developed in relation to the crisis of the newspaper business model, and why social media, blogging and citizen journalism have not filled the gap left by the withdrawal of resources from traditional journalism. It also draws upon Wikileaks as a case study in debates about computational and <b>data-driven</b> <b>journalism,</b> and whether large-scale "leaks" of electronic documents may be the future of investigative journalism. ...|$|E
40|$|The {{coverage}} of Wikileaks' {{huge amounts of}} leaked data was a challenge for newspapers – they {{had to figure out}} how to get stories out of extensive and complex data sets and how to present their findings to readers. The result significantly differs from traditional news reporting; including illustrations, interactive web applications and reading instructions to make the material accessible. This style of news reporting is called <b>data-driven</b> <b>journalism.</b> The international interest in the leaks combined with collaborative work between newspapers from different countries made it a new trend in current journalism. A key lesson from working with this kind of material is that data collection is essential for the effectiveness of the used techniques. If journalists would adapt this insight to their own, internal data collection process, this form of news reporting could be used on a large scale and be much more common. The {{coverage of}} Wikileaks' might give a glimpse of how journalism will look like in the future...|$|E
40|$|ABSTRACT Title of Thesis:	THE PUBLIC CONSCIENCE OF CHICAGO: THE CHICAGO REPORTER AND FOUR DECADES OF INVESTIGATING RACE AND POVERTY 	 	Thomas Brune, Master of Arts, Journalism, 2015 	 Thesis Directed By:	Richard Eaton Professor of Broadcast Journalism Mark Feldstein, Philip Merrill College of Journalism, University of Maryland The Chicago Reporter {{is a small}} nonprofit news {{organization}} founded in 1972 to use investigative and <b>data-driven</b> <b>journalism</b> to uncover and highlight racial and economic disparities in Chicago. It is written for local elites who can implement reforms. Its stories have prompted changes, and it has trained {{a diverse group of}} journalists in the process. But it never has built a broad readership or developed a business plan that doesn’t rely on charity. The question of this thesis is: How has the Chicago Reporter survived for four decades? A review of its history and interviews with its publishers found the Reporter still exists because it has a base at a stable nonprofit, and its reporting on race and poverty draws support from a core group of funders, leaders, and academics. Yet its singular focus has limited expansion, and its recent move to an all-digital operation poses challenges for its future...|$|E
40|$|There {{has never}} been a more {{exciting}} time to be involved in statistics. Emerging data sources provide new sorts of evidence, provoke new sorts of questions, make possible new sorts of answers and shape the ways that evidence is used to influence policy, public opinion and business practices. Significant developments include open data, big data, data visualisation and the rise of <b>data-driven</b> <b>journalism.</b> These developments are changing the nature of the evidence that is available, the ways in which it is presented and used and the skills needed for its interpretation. Educators should place less emphasis on small samples and linear models and more emphasis on large samples, multivariate description and data visualisation. Techniques used to analyse big data need to be taught. The increasing diversity of data usage requires deeper conceptual analysis in the curriculum; this should include explorations of the functions of modelling, and the politics of data and ethics. The data revolution can invigorate the existing curriculum by exemplifying the perils of biassed sampling, corruption of measures and modelling failures. Students need to learn to think statistically and to develop an aesthetic for data handling and modelling based on solving practical problems...|$|E
40|$|Technology {{is shaping}} {{the ways that}} {{evidence}} is used to influence policy and public opinion. Developments include: the semantic web; the Big Data movement, new tools for data visualisation, {{and the rise of}} <b>data-driven</b> <b>journalism.</b> Such developments will have profound effects in terms of the nature of evidence that is gathered, the ways in which it is presented and used, and the skills that will be needed for its interpretation. As such they offer opportunities, but also pose threats to both National Statistics Offices (NSOs) and to statistics educators. A great deal is to be gained from collaborations between NSOs, statistics educators, and other groups. Here, we give examples of the ways that technology is influencing practice, and describe a UK collaboration between the Data Visualisation Centre within the Office for National Statistics and the SMART Centre at Durham University, which sets out to work with journalists and policy makers, and uses Big Data tools to explore success. The opportunities and threats presented by technological developments to NSOs and statistics educators are discussed. We discuss strategies for working effectively with journalists, and other data users...|$|E
40|$|The Open Data Initiative in the UK offers {{incredible}} {{opportunities for}} researchers {{who seek to}} gain insight from the wealth of public and institutional data that is increasingly available from government sources – like NHS prescription and GP referral information – or the information we freely offer online. Coupled with digital technologies that can help teams generate connections and collaborations, these data sets can support large-scale innovation and insight. However, by looking at a comparable explosion in <b>data-driven</b> <b>journalism,</b> this article hopes to highlight some of the ethical questions that may arise from big data. The popularity of the social networking service Twitter to share information during the riots in London in August 2011 produced a real-time record of sense-making of enormous interest to academics, reporters and to Twitter users themselves; however, when analysed and published, academic and journalistic interpretations of aggregate content was transformed and individualized, with potential implications for a user-base that was unaware it was being observed. Similar issues arise in academic research with human subjects. Here, the questions of reflexivity in data design and research ethics are considered through a popular media frame...|$|E
40|$|More {{than two}} years ago Sir Tim Berners-Lee made the {{pronouncement}} that ‘Journalists need to be data-savvy … but now it's also going to be about poring over data and equipping yourself with the tools to analyse it and picking out what's interesting’. 5 This new form of <b>data-driven</b> <b>journalism</b> appears to have been enthusiastically adopted – at least in the rhetoric of news discourse, according to which it is ‘rapidly becoming part of the establishment’. This analysis is a preliminary survey of data-based stories being presented in the national news in the UK, and lays the groundwork for an analysis and typology of the forms and formats of data journalism as a media practice. The analysis shows that while superficial data journalism is being practiced, it is limited in scope and format. No evidence was found of a commitment to data projects among the news outlets examined, and only one instance of recourse to the Freedom of Information Act was seen. Most data presented were superficial, and sourced from traditional outlets. Data journalism is practiced as much for its visual appeal as for its investigative qualities, and the overall impact, especially in the tabloid format is as much decorative as informative...|$|E
40|$|Dissertação de mest., Processamento de Linguagem Natural e Indústrias da Língua, Faculdade de Ciências Humanas e Sociais, Univ. do Algarve, 2013 Nowadays {{social media}} {{play a central}} role in every day life. A huge volume of user-generated data spins around online social networks, such as Twitter, having an {{extraordinary}} impact on the media industry and on the users’ everyday life. More and more users and people use social networks from their computers and smartphones to share their emotions and opinions about the facts happening in the world. Natural language processing and, in particular, sentiment analysis are key technologies to make sense out of the data about news that circulates in the online social networks. The application of opinion mining to news-oriented user-generated contents, such as news-linking tweets, can provide novel views on the news audience behaviour and help to interpret the evolution of sentiments. Applying this capability in the social news-sphere permits (i) to measure the impact of news onto readers and (ii) to gather elements that contain stories. From a broad perspective, the main aim of this research is to face this challenge, that is, to explore how opinion mining (or sentiment analysis) can be adopted into the field of digital media and <b>data-driven</b> <b>journalism...</b>|$|E
40|$|In {{the era of}} <b>data-driven</b> <b>journalism,</b> data {{analytics}} can deliver tools to support journalists in connecting to new and developing news stories, e. g., as echoed in micro-blogs such as Twitter, the new citizen-driven media. In this paper, we propose a framework for tracking and automatically connecting news articles to Twitter conversations as captured by Twitter hashtags. For example, such a system could alert journalists about news that {{get a lot of}} Twitter reaction, so that they can investigate those conversations for new developments in the story, promote their article to a set of interested consumers, or discover general sentiment towards the story. Mapping articles to appropriate hashtags is nevertheless very challenging, due to different language styles used in articles versus tweets, the streaming aspect of news and tweets, as well as the user behavior when marking certain tweet-terms as hashtags. As a case-study, we continuously track the RSS feeds of Irish Times news articles and a focused Twitter stream over a two months period, and present a system that assigns hashtags to each article, based on its Twitter echo. We propose a machine learning approach for classifying and ranking article-hashtag pairs. Our empirical study shows that our system delivers high precision for this task...|$|E
40|$|Megan Knight, 'Data {{journalism in}} the UK: a {{preliminary}} analysis {{of form and}} content', Journal of Media Practice, Vol. 16 (1), Mach 2015, doi: [URL] {{than two years ago}} Sir Tim Berners-Lee made the pronouncement that ???Journalists need to be data-savvy ??? but now it's also going to be about poring over data and equipping yourself with the tools to analyse it and picking out what's interesting???. 5 This new form of <b>data-driven</b> <b>journalism</b> appears to have been enthusiastically adopted ??? at least in the rhetoric of news discourse, according to which it is ???rapidly becoming part of the establishment???. This analysis is a preliminary survey of data-based stories being presented in the national news in the UK, and lays the groundwork for an analysis and typology of the forms and formats of data journalism as a media practice. The analysis shows that while superficial data journalism is being practiced, it is limited in scope and format. No evidence was found of a commitment to data projects among the news outlets examined, and only one instance of recourse to the Freedom of Information Act was seen. Most data presented were superficial, and sourced from traditional outlets. Data journalism is practiced as much for its visual appeal as for its investigative qualities, and the overall impact, especially in the tabloid format is as much decorative as informative...|$|E
40|$|The WikiLeaks Afghanistan war logs contain nearly $ 77, 000 $ {{reports of}} {{incidents}} in the US-led Afghanistan war, covering the period from January 2004 to December 2009. The recent growth of data on complex social systems and the potential to derive stories from them has shifted the focus of journalistic and scientific attention increasingly toward <b>data-driven</b> <b>journalism</b> and computational social science. In this paper we advocate the usage of modern statistical methods for problems of data journalism and beyond, which may help journalistic and scientific work and lead to additional insight. Using the WikiLeaks Afghanistan war logs for illustration, we present an approach that builds intelligible statistical models for interpretable segments in the data, in this case to explore the fatality rates associated with different circumstances in the Afghanistan war. Our approach combines preprocessing by Latent Dirichlet Allocation (LDA) with model trees. LDA is used to process the natural language information contained in each report summary by estimating latent topics and assigning each report to one of them. Together with other variables these topic assignments serve as splitting variables for finding segments in the data to which local statistical models for the reported number of fatalities are fitted. Segmentation and fitting is carried out with recursive partitioning of negative binomial distributions. We identify segments with different fatality rates that correspond to {{a small number of}} topics and other variables as well as their interactions. Furthermore, we carve out the similarities between segments and connect them to stories that have been covered in the media. This gives an unprecedented description of the war in Afghanistan and serves {{as an example of how}} data journalism, computational social science and other areas with interest in database data can benefit from modern statistical techniques. Comment: Published in at [URL] the Annals of Applied Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E

