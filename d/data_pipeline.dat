216|1432|Public
50|$|Data {{provenance}} or data lineage {{can be used}} to {{make the}} debugging of big <b>data</b> <b>pipeline</b> easier. This necessitates the collection of data about data transformations. The below section will explain data provenance in more detail.|$|E
5000|$|AWS <b>Data</b> <b>Pipeline</b> {{provides}} reliable {{service for}} data transfer between different AWS compute and storage services (e.g., Amazon S3, Amazon RDS, Amazon DynamoDB, Amazon EMR). In other words, this service {{is simply a}} data-driven workload management system, which provides a management API for managing and monitoring of data-driven workloads in cloud applications.|$|E
50|$|The {{final step}} in the data flow {{reconstruction}} is the Topological sorting of the association graph. The directed graph created in the previous step is topologically sorted to obtain {{the order in which}} the actors have modified the data. This inherit order of the actors defines the data flow of the big <b>data</b> <b>pipeline</b> or task.|$|E
30|$|In the {{specific}} {{form of a}} workflow called <b>data</b> <b>pipelining,</b> records are passed individually down the pipes. <b>Data</b> <b>pipelining</b> allows the automation of the HTVS process and the integration of several related modelling and database packages. Thus, in addition to orchestration of multiple workflow steps, the <b>data</b> <b>pipelining</b> provides means for seamless data exchange between the individual application modules. Users can share and reuse prepared sets of tasks and workflows to ease their analysis in HTVS projects. Such analysis steps can be later deployed on HPC resources in a simple and automated fashion.|$|R
50|$|For businesses, VoIP obviates {{separate}} {{voice and}} <b>data</b> <b>pipelines,</b> channelling {{both types of}} traffic through the IP network while giving the telephony user a range of advanced abilities.|$|R
50|$|Continuous {{analytics}} is a {{data science}} process that abandons ETLs and complex batch <b>data</b> <b>pipelines</b> {{in favor of}} cloud-native and microservices paradigms. Continuous data processing enables realtime interactions and immediate insights with fewer resources.|$|R
5000|$|The {{need for}} error bounds {{on the results}} - since real world data often contain errors and {{variations}} error bound need to be used (for approximate matching) so that real decisions that have direct impact on people can be made based on these results. [...] Research on error propagation in the full <b>data</b> <b>pipeline</b> from data integration to final analysis is also important.|$|E
50|$|Big Data {{platforms}} have a {{very complicated}} structure. Data is distributed among several machines. Typically the jobs are mapped into several machines and results are later combined by reduce operations. Debugging of a big <b>data</b> <b>pipeline</b> becomes very challenging because of {{the very nature of}} the system. It will not be an easy task for the data scientist to figure out which machine's data has the outliers and unknown features causing a particular algorithm to give unexpected results.|$|E
50|$|Starting with MemSQL 4.1, {{launched}} in September 2015, MemSQL gives users {{the ability to}} install Apache Spark {{as part of the}} MemSQL cluster, and use Spark as an ETL tool to import data into MemSQL.Apache Spark is installed and set up interactively using MemSQL Ops. Ops users can then define the extract, transform, and load phases of their <b>data</b> <b>pipeline</b> to import data into MemSQL.Management and monitoring of running data pipelines can be done within the Ops UI.|$|E
40|$|In {{this paper}} {{we use the}} Adapteva Epiphany manycore chip to {{demonstrate}} how the throughput and the latency of a baseband signal processing chain, typically found in LTE or WiFi, can be optimized {{by a combination of}} task- and data parallelization, and <b>data</b> <b>pipelining.</b> The parallelization and <b>data</b> <b>pipelining</b> are facilitated by the shared memory architecture of the Epiphany, and the fact that a processor on one core can write directly into the memory of any other core on the chip. Comment: Draft paper submitted to and accepted by PDP 2016, 24 th Euromicro International Conference on Parallel, Distributed and Network-Based Processing. Heraklion Crete, Greece, 17 th- 19 th February 201...|$|R
50|$|NoETL is an {{approach}} to end-to-end <b>data</b> <b>pipelines,</b> or <b>data</b> engineering at scale, similar to what NoSQL is to SQL for the persistence and analytics component of those pipelines.The first coiner of the term NoETL is Seth Grimes.|$|R
50|$|Currently, ESA and IPAC {{continue}} {{efforts to}} improve the <b>data</b> <b>pipelines</b> and specialized software analysis tools to yield the best quality calibration and data reduction methods from the mission. IPAC supports ISO observers and data archive users through in-house visits and workshops.|$|R
50|$|This is {{the most}} crucial step in Big Data debugging. The {{captured}} lineage is combined and processed to obtain the data flow of the pipeline. The data flow helps the data scientist or a developer to look deeply into the actors and their transformations. This step allows the data scientist {{to figure out the}} part of the algorithm that is generating the unexpected output. A big <b>data</b> <b>pipeline</b> can go wrong in 2 broad ways. The first is a presence of a suspicious actor in the data-flow. The second being the existence of outliers in the data.|$|E
50|$|The {{idea was}} {{initiated}} by the Polish astronomy Professor Bohdan Paczyński of Princeton University. The prototype instrument and <b>data</b> <b>pipeline</b> were designed and built by Grzegorz Pojmański. The work on the ASAS program began in 1996 with a mere $1 million budget. The automatic telescope, located in Las Campanas Observatory, Chile, was designed to register the brightness of circa one million stars in the Southern Hemisphere. However, it proved very efficient and helped to find many new variable stars. The project was then expanded, and now operates four telescopes located in Las Campanas Observatory. The Chilean observatory is operated by the Carnegie Institution of Washington.|$|E
50|$|In computing, MISD (multiple instruction, single data) {{is a type}} of {{parallel}} computing architecture where many functional units perform different operations on the same <b>data.</b> <b>Pipeline</b> architectures belong to this type, though a purist might say that the data is different after processing by each stage in the pipeline. Fault-tolerant computers executing the same instructions redundantly in order to detect and mask errors, in a manner known as task replication, may be considered to belong to this type. Not many instances of this architecture exist, as MIMD and SIMD are often more appropriate for common data parallel techniques. Specifically, they allow better scaling and use of computational resources than MISD does. However, one prominent example of MISD in computing are the Space Shuttle flight control computers.|$|E
30|$|This section {{discussed}} the main {{requirements for the}} five phases of manufacturing process <b>data</b> analysis <b>pipelines.</b> These non-functional and functional requirements are summarized in Tables 1 and 2. The next section addresses the second research question by discussing the existing <b>data</b> analysis <b>pipelines</b> in academic literature.|$|R
40|$|The ORAC <b>data</b> {{reduction}} <b>pipeline,</b> {{developed for}} UKIRT, {{has been designed}} to be a completely general approach to writing <b>data</b> reduction <b>pipelines.</b> This generality has enabled the JCMT to adapt the system for use with SCUBA with minimal development time using the existing SCUBA data reduction algorithms (Surf) ...|$|R
40|$|In {{this paper}} the authors {{describe}} {{the approach to}} research, develop, and evaluate prototype middleware tools and architectures. The developed tools {{can be used by}} scientists to compose astronomical <b>data</b> analysis <b>pipelines</b> easily. They use the SuperMacho <b>data</b> <b>pipelines</b> as example applications to test the framework. they describe their experience from scheduling and running these analysis pipelines on massive parallel processing machines. they use MCR a Linux cluster machine with 1152 nodes and Luster parallel file system as the hardware test-bed to test and enhance the scalability of the tools...|$|R
50|$|Video synthesizers {{moved from}} analog to the {{precision}} control of digital. The first digital effects {{as exemplified by}} Stephen Beck's Video Weavings used digital oscillators optionally linked to horizontal, vertical, or frame resets to generate timing ramps. These ramps could be gated to create the video image itself and were responsible for its underlying geometric texture. Schier and Vasulka advanced {{the state of the}} art from address counters to programmable (microcodable) AMD Am2901 bit slice based address generators. On the data path, they used 74S181 arithmetic and logic units, previously thought of as a component for doing arithmetic instructions in minicomputers, to process real time video signals, creating new signals representing the sum, difference, AND, XOR, and so on, of two input signals. These two elements, the address generator, and the video <b>data</b> <b>pipeline,</b> recur as core features of digital video architecture.|$|E
5000|$|The Kepler Mission, a space {{observatory}} {{launched in}} March 2009 to locate Earth-like planets, monitors {{a section of}} space containing more than 200,000 stars and takes high-resolution images every 30 minutes. After the operations center gathers this data, it is pipelined to Pleiades in order to calculate the size, orbit, and location of the planets surrounding these stars. As of February 2012, the Kepler mission has discovered 1,235 planets, 5 of which are approximately Earth-sized and orbit within the [...] "habitable zone" [...] where water can exist in all three forms (solid, liquid, gas). After setbacks following the failure of two of Kepler's four reaction wheels, responsible for keeping the spacecraft pointed in the correct direction, in 2013, the Kepler team moved the entire <b>data</b> <b>pipeline</b> to Pleiades, which continues to run light curve analyses from the existing Kepler data.|$|E
50|$|The {{information}} stored {{in terms of}} associations needs to be combined by some means to get the data flow of a particular job. In a distributed system a job is broken down into multiple tasks. One or more instances run a particular task. The results produced on these individual machines are later combined together to finish the job. Tasks running on different machines perform multiple transformations on the data in the machine. All the transformations applied to the data on a machines is stored in the local lineage store of that machines. This information needs to be combined together to get the lineage of the entire job. The lineage of the entire job should help the data scientist understand the data flow of the job and he/she can use the data flow to debug the big <b>data</b> <b>pipeline.</b> The data flow is reconstructed in 3 stages.|$|E
40|$|Big <b>Data</b> <b>Pipelines</b> {{decompose}} complex {{analyses of}} large data sets {{into a series}} of simpler tasks, with independently tuned components for each task. This modular setup allows re-use of components across several different pipelines. However, the interaction of independently tuned pipeline components yields poor end-to-end performance as errors introduced by one component cascade through the whole pipeline, affecting overall accuracy. We propose a novel model for reasoning across components of Big <b>Data</b> <b>Pipelines</b> in a probabilistically well-founded manner. Our key idea is to view the interaction of components as dependencies on an underlying graphical model. Different message passing schemes on this graphical model provide various inference algorithms to trade-off end-to-end performance and computational cost. We instantiate our framework with an efficient beam search algorithm, and demonstrate its efficiency on two Big Data Pipelines: parsing and relation extraction...|$|R
30|$|This survey {{identifies}} {{and addresses}} two research questions {{with the goal}} of supporting data engineers in the development of big <b>data</b> analysis <b>pipelines</b> for manufacturing process data. The first research question addresses the requirements for big <b>data</b> analysis <b>pipelines</b> for manufacturing process data. The second research question surveys the available pipelines in academic literature.|$|R
5000|$|KNIME (...) , the Konstanz Information Miner, is an {{open source}} data analytics, {{reporting}} and integration platform. KNIME integrates various components for machine learning and data mining through its modular <b>data</b> <b>pipelining</b> concept. A graphical user interface allows assembly of nodes for data preprocessing (ETL: Extraction, Transformation, Loading), for modeling and data analysis and visualization. To some extent KNIME can be cosidered as SAS alternative.|$|R
30|$|Future {{work will}} focus on the {{implementation}} and deployment of the big <b>data</b> <b>pipeline</b> in DePuy Ireland. Our aim is to validate the big <b>data</b> <b>pipeline</b> architecture, reassess and extend the requirements presented, quantify the percentage of data sources that can be accessed in the factory using the ingestion process, and estimate the throughput capacity of the pipeline using load testing. Finally, there are two research projects in DePuy Ireland where we plan to use the <b>data</b> <b>pipeline</b> to feed predictive maintenance applications for Wind Turbine and Air Handling Units in the facility.|$|E
40|$|We give an {{overview}} of the operational concepts and architecture of the Kepler Science <b>Data</b> <b>Pipeline.</b> Designed, developed, operated, and maintained by the Science Operations Center (SOC) at NASA Ames Research Center, the Kepler Science <b>Data</b> <b>Pipeline</b> is central element of the Kepler Ground Data System. The SOC charter is to analyze stellar photometric data from the Kepler spacecraft and report results to the Kepler Science Office for further analysis. We describe how this is accomplished via the Kepler Science <b>Data</b> <b>Pipeline,</b> including the hardware infrastructure, scientific algorithms, and operational procedures. The SOC consists of an office at Ames Research Center, software development and operations departments, and a data center that hosts the computers required to perform data analysis. We discuss the high-performance, parallel computing software modules of the Kepler Science <b>Data</b> <b>Pipeline</b> that perform transit photometry, pixel-level calibration, systematic error-correction, attitude determination, stellar target management, and instrument characterization. We explain how data processing environments are divided to support operational processing and test needs. We explain the operational timelines for data processing and the data constructs that flow into the Kepler Science <b>Data</b> <b>Pipeline...</b>|$|E
40|$|Grain Boundaries govern many {{properties}} of polycrystalline materials, including {{the vast majority}} of engineering materials. Evolutionary algorithm can be applied to predict the grain boundary structures in different systems. However, the recognition and classification of thousands of predicted structures is a very challenging work for eye detection in terms of efficiency and accuracy. A <b>data</b> <b>pipeline</b> is developed to accelerate the classification and recognition of grain boundary structures predicted by Evolutionary Algorithm. The <b>data</b> <b>pipeline</b> has three main components including feature engineering of grain boundary structures, density-based clustering analysis and parallel K-Means clustering analysis. With this <b>data</b> <b>pipeline,</b> we could automate the structure analysis and develop better structural and physical understanding of grain boundaries...|$|E
40|$|Abstract. We {{discuss the}} design of new network {{applications}} that collect and manage data and interface with users through portable devices. These applications maintain <b>data</b> <b>pipelines</b> that interact with network information servers and distill information into reduced data models that may be queried and replicated effectively through wireless networks to computing devices of very small size. We present initial design considerations for applications of this class. 1...|$|R
40|$|The {{extensible}} N-Dimensional Data Format (NDF) {{was designed}} and developed in the late 1980 s to provide a data model suitable {{for use in a}} variety of astronomy data processing applications supported by the UK Starlink Project. Starlink applications were used extensively, primarily in the UK astronomical community, and form the basis of a number of advanced <b>data</b> reduction <b>pipelines</b> today. This paper provides an overview of the historical drivers for the development of NDF and the lessons learned from using a defined hierarchical data model for many years in data reduction software, <b>data</b> <b>pipelines</b> and in <b>data</b> acquisition systems. Comment: 19 pages, 7 figures, submitted to the Astronomy & Computing special issue on astronomy data format...|$|R
50|$|Even though use data lineage {{is a novel}} way of {{debugging}} of big <b>data</b> <b>pipelines,</b> {{the process}} is not simple. The challenges are scalability of lineage store, fault tolerance of the lineage store, accurate capture of lineage for black box operators and many others. These challenges must be considered carefully and trade offs between them need to be evaluated to make a realistic design for data lineage capture.|$|R
40|$|The Seamless Integrated <b>Data</b> <b>Pipeline</b> {{system was}} {{proposed}} to the European Union {{in order to}} overcome the information quality shortcomings of the current international supply chain information exchange systems. Next to identification and authorization of stakeholders, secure access control needs to be considered at design time of the new <b>data</b> <b>pipeline</b> system. This challenge is taken up in this paper. First, based on {{an analysis of the}} proposed <b>data</b> <b>pipeline</b> concept, access control requirements are being defined. Second, a new multi-level access control model is being designed. The resulting model organizes access control at two levels, namely, at country and at service level, herewith enabling secure information exchange between global stakeholders. © 2013 Springer-Verlag Berlin Heidelberg...|$|E
3000|$|... a parallelized {{computation}} {{layer to}} support multiple scalable models, {{each with its}} <b>data</b> <b>pipeline</b> and transformation set; [...]...|$|E
40|$|The NVOs {{core data}} mining and archive {{federation}} activities are {{heavily dependent on}} the underlying <b>data</b> <b>pipeline</b> software necessary to translate the raw data into scientifically relevant source detections. The <b>data</b> <b>pipeline</b> software dictates: the raw data storage and retrieval mechanisms, the meaning and format of the fields in the source catalogs, {{and the ability of}} the NVO users to re-analyze raw data for their own purposes. Increasing the performance of the core <b>data</b> <b>pipeline</b> software so that it can address the needs of current and future high data rate surveys is an important activity that should be addressed in concert with the development of the NVO. Comment: 5 pages, including 4 color figures, to appear in proceedings of "Virtual Observatories of the Future...|$|E
40|$|The readout {{system for}} the Aleph central {{tracking}} detector, a large time projection chamber (TPC), consists of more than 100 FASTBUS crates with approximately 1000 FASTBUS modules. The detector and its associated electronics are briefly presented, followed by a more {{detailed description of the}} readout and control system. The discussion covers the sector readout, electronics calibration, front-end <b>data</b> acquisition, <b>data</b> <b>pipelining,</b> and service request handling. Experiences with the system are discusse...|$|R
40|$|Abstract. In {{this paper}} we {{highlight}} {{the suitability of}} MDSP 3 architecture to exploit the <b>data,</b> algorithmic, and <b>pipeline</b> parallelism offered by video processing algorithms like the MPEG- 2 for real-time performance. Most existing implementations extract either <b>data</b> or <b>pipeline</b> parallelism along with Instruction Level Parallelism (ILP) in their implementations. We discuss the design o...|$|R
30|$|Yet, {{the results}} of RQ 1 define the {{requirements}} that should be met by a <b>data</b> analysis <b>pipeline</b> for manufacturing process data. They are agnostic to industry and use case. Thus, process <b>data</b> analysis <b>pipeline</b> should at the least meet these requirements {{and those that are}} specific to the industry and use case. Therefore, {{the results of}} RQ 1 are used to establish the context needed to explain the pipelines in RQ 2.|$|R
