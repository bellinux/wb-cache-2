24|45|Public
25|$|PostScript is an {{interpreted}} {{programming language}} with an implicit global state, so instructions accompanying {{the description of}} one page can affect the appearance of any following page. Therefore, all preceding pages in a PostScript document must be processed to determine the correct appearance of a given page, whereas each page in a PDF document is unaffected by the others. As a result, PDF viewers allow the user to quickly jump to the final pages of a long document, whereas a PostScript viewer needs to process all pages sequentially before being able to display the <b>destination</b> <b>page</b> (unless the optional PostScript Document Structuring Conventions have been carefully complied with).|$|E
2500|$|In the past, the PageRank {{shown in}} the Toolbar was easily manipulated. Redirection from one page to another, either via a HTTP 302 {{response}} or a [...] "Refresh" [...] meta tag, caused the source page to acquire the PageRank of the <b>destination</b> <b>page.</b> Hence, a new page with PR 0 and no incoming links could have acquired PR 10 by redirecting to the Google home page. This spoofing technique was a known vulnerability. Spoofing can generally be detected by performing a Google search for a source URL; if the URL of an entirely different site is displayed in the results, the latter URL may represent the destination of a redirection.|$|E
50|$|Because many {{search engines}} give a penalty {{for using the}} META refresh command, some doorway pages just trick the visitor into {{clicking}} on a link {{to get them to}} the desired <b>destination</b> <b>page,</b> or they use JavaScript for redirection.|$|E
40|$|We present PearlView, a {{graphical}} UI component helping {{users to}} navigate through hyperlists — {{such as the}} search results of a search engine — and preview the <b>destination</b> <b>pages</b> without the disruption of context normally created by {{going back and forth}} between pages. PearlView, unlike thumbnails, provides a comprehensive set of functionality for each page preview...|$|R
5000|$|A10 (Unvalidated redirects {{and forward}}s) : Web {{applications}} frequently redirect and forward users to other pages and websites, and use untrusted data {{to determine the}} <b>destination</b> <b>pages.</b> Without proper validation, attackers can redirect victims to phishing or malware sites, or use forwards to access unauthorized pages. This vulnerability is mainly related with the manipulation of readonly data or data generated previously at server side. HDIV controls all the data server by the server and does not allow the redirection to malicious web sites.|$|R
30|$|The {{parent and}} the <b>destination</b> Web <b>page</b> {{classifiers}} both {{take advantage of}} the textual content present on a hyperlink’s parent and <b>destination</b> Web <b>page,</b> respectively, so as to estimate their relevance to the HME domain. Each resource is parsed; its textual content is extracted; tokenization, stopwords removal, and stemming are applied; and a textual feature vector is generated using the tf.idf term weighting scheme, where tf(t,d) is defined as above and idf(t) is the inverse document frequency of term t in the collection. Each classifier produces a confidence score using different input: the parent <b>page</b> and the <b>destination</b> textual content, respectively.|$|R
50|$|Each <b>destination</b> <b>page</b> offered {{information}} on the local time and date, number of members living there, member tips, hotel reviews, must-see activities, restaurant reviews, local customs, nightlife, off-the-beaten path tips, tourist traps, warnings, transportation, packing lists, shopping, sports travel and general tips.|$|E
50|$|Google Analytics {{provides}} a path function with funnels and goals. A predetermined path of web site pages is specified and every visitor walking the path is a goal. This approach is very helpful when analyzing how many visitors reach a certain <b>destination</b> <b>page,</b> called an end point analysis.|$|E
50|$|Smeet {{operates}} {{without any}} download and is accessible directly in any browser that supports Flash. Smeet is supported on both Mac and PC. The users {{can sign up}} on the <b>destination</b> <b>page</b> or get online directly via the Facebook application without any further registration needed. The Smeet website is also possible to integrate to any other website through iframe.|$|E
40|$|Web {{search engines}} have become {{indispensable}} {{in our daily}} life to help us finding the information we need. Several search tools, for instance Google, use links to select the matching documents against a query. In this paper, we propose a new ranking function that combines content and link rank based on propagation of scores over links. This function propagates scores from source <b>pages</b> to <b>destination</b> <b>pages</b> in relation with query terms. We assessed our ranking function with experiments over two test collections WT 10 g and GOV. We conclude that propagating link scores according to query terms provides significant improvement for information retrieval...|$|R
30|$|It investigates 11 {{different}} hyperlink selection methods {{utilizing the}} link-based classifier {{or a combination}} of the link-based classifier with the parent or the <b>destination</b> Web <b>page</b> classifier, enabled based on the conditions discussed earlier.|$|R
40|$|International audienceWeb {{search engines}} have become {{indispensable}} {{in our daily}} life to help us finding the information we need. Several search tools, for instance Google, use links to select the matching documents against a query. In this paper, we propose a new ranking function that combines content and link rank based on propagation of scores over links. This function propagates scores from source <b>pages</b> to <b>destination</b> <b>pages</b> in relation with query terms. We assessed our ranking function with experiments over two test collections WT 10 g and GOV. We conclude that propagating link scores according to query terms provides significant improvement for information retrieval...|$|R
50|$|In 1996, with iPer, {{probably}} {{for the first time}} for a Web softwarethe author was able to choose the <b>destination</b> <b>page</b> with a click on a visual preview (a sort of Web browser), without having to type the actual file name or the URL, and without having to be online (connected to the Internet) while performing the various edit on his/her hypertext document.|$|E
5000|$|In the past, the PageRank {{shown in}} the Toolbar was easily manipulated. Redirection from one page to another, either via a HTTP 302 {{response}} or a [...] "Refresh" [...] meta tag, caused the source page to acquire the PageRank of the <b>destination</b> <b>page.</b> Hence, a new page with PR 0 and no incoming links could have acquired PR 10 by redirecting to the Google home page. This spoofing technique was a known vulnerability. Spoofing can generally be detected by performing a Google search for a source URL; if the URL of an entirely different site is displayed in the results, the latter URL may represent the destination of a redirection.|$|E
5000|$|PostScript is an {{interpreted}} {{programming language}} with an implicit global state, so instructions accompanying {{the description of}} one page can affect the appearance of any following page. Therefore, all preceding pages in a PostScript document must be processed to determine the correct appearance of a given page, whereas each page in a PDF document is unaffected by the others. As a result, PDF viewers allow the user to quickly jump to the final pages of a long document, whereas a PostScript viewer needs to process all pages sequentially before being able to display the <b>destination</b> <b>page</b> (unless the optional PostScript Document Structuring Conventions have been carefully complied with).|$|E
40|$|We {{present a}} new {{approach}} in web search engines. The web creates new challenges for information retrieval. The vast improvement in information access {{is not the only}} advantage resulting from the keyword search. Additionally, much potential exists for analyzing interests and relationships within the structure of the web. The creation of a hyperlink by the author of a web page explicitly represents a relationship between the source and <b>destination</b> <b>pages</b> which demonstrates the hyperlink structure between web pages. Our web search engine searches not only for the keywords in the web pages, but also for the hyperlink structure between them. Comparing the results of structural web search versus keyword-based search indicates an improved ability to access desired information. We also discuss steps toward mining the queries input to the structural web search engine...|$|R
3000|$|The {{conditions}} {{upon which}} the parent and the <b>destination</b> Web <b>page</b> classifiers are enabled vary and depend on the tradeoff among the advantages and the disadvantages of each approach. Given that the local context of the hyperlinks pointing to the Dark Web often contains insufficient evidence, the <b>destination</b> Web <b>page</b> classifier may be enabled based on the network type encountered (i.e., Surface Web, Tor, I 2 P, Freenet) either when the link-based score is not robust enough (i.e., it produces a score which {{does not provide a}} strong positive or negative estimation about the relevance of the page with the domain of interest) or when the number of the words contained within the local context is not sufficient for producing a concrete estimation. In the former case, a weak hyperlink estimation is assumed when the link-based classifier returns a score (score [...]...|$|R
40|$|The study aims {{to improve}} Web {{navigation}} efficiency by reorganizing Web structure. Navigation efficiency is defined mathematically for both navigation with / without target <b>destination</b> <b>pages,</b> e. g. for experienced and new users. To help experienced users {{not to lose}} their orientation, structure stability is taken into consideration. Stability constraint can also help website designers control the maintaining effort of Web. This study proposes a mathematical programming method to reorganize Web structure {{in order to achieve}} better navigation efficiency. Designer can specify the user requirements and how stable the website structure should be. An e-banking example is given to illustrate how the method works in scenarios where user surfs with target destination. This study has the advantage of assessing and improving navigation efficiency and of relieving the designer of tedious chore to modify the structure in transformation. Keywords:Web Structure, Reorganization, Navigation Efficienc...|$|R
5000|$|... "The Immortal Bard" [...] is {{a science}} fiction short story by Isaac Asimov. It was first {{published}} in the May 1954 issue of Universe Science Fiction, and has since been republished in several collections and anthologies, including Earth Is Room Enough (1957) and The Best Science Fiction of Isaac Asimov (1986). (In Earth Is Room Enough (Panther Books Ltd. reprint 1973 edition) the title of the story is [...] "An Immortal Bard" [...] in the Contents list but [...] "The Immortal Bard" [...] on the <b>destination</b> <b>page.</b> There is a similar, but reversed variation in title with The Author's Ordeal.) Like many of his stories, it is told as a conversation, in this case between two professors at a college faculty's annual Christmas party.|$|E
30|$|Based on the {{observation}} that the local context of a hyperlink may lack strong evidence and thus may affect negatively the link-based classifier’s performance, additional link selection methods are investigated {{in an effort to}} improve the effectiveness of the hybrid focused crawler. In this context, the link-based classifier is accompanied by a parent or a destination Web page classifier, so as to further enhance the link selection policy. As already discussed, given that Web pages tend to link to others with similar content, the global context of hyperlinks on the parent page could be used for adjusting the estimates of relevance determined by the local context; hence, the parent Web page classifier can be used in conjunction with the link-based classifier in an effort to further enhance the assessment process [16]. On the other hand, a destination Web page classifier can be used for reaching concrete estimations about a hyperlink’s <b>destination</b> <b>page</b> relevance to the domain of interest, given that the actual textual context of a page constitutes the most representative means for producing solid estimations. However, this solution entails the potential extra burden of downloading the <b>destination</b> <b>page</b> content; hence, it may deteriorate the focused crawler time performance; thus, it should be sparingly used, only under conditions indicating that the benefit acquired will exceed the efficiency reduction.|$|E
30|$|This work aims {{to develop}} a crawler capable of {{traversing}} both the Surface Web and the darknets present in the Dark Web (i.e., Tor, I 2 P, and Freenet) {{with the goal of}} discovering Web resources on any given topic, with particular focus on topics of interest to LEAs. To this end, it develops a crawler capable of gathering content focused on a given topic by selecting to follow only the hyperlinks that lead to relevant resources. This selection is performed using classification approaches that take advantage of the local context of each hyperlink, {{as well as of the}} global context of the parent and the <b>destination</b> <b>page</b> of a hyperlink. The proposed focused crawler is demonstrated for a specific topic of interest to LEAs: the discovery of Web resources containing recipes for producing HMEs.|$|E
30|$|To {{assess the}} {{effectiveness}} of the proposed hybrid crawler focused on the HME domain, several experiments were performed based on Web pages from the Surface and the Dark Web, after investigating the 11 hyperlink selection methods proposed, which exploit the link-based classifier in conjunction with either the parent or the <b>destination</b> Web <b>page</b> classifier.|$|R
40|$|We {{propose a}} {{two-level}} {{model for the}} World Wide Web: Web pages are stored at the first level, and the links between them pass through a link abstraction layer. This extra layer permits processing {{to be carried out}} as part of link traversal, thereby increasing the expressiveness of links beyond direct connection between pages. This mechanism is implemented using LogicWeb, which allows Web pages to be treated as logic programs. One advantage of LogicWeb is that it enables conceptual Web structures to be coded in the abstraction layer which are amenable to sophisticated querying and automated reasoning. Another benefit is that it allows the easy specification of rules which model the nondeterministic and unpredictable nature of link activation in the Web. 1 Introduction The Web's basic link mechanism is uni-directional, with fixed source and <b>destination</b> <b>pages.</b> When a page is retrieved, it is simply displayed. We extend the expressiveness of links by proposing a two-level model for the We [...] ...|$|R
40|$|Understanding the {{specific}} nature of disorientation in hyperspace {{will benefit from}} a battery of characterizations of the space being navigated, the user navigating the space, and their interaction. This study focuses on a particular consideration for understanding the "lost-in-hyperspace" problem, namely "transitional volatility". Metrics investigated in relation to disorientation and Web site mental models include: 1) the navigational and content changes of a Web site's interface in page-to-page transitions, and 2) the users' ability to reorient themselves to these changes. Metrics to relate to disorientation and Web site mental models include {{the extent to which}} 1) a navigation session is volatile, 2) a user is typically habituated in navigation patches, and 3) a user can predict navigation support changes at <b>destination</b> <b>pages.</b> The primary concern of the study was the effects of the navigational volatility on disorientation and Website mental models for two common hierarchical navigational schemes: partial overview and local context support...|$|R
30|$|Over 2 years, HealthMeasures.net {{visitors}} {{engaged in}} 360, 999 sessions with 1, 549, 200 total page hits. Pages {{with the highest}} number of sessions include 25 descriptive, 12 applications, 9 product, and 4 educational pages. Descriptive pages, such as the PROMIS landing page, drew {{the highest number of}} sessions for new and returning visitors, averaging 10, 821 sessions per <b>destination</b> <b>page.</b> Product pages, such as the Search and View Measures, averaged longer sessions and the most page visits per session, with new visitors who completed a search averaging 93 page visits per session. Visitors to application or educational pages spent an average of 30 seconds longer on a page than visitors to descriptive or product pages. Sessions that included application pages were dominated by returning visitors, with the exception of PROMIS scoring pages which both new and returning users heavily engaged in, averaging 2 : 03 - 3 : 33 minutes per page.|$|E
30|$|The hybrid focused crawler selects the hyperlinks {{to follow}} by {{employing}} a classifier-guided approach, which {{relies on a}} suite consisting of three different classifiers: (i) a link-based classifier, (ii) a parent Web page classifier, and (iii) a destination Web page classifier. The link-based classifier estimates the relevance of a hyperlink to an unvisited resource based on its local context in the parent page, whereas the parent and the destination Web page classifiers estimate {{the relevance of the}} parent and the <b>destination</b> <b>page</b> of a hyperlink, respectively, based on their global (textual) context. These three classifiers are employed by the hyperlink selection methods supported within the link selection component. Each method employs a different combination of the available classifiers, where the link-based classifier is accompanied by the parent or the destination Web page classifier, depending on conditions related to the destination network, the link score produced, or the existence of strong evidence within the local context of a hyperlink.|$|E
40|$|Abstract. An {{approach}} {{for reducing the}} navigation effort for the users of a web site is to enhance its hyperlink structure with additional hotlinks. We address the task of adding at most one such additional outgoing edge to each page of a tree-like site, minimizing the path length, i. e. the expected number of “clicks ” necessary for the user to reach his <b>destination</b> <b>page.</b> Another common formulation of that problem is to maximize the gain, i. e. the path length reduction achieved by the assignment. In this work we analyze the natural greedy strategy, proving that it reaches the optimal gain up to the constant factor of 2. Considering the gain, we also prove {{the existence of a}} PTAS. Finally, we give a polynomial time 2 -approximation which constitutes the first constant factor approximation in terms of the path length. The algorithms ’ performance analyses are made possible by a set of three new basic operations for the transformation of hotlink assignments...|$|E
50|$|The native {{character}} {{encoding scheme}} of MVS and its peripherals is EBCDIC, but the TR instruction {{made it easy}} to translate to other 7- and 8-bit codes. Over time IBM added hardware-accelerated services to perform translation to and between larger codes, hardware-specific service for Unicode transforms and software support of, e.g., ASCII, ISO/IEC 8859, UTF-8, UTF-16, and UTF-32. The software translation services take source and <b>destination</b> code <b>pages</b> as inputs.|$|R
5000|$|Method and {{apparatus}} for paging alternate users, #6,057,782, [...] "The idea {{of allowing}} paging systems that support acknowledgments to reorder {{their list of}} <b>destinations</b> for future <b>pages</b> based on the acknowledgment or lack thereof on previous pages." ...|$|R
40|$|The {{objective}} {{of this research is}} to optimize the link structure of webpages for an e-supermarket. Customers are looking for greater convenience in shopping when they visit the website of an e-supermarket, while e-supermarket managers prefer webpages that contain information about profitable products to be visited more frequently. In order to balance the interests of both parties and to aid the webmaster in updating the website regularly, we present a mathematical model with the {{objective of}} minimizing the overall weighted distances between webpages. An updating algorithm is used to determine the distance between pages. It is proved to be more efficient under certain special circumstances. We propose the statistical Hopfield neural-network and the strategic oscillation-based tabu-search algorithms as solving methods. The former is appropriate for optimizing small-scale problems. The latter is good at solving large-scale problems approximately. The preliminary validity of the model and the performance of the algorithms is demonstrated by experiments on a small website and several large websites, using randomly generated data. The <b>destination</b> <b>pages</b> that customers and website managers preferred are proved to be more accessible after optimization. Department of Industrial and Systems Engineerin...|$|R
40|$|Abstract. This paper {{describes}} a path-based method {{to use the}} multi-step navigation information discovered from website structures for web page ranking. Use of hyperlinks to enhance page ranking has been widely studied. The underlying assumption is that hyperlinks convey recommendations. Although this technique has been used successfully in global web search, it produces poor results for website search, {{because the majority of}} the hyperlinks in local websites are used to organize information and convey no recommendations. This paper defines the Hierarchical Navigation Path (HNP) as a new resource to exploit these hyperlinks for improved web search. HNP is composed of multi-step hyperlinks in visitors ’ website navigation. It provides indications of the content of the <b>destination</b> <b>page.</b> The HierPathExt algorithm is given to extract HNPs in local websites. Then, the PathRank algorithm is created to use HNPs for web page retrieval. The experiments show that our approach results in significant improvements over existing solutions...|$|E
40|$|Search trails mined from browser or toolbar logs {{comprise}} queries and the post-query {{pages that}} users visit. Implicit endorsements from many trails {{can be useful}} for search result ranking, where {{the presence of a}} page on a trail increases its query relevance. Following a search trail requires user effort, yet {{little is known about the}} benefit that users obtain from this activity versus, say, sticking with the clicked search result or jumping directly to the <b>destination</b> <b>page</b> at the end of the trail. In this paper, we present a logbased study estimating the user value of trail following. We compare the relevance, topic coverage, topic diversity, novelty, and utility of full trails over that provided by sub-trails, trail origins (landing pages), and trail destinations (pages where trails end). Our findings demonstrate significant value to users in following trails, especially for certain query types. The findings have implications for the design of search systems, including trail recommendation systems that display trails on search result pages...|$|E
30|$|This work {{proposed}} a hybrid focused crawler capable of seamlessly following hyperlinks pointing to resources hosted on the Surface Web and several darknets of the Dark Web (i.e., Tor, I 2 P, and Freenet) during a single crawl. It employs a classifier-guided approach for selecting the hyperlinks to follow which combines three classifiers: (i) a link-based classifier exploiting the local context of hyperlinks in their parent page, (ii) a parent page classifier {{taking advantage of}} the actual textual content of the parent page of a hyperlink, and (iii) a <b>destination</b> <b>page</b> classifier exploiting the global context of the page a hyperlink points to. These three classifiers are employed by the 11 hyperlink selection methods supported by the link selection component of the focused crawler, including the novel dynamic linear combination approach proposed. Each method employs a different combination of the available classifiers, where the link-based classifier is accompanied by the parent or the destination Web page classifier, depending on conditions related to the destination network and/or the existence of strong evidence within the local context of a hyperlink.|$|E
40|$|The main {{objective}} {{of this paper is}} to optimize the Web-surfing, i. e., to find the optimal path set leading to a target page in a specific Web site with links to other domains. The frequency of accessing various links (access rate), and the time taken to retrieve target pages (retrieval rate) are considered as decisive factors. The access and the retrieval rate are affected by many factors like availability of channels, server capability, the accessing time etc., and are changing periodically. The expected access rate and the required retrieval rate are expressed as fuzzy sets. All paths are identified between the source and <b>destination</b> <b>pages.</b> Users' opinions are collected on each path that leads to target page according to their experience while surfing with regard to access rate and retrieval rate, and a fuzzy opinion matrix is formulated. From the overall fuzzy opinion on each link in a path, a fuzzy Hurwicz opinion set is derived using an optimism-pessimism index. The fuzzy distance between this opinion set and the actual requirement set is estimated. The path that yields minimum fuzzy distance is considered as the optimal path from the source to the destination...|$|R
40|$|This {{study is}} based upon a sample of 517 {{international}} visitors to New Zealand. It suggests that Internet usage {{is based upon}} perceived usefulness and ease of system use. Of additional importance is user confidence about system security. Functional issues of purchasing seem to take priority over information search as users become more familiar with web pages. An additional finding is that entertainment needs are arguably better met by other Internet functions than those provided by <b>destination</b> web <b>pages.</b> Novelty on the Net is perhaps sought from specific sources, and not from the Net generally. Implications are discussed for both measurement and marketing...|$|R
40|$|In many {{applications}} one {{is interested in}} how often two or more objects of interest co-occur. For example, consider a popular web site, which logs all incoming traffic to its site {{in the form of}} weblogs. Weblogs typically record the source and <b>destination</b> <b>pages</b> requested by some user, as well as the time, return code whether the request was successful or not, and so on. Given such weblogs, one might be interestedin finding if there are sets of web pages that many users tend to browse wheneverthey visit the web site. Such “frequent ” sets of web pages give clues totheuserbrowsing behavior and can be used for improving the user’s browsing experience. The quest to mine frequent patterns appears in many other domains. Theprototypical application is market basket analysis, i. e., to mine the sets of items that are frequent bought together, at a supermarket by analyzing the customer shopping carts (the so-called “market baskets”). Once we mine the frequent sets, they allow us to extract association rules among the item sets, where we make some statement about how likely are two sets of items to co-occur or to conditionally occur. For example, in the weblog scenario frequent sets allow us to extract rules like, “Users who visit the sets of pages main, laptops and rebates also visit the pages shopping-cart and checkout”, indicating, perhaps, that the special rebate offer is resulting in more laptop sales. In the case of market baskets, we can find rules like, “Customers who buy Milk and Cereal also tend to buy Bananas”, which may prompt a grocery store to co-locate bananas in the cereal aisle...|$|R
