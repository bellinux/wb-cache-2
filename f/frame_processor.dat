7|24|Public
50|$|Thus, frames with {{detected}} errors {{would be}} essentially unusable {{even if they}} were not deleted by the <b>frame</b> <b>processor.</b>|$|E
5000|$|A {{frame is}} a recipe for [...] "cooking up" [...] a (program) text. Its {{instructions}} say how to blend its ingredients - chunks of frame-text within itself - with the ingredients from other frames. The “chef” is a <b>frame</b> <b>processor</b> that carries out the instructions, i.e. the frame commands, which alter (add, modify, delete) ingredients as necessary, to suit the main recipe.|$|E
5000|$|Formally, a {{frame is}} a {{procedural}} macro consisting of frame-text - zero or more lines of ordinary (program) text and frame commands (that {{are carried out}} by FT’s <b>frame</b> <b>processor</b> as it manufactures custom programs). Each frame is both a generic component in a hierarchy of nested subassemblies, and a procedure for integrating itself with its subassembly frames (a recursive process that resolves integration conflicts in favor of higher level subassemblies). The outputs are custom documents, typically compilable source modules.|$|E
5000|$|... {{fixed point}} numbers are {{sometimes}} used for storing and manipulating images and video <b>frames.</b> <b>Processors</b> with SIMD units aimed at image processing may include instructions suitable for handling packed fixed point data.|$|R
5000|$|The FRP — The <b>Frame</b> Relay <b>Processor</b> — Added <b>Frame</b> Relay to the IPX (implemented the SAR {{function}} in Motorola 56000 DSP's) ...|$|R
5000|$|... #Caption: IBM 701 <b>processor</b> <b>frame,</b> showing 1071 of {{the vacuum}} tubes ...|$|R
5000|$|Context scoping is {{what distinguishes}} FT from other {{modeling}} and construction systems: Each frame constitutes the context into which it integrates its subassembly. In nested subassemblies {{the lower levels}} are progressively more context-free because they integrate less information. Integration conflicts are resolved {{in favor of the}} most context-sensitive frame to assign or insert a parameter - it becomes read-only to all other frames in that frame’s subassembly. [...] In figure 1, frames F and C would conflict if they assign different values to parameter p. So F overrides C - i.e., the <b>frame</b> <b>processor</b> ignores C’s assignment(s) to p, and uses F’s value(s) for p in F and C. Similarly, J can override both F and C, and so on.|$|E
30|$|The passive {{measurement}} method is employed to count network traffic. The MAC RxQ and TxQ {{are part of}} a <b>frame</b> <b>processor,</b> which receives and transmits the data frames from the Ethernet interface GigE Rx and Tx, respectively. The network traffic stored and sent according to its Ethernet interface and the frame forwarding delay are also counted by it.|$|E
40|$|Software {{development}} {{today has}} to meet various demands, such as reducing cost, effort, and time-to-market, increasing quality, handling complexity and product size, or satisfying {{the needs of}} individual customers. This is why many software organizations today do not just develop and maintain single, separate software systems, but a set of systems, a product line. Given {{the fact that the}} systems are typically in the same application domain, it would be inefficient to develop them independently of each other in an ad-hoc way, as this usually leads to high development and maintenance effort. However, a means for systematically developing and maintaining similar products is offered by software product line engineering, as covered by the PuLSETM approach developed at the Fraunhofer IESE. To implement such a product line approach in practice, special technologies are required for effectively identifying reusable artifacts, as well as capturing and controlling their commonalities and variabilities. To do the latter efficiently, the PoLITe project (Product Line Implementation Technologies) explores techniques at the implementation level for managing variability. Frame technology is one of these implementation techniques that, as this report shows, offers advantages for implementing product lines over object-oriented techniques. This report highlights reuse principles and explains the major concepts underlying frame technology. The core concepts are extracted and realized in a plain <b>frame</b> <b>processor.</b> This tool will be used in the following case study, where the evolution of a software system from a single system into a product line will be discussed, each time comparing the frame approach with a conventional object-oriented solution...|$|E
5000|$|... #Caption: PDP-1 Type 30 point-mode CRT {{display and}} console typewriter, with <b>processor</b> <b>frame</b> in background.|$|R
5000|$|As an {{outgrowth}} of his work in neurophysiology, while still working as a post-doctoral fellow and {{an assistant professor of}} neurology, Sgro founded Alacron, Inc. (formerly Corteks, Inc.until 1990) in 1985 to manufacture technologies relevant to his neurological research. In 1989 he commercialized this technology and began developing array <b>processors,</b> <b>frame</b> grabbers, vision <b>processors,</b> and most recently supported advances in BSI sensor technology. Extending his work in machine vision technology, in 2002, Sgro founded FastVision, LLC, a maker of smart cameras, as a subsidiary of Alacron, Inc [...]|$|R
50|$|A {{specification}} frame (SPC) is {{an entire}} assembly’s topmost, hence most context-sensitive <b>frame.</b> The <b>processor</b> starts at an SPC, such as L or M in figure 1, {{in order to}} manufacture a complete program or subsystem. While in principle an SPC could customize every detail, in practice an SPC is {{a small fraction of}} its entire assembly because most of the exceptions (and exceptions to exceptions, etc.) have already been handled by various subassembly frames.|$|R
40|$|The {{purpose of}} this study is to sort peanut kernels by machine vision insted of eyes. The main equiment of the sorting machine are two CCD cameras,a color frame grabber, a high speed <b>frame</b> <b>processor,</b> and a microcomputer. Furthermore,the {{technology}} of microprocessor interface is used to control the image grabbing time and the air blowing time. In this study,image of the stationary samples were analyzed first to establish the methods to remove bad kernels. then, the images of the moving samples were analyzed to investigate the difference between the thresolds of the static and dynamic sorting test. Finally, dynamic sorting test was conducted by using two cameras to grab two different sides of the sample and analyzing these two images immediately. The results of the stationary images analyses reveal that the damage ratio can be used as the sorting parameter for the seriously abnormal- colored kernels; the damage ratio and the average hue have to be used as the sorting parameters for slightly abnormal-colored kernels; the average first difference can be used to remove the shrunk kernels;the compactness can be used to remove the kernels with sprout; and the average hue and the damage ratio are needed to sort out the broken samples. In dynamic sorting test, the sorting parameters used were the same as those used in static sorting test; however, the shrunk samples and samples with sprout were not included in dynamic test. In dynamic test, it took about 0. 44 seconds to sort a sample and the sorting accuracy rate is 94. 8 %。本研究使用機器視覺代替人眼進行落花生仁選別。選別機之主要設備包括 兩部攝影機、影像擷取卡、影像處理加速卡以及微電腦；並以微處理器介 面技術控制取像及吹氣時間。研究過程是，首先分析靜態落花生仁影像， 建立去除不良落花生之方法。然後分析動態影像，探討靜態與動態選別分 劃值之差異。最後使用兩部攝影機擷取與分析落花生仁不同兩面之影像， 進行動態選別試驗。由靜態影像分析之結果得知，重度顏色不正常落花生 仁可以損壞比率值做為選別參數；輕微顏色不正常落花生仁可以損壞比率 值及平均色相值二條件做為選別參數；皺皮落花生仁可以平均一次差分值 做為選別參數；發芽落花生仁可以細密度值做為選別參數；破粒落花生仁 可以平均色相值及損壞比率值二條件做為選別參數。動態選別時，不良落 花生仁之判別方法與靜態選別者相同，惟動態選別時之樣品不包括皺皮及 發芽落花生仁。每一顆落花生仁動態選別所需之時間為 0. 44 秒；動態選別 結果，選別準確率為 94. 8 %...|$|E
40|$|We {{consider}} a multiprocessor graphics architecture object-parallel if graphics primitives {{are assigned to}} processors without regard to screen location, and if each processor completely renders the primitives it is assigned. Such an approach leads to the following problem: the images rendered by all processors must be merged, or composited, {{before they can be}} displayed. At worst, the number of pixels that must be merged is a <b>frame</b> per <b>processor.</b> Perhaps there is a more parsimonious approach to pixel merging in object-parallel architectures than merging a full <b>frame</b> from each <b>processor.</b> In this paper we analyze the number of pixels that must be merged in object-parallel architectures. Our analysis is from the perspective that the number of pixels to be merged {{is a function of the}} depth complexity of the graphics scene to be rendered, and a function of the depth complexity of each processor's subset of the scene to be rendered. We derive a model of depth complexity of graphics scenes ren [...] ...|$|R
50|$|Three former SGI {{employees}} had founded 3dfx in 1994. Their Voodoo Graphics extension card relied on PCI to provide cheap 3D graphics for PC's. Towards {{the end of}} 1996, the cost of EDO DRAM dropped significantly. A card consisted of a DAC, a <b>frame</b> buffer <b>processor</b> and a texture mapping unit, along with 4 MB of EDO DRAM. The RAM and graphics processors operated at 50 MHz. It provided only 3D acceleration and as such the computer also needed a traditional video controller for conventional 2D software.|$|R
50|$|In {{addition}} to those science and engineering types of cameras, an entire industry has been built up around industrial machine vision systems and requirements. The major application has been for high-speed manufacturing. A system typically consists of a camera, a <b>frame</b> grabber, a <b>processor,</b> and communications and recording systems to document or control the manufacturing process.|$|R
50|$|However, {{conversion}} to 3D has problems. Information is unavailable as 2D doesn't have information for a perspective view. Some TVs have a 3D engine to convert 2D content to 3D. Usually, on high frame rate content(and on some slower <b>processors</b> even normal <b>frame</b> rate) the <b>processor</b> isn't fast enough and lag is possible. This {{can lead to}} strange visual effects.|$|R
5000|$|A typical Voodoo Graphics PCI {{expansion}} card {{consisted of a}} DAC, a <b>frame</b> buffer <b>processor</b> and a texture mapping unit, along with 4 MB of EDO DRAM. The RAM and graphics processors operated at 50 MHz. It provided only 3D acceleration and as such the computer also needed a traditional video controller for conventional 2D software. A pass-through VGA cable daisy-chained the video controller to the Voodoo, which was itself connected to the monitor. The method used to engage the Voodoo's output circuitry varied between cards, with some using mechanical relays while others utilized purely electronic components. The mechanical relays emitted an audible [...] "clicking" [...] sound when they engaged and disengaged.|$|R
40|$|International Telemetering Conference Proceedings / October 14 - 16, 1980 / Bahia Hotel, San Diego, CaliforniaThe Advanced Onboard Signal Processor (AOSP) in a {{distributed}} {{signal processing}} computer under development for space radar, electro-optic and communications {{applications in the}} post- 1985 time <b>frame.</b> The <b>processor</b> architecture {{is based on an}} arbitrary-topology network of identical processing elements specialized to perform signal processing and controlled by a distributed operating system. Both the operating system and applications programs are written in a high order language which is efficiently supported by the processing elements. Examples of communication signal processing are presented which show the suitability of AOSP for this application. The design has been validated by extensive simulation and is presently in the breadboard hardware phase...|$|R
50|$|Akeley {{developed}} the <b>frame</b> buffers and <b>processor</b> subsystems {{for the early}} SGI IRIS series products {{and many of the}} CAD tools used to design these and other products. Akeley was instrumental in developing the graphics systems for the Power Series and Onyx systems, including the GTX, the VGX, and the RealityEngine. Akeley also led the design and documentation of the OpenGL graphics software specification, which was supported by Silicon Graphics and many other workstation and personal computer vendors.|$|R
40|$|A smart {{navigation}} system (an Electronic Travel Aid) {{based on an}} object detection mechanism {{has been designed to}} detect the presence of obstacles that immediately impede the path, by means of real time video processing. In this paper this is discussed, keeping in mind the navigation of the visually impaired. A video camera feeds images of the surroundings to a Da-Vinci Digital Media Processor, DM 642, which works on the video, frame by <b>frame.</b> The <b>processor</b> carries out image processing techniques whose result contains information about the object in terms of image pixels. The algorithm aims to select the object which, among all others, poses maximum threat to the navigation. A database containing a total of three sounds is constructed. Hence, each image translates to a beep, where every beep informs the navigator of the obstacles directly in front of him. This paper implements an algorithm that is more efficient as compared to its predecessors...|$|R
40|$|Abstract—In {{this paper}} we address the mapping of mixed-criticality hard {{real-time}} applications on distributed embedded architectures. We assume that the architecture provides both spa-tial and temporal partitioning, thus enforcing enough separation between applications. With temporal partitioning, each application runs in a separate partition, and each partition is allocated several time slots on the processors where the application is mapped. The sequence of time slots for all the applications on a processor are grouped within a Major Frame, which is repeated periodically. We assume that the applications are scheduled using static-cyclic scheduling. We are interested to determine the task mapping to processors, and the sequence {{and size of the}} time slots within the Major <b>Frame</b> on each <b>processor,</b> such that the applications are schedulable. We have proposed a Tabu Search-based approach to solve this optimization problem. The proposed algorithm has been evaluated using several synthetic and real-life benchmarks. I...|$|R
40|$|Crystallographic {{information}} can be determined for bulk specimens in a SEM by utilizing electron backscatter diffraction (EBSD), which is {{also referred to as}} backscatter electron Kikuchi diffraction. This technique provides similar information to that provided by selected area electron channeling (SAEC). However, the spatial resolutions of the two techniques are limited by different processes. In SAEC patterns, the spatial resolution is limited to {approximately} 2 {mu}m by the motion of the beam on the specimen, which results from the angular rocking of the beam and the aberration of the probe forming lens. Therefore, smaller incident probe sizes provide no improvement in spatial resolution of SAEC patterns. In contrast, the spatial resolution for EBSD, which uses a stationary beam and an area detector, is determined by (1) the incident probe size and (2) the size of the interaction volume from which significant backscattered electrons are produced {{in the direction of the}} EBSD detector. The second factor is influenced by the accelerating voltage, the specimen tilt, and the relative orientation of scattering direction and the specimen tilt axis. This study was performed on a Philips XL 30 /FEG SEM equipped with a TexSEM Orientation Imaging Microscopy (OIM) system. The signal from the EBSD detector (SIT camera) is flat- fielded and enhanced in a MTI <b>frame</b> storage/image <b>processor.</b> The Schottky FEG source provides the fine probe sizes ({approximately} 10 nm) desired with sufficient probe current ({approximately} 1 nA) needed for image processing with the low signal/noise EBSD signal...|$|R
3000|$|At this step, each {{processor}} (x,y) sends {{a packet}} containing its coordinates to the processor (x^',y) except possibly when the processor {{is near the}} left edge of its block. Specifically, for these processors, the pixel (x^',y [...]) may be outside {{the image of the}} block in the previous <b>frame.</b> Thus, these <b>processors</b> (x,y) are forced to send to the processor (x^',y [...]). We should specially treat these processors {{in order to ensure that}} after the end of X-routing, each processor will have received packets originated only from a single block. As will be seen, with this guarantee, the implementation of Y-routing is greatly simplified. Notice also that each processor can easily identify this special case. For instance, a processor (x,y) inside the thick block of Figure  1 a should send the packet to the processor (x^',y), only if x^' < a_i 1 x_ 0 + a_i 2 y_ 0 + a_i 3 x_ 0 y_ 0 + a_i 4. Now, we prove the following Lemma.|$|R
40|$|We {{introduce}} a new SIR particle filter that performs tracking in a joint feature space where pixel domain data are fused with measurements obtained from an 18 -channel modulation domain image model. This dual domain processor is capable of maintaining track lock and delivering a high probability of kill against targets in the AMCOM infrared closure sequences, which are challenging because they are characterized by high closure rate dynamics, poor SNR, and maneuvering targets that typically exhibit severe signature evolution and profound magnification changes. In the setup considered here, the track processor receives an initial target designation, which could be obtained from an external detector or from a human in the loop. After the first <b>frame,</b> the track <b>processor</b> must run autonomously without further a priori information. Compared to traditional pixel domain trackers, our results demonstrate that this new dual domain approach provides inherently improved tracking accuracy and facilitates powerful new consistency checks capable of detecting when a template update is needed due to nonstationary target signature evolution...|$|R
40|$|Abstract—In {{this paper}} we are {{interested}} in mixed-criticality embedded real-time applications mapped on distributed hetero-geneous architectures. The architecture provides both spatial and temporal partitioning, thus enforcing enough separation for the critical applications. With temporal partitioning, each application is allowed to run only within predefined time slots, allocated on each processor. The sequence of time slots for all the applications on a processor are grouped within a Major Frame, which is repeated periodically. We assume that the safety-critical applications (on all criticality levels) are scheduled using static-cyclic scheduling and the non-critical applications are scheduled using fixed-priority preemp-tive scheduling. We consider that each application runs in a separate partition, and each partition is allocated several time slots on the processors where the application is mapped. We are interested to determine the sequence and size of the time slots within the Major <b>Frame</b> on each <b>processor</b> such that both the safety-critical and non-critical applications are schedulable. We have proposed a Simulated Annealing-based approach to solve this optimization problem. The proposed algorithm has been evaluated using several synthetic and real-life benchmarks. Keywords-mixed-criticality; real-time systems; temporal-partitioning I...|$|R
40|$|Video {{encoding}} due to {{its high}} processing requirements has been traditionally done using special-purpose hardware. Software solutions have been explored but {{are considered to be}} feasible only for nonreal-time applications requiring low encoding rates. However, a software solution using a general-purpose computing system has numerous advantages: It is more available and flexible and allows experimenting with and hence improving various components of the encoder. In this paper, we present the performance of a software video encoder with MPEG- 2 quality on various parallel and distributed platforms, The platforms include an Intel Paragon XP/S and an Intel iPSC/ 860 hypercube parallel computer as well as various networked clusters of workstations. Our encoder is portable across these platforms and uses a data-parallel approach in which parallelism is achieved by distributing each <b>frame</b> across the <b>processors.</b> The encoder is useful for both real-time and nonreal-time applications, and its performance scales according to the available number of processors. In addition, the encoder provides control over various parameters such as the size of motion search window, buffer management, and bit rate, The performance results include comparisons of execution times, speedups, and frame encoding rates on various systems...|$|R
40|$|Abstract — Video {{encoding}} due to {{its high}} processing requirements has been traditionally done using special-purpose hardware. Software solutions have been explored but {{are considered to be}} feasible only for nonreal-time applications requiring low encoding rates. However, a software solution using a general-purpose computing system has numerous advantages: It is more available and flexible and allows experimenting with and hence improving various components of the encoder. In this paper, we present the performance of a software video encoder with MPEG- 2 quality on various parallel and distributed platforms. The platforms include an Intel Paragon XP/S and an Intel iPSC/ 860 hypercube parallel computer as well as various networked clusters of workstations. Our encoder is portable across these platforms and uses a data-parallel approach in which parallelism is achieved by distributing each <b>frame</b> across the <b>processors.</b> The encoder is useful for both real-time and nonreal-time applications, and its performance scales according to the available number of processors. In addition, the encoder provides control over various parameters such as the size of motion search window, buffer management, and bit rate. The performance results include comparisons of execution times, speedups, and frame encoding rates on various systems. Index Terms—Motion estimation, MPEG- 2, network of workstations, parallel and distributed systems, software, video coding. I...|$|R
40|$|Using {{computer}} generated imaging {{is becoming more}} and more popular in areas such as computer gaming, movie industry and simulation. A familiar scene in the winter months for most us in the Nordic countries is snow. This thesis discusses some of the complex numerical algorithms behind snow simulations. Previous methods for snow simulation have either covered only a very limited aspect of snow, or have been unsuitable for real-time performance. In this thesis, some of these methods are combined into a model for real-time snow simulation that handles both snowflake motion through the air, wind simulation, and accumulation of snow on objects and the ground. With a goal towards achieving real-time performance with more than 25 frames per second, some new parallel methods for the snow model are introduced. Focus is set on efficient parallelization on new SMP and multi-core computer systems. The algorithms are first parallelized in a pure data-parallel manner by dividing the data structures among threads. This scheme is then improved by overlapping inherently sequential algorithms with computations for the following <b>frame,</b> to eliminate <b>processor</b> idle time. A speedup of 1. 9 on modern dual CPU workstations is achieved, while displaying a visually satisfying result in real-time. By utilizing Hyper-Threading enabled dual CPU systems, the speedup is further improved to 2. 0. </p...|$|R
5000|$|Framework, {{launched}} in 1984, {{was the first}} office suite to run on the PC 8086 with the MS-DOS operating system. ValDocs, an even earlier integrated suite, actually comparable to the original Macintosh of 1984 and Apple Lisa of 1982 was produced by Epson, a complete integrated work station based on the previous Zilog Z80 processor and CP/M operating system with GUI and [...] "WYSIWYG" [...] typography on the monitor and printing. Framework offered all this however in the first all-in-one package to run on any PC platform. It was preceded by a few months by its close rival Lotus Symphony. Unlike other integrated products Framework was not created as [...] "plug-in" [...] modules with similar look and feel but as a single windowing workspace representing a desktop metaphor that could manage and outline [...] "Frames" [...] sharing a common underlying format. The initial release included about {{a dozen or so}} frame types (identified by a FRED function, @frametype). Frame types included containers which could be filled up with other frames, empty frames which could become other type of frames based on user input, formulas embedded in them or program output targeting them, word <b>processor</b> <b>frames,</b> flat-database frames and spreadsheet as well as graphic frames. Later versions included a frame type that can hold compiled executable code and the current version include an external type handled by separate applications running on the host operating system.|$|R
40|$|Video-Based Sensor for Tracking 3 -Dimensional Targets The National Aeronautics and Space Administration's (NASAs) Marshall Space Flight Center (MSFC) {{has been}} {{developing}} and testing video-based sensors for automated spacecraft guidance for several years, and {{the next generation of}} video sensor will have tracking rates up to 100 Hz and will be able to track multiple reflectors and targets. The Video Guidance Sensor (VGS) developed {{over the past several years}} has performed well in testing and met the objective of being used as the terminal guidance sensor for an automated rendezvous and capture system. The first VGS was successfully tested in closed-loop 3 -degree-of-freedom (3 - DOF) tests in 1989 and then in 6 -DOF open-loop tests in 1992 and closed-loop tests in 1993 - 4. Development and testing continued, and in 1995 approval was given to test the VGS in an experiment on the Space Shuttle. The VGS flew in 1997 and in 1998, performing well for both flights. During the development and testing before, during, and after the flight experiments, numerous areas for improvement were found. The VGS was developed with a sensor head and an electronics box, connected by cables. The VGS was used in conjunction with a target that had wavelength-filtered retro-reflectors in a specific pattern, The sensor head contained the laser diodes, video camera, and heaters and coolers. The electronics box contained a <b>frame</b> grabber, image <b>processor,</b> the electronics to control the components in the sensor head, the communications electronics, and the power supply. The system works by sequentially firing two different wavelengths of laser diodes at the target and processing the two images. Since the target only reflects one wavelength, it shows up well in one image and not at all in the other. Because the target's dimensions are known, the relative positions and attitudes of the target and the sensor can be computed from the spots reflected from the target. The system was designed to work from I meter out to I 10 meters. The VGS was mounted on the Space Shuttle while its target was mounted on the Spartan free-flyer carried on the same Shuttle flight. The VGS tracked the Spartan at ranges up to 170 m, and the VGS range data very closely matched the range data from the Hand-Held Laser- Rangefinder used by the astronauts on board the Shuttle. While the VGS was designed primarily as a terminal guidance sensor for an automated spacecraft, it could be applied to other uses. It could be used as an alignment aid for an operator of a remote system (giving position and attitude feedback data, as well as a camera view of the target), as a feedback system for a robotic arm, or for automated vehicle guidance. The next generation VGS, with its higher tracking rates, smaller size, and lower power could be used in more places than the original VGS, and by using LED's instead of laser diodes, the system would be eye-safe at any range. Other possible uses include tracking 3 -dimensional objects with retro-reflectors mounted at various locations or motion analysis by placing several retro-reflectors on the moving object and tracking them at high speeds. There are few sensors capable of performing tasks similar to those the VGS can perform, and the next generation VGS will be even more capable than the original. Some of this work is previously presented in the papers...|$|R

