0|1603|Public
40|$|The {{prototype}} {{first in}} <b>first</b> out (FIFO) <b>input</b> <b>buffer</b> {{asynchronous transfer mode}} (ATM) switch {{is known as the}} architecture on which the transfer capacity is considerably limited, and is necessary to be given some improvement. In this paper, a novel parallel read-out structure is introduced to the <b>input</b> <b>buffer</b> ATM switch design to remove such the above limitation. Transfer performances of cell traffics on the novel switch with the three control rules of a cyclic read-out (CRO) cell selection, a parallel read-out random (PROR) cell selection, and a parallel read-out maximum (PROM) queue selection are evaluated by simulations. The simulation results show that the introduced parallel read-out structure gives a drastic buffer size reduction. Especially, PROR and PROM architectures totally offer large amounts of hardware reductions though small hardware increments may be required for paralleling. Consequently it is shown that PROR and PROM <b>input</b> <b>buffer</b> switches give loss performances lower than that of the corresponding output buffer switches with the same level transfer delays...|$|R
40|$|SUMMARY In {{this paper}} {{a fair and}} fast buffer sharing scheme based on the round robin cell {{distribution}} mechanism for an <b>input</b> <b>buffer</b> ATM switch is proposed. This buffer sharing scheme improves {{the performance of the}} <b>input</b> <b>buffer</b> ATM switch in terms of the cell loss ratio and the CDV by equalizing the occupancy of <b>input</b> <b>buffers.</b> In order to make the equalizing and buffer sharing operation fast and simple, the proposed buffer sharing scheme uses a dynamic threshold by which <b>input</b> <b>buffers</b> are separated into two groups according to their occupancy. The buffer sharing scheme equalizes the <b>buffer</b> occupancy of <b>input</b> <b>buffers</b> by distributing incoming cells to the <b>input</b> <b>buffers</b> of lower occupancy first. The performance of the proposed buffer sharing scheme is evaluated by computer simulation. The result shows that the <b>input</b> <b>buffer</b> ATM switch adopting the buffer sharing scheme outperforms those with conventional <b>input</b> <b>buffer</b> schemes based on virtual output queue (VOQ) and the simple round robin in terms of the cell loss ratio, the maximum throughput and the CDV. 1...|$|R
30|$|The LQF scheme {{needs to}} compare the <b>input</b> <b>buffer</b> {{occupancy}} values. Other simpler scheduling schemes can be employed to reduce the computational and hardware complexity of the scheduler. One possible scheduling scheme is to randomly select the <b>input</b> <b>buffer,</b> which is named the RDM scheme. Another scheduling scheme is to group the parallel <b>input</b> <b>buffers</b> and decoders, such that each decoder can only be scheduled to the <b>input</b> <b>buffers</b> within the same group. The decoders in the same group are scheduled according to the LQF scheme. This {{is known as the}} static scheduling scheme or the STC scheme. In this article, each group is assumed to have two <b>input</b> <b>buffers</b> and two UFA decoders. Compared to the LQF scheme, the STC scheme can help reducing the need for multi-port memories and high fan-out multiplexers. It can also simplify the design of the scheduler and the connections between the <b>input</b> <b>buffers</b> and the decoders.|$|R
40|$|Abstract- In NoC (Network on chip), {{the power}} {{consumption}} is major issue while designing. In NoC router, conventional <b>input</b> <b>buffer</b> consume more energy. The paper {{is focused on}} the energy-efficient design of 8 -bit <b>input</b> <b>buffers,</b> which use the characteristic of transmission of data on NoC that has probability of transmitting a zero signal is more than that of transmitting a signal one. The design of energy efficient 8 -bit <b>input</b> <b>buffer</b> reduces the power consumption by 48 % and the chip area 29 % with calculating the gates count as compared with conventional 8 -bit <b>input</b> <b>buffer</b> by using 65 nm cmos technology in simulation...|$|R
5000|$|Peekable reads (read without {{removing}} from pipe's <b>input</b> <b>buffer)</b> ...|$|R
5000|$|Read {{the first}} 10 MB (= 100MB / (9 chunks + 1)) of each sorted chunk into <b>input</b> <b>buffers</b> in main memory and {{allocate}} the remaining 10 MB for an output buffer. (In practice, it might provide better performance {{to make the}} output buffer larger and the <b>input</b> <b>buffers</b> slightly smaller.) ...|$|R
3000|$|When a decoder {{finishes}} decoding a codeword, the scheduler {{needs to}} decide which <b>input</b> <b>buffer</b> the decoder should serve next. It has been discussed in [18 – 20] that serving the longest queue first (LQF) can help making the parallel queues (or <b>input</b> <b>buffers)</b> the most balanced or stable, thus maximising the input data rate R [...]...|$|R
40|$|The {{bidirectional}} Fano algorithm (BFA) {{can achieve}} {{at least two}} times decoding throughput compared to the conventional unidirectional Fano algorithm (UFA). In this paper, bidirectional Fano decoding is examined from the queuing theory perspective. A Discrete Time Markov Chain (DTMC) is employed to model the BFA decoder with a finite <b>input</b> <b>buffer.</b> The relationship between the input data rate, the <b>input</b> <b>buffer</b> size and the clock speed of the BFA decoder is established. The DTMC based modelling {{can be used in}} designing a high throughput parallel BFA decoding system. It is shown that there is a tradeoff between the number of BFA decoders and the <b>input</b> <b>buffer</b> size, and an optimal <b>input</b> <b>buffer</b> size can be chosen to minimize the hardware complexity for a target decoding throughput in designing a high throughput parallel BFA decoding system. Comment: 5 pages, 10 figures, accepted by GLOBECOM 2010, Miami, FL, US...|$|R
30|$|It {{has been}} found that the {{proposed}} DTMC based modeling on the PUFAS-LQF and PBFAS-LQF is ideal when the <b>input</b> <b>buffer</b> size B is large enough (i.e., B ≥ 5). The accuracy of the model degrades as B gets smaller. However, a very short <b>input</b> <b>buffer</b> will not be adopted according to the trade-off between area and decoding throughput as discussed in [21]. Additionally, it has also been found that the accuracy of the model does not depend on the relationship between M and N (i.e., M > N, M = N or M < N) as long as the <b>input</b> <b>buffer</b> size is large enough.|$|R
30|$|The prolog {{consists}} of initialization of local variables, pushing registers over stack for usage inside the function, loading taps coefficients h[n] from memory into registers and defining <b>input</b> <b>buffer</b> as circular. Defining the <b>input</b> as circular <b>buffer</b> removes the overhead {{of an additional}} branch instruction inside the loop. The use of circular buffer prevents the constant test of wrapping. The prolog is to be executed once for L size <b>input</b> <b>buffer.</b> It takes 45 cycles to execute this code.|$|R
3000|$|..., and in {{case there}} is still any {{unassigned}} MRU, these unallocated resources {{should be used to}} flush the <b>input</b> <b>buffers.</b> Since the minimum requirements for the SF have been already allocated, the spectral efficiency can be maximized by transmitting the data from those SFs associated to the best channel conditions. Considering that the status of the <b>input</b> <b>buffers</b> has been updated according to R [...]...|$|R
40|$|The <b>input</b> <b>buffer</b> in Network on Chip (NoC) router plays a {{key role}} in on-chip-network performance, which is {{utilized}} in flow control and virtual channel. However, increase in area and power due to <b>input</b> <b>buffers</b> as the network size gets larger is becoming severe. To solve this problem, a bufferless deflection routing without <b>input</b> <b>buffer</b> was suggested. Since the bufferless deflection routing shows poor performance at high network load, other approaches which combine the deflection routing with small size side buffers were also proposed. Nonetheless these new methods still show deficiencies caused by frequent path collisions. In this paper, we propose a modified deflection routing technique using a location based priority. In comparison wit...|$|R
5000|$|... 1. IBF (<b>Input</b> <b>Buffer</b> Full) - It is an output {{indicating}} that the input latch contains information.|$|R
5000|$|The minimum {{buffer size}} {{for each of}} the 63 {{possible}} services (Service <b>Input</b> <b>Buffers)</b> is 128 bytes.|$|R
40|$|Abstract. In {{memory device}} that is {{contained}} in the digital application, there is a sequence of <b>input</b> <b>buffer.</b> The <b>input</b> <b>buffer’s</b> function is to improve a digital signal and remove noise. The buffer circuit take these input signal with imperfections and convert them in to full digital logic levels by slicing the signals at correct levels which depends upon the switching point voltage. In this paper,using three topologies, that are NMOS, PMOS and Parallel <b>input</b> <b>buffer.</b> It would be present into design, simulation and analysis of all topologies <b>input</b> <b>buffer.</b> The result in this paper {{to determine the best}} of the three topologies to used. The delay time used to determine the best of topologies. Mentor graphic is tools which used in this paper to design and simulation. The technology used in this paper is 0. 35 µm CMOS Technology. Analysis of comparison all of topologies used in this paper based on six parameters. The result of comparison analysis can be seen in more details in this explanation...|$|R
40|$|This paper {{presents}} a simulation study of an <b>input</b> <b>buffered</b> Asynchronous Transfer Mode (ATM) switch running the Request-Grant-Status (RGS) iterative scheduling algorithm. The RGS algorithm {{can reduce the}} effects of head-of-line blocking on the throughput and delay performance of an <b>input</b> <b>buffered</b> ATM switch. During a cell slot interval, the RGS algorithm performs a fixed number (n) of iterations. At each iteration step, the RGS algorithm determines a match between buffered cells and corresponding idle output ports. After the last iteration step, all matched cells are routed through the switch fabric. The simulation {{results show that the}} RGS algorithm can significantly improve the throughput and delay performance of an <b>input</b> <b>buffered</b> ATM switch under a high load. When independent and identical Bernoulli traffic sources are used, the input queues of a 16 x 16 ATM switch, using a strict first-in first-out queueing discipline, saturate at an offered load of approximately 60 %. When the RGS algorithm is used with 5 iterations, the input queues of a 16 x 16 <b>input</b> <b>buffered</b> switch, under the same traffic conditions, d...|$|R
40|$|An <b>input</b> <b>buffered</b> {{packet switch}} called the odd-even {{multicast}} switch is proposed. The packet splitting probability {{of the proposed}} switch is derived and the packet output contention is resolved using the cyclic-priority reservation (CPR) algorithm. The throughput and mean packet delay of the proposed switch are compared with a simple <b>input</b> <b>buffered</b> switch. It is found that the proposed switch gives a significant performance improvement {{at the expense of}} extra packet splitting overhead. link_to_subscribed_fulltex...|$|R
40|$|To {{support the}} Internet's {{explosive}} growth and expansion into a true integrated services network, {{there is a}} need for cost-effective switching technologies that can simultaneously provide high capacity switching and advanced QoS. Unfortunately, these two goals are largely believed to be contradictory in nature. To support QoS, sophisticated packet scheduling algorithms, such as Fair Queueing, are needed to manage queueing points. However, the bulk of current research in packet scheduling algorithms assumes an output buffered switch architecture, whereas most high performance switches (both commercial and research) are <b>input</b> <b>buffered.</b> While output buffered systems may have the desired quality of service, they lack the necessary scalability. <b>Input</b> <b>buffered</b> systems, while scalable, lack the necessary quality of service features. In this paper, we propose the construction of switching systems that are both <b>input</b> and output <b>buffered,</b> with the scalability of <b>input</b> <b>buffered</b> switches and the r [...] ...|$|R
30|$|Mposva[*]=[*]Gffvm, Eq. (59). As {{mentioned}} before, the m vector {{is already}} {{present in the}} <b>input</b> <b>buffer</b> of the MACx. Hence, a RAM port is required to load the rows of Gffv into the other MACx <b>input</b> <b>buffer.</b> This step continues until all the rows of Gffv have been operated on. Once the MACx operations are completed, the output module sends {{a signal to the}} M module. The M module uses this value to build the M constraint vector.|$|R
50|$|In Unix the {{end-of-file}} character (by default EOT) {{causes the}} terminal driver {{to make available}} all characters in its <b>input</b> <b>buffer</b> immediately; normally the driver would collect characters until it sees an end-of-line character. If the <b>input</b> <b>buffer</b> is empty (because no characters have been typed since the last end-of-line or end-of-file), a program reading from the terminal reads a count of zero bytes. In Unix, such a condition is understood as having {{reached the end of}} the file.|$|R
40|$|Abstract—With the {{emergence}} of on-chip networks, router buffer power has become a primary concern. Elastic buffer (EB) flow control utilizes existing pipeline flip-flops in the channels to implement distributed FIFOs, {{eliminating the need for}} <b>input</b> <b>buffers</b> at the routers. EB routers {{have been shown to be}} more efficient than virtual channel routers, as they do not require <b>input</b> <b>buffers</b> or complex logic for managing virtual channels and tracking credits. Wormhole routers are more comparable in terms of complexity because they also lack virtual channels. This paper compares EB and wormhole routers and explores novel hybrid designs to more closely examine the effect of design simplicity and <b>input</b> <b>buffer</b> cost. Our results show that EB routers have up to 25 percent smaller cycle time compared to wormhole and hybrid routers. Moreover, EB flow control requires 10 percent less energy to transfer a single bit through a router and offers three percent more throughput per unit energy as well as 62 percent more throughput per unit area. The main contributor to these results is the cost and delay overhead of the <b>input</b> <b>buffer.</b> Index Terms—On-chip interconnection networks, interconnection architectures. ...|$|R
50|$|During the 1970s, Stuart {{created a}} version of the {{programming}} language FORTH, which he called LaFORTH and is notable for its implementation without an <b>input</b> <b>buffer.</b>|$|R
30|$|E 2 [*]=[*]E 2 am, Eq. (46). The m vector {{is loaded}} into one MACx <b>input</b> <b>buffer</b> using a ROM port. Simultaneously, E 2 a is completed, and step 5 is being executed. Then, E 2 a is {{loaded into the}} other <b>input</b> <b>buffer</b> using a RAM-low port. Once the MACx {{operation}} is completed, the output module sends {{a signal to the}} E module and the E module accesses the MACx output register to add this value to E 5 [*]+[*]E 1 [*]+[*]E 3.|$|R
40|$|Abstract. <b>Input</b> <b>Buffered</b> {{switches}} can {{not directly}} deal with variable size packets. The variable length packets {{need to be}} segmented into fixed length cells before scheduling. Then segmentation and reassembly (SAR) must be used. The traditional SAR scheme can lead to significant loss of fabric bandwidth due to the padding bytes, which requires speed up in switch fabric to compensate for this bandwidth loss. The improved scheme called cell merging can reduce the bandwidth loss greatly but it has low scalability {{and it is difficult}} to select suitable merging size. In this paper, we propose a new method of SAR for <b>Input</b> <b>buffered</b> switches using the queue information of switches and can adjust the segment size dynamically. New scheme is suitable for different traffic model and can provide an excellent delay performance. We evaluate DSAR scheme using simulation, the results show that it outperforms existing segmentation schemes in <b>Input</b> <b>Buffered</b> switches...|$|R
30|$|F_ 2 c=F_ 2 aÃ, Eq. (58). Once {{the input}} module {{receives}} {{a signal that}} step 9 is completed, the F 2 a vector is loaded into one <b>input</b> <b>buffer</b> and the <b>first</b> column of Ã is loaded into the other <b>input</b> <b>buffer</b> of MACx. This step continues until the three columns of Ã have been loaded into the MACx. Once the computations for F 2 c are completed, this value is stored in the memory and a Done signal is set to indicate the completion of this step.|$|R
40|$|An {{experimental}} {{study was conducted}} using a network simulator to investigate the performance of packet communication networks as a function of: the network resource capacities (channels, buffers), the network load (number of virtual channels, virtual channel loads), protocols (flow control, congestion control, routing) and protocol parameters (virtual channel window size, <b>input</b> <b>buffer</b> limits). Performance characteristics are shown and the design of <b>input</b> <b>buffer</b> limits for network congestion control, virtual channel window size and nodal buffer capacity addressed. Network design strategies for the control of load fluctuations are proposed and discussed. 1...|$|R
40|$|This paper {{presents}} a high bandwidth transimpedance amplifier (TIA) {{used as a}} high speed <b>input</b> <b>buffer</b> for next generation serial electrical communication. The <b>input</b> <b>buffer</b> presented in this work is a linear broadband low noise amplifier. It is demonstrated {{that it can be}} used for receiving baseband modulated electrical communication in different modulation formats (NRZ, PAM 4, duobinary, [...] .) for bit rates up to 80 Gbit/s. With good impedance matching up to 50 GHz, an area of 0. 18 mm 2 and a power consumption of 80 mW from a 2. 5 V power supply...|$|R
3000|$|... {{finishes}} decoding a codeword, it can {{be scheduled}} to decode a new codeword {{from one of the}} <b>input</b> <b>buffers,</b> or {{it can be}} scheduled to help another decoder [...]...|$|R
40|$|As Chip Multiprocessor (CMP) design {{moves toward}} many-core architectures, {{communication}} delay in Network-on-Chip (NoC) {{is a major}} bottleneck in CMP design. An emerging non-volatile memory - STT MRAM (Spin-Torque Transfer Magnetic RAM) which provides substantial power and area savings, near zero leakage power, and displays higher memory density compared to conventional SRAM. But STT-MRAM suffers from inherit drawbacks like multi cycle write latency and high write power consumption. So, these problem have to {{addressed in order to}} have an efficient design to incorporate STT-MRAM for NoC <b>input</b> <b>buffer</b> instead of traditional SRAM based <b>input</b> <b>buffer</b> design. Motivated by short intra-router latency, previously proposed write latency reduction technique is explored by sacrificing retention time and a hybrid design of <b>input</b> <b>buffers</b> using both SRAM and STT-MRAM to "hide" the long write latency efficiently is proposed. Considering that simple data migration in the hybrid buffer consumes more dynamic power compared to SRAM, a lazy migration scheme that reduces the dynamic power consumption of the hybrid buffer is also proposed...|$|R
40|$|A {{conventional}} Network-on-Chip (NoC) router uses <b>input</b> <b>buffers</b> {{to store}} in-flight packets. These buffers improve performance, but consume significant power. It {{is possible to}} bypass these buffers when they are empty, reducing dynamic power, but static buffer power, and dynamic power when buffers are utilized, remain. To improve energy efficiency, buffer less deflection routing removes <b>input</b> <b>buffers,</b> and instead uses deflection (misrouting) to resolve contention. However, at high network load,deflections cause unnecessary network hops, wasting power and reducing performance. In this work, we propose a new NoC router design called the minimally-buffered deflection (MinBD) router. This router combines deflection routing with a small 2 ̆ 2 side buffer, 2 ̆ 2 which is much smaller than conventional <b>input</b> <b>buffers.</b> A MinBD router places some network traffic that would have otherwise been deflected in this side buffer, reducing deflections significantly. The router buffers {{only a fraction of}} traffic, thus making more efficient use of buffer space than a router that holds every flit in its <b>input</b> <b>buffers.</b> We evaluate MinBD against input-buffered routers of various sizes that implement buffer bypassing, a buffer less router, and a hybrid design, and show that MinBD is more energy efficient than all prior designs, and has performance that approaches the conventional input-buffered router with area and power close to the buffer less router...|$|R
50|$|The {{purpose of}} this {{fallback}} was {{to ensure that the}} access method would allocate an <b>input</b> <b>buffer</b> set which was large enough to accommodate any and all of the specified datasets.|$|R
30|$|E 4 [*]=[*]E 4 am[*]=[*]E 3 m, Eq. (48). For step 8, the m vector {{is still}} {{present in the}} <b>input</b> <b>buffer,</b> and E 3 is completed, while step 7 is being executed. A single RAM-low port is {{required}} to load the E 3 into the MACx <b>input</b> <b>buffer.</b> Upon completion of the MACx operations, the output module sends {{a signal to the}} E module indicating that E 4 is completed. The E module accesses the RAM input data register to add E 4 to E 5 [*]+[*]E 1 [*]+[*]E 3 [*]+[*]E 2 [*]+[*]P 1 to create the final E value.|$|R
40|$|Abstract Fork/join {{stations}} model synchronization constraints in queuing network {{models of}} many manufacturing and computer systems. We consider a fork/join station with two <b>input</b> <b>buffers</b> and general <b>inputs</b> from finite populations and derive approximate expressions for throughput and mean queue lengths at the <b>input</b> <b>buffers.</b> We {{assume that the}} arrivals to the fork/join stations are renewal, but our approximations only use information about the first two moments of the inter-renewal distributions. Therefore the approximations can be use d to predict performance {{for a variety of}} systems. We verify the accuracy of these approximations against simulation and report sample results...|$|R
40|$|Abstract- This paper {{examines}} the delay performance of multicast switches with multiple input queues per <b>input</b> <b>buffer.</b> Under {{the assumptions of}} a Poisson uniform traffic pattern, random packet assigning policy, and random packet scheduling policy, we derive the packet delay and service time under different fanouts. To verify this analysis, extensive simulations are conducted with various fanouts, the numbers of queues, and packet arrival rates. It is shown that the theoretical results agree with the simulation results well. The analysis proves {{that it is possible}} to predict how much the packet delay could be decreased through introducing more input queues per <b>input</b> <b>buffer...</b>|$|R
40|$|Fork/join {{stations}} model synchronization constraints in queuing network {{models of}} many manufacturing and computer systems. We consider a fork/join station with two <b>input</b> <b>buffers</b> and general <b>inputs</b> from finite populations and derive approximate expressions for throughput and mean queue lengths at the <b>input</b> <b>buffers.</b> We {{assume that the}} arrivals to the fork/join stations are renewal, but our approximations only use information about the first two moments of the inter-renewal distributions. Therefore the approximations {{can be used to}} predict performance for a variety of systems. We verify the accuracy of these approximations against simulation and report sample results...|$|R
5000|$|A switch may be {{composed}} of <b>buffered</b> <b>input</b> ports, a switch fabric and buffered output ports. If first-in first-out (FIFO) <b>input</b> <b>buffers</b> are used, only the oldest packet is available for forwarding. More recent arrivals cannot be forwarded if the oldest packet cannot be forwarded because its destination output is busy.The output may be busy if: ...|$|R
