19|19|Public
25|$|The {{multiplane}} {{camera is}} a special motion picture camera used in the traditional animation process that moves a number of pieces of artwork past the camera at various speeds and at various distances from one another, creating a three-dimensional effect, although not stereoscopic. Various parts of the artwork layers are left transparent, to allow other layers to be seen behind them. The movements are calculated and photographed frame-by-frame, with the result being an illusion of depth by having several layers of artwork moving at different speeds. The further away from the camera, the slower the speed. The multiplane effect is {{sometimes referred to as}} a parallax process. As a former director and animator of Walt Disney Studios, Ub Iwerks in 1933 invented the multiplane camera using four layers of flat artwork before a <b>horizontal</b> <b>camera.</b>|$|E
5000|$|... #Caption: A VistaVision 35mm <b>horizontal</b> <b>camera</b> film frame. The dotted area {{shows the}} area {{actually}} used.|$|E
5000|$|... #Caption: A VistaVision 35 mm <b>horizontal</b> <b>camera</b> film frame (The dotted area {{shows the}} area {{actually}} used.) ...|$|E
50|$|Above 8×10 inches, the formats {{are often}} {{referred}} to as Ultra Large Format (ULF) and may be 11×14, 16×20, or 20×24 inches or as large as film, plates, or cameras are available. Many large formats (e.g., 24×24, 36x36, and 48x48 inches) are <b>horizontal</b> <b>cameras</b> designed to make big negatives for contact printing onto press-printing plates.|$|R
40|$|In this thesis, the {{problems}} of control, autonomous navigation, and localization in an indoor environment are considered for mini-UAV quadrotor. A commonly used localization methods such as GPS sensors or radar are strongly limited {{due to lack of}} signals in indoor. This thesis will present autonomous navigation by using vanishing point and inertial navigation methods. And indoor localization by using artificial landmarks in a known environment by utilizing vertical and <b>horizontal</b> <b>cameras</b> onboard of the quadrotor is proposed. The local data from the inertial measurement units onboard are combined with the proposed localization method...|$|R
5000|$|... #Caption: A 1907 woodcut of a <b>horizontal</b> format folding <b>camera</b> ...|$|R
5000|$|... #Caption: The 35 mm 8 {{perforation}} Technirama <b>horizontal</b> <b>camera</b> film. Note {{the circle}} has been stretched vertically {{by a factor}} of 1.5.|$|E
50|$|The {{terrain is}} {{constructed}} from an undulating landscape of fixed size tiles. The 3D engine renders vehicles, ships, buildings, trees, hostages and projectiles as solid polygons with shadow effects. The game uses a <b>horizontal</b> <b>camera</b> angle where only a boxed {{section of the}} terrain centred on the player's position is visible. This limitation in depth is necessary to achieve an acceptable frame-rate with the stock Amiga hardware.|$|E
50|$|The {{technicians}} at Fleischer Studios {{created a}} distantly related device, called the Stereoptical Camera or Setback, in 1934. Their apparatus used three-dimensional miniature sets built {{to the scale}} of the animation artwork. The animation cels were placed within the setup so that various objects could pass in front of and behind them, and the entire scene was shot using a <b>horizontal</b> <b>camera.</b> The Tabletop process was used to create distinctive results in Fleischer's Betty Boop, Popeye the Sailor, and Color Classics cartoons.|$|E
40|$|In {{order to}} control the puffing of {{impurities}} (neon, argon) and deuterium gas in the main plasma chamber for experiments with a radiative boundary, a fast online determination of the total radiated power from the plasma for a feedback control is required. The total radiated power is normally calculated by a deconvolution of measurements of a <b>horizontal</b> bolometer <b>camera</b> with 40 lines of sight. This time consuming algorithm, which works under the assumption of constant emissivirty on closed flux surfaces, {{can not be used}} online during the experiment to reach the so called CDH-mode. By means of a singular value decomposition a subset of 10 and 18 channels of the <b>horizontal</b> bolometric <b>camera</b> system is chosen for online reconstruction of the radiated power. A new nontrivial selection scheme was established for this purpose. With this subset a regression analysis against the offline calculated power was done. The regression coefficients are then used for a weighted sum over the selected subset of lines of sight to predict the total radiated power online during the discharge. (orig.) 7 refs. Available from TIB Hannover: RA 71 (1 / 297) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekSIGLEDEGerman...|$|R
30|$|The WCA {{of water}} {{repellent}} cotton fabric samples {{were determined by}} spreading cotton samples on flat table. One fine drop of water was placed on fabric samples using medical syringe of 0.5  mL capacity. The pictures were taken using Kodak 15 X Zoom <b>camera.</b> <b>Horizontal</b> and tangential lines were drawn on pictures and WCA were measured using protractor.|$|R
40|$|Orthophotos are a {{standard}} requirement in archaeological documentation; yet they differ in several respects from aerial orthoimaging. The required large scales of end-products call for close-range photography, usually taken from low altitude or with raised <b>cameras</b> for <b>horizontal</b> recording. Special <b>camera</b> platforms {{need to be}} devised to this effect, such as the flexible low-cost devices (small balloon; adapted fishing-rod) use...|$|R
50|$|A {{predecessor}} to the multiplane {{camera was}} used by Lotte Reiniger for her animated feature The Adventures of Prince Achmed (1926). Berthold Bartosch, who worked with Reiniger, used a similar setup in his film L'Idee (1932). The first multiplane camera, using four layers of flat artwork before a <b>horizontal</b> <b>camera,</b> was invented by former Walt Disney Studios animator/director Ub Iwerks in 1933, using parts from an old Chevrolet automobile.His multiplane camera was used {{in a number of}} the Iwerks Studio's Willie Whopper and Comicolor cartoons of the mid-1930s.|$|E
5000|$|A camera dolly is a wheeled cart {{or similar}} device used in filmmaking and {{television}} production to create smooth <b>horizontal</b> <b>camera</b> movements. The camera is mounted to the dolly {{and the camera}} operator and focus puller or camera assistant usually ride on the dolly to push the dolly back and forth. The camera dolly is generally used to produce images which involve moving the camera toward or away from a subject while a take is being recorded, a technique known as a [...] "dolly shot." [...] The dolly grip is the dedicated technician trained to operate the dolly by manually pushing it back and forth.|$|E
50|$|The sixties was {{not only}} the decade of modernist film, but the {{starting}} era of distinctive directorial filmmaking. This can also be observed in Hungary. After his debut films, Így jöttem and Szegénylegények were the first movies where Miklós Jancsó's trademark visual style — long, slow cuts and <b>horizontal</b> <b>camera</b> movement — appear. István Szabó directs his most personal movies during this time, pairing subjectivity with first person narration. After 1956's Professor Hannibal, Zoltán Fábri further elaborates the theme of moral choice in historical times in many of his films from the 1960s, like Isten hozta, őrnagy úr!, Két félidő a pokolban, Nappali sötétség. After trying several genres, the cinema of Károly Makk becomes more unified, creating the most political, dramatic films of his career during the era, with Megszállottak, Elveszett paradicsom, Az utolsó előtti ember.|$|E
50|$|If the camera's angle {{was always}} the same, an optimal viewing {{distance}} could be easily calculated. However the <b>camera's</b> <b>horizontal</b> angle varies as the focal length of its lens changes. If the camera's sensor has fixed dimensions, a shorter focal length (wide angle) lens captures a wider angle of view, requiring the viewer to sit closer to the screen. Conversely, a longer focal length (telephoto) lens captures a narrower angle of view, demanding a more distant viewer position.|$|R
40|$|The {{standard}} setup in {{reconstructing the}} three-dimensional geometry {{of a scene}} {{from a pair of}} stereo images is to have them rectified, in which case the apparent motion of points is <b>horizontal.</b> With pinhole <b>cameras,</b> it is always possible to find two homographies that rectify the images. The method of Fusiello and Irsara assumes that both cameras are the same with principal point at the center, but keeps the focal length as an unknown. The virtual rotations of the two cameras are then evaluated to minimize the vertical motion of points...|$|R
50|$|Before {{releasing}} the 914, Xerox tested {{the market by}} introducing a developed version of the prototype hand-operated equipment known as the Flat-plate 1385. The 1385 was not actually a viable copier because of its speed of operation. As a consequence, it was sold as a platemaker to the offset lithography market, perhaps most notably as a platemaker for the Addressograph-Multigraph Multilith 1250 and related sheet-fed offset printing presses. It {{was little more than}} a high quality, commercially available plate camera mounted as a <b>horizontal</b> rostrum <b>camera,</b> complete with photo-flood lighting and timer. The glass film/plate had been replaced with a selenium-coated aluminum plate. Clever electrics turned this into a quick developing and reusable substitute for film. A skilled user could produce fast, paper and metal printing plates of a higher quality than almost any other method. Having started as a supplier to the offset lithography duplicating industry, Xerox now set its sights on capturing some of offset's market share.|$|R
50|$|The high-resolution, {{digital cinema}} grade video {{projector}} alternately projects right-eye frames and left-eye frames, switching between them 144 times per second. The projector {{is either a}} Texas Instruments Digital Light Processing device or Sony's reflective LCOS (liquid crystal on silicon). A push-pull electro-optical liquid crystal modulator called a ZScreen is placed immediately {{in front of the}} projector lens to alternately polarize each frame. It circularly polarizes the frames clockwise for the right eye and counter-clockwise for the left eye. The audience wears circularly polarized glasses that have oppositely polarized lenses that ensures each eye sees only its designated frame. In RealD Cinema, each frame is projected three times to reduce flicker, a system called triple flash. The source video is usually produced at 24 frames per second per eye (total 48 frames/s), which may result in subtle ghosting and stuttering on <b>horizontal</b> <b>camera</b> movements. A silver screen is used to maintain the light polarization upon reflection and to reduce reflection loss to counter some of the significant light loss due to polarization filter absorption. The result is a 3D picture that seems to extend behind and in front of the screen itself.|$|E
40|$|The JET neutron profile monitor is {{a unique}} {{instrument}} among neutron diagnostics available at large fusion research facilities. The profile monitor comprises two fan shaped multi-collimator cameras, with 10 channels in the <b>horizontal</b> <b>camera</b> and 9 channels in the vertical camera. A schematic drawing of the JE...|$|E
40|$|Conventional {{panoramic}} image stitching techniques require strictly <b>horizontal</b> <b>camera</b> panning, {{only possible}} with specialized photographic equipments. The specific hardware constraints and camera motion controls tremendously compromise their {{potential to be}} used by users with regular camera equipment. This project aims to implement a software application that is capable to create panoramic image mosaics from image sequences taken with regular digital camera. And it also attempts to generate an immersive 2 D viewing experience by mapping the resultant panoramic image mosaic on a 3 D cylinder...|$|E
5000|$|A stat {{camera is}} a large-format {{vertical}} or <b>horizontal</b> stationary <b>camera</b> used to shoot film from camera-ready artwork, and sometimes called a copy camera. This {{is a large}} bellows-type camera which consists of the copy-board, bellows and lens, and filmboard. The vertical type can take up relatively little space, while the horizontal fills two rooms; bellows, lens, and copyboard {{on one side of}} the wall; filmboard and darkroom on the other. The type of film used is black and white [...] "orthochromatic"; i.e., it is more sensitive to some colors than others. Guidelines, or [...] "keylines" [...] are created in light blue which read as white; while anything red or close to a red hue appears as black. The stat camera would be used to shoot color separations (using hue filters for each of the four process colors) and to produce halftone film for printing using a special reticulated gel mask. Screen printers will use the films, or 'positives', to expose the silk screen.|$|R
40|$|A {{robotic system}} that allows a Nomadic SuperScout II robot to press an {{elevator}} button was successfully designed and built. It includes an arm for vertical motion, a finger for <b>horizontal</b> motion, a <b>camera</b> to provide pictures of the elevator buttons for use in visual servoing, and a control box which contains the necessary circuitry to control these components. Software for finding buttons from a camera image, communicating with the various components, and providing motor control was written and implemented. Although hardware failures prevented the full implementation of the visual feedback system {{that would have allowed}} the robot to control the arm autonomously, the arm can press an elevator button under human control and the robot can find elevator buttons from an image. Keywords: Robotics, Computer Vision, Visual Servoing, Motor Control, System Integratio...|$|R
3000|$|In {{the case}} of t = 1, it is {{recorded}} as a mutation point on the p-th bisection line. If the p-th bisection line cannot find any mutation point, {{it is assumed that}} the unit step is [...] d=u_c/n× 2 × 5 (where uc is the <b>camera</b> <b>horizontal</b> pixel). To re-determine the mutation point, use one of the ten lines which are parallel to the bisection line in the range of about five steps on the left and right sides. When the entire straight line is judged and the mutation point of H value is found, the search for other lines in the set range is stopped. If we still cannot find the mutation point, in order to avoid falling into the search dilemma, set the sub-line point of the mutation point as (x [...]...|$|R
30|$|Conci and Lizzi [21] also {{reported}} on the placement of cameras using PSO. In their method, they assumed a Rayleigh distribution for characterizing {{the distance of the}} object and a Gaussian distribution for modeling the <b>horizontal</b> <b>camera</b> FOV, and, their work mainly focused on an indoor environment where the number of cameras is small and the PSO performance is not an issue. Our work, on the contrary, is more intended for applications discussed in [13, 14] where hundreds of cameras or more are randomly distributed in an unknown area. Therefore we focus more on the performance of the algorithm and the relationships between the coverage improvement and the scale of the network. This makes our work complementary to [21].|$|E
40|$|This paper {{presents}} {{a novel approach}} to creating full view panoramic mosaics from image sequences. Unlike current panoramic stitching methods, which usually require pure <b>horizontal</b> <b>camera</b> panning, our system does not require any controlled motions or constraints on how the images are taken (as {{long as there is}} no strong motion parallax). For example, images taken from a hand-held digital camera can be stitched seamlessly into panoramic mosaics. Because we represent our image mosaics using a set of transforms, there are no singularity problems such as those existing at {{the top and bottom of}} cylindrical or spherical maps. Our algorithm is fast and robust because it directly recovers 3 D rotations instead of general 8 parameter planar perspective transforms. Methods to recover camera focal length are also presented. We also present an algorithm for efficiently extracting environment maps from our image mosaics. By mapping the mosaic onto an artibrary texture-mapped polyhedron surrounding t [...] ...|$|E
40|$|Two new {{bolometer}} cameras, for {{horizontal and}} vertical views of the plasma cross-section, are under construction for the Enhanced Performance Phase of JET, to begin in 2005. These cameras replace previous cameras in operation and represent a substantial upgrade in capabilities: more viewing chords over a larger viewing angle, higher detectable energy range, higher sensitivity, lower noise level and therefore, lower detectable signal (similar to 1 mu W/cm(2) for tau = 2 ms versus similar to 70 mu W/cm(2) for tau = 20 ms). The detectors are {{essentially the same as}} those in use on ASDEX-Upgrade (gold energy-absorbing layer, mica substrate, gold meander), whereby the absorber layer has been increased in thickness from 4 to 8 mu m in order to extend sensitivity to 8 keV. Each camera is comprised of 24 channels, with a particularly fine spatial resolution in the divertor region of similar to 8 cm. Definition of the lines-of-sight is attained by a collimator arrangement for the vertical camera and use of pinholes for the <b>horizontal</b> <b>camera.</b> Salient design features of these cameras are described. (c) 2005 Elsevier B. V. All rights reserved...|$|E
40|$|We {{present an}} interactive, {{computational}} approach for assisting users {{with visual impairments}} during photographic documentation of transit problems. Our technique {{can be described as}} a method to improve picture composition, while retaining visual information that is expected to be most relevant. Our system considers the position of the estimated region of interest (ROI) of a photo, and camera orientation. Saliency maps and Gestalt theory are used for guiding the user towards a more balanced picture. Our current implementation for mobile phones uses optic flow to update the internal knowledge of the position of the ROI and tilt sensor readings to correct non <b>horizontal</b> or vertical <b>camera</b> orientations. Using ground truth labels, we confirmed our method proposes valid strategies for improving image composition. Future work includes an optimized implementation and user studies. 1...|$|R
40|$|AbstractIt is {{difficult}} to monitor the displacement of large structures accurately in real time for the health monitoring of structures. We studied a laser displacement monitoring system. When the inclination of the structure is negligible, it can achieve real-time monitoring of the horizontal displacement of the structure precisely. The system is composed of a laser transmitter, a receiver board and a camera. The laser transmitter is fixed on the datum plane. Both the camera and the receiver board are fixed on the structure to be monitored. When the structure has <b>horizontal</b> displacement, that <b>camera</b> monitors the position variation of the laser spot on the receiver board. The actual displacement of the structure is obtained through the camera's built-in program. To evaluate {{the accuracy of the}} system, we performed two sets of model experiments and obtained the conclusion by comparing the experimental data, determining that our laser displacement monitoring system can monitor horizontal displacement {{with a high degree of}} accuracy...|$|R
40|$|Estimating {{the area}} of seabed {{surfaces}} from pictures or videos is an important problem in seafloor surveys. This task is complex to achieve with moving platforms such as submersibles, towed or remotely operated vehicles (ROV), where the recording camera is typically not static and provides an oblique view of the seafloor. A new method for obtaining seabed surface area estimates is presented here, using the classical set up of two laser devices fixed to the ROV frame projecting two parallel lines over the seabed. By combining lengths measured directly from the image containing the laser lines, {{the area of}} seabed surfaces is estimated, {{as well as the}} camera’s distance to the seabed, pan and tilt angles. The only parameters required are the distance between the parallel laser lines and the <b>camera’s</b> <b>horizontal</b> and vertical angles of view. The method was validated with a controlled in situ experiment using a deep-sea ROV, yielding an area estimate error of 1. 5 %. Further applications and generalizations of the method are discussed, with emphasis on deep-sea applications...|$|R
40|$|Abstract. Camera traps {{can detect}} rare and cryptic species, and may enable {{description}} of the stability of populations of threatened species. We investigated the relative performance of cameras oriented horizontally or vertically, and recording mode (still and video) to detect the vulnerable long-nosed potoroo (Potorous tridactylus) as a precursor to population monitoring. Weestablished camera traps for periodsof 13 – 21 daysacross 21 sites inRichmondRangeNational Park innorth-eastNewSouthWales. Each camera trap set consistedof threeKeepGuardKG 680 Vcameras directed at a bait container–one horizontal and one vertical camera in still mode and one <b>horizontal</b> <b>camera</b> in video mode. Potoroos and bandicoots (Perameles nasuta and Isoodon macrourus) were detected at 14 sites and pademelons (Thylogale stigmatica and T. thetis) were detected at 19 sites. We used program PRESENCE to compare detection probabilities for each camera category. The detection probability for all three taxa groups was lowest for the vertical still and similar for the horizontal cameras. The detection probability (horizontal still) was highest for the potoroos (0. 43) compared with the bandicoots (0. 16) and pademelons (0. 25). We estimate that the horizontal stills camera could achieve a 95 % probability of detection of a potoroo within 6 days compared with 8 days using a vertical stills camera. This suggests that horizontal cameras in still mode hav...|$|E
40|$|Camera traps {{can detect}} rare and cryptic species, and may enable {{description}} of the stability of populations of threatened species. We investigated the relative performance of cameras oriented horizontally or vertically, and recording mode (still and video) to detect the vulnerable long-nosed potoroo (Potorous tridactylus) as a precursor to population monitoring. We established camera traps for periods of 13 – 21 days across 21 sites in Richmond Range National Park in north-east New South Wales. Each camera trap set consisted of three KeepGuard KG 680 V cameras directed at a bait container – one horizontal and one vertical camera in still mode and one <b>horizontal</b> <b>camera</b> in video mode. Potoroos and bandicoots (Perameles nasuta and Isoodon macrourus) were detected at 14 sites and pademelons (Thylogale stigmatica and T. thetis) were detected at 19 sites. We used program Presence to compare detection probabilities for each camera category. The detection probability for all three taxa groups was lowest for the vertical still and similar for the horizontal cameras. The detection probability (horizontal still) was highest for the potoroos (0. 43) compared with the bandicoots (0. 16) and pademelons (0. 25). We estimate that the horizontal stills camera could achieve a 95 % probability of detection of a potoroo within 6 days compared with 8 days using a vertical stills camera. This suggests that horizontal cameras in still mode have great potential for monitoring the dynamics of this potoroo population...|$|E
40|$|The future Cherenkov Telescope Array (CTA) {{will consist}} of several tens of telescopes of {{different}} mirror areas. CTA will provide next generation sensitivity to very high energy photons over a wide energy range from few tens of GeV to > 100 TeV. The signals of the photon detectors in the focal plane will be read out with custom-designed, fast digitization and trigger- ing electronics. Within CTA, several camera electronics options are being evaluated. The FlashCam approach is unique here since data processing inside the camera is fully digital. Signal digitization and trigger processing are jointly per- formed in one single readout chain per detector pixel. For a group of pixels, such a chain consists of Flash ADCs and a Field-Programmable Gate Array (FPGA) module, both commercially available and reasonably priced. A camera-wide event trigger is subsequently computed in separate trigger units. The camera data is transferred from the cameras over ethernet to a central computer farm using a custom network protocol. Such a fully digital approach using state-of-the-art components provides accurate triggering and an easily scalable architecture in a cost-effective way. The FlashCam team is also evaluating {{the concept of a}} <b>horizontal</b> <b>camera</b> integration. Here, the photon detector plane is sustained by a monolithic carrier holding photon detectors, high voltage supplies, and preamplifiers only. The signal digitization and triggering electronics are organized in bo ards and crates which are kept at the rear side of the camera body. Such an approach allows different photon detectors to be adopted and might result in cost saving...|$|E
40|$|The {{interaction}} of horizontal jet and free surface was experimentally evaluated, using the Specklegram method and Stereo-PIV. The turbulence at the free surface causes fully three dimensional non-linear phenomena. In order {{to evaluate the}} interactions, the three dimensional free surface shape and velocity distribution {{just beneath the surface}} should be simultaneously measured. Here, Stereoscopic Particle Image Velocimetry (stereo-PIV) and Specklegram Method (Tanaka et al., 2000) were combined to visualize free surface and underlying turbulence simultaneously. The test section was a rectangular tank having a free surface. A circular nozzle was set horizontally beneath the free surface to form a jet. The jet interacted with the free surface, causing the wavy free surface condition. Two cameras were set beneath the test tank to take the stereoscopic view of the turbulent jet. Three-dimensional velocity distribution beneath the surface was obtained on the <b>horizontal</b> plane. Another <b>camera</b> was used for Specklegram Method which visualizes the three-dimensional surface shape. The combined optical technique enabled to measured the free surface wave quantitatively in a high accuracy. Using this quantitative surface information and velocity distribution measured by stereo-PIV, correlation between free surface fluctuation and flow field was obtained...|$|R
40|$|QuickTime {{virtual reality}} (QTVR) is a {{software}} technology that creates, {{on a normal}} computer screen, the illusion of holding and turning a three-dimensional object. QTVR is a practical photo-realistic virtual reality technology that is easily implemented on any current personal computer or via the Internet with no special hardware requirements. Because {{of its ability to}} present dynamic photo-quality images, we reasoned that QTVR can provide a more realistic presentation of anatomic structure than two-dimensional atlas pictures and facilitate study of specimens outside the dissection lab. We created QTVR objects, using portions of the skull, and incorporated them into an instructional program for first-year medical students. To obtain images, the bones of the skull were mounted on a rotating table, and a digital camera was positioned on a swinging arm so that the focal point remained coincident with the rotational center of the object as the camera was panned through a vertical arc. Digital images were captured at intervals of 10 ° rotation of the object (<b>horizontal</b> pan). The <b>camera</b> was then swung through an arc with additional horizontal pan sequences taken at 10 ° intervals of vertical pan. The images were edited to place the object on a solid black background, then assembled into a linear QuickTime movie. The linear movie was processed to yield a QTVR object movie that can be manipulated on vertical and horizontal axes using the mouse. QTVR movies were incorporated into an interactive environment that provided labeling, links to text-based information and self-testing capabilities. This program, Yorick—the VR Skull, has been used in our first-year medical and graduate gross anatomy courses for the past two years. Results of student evaluation of the program indicate that this QTVR-based program is an effective learning tool that is well received by students. Clin. Anat. 13 : 287 – 293, 2000. © 2000 Wiley-Liss, Inc...|$|R
40|$|QuickTime VR is a {{software}} technology which creates, {{on a normal}} computer screen, the illusion of holding and turning a three–dimensional object. QTVR is a practical photo–realistic virtual reality technology which is easily implemented on any current personal computer or via the Internet with no special hardware requirements. We reasoned that QTVR can provide a more realistic presentation of anatomical structure than two dimensional atlas pictures and allow observation of specimens outside the dissection lab. We created QTVR objects from various anatomical specimens, including the skull, which we have incorporated into a self–learning program. To obtain images, {{the bones of the}} skull were mounted on a rotating table, while a digital camera was positioned on a swinging arm so that the focal point remained co–incident with the rotational center of the object as the camera was panned through a vertical arc. Digital images were captured at intervals of 10 ° rotation of the object (<b>horizontal</b> pan). The <b>camera</b> was then swung through an arc with additional horizontal pan sequences taken at 10 ° intervals of vertical pan. The images were edited to place the object on a solid black background, then assembled into a linear QuickTime movie using Adobe Premier. The linear movie was processed with Apple’s QTVR development tools to yield a QTVR object movie which can be manipulated on vertical and horizontal axes using the mouse. QTVR movies were incorporated into an interactive environment, created with Macromedia Director, which provided labeling, links to text–based information and self–testing capabilities. The accompanying demonstration shows the resulting QTVR–based program Yorick: the VR Skull during development, the program was used in our medical gross anatomy course. Student feedback by survey indicated that QTVR–based programs are an effective learning tool...|$|R
