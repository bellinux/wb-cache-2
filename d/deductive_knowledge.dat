21|22|Public
50|$|The logico-deductive method whereby {{conclusions}} (new knowledge) {{follow from}} premises (old knowledge) {{through the application}} of sound arguments (syllogisms, rules of inference), was developed by the ancient Greeks, and has become the core principle of modern mathematics. Tautologies excluded, nothing can be deduced if nothing is assumed. Axioms and postulates are the basic assumptions underlying a given body of <b>deductive</b> <b>knowledge.</b> They are accepted without demonstration. All other assertions (theorems, if we are talking about mathematics) must be proven with the aid of these basic assumptions. However, the interpretation of mathematical knowledge has changed from ancient times to the modern, and consequently the terms axiom and postulate hold a slightly different meaning for the present day mathematician, than they did for Aristotle and Euclid.|$|E
5000|$|Detailed {{knowledge}} regarding Topshe's {{high school}} education is not available, but he {{used to go to}} school during the adventure of Sonar Kella (the sixth book of Feluda series). In the film adaptation of Sonar Kella, Topshe's mother shows her concern regarding his career and study as he spends a lots of time in case solving with Feluda. By the time of Kailashe Kelenkari (the eighth book of the series) he have had his secondary examination ('Madhyamik Pariksha'). Just like Feluda, Topshe also used to read lot of books to enhance his knowledge. Topshe often gets his lessons from Feluda himself. In many cases, Feluda tests his <b>deductive</b> <b>knowledge</b> and he usually passes the test satisfactorily. In the movie [...] Sonar Kella Topshe's father aptly said that Topshe is a lucky boy who got Feluda as his mentor. Topshe is fond of 'Adventures of Tintin' comic series. In the book [...] Kailashe Kelenkari, he is found reading 'Tintin in Tibet'.|$|E
40|$|We {{develop a}} general closure {{semantics}} for deduction rules in knowledge bases, using {{the concept of}} a (<b>deductive)</b> <b>knowledge</b> system proposed in [Wag 94 a] where rules are interpreted as update functions operating on knowledge bases. We first present four important examples of basic knowledge systems: relational databases, (defeasible) factbases, temporal databases, and epistemic states. We then define the notion of a supported closure, and we show that every ampliative <b>deductive</b> <b>knowledge</b> base has a supported closure. Choosing those supported closures which satisfy a certain stability condition as the preferred (or intended) ones we obtain the stable closure semantics for <b>deductive</b> <b>knowledge</b> bases (including normal, extended, disjunctive, and other logic programming and rule-based systems). Contents 1 Introduction 2 2 Basic Concepts 3 2. 1 Knowledge Systems : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4 2. 2 Regular Knowledge Systems : : : : : : : : : : : : : : : [...] ...|$|E
5000|$|F-logic is a {{declarative}} {{object-oriented language}} for <b>deductive</b> databases and <b>knowledge</b> representation.|$|R
40|$|Abstract. Whereas symbol–based systems, like <b>deductive</b> {{reasoning}} devices, <b>knowledge</b> bases, planning systems, or {{tools for}} solving constraint satisfaction problems, presuppose (more or less) {{the consistency of}} data and the consistency of results of internal computations, this {{is far from being}} plausible in real–world applications...|$|R
40|$|Abstract. Second-order {{quantifier}} elimination in {{the context}} of classical logic emerged as a powerful technique in many applications, including the correspondence theory, relational databases, <b>deductive</b> and <b>knowledge</b> databases, knowledge representation, commonsense reasoning and approximate reasoning. In the current paper we first generalize the result of Nonnengart and Sza̷las [17] by allowing second-order variables to appear within higher-order contexts. Then we focus on a semantical analysis of conditionals, using the introduced technique and Gabbay’s semantics provided in [10] and substantially using a third-order accessibility relation. The analysis is done via finding correspondences between axioms involving conditionals and properties of the underlying third-order relation...|$|R
40|$|Given a {{scenario}} in which members of an academic community collaboratively construct and share an archive of news items, several knowledge management challenges arise. The authors' integrated suite of tools, called PlanetOnto, supports a speedy but high quality publishing process, allows ontology-driven document formalization and augments standard browsing and search facilities with <b>deductive</b> <b>knowledge</b> retrieva...|$|E
40|$|This paper {{outlines}} how we {{are exploring}} ways to combine analogical reasoning, <b>deductive</b> <b>knowledge</b> integration, and intelligent information retrieval to create new power tools for intelligence analysis. We summarize efforts on two problems: (1) situation tracking—where {{the goal is to}} maintain a conceptual understanding of an ongoing situation over time, extending it with new information (often gleaned proactively) — and (2) the whodunit problem—identifying a small set of likely perpetrators for an event. 1...|$|E
40|$|We {{describe}} {{a framework for}} symbolically evaluating iterative C code using a deductive approach that automatically discovers and proves program properties. Although verification is not performed, the method can infer detailed program behavior. Software engineering work flows could be enhanced by this type of analysis. Floyd-Hoare verification principles are applied to synthesize loop invariants, using a library of iteration-specific <b>deductive</b> <b>knowledge.</b> When needed, theorem proving is interleaved with evaluation and performed on the fly. Evaluation results {{take the form of}} inferred expressions and type constraints for values of program variables. An implementation using PVS (Prototype Verification System) is presented along with results for sample C functions...|$|E
40|$|Abstract: Boolean logic {{appears to}} be unable to draw {{conclusions}} {{in the presence of}} inconsistent and/or incomplete information. Fortunately, what seems to be impossible to solve applying binary logic can be achieved using non-binary logic. In this work we use the theory of many-value logic to facilitate the <b>deductive</b> process of <b>knowledge</b> assessment. Key–Words: Web-based assessment, learning, intelligent tutoring systems...|$|R
40|$|The European Union {{retained}} the WISECARE project "Work flow Information Systems for European nursing CARE" for funding. The project {{focuses on the}} use of telematics technology for clinical and resource management in oncology care in hospitals. This paper outlines the impact of introducing this kind of advanced nursing informatics application on the management of nursing knowledge. Three shift in knowledge management that will get high attention in WISECARE, are identified. The first is the shift from knowledge dissemination to knowledge sharing. The second is the shift from individual knowledge to organisational knowledge. The third is the shift from <b>deductive,</b> prescriptive <b>knowledge</b> as seen in guidelines, protocols to more inductive, experience based knowledge. The paper emphasizes that the real impact of information technology is not in the automation of existing processes but on the discovery of new ways of organisation and living. status: publishe...|$|R
40|$|Recently {{several authors}} have {{stressed}} and illustrated {{the importance of}} including a second kind of negation (explicit negation) in logic programs besides "negation as failure", and its use in <b>deductive</b> databases, <b>knowledge</b> representation, and nonmonotonic reasoning. By introducing explicit negation into logic programs contradiction may appear. In this work we present two approaches for dealing with contradiction, and show their equivalence. One of the approaches consists in avoiding contradiction, {{and is based on}} restrictions in the adoption of abductive hypotheses. The other approach consists in removing contradiction, and is based on a transformation of contradictory programs into noncontradictory ones, guided by the reasons for contradiction. The work is divided into two parts: one is presented in this paper, and comprises the contradiction avoidance approach, and the other in [16] in this volume, comprises the contradiction removal approach and shows the equivalence between the [...] ...|$|R
30|$|Ontology {{development}} for classifications {{are the main}} concerns in former studies on OBC. However, there are more things that OBC can do. Sources of the knowledge for ontology development can be easily recorded in OBC. Transparent classification processes can be supported by existing tools, {{as can be seen}} in ontology explanation (Kalyanpur et al. 2007) implemented in pellet. 2 The traceable knowledge and transparent classification are able to make the classifying results more comprehensible and convincing, with every step of the classification can be validated. In addition, experiments show that classification accuracy can be further enhanced by combining induction and deduction, as one can obtain consistent <b>deductive</b> <b>knowledge</b> through consistency checking for inductive results (Lécué and Pan 2015).|$|E
40|$|Individuals often cannot address (objective) group injustices {{until they}} develop a (subjective) {{critical}} awareness of them. In three studies, we tested two potential psychological pathways toward critical awareness: Reflection (<b>deductive,</b> <b>knowledge</b> driven) and action (inductive, action driven) mindsets. Across studies, participants {{were exposed to}} an objectively unjust event, enacted by the experimenter. Based on a pilot study (N= 31) and first experiment (N= 28), we developed the hypothesis that action (vs. reflection) mindsets increase group entitativity (due to their reliance on expectancy effects), but not necessarily (subjective) critical awareness of (objective) group injustice. Study 2 (N= 121) confirmed this hypothesis. We discuss the difficulties of developing (subjective) critical awareness of (objective) group injustices...|$|E
40|$|Milord II is an {{architecture}} for developing knowledge [...] based systems. In particular {{we are interested}} in real Expert Systems, that is, those that are useful in a real environment and that have real purposes. To do that we propose a language based on modules as a method for programming in the large. Modules, generic modules and a set of operations among them are the basis of this language. A program in Milord II is then a hierarchical structure of modules. Modules are encapsulated components with a well defined interface. Each module is composed of <b>deductive</b> <b>knowledge</b> (weighted facts and rules), local logic (an algebra declaration) and a local control component (Horn [...] like metarules) ...|$|E
40|$|Within a {{research}} project whose aim is to promote the learning of percutaneous operation in orthopedic surgery, we investigate some representation models of empirical, <b>deductive,</b> and perceptivo-gestural <b>knowledge.</b> From these models, we design an TEL system (Tecnological Enhaced Learning) This project belongs to a multidisciplinary field including computer, orthopedic surgery, medical imaging, didactic and cognitive sciences. The article presents the design principles of TEL with a particular interest {{in the development of}} a simulator. This simulator allows a virtual exercise interacting with the learner in visual, temporal and haptic dimension...|$|R
40|$|Data {{fragmentation}} is {{a well-known}} technique used in distributed database design to support efficient query processing. In <b>deductive</b> databases and <b>knowledge</b> bases, one distinct operation is the computation of transitive closures. In this paper, we employ a fragmentation approach to deductive databases. Fragmentation techniques are investigated in distributed database environments for parallel recursive query processing. Based on the novel fragmentation scheme, a new parallel transitive closure algorithm is proposed. Compared with previous algorithms, our algorithm can explore a much higher level of parallelism with very little communication overhead increase...|$|R
40|$|Abstract. In {{this paper}} we present O-DEVICE, a <b>deductive</b> {{object-oriented}} <b>knowledge</b> base system for reasoning over OWL documents. O-DEVICE imports OWL documents into the CLIPS production rule system by transforming OWL ontologies into an object-oriented schema of the CLIPS Object-Oriented Language (COOL) and instances of OWL classes into COOL objects. The {{purpose of this}} transformation {{is to be able}} to use a deductive object-oriented rule language for reasoning about OWL data. The O-DEVICE data model for OWL ontologies maps classes to classes, resources to objects, property types to class slot (or attribute) definitions and encapsulates resource properties inside resource objects, as traditional OO attributes (or slots). In this way, when accessing properties of a single resource, few joins are required. O-DEVICE is an extension of a previous system, called R-DEVICE, which effectively maps RDF Schema and data into COOL objects and then reasons over RDF data using a deductive object-oriented rule language. ...|$|R
40|$|The {{method of}} the {{research}} mediates a direct confrontation with a real life situations. The proper research is chosen from a field of a reality specific to a sector of knowledge or of the practical human action, condenses the essential and through this, it shows what is generally valid, in the objects, phenomena or events from which it was selected (Chelcea S. 2000). A research represents a support of inductive knowledge, that passes from particular premises to the reveal of general, to the formulation of some generating conclusions (notions, principles, rules), but reverse too, as a base of a <b>deductive</b> <b>knowledge,</b> that passes from general to particular, of concretization of an idea, and some generalizations...|$|E
40|$|Abstract-An {{effective}} {{representation of}} principals ' knowl-edge can greatly improve the efficiency ofcryptographic protocol analysis. In this paper, we propose {{a mechanism to}} represent the <b>deductive</b> <b>knowledge</b> contained {{in a set of}} terms. Using Dolev-Yao model as an example, we design two algorithms to generate the knowledge representation and derive terms, respectively. We prove that using our knowledge representation, a principal can derive a term by using only constructive opera-tions. To demonstrate the advantages of the proposed approach, we integrate it with Athena to build a new protocol verifier. The new approach will drastically reduce the number of states that are generated and analyzed during protocol verification. Experiments on several cryptographic protocols widely used for evaluating protocol verifiers demonstrate the improvements. I...|$|E
40|$|Abstract. An {{important}} issue {{is to know}} whether Web ontology languages, meet the expected requirements of expressiveness and reasoning. This paper aims at contributing to this question in evaluating and comparing several languages. After describing {{the needs of a}} Semantic Web in medicine, it analyses Protégé and DAML+OIL primitives on a concrete medical ontology, the brain cortex anatomy ontology. It draws conclusions about the requirements that a Web ontology language should meet for the representation of medical taxonomy and axioms. The expressiveness of DAML+OIL or OWL DL seems suited to describe the complex taxonomic knowledge. But rules are required for representing the <b>deductive</b> <b>knowledge</b> (dependencies between relations) and to support several tasks (ontology construction, maintenance, verification, query of heterogeneous distributed information sources). Finally, the paper evaluates the features of the next standard OWL and of an hybrid language CARIN-ALN with respect to these requirements. ...|$|E
40|$|Recently {{several authors}} have {{stressed}} {{and showed the}} importance of having a second kind of negation in logic programs for use in <b>deductive</b> databases, <b>knowledge</b> representation, and nonmonotonic reasoning [6, 7, 8, 9, 13, 14, 15, 24]. Different semantics for logic programs extended with :-negation (extended logic programs) have appeared [1, 4, 6, 9, 11, 12, 17, 19, 24] but, contrary to what happens with semantics for normal logic programs, there is no general comparison among them, specially in what concerns the use and meaning of the newly introduced :-negation. The goal {{of this paper is to}} contrast a variety of these semantics in what concerns their use and meaning of :-negation, and its relation to classical negation and to the default negation of normal programs, here denoted by not : To this purpose we define a parametrizeable schema to encompass and characterize a diversity of proposed semantics for extended logic programs, where the parameters are two: one the axioms AX: defin [...] ...|$|R
40|$|This paper {{describes}} {{an approach to}} allow end users to define new procedures through tutorial instruction. Our approach allows users to specify procedures in natural language {{in the same way}} that they would instruct another person, while the system handles incompleteness and ambiguity inherent in natural human instruction and formulates follow up questions. We describe the key features of our approach, which include exposing prior <b>knowledge,</b> <b>deductive</b> and heuristic reasoning, shared learning state, and selectively asking questions to the user. We also describe how those key features are realized in our implemented TellMe system, and present preliminary user studies where non-programmers were able to easily specify complex multi-step procedures...|$|R
40|$|The manifold, {{at times}} {{contrary}} demands and developments professionals in care {{and social work}} are confronted with, {{have led to a}} crisis in professional identity. The authors claim that an inductive strategy is a better strategy than a <b>deductive</b> strategy of <b>knowledge</b> development when the purpose is to strengthen the identity. The former strategy both relates to personal needs, values, knowledge and experience of professionals, ands offers some grip on knowledge development by the professionals themselves. &# 13; This is illustrated by a learning trajectory that was developed for social workers. Describing this trajectory and presenting an evaluation of the results, the authors conclude that the strategy of professional development does have learning effects, though these are restricted to the individual level and do not involve the organisational level...|$|R
40|$|For {{the last}} fifteen years the field of machine {{learning}} has flourished and a number of complex and powerful learning systems have been developed. Knowledge level descriptions of such systems may be most useful for keeping an overview of the practical applicability of these systems. However, Dietterich's (1986) knowledge level analysis yielded very disappointing results: Learning could be characterized as an increase of knowledge in {{only a small number of}} deductive learning systems (e. g. MRS as a <b>deductive</b> <b>knowledge</b> level learning system - DKLL). The other systems were either not describable at the knowledge level (non-deductive knowledge level learning - NKLL) or did not show any increase in knowledge during learning (symbol level learning - SLL). In order to overcome these problems we took Clancey's (1991) criticisms into account and extended Newell's knowledge level into behavior descriptions, where skills and performance are used as additional parameters for characterizing behavior [...] ...|$|E
40|$|Contents 2 3 Introduction 1 1. 1 Logic-based User Modeling [...] 3 1. 1. 1 User Models and Knowledge Bases [...] . 3 1. 1. 2 User Modeling Shells and <b>Deductive</b> <b>Knowledge</b> Bases [...] . 4 1. 2 Other User Modeling Approaches [...] 6 1. 3 Assumption Types and Contents [...] 7 1. 4 User Modeling Shells: Power and Flexibility? [...] 8 1. 5 Overview [...] 12 Assumption Type Representation: a Review 2. 1 2. 2 2. 3 Types of Assumptions {{about the}} User [...] 15 Assumption Types and Graduation [...] . 18 The Partition Approach to Belief Modeling [...] . 20 2. 3. 1 Cohen's Nested Contexts [...] . 21 2. 3. 2 Contexts and Acceptance Attitudes in VIE-DPM [...] . 23 2. 3. 3 Partition {{hierarchies}} in BGP-MS [...] 25 2. 3. 4 Ballira's Nested Belief Models [...] 28 Combining Partitions and Logic [...] . 30 Logic for User Modeling 37 3. 1 Propositional Calculu...|$|E
40|$|This paper {{concerns}} a knowledge structure called method, within a computational model for human oriented deduction. With human oriented theorem proving cast as an interleaving process {{of planning and}} verification, the body of all methods reflects the reasoning repertoire of a reasoning system. While we adopt the general structure of methods introduced by Alan Bundy, we make an essential advancement in that we strictly separate the declarative knowledge from the procedural knowledge. This is achieved by postulating some standard types of knowledge we have identified, such as inference rules, assertions, and proof schemata, together with corresponding knowledge interpreters. Our approach in effect changes the way <b>deductive</b> <b>knowledge</b> is encoded: A new compound declarative knowledge structure, the proof schema, {{takes the place of}} complicated procedures for modeling specific proof strategies. This change of paradigm not only leads to representations easier to understand, it also enables us [...] ...|$|E
40|$|We {{study the}} {{expressive}} power of first order autoepistemic logic 1. We argue that full introspection of rational agents should {{be carried out}} by minimizing positive introspection and maximizing negative introspection. Based on full introspection, we propose the generalized stable semantics that characterizes autoepistemic reasoning processes of rational agents, and show that the breadth of the semantics covers all theories in autoepistemic logic of first order, Moore's AE logic, and Reiter's default logic. Our study demonstrates that autoepistemic logic of first order is a very powerful framework for nonmonotonic reasoning, logic programming, <b>deductive</b> databases, and <b>knowledge</b> representation. Key Words: nonmonotonic reasoning, autoepistemic logic, default logic, logic programming, knowledge representation 1 Introduction An idea rational agent has to decide which set of propositions to believe according to her knowledge. Moore's AE logic is a powerful framework for this kind of int [...] ...|$|R
40|$|Aims: The {{crisis in}} the medical {{paradigm}} was seen {{at the end of}} the 20 th century in the variability of practices, uncertainty of medical-decision making and difficulties in defining normality. Using changes in knowledge production as a marker, our aim was to characterize this paradigm crisis and to foresee its evolution. Methods: We reviewed Medline annually between 1976 and 2004, and counted the number of published case reports versus the number of literature reviews and clinical trial articles. Results : Our results show that of the articles indexed each year the proportion of literature review articles in 1994 was greater than case reports. This proportion is currently increasing. The proportion of case reports articles has fallen by half over the last twenty years. Clinical trial articles are increasing regularly and may exceed case reports in the next decade. Conclusions : The measurement of changes in the medical paradigm provided by our results show changes in the previous equilibrium: - the qualitative approach to disease is giving way to a quantitative approach; - knowledge development is moving towards a scientific process. From being inductive it is becoming <b>deductive.</b> This <b>knowledge</b> is changing and is stratifying through accumulation and review of primary knowledge from different teams; - research into etiologic certainties specific to the mechanistic paradigm is clashing with complexity, which is translated in the use of probabilities. Beyond large numbers, however, the clinical approach and human skills of the doctor remain fundamental prerequisites for care...|$|R
40|$|In {{this paper}} we present R-DEVICE, a <b>deductive</b> {{object-oriented}} <b>knowledge</b> base system for reasoning over RDF metadata. R-DEVICE imports RDF documents into the CLIPS production rule system by transforming RDF triples into COOL objects {{and uses a}} deductive rule language for reasoning about them. R-DEVICE {{is based on an}} OO RDF data model, different than the established triple-based model, which maps resources to objects and encapsulates properties inside resource objects, as traditional OO attributes. In this way, fewer joins are required to access the properties of a single resource resulting in better inferencing/querying performance, as it is experimentally shown in the paper. Furthermore, RDF can interoperate seamlessly with other web data models and languages. The descriptive semantics of RDF may call for dynamic redefinitions of resource classes, which are handled by R-DEVICE effectively. Furthermore, R-DEVICE features a powerful deductive rule language for reasoning on top of RDF metadata. The rule language includes features such as normal and generalized path expressions, stratified negation, aggregate, grouping, and sorting, functions. The rule language supports a second-order syntax, which is efficiently translated into sets of firstorder logic rules using metadata, where variables can range over classes and properties, so that reasoning over the RDF schema can be made. Users can define views which are materialized and incrementally maintained by translating deductive rules into CLIPS production rules that preserve truth. Users can choose between an OPS 5 /CLIPS-like and a RuleML-like syntax. Finally, users can define and use functions through the CLIPS host language...|$|R
40|$|The {{maintenance}} of semantic integrity {{has been recognized}} as a cornerstone issue {{for the development of}} databases and knowledge bases alike. Despite the extensive research conducted during the last two decades, semantic integrity maintenance has yet to become a practical technology. Furthermore, the need for modeling evolving domains has given rise to challenging research issues relating to the incorporation of time in knowledge bases. In this thesis, we study the problem of maintaining the integrity of temporal <b>deductive</b> <b>knowledge</b> bases. We argue that existing approaches in either temporal or deductive databases do not address the problem in a satisfactory manner, nor do they deal with all the issues involved in a unified framework. At first, we propose an assertion language that permits us to express different types of temporal assertions that are not expressible in other formalisms. We define the notion of temporal constraint satisfaction in a bitemporal context. We then follow two [...] ...|$|E
40|$|This paper {{concerns}} a knowledge structure called method, within a compu-tational model for human oriented deduction. With human oriented theoremproving cast as an interleaving process {{of planning and}} verification, the body ofall methods reflects the reasoning repertoire of a reasoning system. While weadopt the general structure of methods introduced by Alan Bundy, we make anessential advancement in that we strictly separate the declarative knowledgefrom the procedural knowledge. This is achieved by postulating some stand-ard types of knowledge we have identified, such as inference rules, assertions,and proof schemata, together with corresponding knowledge interpreters. Ourapproach in effect changes the way <b>deductive</b> <b>knowledge</b> is encoded: A newcompound declarative knowledge structure, the proof schema, takes the placeof complicated procedures for modeling specific proof strategies. This change ofparadigm not only leads to representations easier to understand, it also enablesus modeling the even more important activity of formulating meta-methods,that is, operators that adapt existing methods to suit novel situations. In thispaper, we first introduce briefly the general framework for describing methods. Then we turn to several types of knowledge with their interpreters. Finally,we briefly illustrate some meta-methods...|$|E
40|$|This work primary {{deals with}} Scotus` and Kants {{conception}} of science. The first part is introducing Aristotelian conception of science which Scotus` conception is based on. Through criticism of Henry of Ghent`s ilumination theory {{of knowledge is}} explained natural cognition, hold by John Duns Scotus for its a necesary part of Scotus` conception of science, which this work deals with {{at the end of}} the first part. Second part of this work contains an explanation of terms a priori/a posteriori, synthetic/analytic in the Kantian context. Based on the explanation of these terms is then shown Kants theory of general and necessary knowledge. Conclusion of the second part deals with Kantian conception of science. The third part deals with the comparison of ideas and shows differences in a priori knowledge, in conception of synthetic and analytic judgments in Scotuss and Kants theory. And in the end this work shows that in the concept of science in both of these philosophers is evident Aristotles principle of <b>deductive</b> <b>knowledge</b> of universal and necessary...|$|E
40|$|Abstract: The {{competence}} {{being investigated}} is causal modelling, whereby {{the behavior of}} a physical system is understood through the creation of an explanation or description of the underlying causal relations. After developing a model of causality, I show how the causal modelling competence can arise from a combination of inductive and <b>deductive</b> inference employing <b>knowledge</b> of the general form of causal relations and of the kinds of causal mechanisms that exist in a domain. The hypotheses generated by the causal modelling system range from purely empirical to more and more strongly justified. Hypotheses are justified by explanations derived from the domain theory and by perceptions which instantiate those explanations. Hypotheses never can be proven because the domain theory is neither complete nor consistent. Causal models which turn out to be inconsistent may be repairable by increasing the resolution of explanation and/or perception. During the causal modelling process, many hypotheses may be partially justified and even the leading hypotheses may have only minimal justification. An experiment design capability is proposed whereby the next observation can be deliberately arranged to distinguish several hypotheses or to make particular hypotheses more justified. Experimenting is seen as the active gathering of greate...|$|R
40|$|<b>Deductive</b> {{learners}} acquire <b>knowledge</b> that is implicitly {{available to}} improve {{the performance of the}} problem solver. One of the most known form of deductive learning is the acquisition of macro operators. Macro-operators carry cost as well as bene ts. When the costs outweigh the bene ts, we face the utility problem. The vast numberofmacrosavailable to the learner forces it to be selective toavoid the utility problem. The most common approach to selective macro-learning is using acquisition lters. Such lters try to estimate the utility of a macro before inserting it into the macro knowledge base. One problem with this approach isthatthe utility of a macro strongly depends on the problem being solved. In this work we suggest an alternative approach called utilization ltering. Instead of being selective when the macro is acquired, the learner is selective when the macro is utilized. We propose to use similarity-based ltering. A macro is considered as potentially useful for a particular problem if it proved to be useful for similar problems. Without further knowledge about the states in the search space,we suggest to use the heuristic function to determine similarity between states. Initial testing of this approach in the grid domain showed that indeed it is bene cial to delay selectivity tothe utilization stage. ...|$|R
40|$|The {{competence}} {{being investigated}} is causal modelling, whereby {{the behavior of}} a physical system is understood through the creation of an explanation or description of the underlying causal relations. After developing a model of causality, I show how the causal modelling competence can arise from a combination of inductive and <b>deductive</b> inference employing <b>knowledge</b> of the general form of causal relations and of the kinds of causal mechanisms that exist in a domain. The hypotheses generated by the causal modelling system range from purely empirical to more and more strongly justified. Hypotheses are justified by explanations derived from the domain theory and by perceptions which instantiate those explanations. Hypotheses never can be proven because the domain theory is neither complete nor consistent. Causal models which turn out to be inconsistent may be repairable by increasing the resolution of explanation and/or perception. During the causal modelling process, many hypotheses may be partially justified and even leading hypotheses may have only minimal justification. An experiment design capability is proposed whereby the next observation can be deliberately arranged to distinguish several hypotheses or to make particular hypotheses more justified. Experimenting is seen as the active gathering of greater justification for fewer and fewer hypotheses. MIT Artificial Intelligence Laborator...|$|R
