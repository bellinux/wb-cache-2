83|189|Public
50|$|<b>Data</b> <b>conditioning</b> {{builds on}} {{existing}} data storage functionality {{delivered in the}} I/O path including RAID (Redundant Arrays of Inexpensive Disks), intelligent I/O-based power management, and SSD (Solid-State Drive) performance caching techniques. <b>Data</b> <b>conditioning</b> is enabled both by advanced ASIC controller technology and intelligent software. New <b>data</b> <b>conditioning</b> capabilities can be designed into and delivered via storage controllers in the I/O path or to achieve the data center’s technical and business goals.|$|E
50|$|<b>Data</b> <b>conditioning</b> {{technologies}} delivered {{through a}} <b>Data</b> <b>Conditioning</b> Platform optimize data {{as it moves}} through a computer’s I/O (Input/Output) path or I/O bus—the data path between the main processor complex and storage subsystems. The functions of a <b>Data</b> <b>Conditioning</b> Platform typically reside on a storage controller add-in card inserted into the PCI-e slots of a server. This enables easy integration of new features in a server or a whole data center.|$|E
50|$|<b>Data</b> <b>conditioning</b> {{features}} delivered via a <b>Data</b> <b>Conditioning</b> Platform {{are designed}} to simplify system integration, and minimize implementation risks associated with deploying new technologies by ensuring seamless compatibility with all leading server and storage hardware, operating systems and applications, and meeting all current commercial/off-the-shelf (COTS) standards. By delivering optimization features via a <b>Data</b> <b>Conditioning</b> Platform, data center managers can improve system efficiency and reduce cost with minimal disruption and avoid the need to modify existing applications or operating systems, and leverage existing hardware systems.|$|E
30|$|Include {{the newly}} {{simulated}} value {{as part of}} the <b>conditioning</b> <b>data.</b>|$|R
5000|$|Temperature, Voltage and Pressure: Precision {{integrated}} <b>data</b> acquisition signal <b>conditioning</b> solutions ...|$|R
5000|$|Primary value learned value (PVLV). PVLV simulates {{behavioral}} and neural <b>data</b> on Pavlovian <b>conditioning</b> and the midbrain dopaminergic neurons that fire {{in proportion to}} unexpected rewards (an alternative to TD).|$|R
50|$|<b>Data</b> <b>conditioning</b> {{is the use}} of data {{management}} and optimization techniques which result in the intelligent routing, optimization and protection of data for storage or data movement in a computer system. <b>Data</b> <b>conditioning</b> features enable enterprise and cloud data centers to dramatically improve system utilization and increase application performance lowering both capital expenditures and operating costs.|$|E
50|$|<b>Data</b> <b>Conditioning</b> {{strategies}} {{can also be}} applied to improving server and storage utilization and for better managing {{a wide range of}} hardware and system-level capabilities.|$|E
50|$|<b>Data</b> <b>conditioning</b> {{principles}} can {{be applied}} to any demanding computing environment to create significant cost, performance and system utilization efficiencies, and are typically deployed by data center managers, system integrators, and storage and server OEMs seeking to optimize hardware and software utilization, simplified, non-intrusive technology integration, and minimal risks and performance hits traditionally associated with incorporating new data center technologies.|$|E
5000|$|... #Caption: The IBM Blue Gene/P {{supercomputer}} [...] "Intrepid" [...] at Argonne National Laboratory runs 164,000 {{processor cores}} using normal <b>data</b> center air <b>conditioning,</b> grouped in 40 racks/cabinets {{connected by a}} high-speed 3-D torus network.|$|R
40|$|AbstractTo make {{performance}}-based design decisions, {{an energy}} performance evaluation should {{be integrated into}} the design process. Simple tools can provide easy and rapid performance analysis without requiring detailed information as inputs. This paper presents a review of ten simple tools regarding their application, characteristics, modeling/calculation features and outputs. The key discussion focuses on the establishment of the necessary inputs. This study intends to clarify the effective approach of establishing those necessary inputs and find out the obstacles in this process. Note that building properties (BP) templates can extensively simplify the setup of construction data, internal load <b>data</b> and <b>conditioning</b> <b>data.</b> The lack of a method to efficiently establish energy setting parameters is recognized as the obstacle in the process of BP data setup. Reusing Building Information Model (BIM) data of past well-designed projects {{is expected to be a}} good approach to overcome that obstacle...|$|R
40|$|Multi-sensor {{advanced}} DInSAR analyses {{have been}} performed and compared with two GPS station measurements, in order to evaluate the land subsidence evolution in a 20 -year period, in the Alto Guadalentín Basin where {{the highest rate of}} man-induced subsidence (> 10 cm yr − 1) of Europe had been detected. The control mechanisms have been examined comparing the advanced DInSAR <b>data</b> with <b>conditioning</b> and triggering factors (i. e. isobaths of Plio-Quaternary deposits, soft soil thickness and piezometric level) ...|$|R
50|$|In {{addition}} to these scientific instruments, Mariner 2 had a <b>data</b> <b>conditioning</b> system (DCS) and a scientific power switching (SPS) unit. The DCS was a solid-state electronic system designed to gather information from the scientific instruments on board the spacecraft. It had four basic functions: analog-to-digital conversion, digital-to-digital conversion, sampling and instrument-calibration timing, and planetary acquisition. The SPS unit was designed to perform the following three functions: control of the application of AC power to appropriate portions of the science subsystem, application of power to the radiometers and removal of power from the cruise experiments during radiometer calibration periods, {{and control of the}} speed and direction of the radiometer scans. The DCS sent signals to the SPS unit to perform the latter two functions.|$|E
40|$|Graphics <b>data</b> <b>conditioning</b> program expedites {{engineering}} {{analysis of}} flight data and ensures timely correction of measurement errors. By adding interactive computer graphic displays to existing <b>data</b> <b>conditioning</b> programs, computational results are immediately visible, enabling on-line intervention {{and control of}} computer processing...|$|E
40|$|Processed with LATEX on 2006 / 03 / 30 LIGO-T 060011 - 00 We {{present the}} <b>data</b> <b>conditioning</b> {{procedure}} {{used by the}} Penn State group in the burst search pipeline for the BlockNormal event trigger generator. This <b>data</b> <b>conditioning</b> procedure {{was used in the}} analysis of S 2, S 3 and S 4 data. The procedure consists of several steps which whiten the data and break it up into several non-overlapping frequency bands. For the case of S 3 we calculate a figure of merit for all freqency bands and show that the <b>data</b> <b>conditioning</b> procedure is successful in improving the whiteness of the data most of the time. Content...|$|E
50|$|The PowerLab {{system is}} {{essentially}} a peripheral device designed to perform various functions needed for <b>data</b> acquisition, signal <b>conditioning</b> and pre-processing. Versatile display options and analysis functions are complemented by the ability to export data to other software (such as Microsoft Excel).|$|R
30|$|MPS {{techniques}} {{are not only}} capable of handing the prior maximum entropy models and performing multivariate Gaussian algorithms, but they can act very effectively in using the prior structural models of low entropy having more compatibility with hard <b>conditioning</b> <b>data</b> or prior geological models.|$|R
40|$|This paper {{combines}} {{the use of}} portfolio holdings <b>data</b> and <b>conditioning</b> information {{to create a new}} performance measure. Our conditional weight-based measure has several advantages. Using conditioning information avoids biases in weight-based measures as discussed by Grinblatt and Titman (1993). When conditioning information is used, returns-based measures face a bias if managers can trade between observation dates. The new measures avoid this interim trading bias. We use the new measures to provide fresh insights about performance in a sample of U. S. equity pension fund managers. ...|$|R
40|$|<b>Data</b> <b>conditioning</b> is {{the process}} of making data fit for the purpose for which it is to be used and forms a {{significant}} component of the G-BASE project. This report is part of a series of manuals to record G-BASE project methodology. For <b>data</b> <b>conditioning</b> this has been difficult as applications used for processing data {{and the way in which}} data are reported continue to evolve rapidly and sections of this report have had to be continually updated to reflect this fact. However, the principals of <b>data</b> <b>conditioning</b> have changed little since the BGS regional geochemical mapping started in the late 1960 s. The process of <b>data</b> <b>conditioning</b> is based on one or more quality control procedures applied to the geochemical results as received from the laboratories, the degree of conditioning depending on how the data is to be used. The task is based on "blind" control samples being inserted prior to analysis, a system of quality control described in the G-BASE procedures manual. The first of the <b>data</b> <b>conditioning</b> processes is data verification and error checking, essentially assessing whether the laboratory has done what it was asked to do and results are being reported with reasonable accuracy. Shewhart or control charts form an important part of this process. Once the data has been error checked, verified and accepted from the laboratory, further analysis of the data is carried out. These processes include: a series of x-y plots (of duplicate and replicate samples), more detailed control chart plots, and ANOVA analysis of the duplicate/replicate pairs to allocate variance in the results to sampling, analytical or between site variability. Analysis of both primary and secondary reference material can quantify analytical accuracy and precision. An important part of the <b>data</b> <b>conditioning</b> is the quality assurance and this includes procedures used for dealing with results that have data quality issues and documenting all parts of the <b>data</b> <b>conditioning</b> procedure. The final part of the <b>data</b> <b>conditioning</b> procedure is necessary in order to use the data in context of other previously analysed data sets. This {{is the process}} of normalisation and levelling of the data. In G-BASE this is a very necessary step in order to create seamless geochemical maps and images across campaign boundaries and varying analytical methodologies that have spanned several decades...|$|E
30|$|Many {{drawbacks}} {{are inherent}} to these models; independence assumption between the parameters, difficulties in inferring the statistical distribution parameters, problems in soft <b>data</b> <b>conditioning,</b> and convex and planar assumption of individual fractures are {{limitations of this}} method.|$|E
40|$|Computer based <b>data</b> <b>conditioning</b> {{and display}} {{technique}} detects incipient failure in servo system testing, {{for use in}} prelaunch checkout of complex nonlinear servomechanisms. These phase plane displays enable identification of, on line, unusual or abnormal servo responses which can be displayed compactly in the time domain on a cathode ray tube...|$|E
40|$|A {{number of}} fast {{algorithms}} for computing the Moore-Penrose inverse of structured and block matrices have been designed. However, very {{often they are}} not accurate up to the limitations of <b>data</b> and <b>conditioning</b> of the problem. Thus procedures of improving the accuracy and stability of algorithms are necessary. We consider some ways in which iterative refinement {{may be used to}} improve the computed results. Extensive numerical testing was done in Matlab to compare the performance of some direct and iterative methods for computing the Moore-Penrose inverse of special matrices...|$|R
40|$|An {{efficient}} algorithm of tidal harmonic analysis and prediction {{is presented in}} this paper. Some conditions are found {{by means of the}} known approximate relationships between the harmonic constants of the tidal constituents. A system of linear equations for least squares solutions under these restricted conditions is obtained. In the case of inadequate <b>data,</b> ill <b>conditioning</b> in the system of equations that has appeared in other algorithms is conveniently avoided. In solving the resultant normal equations, the Goertzel iteration is adopted so that the whole computation time is dramatically reduced...|$|R
40|$|Difficulties in {{defining}} truly mechanistic model structures and difficulties of model calibration and validation {{suggest that the}} application of distributed hydrological models is more an exercise in prophecy than prediction. One response to these problems is outlined {{in terms of a}} realistic assessment of uncertainty in hydrological prophecy, together with a framework (GLUE) within which such ideas can be implemented. It is suggested that a post-modernistic hydrology will recognise the uncertainties inherent in hydrological modelling and will focus attention on the value of <b>data</b> in <b>conditioning</b> hydrological prophecies...|$|R
40|$|An {{overview}} of some tools and techniques being developed for <b>data</b> <b>conditioning</b> (regression of instrumental and environmental {{artifacts from the}} data channel), detector design evaluation (modeling the science "reach" of alternative detector designs and configurations), noise simulations for mock data challenges and analysis system validation, and analyses {{for the detection of}} gravitational radiation from gamma-ray burst sources...|$|E
40|$|Psychological concepts, such as anxiety, can be {{measured}} ‘in the wild’ using a range of wearable sensors. The measurement of psychophysiological signals in the field naturally includes a number of confounds such as noise, artefacts and baseline wander. Before getting a general idea {{of what may be}} possible from working with the data, it is necessary to use <b>data</b> <b>conditioning</b> methods to remove these unwanted influences...|$|E
40|$|We {{present the}} noise {{analysis}} package library (NAP), a new tool to perform noise studies and <b>data</b> <b>conditioning</b> {{on the data}} produced by gravitational wave detectors. We describe its design and report {{the results of the}} application of parametric spectral estimation, whitening, line removal with adaptive notch filters and noise removal using a multicoherence procedure on the data taken by the Virgo interferometer during the C 5 engineering run...|$|E
50|$|The primary value learned value (PVLV) {{model is}} a {{possible}} explanation for the reward-predictive firing properties of dopamine (DA) neurons. It simulates behavioral and neural <b>data</b> on Pavlovian <b>conditioning</b> and the midbrain dopaminergic neurons that fire in proportion to unexpected rewards. It is an alternative to the temporal-differences (TD) algorithm.|$|R
40|$|This paper {{examines}} {{the importance of}} space to per capita GDP growth in Brazil for the period 1980 – 2004 at the micro-regional level. The role of space is investigated by applying a spatial filter that eliminates the spatial dependence of the data and allows comparison with the original <b>data.</b> The <b>conditioning</b> variables become insignificant after removing spatial dependence. This suggests that the statistical significance of the growth determinants is intrinsically linked to geographical location and indicates the importance of space to regional growth in Brazil. Moreover, these {{results show that the}} convergence process is different across spatial regimes after removing this dependence...|$|R
40|$|Suitable {{training}} images (TIs) for multiple-point statistics (MPS) {{are difficult}} to identify in real-case three-dimensional applications, posing challenges for modelers trying to develop realistic subsurface models. This study demonstrates that two-dimensional geophysical images, when employed as training and <b>conditioning</b> <b>data,</b> can provide sufficient information for three-dimensional MPS simulations. The advantage of such a data-driven approach {{is that it does}} not rely on any external (possibly inappropriate) TI. The disadvantage is that three-dimensional MPS simulations must be carried out based on two-dimensional information. Three different approaches (two existing, one new) are tested to overcome this problem. The two existing approaches rely on three-dimensional reconstruction of incomplete datasets and on sequential two-dimensional simulations, respectively. The third approach is a newly proposed combination of the two former approaches. The three approaches are applied to model the three-dimensional facies structure of an alluvial aquifer based on high-resolution ground-penetrating radar cross-sections. The quality of each simulation outcome is evaluated based on the similarity of its multiple-point histogram (MPH) to reference MPHs derived from geophysical images. This evaluation reveals that the first approach (three-dimensional reconstruction) performs well close to <b>conditioning</b> <b>data,</b> but farther away from the data the simulation results deteriorate. Quite conversely, the second approach (sequential two-dimensional) performs well when only few <b>conditioning</b> <b>data</b> exist, but with increasing simulation sequence the quality decreases. The newly proposed third approach integrates the benefits of both approaches and is found to reproduce the reference MPHs significantly better than either of the two other approaches alone...|$|R
40|$|We {{estimate}} dynamic {{effects of}} works councils on labor productivity using newly available information from West German establishment panel <b>data.</b> <b>Conditioning</b> on plant fixed effects and control variables, we find negative productivity effects {{during the first}} five years after council introduction, but a steady and substantial increase in the councils' productivity effect thereafter. Given the frequently reported positive correlation between council existence and plant productivity, this finding supports causal interpretations...|$|E
40|$|We {{consider}} {{the process of}} setting up a traffic simulation based on traffic measurements. This process consists of several steps: data acquisition, interfacing, <b>data</b> <b>conditioning,</b> modeling and simulation. We discuss some important aspects that {{should be taken into}} account in each step of the process, and we also illustrate the entire process using a real-life traffic example. Copyright c fl 2000 IFAC Keywords: data processing, data acquisition, preprocessing, computer simulation, filtering, interpolation, road traffic 1...|$|E
40|$|A {{case study}} from the Tertiary of the North Sea is {{presented}} in which a well tie is improved {{through the application of}} zero offset processing and well log velocity prediction. The well ties are quantified using the techniques of White (1980) and White and Walden (1984) and log conditioning employs the Xu-White sand/clay velocity model (Xu and White, 1995 and 1996). A well tie of a migrated stack section to a zero offset synthetic, produced with minimal <b>data</b> <b>conditioning,</b> gave a poor tie and event identification was in doubt. Additional <b>data</b> <b>conditioning</b> improved the tie dramatically (from 43 % to 72 % energy predicted and a reduction in phase error on the wavelet estimation from> 20 o to < 10 o). These enhancements in the well tie are important in justifying decisions to perform wavelet deconvolution, zero phasing or seismic trace inversion to acoustic impedance as well as correctly identifying the top reservoir reflection. This study also demonstrates that detailed log conditioning is crucial in this area to predict a diagnostic AVO response related to the presence of hydrocarbons. This study uses a Tertiary example from the North Sea to illustrate the importance of dat...|$|E
40|$|The {{aim of this}} {{research}} is improve quality of biology learning for conditioning class, student attitude in class, the performance of teacher and student motivation of learning in student class VII-D 1 st Junior High School Of Jaten through the application of strategies for inquiry learning combined audio visual media. This research was classroom action research with planning, action, observation, and reflection steps. Data was collected using questionnaire, observation, and interview. The validation of data using method and observer triangulation techniques. The data analyzed by descriptive. The result in cycles I describes that mean of observation <b>data</b> in <b>conditioning</b> class indicators are 70, 20...|$|R
40|$|Stochastic {{modeling}} of interdependent continuous spatial attributes is now routinely {{carried out in}} the minerals industry through multi-Gaussian conditional simulation algorithms. However, transformed <b>conditioning</b> <b>data</b> frequently violate multi-Gaussian assumptions in practice, resulting in poor reproduction of correlation between variables in the resultant simulations. Furthermore, the maximum entropy property that is imposed on the multi-Gaussian simulations is not universally appropriate. A new Direct Sequential Cosimulation algorithm is proposed here. In the proposed approach, pair-wise simulated point values are drawn directly from the discrete multivariate conditional distribution under an assumption of intrinsic correlation with local Ordinary Kriging weights used to inform the draw probability. This generates multivariate simulations with two potential advantages over multi-Gaussian methods: (1) inter-variable correlations are assured because the pair-wise inter-variable dependencies within the untransformed <b>conditioning</b> <b>data</b> are embedded directly into each realization; and (2) the resultant stochastic models are not constrained by the maximum entropy properties of multi-Gaussian geostatistical simulation tools...|$|R
40|$|The use of {{additional}} types of observational data {{has often been}} suggested to alleviate the ill-posedness inherent to parameter estimation of groundwater models and constrain model uncertainty. Disinformation in observational data caused by errors in either the observations or the chosen model structure may, however, confound the value of adding observational <b>data</b> in model <b>conditioning.</b> This paper uses the global generalized likelihood uncertainty estimation methodology to investigate the value of different observational data types (heads, fluxes, salinity, and temperature) in conditioning a groundwater flow and transport model of an extensively monitored field site in the Netherlands. We compared model conditioning using the real observations to a synthetic model experiment, to demonstrate the possible influence of disinformation in observational <b>data</b> in model <b>conditioning.</b> Results showed {{that the value of}} different conditioning targets was less evident when conditioning to real measurements than in a measurement error-only synthetic model experiment. While in the synthetic experiment, all conditioning targets clearly improved model outcomes, minor improvements or even worsening of model outcomes was observed for the real measurements. This result was caused by errors in both the model structure and the observations, resulting in disinformation in the observational data. The observed impact of disinformation in the observational data reiterates the necessity of thorough data validation and the need for accounting for both model structural and observational errors in model conditioning. It further suggests caution when translating results of synthetic modeling examples to real-world applications. Still, applying diverse <b>conditioning</b> <b>data</b> types was found to be essential to constrain uncertainty, especially in the transport of solutes in the model...|$|R
