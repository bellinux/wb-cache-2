1951|1683|Public
5|$|If all entries of A {{below the}} main {{diagonal}} are zero, A {{is called a}}n upper triangular matrix. Similarly if all entries of A above the main diagonal are zero, A is called a lower triangular matrix. If all entries outside the main diagonal are zero, A is called a <b>diagonal</b> <b>matrix.</b>|$|E
5|$|The LU {{decomposition}} factors matrices as {{a product}} of lower (L) and an upper triangular matrices (U). Once this decomposition is calculated, linear systems can be solved more efficiently, by a simple technique called forward and back substitution. Likewise, inverses of triangular matrices are algorithmically easier to calculate. The Gaussian elimination is a similar algorithm; it transforms any matrix to row echelon form. Both methods proceed by multiplying the matrix by suitable elementary matrices, which correspond to permuting rows or columns and adding multiples of one row to another row. Singular value decomposition expresses any matrix A {{as a product}} UDV∗, where U and V are unitary matrices and D is a <b>diagonal</b> <b>matrix.</b>|$|E
25|$|In the {{spectral}} decomposition of matrices, lambda indicates the <b>diagonal</b> <b>matrix</b> of the eigenvalues of the matrix.|$|E
50|$|The set of n&times;n {{generalized}} permutation matrices with {{entries in}} a field F forms a subgroup of the general linear group GL(n,F), in which the group of nonsingular <b>diagonal</b> <b>matrices</b> Δ(n, F) forms a normal subgroup. Indeed, the generalized permutation matrices are the normalizer of the <b>diagonal</b> <b>matrices,</b> meaning that the generalized permutation matrices are the largest subgroup of GL in which <b>diagonal</b> <b>matrices</b> are normal.|$|R
40|$|AbstractMany {{people would}} say that Calderón–Zygmund {{operators}} have almost <b>diagonal</b> <b>matrices</b> in orthonormal wavelet bases. We will show that this statement is not true as stated. In contrast, the "nonstandard matrix representation" of Calderón–Zygmund operators always yields almost <b>diagonal</b> <b>matrices.</b> The Beyklin–Coifman–Rokhlin fast algorithm amounts to replacing these almost <b>diagonal</b> <b>matrices</b> by banded ones. We compute the operator norm of the error term in this approximation and give sharp estimates...|$|R
2500|$|Let k = R or C. Then SL(2,k) is {{generated}} by the three subgroups of lower and upper unitriangular matrices, L and U', and the <b>diagonal</b> <b>matrices</b> D. It is also generated by the lower (or upper) unitriangular <b>matrices,</b> the <b>diagonal</b> <b>matrices</b> and the matrix ...|$|R
25|$|Every square <b>{{diagonal}}</b> <b>matrix</b> is symmetric, {{since all}} off-diagonal entries are zero. Similarly, each diagonal {{element of a}} skew-symmetric matrix must be zero, since each is its own negative.|$|E
25|$|A can {{therefore}} be decomposed into a matrix composed of its eigenvectors, a <b>diagonal</b> <b>matrix</b> with its eigenvalues along the diagonal, and the inverse of the matrix of eigenvectors. This is called the eigendecomposition {{and it is a}} similarity transformation. Such a matrix A is said to be similar to the <b>diagonal</b> <b>matrix</b> Λ or diagonalizable. The matrix Q is the change of basis matrix of the similarity transformation. Essentially, the matrices A and Λ represent the same linear transformation expressed in two different bases. The eigenvectors are used as the basis when representing the linear transformation asΛ.|$|E
25|$|A matrix {{that is not}} {{diagonalizable}} {{is said to be}} defective. For defective matrices, {{the notion}} of eigenvectors generalizes to generalized eigenvectors and the <b>diagonal</b> <b>matrix</b> of eigenvalues generalizes to the Jordan normal form. Over an algebraically closed field, any matrix A has a Jordan normal form and therefore admits a basis of generalized eigenvectors and a decomposition into generalized eigenspaces.|$|E
50|$|The matrix and its inverse are <b>diagonal</b> <b>matrices.</b>|$|R
2500|$|Using the <b>diagonal</b> <b>matrices</b> in this {{construction}} {{defines a}} Cartan subalgebra [...] of : {{the rank of}} [...] is , and the <b>diagonal</b> [...] <b>matrices</b> determine an -dimensional abelian subalgebra.|$|R
40|$|AbstractExact {{values of}} the average linear n-widths {{with respect to the}} {{standard}} Gaussian measure on Rm are determined for <b>diagonal</b> <b>matrices,</b> and are applied to deduce several new results on linear n-widths in the average and probabilistic settings, including the sharp upper and lower estimates of the linear (n,δ) -widths of <b>diagonal</b> <b>matrices...</b>|$|R
25|$|Such a {{transformation}} {{is called a}} diagonalizable matrix since in the eigenbasis, the transformation is represented by a <b>diagonal</b> <b>matrix.</b> Because operations like matrix multiplication, matrix inversion, and determinant calculation are simple on diagonal matrices, computations involving matrices are much simpler if we can bring the matrix to a diagonal form. Not all matrices are diagonalizable (even over an algebraically closed field).|$|E
25|$|F is {{the space}} of p by q {{matrices}} over R with p ≠ q. In this case L(a,b)c= ab't'c + cb't'a with inner product (a,b) = Tr ab't. This is Koecher's construction for the involution on E = H'p + q(R) given by conjugating by the <b>diagonal</b> <b>matrix</b> with p digonal entries equal to 1 and q to −1.|$|E
25|$|If one of {{the degrees}} is zero (that is the {{corresponding}} polynomial is a nonzero constant), then there are zero rows consisting of coefficients of the other polynomial, and the Sylvester matrix is a <b>diagonal</b> <b>matrix</b> of dimension the degree of the non-constant polynomial, with the all diagonal coefficients equal to the constant polynomial. If m = n = 0, then the Sylvester matrix is the empty matrix with zero rows and zero columns.|$|E
30|$|W| 0,Λ,D) by Equation (26), {{while both}} Λ,D are <b>diagonal</b> <b>matrices.</b>|$|R
40|$|AbstractAn {{infinite}} matrix is regular {{if it is}} a limit preserving {{transformation in}} the domain of convergent sequences. A classical Hausdorff theorem states that some <b>matrices</b> composed of <b>diagonal</b> <b>matrices</b> and matrices, whose elements are defined as alternating Newton symbols, are regular iff the diagonal elements of the above <b>diagonal</b> <b>matrices</b> are consecutive moments of a Stieltjes integral. In this paper an analogous theorem for multiindex infinite matrices is proved...|$|R
5000|$|... can be {{described}} explicitly. In terms of generators <b>diagonal</b> <b>matrices</b> act as ...|$|R
25|$|Conversely, {{suppose a}} matrix A is diagonalizable. Let P be a {{non-singular}} square matrix such that P−1AP is some <b>diagonal</b> <b>matrix</b> D. Left multiplying both by P, AP = PD. Each column of P {{must therefore be}} an eigenvector of A whose eigenvalue is the corresponding diagonal element of D. Since the columns of P must be linearly independent for P to be invertible, there exist n linearly independent eigenvectors of A. It then follows that the eigenvectors of A form a basis {{if and only if}} A is diagonalizable.|$|E
25|$|The {{condition}} that γ'A(λ) ≤ μ'A(λ) can be proven by considering a particular eigenvalue ξ of A and diagonalizing the first γ'A(ξ) columns of A {{with respect to}} the eigenvectors of ξ, described in a later section. The resulting similar matrix B is block upper triangular, with its top left block being the <b>diagonal</b> <b>matrix</b> ξI'γ'A(ξ). As a result, the characteristic polynomial of B will have a factor of (ξ−λ)γ'A(ξ). The other factors of the characteristic polynomial of B are not known, so the algebraic multiplicity of ξ as an eigenvalue of B is no less than the geometric multiplicity of ξ as an eigenvalue of A. The last element of the proof is the property that similar matrices have the same characteristic polynomial.|$|E
25|$|If the {{operator}} is originally {{given by a}} square matrix M, then its Jordan normal form is also called the Jordan normal form of M. Any square matrix has a Jordan normal form if the field of coefficients is extended to one containing all the eigenvalues of the matrix. In spite of its name, the normal form for a given M is not entirely unique, {{as it is a}} block <b>diagonal</b> <b>matrix</b> formed of Jordan blocks, the order of which is not fixed; it is conventional to group blocks for the same eigenvalue together, but no ordering is imposed among the eigenvalues, nor among the blocks for a given eigenvalue, although the latter could for instance be ordered by weakly decreasing size.|$|E
5000|$|There is a {{generalization}} of this action of SL(2,C) to A and its compactification X. In order to define this action, note that SL(2,C) {{is generated by}} the three subgroups of lower and upper unitriangular <b>matrices</b> and the <b>diagonal</b> <b>matrices.</b> It is also generated by the lower (or upper) unitriangular <b>matrices,</b> the <b>diagonal</b> <b>matrices</b> and the matrix ...|$|R
3000|$|... are <b>diagonal</b> <b>matrices.</b> Volatility {{is modeled}} without {{interaction}} between the assets to simplify the model.|$|R
50|$|In particular, the <b>diagonal</b> <b>matrices</b> form a subring of {{the ring}} of all n-by-n matrices.|$|R
500|$|It is {{a square}} matrix of order n, {{and also a}} special kind of <b>diagonal</b> <b>matrix.</b> It is called an {{identity}} matrix because multiplication with it leaves a matrix unchanged: ...|$|E
500|$|... and {{the power}} of a <b>diagonal</b> <b>matrix</b> can be {{calculated}} by taking the corresponding powers of the diagonal entries, which is much easier than doing the exponentiation for A instead. This can be used to compute the matrix exponential eA, a need frequently arising in solving linear differential equations, matrix logarithms and square roots of matrices. To avoid numerically ill-conditioned situations, further algorithms such as the Schur decomposition can be employed.|$|E
500|$|The eigendecomposition or diagonalization expresses A as {{a product}} VDV−1, where D is a <b>diagonal</b> <b>matrix</b> and V is a {{suitable}} invertible matrix. [...] If A can be written in this form, it is called diagonalizable. More generally, and applicable to all matrices, the Jordan decomposition transforms a matrix into Jordan normal form, {{that is to say}} matrices whose only nonzero entries are the eigenvalues λ1 to λn of A, placed on the main diagonal and possibly entries equal to one directly above the main diagonal, as shown at the right. Given the eigendecomposition, the nth power of A (that is, n-fold iterated matrix multiplication) can be calculated via ...|$|E
5000|$|Using the {{notation}} that is sometimes used to concisely describe <b>diagonal</b> <b>matrices,</b> we can write: ...|$|R
5000|$|For a Jordan frame , let [...] There is {{an action}} of [...] on [...] which extends to V. If [...] and , then [...] and [...] give {{the action of}} the product of the lower and upper unitriangular matrices. If [...] with , then the {{corresponding}} product of <b>diagonal</b> <b>matrices</b> act as , where [...] In particular the <b>diagonal</b> <b>matrices</b> give an action of [...] and [...]|$|R
5000|$|To {{incorporate}} likelihood (unary) terms {{into the}} algorithm, {{it was shown}} in [...] that one may optimize the energyfor positive, <b>diagonal</b> <b>matrices</b> [...] and [...] Optimizing this energy leads to the system of linear equationsThe set of seeded nodes, , may be empty in this case (i.e., [...] ), but {{the presence of the}} positive <b>diagonal</b> <b>matrices</b> allows for a unique solution to this linear system.|$|R
2500|$|Furthermore, because Λ is a <b>diagonal</b> <b>matrix,</b> its inverse {{is easy to}} calculate: ...|$|E
2500|$|In general, {{a square}} complex matrix A {{is similar to}} a block <b>diagonal</b> <b>matrix</b> ...|$|E
2500|$|... every matrix in H0 is {{conjugate}} to a <b>diagonal</b> <b>matrix</b> by {{a matrix}} M in H0.|$|E
5000|$|The unitary group U(n) has as a maximal torus the {{subgroup}} of all <b>diagonal</b> <b>matrices.</b> That is, ...|$|R
50|$|Decomposition: , where S is doubly {{stochastic}} and D1 and D2 {{are real}} <b>diagonal</b> <b>matrices</b> with strictly positive elements.|$|R
40|$|AbstractIn {{this paper}} the {{construction}} of <b>diagonal</b> <b>matrices,</b> in some sense approximating the inverse of a given square matrix, is described. The matrices are constructed using the well-known computer algebra system Maple. The techniques we show are applicable to square matrices in general. Results are given for use in Parallel diagonal-implicit Runge-Kutta (PDIRK) methods. For an s-stage Radau IIA corrector we conjecture s! possibilities for the <b>diagonal</b> <b>matrices...</b>|$|R
