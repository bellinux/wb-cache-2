4|4|Public
40|$|We compare two {{different}} models for {{assets and liabilities}} for an in-surance company that can {{be considered in the}} standard approach to solvency assessment and in particular, in determining the required target capital. The first model is suggested by a joint working party by members in CEA, Comite ́ Européen des Assurances, and is based on the duration concept and the second one is based on ideas from Arbitrage Pricing The-ory (APT). An application of these approaches to two specific insurance contracts indicates that, among other things, the <b>duration-based</b> <b>approach</b> to solvency assessment suggests larger target capital requirement than the one based on APT. Acknowledgments: The {{authors would like to thank}} Christer Borell and Sture Holm, Chalmers University of Technology, Arne Sandström, Swedish In-surance Federation, and Christer Stolt, Länsförsäkringar, for valuable discus-sions and support. ∗Supported by the Swedish Insurance Federation...|$|E
40|$|Dynamic {{development}} {{in the area of}} value-at-risk (VaR) estimation and growing implementation of VaR-based risk valuation models in investment companies stimulate the need for statistical methods of VaR models evaluation. Following recent changes in Basel Accords, current UE banking supervisory regulations require internal VaR model backtesting, which provides another strong incentive for research on relevant statistical tests. Previous studies have shown that commonly used VaR independence Markov-chain-based testing procedure exhibits low power, which constitutes a particularly serious problem in the case of finite-sample settings. In the paper, {{as an alternative to the}} popular Markov test an overview of the group of duration-based VaR backtesting procedures is presented along with exploration of their statistical properties while rejecting a non-realistic assumption of infinite sample size. The Monte Carlo test technique was adopted to provide exact tests, in which asymptotic distributions were replaced with simulated finite sample distributions. A Monte Carlo study, based on the GARCH model, was designed to investigate the size and the power of the tests. Through the comparative analysis we found that, in the light of observed statistical properties, the <b>duration-based</b> <b>approach</b> was superior to the Markov test...|$|E
40|$|International audienceWhat we {{will call}} the - is {{computed}} conventionally by adding up age-specific marital fertility rates {{in the hope of}} estimating the number of children ever born to a woman who is married throughout her childbearing years. Demographers have long been strongly skeptical about this quantity because it normally indicates implausibly many children. Our analysis of data from the Romanian GGS confirms this finding, and we propose an alternative - computed in the spirit of parity-progression ratios. At the same time, we extend the method to cover any type of living arrangement (cohabitation, marriage, non-partnered arrangement, and so on). Because each resulting total union-type fertility rate () explicitly accounts for the living arrangement, it improves on the conventional total fertility rate (TFR), which does not. We embed the investigation in an event-history analysis with fixed and time-varying control covariates and find patterns of relative risks for such variables that reveal interesting features of childbearing behavior in the Romanian data, which we use to illustrate the method. In most cases, these patterns are quite robust against model re-specification, including the shift from the age-based to the <b>duration-based</b> <b>approach.</b> Since, the number of female respondents is "only" about 6, 000 (minus records that cannot be used for the current purpose) in a normal single-round GGS, there is considerable inherent random variation in the data set, but we show that simple few-term moving average graduation suffices to overcome this problem...|$|E
40|$|Charging schemes {{derived from}} {{theoretical}} considerations include both usage and <b>duration-based</b> <b>approaches.</b> This deliverable considers schemes from this two approaches. Combinations of volume and duration based charging also {{feature in the}} charging schemes developed for specific case studies, which include a LAN interconnect using DBR and SBR, Intranet over ATM and Multi-media to the desktop with browse, loan, buy and play components. Evaluations are performed {{on the basis of}} techno-economic analyses, charging scheme criteria, and experimental and theoretical assessments. The deliverable rounds off with a discussion on MIB requirements, measurement points and the relationship of charging schemes to service components. Keywords: charging algorithms, static charging schemes, charging parameters, evaluation criteria...|$|R
40|$|In this paper, we give an {{overview}} of the state-of-the-art in the econometric literature on the modeling of so-called financial point processes. The latter are associated with the random arrival of specific financial trading events, such as transactions, quote updates, limit orders or price changes observable based on financial high-frequency data. After discussing fundamental statistical concepts of point process theory, we review durationbased and intensity-based models of financial point processes. Whereas <b>duration-based</b> <b>approaches</b> are mostly preferable for univariate time series, intensity-based models provide powerful frameworks to model multivariate point processes in continuous time. We illustrate the most important properties of the individual models and discuss major empirical applications. Financial point processes, dynamic duration models, dynamic intensity models. ...|$|R
40|$|Currently, {{the most}} common {{approach}} used in project planning tools is the Critical Path Method (CPM). While this method was a great improvement over the basic Gantt chart technique being used at the time, it now suffers from three primary flaws: (1) task duration is an input, (2) productivity impacts are not considered, and (3) management corrective actions are not included. Today, computers have exceptional computational power to handle complex simulations of task e) (eculion and project management activities (e. g., dynamically changing the number of resources assigned to a task when it is behind schedule). Through research under a Department of Defense contract, the author and the ViaSim team have developed a project simulation tool that enables more realistic cost and schedule estimates by using a resource-based model that literally turns the current <b>duration-based</b> CPM <b>approach</b> "on its head. " The approach represents a fundamental paradigm shift in estimating projects, managing schedules, and reducing risk through innovative predictive techniques...|$|R
40|$|Asset {{variance}} and covariance {{are fundamental}} for {{financial risk management}} and many finance applications. With the advent of tick-by-tick high-frequency data, the estimation of univariate variances and multivariate covariance matrices has attracted more attention from econometricians. Many of the proposed high-frequency variance and covariance estimators are based on time-domain measurements. In this thesis, we investigate variance and covariance estimators constructed on the price domain: the price duration based variance and covariance estimators. A price event occurs when the absolute cumulative price change equals or exceeds a pre-specified threshold value. The time taken between two consecutive price events is a price duration. Intuitively, shorter durations are indicative of higher volatility. The <b>duration-based</b> <b>approach</b> provides a new angle {{to look at the}} high-frequency data, additionally, the duration based variance and covariance estimators are shown to be more efficient than competing time-domain high-frequency estimators. The information advantage of the duration based approach is demonstrated through two empirical applications, a volatility forecasting exercise and an out-of-sample globalminimum-variance portfolio allocation problem. The duration based estimators are shown to provide both better forecasting performance and better portfolio allocation results. The paper in Chapter 2 is under the first round Revise&Resubmit to the Journal of Business & Economic Statistics. In Chapter 2, we discuss the estimation of univariate variance using price durations. Variance estimation using high-frequency data needs {{to take into account the}} effect of market microstructure (MMS) noise, including discrete transaction times, discrete price levels, and bid/ask spreads, as well as price jumps. The price duration estimator has a built-in feature to be robust to large price jumps, while its robustness against the MMS noise is achieved through a careful selection of the threshold value that defines a price event. We discuss the selection of this optimal threshold value through both simulation and empirical evidence. We devise both a non-parametric and a parametric estimator. For the estimation of integrated variance at a daily frequency, the non-parametric duration based variance estimator suffices, while the parametric estimator additionally provides us with an instantaneous variance estimator. As an empirical application to 20 DJIA stocks, we compare the volatility forecasting performance of three classes of volatility estimators, including the realized volatility, the option implied volatility, and the price duration based volatility estimators, on one-day, one-week, and one-month horizons. Forecasting comparisons among individual estimators, as well as in a combination setup, are considered. The duration based estimators, especially the parametric price duration volatility estimator, are found to provide more accurate out-of-sample forecasts. In Chapter 3, we introduce a covariance matrix estimator using price durations. In the multivariate setting, there is the additional issue of nonsynchronous trade arrival times when estimating a high-dimensional variance-covariance matrix using tick-by-tick transaction data. Through simulation, we assess the effects of the lasttick time-synchronization method and MMS noise on the duration based covariance estimator, and compare its accuracy and efficiency with other candidate covariance estimators. Since the covariance matrix is estimated on a pairwise basis, it is not guaranteed to be positive semi-definite (psd). To reduce the number of negative eigenvalues produced by a non-psd matrix, we devise an averaging estimator which is the average of a wide range of duration based covariance matrix estimators. This estimator is applied to a portfolio of 19 DJIA stocks on an out-of-sample global minimum variance portfolio allocation problem where the objective is to minimize the one-day ahead portfolio variance. A simple shrinkage technique is used to improve non-psd and ill-conditioned matrices. The price duration covariance matrix estimator is shown to provide a comparably low portfolio variance while yielding considerably lower portfolio turnover rates than previous estimators...|$|E
40|$|EasyAlign is a {{user-friendly}} automatic phonetic {{alignment tool}} for continuous speech. It is {{developed as a}} plug-in of Praat, and it is freely available. Its main advantage is that one can easily align speech from an orthographic transcription. It requires a few minor manual steps {{and the result is}} a multi-level annotation within a TextGrid composed of phonetic, syllabic, lexical and utterance tiers. Evaluation of EasyAlign was performed according to three approaches: a boundary-based, a <b>duration-based</b> and segment-based <b>approach.</b> Results are very promising, showing, on the one hand, little difference between EasyAlign and human alignment, and a good generalization of the training, on the other one. EasyAlign is fully available for French and Spanish, while other languages such as English, Taiwan Min are under development thanks to a growing interest of community users...|$|R

