4|27|Public
40|$|Three-dimensional (3 -D) radiative-transfer {{effects are}} {{a major source of}} {{retrieval}} errors in satellite-based optical remote sensing of clouds. The challenge is that 3 -D effects manifest themselves across multiple satellite pixels, which traditional single-pixel approaches cannot capture. In this study, we present two multi-pixel retrieval approaches based on deep learning, a technique that is becoming increasingly successful for complex problems in engineering and other areas. Specifically, we use deep neural networks (DNNs) to obtain multi-pixel estimates of cloud optical thickness and column-mean cloud droplet effective radius from multispectral, multi-pixel radiances. The first <b>DNN</b> <b>method</b> corrects traditional bispectral retrievals based on the plane-parallel homogeneous cloud assumption using the reflectances at the same two wavelengths. The other <b>DNN</b> <b>method</b> uses so-called convolutional layers and retrieves cloud properties directly from the reflectances at four wavelengths. The DNN methods are trained and tested on cloud fields from large-eddy simulations used as input to a 3 -D radiative-transfer model to simulate upward radiances. The second DNN-based retrieval, sidestepping the bispectral retrieval step through convolutional layers, is shown to be more accurate. It reduces 3 -D radiative-transfer effects that would otherwise affect the radiance values and estimates cloud properties robustly even for optically thick clouds...|$|E
30|$|Given a labeled or an unlabeled {{workflow}} by type, the workflow is tagged into unique motifs by <b>DNN</b> <b>method.</b> Tagging unique motifs by DNN {{helps to}} improve workflows by determining workflow motif relations among workflows. After searching the most similar workflow in the dataset with the given complex workflow, DNN can give an insight about {{the relation of}} the given workflow and the detected workflow. For finding similarity of workflows this work uses a set based distance function called OSPA [19]. OSPA is a systematic alternative of the squared error [19].|$|E
40|$|The use {{of context}} {{information}} in a scene is an important aid for full semantic scene understanding in security and surveillance applications. To this end, this paper presents an innovative semantic context-labeling algorithm for three context classes, trading-off quality and real-time execution. Our system consists of three consecutive stages: image segmentation, region-based feature extraction and classification. We propose the joint use of the features color in HSV space, texture from Gabor filters and spatial context, {{in combination with the}} Directional Nearest Neighbor (<b>DNN)</b> <b>method</b> for constructing the undirected graph for segmentation. Compared to recent literature, this combination is over 35 times faster and achieves a coverability rate that is 65 % higher...|$|E
40|$|Recently {{there has}} been {{significant}} research on power generation, distribution and transmission efficiency {{especially in the case}} of renewable resources. The main objective is reduction of energy losses and this requires improvements on data acquisition and analysis. In this paper we address these concerns by using consumers' electrical smart meter readings to estimate network loading and this information can then be used for better capacity planning. We compare Deep Neural Network (<b>DNN)</b> <b>methods</b> with traditional methods for load forecasting. Our results indicate that <b>DNN</b> <b>methods</b> outperform most traditional methods. This comes at the cost of additional computational complexity but this can be addressed with the use of cloud resources. We also illustrate how these results can be used to better support dynamic pricing. Comment: presented at 2016 ICML Workshop on #Data 4 Good: Machine Learning in Social Good Applications, New York, N...|$|R
40|$|Deep {{neural network}} (DNN) based {{approaches}} hold significant potential for reinforcement learning (RL) and have already shown remarkable gains over state-of-art methods {{in a number}} of applications. The effectiveness of <b>DNN</b> <b>methods</b> can be attributed to leveraging the abundance of supervised data to learn value functions, Q-functions, and policy function approximations without the need for feature engineering. Nevertheless, the deployment of DNN-based predictors with very deep architectures can pose an issue due to computational and other resource constraints at test-time {{in a number of}} applications. We propose a novel approach for reducing the average latency by learning a computationally efficient gating function that is capable of recognizing states in a sequential decision process for which policy prescriptions of a shallow network suffices and deeper layers of the DNN have little marginal utility. The overall system is adaptive in that it dynamically switches control actions based on state-estimates in order to reduce average latency without sacrificing terminal performance. We experiment with a number of alternative loss-functions to train gating functions and shallow policies and show that in a number of applications a speed-up of up to almost 5 X can be obtained with little loss in performance...|$|R
40|$|In daily communications, Arabs use local dialects {{which are}} hard to {{identify}} automatically using conventional classification methods. The dialect identification challenging task becomes more complicated when dealing with an under-resourced dialects belonging to a same county/region. In this paper, we start by analyzing statistically Algerian dialects in order to capture their specificities related to prosody information which are extracted at utterance level after a coarse-grained consonant/vowel segmentation. According to these analysis findings, we propose a Hierarchical classification approach for spoken Arabic algerian Dialect IDentification (HADID). It takes advantage {{from the fact that}} dialects have an inherent property of naturally structured into hierarchy. Within HADID, a top-down hierarchical classification is applied, in which we use Deep Neural Networks (<b>DNNs)</b> <b>method</b> to build a local classifier for every parent node into the hierarchy dialect structure. Our framework is implemented and evaluated on Algerian Arabic dialects corpus. Whereas, the hierarchy dialect structure is deduced from historic and linguistic knowledges. The results reveal that within, the best classifier is DNNs compared to Support Vector Machine. In addition, compared with a baseline Flat classification system, our HADID gives an improvement of 63. 5...|$|R
40|$|In this paper, {{we propose}} a {{deep neural network}} (DNN) -based {{automatic}} modulation classification (AMC) for digital communications. While conventional AMC techniques perform well for additive white Gaussian noise (AWGN) channels, classification accuracy degrades for fading channels where the amplitude and phase of channel gain change in time. The key contributions of this paper are in two phases. First, we analyze {{the effectiveness of a}} variety of statistical features for AMC task in fading channels. We reveal that the features that are shown to be effective for fading channels are different from those known to be good for AWGN channels. Second, we introduce a new enhanced AMC technique based on <b>DNN</b> <b>method.</b> We use the extensive and diverse set of statistical features found in our study for the DNN-based classifier. The fully connected feedforward network with four hidden layers are trained to classify the modulation class for several fading scenarios. Numerical evaluation shows that the proposed technique offers significant performance gain over the existing AMC methods in fading channels...|$|E
40|$|Abstract- Bayesian model-based {{reinforcement}} learning can be formulated as a partially observable Markova decision process (POMDP) {{to provide a}} principled framework for optimally balancing exploitation and exploration. Then, a POMDP solver {{can be used to}} solve the problem. If the prior distribution over the environment’s dynamics is a product of dirichlet distributions, the POMDP’s optimal value function can be represented using a set of multivariate polynomials. Unfortunately, the size of the polynomials grows exponentially with the problem horizon [3]. During machine learning agent required lots of training inputs of execution cycle. Due to this situation look up table contain huge amount of data base. In this paper, we observe the use of dynamic neural network tree search (DNNTS) algorithm for large POMDPs, to solve the Bayesian {{reinforcement learning}} problem. The keen idea of DNN tree search is to train agent and act as a NN classifier to help agent for taking self decision without prior knowledge of the system during data learning. We will show that such an algorithm successfully searches for a near-optimal policy and achieve goal. Experiments show that the used <b>DNN</b> <b>methods</b> improve performance of Bayesian reinforcement learning in the context of training episodes, reward and discount rate...|$|R
30|$|This work is on both {{finding an}} {{appropriate}} co-occurrence matrix of workflow motifs and a <b>DNN</b> embedding <b>method.</b> By <b>DNN,</b> feature vectors are constructed {{based on the}} structure of workflows and processing the tags in the context. Based on DNN the tags that are repeated more closely together have similar vectors. Using discovery of workflow motifs by co-occurrence frequency of tags within workflows, prediction of an improved workflow becomes more convenient. For example, a reasonable order of workflow motifs can be derived based on extracted concepts. As such the work unites small data into enough big data to benefit from DNN.|$|R
30|$|Random {{search is}} one of the {{simplest}} ways to optimize <b>DNN</b> hyperparameters. This <b>method</b> iteratively generates hyperparameter settings and evaluates the objective function. Random search has excellent parallelization and can handle integer and categorical hyperparameters naturally. Bergstra and Bengio demonstrated that random search outperforms a manual search by a human expert and grid search [4].|$|R
40|$|In recent years, Deep Neural Networks (<b>DNN)</b> based <b>methods</b> have {{achieved}} remarkable {{performance in a}} wide range of tasks and have been among the most powerful and widely used techniques in computer vision. However, DNN-based methods are both computational-intensive and resource-consuming, which hinders the application of these methods on embedded systems like smart phones. To alleviate this problem, we introduce a novel Fixed-point Factorized Networks (FFN) for pretrained models to reduce the computational complexity as well as the storage requirement of networks. The resulting networks have only weights of - 1, 0 and 1, which significantly eliminates the most resource-consuming multiply-accumulate operations (MACs). Extensive experiments on large-scale ImageNet classification task show the proposed FFN only requires one-thousandth of multiply operations with comparable accuracy...|$|R
40|$|The Violent Scenes Detection task aims at {{evaluating}} algo-rithms {{that automatically}} localize violent segments in both Hollywood movies and short web videos. The denition {{of violence is}} subjective: segments that one would not let an 8 years old child see in a movie because they con-tain physical violence". This is a highly challenging problem because of the strong content variations among the posi-tive instances. In this year's evaluation, we adopted our recently proposed classication method to fuse multiple fea-tures using Deep Neural Networks (<b>DNN).</b> The <b>method</b> was named regularized DNN. We extracted a set of visual and audio features, which have been observed useful. We then applied the regularized DNN for feature fusion and classi-cation. Results indicate that using multiple features is stil...|$|R
50|$|Deaf News Network (summarized as 'DNN') is a {{relatively}} new comer to the news industry. With a soft launch on June 10, 2013 building up to the grand opening on July 4, DNN provides news using the crowd-source <b>method.</b> <b>DNN</b> also provides both select local and nationwide news—a rarity in the industry. Headquartered in Surprise, Arizona and run by two Rochester Institute of Technology graduates, DNN aims to be {{a major player in the}} industry.|$|R
40|$|The {{backpropagation}} algorithm for calculating gradients {{has been}} widely used in computation of weights for deep neural networks (<b>DNNs).</b> This <b>method</b> requires derivatives of objective functions and has some difficulties finding appropriate parameters such as learning rate. In this paper, we propose a novel approach for computing weight matrices of fully-connected DNNs by using two types of semi-nonnegative matrix factorizations (semi-NMFs). In this method, optimization processes are performed by calculating weight matrices alternately, and backpropagation (BP) is not used. We also present a method to calculate stacked autoencoder using a NMF. The output results of the autoencoder are used as pre-training data for DNNs. The experimental results show that our method using three types of NMFs attains similar error rates to the conventional DNNs with BP. Comment: 9 pages, 2 figure...|$|R
40|$|Many natural {{language}} understanding (NLU) tasks, such as shallow parsing (i. e., text chunking) and semantic slot filling, require the assignment of representative labels to the meaningful chunks in a sentence. Most of the current deep neural network (<b>DNN)</b> based <b>methods</b> consider these tasks as a sequence labeling problem, in which a word, rather than a chunk, is treated as the basic unit for labeling. These chunks are then inferred by the standard IOB (Inside-Outside-Beginning) labels. In this paper, we propose an alternative approach by investigating the use of DNN for sequence chunking, and propose three neural models so that each chunk can {{be treated as a}} complete unit for labeling. Experimental results show that the proposed neural sequence chunking models can achieve start-of-the-art performance on both the text chunking and slot filling tasks. Comment: Accepted by AAAI 201...|$|R
40|$|Recently, several {{optimization}} {{methods have}} been successfully applied to the hyperparameter optimization of deep neural networks (<b>DNNs).</b> The <b>methods</b> work by modeling the joint distribution of hyperparameter values and corresponding error. Those methods become less practical when applied to modern DNNs whose training may take {{a few days and}} thus one cannot collect sufficient observations to accurately model the distribution. To address this challenging issue, we propose a method that learns to transfer optimal hyperparameter values for a small source dataset to hyperparameter values with comparable performance on a dataset of interest. As opposed to existing transfer learning methods, our proposed method does not use hand-designed features. Instead, it uses surrogates to model the hyperparameter-error distributions of the two datasets and trains a neural network to learn the transfer function. Extensive experiments on three CV benchmark datasets clearly demonstrate the efficiency of our method...|$|R
40|$|Deep Neural Networks (DNNs) {{have shown}} to {{outperform}} traditional methods in various visual recognition tasks including Facial Expression Recognition (FER). In spite of efforts made {{to improve the}} accuracy of FER systems using <b>DNN,</b> existing <b>methods</b> still are not generalizable enough in practical applications. This paper proposes a 3 D Convolutional Neural Network method for FER in videos. This new network architecture consists of 3 D Inception-ResNet layers followed by an LSTM unit that together extracts the spatial relations within facial images {{as well as the}} temporal relations between different frames in the video. Facial landmark points are also used as inputs to our network which emphasize on the importance of facial components rather than the facial regions that may not contribute significantly to generating facial expressions. Our proposed method is evaluated using four publicly available databases in subject-independent and cross-database tasks and outperforms state-of-the-art methods. Comment: To appear in 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW...|$|R
40|$|In this paper, {{we propose}} a novel method to adapt context-dependent {{deep neural network}} hidden Markov model (CD-DNN-HMM) with only limited number of {{parameters}} by {{taking into account the}} underlying factors that contribute to the distorted speech signal. We derive this factorized adaptation method from the perspectives of joint factor analysis and vector Taylor series expansion, respectively. Evaluated on Aurora 4, the proposed method can get 19. 0 % and 10. 6 % relative word error rate reduction on test set B and D with only 20 adaptation utterances, and can have decent improvement with as few as two adaptation utterances. We also show that the proposed method is better than feature discriminative linear regression (fDLR), an existing <b>DNN</b> adaptation <b>method.</b> Its small number of parameters and short training time offer an attractive solution to low-footprint speech applications. Index Terms — deep neural network, factorized adaptation, joint factor analysis, vector Taylor serie...|$|R
40|$|Deep Neural Network (DNN) {{has become}} a {{standard}} method in many ASR tasks. Recently there is considerable interest in "informed training" of DNNs, where DNN input is augmented with auxiliary codes, such as i-vectors, speaker codes, speaker separation bottleneck (SSBN) features, etc. This paper compares different speaker informed <b>DNN</b> training <b>methods</b> in LVCSR task. We discuss mathematical equivalence between speaker informed DNN training and "bias adaptation" which uses speaker dependent biases, and give detailed analysis on influential factors such as dimension, discrimination and stability of auxiliary codes. The analysis is supported by experiments on a meeting recognition task using bottleneck feature based system. Results show that i-vector based adaptation is also effective in bottleneck feature based system (not just hybrid systems). However all tested methods show poor generalisation to unseen speakers. We introduce a system based on speaker classification followed by speaker adaptation of biases, which yields equivalent performance to an i-vector based system with 10. 4 % relative improvement over baseline on seen speakers. The new approach {{can serve as a}} fast alternative especially for short utterances...|$|R
40|$|In speech analysis, the {{information}} about the glottal source is obtained from speech by using glottal inverse filtering (GIF). The accuracy of state-of-the-art GIF methods is sufficiently high when the input speech signal is of high-quality (i. e., with little noise or reverberation). However, in realistic conditions, particularly when GIF is computed from coded telephone speech, the accuracy of GIF methods deteriorates severely. To robustly estimate the glottal source under coded condition, a deep neural network (<b>DNN)</b> -based <b>method</b> is proposed. The proposed <b>method</b> utilizes a <b>DNN</b> to map the speech features extracted from the coded speech to the glottal flow waveform estimated from the corresponding clean speech. To generate the coded telephone speech, adaptive multi-rate (AMR) codec is utilized which is a widely used speech compression method. The proposed glottal source estimation method is compared with two existing GIF methods, closed phase covariance analysis (CP) and iterative adaptive inverse filtering (IAIF). The results indicate that the proposed DNN-based method is capable of estimating glottal flow waveforms from coded telephone speech with a considerably better accuracy in comparison to CP and IAIF. Peer reviewe...|$|R
40|$|We {{evaluate}} a real-time multi-channel dereverbera-tion method {{for the application}} to speech recognition with deep neural networks (<b>DNN).</b> The dereverberation <b>method</b> is based on modeling the reverberated signal as a mixture of a fully coherent direct path signal and a diffuse reverberation component, and estimating the coherent-to-diffuse power ratio (CDR) from the spatial coherence of the signals. The method can operate in real-time, i. e., without requiring processing of entire utterances. We compare CDR estimators which are “blind”, i. e., do not require information {{about the direction of}} arrival (DOA) of the target signal, with estimators which make use of a DOA estimate. The impact of the dereverberation method on speech recognition accuracy with different DNN-based acoustic models is investigated with the RE-VERB challenge corpus and the Kaldi speech recognition toolkit...|$|R
40|$|Environmental audio tagging is a newly {{proposed}} task {{to predict}} {{the presence or absence}} of a specific audio event in a chunk. Deep neural network (<b>DNN)</b> based <b>methods</b> have been successfully adopted for predicting the audio tags in the domestic audio scene. In this paper, we propose to use a convolutional neural network (CNN) to extract robust features from mel-filter banks (MFBs), spectrograms or even raw waveforms for audio tagging. Gated recurrent unit (GRU) based recurrent neural networks (RNNs) are then cascaded to model the long-term temporal structure of the audio signal. To complement the input information, an auxiliary CNN is designed to learn on the spatial features of stereo recordings. We evaluate our proposed methods on Task 4 (audio tagging) of the Detection and Classification of Acoustic Scenes and Events 2016 (DCASE 2016) challenge. Compared with our recent DNN-based method, the proposed structure can reduce the equal error rate (EER) from 0. 13 to 0. 11 on the development set. The spatial features can further reduce the EER to 0. 10. The performance of the end-to-end learning on raw waveforms is also comparable. Finally, on the evaluation set, we get the state-of-the-art performance with 0. 12 EER while the performance of the best existing system is 0. 15 EER. Comment: Accepted to IJCNN 2017, Anchorage, Alaska, US...|$|R
40|$|Condition-based {{maintenance}} {{is critical to}} reduce the costs of maintenance and improve the production efficiency. Data-driven method based on neural network (NN) {{is one of the}} most used models for mechanical components condition recognition. In this paper, we introduce a new bearing condition recognition method based on multifeatures extraction and deep neural network (<b>DNN).</b> First, the <b>method</b> calculates time domain, frequency domain, and time-frequency domain features to represent characteristic of vibration signals. Then the nonlinear dimension reduction algorithm based on deep learning is proposed to reduce the redundancy information. Finally, the top-layer classifier of deep neural network outputs the bearing condition. The proposed method is validated using experiment test-bed bearing vibration data. Meanwhile some comparative studies are performed; the results show the advantage of the proposed method in adaptive features selection and superior accuracy in bearing condition recognition...|$|R
40|$|This article {{presents}} {{the possibility of}} using of multiple regression analysis (MRA) and dynamic neural network (DNN) for prediction of stability of Hydrocortisone 100 mg (in a form of hydrocortisone sodium succinate) freeze-dried powder for injection packed into a dual chamber container. Degradation products of hydrocortisone sodium succinate: free hydrocortisone and related substances (impurities A, B, C, D and E; unspecified impurities and total impurities) were followed during stress and formal stability studies. All data obtained during stability studies were used for in silico modeling; multiple regression models and dynamic neural networks as well, in order to compare predicted and observed results. High values of coefficient of determination (0. 950. 99) were gained using MRA and <b>DNN,</b> so both <b>methods</b> are powerful tools for in silico stability studies, but superiority of DNN over mathematical modeling of degradation was also confirmed...|$|R
40|$|Abstract—In {{acoustic}} modeling, speaker {{adaptive training}} (SAT) {{has been a}} long-standing technique for the traditional Gaussian mixture models (GMMs). Acoustic models trained with SAT become independent of training speakers and generalize better to unseen testing speakers. This paper ports the idea of SAT to deep neural networks (DNNs), and proposes a framework to perform feature-space SAT for DNNs. Using i-vectors as speaker representations, our framework learns an adaptation neural network to derive speaker-normalized features. Speaker adaptive models are obtained by fine-tuning DNNs in such a feature space. This framework {{can be applied to}} various feature types and network structures, posing a very general SAT solution. In this work, we fully investigate how to build SAT-DNN models effectively and efficiently. First, we study the optimal configurations of SAT-DNNs for large-scale acoustic modeling tasks. Then, after presenting detailed comparisons between SAT-DNNs and the existing <b>DNN</b> adaptation <b>methods,</b> we propose to combine SAT-DNNs and model-space DNN adaptation during decoding. Finally, to accelerate learning of SAT-DNNs, a simple yet effective strategy, frame skipping, is employed {{to reduce the size of}} training data. Our experiments show that compared with a strong DNN baseline, the SAT-DNN model achieves 13. 5 % and 17. 5 % relative improvement on word error rates (WERs), without and with model-space adaptation applied respectively. Data reduction based on frame skipping results in 2 × speed-up for SAT-DNN training, while causing negligible WER loss on the testing data. Index Terms—Deep neural networks, speaker adaptive train-ing, acoustic modeling. I...|$|R
40|$|This paper {{presents}} a unified i-vector framework for language identification (LID) based on deep bottleneck networks (DBN) trained for {{automatic speech recognition}} (ASR). The framework covers both front-end feature extraction and back-end modeling stages. The output from different layers of a DBN are exploited to improve {{the effectiveness of the}} i-vector representation through incorporating a mixture of acoustic and phonetic information. Furthermore, a universal model is derived from the DBN with a LID corpus. This is a somewhat inverse process to the GMM-UBM method, in which the GMM of each language is mapped from a GMM-UBM. Evaluations on specific dialect recognition tasks show that the DBN based i-vector can achieve significant and consistent performance gains over conventional GMM-UBM and <b>DNN</b> based i-vector <b>methods.</b> The generalization capability of this framework is also evaluated using DBNs trained on Mandarin and English corpuses. Index Terms: Language Identification, Deep Neural Network, Deep Bottleneck Feature, i-vector representatio...|$|R
40|$|International audienceWe {{consider}} {{the problem of}} robust automatic speech recognition (ASR) in noisy conditions. The performance improvement brought by speech enhancement is often limited by residual distortions of the enhanced features, which {{can be seen as}} a form of statistical uncertainty. Uncertainty estimation and propagation methods have recently been proposed to improve the ASR performance with deep neural network (DNN) acoustic models. However, the performance is still limited due to the use of uncertainty only during decoding. In this paper, we propose a consistent approach to account for uncertainty in the enhanced features during both training and decoding. We estimate the variance of the distortions using a DNN uncertainty estimator that operates directly in the feature maximum likelihood linear regression (fMLLR) domain and we then sample the uncertain features using the unscented transform (UT). We report the resulting ASR performance on the CHiME- 2 and CHiME- 3 datasets for different uncertainty estimation/propagation techniques. The proposed <b>DNN</b> uncertainty training <b>method</b> brings 4 % and 8 % relative improvement on these two datasets, respectively, compared to a competitive fMLLR-domain DNN acoustic modeling baseline...|$|R
40|$|Pairwise {{classification}} is a computational {{problem to}} determine whether a given ordered pair of objects satisfies a binary relation R which is specified implicitly by a set of training data used for ‘learning’ R. It is an important component for entity resolution, network link prediction, protein-protein interaction prediction, and so on. Although deep neural networks (<b>DNNs)</b> outperform other <b>methods</b> in many tasks and have thus attracted the attention of machine learning researchers, there have been few studies of applying a DNN to pairwise classification. Important properties of pairwise classification include using feature conjunctions across examples. Also, it is known that making the classifier invariant to the data order is an important property in applications with a symmetric relation R, including those applications mentioned above. We first show that a simple DNN with fully connected layers cannot satisfy these properties and then present a pairwise DNN satisfying these properties. As an example of pairwise classification, we use the author matching problem, which is the problem of determining whether two author names in different bibliographic data sources refer to the same person. We show that the method using our model outperforms methods using a support vector machine and simple DNNs...|$|R
40|$|In this article, {{we propose}} a {{transfer}} learning method for deep neural networks (DNNs). Deep learning {{has been widely}} used in many applications. However, applying deep learning is problematic when {{a large amount of}} training data are not available. One of the conventional methods for solving this problem is transfer learning for DNNs. In the field of image recognition, state-of-the-art transfer learning <b>methods</b> for <b>DNNs</b> re-use parameters trained on source domain data except for the output layer. However, this method may result in poor classification performance when the amount of target domain data is significantly small. To address this problem, we propose a method called All-Transfer Deep Learning, which enables the transfer of all parameters of a <b>DNN.</b> With this <b>method,</b> we can compute the relationship between the source and target labels by the source domain knowledge. We applied our method to actual two-dimensional electrophoresis image~(2 -DE image) classification for determining if an individual suffers from sepsis; the first attempt to apply a classification approach to 2 -DE images for proteomics, which has attracted considerable attention as an extension beyond genomics. The results suggest that our proposed method outperforms conventional transfer learning <b>methods</b> for <b>DNNs.</b> Comment: Long version of article published at ECAI 2016 (9 pages, 13 figures, 8 tables...|$|R
40|$|Recently, {{automatic}} {{speech recognition}} (ASR) systems that use deep neural networks (DNNs) for acoustic modeling have attracted huge research interest. This {{is due to the}} recent results that have significantly raised {{the state of the art}} performance of ASR systems. This dissertation proposes a number of new methods to improve the state of the art ASR performance by exploiting the power of <b>DNNs.</b> The first <b>method</b> exploits domain knowledge in designing a special neural network (NN) structure called a convolutional neural network (CNN). This dissertation proposes to use the CNN in a way that applies convolution and pooling operations along frequency to handle frequency variations that commonly happen due to speaker and pronunciation differences in speech signals. Moreover, a new CNN structure called limited weight sharing is proposed to better suit special spectral characteristics of speech signals. Our experimental results have shown that the use of a CNN leads to 6 - 9 % relative reduction in error rate. The second proposed method deals with speaker variations in a more explicit way through using a new speaker code based adaptation. This method adapts the speech acoustic model to a new speaker by learning a suitable speaker representation based on a small amount of adaptation data from each target speaker. This method alleviates the need to modify any model parameters as is done with other commonly used adaptation methods for neural networks. This greatly reduces the number of parameters to estimate during adaptation; hence, it allows rapid speaker adaptation. The third proposed method aims to handle the temporal structure within speech segments by using a deep segmental neural network (DSNN). The DSNN model alleviates the need to use an HMM model as it directly models the posterior probability of the label sequence. Moreover, a segment-aware NN structure has been proposed. It is able to model the dependency among speech frames within each segment and performs better than the conventional frame based DNNs. Experimental results show that the proposed DSNN can significantly improve recognition performance as compared with the conventional frame based models...|$|R
40|$|This {{dissertation}} outlines work {{related to}} Speech Segmentation – segmenting an audio recording into regions {{of speech and}} non-speech, and Speaker Diarization – further segmenting those regions into those pertaining to homogeneous speakers. Knowing not only what was said but also who said it and when, has many useful applications. As well as providing a richer level of transcription for speech, we will show how such knowledge can improve Automatic Speech Recognition (ASR) system performance and can also benefit downstream Natural Language Processing (NLP) tasks such as machine translation and punctuation restoration. While segmentation and diarization {{may appear to be}} relatively simple tasks to describe, in practise we find that they are very challenging and are, in general, ill-defined problems. Therefore, we first provide a formalisation of each of the problems as the sub-division of speech within acoustic space and time. Here, we see that the task can become very difficult when we want to partition this domain into our target classes of speakers, whilst avoiding other classes that reside in the same space, such as phonemes. We present a theoretical framework for describing and discussing the tasks as well as introducing existing state-of-the-art methods and research. Current Speaker Diarization systems are notoriously sensitive to hyper-parameters and lack robustness across datasets. Therefore, we present a method which uses a series of oracle experiments to expose the limitations of current systems and to which system components these limitations can be attributed. We also demonstrate how Diarization Error Rate (DER), the dominant error metric in the literature, is not a comprehensive or reliable indicator of overall performance or of error propagation to subsequent downstream tasks. These results inform our subsequent research. We find that, as a precursor to Speaker Diarization, the task of Speech Segmentation is a crucial first step in the system chain. Current methods typically do not account for the inherent structure of spoken discourse. As such, we explored a novel method which exploits an utterance-duration prior in order to better model the segment distribution of speech. We show how this method improves not only segmentation, but also the performance of subsequent speech recognition, machine translation and speaker diarization systems. Typical ASR transcriptions do not include punctuation and the task of enriching transcriptions with this information is known as ‘punctuation restoration’. The benefit is not only improved readability but also better compatibility with NLP systems that expect sentence-like units such as in conventional machine translation. We show how segmentation and diarization are related tasks that are able to contribute acoustic information that complements existing linguistically-based punctuation approaches. There is a growing demand for speech technology applications in the broadcast media domain. This domain presents many new challenges including diverse noise and recording conditions. We show that the capacity of existing GMM-HMM based speech segmentation systems is limited for such scenarios and present a Deep Neural Network (<b>DNN)</b> based <b>method</b> which offers a more robust speech segmentation method resulting in improved speech recognition performance for a television broadcast dataset. Ultimately, we are able to show that the speech segmentation is an inherently ill-defined problem for which the solution is highly dependent on the downstream task that it is intended for...|$|R

