13|0|Public
40|$|The {{popular press}} {{attaches}} particular significance to certain numerical {{values of the}} <b>Dow-Jones</b> <b>index.</b> These magic numbers {{are referred to as}} `resistance levels' or `psychological barriers. ' We examine 38 years of closing values of this index to see if it is of any help in predicting future stock market returns. <b>Dow-Jones</b> <b>index,</b> psychological barriers, resistance levels, market efficiency...|$|E
40|$|The {{relationship}} between the <b>Dow-Jones</b> <b>Index</b> returns and Madrid Stock Index returns is observed. Using daily data for the period 1988 - 1989 significant effect are found, being the <b>Dow-Jones</b> <b>Index</b> returns a leading indicator for Madrid returns condicional mean. The effects are asymmetric: negative changes in the <b>Dow-Jones</b> <b>Index</b> returns have twice the effect than positive ones; and nonlinear as the influence of Black Friday, October 13, 1989 suggests. The "meteor shower" effects between boths markets volatilities is documented. Daily traing volume has some explanatory power for the conditional variance of daily returns. Day of the weeks effects are examined and {{it is found that}} the average return on Thursday is abnormally high...|$|E
40|$|We show, {{using the}} Dow-Jones volume {{as a working}} example, that {{techniques}} borrowed from Signal Processing and Internet traffic model-ing (such as the envelope, the Instantaneous Frequency, the Averaging function etc.) can help with the analysis of financial data. As an ap-plication, we successfully confirm the 4 year cycle of the <b>Dow-Jones</b> <b>index...</b>|$|E
40|$|A {{stochastic}} differential equationdriven by a Brownian motion {{where the}} dispersion {{is determined by}} a parameter is considered. The parameter undergoes a change {{at a certain time}} point. Estimates of the time change point and the parameter, before and after that time, is considered. The estimates were presented in Lacus 2008. Two cases are considered: (1) the drift is known, (2) the drift is unknown and the dispersion space-independent. Applications to <b>Dow-Jones</b> <b>index</b> 1971 - 1974   and Goldmann-Sachs closings 2005 [...] May 2009 are given...|$|E
40|$|The {{discovery}} of evolutionary laws of financial market is always {{built on the}} basis of financial data. Any financial market must be controlled by some basic laws, including macroscopic level, submicroscopic level and microscopic level laws. How to discover its necessity-laws from financial data is the most important task of financial market analysis and prediction. Based on the evolutionary computation, this paper proposes a multi-level and multi-scale evolutionary modeling system which models the macro-behavior of the stock market by ordinary differential equations while models the micro-behavior of the stock market by natural fractals. This system can be used to model and predict the financial data(some time series), such as the stock market data of <b>Dow-Jones</b> <b>index</b> and IBM stock price, and always get good results. ...|$|E
40|$|Abstract: This article {{displays}} {{an application}} of the statistical method moti-vated by Bruno de Finetti’s operational subjective theory of probability. We use exchangeable forecasting distributions based on mixtures of linear com-binations of exponential power (EP) distributions to forecast the sequence of daily rates of return from the <b>Dow-Jones</b> <b>index</b> of stock prices over a 20 year period. The operational subjective statistical method for comparing distributions {{is quite different from}} that commonly used in data analysis, because it rejects the basic tenets underlying the practice of hypothesis test-ing. In its place, proper scoring rules for forecast distributions are used to assess the values of various forecasting strategies. Using a logarithmic scoring rule, we find that a mixture linear combination of EP distributions scores markedly better than does a simple mixture over the EP family, which scores much better than does a simple Normal mixture. Surprisingly, a mix-ture over a linear combination of three Normal distributions also makes a substantial improvement over a simple Normal mixture, although it does not quite match the performance of even the simple EP mixture. All sub-stantive forecasting improvements become most marked after extreme tail phenomena were actually observed in the sequence, in particular after the abrupt drop in market prices in October, 1987. However, the improvements continue to be apparent over the long haul of 1985 - 2006 which has seen a number of extreme price changes. This result is supported by an analysis of the Negentropies embedded in the forecasting distributions, and a proper scoring analysis of these Negentropies as well. Key words: <b>Dow-Jones</b> <b>index,</b> exponential power distributions, fat tails, log-arithmic scoring rule, mixture distributions, partial exchangeability, proper scoring rules, subjective probability, subjectivist statistical methods. 1...|$|E
40|$|We {{study the}} Heston model, where {{the stock price}} {{dynamics}} is governed by a geometrical (multiplicative) Brownian motion with stochastic variance. We solve the corresponding Fokker-Planck equation exactly and, after integrating out the variance, find an analytic formula for the time-dependent probability distribution of stock price changes (returns). The formula is in excellent agreement with the <b>Dow-Jones</b> <b>index</b> for the time lags from 1 to 250 trading days. For large returns, the distribution is exponential in log-returns with a time-dependent exponent, whereas for small returns it is Gaussian. For time lags longer than the relaxation time of variance, the probability distribution can be expressed in a scaling form using a Bessel function. The Dow-Jones data for 1982 - 2001 follow the scaling function for seven orders of magnitude. ...|$|E
40|$|Backtesting {{methods are}} {{statistical}} tests designed to uncover excessive risk-taking from financial institutions. We show {{in this paper}} that these methods {{are subject to the}} presence of model risk produced by a wrong specification of the conditional VaR model, and derive its effect on the asymptotic distribution of the relevant out-of-sample tests. We also show that in the absence of estimation risk, the unconditional backtest is affected by model misspecification but the independence test is not. Our solution for the general case consists on proposing robust subsampling techniques to approximate the true sampling distribution of these tests. We carry out a Monte Carlo study to see the importance of these effects in finite samples for location-scale models that are wrongly specified but correct on “average ”. An application to <b>Dow-Jones</b> <b>Index</b> shows the impact of correcting for model risk on backtesting procedures for different dynamic VaR models measuring risk exposure...|$|E
40|$|Randomness and regularities in Finance {{are usually}} treated in {{probabilistic}} terms. In this paper, we develop {{a completely different}} approach in using a non-probabilistic framework based on the algorithmic information theory initially developed by Kolmogorov (1965). We present some elements of this theory and show why it is particularly relevant to Finance, and potentially to other sub-fields of Economics as well. We develop a generic method to estimate the Kolmogorov complexity of numeric series. This approach {{is based on an}} iterative "regularity erasing procedure" implemented to use lossless compression algorithms on financial data. Examples are provided with both simulated and real-world financial time series. The contributions of this article are twofold. The first one is methodological : we show that some structural regularities, invisible with classical statistical tests, can be detected by this algorithmic method. The second one consists in illustrations on the daily <b>Dow-Jones</b> <b>Index</b> suggesting that beyond several well-known regularities, hidden structure may in this index remain to be identified...|$|E
40|$|The {{most common}} {{stochastic}} volatility models {{such as the}} Ornstein-Uhlenbeck (OU), the Heston, the exponential OU (ExpOU) and Hull-White models define volatility as a Markovian process. In this work we check of {{the applicability of the}} Markovian approximation at separate times scales and will try to answer the question which of the stochastic volatility models indicated above is the most realistic. To this end we consider the volatility at both short (a few days) and long (a few months) time scales as a Markovian process and estimate for it the coefficients of the Kramers-Moyal expansion using the data for <b>Dow-Jones</b> <b>Index.</b> It has been found that the empirical data allow to take only the first two coefficients of expansion to be non zero that define form of the volatility stochastic differential equation of Ito. It proved to be that for the long time scale the empirical data support the ExpOU model. At the short time scale the empirical model coincides with ExpOU model for the small volatility quantities only. ...|$|E
40|$|Several {{problems}} arising in Economics and Finance {{are analyzed}} using concepts and quantitative methods from Physics. Here is the abridged abstact: Chapter 1 : By analogy with energy, the equilibrium probability distribution of money must follow the exponential Boltzmann-Gibbs law {{characterized by an}} effective temperature equal to the average amount of money per economic agent. A thermal machine which extracts a monetary profit can be constructed between two economic systems with different temperatures. Chapter 2 : Using data from several sources, {{it is found that}} the distribution of income is described for the great majority of population by an exponential distribution, whereas the high-end tail follows a power law. The Lorenz curve and Gini coefficient were calculated and are shown to be in good agreement with both income and wealth data sets. Chapter 3 : The Heston model where stock-price dynamics is governed by a geometrical (multiplicative) Brownian motion with stochastic variance is studied. The corresponding Fokker-Planck equation is solved exactly. Integrating out the variance, an analytic formula for the time-dependent probability distribution of stock price changes (returns) is found. The formula is in excellent agreement with the <b>Dow-Jones</b> <b>index</b> for the time lags from 1 to 250 trading days. ...|$|E
40|$|Principal {{component}} analysis of equity options on Dow-Jones …rms reveals a strong factor structure. The …rst principal component explains 77 % {{of the variation}} in the equity volatility level, 77 % {{of the variation in}} the equity option skew, and 60 % of the implied volatility term structure across equities. Furthermore, the …rst principal component has a 92 % correlation with S&P 500 index option volatility, a 64 % correlation with the index option skew, and a 80 % correlation with the index option term structure. We develop an equity option valuation model that captures this factor structure. The model assumes a Heston (1993) style stochastic volatility model for the market return but additionally allows for stochastic idiosyncratic volatility for each …rm. The model predicts that …rms with higher betas have higher implied volatilities, consistent with the empirical …ndings in Duan and Wei (2009). It also predicts that …rms with higher betas have steeper moneyness and term structure slopes. We provide a tractable approach for estimating the model on index and equity option data. The model provides a good …t to a large panel of options on stocks in the <b>Dow-Jones</b> <b>index,</b> and the data support the cross-sectional implications of the model. JEL Classi…cation: G 10; G 12; G 13...|$|E
40|$|This article {{displays}} {{an application}} of the statistical method motivated by Bruno de Finetti's operational subjective theory of probability. We use exchangeable forecasting distributions based on mixtures of linear combinations of exponential power (EP) distributions to forecast the sequence of daily rates of return from the <b>Dow-Jones</b> <b>index</b> of stock prices over a 20 year period. The operational subjective statistical method for comparing distributions {{is quite different from}} that commonly used in data analysis, because it rejects the basic tenets underlying the practice of hypothesis testing. In its place, proper scoring rules for forecast distributions are used to assess the values of various forecasting strategies. Using a logarithmic scoring rule, we find that a mixture linear combination of EP distributions scores markedly better than does a simple mixture over the EP family, which scores much better than does a simple Normal mixture. Surprisingly, a mixture over a linear combination of three Normal distributions also makes a substantial improvement over a simple Normal mixture, although it does not quite match the performance of even the simple EP mixture. All substantive forecasting improvements become most marked after extreme tail phenomena were actually observed in the sequence, in particular after the abrupt drop in market prices in October, 1987. However, the improvements continue to be apparent over the long haul of 1985 - 2006 which has seen a number of extreme price changes. This result is supported by an analysis of the Negentropies embedded in the forecasting distributions, and a proper scoring analysis of these Negentropies as well...|$|E

