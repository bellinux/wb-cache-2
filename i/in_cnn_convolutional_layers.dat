0|2924|Public
30|$|Masci et al. [80]: A {{convolutional}} {{neural network}}(CNN) {{is a type}} of feed-forward artificial neural network where the individual neurons respond to overlapping regions in the visual field. <b>In</b> <b>CNN,</b> <b>convolutional</b> <b>layer</b> performs a 2 D filtering between input images, x and a bank of filters, w producing another set of images, h. A nonlinear activation function is applied to h just as for standard multilayer networks. Pooling layer reduces the dimensionality of the input by a constant factor and also undertakes feature selection. The input images are tiled in nonoverlapping subregions from which only one output value (max. or avg.) is extracted. Subsequent, fully connected layer performs a linear combination of the input vector with a weight matrix. Max-pooling convolutional neural networks (MPCNNs) perform feature extraction and classification jointly. With 7 % error rate, MPCNN performed much better than SVM for seven defects in cold strips. <b>In</b> <b>CNN,</b> the number of free parameters does not grow proportionally with the input dimensions and therefore performs better in terms of many benchmarks.|$|R
40|$|Deep <b>convolutional</b> neural {{networks}} (<b>CNNs),</b> which {{are at the}} heart of many new emerging applications, achieve remarkable performance in audio and visual recognition tasks, at the expense of high computational complexity, limiting their deployability. <b>In</b> modern <b>CNNs,</b> <b>convolutional</b> <b>layers</b> mostly consume 90 % of the processing time during a forward inference and acceleration of these layers are of great research and commercial interest. In this paper, we examine the effects of co-optimizing internal structures of <b>convolutional</b> <b>layers</b> and underlying implementation of fundamental convolution operation. We demonstrate that a combination of these methods can have a big impact on the overall speed-up of a CNN, achieving a tenfold increase over baseline. We also introduce a new class of fast 1 -D convolutions for CNNs using the Toom-Cook algorithm. We show that our proposed scheme is mathematically well grounded, robust, does not require any time-consuming retraining, and still achieves speedups solely from <b>convolutional</b> <b>layers</b> with no loss in baseline accuracy...|$|R
40|$|A {{defining}} {{problem in}} spoken language identification (LID) {{is how to}} design effective representations which allow features to be extracted that are specific to language information. Recent advances in deep neural networks for feature extraction have led to significant improvements in results, with deep end-to-end methods proving effective. In this paper, a novel network is proposed and explored that models an effective representation using first and second-order statistics of features extracted from a well-trained phoneme-related DNN bottleneck network followed by a stack of <b>CNN</b> <b>convolutional</b> <b>layers.</b> The high-order statistics extracted through second order pooling at {{the output of the}} CNN are robust to speaker and channel variability, and background noise. Evaluation with NIST LRE 2009 shows improved performance compared to current state-of-the-art systems, achieving over 33...|$|R
40|$|Although Deep <b>Convolutional</b> Neural Networks (<b>CNNs)</b> have liberated {{their power}} in various {{computer}} vision tasks, {{the most important}} components of <b>CNN,</b> <b>convolutional</b> <b>layers</b> and fully connected layers, are still limited to linear transformations. In this paper, we propose a novel Factorized Bilinear (FB) layer to model the pairwise feature interactions by considering the quadratic terms in the transformations. Compared with existing methods that tried to incorporate complex non-linearity structures into CNNs, the factorized parameterization makes our FB layer only require a linear increase of parameters and affordable computational cost. To further {{reduce the risk of}} overfitting of the FB layer, a specific remedy called DropFactor is devised during the training process. We also analyze the connection between FB layer and some existing models, and show FB layer is a generalization to them. Finally, we validate the effectiveness of FB layer on several widely adopted datasets including CIFAR- 10, CIFAR- 100 and ImageNet, and demonstrate superior results compared with various state-of-the-art deep models. Comment: Accepted by ICCV 201...|$|R
40|$|In this work, we {{perform an}} {{exploratory}} study on synthesizing deep neural networks using biological synaptic strength distributions, {{and the potential}} influence of different distributions on modelling performance particularly for the scenario associated with small data sets. Surprisingly, a <b>CNN</b> with <b>convolutional</b> <b>layer</b> synaptic strengths drawn from biologically-inspired distributions such as log-normal or correlated center-surround distributions performed relatively well suggesting a possibility for designing deep neural network architectures {{that do not require}} many data samples to learn, and can sidestep current training procedures while maintaining or boosting modelling performance...|$|R
40|$|<b>Convolutional</b> neural {{networks}} (<b>CNNs)</b> are revolutionizing {{a variety of}} machine learning tasks, but they present significant computational challenges. Recently, FPGA-based accelerators have been proposed to improve the speed and efficiency of CNNs. Current approaches construct a single processor that computes the CNN layers one at a time; this single processor is optimized to maximize the overall throughput at which the collection of layers are computed. However, this approach leads to inefficient designs because the same processor structure is used to compute CNN layers of radically varying dimensions. We present a new CNN accelerator paradigm and an accompanying automated design methodology that partitions the available FPGA resources into multiple processors, {{each of which is}} tailored for a different subset of the <b>CNN</b> <b>convolutional</b> <b>layers.</b> Using the same FPGA resources as a single large processor, multiple smaller specialized processors result in increased computational efficiency and lead to a higher overall throughput. Our design methodology achieves 1. 51 x higher throughput than {{the state of the art}} approach on evaluating the popular AlexNet CNN on a Xilinx Virtex- 7 FPGA. Our projections indicate that the benefit of our approach increases with the amount of available FPGA resources, already growing to over 3 x over the state of the art within the next generation of FPGAs...|$|R
40|$|As one of {{the most}} popular deep {{learning}} models, convolution neural network (CNN) has achieved huge success in image information extraction. Traditionally CNN is trained by supervised learning method with labeled data and used as a classifier by adding a classification layer in the end. Its capability of extracting image features is largely limited due to the difficulty of setting up a large training dataset. In this paper, we propose a new unsupervised learning CNN model, which uses a so-called convolutional sparse auto-encoder (CSAE) algorithm pre-Train the CNN. Instead of using labeled natural images for CNN training, the CSAE algorithm can be used to train the CNN with unlabeled artificial images, which enables easy expansion of training data and unsupervised learning. The CSAE algorithm is especially designed for extracting complex features from specific objects such as Chinese characters. After the features of articficial images are extracted by the CSAE algorithm, the learned parameters are used to initialize the first <b>CNN</b> <b>convolutional</b> <b>layer,</b> and then the CNN model is fine-Trained by scene image patches with a linear classifier. The new CNN model is applied to Chinese scene text detection and is evaluated with a multilingual image dataset, which labels Chinese, English and numerals texts separately. More than 10 % detection precision gain is observed over two CNN models...|$|R
40|$|Rotation {{invariance}} and translation invariance {{have great}} values in image recognition tasks. In this paper, we {{bring a new}} architecture <b>in</b> <b>convolutional</b> neural network (<b>CNN)</b> named cyclic <b>convolutional</b> <b>layer</b> to achieve rotation invariance in 2 -D symbol recognition. We can also get the position and orientation of the 2 -D symbol by the network to achieve detection purpose for multiple non-overlap target. Last but not least, this architecture can achieve one-shot learning in some cases using those invariance. Comment: 7 pages, 4 figure...|$|R
30|$|Recently, neural {{networks}} (NNs) {{have become increasingly}} popular in the natural language processing (NLP) community. They learn distributed word and text representations and achieve state-of-the-art results {{on a range of}} NLP tasks. Compared with BOW representation, NNs are able to capture word order and even complex structures of textual data. Besides that, the internal semantics of words are captured due to the distributed representation. CNNs are used for sentiment analysis <b>in</b> [8, 9]. <b>CNNs</b> use <b>convolutional</b> <b>layers</b> to extract ngram features and use max-pooling layers to select the most distinct one. Recursive NNs (RecNNs), proposed by [23], construct {{neural networks}} on the basis of parse trees and are able to extract fine-grained information of sentences. Another family of NNs is Recurrent NNs (RNNs). Words in the text are fed into RNNs one by one, updating the states of hidden layers. In theory, the hidden layers store all previous information, and the hidden layer of the last word can be used as the representation of the whole text.|$|R
40|$|Minutiae play a {{major role}} in {{fingerprint}} identification. Extracting reliable minutiae is difficult for latent fingerprints which are usually of poor quality. As the limitation of traditional handcrafted features, a fully convolutional network (FCN) is utilized to learn features directly from data to overcome complex background noises. Raw fingerprints are mapped to a correspondingly-sized minutia-score map with a fixed stride. And thus a large number of minutiae will be extracted through a given threshold. Then small regions centering at these minutia points are entered into a <b>convolutional</b> neural network (<b>CNN)</b> to reclassify these minutiae and calculate their orientations. The <b>CNN</b> shares <b>convolutional</b> <b>layers</b> with the fully convolutional network to speed up. 0. 45 second is used on average to detect one fingerprint on a GPU. On the NIST SD 27 database, we achieve 53 % recall rate and 53 % precise rate that outperform many other algorithms. Our trained model is also visualized to show that we have successfully extracted features preserving ridge information of a latent fingerprint...|$|R
40|$|The {{convolutional}} {{neural network}} (ConvNet or CNN) {{has proven to be}} very successful in many tasks such as those in computer vision. In this conceptual paper, we study the generative perspective of the discriminative <b>CNN.</b> <b>In</b> particular, we propose to learn the generative FRAME (Filters, Random field, And Maximum Entropy) model using the highly expressive filters pre-learned by the <b>CNN</b> at the <b>convolutional</b> <b>layers.</b> We show that the learning algorithm can generate realistic and rich object and texture patterns in natural scenes. We explain that each learned model corresponds to a new CNN unit at a layer above the layer of filters employed by the model. We further show {{that it is possible to}} learn a new layer of CNN units using a generative CNN model, which is a product of experts model, and the learning algorithm admits an EM interpretation with binary latent variables...|$|R
40|$|<b>Convolutional</b> neural {{networks}} (<b>CNNs)</b> {{have emerged as}} one of the most successful machine learning technologies for image and video processing. The most computationally intensive parts of <b>CNNs</b> are the <b>convolutional</b> <b>layers,</b> which convolve multi-channel images with multiple kernels. A common approach to implementing <b>convolutional</b> <b>layers</b> is to expand the image into a column matrix (im 2 col) and perform Multiple Channel Multiple Kernel (MCMK) convolution using an existing parallel General Matrix Multiplication (GEMM) library. This im 2 col conversion greatly increases the memory footprint of the input matrix and reduces data locality. In this paper we propose a new approach to MCMK convolution that is based on General Matrix Multiplication (GEMM), but not on im 2 col. Our algorithm eliminates the need for data replication on the input thereby enabling us to apply the convolution kernels on the input images directly. We have implemented several variants of our algorithm on a CPU processor and an embedded ARM processor. On the CPU, our algorithm is faster than im 2 col in most cases. Comment: Camera ready version to be published at ASAP 2017 - The 28 th Annual IEEE International Conference on Application-specific Systems, Architectures and Processors. 6 page...|$|R
40|$|Deep {{learning}} with a <b>convolutional</b> neural network (<b>CNN)</b> {{has been proved}} to be very effective in feature extraction and representation of images. For image classification problems, this work aim at finding which classifier is more competitive based on high-level deep features of images. In this report, we have discussed the nearest neighbor, support vector machines and extreme learning machines for image classification under deep convolutional activation feature representation. Specifically, we adopt the benchmark object recognition dataset from multiple sources with domain bias for evaluating different classifiers. The deep features of the object dataset are obtained by a well-trained <b>CNN</b> with five <b>convolutional</b> <b>layers</b> and three fully-connected layers on the challenging ImageNet. Experiments demonstrate that the ELMs outperform SVMs in cross-domain recognition tasks. In particular, state-of-the-art results are obtained by kernel ELM which outperforms SVMs with about 4 % of the average accuracy. The features and codes are available in [URL] 7 pages, 4 figure...|$|R
40|$|Augmenting {{datasets}} by transforming inputs in a {{way that}} does not change the label is a crucial ingredient of the state of the art methods for object recognition using neural networks. However this approach has (to our knowledge) not been exploited successfully in speech recognition (with or without neural networks). In this paper we lay the foundation for this approach, and show one way of augmenting speech datasets by transforming spectrograms, using a random linear warping along the frequency dimension. In practice this can be achieved by using warping techniques that are used for vocal tract length normalization (VTLN) - with the difference that a warp factor is generated randomly each time, during training, rather than fitting a single warp factor to each training and test speaker (or utterance). At test time, a prediction is made by averaging the predictions over multiple warp factors. When this technique is applied to TIMIT using Deep Neural Networks (DNN) of different depths, the Phone Error Rate (PER) improved by an average of 0. 65 % on the test set. For a <b>Convolutional</b> neural network (<b>CNN)</b> with <b>convolutional</b> <b>layer</b> <b>in</b> the bottom, a gain of 1. 0 % was observed. These improvements were achieved without increasing the number of training epochs, and suggest that data transformations should be an important component of training neural networks for speech, especially for data limited projects. Proceedings of the 30 t...|$|R
30|$|Besides the softmax loss (classification loss) {{that was}} {{commonly}} used <b>in</b> previous <b>CNN</b> frameworks, we introduce a triplet loss (similarity loss) in our proposed architecture. Meanwhile, {{different from the}} original convolution layer <b>in</b> conventional <b>CNN</b> architectures, a doubly <b>convolutional</b> <b>layer</b> is introduced <b>in</b> the proposed <b>CNN</b> architecture.|$|R
30|$|<b>Convolutional</b> <b>layer.</b> One hundred twenty-eight kernels of 3 × 7 × 7 and 3 × 3 × 3 {{combined}} with the ReLU layer, the features in the input image are extracted furthermore. Same as the first layer <b>in</b> our <b>CNN</b> architecture.|$|R
40|$|<b>Convolutional</b> Neural Networks (<b>CNNs)</b> {{have shown}} to be {{powerful}} classification tools in tasks that range from check reading to medical diagnosis, reaching close to human perception, {{and in some cases}} surpassing it. However, the problems to solve are becoming larger and more complex, which translates to larger CNNs, leading to longer training times that not even the adoption of Graphics Processing Units (GPUs) could keep up to. This problem is partially solved by using more processing units and distributed training methods that are offered by several frameworks dedicated to neural network training. However, these techniques do not take full advantage of the possible parallelization offered by CNNs and the cooperative use of heterogeneous devices with different processing capabilities, clock speeds, memory size, among others. This paper presents a new method for the parallel training of CNNs that can be considered as a particular instantiation of model parallelism, where only the <b>convolutional</b> <b>layer</b> is distributed. In fact, the convolutions processed during training (forward and backward propagation included) represent from 60 - 90 % of global processing time. The paper analyzes the influence of network size, bandwidth, batch size, number of devices, including their processing capabilities, and other parameters. Results show that this technique is capable of diminishing the training time without affecting the classification performance for both CPUs and GPUs. For the CIFAR- 10 dataset, using a <b>CNN</b> with two <b>convolutional</b> <b>layers,</b> and 500 and 1500 kernels, respectively, best speedups achieve 3. 28 × using four CPUs and 2. 45 × with three GPUs. Modern imaging datasets, larger and more complex than CIFAR- 10 will certainly require more than 60 - 90 % of processing time calculating convolutions, and speedups will tend to increase accordingly...|$|R
30|$|The goal of {{this work}} is to compare the feature {{learning}} ability of <b>convolutional</b> (<b>CNN)</b> and fully connected layers (DNN), {{and that is why}} the compared models have the same architectures except for the first hidden layer. This allows us to fairly compare the spectrogram-modeling capabilities of both types of layers. The reader can refer to recent studies such as [12] in which the number of <b>convolutional</b> <b>layers</b> is adequately evaluated.|$|R
30|$|In {{the first}} <b>convolutional</b> <b>layer,</b> the input image patch is {{continuously}} filtered with 48 feature maps of 3 × 11 × 11 and 3 × 7 × 7 kernels with a stride of 2. The second <b>convolutional</b> <b>layer</b> continuously filters {{the output of}} the first <b>convolutional</b> <b>layer</b> with 128 feature maps of 3 × 9 × 9 and 3 × 5 × 5 kernels. The third <b>convolutional</b> <b>layer</b> filters {{the output of the}} second <b>convolutional</b> <b>layer</b> with 128 feature maps of 3 × 7 × 7 and 3 × 3 × 3 kernels, sequentially. The following fully connected layer has 512 neurons.|$|R
30|$|In [7], D. Zeiler {{presented}} a view, the <b>convolutional</b> <b>layers</b> <b>in</b> the <b>CNN</b> played the analogous role of traditional feature descriptors that extracted feature map from images. And the <b>convolutional</b> <b>layers</b> are changeable compared with traditional feature descriptors. We {{just need to}} train the CNN with different image datasets. When we want to obtain a special feature map to distinguish similar categories, we only need to train the CNN with a small image dataset that constituted by the similar categories. This characteristic of the CNN makes it possible that we obtain special feature map of all similar categories.|$|R
5000|$|Shared weights: <b>In</b> <b>CNNs,</b> each filter is {{replicated}} {{across the}} entire visual field. These replicated units share the same parameterization (weight vector and bias) and form a feature map. This means that all the neurons in a given <b>convolutional</b> <b>layer</b> respond to the same feature (within their specific response field). Replicating units in this way allows for features to be detected regardless of {{their position in the}} visual field, thus constituting the property of translation invariance.|$|R
40|$|We {{present a}} new deep network <b>layer</b> called “Dynamic <b>Convolutional</b> <b>Layer</b> ” {{which is a}} {{generalization}} of the con-volutional <b>layer.</b> The conventional <b>convolutional</b> <b>layer</b> uses filters that are learned during training and are held constant during testing. In contrast, the dynamic <b>convolutional</b> <b>layer</b> uses filters that will vary from input to input during testing. This is achieved by learning a function that maps the input to the filters. We apply the dynamic <b>convolutional</b> <b>layer</b> to the application of short range weather prediction and show performance improvements compared to other baselines. 1...|$|R
40|$|DS (Delay-sum) {{beamformer}} ? ???? ?? ??? ??? <b>CNN</b> (<b>Convolutional</b> neural network) ? ?????, ?? ?? ????? <b>CNN</b> ? ?? ???? ?? ???? ??[1]? ?????. ??? ??? ????? ?? ????? ???? CNN ? ????? ?? ???? PMWF (Parameterized multichannel non-causal Wiener filter) ? ???? ?? ?? ????. ??? ?? ??? ?? ??? PMWF ??? CNN ??? crosschannel CNN ? DS beamformer ?? ????? ?? ???. ? ??? 2015 ?? ??(???????) ? ???? ??????? ??? ?? ??? ???(No. 2014 R 1 A 2 A 1 A 10049735) ...|$|R
30|$|<b>Convolutional</b> <b>layer</b> 1. There are 48 kernels (size 17 × 17, stride of 2) in {{the first}} <b>convolutional</b> <b>layer,</b> which is {{combined}} with one maxout operator and one max pooling layer.|$|R
30|$|<b>Convolutional</b> <b>layer</b> 2. 128 kernels of size 3 × 5 × 5 (with a stride of 2) {{are applied}} to the input banana image in the second layer {{combined}} with the ReLU. A max pooling <b>layer</b> follows this <b>convolutional</b> <b>layer.</b>|$|R
30|$|When {{comparing}} padded vs. non-padded <b>convolutional</b> <b>layers,</b> {{we found}} {{little difference in}} classification performance; however, using padding to enable creation of deeper networks increased training time by 2.72 ×, and using padding while doubling the number of <b>convolutional</b> <b>layers</b> increased training time by 3.26 ×.|$|R
30|$|<b>Convolutional</b> <b>layer</b> 1. 48 kernels of size 3 × 7 × 7 (3 {{represents}} {{the number of}} RGB channels, with a stride of 2) are applied to the input banana image in the first layer combined with the ReLU. A max pooling <b>layer</b> follows this <b>convolutional</b> <b>layer.</b>|$|R
30|$|<b>Convolutional</b> {{neural network}} (<b>CNN)</b> {{is a classic}} visual {{learning}} method. With the development of large-scale computing and GPU accelerating, huge CNN frameworks can be set with tens of <b>convolutional</b> <b>layers.</b> The newest development of residual network [12] applied in image classification even used more than 150 <b>convolutional</b> <b>layers.</b>|$|R
30|$|As noted above, {{when using}} log-m {{embedding}} for our two alphabet sizes, {{we have an}} input volume of 8 × l. Without padding, each <b>convolutional</b> <b>layer</b> reduces the dimension by 2. Thus, after three layers {{we are looking at}} an (8 - 3 * 2) × (l- 3 * 2) or 2 × (l- 6) volume. Since we have a dimension of 2 in one direction, no further <b>convolutional</b> <b>layers</b> with 3 × 3 filters can be applied. This sets a limit of three for the maximum number of 3 × 3 <b>convolutional</b> <b>layers</b> that can be used with this embedding. By using padding, this limit can be removed. Since padding adds a border of zeroes around our input volume to prevent its dimension from being reduced by each <b>convolutional</b> <b>layer,</b> we no longer have a limit to the number of layers employed. Thus, we can design a network similar to AlexNet [7], one of the highest performing network architectures for image classification. This architecture consists of stacking three 3 × 3 <b>convolutional</b> <b>layers,</b> followed by a 2 × 2 max pooling layer. Again, our small dimension (8) imposes a limit on the number of layer stacks we can have due to max pooling. After our first max pooling layer, we have a 4 × (l/ 2) volume, after the second we have a 2 × (l/ 4) volume and no further 3 × 3 convolutions can be performed. Thus, for the deepest network, we can construct an architecture consisting of three <b>convolutional</b> <b>layers</b> followed by a max pooling <b>layer,</b> three more <b>convolutional</b> <b>layers,</b> a second max pooling layer, then the fully connected layers.|$|R
40|$|In this note, we want {{to focus}} on aspects related to two {{questions}} most people asked us at CVPR about the network we presented. Firstly, What is the relationship between our proposed layer and the deconvolution layer? And secondly, why are convolutions in low-resolution (LR) space a better choice? These are key questions we tried to answer in the paper, but {{we were not able to}} go into as much depth and clarity as we would have liked in the space allowance. To better answer these questions in this note, we first discuss the relationships between the deconvolution layer in the forms of the transposed convolution <b>layer,</b> the sub-pixel <b>convolutional</b> <b>layer</b> and our efficient sub-pixel <b>convolutional</b> <b>layer.</b> We will refer to our efficient sub-pixel <b>convolutional</b> <b>layer</b> as a <b>convolutional</b> <b>layer</b> in LR space to distinguish it from the common sub-pixel <b>convolutional</b> <b>layer.</b> We will then show that for a fixed computational budget and complexity, a network with convolutions exclusively in LR space has more representation power at the same speed than a network that first upsamples the input in high resolution space. Comment: This is a note to share some additional insights for our the CVPR pape...|$|R
30|$|We use VGG- 16 as the {{baseline}} network, which is pre-trained with ImageNet [23] dataset. It has 13 <b>convolutional</b> <b>layer</b> and three fully connected layers. In {{order to improve}} detection speed and reduce the parameters, we use <b>convolutional</b> <b>layer</b> instead {{of the last three}} fully connected layers. It has been proved to be effective in paper [8].|$|R
5000|$|... #Caption: Neurons of a <b>convolutional</b> <b>layer</b> (blue), {{connected}} to their receptive field (red) ...|$|R
40|$|We propose local binary {{convolution}} (LBC), {{an efficient}} alternative to <b>convolutional</b> <b>layers</b> in standard <b>convolutional</b> neural networks (<b>CNN).</b> The design principles of LBC {{are motivated by}} local binary patterns (LBP). The LBC layer comprises {{of a set of}} fixed sparse pre-defined binary convolutional filters that are not updated during the training process, a non-linear activation function and a set of learnable linear weights. The linear weights combine the activated filter responses to approximate the corresponding activated filter responses of a standard <b>convolutional</b> <b>layer.</b> The LBC layer affords significant parameter savings, 9 x to 169 x in the number of learnable parameters compared to a standard <b>convolutional</b> <b>layer.</b> Furthermore, the sparse and binary nature of the weights also results in up to 9 x to 169 x savings in model size compared to a standard <b>convolutional</b> <b>layer.</b> We demonstrate both theoretically and experimentally that our local binary convolution layer is a good approximation of a standard <b>convolutional</b> <b>layer.</b> Empirically, <b>CNNs</b> with LBC layers, called local binary convolutional neural networks (LBCNN), achieves performance parity with regular CNNs on a range of visual datasets (MNIST, SVHN, CIFAR- 10, and ImageNet) while enjoying significant computational savings. Comment: To appear in CVPR 2017 as Spotligh...|$|R
40|$|<b>Convolutional</b> neural {{networks}} (<b>CNNs)</b> are {{a variant of}} deep {{neural networks}} (DNNs) optimized for visual pattern recognition, which are typically trained using first order learning algorithms, particularly stochastic gradient descent (SGD). Training deeper CNNs (deep learning) using large data sets (big data) {{has led to the}} concept of distributed machine learning (ML), contributing to state-of-the-art performances in solving computer vision problems. However, there are still several outstanding issues to be resolved with currently defined models and learning algorithms. Propagations through a <b>convolutional</b> <b>layer</b> require flipping of kernel weights, thus increasing the computation time of a CNN. Sigmoidal activation functions suffer from gradient diffusion problem that degrades training efficiency, while others cause numerical instability due to unbounded outputs. Common learning algorithms converge slowly and are prone to hyperparameter overfitting problem. To date, most distributed learning algorithms are still based on first order methods that are susceptible to various learning issues. This thesis presents an efficient CNN model, proposes an effective learning algorithm to train CNNs, and map it into parallel and distributed computing platforms for improved training speedup. The proposed <b>CNN</b> consists of <b>convolutional</b> <b>layers</b> with correlation filtering, and uses novel bounded activation functions for faster performance (up to 1. 36 x), improved learning performance (up to 74. 99 % better), and better training stability (up to 100 % improvement). The bounded stochastic diagonal Levenberg-Marquardt (B-SDLM) learning algorithm is proposed to encourage fast convergence (up to 5. 30 % faster and 35. 83 % better than first order methods) while having only a single hyperparameter. B-SDLM also supports mini-batch learning mode for high parallelism. Based on known previous works, this is among the first successful attempts of mapping a stochastic second order learning algorithm to be deployed in distributed ML platforms. Running the distributed B-SDLM on a 16 - core cluster achieves up to 12. 08 x and 8. 72 x faster to reach a certain convergence state and accuracy on the Mixed National Institute of Standards and Technology (MNIST) data set. All three complex case studies tested with the proposed algorithms give comparable or better classification accuracies compared to those provided in previous works, but with better efficiency. As an example, the proposed solutions achieved 99. 14 % classification accuracy for the MNIST case study, and 100 % for face recognition using AR Purdue data set, which proves the feasibility of proposed algorithms in visual pattern recognition tasks...|$|R
50|$|Alexnet {{contained}} only 8 layers, first 5 were <b>convolutional</b> <b>layers</b> {{followed by}} fully connected layers.|$|R
30|$|Convolutional neural {{networks}} consist of sparsely connected <b>convolutional</b> <b>layers</b> followed by fully connected dense layers (these dense layers are {{equivalent to a}} multilayer perceptron neural network). <b>Convolutional</b> <b>layers</b> are sparsely connected. Neurons in these layers are connected to a small region of the previous layer known as the receptive field, instead of the entire previous layer as is found in a dense layer. The most common receptor field sizes are small, such as 3 × 3 or 5 × 5 neurons. By being sparsely connected CNNs view and learn local correlations. The <b>convolutional</b> <b>layers</b> in the network are followed by several densely connected layers with the last layer containing one neuron for each possible classification outcome.|$|R
