0|10000|Public
30|$|An inter-coder {{reliability}} test was conducted, the coders being {{the writer and}} a research assistant. The test drew on Cohen’s kappa that calculates a degree of agreement between multiple coders, rectifying some agreement which occurs by chance (see Loewen and Plonsky 2015). The author provided the research assistant with <b>explanations</b> <b>of</b> <b>the</b> <b>rubrics</b> and she analysed the writing tasks independently. Agreement for each <b>of</b> <b>the</b> <b>rubrics</b> used in this article was calculated. Scores provided by the test were 0.9 for expositions (analytical), 0.4 for expositions (hortatory), 1.0 for personal reflections (opinion), 0.7 for personal reflections (imaginary), 0.9 for personal reflections (descriptive), 0.8 for discussions, 1.0 for sequential explanation, and 1.0 for an unclear task. Even {{though there is no}} common standard as to what score represents an acceptable score, scores as low as 0.7 can generally be considered to indicate an acceptable level of agreement (Fleiss et al. 2003). With this standard, all <b>the</b> classifications <b>of</b> <b>the</b> <b>rubrics</b> except for expositions (hortatory) can be considered to be at an acceptable level <b>of</b> agreement. <b>The</b> reason why the score for expositions (hortatory) was low may be attributed to <b>the</b> complexity <b>of</b> <b>the</b> two types <b>of</b> expositions, hortatory and analytical. For a writing task in the examinations such as “Do you think that rich countries should give aid to poor countries?”, examinees can write an essay to justify their ideas or convince people in rich countries to take some action.|$|R
40|$|In 2010, Oregon Department of Education (ODE) {{developed}} {{a set of}} rubrics designed to score a state required performance assessment targeting Science Inquiry (SI) and Engineering Design (ED) skills. During <b>the</b> development <b>of</b> <b>the</b> <b>rubrics,</b> ODE invited six panels of teachers to provide feedback on an early draft <b>of</b> <b>the</b> <b>rubrics.</b> This case study analyzed the teachers 2 ̆ 7 feedback and <b>the</b> revisions <b>of</b> <b>the</b> <b>rubrics</b> to identify <b>the</b> types <b>of</b> feedback teachers offered and how ODE used that feedback to develop <b>the</b> <b>rubrics.</b> <b>The</b> results showed the teachers 2 ̆ 7 feedback focused on defining the skills {{students were asked to}} demonstrate and distinguishing levels of student performance. There was clear evidence that the teachers 2 ̆ 7 feedback had a substantial impact on <b>the</b> development <b>of</b> <b>the</b> <b>rubrics.</b> These results suggest that teachers can add substantial value during <b>the</b> development <b>of</b> a state issued assessment tool...|$|R
5000|$|... #Subtitle level 3: Reform <b>of</b> <b>the</b> <b>rubrics</b> and liturgical {{calendar}} ...|$|R
30|$|To {{validate}} <b>the</b> <b>rubric,</b> {{our team}} developed empirical studies to examine <b>the</b> technical quality <b>of</b> <b>the</b> <b>rubric.</b> We took an argument-based approach to establish validity, which views validity {{as a process}} of triangulating evidence and logical analysis to evaluate claims and propositions of an interpretive argument within <b>the</b> constraints <b>of</b> <b>the</b> assessment’s intended interpretations and uses (AERA, APA and NCME 2014). The validity studies occurred across two separate phases. <b>The</b> first phase <b>of</b> activities involved expert review <b>of</b> <b>the</b> <b>rubric</b> and rating <b>the</b> importance <b>of</b> attributes covered {{in order to ensure that}} <b>the</b> <b>rubric</b> appropriately measures our targeted construct within <b>the</b> context <b>of</b> extant literature, and as a way of establishing construct validity <b>of</b> <b>the</b> <b>rubric.</b> During <b>the</b> second phase <b>of</b> activities, we ensured usage of rubric was consistent across raters to determine inter-rater reliability.|$|R
30|$|Validating <b>the</b> <b>rubric</b> also {{demonstrated}} that the various problems posed to students within STEAM activities, compounded by <b>the</b> contextualized nature <b>of</b> learning and instruction in K- 12 classrooms (where students, structure and resources vary greatly), complicate effective use <b>of</b> <b>the</b> <b>rubric.</b> One way to address consistent use <b>of</b> <b>the</b> <b>rubric</b> is ensuring a common {{understanding of how to}} use <b>the</b> <b>rubric</b> across contexts. To that end, separate researcher and teacher instructions for use have been developed with setup tips, explanations and definitions for the dimensions and attributes <b>of</b> <b>the</b> <b>rubric,</b> and suggestions for how to use the observation notes provided within each dimension. Setup tips include instructions for video-recording instances of student collaboration, capturing verbal interactions, noting the problem or scenario being worked on, suggesting <b>the</b> length <b>of</b> each observation, and avoiding disruptions as students are working.|$|R
5000|$|Each rubric {{consists}} in an alphanumeric code (the letter {{corresponds to the}} chapters and the number to the component) and each title <b>of</b> <b>the</b> <b>rubric</b> (<b>the</b> assigned name) is expressed and explained by: ...|$|R
40|$|The EAP written {{multiple}} trait <b>rubric</b> used in <b>the</b> City University <b>of</b> Hong Kong {{is believed}} to be of central importance to formative and high-stakes summative assessment in the institution. Crucial to both of these roles are the perceptions and interpretations <b>of</b> <b>the</b> key stakeholders: teachers and students. The learning and test scores deriving from <b>the</b> <b>rubric</b> are filtered entirely through these stakeholders. Investigating <b>the</b> perceived effects <b>of</b> <b>the</b> <b>rubric</b> on <b>the</b> EAP assessment's validity, reliability and student learning (three key strands revealed in testing literature) is seen as being essential as proof <b>of</b> <b>the</b> <b>rubric's</b> value. This paper presents an analysis of teacher (n= 25) and student (n= 123) perceptions of an EAP rubric, investigating core elements of both, comparing them, and probing into whether teachers' interpretations of rubrics influence their students. A mixed-methods study seeks to determine perceptions through combining qualitative analysis of interview data with quantitative analysis of questionnaire responses. Key elements of rubrics and how they both impact and are impacted by stakeholder perceptions are discussed. Findings indicate several strong trends in student and teacher perceptions <b>of</b> <b>the</b> <b>rubric,</b> and tentatively illustrate how teachers may affect their students. Arguments are made for a greater focus on standardising the teaching and learning <b>of</b> <b>the</b> <b>rubric,</b> for greater realisation <b>of</b> <b>the</b> learning potential <b>of</b> <b>the</b> <b>rubric,</b> and for investigating <b>the</b> appropriacy <b>of</b> certain domains and wordings. published_or_final_versionApplied English StudiesMasterMaster of Arts in Applied Linguistic...|$|R
5000|$|Ali Baddou : Literature {{and culture}} (<b>of</b> <b>the</b> <b>rubric</b> Ali a lu), {{official}} joker officiel of Michel Denisot ...|$|R
40|$|<b>The</b> goal <b>of</b> this {{exploratory}} {{study was to}} investigate if color coding the scores on an evaluation rubric had any effects on 1) <b>the</b> students’ perceptions <b>of</b> <b>the</b> <b>rubric,</b> and 2) their performance on self-assessments. Although <b>the</b> use <b>of</b> a color-coding system on rubrics has been used previously, primarily for K- 12 students, the system used a different color for each concept or component <b>of</b> <b>the</b> <b>rubric,</b> rather than using color as a visual cue for student performance...|$|R
5000|$|A College {{tradition}} holds (with reported sightings) that Ford's ghost wanderings <b>the</b> <b>Rubrics,</b> that “his ghost, {{dressed in}} wig, gown and knee breeches, {{is said to}} walk by <b>the</b> side <b>of</b> <b>the</b> <b>Rubrics</b> at dusk.” ...|$|R
50|$|A History <b>of</b> <b>the</b> Nonjurors, their Controversies and Writings, with Remarks on some <b>of</b> <b>the</b> <b>Rubrics</b> in <b>the</b> Book <b>of</b> Common Prayer, London, 1845.|$|R
40|$|In this paper, {{the author}} {{discusses}} <b>the</b> development <b>of</b> an appraisal instrument designed for evaluating submissions to The Qualitative Report-the TQR Rubric. Following a description <b>of</b> <b>the</b> context <b>of</b> TQR, she explains {{what led to}} <b>the</b> development <b>of</b> <b>the</b> TQR <b>Rubric</b> and describes its components. She concludes by presenting <b>the</b> plan <b>of</b> implementation <b>of</b> <b>the</b> <b>rubric</b> and a discussion <b>of</b> how <b>the</b> TQR <b>Rubric</b> 2 ̆ 7 s elements relate to notions of quality presented in the literature...|$|R
50|$|You wrote, {{we played}} (Вы написали — мы сыграли) — a {{replacement}} <b>of</b> <b>the</b> Aperitif <b>rubric</b> for <b>the</b> {{third and fourth}} seasons, featuring sketches written by viewers for a contest. Most <b>of</b> <b>the</b> sent stories were already well-known among people or were harmless, making the actors come up with half <b>of</b> <b>the</b> <b>rubric's</b> content by themselves.|$|R
40|$|Rubrics {{are being}} used {{in a wide variety}} of {{disciplines}} in higher education to evaluate the assessments and provide feedback to students. Rubrics are traditionally implemented as paper - based table to grade assessments and provide feedback. <b>The</b> advancement <b>of</b> technology has integrated <b>the</b> <b>Rubric</b> Tool into Learning Management Systems (LMS) like Blackboard to facilitate online marking. In this conceptual paper, we aim to evaluate <b>the</b> effectiveness <b>of</b> <b>the</b> <b>Rubric</b> Tool. We propose an integrated conceptual framework applying various theories from the extant literature to assess <b>the</b> effectiveness <b>of</b> <b>the</b> <b>Rubric</b> Tool. Furthermore, we use the framework to evaluate <b>the</b> effectiveness <b>of</b> <b>the</b> Blackboard LMS <b>Rubric</b> Tool applied in an undergraduate course for grading assessments under <b>the</b> settings <b>of</b> an Australian International University in Asia...|$|R
3000|$|To analyze <b>the</b> content <b>of</b> <b>the</b> blog posts, a rubric was {{developed}}, {{prior to}} reading the blog. It was developed {{prior to the}} reading, not afterwards, {{in order to avoid}} bias in <b>the</b> construction <b>of</b> <b>the</b> instrument. <b>The</b> <b>rubric</b> included elements that might be expected to be included in <b>the</b> written reflections <b>of</b> a sojourner experiencing a new culture for the first time. <b>The</b> <b>rubric</b> included four elements: Culture Shock, Intercultural Communication Challenges (verbal and nonverbal), Cross-Cultural Comparison, and Intercultural Adaptation. Each of those elements <b>of</b> <b>the</b> <b>rubric</b> was operationalized in detail based on the relevant literature discussed earlier. <b>The</b> <b>rubric</b> is as follows: [...]...|$|R
30|$|This {{limitation}} {{also brings}} about another {{problem on the}} lengthy and vague descriptors <b>of</b> <b>the</b> <b>rubric.</b> As mentioned in <b>the</b> early part <b>of</b> this section, <b>the</b> lengthy and vague descriptors in <b>the</b> <b>rubric</b> discourage students to read it because it can just trigger anxiety and probably, further confusion.|$|R
30|$|Last, we {{determined}} that <b>the</b> current style <b>of</b> <b>the</b> <b>rubric</b> {{would need to}} be edited to include a place for users to make notes in order to provide additional feedback to the student, as necessary.|$|R
40|$|<b>The</b> purpose <b>of</b> {{this study}} was to develop {{specific}} rubrics for assessing critiques in English 202, an English as a foreign language course at the Lebanese American University to explore <b>the</b> effectiveness <b>of</b> these <b>rubrics</b> on students' writing and to investigate students' attitudes and reactions to <b>the</b> use <b>of</b> these <b>rubrics.</b> After development <b>of</b> <b>the</b> <b>rubrics,</b> students wrote an in-class critique of an article, using <b>the</b> <b>rubrics</b> as a guide. Based on teachers' comments and grades on <b>the</b> <b>rubrics,</b> students revised and wrote the second draft. Students were then administered an anonymous online survey asking them for feedback on <b>the</b> use <b>of</b> <b>rubrics.</b> A t-test comparing students' grades on the first and second drafts revealed a significant difference, suggesting that use <b>of</b> <b>the</b> <b>rubrics</b> helped students effectively revise their drafts. The survey results support this hypothesis because students revealed positive attitudes and reactions. Based on the findings, we recommend that consistent use of well-developed rubrics can enhance outcomes for English learners enrolled in writing courses. <b>The</b> purpose <b>of</b> {{this study was}} to develop specific rubrics for assessing critiques in English 202, an English as a foreign language course at the Lebanese American University to explore <b>the</b> effectiveness <b>of</b> these <b>rubrics</b> on students' writing and to investigate students' attitudes and reactions to <b>the</b> use <b>of</b> these <b>rubrics.</b> After development <b>of</b> <b>the</b> <b>rubrics,</b> students wrote an in-class critique of an article, using <b>the</b> <b>rubrics</b> as a guide. Based on teachers' comments and grades on <b>the</b> <b>rubrics,</b> students revised and wrote the second draft. Students were then administered an anonymous online survey asking them for feedback on <b>the</b> use <b>of</b> <b>rubrics.</b> A t-test comparing students' grades on the first and second drafts revealed a significant difference, suggesting that use <b>of</b> <b>the</b> <b>rubrics</b> helped students effectively revise their drafts. The survey results support this hypothesis because students revealed positive attitudes and reactions. Based on the findings, we recommend that consistent use of well-developed rubrics can enhance outcomes for English learners enrolled in writing courses. PublishedN/...|$|R
30|$|It {{seems that}} in {{analytic}} methods, the precision and rigidity of various subcomponents refrain the raters from deviation from <b>the</b> principles <b>of</b> <b>the</b> <b>rubric,</b> and such precision and detailed scrutiny leads to <b>the</b> efficiency <b>of</b> <b>the</b> method.|$|R
30|$|Following the {{criteria}} and scale in <b>the</b> <b>rubric</b> {{does not imply}} that the teacher-raters’ assessment constructs changed or that they agreed <b>the</b> <b>Rubric</b> was <b>the</b> best tool for assessing the given task. In fact, Eunice and Logan expressed dissatisfaction in <b>the</b> <b>Rubric</b> and thought that it did not fit well for assessing short descriptive EFL writing. Specifically, Logan thought that <b>the</b> <b>Rubric</b> was too long, complex and difficult to use. In addition, Eunice believed that <b>the</b> <b>Rubric</b> should put more emphasis on vocabulary and sentence-level issues rather than on <b>the</b> strength <b>of</b> ideas. Ben and Matt did not like the fact that <b>the</b> <b>Rubric</b> covered grammar and mechanics under the same category; both wanted to split the categories and put a stronger emphasis on grammar. Despite their dissatisfaction with certain areas <b>of</b> <b>the</b> <b>Rubric,</b> <b>the</b> teacher-raters in the study still made an effort to follow it. For them, <b>the</b> <b>Rubric</b> overrode their personal assessment beliefs. In the interview, Susan commented, “I usually follow <b>the</b> <b>rubric</b> in <b>the</b> order it is given to me, because I think that is what I am asked to do. I do not put my two cents in. I try to stay true to <b>the</b> intent <b>of</b> <b>the</b> <b>rubric,</b> without any embellishment on my part.” Susan also added that when giving grades she tells her students, “It wasn’t me who gave you this grade, <b>the</b> <b>rubric</b> did”.|$|R
40|$|<b>The</b> purpose <b>of</b> {{this study}} is to {{determine}} the relation between the teachers’ and <b>the</b> students’ understanding <b>of</b> music <b>rubrics.</b> <b>The</b> study will present <b>the</b> use <b>of</b> music <b>rubrics</b> in three different schools. In these schools, observations were performed as well as interviews with the teachers and their students. A study <b>of</b> <b>the</b> <b>rubrics</b> on each school was also executed to construct appropriate questions and an observation schedule. To analyse our material a phenomenographical method was used. The result shows that the teachers and students have similar understandings and interpretations <b>of</b> <b>the</b> content as well as <b>the</b> purpose <b>of</b> <b>the</b> music <b>rubric.</b> ...|$|R
6000|$|On <b>the</b> communion-table <b>of</b> <b>the</b> {{pretty little}} church there was spread the [...] "fair white cloth" [...] <b>of</b> <b>the</b> <b>rubric.</b> It was <b>the</b> day for <b>the</b> monthly celebration <b>of</b> <b>the</b> Sacrament, that met <b>the</b> {{religious}} requirements <b>of</b> <b>the</b> village.|$|R
30|$|For performance-based assessment, rubrics are {{a central}} tool that adds reliability, validity, and {{transparency}} to assessments. This {{study shows that}} experienced teachers-raters were impacted by the content and nature <b>of</b> <b>the</b> <b>rubric’s</b> scale, and thus {{made an effort to}} follow it.|$|R
40|$|<b>The</b> {{objective}} <b>of</b> {{this research}} {{is to develop a}} rubric for portfolio assessment in writing of grade VIII students at SMP Negeri 15 Yogyakarta. This research applied a Research and Development study which adopts <b>the</b> model <b>of</b> R & D proposed by Dick and Carey in Gall, Gall and Borg (2003). The data collected were qualitative and quantitative. The qualitative data were in <b>the</b> form <b>of</b> interview transcript and the quantitative data were in <b>the</b> form <b>of</b> students’ writing scores. These data were used to evaluate and then to revise <b>the</b> <b>rubric.</b> On <b>the</b> basis <b>of</b> some theories related to <b>the</b> development <b>of</b> effective scoring <b>rubrics</b> and <b>the</b> result <b>of</b> <b>the</b> needs analysis, <b>the</b> first draft <b>of</b> <b>the</b> <b>rubric</b> was then developed. The researcher consulted <b>the</b> <b>rubric</b> to a writing expert to know whether <b>the</b> <b>rubric</b> meets criteria which are appropriate to measure the students’ writing. After <b>the</b> <b>rubric</b> was implemented, an evaluation was then conducted. The researcher conducted interviews with the English teachers who participated in the implementation to find out the strengths and weaknesses <b>of</b> <b>the</b> <b>rubric</b> and to ask the teachers’ suggestion concerning <b>the</b> <b>rubric.</b> <b>The</b> data <b>of</b> <b>the</b> interview were used to revise <b>the</b> <b>rubric.</b> <b>The</b> research was conducted through six steps, they are: researching and collecting the information, planning, developing <b>the</b> <b>rubric,</b> obtaining expert judgment, field testing and developing <b>the</b> final product <b>of</b> <b>the</b> <b>rubric.</b> <b>The</b> instrument used in collecting the data was interviews and questionnaires to cross check <b>the</b> result <b>of</b> <b>the</b> interviews. The finding shows that there are four dimensions of portfolio assessment in the rubric: (1) characteristics <b>of</b> <b>the</b> writer, (2) characteristics <b>of</b> <b>the</b> portfolio as a whole, (3) characteristics of individual texts, and (4) intratextual features. Each dimension is written in three properties <b>of</b> <b>the</b> rubric: criteria, levels and descriptors. Each dimension has criteria which indicate good performance on a task. Each criterion is graded in five levels <b>of</b> performances, and <b>the</b> descriptors tell precisely what the performance looks like at each level...|$|R
40|$|DESIGNING A RUBRIC TO ASSESS VOCATIONAL HIGH SCHOOL STUDENTS’ WRITING By: Ingrita Dewi Puspasari NIM 06202241042 ABSTRACT This study aims {{to design}} an {{analytical}} rubric to assess vocational high school students’ writing. It {{is an effort}} to overcome writing assessment problems in vocational high schools. This was a research and development study. It was conducted by modifying and simplifying the model proposed by Jolly and Bolitho in Tomlinson (1998) into the following procedures: conducting a needs analysis, exploring the needs, designing <b>the</b> <b>rubric,</b> implementing <b>the</b> <b>rubric,</b> evaluating <b>the</b> <b>rubric,</b> and finally writing <b>the</b> final draft <b>of</b> <b>the</b> <b>rubric.</b> Questionnaires and interviews were used as the instruments to gather the data in this study. Based on <b>the</b> result <b>of</b> <b>the</b> needs analysis, the researcher designed <b>the</b> <b>rubric</b> by involving a writing expert to investigate construct validity until <b>the</b> <b>rubric</b> was considered appropriate to be implemented. <b>The</b> <b>rubric</b> was implemented in two stages: tryout and implementation. The tryout was conducted by using <b>the</b> <b>rubric</b> which was validated by the writing expert. <b>The</b> <b>rubric</b> was tried out to evaluate students’ writing. Three English teachers were involved in this stage. <b>The</b> evaluation <b>of</b> <b>the</b> <b>rubric</b> after <b>the</b> tryout was conducted to obtain teachers’ or raters’ experience in assessing students’ performance through <b>the</b> <b>rubric.</b> <b>The</b> revision then was made as necessary based on the teachers’ or raters’ suggestions. <b>The</b> revised <b>rubric</b> was then implemented to evaluate students’ writing in the implementation stage. Based on <b>the</b> result <b>of</b> <b>the</b> implementation, <b>the</b> <b>rubric</b> was considered appropriate to evaluate students’ writing. Therefore, it was regarded as <b>the</b> final draft <b>of</b> <b>the</b> <b>rubric.</b> <b>The</b> findings show that <b>the</b> designed <b>rubric</b> covers eight aspects of writing performance, namely relevance and adequacy of content, compositional organization, cohesion, adequacy of vocabulary for purpose, grammar, mechanical accuracy I (punctuation), mechanical accuracy II (spelling) and mechanical accuracy III (capitalization). Each aspect of writing performance is written in three important properties of rubric, namely (1) criteria, (2) levels of scores, and (3) descriptors. The criteria represent indicators of good performance on a task. Each aspect of writing is graded into five levels of scores ranging from one to five. In addition, each score has its descriptor which describes specifically what performance looks like. The reliability coefficients that measure <b>the</b> consistency <b>of</b> ratings among different raters are also obtained through the Pearson Product Moment correlation using SPSS 13. 0 and the results show that <b>the</b> designed <b>rubric</b> has high agreement among raters as the reliability coefficients are all above 0. 800...|$|R
30|$|Additionally, {{survey and}} {{interview}} questions asked panelists to rate <b>the</b> <b>rubric’s</b> ease <b>of</b> use and suggest recommendations for improvement. Panelists {{were also asked}} to rate their ability to understand terms, use and score all <b>of</b> <b>the</b> dimensions, to reach consensus, and whether <b>the</b> second iteration <b>of</b> <b>the</b> <b>rubric</b> appropriately assesses CPS in STEAM activities. In general, they responded that they had little difficulty learning to use <b>the</b> <b>rubric,</b> <b>the</b> instructions to clarify terms were helpful and important, reaching consensus was not difficult, and they experienced very little difficulty in using <b>the</b> <b>rubric</b> to individually determine a score. Using a Likert scale (1 = very poorly, 5 = very well) to indicate to what extent they believed <b>the</b> <b>rubric</b> appropriately assesses CPS in STEAM activities, five panelists rated it as 5, with just one panelist rating it as 3. In follow-up interviews, that panelist expressed hesitancy over <b>the</b> length <b>of</b> <b>the</b> <b>rubric.</b> He admitted that he found it easy to use while scoring students in the videos, however he was not sure how accurately he could assess an entire classroom of students and therefore was unsure if it would appropriately assess CPS in STEAM activities within his classroom.|$|R
5000|$|In {{his book}} <b>The</b> Simplification <b>of</b> <b>the</b> <b>Rubrics,</b> {{explaining}} <b>the</b> changes, Monsignor Annibale Bugnini commented, [...] "The present decree has a contingent character. It {{is essentially a}} bridge between the old and the new, and if you will, an arrow indicating the direction taken by the current restoration." ...|$|R
40|$|This study seeks firstly to {{understand}} <b>the</b> types <b>of</b> ePortfolios that <b>the</b> learners {{are expected to}} develop as per <b>the</b> <b>rubric,</b> secondly {{to understand}} <b>the</b> various types <b>of</b> knowledge that learners demonstrate during <b>the</b> process <b>of</b> creating ePortfolios, thirdly to determine the ePortfolio activity systems of second year learners and fourthly to determine <b>the</b> effectiveness <b>of</b> <b>the</b> <b>rubric</b> in assessing <b>the</b> various types <b>of</b> knowledge demonstrated by the learners while creating their ePortfolio...|$|R
40|$|This study investigates <b>the</b> {{assessment}} <b>of</b> creative {{essays in}} grade 10 Sesotho home language. Nine participants from {{a total of}} six schools took part in the research. For <b>the</b> purpose <b>of</b> this study, no literature was found on <b>the</b> assessment <b>of</b> Sesotho essays (or essay writing in any other African language) in general or specific to creative writing in high schools in South Africa. The literature on English first language teaching and English second language teaching were then used to theoretically contextualise the writing and assessment of creative writing essays in Sesotho home language in South African high schools. Data were collected through questionnaires completed by teachers, an analysis of a sample of marked scripts (representing above average, average and below average grades) and interviews with teachers (tailored to investigate <b>the</b> asset <b>of</b> creativity and <b>the</b> aspect <b>of</b> style in Sesotho creative writing essays). The researcher manually coded open-ended responses in the questionnaires. Interview responses were coded with Atlas. ti version 7. Frequencies were calculated for the close-ended questions in the questionnaire. Participating teachers perceived their assessment <b>of</b> essays with <b>the</b> use <b>of</b> <b>the</b> <b>rubric</b> and <b>the</b> correction to be standardised. This was evident in their awarding of marks. It was found in this study that teachers generally award marks around 60 %. However, their report that they use comments as per their responses in the questionnaire was disproven by <b>the</b> lack <b>of</b> comments in <b>the</b> scripts analysed in this study. There was also no relationship observed between the correction code frequencies observed in the marked essays that were analysed and the marks granted for specific sections <b>of</b> <b>the</b> <b>rubric.</b> This study recommends use <b>of</b> <b>the</b> <b>rubric</b> in earlier drafts <b>of</b> <b>the</b> writing process. In addition, it proposes an expansion <b>of</b> <b>the</b> marking grid used to provide clearer feedback via <b>the</b> revised <b>rubric</b> to <b>the</b> learners. Due to the participating teachers’ evident lack of clarity on what style in Sesotho home language essays entail, it was inferred that teachers are not clear on the distinctions between different essay assessment criteria in <b>the</b> <b>rubric.</b> A recommendation was <b>the</b> development <b>of</b> a <b>rubric</b> guide, which would clearly indicate to teachers what each criterion <b>of</b> <b>the</b> <b>rubric</b> assesses. Master...|$|R
50|$|On 10 April 1848, {{a voting}} process and {{discussion}} {{was made on}} <b>the</b> regularisation <b>of</b> <b>the</b> <b>rubrics</b> for <b>the</b> Feast <b>of</b> <b>the</b> Immaculate Conception to be celebrated in that country. By 1849, this decree was published at the 7th Provincial Council <b>of</b> Baltimore. <b>The</b> decrees were signed and witnessed by Cardinal Giacomo Filippo Fransoni.|$|R
25|$|The {{cardinal}} {{said that}} the Vatican was preparing to instruct seminaries to teach all students <b>the</b> Tridentine form <b>of</b> <b>the</b> Roman Rite. <b>The</b> complexity <b>of</b> <b>the</b> <b>rubrics</b> {{makes it difficult for}} priests accustomed to the simpler modern form to celebrate the Tridentine form properly, and it is unclear how many have the required knowledge.|$|R
5000|$|The term Soviet Nonconformist Art {{refers to}} art {{produced}} in the former Soviet Union from 1953 to 1986 (after <b>the</b> death <b>of</b> Joseph Stalin until <b>the</b> advent <b>of</b> Perestroika and Glasnost) outside <b>of</b> <b>the</b> <b>rubric</b> <b>of</b> Socialist Realism. Other terms {{used to refer to}} this phenomenon are [...] "unofficial art" [...] or [...] "underground art." ...|$|R
40|$|This article {{describes}} the procedures employed to develop a generic scoring rubric to evaluate students’ responses on functional behavior assessment cases. <b>The</b> generic <b>rubric</b> was then used to develop case specific scoring <b>rubrics.</b> <b>The</b> components <b>of</b> <b>the</b> generic scoring <b>rubric,</b> its effectiveness to reliably score students’ responses across cases, and its utility in evaluating students’ responses for errors are described. <b>The</b> components <b>of</b> <b>the</b> scoring <b>rubric</b> helped in reliably scoring students’ responses. <b>The</b> importance <b>of</b> scoring <b>rubrics,</b> in <b>the</b> context <b>of</b> <b>the</b> recent mandates <b>of</b> accreditation agencies like National Council for Accreditation <b>of</b> Teacher Education, <b>the</b> limitations <b>of</b> <b>the</b> scoring <b>rubric,</b> and suggestions for further research are discussed...|$|R
40|$|<b>The</b> aim <b>of</b> this {{research}} was to gain evidence based arguments for <b>the</b> use <b>of</b> <b>the</b> scoring <b>rubric</b> for performance assessment of information literacy [1] in Dutch Universities of Applied Sciences. Faculty members from four different departments <b>of</b> <b>The</b> Hague University were interviewed on {{the ways in which}} they use <b>the</b> scoring <b>rubric</b> and their arguments for it. A fifth lecturer answered the main question by email. The topic list, which has been used as a guide for the interviews, was based on subject analysis of scholar literature on <b>rubric</b> use. Four <b>of</b> <b>the</b> five respondents used (parts <b>of)</b> <b>the</b> <b>rubric</b> for <b>the</b> measurement <b>of</b> students’ performances in information use but none <b>of</b> them used <b>the</b> <b>rubric</b> as it is. What the faculty staff told the researcher is that <b>the</b> <b>rubric</b> helped them to improve the grading criteria for existing assignments. Only one respondent used <b>the</b> <b>rubric</b> itself, but this lecturer extended it with some new criteria on writing skills. It was also discovered that <b>the</b> <b>rubric</b> is not only used for grading but also for <b>the</b> development <b>of</b> new learning content on research skills. [De hier gepubliceerde versie is het 'accepted paper' van het origineel dat is gepubliceerd op www. springerlink. com. De officiële publicatie kan worden gedownload op [URL]...|$|R
2500|$|The code as it survives was {{not written}} in the king's name and <b>the</b> 12th-century author <b>of</b> <b>the</b> <b>rubric</b> may have been {{influenced}} by Bede in his attribution. [...] <b>The</b> lack <b>of</b> attribution in <b>the</b> original text may be a sign that law-making was not primarily a royal activity as it was to become in later centuries.|$|R
30|$|It was {{discovered}} that students consistently use rubrics after their performance assessments, not really {{before and during the}} performance. A majority <b>of</b> <b>the</b> students distinctly mentioned that they do not read <b>the</b> <b>rubrics</b> before <b>the</b> assessment because it makes them anxious and in a way limits them on what they should do (this will be further elaborated in the students’ perceived limitations <b>of</b> <b>the</b> <b>rubric).</b> Students, who would read it before <b>the</b> assessment, use <b>rubrics</b> to remind them <b>of</b> <b>the</b> expectations that were set during instruction.|$|R
