0|43|Public
5000|$|Improved Ability to Manage RSS Feeds by {{allowing}} users to arrange and <b>file</b> <b>feeds</b> {{just as they}} do with their favorites and bookmarks.|$|R
40|$|The aim of {{the study}} was to assess the {{dynamics}} of specific microelements in the blood of calves in particular farm depending on the influence of selected feed supplements. In the first and in the second experiment the calves were divided in to four groups - 3 experimental and one control. Experimental groups were <b>filed</b> <b>feed</b> supplements to support active immunity according to the methodology in the first attempt over about 2 weeks, in the second for 3 weeks. Observed data and analysis were statistically evaluated and compared with each other. The first experiment was performed from August to November 2013, the second from November 2014 to January 2015...|$|R
50|$|Orb 2.0 beta is Ajax-based {{and offers}} a home page similar to Google Personalized Homepage or netvibes. It is {{organized}} into tabs, with each tab containing user-defined modules, such as TV guide, personal media <b>files,</b> RSS/Atom <b>feeds</b> such as YouTube videos, the local weather forecast, etc...|$|R
50|$|Base Feeder is a Microsoft Windows based {{utility to}} create Atom and RSS XML <b>feed</b> <b>files</b> for {{submission}} to Google Base.|$|R
40|$|Part 1 : Decision Support Systems, Intelligent Systems and Artificial Intelligence ApplicationsInternational audienceIn view of existed not {{flexibility}} and low efficiency in establishing <b>feeding</b> <b>file</b> of large-scale farms or farmer farms of pigs, by adopting intelligent PDA or mobile phone as application platform, combining with. Net 2005 language and SQL Server 2005 CE database {{as well as}} TD-SCDMA wireless wide band communication linking Internet as data transmission method, this study suggested data criterions on feeding process information collection of pigs, and developed a mobile PDA or phone system to track swine feeding process data, such as operators and main inputs, and to trace pork quality safety. The running of the system shows that it realized all kinds of data collecting and wireless submission including ear tag wearing and movements, immunity events, feeds and veterinary drugs used as well as casual inspection data, and also achieved remote data maintaining for pig’s <b>feeding</b> <b>files</b> and deepness inquiry to pork quality. The system not only makes up a deficiency from table data recording system for <b>feeding</b> <b>file</b> setting of a large-scale swine farm, but also {{is a kind of}} effective solution for farmer farm to set up swine <b>feeding</b> <b>files.</b> Furthermore, the system is a kind of mobile and convenient supervising tool to service official veterinarian to carry out their work. Finally, with the TDSCDMA technology prevalence and communication fee decrease, the system will take part in a important role in constructing Chinese pork quality safety traceability system...|$|R
50|$|AJAX-based {{interface}} allows displaying abstract, BibTeX {{entry and}} links to DOI, full text, slides and poster <b>files.</b> RSS <b>feeds</b> and permanent links are available per year, research area, keyword and document type. Adding new publications can be done via an online form (either by BibTeX code or by filling in specific fields) or by directly editing the BibTeX file. Bibliographic entries can be exported as BibTeX through unAPI, making it compatible with Zotero.|$|R
40|$|This {{research}} {{introduces a}} method of steganalysis by means of neural networks and its structure optimization. The main aim is to explain the approach of revealing a hidden content in jpeg <b>files</b> by <b>feed</b> forward neural network with Levenberg-Marquardt training algorithm. This work is also concerned to description of data mining techniques for structure optimization of used neural network. The results showed almost 100 % success of detection. © Springer-Verlag Berlin Heidelberg 2013...|$|R
5000|$|Stream - record vary in length, {{and every}} record is {{separated}} from the next one by a termination character. A text file {{is an example of}} a stream-format <b>file</b> using line <b>feed</b> or carriage return to separate records.|$|R
50|$|Functionally, the {{operation}} of C News {{is very much like}} that of B News. One major difference was that C News was written with portability in mind. It ran on many variants of Unix and even MS-DOS. The relaynews program that handled article <b>filing</b> and <b>feeding</b> was carefully optimized and designed to process articles in batches, while B News processed one article per program invocation. The authors claimed that relaynews could process articles 19 times as quickly as B News.|$|R
50|$|WebFetch {{can collect}} {{various types of}} inputs via plugin modules and send or store them to various outputs also via plugin modules. Inputs include RSS, Atom, local news <b>feed</b> <b>files,</b> and perl data structures. Outputs include perl data structures, the Template Toolkit and pages in TWiki systems.|$|R
50|$|Also {{note that}} of the above competitors, {{including}} Essbase, all use heterogenous relational (Microsoft SQL Server, Oracle, IBM DB/2, TeraData, Access, etc.) or non-relational data sourcing (Excel, text Files, CSV <b>Files,</b> etc.) to <b>feed</b> the cubes (facts and dimensional data), except for Oracle OLAP which may only use Oracle relational sourcing.|$|R
30|$|Our {{experiences}} consisted in sequentially reading {{entries from}} {{a set of}} log <b>files</b> and <b>feeding</b> them directly to our cache server. As {{a result of this}} approach, issues related to concurrency are not considered, except later in Section 2. For each experiment we executed multiple simulations, logging the values obtained in each individual simulation and computing the corresponding averages. The values presented in this section correspond to the averages obtained. The experiments were executed on a single machine equipped with an Intel Core i 7 870 2.93 GHz CPU (4 cores) and 12 GB of RAM running Ubuntu Linux.|$|R
50|$|The Varonis Metadata Framework is {{implemented}} at two levels. Non-intrusive monitoring resides on <b>file</b> servers, <b>feeding</b> both real-time <b>file</b> event information and ACLs {{to a separate}} server. The collected data is stored in a database. The second part, the IDU analytics engine, performs statistical analysis to derive data owners, baseline user activity, and user groupings. The Metadata Framework is able to incrementally index file metadata, thereby allowing it to efficiently maintain {{the current state of}} file metadata in its database. DatAdvantage presents this information to IT administrators. As of April 30, 2016, Varonis had 28 issued patents in the United States and 43 pending U.S. patent applications.|$|R
2500|$|The term [...] "King Kong defense" [...] {{was quickly}} popularized by blogs, <b>file</b> sharing news <b>feeds,</b> and media {{reports on the}} Pirate Bay trial. It has been {{compared}} to the Chewbacca defense from the TV series South Park, citing a reference to the [...] "jungles of Cambodia" [...] as [...] "the kind of extraneous detail that makes the Chewbacca defense hilarious".|$|R
5000|$|Podcasting {{software}} {{is starting to}} integrate BitTorrent to help podcasters deal with the download demands of their MP3 [...] "radio" [...] programs. Specifically, Juice and Miro (formerly known as Democracy Player) support automatic processing of [...]torrent <b>files</b> from RSS <b>feeds.</b> Similarly, some BitTorrent clients, such as µTorrent, are able to process web feeds and automatically download content found within them.|$|R
5000|$|Usually {{these are}} {{expressed}} as file sources. The <b>file</b> sources would <b>feed</b> compression filters, {{the output of}} the compression filters would feed into a multiplexer that would combine the two inputs and produce a single output. (An example of a multiplexer would be an MPEG transport stream creator.) Finally the multiplexer output <b>feeds</b> into a <b>file</b> [...] sink, which would create a file from the output.|$|R
40|$|Abstract. Multiprocessor-System-on-a-Chip {{architectures}} offer multiple granularities of parallelism. While harnessing {{the lowest}} levels by means of vectorization and instruction scheduling only requires local information about the code {{and one of the}} cores, coarser levels raise interdependent trade-offs which necessitate a global approach. This paper introduces Gomet: an extension to GCC which combines a hierarchical intermediate representation of the input program and a highlevel machine description to achieve multi-grain parallelism adaptation. Gomet builds this representation from an inter-procedural dependence analysis, and transforms it to fit the target hardware. Then, it generates specialized C source <b>files</b> to <b>feed</b> the native compiler of each core of the target. Early evaluation of Gomet with simple programs show encouraging results and motivate further developments. ...|$|R
5000|$|Worldeka {{was founded}} by brother sister team Charlie and Sophie Cox. The company is based in London, England but has representatives in the US and Australia {{as well as in}} other main cities across the UK. The site has often been {{described}} as using social media for social change, much of its interface is akin to a social network with diggable media <b>files,</b> RSS <b>feeds</b> and social interaction, Worldeka themselves use the phrase [...] "a social network with a conscience" [...] The site also offers project management tools and collaborative features enabling charities to work together on specific projects, as well as combine intelligence and resources to aid humanitarian and disaster relief.|$|R
40|$|Data feed {{management}} {{is a critical}} component of many data intensive applications that depend on reliable data delivery to support real-time data collection, correlation and analysis. Data is typically collected {{from a wide variety of}} sources and organizations, using a range of mechanisms- some data are streamed in real time, while other data are obtained at regular intervals or collected in an ad hoc fashion. Individual applications are forced to make separate arrangements with feed providers, learn the structure of incoming files, monitor data quality, and trigger any processing necessary. The Bistro data feed manager, designed and implemented at AT&T Labs-Research, simplifies and automates this complex task of data feed management: efficiently handling incoming raw <b>files,</b> identifying data <b>feeds</b> and distributing them to remote subscribers. Bistro supports a flexible specification language to define logical data feeds using the naming structure of physical data files, and to identify feed subscribers. Based on the specification, Bistro matches data <b>files</b> to <b>feeds,</b> performs <b>file</b> normalization and compression, efficiently delivers files, and notifies subscribers using a trigger mechanism. We describe our feed analyzer that discovers the naming structure of incoming data files to detect new feeds, dropped feeds, feed changes, or lost data in an existing feed. Bistro is currently deployed within AT&T Labs and is responsible for the real-time delivery of over 100 different raw feeds, distributing data to several large-scale stream warehouses...|$|R
50|$|For example, audio CDs contain 16-bit, 44.1 kHz LPCM-encoded audio samples {{interleaved}} with secondary data {{streams and}} synchronization and error correction info. The ripping software tells the CD drive's firmware {{to read this}} data and parse out just the LPCM samples. The software then dumps them into a WAV or AIFF <b>file,</b> or <b>feeds</b> them to another codec to produce, for example, a FLAC or MP3 file. Depending on {{the capabilities of the}} ripping software, ripping may be done on a track-by-track basis, or all tracks at once, or over a custom range. The ripping software may also have facilities for detecting and correcting errors during or after the rip, as the process is not always reliable, especially when the CD is damaged or defective.|$|R
40|$|This Technical Paper {{describes}} the processes {{for creating a}} key input dataset for the Regional Dimensions project. This input dataset is the so-called HES Linkage <b>File,</b> which <b>feeds</b> into the 'reweighting' process for converting a set of national household weights into small-area (SLA) household weights. The processes described here prepare the selected variables from the ABS 1998 - 99 Household Expenditure Survey (HES) for matching to the Census data, in order for 'reweighting' to take place. The preparations included recoding, uprating, and record imputation of person-level, family-level, and household-level variables; {{as well as the}} creation of unit records for children and non-private dwelling residents. These preparations involved a suite of SAS programs, which, together with a data dictionary, are also documented in this paper. reweighting, small-area statistics, spatial microsimulation...|$|R
50|$|Conduit is an {{open-source}} synchronization {{program for}} GNOME. It allows {{the user to}} synchronize information to and from various destinations. For instance, {{it can be used}} to synchronise photos on the users computer with various websites (such as Flickr, Picasa and SmugMug). Other types of information may be synchronized, such as <b>files,</b> folders, RSS <b>feeds,</b> emails, notes, contacts, calendars, and tasks. The program uses a drag-and-drop interface to give a visual representation of what is going to be done.|$|R
50|$|Application Consistency, {{similar to}} Transaction consistency, is applied on a grander scale. Instead {{of having the}} scope of a single transaction, data must be {{consistent}} {{within the confines of}} many different transaction streams from one or more applications.An application may be made up of many different types of data, various types of <b>files</b> and data <b>feeds</b> from other applications. Application consistency is the state in which all related files and databases are synchronized representing the true status of the application.|$|R
50|$|RapidFeeds {{is one of}} the few {{providers}} offering services {{similar to}} the ones offered by FeedBurner (acquired by Google Inc.) apart from the notable Chinese alternative FeedSky. Although not as rich in features as FeedBurner, the possibility to create, edit, update and publish a feed exclusively on a web interface instead of uploading the <b>feed</b> <b>file</b> makes it a valuable alternative. It requires even less familiarity with XML technology and provides the possibility of publishing directly into feeds from any internet connected computer.|$|R
50|$|The software, {{originally}} {{developed for the}} intelligence community, allows users to load data from XML <b>files,</b> databases, RSS <b>feeds,</b> web services, HTML files, Microsoft Word, PowerPoint, Excel, CSV, Adobe PDF, TXT files, etc. and analyze it {{with a variety of}} visualizations and tools. The system integrates structured, unstructured, geospatial, and multimedia data, offering comparisons of information at multiple levels of abstraction, simultaneously and in near real-time. In addition Starlight allows users to build their own named entity-extractors using a combination of algorithms, targeted normalization lists and regular expressions in the Starlight Data Engineer (SDE).|$|R
40|$|We {{present a}} new {{efficient}} paradigm for signing digital streams. The problem of signing digital streams {{to prove their}} authenticity is substantially different from the problem of signing regular messages. Traditional signature schemes are message oriented and require the receiver to process the entire message before being able to authenticate its signature. However, a stream is a potentially very long (or infinite) sequence of bits that the sender sends to the receiver and the receiver is required to consumes the received bits at {{more or less the}} input rate and without excessive delay. Therefore it is infeasible for the receiver to obtain the entire stream before authenticating and consuming it. Examples of streams include digitized video and audio <b>files,</b> data <b>feeds</b> and applets. We present two solutions to the problem of authenticating digital streams. The first one is for the case of a finite stream which is entirely known to the sender (say a movie). We use this constraint to devise [...] ...|$|R
50|$|An RSS editor is a {{software}} application for writing and editing RSS feeds offline (i.e. {{on the local}} computer). These applications are also often called desktop RSS editors. Usually RSS feeds are automatically generated out of databases from Content Management Systems (CMS). Some other typical sources for RSS feeds are blogs and websites like Digg. However, there are also several, manually edited RSS feeds (mostly with editorial content), which are maintained offline. After the development and creation of such feeds in an RSS editor application, the <b>feed</b> <b>file</b> is usually transmitted via FTP to the web server. Most RSS editors offer a corresponding, integrated functionality for that.|$|R
40|$|ABSTRACT The data <b>file</b> for <b>feed</b> {{formulation}} {{of a major}} broiler integrator was used in studies of formulation techniques that affect proportionality among required nutrients. 1) The current practice, often called "optimum density " or "nutrient factoring, " was examined. In optimum density rations, the constraint on {{the weight of the}} diet is relaxed. It was demonstrated that this method merely results in proportionate changes in specified nutrient and ingredient minimums and maximums. Optimum density rations may reduce the cost of the diet in terms of cost per unit of energy only because of the altered constraints. 2) In a formulation related to optimum density, mixing and farm distribution cost was added to the feed mix problem on a per ton basis. This cost did not affect the constraint proportions of the ration (optimum density) until in excess of $ 100 / ton. 3) A method of creating proportionality coefficients that allows nutrient ratios to be specified instead of levels and ranges is presented. It is expected to be helpful to nutritionists in formulating rations in which nutrients are desired to be in precise ratios without altering the weight of the ration. (Key words: linear programming, optimum density, nutrient factoring, proportionality...|$|R
3000|$|The [...] "RSS" [...] {{is another}} version of the Web {{application}} and it uses the really simple syndication (RSS) Web service provided by the same newspaper. This Web application gets a RSS feed (asynchronously) containing XML elements that will be parsed with HTML elements in the main page. The buttons on the detailed information page {{are linked to the}} Desktop version allowing users to continue his reading. Authors modified the template in order to include JavaScript functions to retrieve the time required to conclude the page display and the number of items included on the <b>feed</b> <b>file.</b> The 'RSS' version does not requests any kind of multimedia files as opposed with other Web applications in order to compare how much JavaScript computation is needed to complete the information display.|$|R
40|$|AbstractWe {{present a}} new {{efficient}} paradigm for signing digital streams. The problem of signing digital streams {{to prove their}} authenticity is substantially different from the problem of signing regular messages. Traditional signature schemes are message oriented and require the receiver to process the entire message before being able to authenticate its signature. However, a stream is a potentially very long (or infinite) sequence of bits that the sender sends to the receiver and the receiver is required to consume the received bits at {{more or less the}} input rate and without excessive delay. Therefore it is infeasible for the receiver to obtain the entire stream before authenticating and consuming it. Examples of streams include digitized video and audio <b>files,</b> data <b>feeds,</b> and applets. We present two solutions to the problem of authenticating digital streams. The first one is for the case of a finite stream which is entirely known to the sender (say a movie). We use this constraint to devise an extremely efficient solution. The second case is for a (potentially infinite) stream which is not known in advance to the sender (for example a live broadcast). We present proofs of security of our constructions. Our techniques also have applications in other areas, for example, efficient authentication of long files when communication is at a cost and signature-based filtering at a proxy server...|$|R
40|$|RSS is {{a format}} - {{specified}} in XML - useful for distribution of Web content by syndication. In particular RSS delivers {{information on the}} Web inside an XML <b>file</b> called <b>feed,</b> webfeed, RSS stream, by RSS channels. RSS is used by many content providers as news websites, weblogs, e-journals, virutal bookstores and podcasting. Production, distribution and echange of information is the aim of RSS moving, because Web feeds provide web content or summaries of web content together with links to the full versions of the content, and other metadata. Particular attenzion must be done on metadata. Acronym RSS stands for three different names: Rich Site Summary (RSS 0. 91) RDF Site Summary (RSS 0. 9 and 1. 0) Really Simple Syndication (RSS 2. 0) It exists in sever different versions. RSS is a tool {{in order to obtain}} personalized service inside a framework looking as a standard service offert. In addition to facilitating syndication, web feeds allow a website's frequent readers to track updates on the site using an aggregator in a context of customized services for users. In such a dimension library services are so much involved in RSS feeds adoption. RSS can be a promise for new strategies inside innovative services taylored on users and also {{in order to have a}} tool looking towards Web semantic. The paper examines the international literature on RSS and LIS environment on the basis of some service categories detected...|$|R
50|$|Comparison sites {{can also}} collect data through a data <b>feed</b> <b>file.</b> Merchants provide {{information}} electronically {{in a set}} format. This data is then imported by the comparison website. Some third party businesses are providing consolidation of data feeds so that comparison sites {{do not have to}} import from many different merchants. Affiliate networks such as LinkShare, Commission Junction or TradeDoubler aggregate data feeds from many merchants and provide them to the price comparison sites. Many of the popular shopping websites like Amazon and Flipkart provides direct affiliation to the customer who wants to become affiliate partner. They provides their own API to the affiliate partner to show their products with specifications to the affiliate partner's website. For example, Shoesdekho is one of their affiliate partner where you can find shoes price comparison from Flipkart, Amazon, Jabong, Snapdeal etc. This enables price comparison sites to monetize the products contained in the feeds by earning commissions on click through traffic. Other price comparison sites like PriceGrabber have deals with merchants and aggregate feeds using their own technology.|$|R
40|$|RSS is a XML based format. The Current popular {{version of}} RSS is RSS version 2. 0. The purpose of adding an RSS feed to your site {{is to show}} if {{anything}} new {{is added to the}} site. For example, if a new article or blog or news item is added to your site that should automatically appear in the RSS feed so that the visitors/ RSS readers will automatically get updated about this new addition. The RSS feed is also called RSS channel. There are two main elements of the RSS XML file, one is the header or channel element that describes the details about the site/feeder and other is the body or item element that describes the consists of individual articles/entries updated in the site. As the format of the RSS <b>feed</b> <b>file</b> is pretty simple, it can be coded in any language, ASP, PHP or anything of that sort. We will build an RSS feeder using classical ASP (Active Server Pages) code in this article. Comment: 11 pages, 1 figur...|$|R
40|$|This report {{describes}} GDR (for Graph Drawing), a {{tool for}} editing graphs and animating graph algorithms. The motivation for animation tools comes primarily from the classroom; students often have difficulty mastering the formal concepts of graph theory, even though they usually have no trouble in following the associated visual intuitions. An appropriate animation tool can provide an invaluable link between formalism and graphical intuition, since students are {{given the opportunity to}} see how a formal presentation of a graph algorithm directly translates into visually intuitive operations on graphs. For this reason, animation systems can also play a very useful research role as testbeds for prototyping new algorithms and for testing conjectures about graphs. The design of GDR was strongly influenced by our desire for the tool to be easyto -use (even by relatively inexperienced programmers), portable, and flexible. Our approach was to develop GDR as a tool rather than a self-contained system, so that it is easily modified and simple to interface with other software. This has led to two approaches to using GDR in conjunction with other tools. In the first, GDR functions as a graph editor; users create graphs using the tool, save them in <b>files,</b> and <b>feed</b> the <b>files</b> as input into the other tools. This mode of interaction only requires that front-ends be written for the other tools that can parse the (very simple) output generated by GDR. The second mode of interaction is object-oriented; GDR provides a high-level interface to graph objects that programmers can write programs to manipulate (using calls to functions implemented in GDR). Users of GDR can then create graphs and apply the programmer-supplied routines to these graphs. Moreover, GDR is written in C and uses the libr [...] ...|$|R
40|$|Abstract. Spontaneous nasal {{tumors are}} rare in mice, {{and only one}} {{adenocarcinoma}} and two more primary neoplasms of the nose have been observed in our <b>files</b> of long-term <b>feeding</b> studies, which are composed of 3, 419 male and 3, 521 female CD- 1 (Crl:CD-I @ (1 CR) BR) mice. This adenocarcinoma was a 1 -cm-diameter mass observed grossly in the right nasal cavity of a 454 -day-old, male CD- 1 mouse from a treated group in a bioassay study conducted with 340 males and 340 females. The neoplastic epithelial cells affected the normal nasal architecture {{on the right side}} of the nose. Roughly, tumor neoplastic cells of the outer, lateral portion occurred as cuboidal to low columnar cells with basilarily located nuclei and eosinophilic cytoplasm. These cells were arranged in cylindrical profiles and frequently entrapped acini of the glands of the maxillary sinus. Neoplastic epithelial cells of the inner, medial portion appeared as serous acinar or ductular structures circum-scribed by multiple lagers of myoepithelial-like cells. Staining failed to demonstrate mucous secretion. The site of origin of this neoplasm appeared to be the serous glands of the maxillary sinus. The adenocarcinoma was believed to be spontaneous...|$|R
40|$|BACKGROUND: {{tracheoesophageal fistula}} (TEF) {{may result from}} cancer or {{mechanical}} ventilation. Endoscopic Gastrostomy or Gastrojejunostomy (PEG/PEG-J) is used for nutritional support. OBJECTIVE: in TEF-patients, evaluating nutritional status when PEG is performed, safety of PEG/PEG-J and clinical outcome. METHODS: from the <b>files</b> of PEG/PEG-J <b>feed</b> TEF-patients we collected: clinical data, Body Mass Index, albumin, transferrin and cholesterol when gastrostomy was performed, and clinical outcome globally and according with the TEF cause: Group 1 : complication of mechanical ventilation, Group 2 : cancer. RESULTS: twelve patients, 18 - 91 years (median: 53), 11 PEG, one PEG-J: six complications of ventilation (neurological diseases), 6 cancers. Mean period from TEF diagnosis until gastrostomy: 2 months in Group 1, 10 months in Group 2. In {{the day of the}} gastrostomy, patients presented with malnutrition parameters, most strikingly in the cancer group. Group 1 : died a single patient, 3 closed the TEF, resuming oral intake, 2 are still PEG-feed. All cancer patients died (7 months after gastrostomy). One needed a jejunal extension to create a PEG-J. No more complications. CONCLUSION: PEG/PEG-J was safe in TEF-patients, but cancer patients underwent gastrostomy too late. In TEF-patients, PEG/PEG-J should be considered in a regular basis, earlier in the disease evolution, before established malnutrition. "info:eu-repo/semantics/publishedVersio...|$|R
