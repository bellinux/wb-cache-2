12|26|Public
40|$|Issue: Contamination {{limits the}} {{ultimate}} marketability {{of the full}} range of PET plastic containers collected by local recycling programs. Contamination reduces the value of recyclable PET; it hinders processing and causes unproductive downtime and clean-up expenses for PET processors, reclaimers and end-users; and, it results in unnecessary manufacturing waste from the PET recycling process. Background: Intermediate plastic processors take in baled (and in some cases loose) plastic containers that have been separated from other recycable materials at MRFs, buyback or drop-off centers. <b>Intermediate</b> <b>processors</b> then granulate the containers for sale as “dirty regrind ” to reclaimers and end-users. In most cases, plastic <b>intermediate</b> <b>processors</b> take in loose plastic bottles and produce baled plastics for sale to plastic recycling facilities (PRFs), reclaimers or end-users. Most PRFs are designed to separate plastics into their individual resin categories (if they accept bales of mixed plastic bottles), and then further separate each plastic resin type by color or other market specification parameters. These color segregated plastic resins are then fed into granulators at PRFs or reclaimers to produce dirty regrind. Another major function of the plastics intermediat...|$|E
40|$|An {{end-to-end}} data {{delivery protocol}} for dynamic communication networks is presented. The protocol uses bounded sequence numbers and can tolerate both link failures and (intermediate) processor crashes. Previous bounded end-to-end protocols could not tolerate crashes. We present a self-stabilizing {{version of the}} algorithm that can recover from crashes of the sender and the receiver {{as well as of}} <b>intermediate</b> <b>processors.</b> Starting with the network in an arbitrary state, the self-stabilizing version guarantees proper transmission of messages following a finite convergence period...|$|E
40|$|AbstractWe show a {{lower bound}} of Ω(min{log m, √n}) on the {{slowdown}} of any deterministic emulation of a PRAM memory with m cells and n I/O ports on an n-processor bounded-degree network. The bound is weak; unlike all previous bounds, however, {{it does not}} depend on the unnatural assumption of point-to-point communication which says, roughly, that messages in transit cannot be duplicated by <b>intermediate</b> <b>processors.</b> For m sufficiently large relative to n, the new bound implies the optimality of a simple emulation on a mesh-of-trees network...|$|E
5000|$|IBM offered many Model Groups {{and models}} of the 4300 family, ranging from the entry level 4331 to the 4381, {{described}} as [...] "one {{of the most powerful}} and versatile <b>intermediate</b> system <b>processors</b> ever produced by IBM." ...|$|R
40|$|Abstract. Thermal energy {{generated}} by the friction between the disc and pad is transferred to both components and causes thermal expansion of material of each component, and futher affects the friction contact condition. This is the main factor of the thermoelastic instability (TEI) of a disc brake. In this study, TEI is analyzed using the finite element analysis technique. Three dimensional thermo-mechanical analysis model of the disc brake system is created. An <b>intermediate</b> <b>processor</b> based on the staggered approach is used to exchange analysis results: temperature, friction contact power, nodal displacement and deformation. Disc thickness variation (DTV) and temperature distribution of the disc are calculated, and the tendency and meaning of the results are discussed...|$|R
50|$|KS X 6001, {{standard}} for file specification of Korean word <b>processor</b> <b>intermediate</b> document, describes rules for line breaking {{at the end}} of page.|$|R
40|$|A {{fault-tolerant}} distributed k-mutual exclusion algorithm which adjusts to node mobility is presented, {{along with}} {{proof of correctness}} and simulation results. The algorithm requires nodes to communicate with only their current neighbors, making it well-suited to the ad hoc environment. Experimental results indicate that adaptation to mobility can improve performance over that of similar non-adaptive algorithms when nodes are mobile. 1 Introduction In an ad hoc mobile network, a pair of processors communicates by transmitting messages either over a direct wireless link, or over a sequence of wireless links including one or more <b>intermediate</b> <b>processors</b> to pass the message along. Direct communication is possible only between pairs of processors that lie within one another's transmission radius. Wireless link " occur when previously communicating nodes move such that {{they are no longer}} within transmission range of each other. Likewise, wireless link " occur when [...] ...|$|E
40|$|Distributed memory {{parallel}} {{systems are}} composed of processor/memory modules that communicate by the exchange of messages through an underlying communication network. If these message [...] passing systems {{are composed of}} point-to-point links, then processors can send messages directly to any processor {{to which they are}} connected by a direct link. If the message is destined to a non-neighboring processor, it has to hop through <b>intermediate</b> <b>processors.</b> In the case of multiprocessors (or parallel computers), the communication network is called interconnection network. Its topology is both especially designed for the multiprocessor and known by all the processors. On the other hand, when the message [...] passing system is composed of independent computers, it is better known as a computer network. In this case, the topology of the communication network is not necessarily known by every member of the network, implying the need for efficient protocols that guarantee asynchronous robust communicat [...] ...|$|E
40|$|Efficient {{communication}} in networks {{is a prerequisite}} to exploit the performance of large parallel systems. For this reason much effort {{has been done in}} recent years to develop efficient communication mechanisms. In this paper we survey the foundations and recent developments in designing and analyzing efficient packet routing algorithms. 1 Introduction Communication among the processors of a parallel computer usually requires {{a large portion of the}} runtime of a parallel algorithm. These computers are often realized as sparse networks of a large number of processors such that each processor can directly communicate with a few neighbours only. Thus, most of the communication must proceed through <b>intermediate</b> <b>processors.</b> One of the basic problems in this context is to route simultaneously many messages through the network. Telecommunicaton networks, computer networks in companies and universities, or the internet are examples for networks that have to process many communication reques [...] ...|$|E
40|$|When {{processing}} {{multimedia data}} streams in a distributed system {{it is often}} advantageous if data, in particular, continuous data such as video and audio data, can be sent directly from the source to the sink device. An example is a video conferencing application with source devices (cameras, microphones) transmitting directly to sink devices (frame buffers, speakers) without involving an <b>intermediate</b> <b>processor.</b> This approach saves communication bandwidth and processor cycles. Further, it reduces latency and also latency jitter. Unfortunately, systems in use today do not support this kind of direct communication. Even worse, current systems do not provide real-time or quality of service (QoS) guarantees {{making it impossible to}} process or transfer continuous data within specified latency times. This paper presents the design and implementation of a networked frame buffer with window management support. The frame buffer is implemented as an independent node of a proprietary desk area netw [...] ...|$|R
30|$|Privacy-aware systems {{based on}} HE require all parties {{involved}} in the distributed computation to agree upon a specific computational protocol. In many practical situations such as video surveillance and web content distribution, the types of possible computations on the data are practically unbounded and uncontrollable by the owner. Thus, sensitive information within the data must first be redacted before being released to others. An important consideration for many applications is that the redaction process needs to be reversible. For example, the redaction may be carried out at an <b>intermediate</b> <b>processor</b> which {{does not have the}} ownership of the data. The heterogeneity in access privileges among receivers may also require selective revealing of redacted objects. In addition, the reversibility of the redaction process can fulfill the liability of faithfully preserving contents such as surveillance videos which might be used in legal proceedings. Three papers in this special issue consider different aspects of this reversibility problem.|$|R
40|$|In this note, we answer two {{questions}} arising in broadcasting problems in networks. We first describe {{a new family}} of minimum broadcast graphs. Then we give a proof due to Alon of the NP-completeness of finding disjoint spanning trees of minimum depth, rooted at a given vertex. 1 Introduction In the design and use of parallel computers, different elements are important. Among them are the topology of the interconnection network and the communication scheme. In this paper, we focus on one important communication problem: Broadcasting = Sending a message from a given vertex to all other vertices. The initiator is also called the root, and the broadcasting problem is also called OTA (OneTo -All). We consider the usual store-and-forward model for routing, in which a message that passes through intermediate nodes is stored in each <b>intermediate</b> <b>processor</b> before reaching its final destination. Two kinds of communication schemes are usually considered: half duplex and full duplex. In t [...] ...|$|R
40|$|There is an {{increasing}} interest {{in the design of}} dense graphs as models of interconnection networks for massively parallel processing systems. In this paper an evolutionary algorithm for this problem ((ffi; D) graph problem) is presented by which a new maximal graph with degree 6, diameter 3 and order 108 was discovered. The considered graph is a Cayley graph of a permutation group represented by its generator set. Keywords: multiprocessor system, interconnection network, Cayley graph, evolutionary programming, parallel processing. 1 Introduction An important goal in the construction of parallel processing systems is a short transmission delay. As the number of links per processor is limited, it is impossible to connect every pair of processors directly via point-to-point connections. Therefore the messages have to pass through <b>intermediate</b> <b>processors</b> increasing the transmission delay dependent on the number of processors on the route. Defining the length of a route as the number o [...] ...|$|E
40|$|This paper {{describes}} several algorithms {{to perform}} all-to-all communication on a two-dimensional mesh connected computer with wormhole routing. We discuss both direct algorithms, in which data is sent directly from source to destination processor, and indirect algorithms in which data is sent through {{one or more}} <b>intermediate</b> <b>processors.</b> We propose algorithms for both power-of-two and non power-of-two meshes {{as well as an}} algorithm which works for any arbitrary mesh. We have developed analytical models to estimate the performance of the algorithms on the basis of system parameters. Performance results obtained on the Intel Touchstone Delta are compared with the estimated values. 1 Introduction The need for scalable parallel computers has resulted in the mesh emerging as a popular interconnection network for distributed memory multicomputers. The Intel Touchstone Delta, the Intel Paragon and the Symult 2010 use a two-dimensional mesh while the MIT J-machine and the Mosaic computer deve [...] ...|$|E
40|$|This paper {{examines}} how computation can be mapped across the nodes of a distributed search system to effectively utilize available resources. We specifically address computationally intensive search of complex data, such as contentbased retrieval of digital images or sounds, where sophisticated algorithms must be evaluated on {{the objects of}} interest. Since these problems require significant computation, we distribute the search over a collection of compute nodes, such as active storage devices, <b>intermediate</b> <b>processors</b> and host computers. A key challenge with mapping the desired computation to the available resources is that the most efficient distribution depends on several factors: relative power and number of compute nodes; network bandwidth between the compute nodes; the cost of evaluating query predicates; and the selectivity of the given query. This wide range of variables renders manual partitioning of the computation infeasible, particularly since some of the parameters (e. g., available network bandwidth) can change {{during the course of}} a search. This paper proposes several techniques for dynamic partitioning of computation, and demonstrates that they can significantly improve efficiency for distributed search applications...|$|E
40|$|We {{study the}} problem of packet routing in {{synchronous}} networks. We put forward a notion of greedy hot-potato routing algorithms and devise techniques for analyzing such algorithms. A greedy hot-potato routing algorithm is one where ffl The processors have no buffer space for storing delayed packets. Therefore, each packet must leave any <b>intermediate</b> <b>processor</b> at the step following its arrival. ffl Packets always advance towards their destination if they can. Namely, a packet must leave its current intermediate node via a link which takes it closer to its destination, unless all these links are taken by other packets. Moreover, in this case all these other packets must advance towards their destinations. We use potential function analysis to obtain an upper bound of O(n p k) on the running time of a wide class of algorithms in the 2 -dimensional n Θ n mesh, for routing problems with total of k packets. The same techniques can be generalized to obtain an upper bound of O(exp(d) [...] ...|$|R
40|$|An {{on-board}} autonomous exploration {{system that}} fuses data from multiple sensors, and makes {{decisions based on}} scientific goals is being developed using a series of artificial neural networks. Emphasis is placed on classifying minerals into broad geological categories by analyzing multispectral data from an imaging spectrometer. Artificial neural network architectures are being investigated for pattern matching and feature detection, information extraction, and decision making. As a first step, a stereogrammetry net extracts distance data from two gray scale stereo images. For each distance plane, the output is the probable mineral composition of the region, {{and a list of}} spectral features such as peaks, valleys, or plateaus, showing the characteristics of energy absorption and reflection. The classifier net is constructed using a grandmother cell architecture: an input layer of spectral data, an <b>intermediate</b> <b>processor,</b> and an output value. The feature detector is a three-layer feed-forward network that was developed to map input spectra to four geological classes, and will later be expanded to encompass more classes. Results from the classifier and feature detector nets will help to determine the relative importance of the region being examined with regard to current scientific goals of the system. This information is fed into a decision making neural net along with data from other sensors to decide on a plan of activity. A plan may be to examine the region at higher resolution, move closer, employ other sensors, or record an image and transmit it back to Earth...|$|R
40|$|We {{consider}} the power given to adversary scheduler of an asynchronous system and define the value-oblivious scheduler. At each step this scheduler determines the next processor to {{operate based on}} the full history {{of the dynamics of}} the execution; the scheduler is oblivious to the <b>intermediate</b> values the <b>processors</b> manipulate. We argue that the value-oblivious scheduler captures the possible sources of asynchrony in real systems...|$|R
40|$|This paper {{develops}} {{a theory of}} the precautionary demand for commodity stocks. It suggests that commodity stocks are held for precautionary purposes by producers, consumers, and <b>intermediate</b> <b>processors,</b> while speculators hold stocks on the expectation of capital gains from a subsequent price rise. Producer and consumer stocks usually account for the largest share of commercial stocks held {{at any point in}} time. For example, at the end of 1990, stocks held by producers and consumers of copper were 72 percent of all commercial stocks of the market economy countries. Yet, the theory explaining the behavior of this class of stocks has not progressed much beyond the concept of convenience yield, first introduced by Kaldor (1939). This paper proposes an alternative theory. Holding of stocks by producers and consumers is viewed as precautionary behavior towards output and price risks. As a theory of behavior towards risks, the precautionary stock demand model encompasses speculative demand by both producers and consumers. Furthermore, both stocks and futures are treated as precautionary instruments, in contrast to the dichotomy that only stocks provide convenience yield while futures are hedging instruments. Access to Markets,Markets and Market Access,Economic Theory&Research,Environmental Economics&Policies,Non Bank Financial Institutions...|$|E
40|$|Health care, {{in common}} with many other industries, is {{generating}} large amounts of routine data, data that are challenging to process, analyse or curate, so-called ‘big data’. A challenge for health informatics is {{to make sense of}} these data. Part of the answer will come from the development of ontologies that support the use of heterogeneous data sources and the development of <b>intermediate</b> <b>processors</b> of health information (IPHI). IPHI will sit between the generators of health data and information, often the providers of health care, and the managers, commissioners, policy makers, researchers, and the pharmaceutical and other healthcare industries. They will create a health ecosystem by processing data in a way that stimulates improved data quality and potentially health care delivery by providers of health care, and by providing greater insights to legitimate users of data. Exemplars are provided of how a health ecosystem might be encouraged and developed to promote patient safety and more efficient health care. These are in the areas of how to integrate data around the unsafe use of alcohol and to explore vaccine safety. A challenge for IPHI is how to ensure that their processing of data is valid, safe and maintains privacy. Development of the healthcare ecosystem and IPHI should be actively encouraged internationally. Governments, regulators and providers of health care should facilitate access to health data and the use of national and international comparisons to monitor standards. However, most importantly, they should pilot new methods of improving quality and safety through the intermediate processing of health data. </p...|$|E
40|$|The largest wool {{exporter}} in {{the world}} is Australia, where wool being a major export is worth over AUD $ 2 billion per year and constitutes about 17 per cent of all agricultural exports. Most Australian wool is sold by auctions in three regional centres. The prices paid in these auction markets are used by the Australian production and service sectors to identify the quality preferences of the international retail markets and the <b>intermediate</b> <b>processors.</b> One ongoing problem faced by wool growers has been the lack of clear market signals on the relative importance of wool attributes with respect to the price they receive at auction. The goal of our research is to model the structure of Australian wool auction prices. We aim to optimise the information that can be extracted and used by the production and service sectors in producing and distributing the raw wool clip. Most of the previous methods of modelling and predicting wool auction prices employed by the industry have involved multiple-linear regressions. These methods have proven to be inadequate because they have too many assumptions and deficiencies. This has prompted alternative approaches such as neural networks and tree-based regression methods. In this thesis we discuss these alternative approaches. We observe that neural network methods offer good prediction accuracy of price but give minimal understanding of the price driving variables. On the other hand, tree-based regression methods offer good interpretability of the price driving characteristics but do not give good prediction accuracy of price. This motivates a hybrid approach that combines the best of the tree-based methods and neural networks, offering both prediction accuracy and interpretability. Additionally, there also exists a wool specifications problem. Industrial sorting of wool during harvest, and at the start of processing, assembles wool in bins according to the required wool specifications. At present this assembly is done by constraining the range of all specifications in each bin, and having either {{a very large number of}} bins, or a large variance of characteristics within each bin. Multiple-linear regression on price does not provide additional useful information that would streamline this process, nor does it assist in delineating the specifications of individual bins. In this thesis we will present a hybrid modular approach combining the interpretability of a regression tree with the prediction accuracy of neural networks. Our procedure was inspired by Breiman and Shang’s idea of a “representer tree” (also known as a “born again tree”) but with two main modifications: 1) we use a much more accurate Neural Network in place of a multiple tree method, and 2) we use our own modified smearing method which involves adding Gaussian noise. Our methodology has not previously been used for wool auction data and the accompanying price prediction problem. The numeric predictions from our method are highly competitive with other methods. Our method also provides an unprecedented level of clarity and interpretability of the price driving variables in the form of tree diagrams, and the tabular form of these trees developed in our research. These are extremely useful for wool growers and other casual observers who may not have a higher level understanding of modelling and mathematics. This method is also highly modular and can be continually extended and improved. We will detail this approach and illustrate it with real data. The more accurate modelling and analysis helps wool growers to better understand the market behaviour. If the important factors are identified, then effective strategies can be developed to maximise return to the growers. In Chapter 1 of this thesis, we present a brief overview of the Australian wool auction market. We then discuss the problems faced by the wool growers and their significance, which motivate our research. In Chapter 2, we define the predictive aspect of the modelling problem and present the data that is available to us for our research. We introduce the assumptions that must be made in order to model the auction data and predict the wool prices. Chapter 3 discusses neural networks and their potential in our wool auction problem. Neural networks are known to give good results in many modern applications resolving industrial problems. As a result of the popularity of such methods and the ongoing development of them, our research partner, the Department of Agriculture and Food, Government of Western Australia, performed a preliminary investigation into neural networks and found them to give satisfactory predictions of wool auction prices. In our Chapter 3, we perform an analysis and assessment of neural networks, specifically, the generalised regression neural networks (GRNN). We look at the strengths and weaknesses of GRNN, and apply them to the wool auction problem and comment on their relevance and usability in our wool problem. We detail the problems we face, and why neural networks alone may not be the best approach for the wool auction problem, thus laying the foundation for the development of our hybrid modular approach in Chapter 5. We also use the numerical prediction results from GRNN as the benchmark in our comparisons of different modelling methods in the rest of this thesis. Chapter 4 details the tree-based regression methods, as an alternate approach to neural networks. In analysing the tree-based methods with our wool auction data, we illustrate the tree methods’ advantages over neural networks, as well as the trade-offs, with our auction data. We also demonstrate how powerful and useful a tree diagram can be to the wool auction problem. And in this Chapter, we improve a typical tree diagram further by introducing our own tabular form of the tree, which can be of immerse use to wool growers. In particular, we can use our tabular form to solve the wool specification problem mentioned earlier, and we incorporate this tabular form as part of a new hybrid methodology in Chapter 5. In Chapter 4 we also consider the ensemble methods such as bootstrap aggregating (bagging) and random forests, and discuss their results. We demonstrate that, the ensemble methods provide higher prediction accuracies than ordinary regression trees by introducing many trees into the model. But this is at the expense of losing the simplicity and clarity of having only a single tree. However, the study of assemble methods do end up providing an excellent idea for our hybrid approach in Chapter 5. Chapter 5 details the new hybrid approach we developed as a result of our work in Chapters 3 and 4 using neural networks and tree-based regression methods. Our hybrid approach combines the two methods with their respective strengths. We apply our new approach to the data, compare the results with our earlier work in neural networks and tree-based regression methods, then discuss the results. Finally, we conclude our thesis with Chapter 6, discussing the potential of our new hybrid approach and the directions of possible future works...|$|E
40|$|A System-On-Chip (SoC) is {{a complex}} {{integrated}} circuit that combines blocks of processor, memory and peripheral devices in one chip. SoCs often form the main or the only component of embedded systems. The advantages of the SoC include improvements in performance, size, reliability, power dissipation, cost, and design turn-around time. The hardware blocks – {{sometimes referred to as}} intellectual property cores or just IPs – are connected using a proprietary or open on-chip bus (OCB). The SoCs may be fabricated as application-specific integrated circuits (ASICs) or field-programmable gate arrays (FPGAs). The non-recurring engineering (NRE) costs for ASICs are much higher although the unit cost for the finished product is lower. For simpler designs and/or lower production runs, FPGAs are usually more cost-effective. One of the costs in implementing an SoC is acquiring the source code or designing the required cores. An approach for reducing costs is to use open source hardware. Open source cores have the advantages of zero license and royalty cost, ability to modify the cores at will, no limitation on supply and maintenance, portability and simplified prototyping. We discuss our implementation of a skeleton SoC incorporating a DLX processor, the Wishbone on-chip bus, and a memory system. The processor bus- memory combination forms a foundation to which a designer can add more cores such as memory and peripherals as long as they comply with the Wishbone protocol. The DLX processor and memory are described in VHDL, while the Wishbone module is in Verilog HDL. Quartus II software is used to synthesize, compile and verify the functionality of CPU and Wishbone by simulation and timing analysis. The partial SoC system is implemented in Altera APEX 20 KE 200 FPGA board. Nios, which is the core processor in the FPGA board, is used as an <b>intermediate</b> <b>processor</b> which communicates with DLX {{and the rest of the}} system via Avalon Bus Protocol to verify system operation and functionality in real hardware environment...|$|R
40|$|We price {{discretely}} monitored options {{when the}} underlying evolves according to different exponential Lévy processes. By geometric randomization of the option maturity, we transform the $n$-steps backward recursion that arises in option pricing into an integral equation. The option price is then obtained solving n independent integral equations by a suitable quadrature method. Since the integral equations are mutually independent, we can exploit the potentiality of a grid computing architecture. The primary performance disadvantage of grids is the slow communication speeds between nodes. However, our algorithm is well-suited for grid computing since the integral equations {{can be solved}} in parallel, without the need to communicate <b>intermediate</b> results between <b>processors.</b> Moreover, numerical experiments on a cluster architecture show the good scalability properties of our algorithm...|$|R
2500|$|... “Distributed” or “grid” {{computing}} {{in general}} is a special type of parallel computing that relies on complete computers (with onboard CPUs, storage, power supplies, network interfaces, etc.) connected to a network (private, public or the Internet) by a conventional network interface producing commodity hardware, compared to the lower efficiency of designing and constructing {{a small number of}} custom supercomputers. The primary performance disadvantage is that the various processors and local storage areas do not have high-speed connections. This arrangement is thus well-suited to applications in which multiple parallel computations can take place independently, without the need to communicate <b>intermediate</b> results between <b>processors.</b> The high-end scalability of geographically dispersed grids is generally favorable, due to the low need for connectivity between nodes relative to the capacity of the public Internet.|$|R
40|$|Abstract. Regular {{arrays of}} {{processing}} elements in VLSI {{have proved to}} be suitable for high-speed execution of many matrix operations. To execute an arbitrary computational algorithm on such processing arrays, it has been suggested mapping the given algorithm directly onto a regular array. The computational algorithm is represented by a data-flow graph whose nodes are to be mapped onto processors in the VLSI array. This study examines the complexity of mapping data-flow graphs onto square and hexagonal arrays of processors. We specifically consider the problem of routing data from processors in a given (source) sequence to another (target) sequence. We show that under certain conditions, the above problem is equivalent to the one of finding a minimum-diameter cyclic arrangement. The complexity of the latter problem is analyzed and upper and lower bounds on the number of <b>intermediate</b> rows of <b>processors</b> (between the source and target rows) are derived...|$|R
40|$|Users of PC-NOW {{platforms}} often {{expect the}} availability of message-passing libraries such as PVM or the emerging MPI standard. Such libraries provide a set of facilities to build parallel applications. In this paper we identify problems and propose solutions related to message-passing library implementation {{on top of a}} remote-write facility. Remote-write primitives allow the direct mapping of data to the memory of a remote node. The goal is to reduce communication overhead by avoiding costly <b>intermediate</b> copies and <b>processor</b> context switches. The reduced overhead allows a cluster of workstations to achieve better performance since the latency is reduced and the large interconnection network bandwidth is better exploited. Performance measurements conducted on the MPC platform developed at Pierre and Marie Curie University in Paris show that remote-write implementations of message-passing libraries are a promising way for increasing scalability and performance of parallel applications bu [...] ...|$|R
40|$|Regular {{arrays of}} {{processing}} elements in VLSI {{have proved to}} be suitable for high-speed execution of many matrix operations. To execute an arbitrary computational algorithm on such processing arrays, it has been suggested mapping the given algorithm directly onto a regular array. The computational algorithm is represented by a data-flow graph whose nodes are to be mapped onto processors in the VLSI array. This study examines the complexity of mapping data-flow graphs onto square and hexagonal arrays of processors. We specifically consider the problem of routing data from processors in a given (source) sequence to another (target) sequence. We show that under certain conditions, the above problem is equivalent to the one of finding a minimum-diameter cyclic arrangement. The complexity of the latter problem is analyzed and upper and lower bounds on the number of <b>intermediate</b> rows of <b>processors</b> (between the source and target rows) are derived...|$|R
40|$|MicroMAS) is a 3 U cubesat (30 xlOxlO cm, 4 kg) {{hosting a}} passive {{microwave}} spectrometer operating near the 118. 75 -GHz oxygen absorption line. The {{focus of the}} first MicroMAS mission (hereafter, MicroMAS- 1) is to observe convective thunderstorms, tropical cyclones, and hurricanes from a near-equatorial orbit at approximately 500 -km altitude. A MicroMAS flight unit is currently being developed {{in anticipation of a}} 2014 launch. A parabolic reflector is mechanically rotated as the spacecraft orbits the earth, thus directing a cross-track scanned beam with FWHM beam width of 2. 2 -degrees, yielding an approximately 25 -km diameter footprint from a nominal altitude of 500 km. Radiometric calibration is carried out using observations of cold space, the earth's limb, and an internal noise diode that is weakly coupled through the RF front-end electronics. A key technology feature is the development of an ultra-compact <b>intermediate</b> frequency <b>processor</b> module for channelization, detection, and A-to-D conversion. The antenna system and RF front-end electronics are highly integrated and miniaturized. A MicroMAS- 2 mission is currently being planned using a multi band spectrometer operating near 118 and 183 GHz in a sun synchronous orbit of approximately 800 -km altitude. A HyMAS- 1 (Hyperspectral Microwave Atmospheric Satellite) mission with approximately 50 channels near 118 and 183 GHz is also being planned. In this paper, the mission concept of operations will be discussed, the radiometer payload will be described, and the spacecraft subsystems (avionics, power, communications, attitude determination and control, and mechanical structures) will be summarized. I...|$|R
40|$|Farmers’ Willingness to Grow Switchgrass as a Cellulosic Bioenergy Crop: A Stated Choice Approach Agriculture’s role as {{a source}} of {{feedstocks}} in a potential lignocellulosic-based biofuel industry is a critical economic issue. Several studies have assessed the technical feasibility of producing bioenergy crops on agricultural lands. However, few of these studies have assessed farmers’ willingness to produce or supply bioenergy crops or crop residues. Biomass markets for bioenergy crops do not exist, and developing these markets may take several years. Therefore, an important, yet unaddressed question is under what contractual or pricing arrangements farmers will grow biomass for bioenergy in these nascent markets. The {{purpose of this paper is}} to examine farmers’ willingness to produce switchgrass under alternative contractual, pricing, and harvesting arrangements. Contracts are likely to be the preferred method to bring together producers and processors of biomass for bioenergy. Contract design may vary across farmers and crop type, and may include attributes specific to annual crops, contract length, quantity or acreage requirements, quality specifications, payment dates, and other important features. A stated choice survey was administered in three, six-county areas of Kansas by Kansas State University and the USDA, National Agricultural Statistics Service from November 2010 to January 2011 to assess farmers’ willingness to produce cellulosic biomass under different contractual arrangements. This paper focuses on the switchgrass stated choice experiment from the survey. The stated choice experiment asked farmers to rank their preferred contractual arrangement from two contract options and one “do not adopt” option. Contractual attributes included percentage net returns above the next best alternative (e. g. CRP or hay production), contract length, a custom harvest option, insurance availability, and a seed-cost share option. Respondents then ranked their preferred contract option. The survey also collected data on farm characteristics, bioenergy crop preferences, socio-economic demographics, risk preferences, and marketing behavior. The survey used a stratified sample of farmers who farm more than 260 acres and grow corn. A total of 460 surveys were administered with a 65 percent completion rate. The underlying theoretical model uses the random utility model (RUM) approach to assess farmers’ willingness to grow switchgrass for bioenergy and determine the contractual attributes most likely to increase the likelihood of adoption. This framework allows us to define the “price,” or farmers’ mean willingness to accept, for harvested biomass sold to an <b>intermediate</b> <b>processor.</b> The estimated choice models follow the approach of Boxall and Adamowicz (2002) to capture heterogeneity across farmers and geographic regions due to management differences, conservation practices, and risk preferences. Using the percentage net return above CRP or hay production allows prices to float to levels that will entice farmers to adopt switchgrass. This will help determine a market price for bioenergy crops based on current market and production conditions without specifying an exact monetary value for the biomass. In addition, the survey results will facilitate contract designs between biorefineries and farmers while informing policymakers and the biofuel industry about farmers’ willingness to supply biomass for bioenergy production. Reference: Boxall, P. C. and W. L. Adamowicz, “Understanding Heterogeneous Preferences in Random Utility Models: A Latent Class Approach,” Environmental and Resource Economics 23 (2002) : 421 – 446. Biofuels, Cellulosic, Biomass, Switchgrass, Farmers, Willingness to Pay, Crop Production/Industries, Production Economics, Resource /Energy Economics and Policy,...|$|R
40|$|International Telemetering Conference Proceedings / October 17 - 20, 1994 / Town & Country Hotel and Conference Center, San Diego, CaliforniaA {{system to}} {{generate}} a contiguous high speed time division multiplexed (TDM) spacecraft downlink data stream has been developed. The 25 MBPS downlink data stream contains high rate real time imager data, <b>intermediate</b> rate subsystem <b>processor</b> data, and low rate spacecraft housekeeping data. Imager data is transferred directly into the appropriate TDM downlink data window using control signals and clocks generated in the central data formatter and distributed to the data sources. Cable and electronics delays inherent in this process can amount to several clock periods, while the uncertainty and variations in those delays (e. g. temperature effects) can exceed the clock period. Unique (patent pending) electronic circuitry has {{been included in the}} data formatter to sense the total data gathering delay for each high speed data source and use the results to control series programmable delay elements to equalize the delays from all sources and permit the formation of a contiguous output data stream...|$|R
40|$|Lincoln Laboratory and NASA's Goddard Space Flight Center have teamed {{to re-use}} an {{existing}} instrument platform, the CoSMIR/CoSSIR system for atmospheric sounding, {{to develop a}} new capability in hyperspectral filtering, data collection, and display. The volume of the scanhead accomodated an intermediate frequency processor(IFP), that provides the filtering and digitization of the raw data and the interoperable remote component (IRC) adapted to CoSMIR, CoSSIR, and HyMAS that stores and archives the data with time tagged calibration and navigation data. The first element of the work is the demonstration of a hyperspectral microwave receiver subsystem that was recently shown using a comprehensive simulation study to yield performance that substantially exceeds current state-of-the-art. Hyperspectral microwave sounders with approximately 100 channels offer temperature and humidity sounding improvements similar to those obtained when infrared sensors became hyperspectral, but with the relative insensitivity to clouds that characterizes microwave sensors. Hyperspectral microwave operation is achieved using independent RF antenna/receiver arrays that sample the same area/volume of the Earth's surface/atmosphere at slightly different frequencies and therefore synthesize a set of dense, finely spaced vertical weighting functions. The second, enabling element of the proposal is the development of a compact 52 -channel <b>Intermediate</b> Frequency <b>processor</b> module. A principal challenge {{in the development of a}} hyperspectral microwave system is the size of the IF filter bank required for channelization. Large bandwidths are simultaneously processed, thus complicating the use of digital back-ends with associated high complexities, costs, and power requirements. Our approach involves passive filters implemented using low-temperature co-fired ceramic (LTCC) technology to achieve an ultra-compact module that can be easily integrated with existing radio frequency front-end technology. This IF processor is universally applicable to other microwave sensing missions requiring compact IF spectrometry. The data include 52 operational channels with low IF module volume (less than 100 cubic centimeters) and mass (less than 300 grams) and linearity better than 0. 3 percent over a 330, 000 dynamic range...|$|R
30|$|In this paper, we {{consider}} a three-stage AFSP with blocking times and sequence dependent setup times. To make {{this type of}} assembly flowshop more realistic our research added the blocking times limitation (buffer[*]=[*] 0) to the model presented in [5]. Sequence-dependent setup time says that setup time of a job in position i on machine j depends on the current job and the previous job on this machine. Once its processing is completed on a processor in {{the first or second}} stage, a product is transferred directly to either an available processor in the next stage (or another downstream stage depending on the product processing route), or a buffer ahead of that stage when such an intermediate buffer is available. However, when an intermediate buffer is unavailable, the product remains blocking the processor until a downstream processor becomes available [6]. In general, blocking scheduling problems arise in modern manufacturing environments with limited <b>intermediate</b> buffers between <b>processors,</b> such as just-in-time production systems or flexible assembly lines, and those without intermediate buffers, such as surface mount technology (SMT) lines in the electronics industry for assembling printed circuit boards, which includes three different stages in the following sequence: solder printing, component placement and solder reflow [7].|$|R
40|$|The {{current trends}} in high {{performance}} computing show that large machines {{with tens of}} thousands of processors will soon be readily available. The IBM Bluegene-L machine with 128 k processors (which is currently being deployed) is an important step in this direction. In this scenario, it is going to be a significant burden for the programmer to manually scale his applications. This task of scaling involves addressing issues like load-imbalance and communication overhead. In this thesis, we explore several communication optimizations to help parallel applications to easily scale on a large number of processors. We also present automatic runtime techniques to relieve the programmer from the burden of optimizing communication in his applications. This thesis explores processor virtualization to improve communication performance in applications. With processor virtualization, the computation is mapped to virtual processors (VPs). After one VP has finished computation and is waiting for responses to its messages, another VP can compute, thus overlapping communication with computation. This overlap is only effective if the processor overhead of the communication operation is a small fraction of the total communication time. Fortunately, with network interfaces having co-processors, this happens to be true and processor virtualization has a natural advantage on such interconnects. The communication optimizations we present in this thesis, are motivated by applications such as NAMD (a classical molecular dynamics application) and CPAIMD (a quantum chemistry application). Applications like NAMD and CPAIMD consume a fair share of the time available on supercomputers. So, improving their performance would be of great value. We have successfully scaled NAMD to 1 TF of peak performance on 3000 processors of PSC Lemieux, using the techniques presented in this thesis. We study both point-to-point communication and collective communication (specifically all-to-all communication). On a large number of processors all-to-all communication can take several milli-seconds to finish. With synchronous collectives defined in MPI, the processor idles while the collective messages are in flight. Therefore, we demonstrate an asynchronous collective communication framework, to let the CPU compute while the all-to-all messages are in flight. We also show that the best strategy for all-to-all communication depends on the message size, number of processors and other dynamic parameters. This suggests that these parameters can be observed at runtime and used to choose the optimal strategy for all-to-all communication. In this thesis, we demonstrate adaptive strategy switching for all-to-all communication. The communication optimization framework presented in this thesis, has been designed to optimize communication in the context of processor virtualization and dynamic migrating objects. We present the streaming strategy to optimize fine grained object-to-object communication. In this thesis, we motivate the need for hardware collectives, as processor based collectives can be delayed by <b>intermediate</b> that <b>processors</b> busy with computation. We explore a next generation interconnect that supports collectives in the switching hardware. We show the performance gains of hardware collectives through synthetic benchmarks...|$|R
40|$|The Micro-sized Microwave Atmospheric Satellite (MicroMAS) is a dual-spinning 3 U CubeSat {{equipped}} with a passive microwave spectrometer that observes nine channels near the 118. 75 -GHz oxygen absorption line. The focus of this MicroMAS mission (hereafter, MicroMAS- 1) is to observe convective thunderstorms, tropical cyclones, and hurricanes from a near-equatorial orbit. The MicroMAS- 1 flight unit is currently being developed by MIT Lincoln Laboratory, the MIT Space Systems Laboratory, and the MIT Department of Earth and Planetary Sciences for a 2014 launch to be provided by the NASA CubeSat Launch Initiative program. As a low cost platform, MicroMAS offers the potential to deploy multiple satellites than can provide near-continuous views of severe weather. The existing architecture of few, high-cost platforms, infrequently view the same earth area which can miss rapid changes in the strength and direction of evolving storms thus degrading forecast accuracy. The 3 U CubeSat has dimensions of 10 x 10 x 34. 05 cm 3 and a mass of approximately 4 kg. The payload is housed in the “lower” 1 U of the dualspinning 3 U CubeSat, and is mechanically rotated approximately once per second as the spacecraft orbits the Earth. The resulting cross-track scanned beam has a FWHM beam width of 2. 4 º, and has an approximately 20 -km diameter footprint at nadir incidence from a nominal altitude of 500 km. Radiometric calibration is carried out using observations of cold space, the Earth 2 ̆ 7 s limb, and an internal noise diode that is weakly coupled through the RF front-end electronics. In addition to the dual-spinning CubeSat, a key technology development is the ultra-compact <b>intermediate</b> frequency <b>processor</b> (IFP) module for channelization, detection, and analog-to-digital conversion. The payload antenna system and RF front-end electronics are highly integrated, miniaturized, and optimized for low-power operation. To support the spinning radiometer payload, the structures subsystem incorporates a brushless DC zerocogging motor, an optical encoder and disk, a slip ring, and a motor controller. The attitude determination and control system (ADCS) utilizes reaction wheels, magnetorquers, Earth horizon sensors, peak power tracking, a magnetometer, and a gyroscope. The communications system operates at S-band using the Open System of Agile Ground Stations (OSAGS) with a 2. 025 — 2. 120 GHz uplink and 2. 200 — 2. 300 GHz downlink at 230 kbps. MicroMAS- 1 uses a Pumpkin CubeSat Motherboard with a Microchip PIC 24 microcontroller as the flight computer running Pumpkin’s Salvo Real Time Operating System. Thermal management includes monitoring with thermistors, heating, and passive cooling. Power is generated using four double-sided deployable 3 U solar panels and one 2 U bodymounted panel with UTJ cells and an electrical power system (EPS) with 30 W-hr lithium polymer batteries from Clyde Space. Tests with the MicroMAS- 1 Engineering Design Model (EDM) have resulted in modifications to the spinning assembly, stack and ADCS system and have informed {{the development of the}} flight model subsystems...|$|R
40|$|The {{progress}} in CMOS technology {{has entered the}} sub-micron realm, and the technology will approach its limits within about 15 years. Already various novel information processing devices, based on quantum mechanical effects at the nanometer scale, have been widely investigated and some have been successfully demonstrated at the circuit level. This advance in nanoelectronic devices has also motivated efforts in the research of nanoelectronic and quantum computer architectures. Due to the components' poor reliabilities, these architectures {{will have to be}} robust against device and interconnect failures. In order to avoid power dissipation problems, the components will have to be applied in the quantum mechanical domain, while due to potential problems in interconnects, the components should be locally interconnected only. This dissertation is devoted to pursuing solutions to architectural issues that come up when designing a nanoelectronic computer. It explores the possibility of building viable and reliable computer systems from novel nanoelectronic and quantum devices. In particular, parallel processor architectures that are fault-tolerant and locally-coupled have been researched. Chapter 1 presents an introduction to the issues that play a role in nanoelectronics, in contrast with microelectronics, and discusses implications for nanocomputer architectures. A brief review of the current status in nanoelectronics and recent {{progress in}} nanoarchitecture research is presented in Chapter 2. Chapter 3 describes research on fault-tolerant architectures. We review von Neumann's NAND multiplexing technique and extended his study from a high degree of redundancy to a fairly low degree of redundancy. We show the stochastic Markovian nature of a multi-stage multiplexing system and work out its characteristics. We develop a system architecture based on the NAND multiplexing structure that copes with the problem of random background charges in single electron tunneling (SET) circuits. Our study shows that, although a rather large amount of redundant components is required, an architecture based on the multiplexing technique could be a fault-tolerant system solution for the integration of unreliable nanoelectronic devices affected by dominant transient errors. In addition, in Chapter 4, a defect- and fault-tolerant architecture is proposed, that uses the multiplexing technique for its fundamental circuits and a hierarchical reconfigurability in the overall system. It is shown that the required redundancy could be brought back to a moderate level by adding reconfigurability to the system concept. This architecture is robust in an efficient way against both manufacturing defects and transient faults, and tolerates a gate error rate of up to 10 which, for any current microelectronic system, would be unacceptable. Derived from von Neumann's multiplexing technique, we propose triplicated interwoven redundancy (TIR), as a generalization of triple modular redundancy (TMR), but then with random interconnections. A prototype processor architecture and its simulation-based reliability model have been set-up and are used to evaluate the fault-tolerance. The processor is, by way of comparison, implemented using both TIR as well as so-called quadded logic. In general, the reliability of a TIR circuit is comparable with that of an equivalent TMR circuit while, for certain interconnect patterns, the TIR structure may present an inferior performance to TMR, due to its interwoven nature in gate interconnections. TIR can be extended to higher orders, which we label N-tuple interwoven redundancy (NIR). The use of 5 -tuple interwoven redundancy leads to an economical redundancy factor of less than 10 for the reconfigurable system architecture. It has been shown that the design and implementation of restorative devices (voters) are important for TIR/NIR and quadded structures. Only with a simple voter design is it possible to obtain -with a higher order of NIR- a better system reliability than with TIR. TIR or NIR is in particular suitable for implementation in molecular nanocomputers, which are likely to be fabricated by a manufacturing process of stochastic chemical assembly. In Chapter 5, superconducting circuits of Josephson junctions have been investigated with as aim to possibly use them in locally-connected processor structures. Both a classical SIMD computer architecture and an array-based quantum computer structure are presented that use the same basic circuit, the Josephson junctions. Our ideal is that the classical computer can serve as a pre-, post- and <b>intermediate</b> <b>processor</b> for the quantum computation that is performed {{in the heart of the}} Josephson circuit array. As such, it then establishes a heterogeneous quantum/classical computer for implementations of algorithms such as Shor's factoring algorithm which mixes classical computation steps with quantum computation steps in a single algorithm. Although not specifically worked out and discussed in this study in detail, an architecture in the form of an all-reversible computing network based on superconducting circuits of Josephson junctions, could in principle be used for this. A quantum CNN (cellular nonlinear networks) architecture using the Josephson circuits has also been proposed, presenting a novel computing paradigm for Josephson circuits. Since classical computing architectures (SIMD arrays), quantum computing architectures and semi-quantum computing architectures (quantum CNNs) can be simultaneously studied on the same device, the Josephson circuit is a good vehicle for investigating the architectural issues of quantum and nanoelectronic computer systems, independently from the question of which device will be the ultimate implementation vehicle. This last chapter concludes this dissertation, which can be placed in the "early days" of research on architectures of nanoelectronic and quantum computers. And beyond this thesis: The scientific papers that form the foundation of the chapters in this thesis have meanwhile been followed up by many new studies in fault-tolerant techniques such as using Monte Carlo simulations, bifurcation theory and an exact analysis using combinatorial arguments to investigate the error behavior in a multiplexed nanosystem of Markov chains. Moreover, a probabilistic-based methodology has been proposed for designing nanocomputer architectures based on Markov Random Fields (MRF), and CAD tools are being developed to automate the evaluation of various fault-tolerant schemes and their reliability/redundancy trade-offs. The redundancy techniques, originating from von Neumann, are basically error-correcting codes (ECC). The multiplexing construction boils down to the use of a repetition code, in which each symbol of a message is repeated many times to create redundancy. The use of error-correcting codes, as well as the issue of fault-tolerance in nanocomputing in general, awaits further investigation. Novel computing systems, envisioned now as adaptive systems based on molecular electronics, biology-inspired self-learning and -evolving systems, nonlinear dynamical systems and quantum computers, may in the long term emerge, possibly leading to new types of algorithms and architectures. The choice of algorithms and architectures must aim towards applications in nanotechnology. An architecture will strongly influence the design of devices and circuits, and vice versa: the opportunities and problems found in nanoelectronic devices and circuits will strongly influence the choice of an architecture. In research on nanocomputer architectures, therefore, an interdisciplinary approach must be followed and the success will eventually rely upon a multidisciplinary effort in the fields of chemistry, physics, electrical engineering, computer science, and, perhaps, many others. Applied Science...|$|R
40|$|The Hyperspectral Microwave Atmospheric Sounder (HyMAS) {{is being}} {{developed}} at Lincoln Laboratories and accommodated by the Goddard Space Flight Center for a flight opportunity on a NASA research aircraft. The term "hyperspectral microwave" is used to indicate an all-weather sounding that performs equivalent to hyperspectral infrared sounders in clear air with vertical resolution of approximately 1 km. Deploying the HyMAS equipped scanhead with the existing Conical Scanning Microwave Imaging Radiometer (CoSMIR) shortens the path to a flight demonstration. Hyperspectral microwave is achieved {{through the use of}} independent RF antennas that sample the volume of the Earth s atmosphere through various levels of frequencies, thereby producing a set of dense, spaced vertical weighting functions. The simulations proposed for HyMAS 118 / 183 -GHz system should yield surface precipitation rate and water path retrievals for small hail, soft hail, or snow pellets, snow, rainwater, etc. with accuracies comparable to those of the Advanced Technology Microwave Sounder. Further improvements in retrieval methodology (for example, polarization exploitation) are expected. The CoSMIR instrument is a packaging concept re-used on HyMAS to ease the integration features of the scanhead. The HyMAS scanhead will include an ultra-compact <b>Intermediate</b> Frequency <b>Processor</b> (IFP) module that is mounted inside the door to improve thermal management. The IFP is fabricated with materials made of Low-Temperature Co-fired Ceramic (LTCC) technology integrated with detectors, amplifiers, A/D conversion and data aggregation. The IFP will put out 52 channels of 16 bit data comprised of 4 - 9 channel data streams for temperature profiles and 2 - 8 channel streams for water vapor. With the limited volume of the existing CoSMIR scanhead and new HyMAS front end components, the HyMAS team at Goddard began preliminary layout work inside the new drum. Importing and re-using models of the shell, the scan head computer, and the slip rings developed for CoSMIR was the starting point. The next step was to modify the antenna faceplate to accommodate the dimensions of the three dual polarization Gaussian Optics Antenna (GOA) assemblies. Two mechanical concepts for the core technology, the hyperspectral IFP, were captured in a design tradeoff. Connector models considered minimum bend radii for the IFP analog connectors. Hyperspectral imaging is accomplished by strategically using a short wavelength intermediate frequency of 18 - 29 GHz, and thus reducing the size of components in the connection of the front end to the IFP. The SMK (2. 92 mm) Series connector will lay near the hinge line to minimize its flexing. The digital output of the IFP will use a Serial Peripheral Interface (SPI) that must be accommodated by the scan head computer. To make that computer more reliable, maintainable, and forward compatible with the 52 HyMAS channels, a testbed of the scan head, calibration, and archive computers and the PIC 24 microprocessor that resides on the IFP is in development. The computers will be programmed using a new framework application called Interoperable Remote Component (IRC). This software allows flexibility to program computers that communicate with each other and can adapt easily to the emerging HyMAS requirements for data format, algorithms, and graphical user interface (GUI). It is expected that the CoSMIR instrument will cut over to the IRC after it is adapted on an updated CoSMIR testbed...|$|R
