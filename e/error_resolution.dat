45|274|Public
2500|$|In the United States, PayPal is {{licensed}} as a money transmitter, on a state-by-state basis. But {{state laws}} vary, as do their definitions of banks, narrow banks, money services businesses and money transmitters. [...] Although PayPal is not {{classified as a}} bank, the company is subject {{to some of the}} rules and regulations governing the financial industry including Regulation E consumer protections and the USA PATRIOT Act. The most analogous regulatory source of law for PayPal transactions comes from P2P payments using credit and debit cards. Ordinarily, a credit card transaction, specifically the relationship between the issuing bank and the cardholder, is governed by the Truth in Lending Act (TILA) 15 U.S.C. §§ 1601-1667f as implemented by Regulation Z, 12 C.F.R. 226, (TILA/Z). TILA/Z requires specific procedures for billing errors, dispute resolution and limits cardholder liability for unauthorized charges. Similarly, the legal relationship between a debit cardholder and the issuing bank is regulated by the Electronic Funds Transfer Act (EFTA) 15 U.S.C. §§ 1693-1693r, as implemented by Regulation E, 12 C.F.R. 205, (EFTA/E). EFTA/E is directed at consumer protection and provides strict <b>error</b> <b>resolution</b> procedures. However, because PayPal is a payment intermediary and not otherwise regulated directly, TILA/Z and EFTA/E do not operate exactly as written once the credit/debit card transaction occurs via PayPal. Basically, unless a PayPal transaction is funded with a credit card, the consumer has no recourse in the event of fraud by the seller.|$|E
5000|$|... sensor {{calibration}} (particle-sizing calibration, coincidence <b>error,</b> <b>resolution</b> and flow rate limit determination etc.) ...|$|E
50|$|The {{financial}} institution must give the customer notice of their liability {{in case the}} card is lost or stolen. This must include a phone number for reporting the loss and a description of its <b>error</b> <b>resolution</b> process.|$|E
40|$|The {{use of a}} fiber-optic-link CCD-detector Fabry-Perot {{interferometer}} (McMillan et al., 1985, 1986, and 1988) {{to obtain}} high-accuracy measurements of stellar Doppler shifts at KPNO is described in detail and illustrated with sample data. Particular attention is given to accuracy requirements and techniques for reducing <b>errors,</b> <b>resolution</b> (orders of 50 mA at wavelength 4300 A are separated by 640 mA), CCD sensitivity, observing and data-processing operations, and the control of environmental conditions. Standard-deviation data and statistics on seven solar-type stars are presented in tables, and the time evolution of the radial velocity of Beta Com is shown in a graph...|$|R
40|$|The variational {{multiscale}} method (VMM) {{provides a}} general framework {{for construction of}} multiscale finite element methods. In this {{paper we propose a}} method for parallel solution of the fine scale problem based on localized Dirichlet problems which are solved numerically. Next we present a posteriori error estimates for VMM which relates the error in linear functionals and the energy norm to the discretization <b>errors,</b> <b>resolution</b> and size of patches in the localized problems, in the fine scale approximation. Based on the a posteriori error estimates we propose an adaptive VMM with automatic tuning of the critical parameters. We primary study elliptic second order partial di#erential equations with highly oscillating coe#cients or localized singularities...|$|R
40|$|Conflict {{resolution}} {{is the process}} of reaching a decision using the opinions of multiple knowledge sources as input. It integrates two closely related concepts: reasoning about uncertainty and constraint propagation as applied to problems involving knowledge integration. This paper introduces the concept of characteristic <b>error</b> conflict <b>resolution</b> which is unique in treating each knowledge source as a separate entity whose validity b determined only from the information the knowledge source itself provides. The advantages of characteristic <b>error</b> conflict <b>resolution</b> are that the knowledge sources themselves provide the information necessary to determine the current context of the conflict and that the outcome of the resolution process is dependent only upon this locally determined context. This leads to an easily extensible generic approach to conflict resolution. 1...|$|R
50|$|If similar Production error {{occurred}} in the past then the issue resolution steps are retrieved from the support knowledge base and error is resolved using those steps. If it is a new Production error then new Production <b>error</b> <b>resolution</b> steps are created and Production error is resolved. The new Production <b>error</b> <b>resolution</b> steps are recorded in the knowledge base for the future usage. For major Production errors (critical infrastructure or application failures), a phone conference call is initiated and all required support persons/teams join the call and they all work together to resolve the error. This is also called as an Incident Management. If a problem occurs repeatedly then it is recorded and tracked using appropriate tools and processes until it is resolved permanently. This is also called as Problem Management. The issue is closed only after the customer or end user agrees that the problem is resolved.|$|E
50|$|If any similar {{business}} application errors {{occurred in the}} past then the issue resolution steps are retrieved from the support knowledge base and the error is resolved using those steps. If it is a new support error, then new issue resolution steps are created and the error is resolved. The new support <b>error</b> <b>resolution</b> steps are recorded in the knowledge base for future use. For major {{business application}} errors (critical infrastructure or application failures), a phone conference call is initiated and all required support persons/teams join the call and they all work together to resolve the error.|$|E
50|$|In the United States, PayPal is {{licensed}} as a money transmitter, on a state-by-state basis. But {{state laws}} vary, as do their definitions of banks, narrow banks, money services businesses and money transmitters. Although PayPal is not {{classified as a}} bank, the company is subject {{to some of the}} rules and regulations governing the financial industry including Regulation E consumer protections and the USA PATRIOT Act. The most analogous regulatory source of law for PayPal transactions comes from P2P payments using credit and debit cards. Ordinarily, a credit card transaction, specifically the relationship between the issuing bank and the cardholder, is governed by the Truth in Lending Act (TILA) 15 U.S.C. §§ 1601-1667f as implemented by Regulation Z, 12 C.F.R. 226, (TILA/Z). TILA/Z requires specific procedures for billing errors, dispute resolution and limits cardholder liability for unauthorized charges. Similarly, the legal relationship between a debit cardholder and the issuing bank is regulated by the Electronic Funds Transfer Act (EFTA) 15 U.S.C. §§ 1693-1693r, as implemented by Regulation E, 12 C.F.R. 205, (EFTA/E). EFTA/E is directed at consumer protection and provides strict <b>error</b> <b>resolution</b> procedures. However, because PayPal is a payment intermediary and not otherwise regulated directly, TILA/Z and EFTA/E do not operate exactly as written once the credit/debit card transaction occurs via PayPal. Basically, unless a PayPal transaction is funded with a credit card, the consumer has no recourse in the event of fraud by the seller.|$|E
40|$|ABSTRACT Human {{matching}} {{between different}} fields {{of view is}} a difficult problem in intelligent video surveillance; whereas fusing multiple features has become a strong tool to solve it. In order to guide the fusion scheme, {{it is necessary to}} evaluate the matching performance of these features. In this paper, four typical features are chosen for the evaluation. They are the Color Histogram, UV Chromaticity, Major Color Spectrum Histogram, and Scale-Invariant Features (SIFT). Quantities of video data are collected to test their general accuracy, robustness, and real-time applicability. The robustness is measured under the conditions of illumination changes, Gaussian and salt noises, foreground <b>errors,</b> <b>resolution</b> changes, and camera angle differences. The experimental results show that the four features bear distinctive performances under the different conditions, which will provide important references for the feature fusion methods...|$|R
40|$|This paper {{discusses}} {{the problem of}} context-free position estimation using a stereo vision system with moveable eyes. Exact and approximate equations are developed linking position to measureable quantities of the image-space, and an algorithm for finding these quantities is suggested in rough form. An estimate of <b>errors</b> and <b>resolution</b> limits is provided...|$|R
40|$|The {{purpose of}} this {{research}} was to quantify the statistical relationship between spatial <b>resolution</b> <b>error</b> in a regional surface flux model and surface parameter heterogeneity. Three studies were carried out. In the first, <b>resolution</b> <b>error</b> in albedo, canopy height, canopy resistance, Normalized Difference Vegetation Index (NDVl) and calculated evapotranspiration was quantified for a site in Ellington, Connecticut. These values corresponded well with errors predicted by an equation based on spatial autocorrelations. In the second study, the effect of surface cover patch size and patch contrast on <b>resolution</b> <b>error</b> was investigated using six artificial landscapes. The error prediction equation appeared sensitive to these scene-dependent components of spatial heterogeneity. In the third study, the evapotranspiration model 2 ̆ 7 s relative sensitivity to the spatial averaging error in its input parameters was quantified using four methods: (1) simple algebraic test, (2) varying parameter resolutions, (3) predicting sensitivity using partial derivatives and (4) analysis of probable error vectors. Error vector results were presented using a color compositing data visualization technique. All methods indicated that error in canopy resistance was mainly responsible for <b>resolution</b> <b>error</b> in calculated ET. The scene-dependence of the model 2 ̆ 7 s sensitivity appears to be driven by thematic misclassification probabilities and potential parameter error vectors. ...|$|R
40|$|The role of {{unmanned}} {{vehicles in}} military and commercial environments continues to expand, resulting in Shared Manned-Unmanned (SMU) domains. While {{the introduction of}} unmanned vehicles can have many benefits, humans operating within these environments must shift to high-level supervisory roles, which will require them to resolve system errors. <b>Error</b> <b>resolution</b> in current Human Supervisory Control (HSC) domains is performed using a checklist; the error is quickly identified, and then resolved using the steps outlined by the checklist. Background research into <b>error</b> <b>resolution</b> identified three attributes that impact the effectiveness of an <b>error</b> <b>resolution</b> checklist: domain predictability, sensor reliability, and time availability. These attributes were combined into a Checklist Attribute Model (CAM), demonstrating that HSC domains {{with high levels of}} complexity (e. g. SMU domains) are ill-suited to <b>error</b> <b>resolution</b> using traditional checklists. In particular, it was found that more support was required during such error identification, as data is uncertain and unreliable. A new <b>error</b> <b>resolution</b> checklist, termed the GUIDER (Graphical User Interface for Directed Error Recovery) Probabilistic Checklist, was developed to aid the human during the error identification process in SMU domains. Evaluation was performed through a human performance experiment requiring participants to resolve errors in a simulated SMU domain using the GUIDER Probabilistic Checklist and a traditional checklist tool. Thirty-six participants were recruited, and each was assigned to a single checklist tool condition. Participants completed three simulated error scenarios. The three scenarios had varying sensor reliability levels (low, medium, high) to gauge the impact of uncertainty on the usefulness of each checklist tool. The human performance experiment showed that the addition of error likelihood data using an intuitive visualization through the GUIDER Probabilistic Checklist improved <b>error</b> <b>resolution</b> in uncertain settings. In settings with high certainty, there was no difference found between the performances of the two checklists. While positive, further testing is required in more realistic settings to validate both the effectiveness of the GUIDER Probabilistic Checklist tool and the Checklist Attribute Model. by Jacqueline M. Tappan. Thesis (S. M.) [...] Massachusetts Institute of Technology, Engineering Systems Division, 2010. Cataloged from PDF version of thesis. Includes bibliographical references (p. 151 - 153) ...|$|E
40|$|Hyperarticulate {{speech to}} {{computers}} remains a poorly understood phenomenon, {{in spite of}} its association with elevated recognition errors. The present research analyzes the type and magnitude of linguistic adaptations that occur when people engage in <b>error</b> <b>resolution</b> with computers. A semi-automatic simulation method incorporating a novel error generation capability was used to collect speech data immediately before and after system recognition errors, and under conditions varying in error base-rates. Data on original and repeated spoken input, which were matched on speaker and lexical content, then were examined for type and magnitude of linguistic adaptations. Results indicated that speech during <b>error</b> <b>resolution</b> primarily was longer in duration, including both elongation of the speech segment and substantial relative increases in the number and duration of pauses. It also contained more clear speech phonological features and fewer spoken disfluencies. Implications of these findings a [...] ...|$|E
40|$|This paper {{describes}} how speakers adapt their language during <b>error</b> <b>resolution</b> when {{interacting with the}} animated agent Pixie. A corpus of spontaneous human-computer interaction was collected at the Telecommunication museum in Stockholm, Sweden. Adult and children speakers were compared with respect to user behavior and strategies during <b>error</b> <b>resolution.</b> In this study, 16 adults and 16 children speakers {{were randomly selected from}} a corpus from almost 3. 000 speakers. This sub-corpus was then analyzed in greater detail. Results indicate that adults and children use partly different strategies when their interactions with Pixie become problematic. Children tend to repeat the same utterance verbatim, altering certain phonetic features. Adults, on the other hand, often modify other aspects of their utterances such as lexicon and syntax. Results from the present study will be useful for constructing future spoken dialogue systems with improved error handling for adults as well as children...|$|E
40|$|Data {{processing}} and error analysis of noise and systematic <b>errors</b> in high <b>resolution</b> infrared radiometer cloud cover data from Nimbus 2. Prepared at Goddard Space Flight Center. "April 1970. " [...] Cover. Major NASA subject terms: Cloud Cover; Error Analysis; Infrared Radiation; Nimbus 2 Satellite; Radiometers; Random Noise; Data Processing; Error Correcting Devices; Meteorological Charts; Temperature Gradients;Includes bibliographical references (p. 13 - 14). Data {{processing and}} error analysis of noise and systematic <b>errors</b> in high <b>resolution</b> infrared radiometer cloud cover data from Nimbus 2. Mode of access: Internet...|$|R
40|$|Natural Language Processing (NLP) techniques, such as toponym {{detection}} and resolution, {{are an integral}} part of most Geographic Information Retrieval (GIR) architectures. Without these components, synonym detection, ambiguity resolution and accurate toponym expansion would not be possible. However, there are many important factors affecting the success of an NLP approach to GIR, including toponym detection <b>errors,</b> toponym <b>resolution</b> <b>errors,</b> and query overloading. The aim of this paper is to determine how severe these errors are in state-of-the-art systems, and to what extent they affect GIR performance. We show that a careful choice of weighting schemes in the IR engine can minimize the negative impact of these errors on GIR accuracy. We provide empirical evidence from the GeoCLEF 2005 and 2006 datasets to support our observations...|$|R
40|$|This paper {{reviews the}} best {{practices}} and challenges for project managers and developers involved in implementing text-mining applications. With focus on rule-based information extraction, and references to actual cases, the authors {{share their experiences}} from developing several text-mining applications in diverse industries. First, project management issues are discussed, including a process for capturing business requirements and mapping them into features and linguistic patterns, development of linguistic rules, rule development standards, performance metrics, and an evaluation methodology. Linguistic representations such as sub-syntactic, syntactic, semantic, and application-specific rules are identified. Special {{emphasis is placed on}} post-information extraction processing, such as improving the relevance of the extracted information, summarization models, techniques for handling typographical <b>errors,</b> <b>resolution</b> of temporal information, resolution of uniqueness of features and events, anaphora resolution, and a discussion on shallow vs. full parsing. Lastly, the paper discusses various utilities to help with the development of a text-mining application, such as feature analysis, visualization, database connectivity, source document pre-processing, and rule authoring tools...|$|R
40|$|In this paper, the August spoken {{dialogue}} {{system is}} described. This experimental Swedish dialogue system, which featured an animated talking agent, {{was exposed to}} the general public during a trial period of six months. The construction of the system was partly motivated by the need to collect genuine speech data from people with little or no previous experience of spoken dialogue systems. A corpus of more than 10, 000 utterances of spontaneous computer-directed speech was collected and empirical linguistic analyses were carried out. Acoustical, lexical and syntactical aspects of this data were examined. In particular, user behavior and user adaptation during <b>error</b> <b>resolution</b> were emphasized. Repetitive sequences in the database were analyzed in detail. Results suggest that computer-directed speech during <b>error</b> <b>resolution</b> is increased in duration, hyperarticulated and contains inserted pauses. Design decisions which may have influenced how the users behaved when they interacted with August are discussed and implications for the development of future systems are outlined...|$|E
40|$|Some two-scale {{finite element}} discretizations are {{introduced}} {{for a class}} of linear partial differential equations. Both boundary value and eigenvalue problems are studied. Based on the two-scale <b>error</b> <b>resolution</b> techniques, several two-scale finite element algorithms are proposed and analyzed. It is shown {{that this type of}} two-scale algorithms not only significantly reduces the number of degrees of freedom but also produces very accurate approximations...|$|E
40|$|A bstr act: Using {{speech input}} {{to augment the}} remote control can be an {{alternative}} interaction technique for interactive television. However, {{little is known about}} how to design such a system that is suitable for the home environment. In this paper we explore possible <b>error</b> <b>resolution</b> strategies in the case of speech recognition errors in a T V-setting. From previous research two techniques have been identified: repetition of input by the user and choice of interpretation. From these two techniques, four alternative strategies, suitable for a T V-environment, were designed and tested with a W izard-of-Oz method: one with repetition and three based on choosing the best alternative from an n-best list given in audio and visual mode, alone or combined. T he results show that displaying an n-best list gives the most efficient interaction. R edundant audio feedback does not influence the performance. K eywor ds: Speech input, <b>error</b> <b>resolution,</b> interactive television, electronic program guide (E PG), Wizard-of-Oz 1 I ntr oduction In the near future we will have many thousands of T V channels to choose from. Consequently, selecting T V content may become too cumbersome to handle with a remote control, since it does no...|$|E
2500|$|In 2017 a [...] "Fan patch" [...] fixed {{two major}} {{problems}} of the PC version, an <b>error</b> in the <b>resolution</b> setting and general performance problems even with beyond requirements hardware.|$|R
50|$|Automated Build Studio: Automated Build Studio, or ABS, is an {{automated}} build and release management system that offers {{drag and drop}} macro creation, automated nightly builds and continuous integration. AQtrace: AQtrace is an <b>error</b> reporting and <b>resolution</b> system.|$|R
40|$|Predicted device {{reliability}} for nanoelectronics indicates that redundant design {{will be necessary}} to build reliable nanosystems. The study of such systems requires the evaluation of the error probabilities associated to the fabrication process complexity. In this paper we compare two layouts for a basic NAND gate used to implement NAND Multiplexing (NM) redundant gates. To analyse the effects of the layouts, we derive models to calculate the error probability of each gate part according to the <b>resolution</b> <b>errors</b> of the manufacturing process. Our results indicate that gates built with diode-logic topologies are more reliable than gates built with CMOS like topologies and that the <b>resolution</b> <b>errors</b> limit the redundancy of practical NM gates...|$|R
40|$|The article first {{describes}} mobile {{financial services}} for consumers {{and the types}} of companies participating in the provision of those services. Anticipated consumer problems are explored, including: security, privacy, unauthorized transfers, <b>error</b> <b>resolution,</b> viruses, system breakdown, consumer mistake, and the need for documentation and a history of transactions. Public policy and government regulatory issues are examined. Applicable U. S. state and federal laws are reviewed; their gaps and inadequacies are identified. The article concludes with a description of a proposed Model Law that provides satisfactory consumer protection...|$|E
40|$|We {{investigate}} the feasibility {{to improve the}} <b>error</b> <b>resolution</b> through automation like natural language-based and statistical analysis algorithms, in order to detect errors and security issues, and to convert error messages from encrypted into human-readable ones in a heterogonous software environment. To reach this goal we study a real case using the data extracted from PopCon, a package used for the population of CMS Condition Databases, that is embedded into CMS Software framework, CMSSW, and relies on different underlying applications such as ORACLE, POOL, CORAL in order to perform database transactions...|$|E
40|$|Henry Ford Hospital, a 1, 049 bed, Detroit area {{teaching}} hospital and ambulatory services center, {{is in the}} final stages of implementing a vendor-supplied, laboratory computer system. Implementation planning was guided by the desire to minimize disruptions within the hospital system. Efforts focused on obtaining user input into the process of moving from a manual to an automated results reporting system. Operational characteristics as well as mechanisms for eliciting this user response while maintaining realistic expectations for system capabilities are discussed. Special emphasis is given to order entry concerns, result reports and <b>error</b> <b>resolution...</b>|$|E
40|$|Clothing {{designers}} and manufacturers use traditional body dimensions as their basis. When 3 D-whole body scanners {{are introduced to}} determine the body dimensions, a conversion has to be made, since scan determined circumference measures are slightly larger than the traditional values. This pilot study showed that the conversion factor is 3 to 4 % for the biceps and calf. Shadow scanners have more error sources for circumference determination than 3 D-scanners (regression errors, rotation errors, location <b>errors,</b> and <b>resolution</b> <b>errors)</b> which lead to an overall error of 27 mm. For relatively tight fitting clothing {{the accuracy of the}} shadow scanning is insufficient for appropriate clothing size determination or automatic grading. For loose fitting clothing, however, like work clothing, the required fitting accuracy may be delivered by a shadow scanner. It is expected that the introduction of whole body scanners in the clothing industry is a matter of time, since they offer considerable advantages over the traditional methods in accuracy, speed and costs...|$|R
40|$|In {{this paper}} a new {{parameter}} estimation based criterion for two-point resolution is proposed. Unlike the classical resolution criteria, the new criterion takes account of noise and systematic <b>errors.</b> A <b>resolution</b> limit {{in terms of}} the observations is derived. This limit depends on the point spread function used and the degree of coherence supposed. For statistical observations the probability of resolution {{as a function of the}} SNR is derived. This probability can be used as a performance measure in the assessment of optical instruments. 1...|$|R
40|$|The fission yeast Schizosaccharomyces pombe {{switches}} mating type by transposition of {{a copy of}} DNA {{derived from}} {{either of the two}} storage cassettes, mat 2 -P and mat 3 -M, into the expression locus, mat 1. The recombinational event of switching is initiated by a double-stranded DNA break present in approximately 20 % of the molecules at mat 1. Fifty-three mutants defective in switching of mating type have been isolated previously, and each has been assigned to 1 of 10 linkage groups. One group consists of cis-acting mutations at mat 1, which reduce the amount of the DNA double-strand cut. The remaining nine groups are mutations in genes that are unlinked to the mating-type locus and are studied here. Three (swi 1, - 3, - 7) are required for formation of the double-strand cut, whereas the others are not. Mutants of three genes (swi 4, - 8, - 9) undergo high-frequency rearrangement of the mating-type locus indicative of <b>errors</b> of <b>resolution</b> of recombinational intermediates. The remaining three (swi 2, - 5, - 6) have normal levels of cut, do not make <b>errors</b> of <b>resolution,</b> and possibly are required either for efficient utilization of the cut or determining the directionality of switching. The data suggest that the switching process can be dissected into genetically distinguishable steps...|$|R
40|$|This paper {{describes}} {{a study on}} <b>error</b> <b>resolution</b> strategies chosen when speech recognition errors occur in a command and control environment. Thirty-eight subjects participated in the study. The results indicate that strategies differ depending {{on the length of}} the error sequence. Users chose the following resolution strategies: switch to another input device, accept result, repeat command, work around using another set of commands to reach the same goal, or change goal. After incorrect recognition, users select another resolution strategy (e. g., the switch strategy). Users gave up on speech input after a maximum affive attempts to achieve the desired result...|$|E
40|$|When {{speaking}} to interactive systems, people sometimes hyperarticulate — or adopt a clarified form of speech {{that has been}} associated with increased recognition errors. The goals of the present study were: (1) to establish a flexible simulation method for studying users ’ reactions to system errors, (2) to analyze the type and magnitude of linguistic adaptations in speech during human-computer <b>error</b> <b>resolution,</b> (3) to provide a unified theoretical model for interpreting and predicting users ’ spoken adaptations during system error handling, and (4) to outline the implications for developing more robust interactive systems. A semi-automatic simulation method with a novel error generation capability was developed to compare users ’ speech immediately before and after system recognition errors, and under conditions varying in error base-rate. Matched original-repeat utterance pairs then were analyzed for type and magnitude of linguistic adaptation. When resolving errors with a computer, it was revealed that users actively tailor their speech along a spectrum of hyperarticulation, and as a predictable reaction to their perception of the computer as an &quot;at risk &quot; listener. During both low and high error rates, durational changes were pervasive, including elongation of the speech segment and large relative increases in the number and duration of pauses. During a high error rate, speech also was adapted to include more hyper-clear phonological features, fewer disfluencies, and change in fundamental frequency. The two-stage CHAM model (Computer-elicited Hyperarticulate Adaptation Model) is proposed to account for these changes in users ’ speech during interactive <b>error</b> <b>resolution...</b>|$|E
40|$|Security-typed {{languages}} provide information-flow security {{guarantees to}} programs which compile successfully, ensuring that when run, protected data does not leak. However, when a program fails to compile, {{the reason for}} the error is often unclear as reasoning about the information flows involved can be quite subtle. We present a general model for blame that answers queries regarding the potential sources of an error in a security-typed program. We prove that the model enables the retrieval of a minimal set of constraints that contributed to the error. We then apply this framework to the problem of retrofitting possibly-insecure Java programs into information-flow secure programs. We find that the use of our blame model improves <b>error</b> <b>resolution</b> and enables reasoning over possible resolutions at the code-level...|$|E
40|$|Tomography in {{seismology}} {{often leads}} to underdetermined and inconsistent systems of linear equations. When solving, {{care must be taken}} to keep the propagation of data errors under control. In this paper I test the applicability of three types of damped least-squares algorithms to the kind of sparse matrices encountered in seismic tomography: (1) singular value decomposition with Lanczos iteration, (2) conjugate gradient iteration with the LSQR algorithm, and (3) the Dines-Lytle method. Lanczos iteration may be applied to large sparse systems of low rank to calculate solutions by singular value decomposition but becomes impractical with problems of larger size. The Paige-Saunders algorithm (LSQR), which incorporates Lanczos' iteration into a conjugate gradient method, provides a least-squares solution to the system with acceptable filtering properties. In a synthetic tomographic experiment, it proved to be converging up to an order of magnitude faster than the Dines-Lytle algorithm, a stationary iterative process. For large tomographic systems, where restrictions in the available computer time pose limitations on the number of iterations, this indicates that conjugate directions methods are to be preferred to the more commonly applied Gauss-Seidel type of algorithms. To avoid unwarranted conclusions when the problem is severely underdetermined or undermined with large data <b>errors,</b> <b>resolution</b> analysis must be applied to the data set. An algorithm for the determination of the resolution in sparse systems is given...|$|R
40|$|A {{numerical}} filter {{inversion technique}} that reduces wide-angle satellite measurements to top-of-the-atmosphere radiant exitances {{has been proposed}} for the Earth Radiation Budget Experiment (ERBE). The matrix formulation of this technique is presented, and {{the design of the}} numerical filter is discussed. The filter is smoothed with a singular value decomposition. The inversion process is simulated by generating synthetic measurements from a 24 degree spherical harmonic radiation field derived from Nimbus 6 ERB data. The numerical filter is applied to these measurements after they are corrupted with instrument error. The results are curves of expected <b>error</b> versus <b>resolution</b> area...|$|R
40|$|Iterative back-projection {{tomography}} and generalized inversion without blocks (‘no-block’) are {{two different}} inversion techniques developed recently for 3 -D studies, and are commonly applied to the inversion of travel-time data. In this study, we compare the two methods and derive {{one from the other}} under certain assumptions. We then apply these two methods to the attenuation problem, inverting for the quality factor, Q, of the medium. Usually, travel-time inversion involves large data sets and fine resolution is not possible if generalized inversion is applied. A relatively small data set with little redundancy enables us to apply both techniques with similar resolution. We applied the methods to the data sets obtained for two areas in southern California, the Coso-Indian Wells region and Imperial Valley. The results obtained by the two methods are very similar. Back-projection tomography is a direct and fast method for this type of problem. However, it does not provide formal <b>error</b> estimates and <b>resolution.</b> The no-block inversion requires more computational time, but formal <b>errors</b> and <b>resolution</b> can be directly computed for the final model. Thus, application of the two methods to the same data set enhances the objectivity of the final result...|$|R
