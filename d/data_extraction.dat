5329|1347|Public
25|$|Effective <b>data</b> <b>extraction</b> {{achieved}} through the pre-structuring of aggregated data.|$|E
25|$|In 2002, {{he became}} a {{founding}} partner of Groupcall, which specialises in providing communication software and <b>data</b> <b>extraction</b> tools to the education, public and business sectors. His initial involvement arose from concerns for his children's safety.|$|E
25|$|Curve plots: VisIt can plot curves {{from data}} read from files {{and it can}} be used to extract and plot curve data from higher-dimensional {{datasets}} using lineout operators or queries. The curves in the featured image correspond to elevation data along lines drawn on DEM data and were created with the feature lineout capability. Lineout allows you to interactively draw a line, which specifies a path for <b>data</b> <b>extraction.</b> The resulting data was then plotted as curves.|$|E
50|$|EPPI-Reviewer is a web {{application}} that enables researchers {{to manage the}} entire lifecycle of a review in a single location. Users are able to upload studies for screening, complete keywording and <b>data</b> <b>extractions</b> and analyse the results over the internet. Analytical functions include meta-analysis, summary tables and support for qualitative synthesis.|$|R
40|$|Here, {{we propose}} a <b>data</b> hiding and <b>extraction</b> {{procedure}} for high resolution AVI (Audio Video Interleave) videos. Although AVI videos are large in size {{but it can}} be transmitted from source to target over network after processing the source video by using these <b>Data</b> Hiding and <b>Extraction</b> procedure securely. There are two different procedures, which are used here at the sender’s end and receiver’s end respectively. The procedures are used here as the key of <b>Data</b> Hiding and <b>Extraction...</b>|$|R
40|$|Li, Longzhuang, Liu, Yonghuai, Obregon, A., Weatherston, M. Visual Segmentation-Based <b>Data</b> Record <b>Extraction</b> From Web Documents. Proceedings of IEEE International Conference on Information Reuse and Integration, 2007, pp. 502 - 507. Sponsorship: IEEESemi-structured data records {{contained}} in the Web pages provide useful information for shopping agents and metasearch engines. In this paper, we present a visual segmentation-based <b>data</b> record <b>extraction</b> (VSDR) method to extract data records from those Web pages. VSDR method first segments a Web page into semantic blocks using the spatial closeness and visual resemblance of data records, then neighboring and non-neighboring data records are extracted based on a compress and collapse technique. Experimental results slum that unlike the existing methods which only generate good results on their test domains, VSDR is a general <b>data</b> record <b>extraction</b> method that is able to produce quite stable and good results {{on a wide range}} of Web pages...|$|R
25|$|IRI {{software}} {{is designed to}} transform, convert, report, and protect large data volumes rapidly in distributed, heterogeneous computing environments. These functions are built into the CoSort package or through spin-offs for <b>data</b> <b>extraction,</b> generation, security, and migration. Each tool uses the same graphical IDE built on Eclipse, and metadata format for defining and manipulating data. IRI's open data definition file format is also supported by AnalytiX DS and Meta Integration Technology (MITI) so that third-party ETL, BI, and data modeling tool users can convert or re-use their existing metadata in IRI product environments.|$|E
2500|$|... "For data {{warehouse}} and data mart applications, CoSort performs source <b>data</b> <b>extraction,</b> data cleansing, sorting, reformatting, data type conversion, aggregation, and indexing, {{all in a}} single pass. Most operational data in commercial and public sector enterprises reside internally in sequential flat files, (relational) database tables, or are imported from data tapes and transmissions generated externally. These historical databases are optimized for ad hoc queries and transactions, rather than for extraction. CoSort accepts multiple input files (large-scale tables or flat-file data dumps), or records streaming through pipes, to perform conditional selection on records for downstream processes." [...] - Dennis Hill, Database Trends Magazine, July 1999 ...|$|E
5000|$|Typical {{unstructured}} data sources include web pages, emails, documents, PDFs, scanned text, mainframe reports, spool files, classifieds, etc. Which is further used for sales / marketing leads. [...] Extracting data from these unstructured sources {{has grown into}} a considerable technical challenge where as historically <b>data</b> <b>extraction</b> has had to deal with changes in physical hardware formats, the majority of current <b>data</b> <b>extraction</b> deals with extracting data from these {{unstructured data}} sources, and from different software formats. This growing process of <b>data</b> <b>extraction</b> from the web is referred to as Web scraping.|$|E
2500|$|Apple {{has made}} clear its stance on privacy and as such has made {{available}} Transparency Reports on the Governmental Requests it receives. Apple states plainly, [...] "On devices running iOS 8 and later versions, your personal data is placed {{under the protection of}} your passcode. For all devices running iOS 8 and later versions, Apple will not perform iOS <b>data</b> <b>extractions</b> in response to government search warrants because the files to be extracted are protected by an encryption key that is tied to the user's passcode, which Apple does not possess." ...|$|R
40|$|Time-of-flight {{cameras are}} used on mobiles service robots to build 3 D {{maps of the}} environment. Important steps are sensor <b>data</b> filtering, feature <b>extraction,</b> <b>data</b> {{association}} and map aggregation. 3 D maps are used for various applications like collision avoidance during navigation and manipulation or visualization during tele-operation of robots...|$|R
40|$|Flight test {{instrumentation}} engineers {{are provided with}} an introduction to digital processes on aircraft. Flight {{test instrumentation}} systems are rapidly evolving from analog intensive to digital intensive systems, {{including the use of}} onboard digital computers. Topics include: measurements that are digital in origin, sampling, encoding, transmitting, and storing of data. Particular emphasis is placed on modern avionic data bus architectures and what to be aware of when extracting data from them. Some example <b>data</b> <b>extractions</b> are given. Tradeoffs between digital logic families, trends in digital development, and design testing techniques are discussed. An introduction to digital filtering is also covered...|$|R
5000|$|Effective <b>data</b> <b>extraction</b> {{achieved}} through the pre-structuring of aggregated data.|$|E
50|$|There {{were always}} two main {{components}} comprising the system, the <b>data</b> <b>extraction</b> component {{and the data}} conversion component. In {{the first version of}} the system the <b>data</b> <b>extraction</b> component was an SQR script authored originally by MRO software for Sapphire. The data transformation engine was originally authored by Sapphire in Visual Basic.|$|E
5000|$|A <b>data</b> <b>extraction</b> and {{presentation}} tool for custom analysis and data management ...|$|E
2500|$|Emerging <b>data</b> mining, feature <b>extraction,</b> {{image and}} video processing, and human-computer {{interaction}} applications ...|$|R
30|$|<b>Data</b> acquisition, feature <b>extraction</b> and/or expert {{modelling}} {{to derive}} proxy {{values for the}} analyzed ESs.|$|R
40|$|Big Data {{enhances the}} {{possibilities}} for storing personal data extracted from social media and web search on an unprecedented scale. This paper draws on {{the political economy of}} information which explains why the online industry fails to self-regulate, resulting in increasingly insidious web-tracking technologies. Content analysis of historical blogs and request for comments on HTTP cookies published by the Internet Engineering Task Force illustrates how cookie technology was introduced in the mid- 1990 s, amid stark warnings about increased system vulnerabilities and deceptive personal <b>data</b> <b>extractions.</b> In conclusion, online users today are left with few alternatives but to enter into unconscionable contracts about the extraction of their personal data when using the Internet for private purposes...|$|R
5000|$|... ° Data Staging Component 1. <b>Data</b> <b>Extraction</b> 2. Data Transformation 3. Data Loading ...|$|E
5000|$|<b>Data</b> <b>extraction</b> is a GIS process {{similar to}} vector overlay, {{though it can}} be used in either vector or raster data analysis. Rather than {{combining}} the properties and features of both datasets, <b>data</b> <b>extraction</b> involves using a [...] "clip" [...] or [...] "mask" [...] to extract the features of one data set that fall within the spatial extent of another dataset.|$|E
5000|$|... {{compromised}} data tracking and <b>data</b> <b>extraction</b> from botnets; automatic search and monitoring of [...] "underground" [...] forums; ...|$|E
40|$|Abstract. We {{propose a}} speech {{comprehension}} software architecture {{to represent the}} flow of the natural processing of auditory sentences. The computational implementation applies wavelets transforms to speech sig-nal codification and <b>data</b> prosodic <b>extraction,</b> and connectionist models to syntactic parsing and prosodic-semantic mapping. ...|$|R
40|$|Algorithms for {{geometric}} {{determination and}} analysis of photogrammetric <b>data</b> Feature <b>extraction</b> from multisensor, multiresolution, multitemporal imagery Image understanding Integrated sensor orientation Image sequence analysis Algorithms for digital photogrammetric systems and their GIS integration GIS concepts, with particular emphasis on integration of image dat...|$|R
50|$|The Middleware: The “middle man” {{between the}} Edgeware and the Centralware. Capable of {{providing}} {{functions such as}} <b>data</b> filtration, <b>extraction</b> of meaningful information, aggregation and messaging of data from the Edgeware, and distribution of {{the information to the}} proper destination/ web service accordingly.|$|R
5000|$|Development {{of a novel}} {{information}} system for data logging, <b>data</b> <b>extraction</b> and data exchange with existing related systems.|$|E
50|$|A 2017 {{data dump}} {{suggests}} Cellebrite sold its <b>data</b> <b>extraction</b> products to Turkey, the United Arab Emirates and Russia.|$|E
50|$|The data {{suggests}} Cellebrite {{sold its}} <b>data</b> <b>extraction</b> products to {{countries such as}} Turkey, the United Arab Emirates and Russia.|$|E
40|$|Microarrays allow {{measuring}} the expression {{level of a}} large number of genes under different experimental samples or environmental conditions. The data generated from them are called gene expression <b>data.</b> The <b>extraction</b> of biological relevant knowledge from this data is not a trivial task. Gene expressio...|$|R
3000|$|Data {{processing}} Make {{the necessary}} transformations of <b>data</b> using an <b>extraction,</b> transformation, and loading (ETL) techniques between different data sources.|$|R
40|$|In {{this paper}} we {{proposed}} a new approach called Extended Fivatech from the problem of multiple input pages. Extended FivaTech can reduce the schema and templates for each individual web page generated from a CGI program. It also merge the multiple input pages into the single page to generate the dynamic web pages Extended FiaTech uses tree templates. Extended FivaTech reduce the schema templates for each individual Deep web site which contains either singleton or multiple data records in one Web page and merging of web pages into a single page. Extended FivaTech applies tree merging, tree alignment and mining techniques, merging of the input pages to get the outstanding task. These experiments show a good result for the web pages used in many web <b>data</b> <b>extractions.</b> 1...|$|R
50|$|To {{achieve an}} {{effective}} data migration procedure, {{data on the}} old system is mapped to the new system utilising a design for <b>data</b> <b>extraction</b> and data loading. The design relates old data formats to the new system's formats and requirements. Programmatic data migration may involve many phases but it minimally includes <b>data</b> <b>extraction</b> where data is read from the old system and data loading where data is written to the new system.|$|E
50|$|These {{tools are}} not so much lead {{generation}} software but more <b>data</b> <b>extraction</b> tools that can assist the early stages of a lead generation process.|$|E
5000|$|... 2011 Created a tool called OWADE, meaning Offline Windows Analysis and <b>Data</b> <b>Extraction,</b> that {{bypassed}} encryption on a Windows PC's {{hard drive}} for forensics purposes.|$|E
40|$|We {{have been}} working on two {{different}} KDD systems for scientific data. One system involves comparative genomics, where the database contains more than 60, 000 plant gene and protein sequences plus results extracted from similarity searches against public sequence databases. The second system supports a several-decades long longitudinal field study of chimpanzee behavior. Both systems have components for the storing of raw data and for cleaning data before querying begins and for displaying <b>data</b> <b>extractions.</b> Both systems use a relational DBMS. In this paper we report on a) the extensions we made to the DBMS to support our analysis of the data, and b) the way that we used those extensions as, with users, we developed a thought from an initial idea to a richer analysis. We have found tha...|$|R
50|$|NI {{extracts}} and correlates {{information such}} as who contacts whom, when where and how, providing situational awareness for Lawful Interception and Cyber Security. Real-time <b>data</b> capture, <b>extraction</b> and analysis allow security specialists to take preventive measures and protect network assets in real time as a complement post-mortem analysis after an attack.|$|R
30|$|In {{the next}} {{subsections}} we provide further {{details of the}} experimental setup we followed (preliminary <b>data</b> analysis, feature <b>extraction,</b> feature selection, and model building).|$|R
