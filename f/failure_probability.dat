2123|886|Public
25|$|Another {{significant}} {{application of}} probability theory {{in everyday life}} is reliability. Many consumer products, such as automobiles and consumer electronics, use reliability theory in product design to reduce the probability of failure. <b>Failure</b> <b>probability</b> may influence a manufacturer's decisions on a product's warranty.|$|E
25|$|Cuckoo hashing, another {{technique}} for implementing hash tables, guarantees constant time per lookup (regardless of the hash function). Insertions into a cuckoo hash table may fail, causing the entire table to be rebuilt, but such failures are sufficiently {{unlikely that the}} expected time per insertion (using either a truly random hash function or a hash function with logarithmic independence) is constant. With tabulation hashing, on the other hand, the best bound known on the <b>failure</b> <b>probability</b> is higher, high enough that insertions cannot be guaranteed to take constant expected time. Nevertheless, tabulation hashing is adequate to ensure the linear-expected-time construction of a cuckoo hash table for a static set of keys that does not change as the table is used.|$|E
2500|$|By {{the upper}} bound on burst error {{detection}} (...) , we know that a cyclic code can not detect all bursts of length [...] However cyclic codes can indeed detect most bursts of length [...] The reason is that detection fails only when the burst is divisible by [...] Over binary alphabets, there exist [...] bursts of length [...] Out of those, only [...] are divisible by [...] Therefore, the detection <b>failure</b> <b>probability</b> is very small (...) assuming a uniform distribution over all bursts of length [...]|$|E
40|$|Abstract [...] System-on-Chip designs {{often have}} a large numberof timing domains. Communication between these domains {{requires}} synchronization, and the <b>failure</b> <b>probabilities</b> of thesesynchronizers must be characterized accurately to ensure the robustness of the complete system. We present a novel approachfor determining the <b>failure</b> <b>probabilities</b> of synchronizer circuits. Our approach using numerical integration {{to account for the}} non-linear behaviour of real synchronizer circuits. We complement this with small-signal techniques to enable accurate estimationof extremely small <b>failure</b> <b>probabilities.</b> Our approach is fully automated, is suitable for integration into circuit simulationtools such as SPICE and enables accurate characterization of extremely small <b>failure</b> <b>probabilities.</b> 1...|$|R
5000|$|Tasks {{along the}} {{development}} process with a success and <b>failure</b> <b>probabilities</b> ...|$|R
40|$|This report {{addresses}} the potential application of probabilistic fracture mechanics computer codes {{to support the}} Proactive Materials Degradation Assessment (PMDA) program as a method to predict component <b>failure</b> <b>probabilities.</b> The present report describes probabilistic fracture mechanics calculations that were performed for selected components using the PRO-LOCA and PRAISE computer codes. The calculations address the failure mechanisms of stress corrosion cracking, intergranular stress corrosion cracking, and fatigue for components and operating conditions that are known to make particular components susceptible to cracking. It was demonstrated that the two codes can predict essentially the same <b>failure</b> <b>probabilities</b> if both codes start with the same fracture mechanics model and the same inputs to the model. Comparisons with field experience showed that both codes predict relatively high <b>failure</b> <b>probabilities</b> for components under operating conditions that have resulted in field failures. It was found that modeling assumptions and inputs tended to give higher calculated <b>failure</b> <b>probabilities</b> than those derived from data on field failures. Sensitivity calculations were performed to show that uncertainties in the probabilistic calculations were sufficiently large to explain the differences between predicted <b>failure</b> <b>probabilities</b> and field experience...|$|R
5000|$|... #Subtitle level 3: Increasing rebuild {{time and}} <b>failure</b> <b>probability</b> ...|$|E
5000|$|<b>Failure</b> <b>probability</b> (Reliability analysis): {{approximation}} (FORM), simulation (Monte Carlo, LHS, Directional sampling) ...|$|E
5000|$|A seismic {{fragility}} {{method that}} uses an associated earthquake acceleration level {{to determine a}} components <b>failure</b> <b>probability</b> ...|$|E
5000|$|The {{summation}} {{of each of}} the <b>failure</b> path <b>probabilities</b> provided the total <b>failure</b> path <b>probability</b> (FT) ...|$|R
40|$|This paper {{presents}} a semi-analytic method for computing frequency dependent means, variances, and <b>failure</b> <b>probabilities</b> for arbitrarily large-order closed-loop dynamical systems possessing a single uncertain parameter or with multiple highly correlated uncertain parameters. The approach {{will be shown}} to not suffer from the same computational challenges associated with computing <b>failure</b> <b>probabilities</b> using conventional FORM/SORM techniques. The approach is demonstrated by computing the probabilistic frequency domain performance of an optimal feed-forward disturbance rejection scheme...|$|R
40|$|In-service {{inspections}} (ISI) {{of pipes}} {{in the nuclear}} power plants are currently performed based on mandated requirements in the ASME Section XI, {{which is based on}} deterministic approach of the critical welds. The 20 years of ISI experience in U. S. A. has revealed less correlation between the critical welds and actual failures, and much conservatism in current ISI requirements. To reduce those problems, risk-informed ISI technology has been developed and proved to be useful. This paper presented a method for predicting piping <b>failure</b> <b>probabilities</b> in an application of risk-informed ISI, and analyzed the effect of input parameters on piping <b>failure</b> <b>probabilities.</b> Results generated using this approach revealed that the calculated <b>failure</b> <b>probabilities</b> can be sensitive to the different types of stressors, crack size distribution, inspection interval, etc [...] 1...|$|R
50|$|Obtain event failure probabilities: If the <b>failure</b> <b>probability</b> {{can not be}} {{obtained}} use fault tree analysis to calculate it.|$|E
5000|$|A {{common cause}} module to {{determine}} a group common cause <b>failure</b> <b>probability</b> for groups of up to six redundant components ...|$|E
5000|$|... “... {{inherits}} all mathematical {{characteristics and}} properties of a <b>failure</b> <b>probability</b> except that we {{allow it to}} be larger than 1...” ...|$|E
40|$|The {{goal of the}} Machine Learning and Traveling Repairman Problem (ML&TRP) is to {{determine}} a route for a “repair crew, ” which repairs nodes on a graph. The repair crew aims to minimize the cost of failures at the nodes, but as in many real situations, the <b>failure</b> <b>probabilities</b> are not known and must be estimated. We introduce two formulations for the ML&TRP, where the first formulation is sequential: <b>failure</b> <b>probabilities</b> are estimated at each node, and then a weighted version of the traveling repairman problem is used to construct the route from the failure cost. We develop two models for the failure cost, based on whether repeat failures are considered, or only the first failure on a node. Our second formulation is a multi-objective learning problem for ranking on graphs. Here, we are estimating <b>failure</b> <b>probabilities</b> simultaneously with determining the graph traversal route; the choice of route influences the estimated <b>failure</b> <b>probabilities.</b> This is in accordance with a prior belief that probabilities that cannot be well-estimated will generally be low. It also agrees with a managerial goal of finding a scenario where the data can plausibly support choosing a route that has a low operational cost. 1...|$|R
3000|$|... {{participate}} in the retransmission, the average <b>failure</b> <b>probabilities</b> that the retransmission is unsuccessful in an idle and busy timeslot can be computed as [...]...|$|R
40|$|Landing safety {{assessment}} {{is an integral}} element of the planning of a landing mission. This thesis contributes to such assessment with the modelling and deduction of the functional limits of a legged landing system and its terrain-related <b>failure</b> <b>probabilities.</b> A mathematical method has been developed to determine these terrain-related <b>failure</b> <b>probabilities.</b> The lander's touchdown dynamics is represented by a high-fidelity numerical multibody simulation which is validated by experimental data from a dedicated test campaign. The analysis of the terrain-related <b>failure</b> <b>probabilities</b> remains incomplete without knowledge about the geotechnical properties of the landing site. This information is obtained from a landing site characterization. An analysis step extracts this information from high resolution digital terrain models under consideration of the specific baselength determined by the landing platform's footprint. A robotic lunar landing mission is used as application case study...|$|R
5000|$|... where [...] is the <b>failure</b> <b>probability</b> density {{function}} and [...] is {{the length of}} the period of time (which is assumed to start from time zero).|$|E
5000|$|... samples, where [...] is the {{learning}} error and [...] is the <b>failure</b> <b>probability.</b> Thus, the sample-complexity is a linear {{function of the}} VC dimension of the hypothesis space.|$|E
50|$|For Monte Carlo {{decision}} algorithms with two-sided error, the <b>failure</b> <b>probability</b> may {{again be}} reduced by running the algorithm k times and returning the majority function of the answers.|$|E
40|$|System-on-Chip designs {{often have}} {{a large number of}} timing domains. Communication between these domains {{requires}} synchronization, and the <b>failure</b> <b>probabilities</b> of these synchronizers must be characterized accurately to ensure the robustness of the complete system. We present a novel approach for determining the <b>failure</b> <b>probabilities</b> of synchronizer circuits. We use numerical intergration to perform large-signal analysis that accounts for the non-linear behaviour of real synchronizer circuits. We complement this with small-signal techniques to characterize behaviours near the metastable equilibrium. This combination overcomes the limitations of traditional techniques: the large-signal analysis accounts for the transfer of metastable behaviour between synchronizer stages; and the small-signal techniques overcome the limitations of numerical accuracy inherent in pure simulation approaches. Our approach is fully automated, is suitable for integration into circuit simulation tools such as SPICE, and enables accurate characterization of extremely small <b>failure</b> <b>probabilities...</b>|$|R
40|$|The {{paper is}} devoted to the {{estimation}} of small <b>failure</b> <b>probabilities,</b> i. e. on the reliability analysis, using the non-parametric model of the large-scale structure. Two major questions are addressed in this respect: (i) What are the effects of the model uncertainties on the prediction of very small <b>failure</b> <b>probabilities,</b> i. e. on the tails of the response distribution? This question will be answered by studying the differences in the <b>failure</b> <b>probabilities</b> predicted by the non-parametric approach and by the parametric approach, respectively. (ii) The non-parametric model of uncertainties features {{a very large number of}} random variables, thus leading to a high-dimensional reliability problem [3]. Due to the non-linear nature of the response, with respect to the space of the input random variables, this problem is very challenging. The performance of methods applicable to igh-dimensional reliability problems (Line Sampling, Subset Simulation) is studied...|$|R
40|$|The paper {{summarizes}} the main {{findings of the}} joint project between JRC and Swedis partners to determine <b>failure</b> <b>probabilities</b> for copper cast iron inserts for disposal of spent nuclear fuel. The paper describes the statistical test programme and it key results, how these data are used in a probabilistic analysis for canister failure. It is shown that <b>failure</b> <b>probabilities</b> are extremely low. The results of the paper {{can be used to}} derive acceptance criteria for defects, material properties and geometrical design parameters. JRC. F. 4 -Nuclear design safet...|$|R
50|$|Therefore, {{under the}} {{condition}} that all of a task’s sub-tasks are fully represented within a HRAET, and the <b>failure</b> <b>probability</b> for each sub-task is known, this makes it possible to calculate the final reliability for the task.|$|E
50|$|Another {{significant}} {{application of}} probability theory {{in everyday life}} is reliability. Many consumer products, such as automobiles and consumer electronics, use reliability theory in product design to reduce the probability of failure. <b>Failure</b> <b>probability</b> may influence a manufacturer's decisions on a product's warranty.|$|E
5000|$|Events in a {{fault tree}} are {{associated}} with statistical probabilities. For example, component failures may typically occur at some constant failure rate λ (a constant hazard function). In this simplest case, <b>failure</b> <b>probability</b> depends on the rate λ and the exposure time t: ...|$|E
40|$|Quorum systems {{serve as}} a basic tool {{providing}} a uniform and reliable way to achieve coordination in a distributed system. They are useful for distributed and replicated databases, name servers, mutual exclusion, and distributed access control and signatures. The un-availability of a quorum system is the probability of the event that no live quorum exists in the system. When such an event occurs the service is completely halted. The un-availability is widely accepted as the measure by which quorum systems are evaluated. In this paper we characterize the optimal availability quorum system in the general case, when the <b>failure</b> <b>probabilities</b> may take any value in the range 0 ! p i ! 1. Then {{we deal with the}} practical scenario in which the <b>failure</b> <b>probabilities</b> are unknown, but can be estimated. We give a robust and efficient algorithm that calculates a near optimal quorum system based on the estimated <b>failure</b> <b>probabilities.</b> Keywords: Quorum systems, distributed computing, fault tolerance [...] ...|$|R
5000|$|Intuitively, {{both these}} {{formulae}} {{can be explained}} {{from the point of}} view of <b>failure</b> <b>probabilities.</b> First of all, let's note that the probability of a system failing within a certain timeframe is the inverse of its MTBF. Then, when considering series of components, failure of any component leads to the failure of the whole system, so (assuming that <b>failure</b> <b>probabilities</b> are small, which is usually the case) <b>probability</b> of the <b>failure</b> of the whole system within a given interval can be approximated as a sum of <b>failure</b> <b>probabilities</b> of the components. With parallel components the situation is a bit more complicated: the whole system will fail if and only if after one of the components fails, the other component fails while the first component is being repaired; this is where MDT comes into play: the faster the first component is repaired, the less is the [...] "vulnerability window" [...] for the other component to fail.|$|R
40|$|International audienceIn {{reliability}} analysis, comparing {{system reliability}} {{is an essential}} task when designing safe systems. When the <b>failure</b> <b>probabilities</b> of the system components (assumed to be independent) are precisely known, this task is relatively simple to achieve, as system reliabilities are precise numbers. When <b>failure</b> <b>probabilities</b> are ill-known (known to lie in an interval) {{and we want to}} have guaranteed comparisons (i. e., declare a system more reliable than another when it is for any possible probability value), there are different ways to compare system reliabilities. We explore the computational problems posed by such extensions, providing first insights about their pros and cons...|$|R
50|$|HALT {{is a test}} {{technique}} called test-to-fail, where a product is tested until failure. HALT does not help to determine or demonstrate the reliability value or <b>failure</b> <b>probability</b> in field. Many accelerated life tests are test-to-pass, meaning {{they are used to}} demonstrate the product life or reliability.|$|E
5000|$|Note {{that the}} {{confidence}} parameter [...] {{does not appear}} in the definition of learning. This is because the main purpose of [...] is to allow the learning algorithm a small probability of failure due to an unrepresentative sample. Since now [...] always guarantees to meet the approximation criterion , the <b>failure</b> <b>probability</b> is no longer needed.|$|E
5000|$|The key is {{the left}} {{tail of the}} {{distribution}} of [...] It was not successfully identiﬁed until Weibull in 1939 recognized that the tail is a power law. Denoting the tail exponent as , one can then show that, if the structure is sufficiently larger than one RVE (i.e., if [...] ), the <b>failure</b> <b>probability</b> of a structure as a function of [...] is ...|$|E
40|$|ABSTRACT: The Delphi {{technique}} via {{the expert}} elicitation method becomes extremely handy particularly {{in view of}} limited availability of data in determining <b>failure</b> <b>probabilities</b> of onshore transmission pipelines in the Niger Delta region of Nigeria occasioned by third party activity. Using, ten (10) experts opinion elucidated individually via email questionnaires and summarizing their responses in linguistic languages expression that were converted into <b>failure</b> <b>probabilities</b> for twelve (12) identified third part activities in the Niger Delta region of Nigeria using Fuzzy Set Theory tools. The {{results show that the}} neglect by government has the highest <b>probability</b> of <b>failure</b> of 0. 1698200...|$|R
40|$|AbstractIncidents {{and rolling}} stock {{breakdowns}} are commonplace in rapid transit rail systems and may disrupt the system performance imposing deviations from planned operations. A network design model is proposed {{for reducing the}} effect of disruptions less likely to occur. <b>Failure</b> <b>probabilities</b> are considered functions {{of the amount of}} services and the rolling stock's routing on the designed network so that they cannot be calculated a priori but result from the design process itself. A two recourse stochastic programming model is formulated where the <b>failure</b> <b>probabilities</b> are an implicit function of the number of services and routing of the transit lines...|$|R
40|$|Financial {{constraints}} {{have been}} found {{to play an important role}} on various aspects of firm behavior. Yet, their effects on firm survival have been largely neglected. We use a panel of 61, 496 UK firms over the period 1997 – 2002 to study the effects of financial variables on firms' <b>failure</b> <b>probabilities,</b> differentiating firms into globally engaged and purely domestic. Estimating a wide range of specifications, we find that lower collateral and higher leverage result in higher <b>failure</b> <b>probabilities</b> for purely domestic than for globally engaged firms. This can be seen as evidence that global engagement shields firms from financial constraints...|$|R
