26|51|Public
50|$|CANopen {{devices are}} {{required}} {{to make the transition}} from the state Initializing to Pre-operational automatically during bootup. When this transition is made, a single <b>heartbeat</b> <b>message</b> is sent to the bus. This is the bootup protocol.|$|E
50|$|All devices send a <b>{{heartbeat}}</b> <b>message</b> to the hub on {{a regular}} basis (typically 5 minutes).When disconnecting, they also can send a special heartbeat end message for the hub to radiate them out of his list.|$|E
50|$|A BSP Master and {{multiple}} grooms are {{started by the}} script. Then, the bsp master starts up with a RPC server for groom servers. Groom servers starts up with a BSPPeer instance and a RPC proxy to contact the bsp master. After started, each groom periodically sends a <b>heartbeat</b> <b>message</b> that encloses its groom server status, including maximum task capacity, unused memory, and so on.|$|E
50|$|A device {{chooses a}} free UDP port and sends <b>heartbeat</b> <b>messages</b> from that port to the hub on theIANA {{registered}} UDP port 3865.|$|R
40|$|A dummy traffic {{strategy}} is described {{that can be}} implemented by mix nodes in an anonymous communication net-work to detect and counter active (n - 1) attacks and their variants. <b>Heartbeat</b> <b>messages</b> are sent anonymously from the mix node back to itself {{in order to establish}} its state of connectivity {{with the rest of the}} network. In case the mix is under attack, the flow of <b>heartbeat</b> <b>messages</b> is interrupted and the mix takes measures to preserve the quality of the anonymity it provides by introducing decoy messages...|$|R
30|$|The <b>heartbeat</b> <b>{{messages}}</b> used in SHAM are {{the same}} as alive messages that are communicated to detect node’s failure in other systems, i.e., PASTRY [15]. That is, a <b>heartbeat</b> or alive <b>message</b> is a simple ping command from one peer to another peer in the overlay. Thus, such “soft” messages are not considered as an overhead for the traffic of the overlay as they might traverse through nodes that are not in the overlay.|$|R
50|$|Each {{time the}} bsp master {{receives}} a <b>heartbeat</b> <b>message,</b> it brings up-to-date groom server status - the bsp master {{makes use of}} groom servers' status in order to effectively assign tasks to idle groom servers - and returns a heartbeat response that contains assigned tasks and others actions that a groom server has to do. For now, we have a FIFO job scheduler and very simple task assignment algorithms.|$|E
5000|$|A <b>heartbeat</b> <b>message</b> in signal {{processing}} {{is a message}} sent from an originator to a destination that enables the destination to identify {{if and when the}} originator fails or is no longer available. [...] Heartbeat messages are typically sent non-stop on a periodic or recurring basis from the originator's start-up until the originator's shutdown. When the destination identifies a lack of heartbeat messages during an anticipated arrival period, the destination may determine that the originator has failed, shutdown, or is generally no longer available. Heartbeat messages may be used for high-availability and fault tolerance purposes.|$|E
5000|$|Raft {{achieves}} consensus via {{an elected}} leader. A server in a raft cluster {{is either a}} leader or a follower, and can be a candidate in the precise case of an election (leader unavailable) [...] The leader is responsible for log replication to the followers. It regularly informs the followers of its existence by sending a <b>heartbeat</b> <b>message.</b> Each follower has a timeout (typically between 150 and 300 ms) in which it expects the heartbeat from the leader. The timeout is reset on receiving the heartbeat. If no heartbeat is received the follower changes its status to candidate and starts a leader election.|$|E
3000|$|... =min(30, 59.99). In Step 2, {{the input}} values are {{processed}} initializing η equal to 30.00. After that, the algorithm continuously reduces {{the value of}} η until f(η)≥ 432, 000. The final value is η= 14.6 s, which is employed by monitored processes to periodically send <b>heartbeat</b> <b>messages</b> to meet the QoS requirements of App [...]...|$|R
30|$|In push model, the {{components}} of the grid starts sending <b>heartbeat</b> <b>messages</b> at regular time intervals to a central failure detector. If failure detector does not receive a message from one or more grid components within a specified time, then failure detector assumes and considers the problem as a failure of that component (Garg and Singh 2011).|$|R
50|$|Kubelet {{monitors}} {{the state of}} a pod and if not in the desired state, the pod will be redeployed to the same node. The node status is relayed every few seconds via <b>heartbeat</b> <b>messages</b> to the master. Once the master detects a node failure, the Replication Controller observes this state change and launches pods on other healthy nodes.|$|R
30|$|If f is {{less than}} θfL, a <b>heartbeat</b> <b>message</b> request is sent by the LMA to the most {{overloaded}} HMAGs to inform them to perform a load balancing action. The LMA uses the received loads from all the related HMAGs to determine the most overloaded HMAGs. A new flag is also added to the <b>heartbeat</b> <b>message</b> is named F and is set to 1 if the <b>heartbeat</b> <b>message</b> comes from the LMA entity to the related HMAGs or zero if the <b>heartbeat</b> <b>message</b> comes from the HMAG to the related MAGs.|$|E
30|$|The timeout {{interval}} τ is thus adjusted {{after each}} <b>heartbeat</b> <b>message</b> arrives.|$|E
30|$|Once the {{overloaded}} HMAG {{receives the}} <b>heartbeat</b> <b>message,</b> a load balancing action is performed {{by sending a}} <b>heartbeat</b> <b>message</b> to the related MAGs, which in turn selects the HMNs in the overlapped area. The HMNs must change their point of attachment to another MAG. The criteria of the HMN selection {{are discussed in the}} next subsection.|$|E
50|$|Raft uses {{randomized}} election timeout {{to ensure}} that split votes problem are resolved quickly. This should reduce {{the chance of a}} split vote because servers won't become candidates at the same time : a single server will timeout, win the election, then become leader and sends <b>heartbeat</b> <b>messages</b> to other servers before any of the followers can become candidates.|$|R
5000|$|Detecting a node's {{departure}} can be done, for instance, via <b>heartbeat</b> <b>messages</b> that periodically broadcast {{routing table}} information between neighbors. After a predetermined period of silence from a neighbor, that neighboring node is determined as failed {{and is considered}} a departing node. [...] Alternatively, a node that is willingly departing may broadcast such a notice to its neighbors.|$|R
3000|$|... [...]) is {{responsible}} for sending <b>heartbeat</b> <b>messages.</b> It is executed on a host that is monitored. In order {{to communicate with the}} Monitor, the following objects are maintained: Monitor IP address (ipMonitor), Monitor port (portMonitor), monitored process OID (myOidInMonitor, an Object Identifier is a name used to identify the monitored process in the MIB), heartbeat interval (frequencyHB), and the process identifier in the local host (processID).|$|R
30|$|Subsequently, the HMAG k {{uses the}} MAGs load information, which {{is stored in}} its policy {{database}} to compute the f value and the compares it with θf. If f <θf, the HMAG k will send a <b>heartbeat</b> <b>message</b> to its related MAGs to inform the most overloaded MAGs to perform a load balancing action. In this work, the extended field (F flag and load status field) in the <b>heartbeat</b> <b>message</b> that is given by [13] is reused in the same way.|$|E
30|$|Once the HMAG {{receives}} the total loads pt from each related MAGs, the HMAG send these loads to the related LMA periodically using the <b>heartbeat</b> <b>message.</b>|$|E
30|$|Another {{object of}} the appNotifyGroup is receiveHB (see Fig. 4) {{which is used to}} receive {{heartbeat}} messages from a monitored process. Every time a <b>heartbeat</b> <b>message</b> arrives, fdMIB updates the corresponding process state object in the monitorHostGroup table.|$|E
5|$|The {{problem can}} be fixed by {{ignoring}} <b>Heartbeat</b> Request <b>messages</b> that ask for more data than their payload needs.|$|R
30|$|In the LB-CPMIPv 6, the HMN {{that has}} a {{real-time}} session will not be selected {{during the process of}} the load balancing; this restriction relieves the critical applications from service disruption. Furthermore, the CSPMIPv 6 handover signaling has been extended to be adapted with the newly proposed load balancing mechanism. Moreover, the LPBA, PBA, and the <b>heartbeat</b> <b>messages</b> are modified to enable sharing of the domain number for the new load balancing mechanism.|$|R
40|$|Abstract—Untraceability of {{vehicles}} {{is an important}} requirement in future vehicle communications systems. Unfortunately, <b>heartbeat</b> <b>messages</b> used by many safety applications provide {{a constant stream of}} location data, and without any protection measures, they make tracking {{of vehicles}} easy even for a passive eavesdropper. One commonly known solution is to transmit heartbeats under pseudonyms that are changed regularly in order to obfuscate the trajectory of vehicles. However, this approach is effective only if some silent period is kept during the pseudonym change and several vehicles change their pseudonyms nearly {{at the same time and}} at the same location. Unlike previous works that proposed explicit synchronization between a group of vehicles and/or required pseudonym change in a designated physical area (i. e., a static mix zone), we propose a much simpler approach that does not need any explicit cooperation between vehicles and any infrastructure support. Our basic idea is that vehicles should not transmit <b>heartbeat</b> <b>messages</b> when their speed drops below a given threshold, say 30 km/h, and they should change pseudonym during each such silent period. This ensures that vehicles stopping at traffic lights or moving slowly in a traffic jam will all refrain from transmitting heartbeats and change their pseudonyms nearly at the same time and location. Thus, our scheme ensures both silent periods and synchronized pseudonym change in time and space, but it does so in an implicit way. We also argue that the risk of a fatal accident at a slow speed is low, and therefore, our scheme does not seriously impact safetyof-life. In addition, refraining from sending <b>heartbeat</b> <b>messages</b> when moving at low speed also relieves vehicles of the burden of verifying a potentially large amount of digital signatures, and thus, makes it possible to implement vehicle communications with less expensive equipments. I...|$|R
30|$|Finally, every HMAG in the CSPMIPv 6 domain has to send its load that is {{received}} from the related MAGs to the LMA. This is performed using the <b>heartbeat</b> <b>message</b> to compute the overall system performance and operates as follows.|$|E
30|$|Initially, every {{application}} {{that needs to}} obtain information about the states of monitored processes configures the quality of service (QoS) it expects from IFDS. Then, the detection service computes the heartbeat interval (η) that must be applied {{in order to provide}} the required QoS. The Monitor Host then configures η on every Monitored Host, so that periodically at an η interval, the monitored host will send back a <b>heartbeat</b> <b>message</b> to the Monitor Host.|$|E
30|$|In {{the case}} of a node’s departure, the system needs to be {{informed}} and updated as well. However, due to the dynamism of P 2 P networks, nodes tend to leave ungracefully without informing the system. Nodes in SHAM rely on system maintenance messages to keep their routing tables updated. However, in the absence of such messages, SHAM nodes depend on the timeout counters and self-initiated heartbeat messages to check the availability of their neighbors. Each neighbor receives the <b>heartbeat</b> <b>message</b> responds with an alive message and resets the timeout counter of the neighbor that sent it.|$|E
3000|$|... [...]), which {{corresponds}} to a lower bound on the probability that <b>heartbeat</b> <b>messages</b> are received before the timeout interval expires. These parameters are used by a configurator which relies on another system called Adaptare that is a middleware that computes the timeout by estimating distributions based on the stochastic properties of the system on which the failure detector is running. The system was evaluated and presented sound results, {{especially in terms of}} the average mistake recurrence time and coverage.|$|R
40|$|Abstract – In {{this paper}} the {{medium access control}} (MAC) me-thod of the {{upcoming}} vehicular communication standard IEEE 802. 11 p has been simulated in a highway scenario with periodic broadcast of time-critical packets (so-called <b>heartbeat</b> <b>messages)</b> in a vehicle-to-vehicle situation. The 802. 11 p MAC method is based on {{carrier sense multiple access}} (CSMA) where nodes lis-ten to the wireless channel before sending. If the channel is busy, the node must defer its access and during high utilization periods this could lead to unbounded delays. This well-known property of CSMA is undesirable for time-critical communications. The simulation results reveal that a specific node/vehicle is forced to drop over 80 % of its <b>heartbeat</b> <b>messages</b> because no channel ac-cess was possible before the next message was generated. To overcome this problem, we propose to use self-organizing time division multiple access (STDMA) for real-time data traffic be-tween vehicles. This MAC method is already successfully applied in commercial surveillance applications for ships (AIS) and air-planes (VDL mode 4). Our initial results indicate that STDMA outperforms CSMA for time-critical traffic safety applications in ad hoc vehicular networks. I...|$|R
3000|$|... [...]) were {{proposed}} {{to compute the}} interval (η) on which monitored processes send heartbeats. IFDS was implemented using SNMP and Web Services for enabling communication among applications across the Internet. Experimental results were presented, in which the failure detector service ran both on a single LAN and on PlanetLab. In this case, monitored processes run on hosts of five continents. The results show {{the effectiveness of the}} adaptive timeout with different intervals of <b>heartbeat</b> <b>messages.</b> On the one hand, the η [...]...|$|R
40|$|Abstract — This paper proposes {{an on-line}} two phase fault {{diagnosis}} algorithm for arbitrary connected networks. The algorithm addresses a realistic fault model considering crash and value faults in the nodes. Fault diagnosis {{is achieved by}} comparing the <b>heartbeat</b> <b>message</b> generated by neighboring nodes and dissemination of decision made at each node. Theoretical analysis shows that time and message complexity of the diagnosis scheme is O(n) for a n-node network. The message and time complexity are comparable to the existing state of art approaches and thus well suited for design of different fault tolerant wireless communication networks [...] Index Terms — On-line diagnosis, two phase diagnosis, value faults, dynamic fault environment...|$|E
30|$|Failures {{are mostly}} {{inevitable}} when Hadoop runs at large scales. Consequently, Hadoop is {{designed as a}} fault-tolerant framework that can handle various failures with minimum impact {{on the quality of}} service. There are three different failure modes, task failure, TaskTracker failure, and JobTracker failure. When the TaskTracker detects a task failure, it will mark the task attempt as failed, free the task slot on which the task is running, and notify the JobTracker of the failure in its <b>heartbeat</b> <b>message.</b> The JobTracker will then try to reschedule execution of that task on a different TaskTracker. The whole job will fail, if any task fails a configurable number of times (four times by default), which usually means the user code is buggy. TaskTracker failure occurs, when the JobTracker hasn’t received any <b>heartbeat</b> <b>message</b> from certain TaskTracker for a configurable period of time (10  minutes by default). TaskTracker failure is a much more serious failure mode than task failure, because the intermediate output of all map tasks that previously ran and finished on the failed TaskTracker becomes inaccessible. In this case, the JobTracker will rerun all those completed map tasks, and reschedule any tasks in progress on other TaskTrackers. JobTracker failure is the most serious failure mode, but it is not likely to happen as the chance that a particular machine fails is low. In the case of JobTracker failure, Hadoop provides a configuration option that can attempt to recover all jobs that were running at the time the failure occurred.|$|E
40|$|Topology {{management}} schemes conserve {{energy in}} wireless ad hoc networks by identifying redundant nodes that may turn off their radios or other components while maintaining connectivity. We present Naps, a randomized topology management scheme {{that does not}} rely on geographic location information, provides flexibility in the target density of waking nodes, and sends only a periodic <b>heartbeat</b> <b>message</b> between waking neighbors; thus it is implementable even on modest hardware. We formally analyze the connectivity of the waking graphs produced by Naps, showing that these graphs have nearly complete connectivity even at relatively low densities. We examine simulation results {{for a wide range}} of initial deployment densities and for heterogeneous and mobile deployments...|$|E
40|$|Failure {{detection}} is at {{the core}} of most fault tolerance strategies, but it often depends on reliable communication. We present new algorithms for failure detectors which are appropriate as components of a fault tolerance system that can be deployed in situations of adverse network conditions (such as loosely connected and administered computing grids). It packs redundancy into <b>heartbeat</b> <b>messages,</b> thereby improving on the robustness of the traditional protocols. Results from experimental tests conducted in a simulated environment with adverse network conditions show significant improvement over existing solutions...|$|R
30|$|The Monitor Host Group (monitorHostGroup) stores {{information}} about monitored processes, including IP addresses (ipAddr), port numbers (portNumber), process IDs (processID), state (statusHost), number of false detections (falseDetection), heartbeat interval required (reqFreq), an {{estimation of the}} probabilistic behavior of message delays (V_D), message loss probability (P_L), QoS parameters (current_TD, current_TM and current_TMR) these parameters are continuously updated and are used to check whether any QoS requirement has been broken (e.g., current_TD >TD_U), among others. Objects of the monitorHostGroup trigger and execute process monitoring. For instance, they can start threads that receive <b>heartbeat</b> <b>messages</b> and compute timeouts.|$|R
40|$|We {{present a}} {{replication}} control protocol for distributed file systems that can guarantee strict consistency or sequential consistency while imposing no performance overhead for normal reads. The protocol uses a primary-copy scheme with server redirection when concurrent writes occur. It tolerates {{any number of}} component omission and performance failures, even when these lead to network partition. Failure detection and recovery are driven by client accesses. No <b>heartbeat</b> <b>messages</b> or expensive group communication services are required. We have implemented the protocol in NFSv 4, the emerging Internet standard for distributed filing...|$|R
