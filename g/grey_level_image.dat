58|4105|Public
5000|$|To {{measure the}} {{complexity}} of an image region [...] around point [...] with shape ,a descriptor [...] that takes on values (e.g., in an 8 bit <b>grey</b> <b>level</b> <b>image,</b> D would range from 0 to 255 for each pixel) is definedso that , the probability of descriptor value occurs in region [...] can be computed.Further, the entropy of image region [...] can compute asUsing this entropy equation we can further calculate [...] for every point and region shape [...] A more complex region, like the eye region, has a more complex distributor and hence higher entropy.|$|E
40|$|We {{present an}} {{automated}} {{system for the}} analysis of edge based structure for use in morphomet-ric studies. The current work takes a <b>grey</b> <b>level</b> <b>image</b> of a Drosophila wing as input and extracts the coordinates of 15 landmarks. The proposed method extracts the ridges (linear features such as wing veins) using the knowledge of their known grey level profile and the noise character...|$|E
40|$|The {{problems}} of character recognition are today mainly due to imperfect thresholding and segmentation. In this paper {{a new approach}} to text recognition is presented which attempts to avoid these problems by working directly on grey level images and treating an entire word at the time. The features are found from the grey levels of the image, and a hidden Markov model is defined for each character. During recognition the most probable combination of models is found for each word by the use of dynamic programming. 1 Introduction One of the remaining problems in character recognition, is segmentation. Both segmentation of text from background in the <b>grey</b> <b>level</b> <b>image,</b> and segmentation of the constituents of each character in the binary image, is a problem. In this paper we present a method which attempts at avoiding these segmentation problems by working directly on the <b>grey</b> <b>level</b> <b>image</b> and perform recognition word by word instead of character by character. Little work has been done in [...] ...|$|E
30|$|The ILF {{method is}} based on {{constructing}} from the analyzed 2 -D <b>grey</b> <b>level</b> <b>images</b> two 1 -D sequences that are called landscapes.|$|R
40|$|Abstract: The {{automatic}} binarization of <b>grey</b> <b>level</b> <b>images</b> or {{the automatic}} determination of optimum threshold value that separates objects from their background {{is still a}} difficult and challenging problem in many applications {{in the area of}} digital image processing. A new algorithm is proposed for the optimum automatic thresholding of digital images. The algorithm is based on determining the best threshold value that maximizes a correlation function between the binary and original <b>grey</b> <b>level</b> <b>images.</b> Experimental results to compare the proposed algorithm to various thresholding techniques are also presented. 1...|$|R
30|$|In particular, several {{researchers}} {{have investigated the}} relationship between fuzzy sets and DS evidence theory. Most analytic fuzzy approaches are derived from Bezdek's FCM algorithm applied to the <b>grey</b> <b>level</b> <b>images</b> to automatically determine the membership degree of each pixel.|$|R
40|$|In our {{algorithm}} the gradient of 8 (the gra-dient direction) in a <b>grey</b> <b>level</b> <b>image</b> {{is employed}} for detecting candidates for corner points. Two theorems {{show that this}} quantity attains a local maximum at a corner point. Two approaches, median filtering and Hough transform are used effectively to distinguish corner points from noise points. The corner points formed by our algorithm are used in interframe matching...|$|E
40|$|Two simple {{algorithms}} {{are described}} for displaying orthographic projections of surfaces. The first, called RELIEF-PLOT, produces a three-dimensional plot of a surface z = f(x,y). The second, called SHADED-IMAGE, adds information about surface reflectivity and source illumination {{to produce a}} <b>grey</b> <b>level</b> <b>image</b> of a surface z- f(x,y). Both algorithms demonstrate how a systematic profile expansion {{can be used to}} do hidden surface elimination essentially for free...|$|E
40|$|Abstract. The main {{aim of this}} {{contribution}} is comparison of method for evaluation of nonwovens surface uniformity based on the data {{in the form of}} rectangular arrays (quadrat method). These data can be obtained from digital images where the variation of mass is characterizes by the variation of <b>grey</b> <b>level</b> <b>image.</b> The evaluation of uniformity is based on the variation coefficient model, ANOVA model and spatial descriptors of irregularit...|$|E
40|$|For robust feature {{tracking}} on {{a mobile}} robot moving with about 0. 5 m/s {{in an office}} environment, {{it is necessary to}} keep up with a frame rate of at least 10 Hz. For this case we introduce an unpublished method for real-time line segment extraction from <b>grey</b> <b>level</b> <b>images</b> by contour tracing. A special emphasis is laid on efficient algorithms for edge finding, contour tracing and symbolizing, to achieve the demanded frame rate for 512 x 512 8 bit <b>grey</b> <b>level</b> <b>images</b> on a single processor system. All algorithms are geared to real time operation, avoiding recursions, enabling to priorize the processing order and aborting in case the deadline has been reached. ...|$|R
40|$|Abstract. The {{problems}} of character recognition are today mainly due to imperfect thresholding and segmentation. In this paper {{a new approach}} to text recognition is presented which attempts to avoid these problems by working directly on <b>grey</b> <b>level</b> <b>images</b> and treating an entire word at the time. The features are found from the <b>grey</b> <b>levels</b> of the <b>image,</b> and a hidden Markov model is de ned for each character. During recognition the most probable combination of models is found for each word by the use of dynamic programming. ...|$|R
40|$|Real time object {{tracking}} is {{a problem}} which involves extraction and processingofcriticalinformation from complex and uncertain image data in a short time. In this study we present a global based approach for object tracking in real <b>images.</b> Knowing <b>grey</b> <b>level</b> <b>images</b> between target and estimated region containing the tracked objects we employe and artificial neural network (ANN) ...|$|R
40|$|In this paper, {{we present}} a novel {{algorithm}} {{which is based on}} the concept of free angle for the purpose of texture region boundary identification. The input to the algorithm is a <b>grey</b> <b>level</b> <b>image.</b> The output is a binary edge map marking the closed contour boundaries of the textured regions. The algorithm has been tested on several remotely sensed images and gives very good and robust results. ...|$|E
40|$|In this study, an {{automated}} system for feature recognition in digital images is presented. A fully automated system to extract morphometric landmarks in digital images of Drosophila {{is very important}} as it will replace the most time consuming and labour intensive process of the manual identification. The current work takes a <b>grey</b> <b>level</b> <b>image</b> of Drosophila wing as input and extracts the coordinates of 15 landmarks. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
40|$|We {{present some}} new {{techniques}} for shape approximation with fractals, using iterated function system, a powerful method which allows good {{control on the}} resulting fractal. The main point discussed here can be stated as follows : given a <b>grey</b> <b>level</b> <b>image</b> A, find a few number of functions and associated probabilities that approximately generate A. Two directions have been explored : the first uses a gradient method, thus {{it was necessary to}} define a smooth error function; the second one is based upon the ideas of simulated annealing. We then generalize the methods to a broader class of functions, and present some results...|$|E
40|$|An {{algorithm}} is described which will form an edge image by detecting {{the edges of}} features in a particular spectral band of a digital satellite image. It is capable also of forming composite multispectral edge images. In addition, an edge image correlation {{algorithm is}} presented which performs rapid automatic registration of the edge images and, consequently, the <b>grey</b> <b>level</b> <b>images...</b>|$|R
30|$|Two {{important}} {{issues in the}} evaluation function are the model and the features. The model can be either user independent as explained {{in the previous section}} or user specific. This we will discuss in the first paragraph below. As features, we propose edge <b>images,</b> instead of <b>grey</b> <b>level</b> <b>images,</b> which reduce the number of local minima in the evaluation. This will be explained in the second paragraph.|$|R
40|$|Fractal {{image coding}} {{has been used}} {{successfully}} to encode digital <b>grey</b> <b>level</b> <b>images.</b> Especially at very low bitrates fractal coders perform better than cosine-transform-based JPEG coders. A block-based fractal image coder is able to exploit the redundancy of <b>grey</b> <b>level</b> <b>images</b> by describing image blocks through contractively transformed blocks of the same image. Previous fractal coders used affine linear transformations in combination with 1 st order luminance transformations that change the brightness and scale the luminance values of image blocks. We propose an extension to high order luminance transformations that operate in the frequency domain. With this transformation and an adaptive coding scheme a better approximation of image blocks can be achieved. Bitrate reductions are higher than those achieved by "spatial-domain " fractal coding schemes. An additional effect of this new transformation is a better convergence at the decoder. I. INTRODUCTION AND OVERVIEW The principle of fractal image coding consists in finding a construction rule that produces a fractal image which approximates the original image. Redundancy reduction is achieved by describing the original image through contracted parts of the same image (self-transformability) ...|$|R
40|$|We {{describe}} a speechreading (lipreading) system purely based on visual features extracted from <b>grey</b> <b>level</b> <b>image</b> sequences of the speaker's lips. Active shape models {{are used to}} track the lip contours while visual speech information is extracted from {{the shape of the}} contours. The distribution and temporal dependencies of the shape features are modelled by continuous density Hidden Markov Models. Experiments are reported for speaker independent recognition tests of isolated digits. The analysis of individual feature components suggests that speech relevant information is embedded in a low dimensional space and fairly robust to inter- and intraspeaker variability...|$|E
40|$|An {{adaptive}} nonlinear filtering {{technique has}} been recently developed {{on the basis}} of the integration between fuzzy and topological measures, for the filtering of a selected object in a <b>grey</b> <b>level</b> <b>image.</b> The extension of this approach to the filtering of the whole image is presented, with the latest obtained results. The performance of the proposed technique has been evaluated both from a qualitative point of view and from quantitative evaluation using standard error measures, with respect to different kinds of noise. The proposed methodology potentiates the previous works, overcoming some of their drawbacks and improving the performanc...|$|E
40|$|DNA {{microarray}} {{image analysis}} {{is an important}} topic {{from the point of}} view of bioinformatics. A microarray consisting of several spots with varied gene expression levels is a <b>grey</b> <b>level</b> <b>image.</b> This study provides a simple framework to classify microarray spots based upon their gene expression levels. This work demonstrates possibility to unravel topological characteristics of microarrays using powerful computers and established tools like mathematical morphology. These characteristics provide new insights in understanding microbiological phenomena in a quantitative manner. In this study we provide a simple but elegant framework, based on mathematical morphology, to unravel various morphological characteristics of microarray spots...|$|E
40|$|This paper {{introduces}} three neural based binarization techniques. These techniques {{start with}} a Self Organizing Map (SOM) applied on the image to extract its most representative <b>grey</b> <b>levels</b> or colors. The classification goes further in two different ways. In the case of <b>grey</b> <b>level</b> <b>images,</b> the Kmeans algorithm or Sauvola's or Niblack's thresholds are used, whereas a Multi Layer Perceptron (MLP) {{is used in the}} case of color images. The obtained results are discussed and we show that they are better than those of some classical binarization techniques...|$|R
40|$|Book {{description}} : Multivariate {{imagery is}} now a very common tool in numerous applications, ranging from satellite remote sensing and astrophysics to biomedical imagery, monitoring of the environment or industrial inspection. Multivariate must be understood in th emost general way: color and multispectral imaging, but also multimodal, multisource or multitemporal imagery. In all the cases, the multivariate image corresponds {{to a set of}} standard <b>grey</b> <b>level</b> <b>images.</b> The avalability of the additional diversity, be it spectral temporal and s. o., provides an invaluable source of information, enabling to consider a wide range of new applications. However,in order to address these applications, theoretical developments are required in terms of signal and image processing, or, more generally speaking, information processing. As a matter of fact, most of the standard algorithms designed for <b>grey</b> <b>level</b> <b>images</b> do not generalize easily to multidimensional spaces and some specific derivations are required. This book aims at presenting the most recent advances in signal and image processing for the analysis of multivariate data. It should be helpful for electrical engineers, PhD students and researcher working in the field of signal processing, but also for any engineer dealing with some specific application where multidimensional data are processed...|$|R
40|$|A {{dedicated}} {{hardware system}} is developed {{for a recent}} class of nonlinear hybrid filters called Order Statistics-Rational Hybrid Filters (OSRHF). The performance of these filters is also studied and compared against three effective nonlinear filters from the literature. The application at hand is noise filtering in <b>grey</b> <b>level</b> <b>images.</b> The proposed hardware system uses a residue number system (RNS) to compute the numerator and the denominator of the rational filter. The resulting structure is suitable for direct implementation on FPGAs...|$|R
40|$|Green's theorem evaluates {{a double}} {{integral}} {{over the region}} of an object by a simple integration along the boundary of the object. It {{has been used in}} moment computation since the shape of a binary object is totally determined by its boundary. By using a discrete analogue of Green's theorem, we present a new algorithm for fast computation of geometric moments. The algorithm is faster than previous methods, and gives exact results. The importance of exact computation is discussed by examining the invariance of Hu's moments. A fast method for computing moments of regions in <b>grey</b> <b>level</b> <b>image,</b> using discrete Green's theorem, is also presented...|$|E
40|$|The {{feasibility}} of realising a low cost wearable face recognition aid {{based on a}} robust correlation algorithm is investigated. The aim {{of the study is}} to determine the limiting spatial and grey level resolution of the probe and gallery images that would support successful prompting of the identity of input face images. Low spatial and grey level resolution images are obtained from good quality image data algorithmically. The tests carried out on the XM 2 VTS database demonstrate that robust correlation is very resilient to degradations of spatial and <b>grey</b> <b>level</b> <b>image</b> resolution. Correct prompts have been generated in 98 % cases even for severely degraded images...|$|E
40|$|We {{present the}} Curved Iterative Boundary Locator (CIBL), {{which is a}} new {{algorithm}} for determining the position and local radius of curvature of a boundary which {{is based upon the}} IBL algorithm [l]. Both the IBL and CIBL use the <b>grey</b> <b>level</b> <b>image</b> directly, rather than an edge image, and this distinguishes them from conventional robust model fitting techniques. The performance of the CIBL is evaluated for a realistic image domain and the results are compared with data obtained from robust Least Squares ellipse fitting. We conclude that the CIBL and its variants provide a powerful technique for robust analysis, particularly in the area of industrial inspection where dimension measurement to high precision is often required. ...|$|E
30|$|We have {{proposed}} an evaluation protocol {{to assess the}} results of line and circle detection algorithms on natural <b>grey</b> <b>level</b> <b>images</b> and applied this evaluation on the DHT, first to provide a parametric study of these algorithms and then to compare them to other classic and state-of-the-art versions of line and circle detectors based on Hough transforms. We {{have shown that the}} qualitative results obtained by the DHT are of the same level than the other HT, while being less sensitive to image perturbations, and more computationally efficient.|$|R
40|$|Abstract This paper {{describes}} how animat-based “food foraging ” techniques may {{be applied to}} the design of low-level image processing algorithms. First, we show how we implemented the food foraging application using the EASEA software package. We then use this technique to evolve an animat and learn how to move inside images and detect high-gradient lines with a minimum explora-tion time. The resulting animats do not use standard “scanning + filtering ” tech-niques but develop other image exploration strategies close to contour tracking. Experimental results on <b>grey</b> <b>level</b> <b>images</b> are presented. ...|$|R
40|$|Comunicación presentada en el VII Symposium Nacional de Reconocimiento de Formas y Análisis de Imágenes, SNRFAI, Barcelona, abril 1997. Snake based {{tracking}} over a textured {{potential is}} applied to recover the 3 D structure of a complex organ, the eye, from a sequence of slices. <b>Grey</b> <b>level</b> <b>images</b> are first analysed by a probabilistic method to derive a suitable potential. Then, the snake is initialised by hand and, finally, its energy function is minimised. Temporal discontinuity problems {{due to the nature}} of the images (microscopic slices tinted and taken by hand) are solved...|$|R
40|$|In this paper, we {{presented}} an image segmentation algorithm based on adaptive weighted mathematical morphology edge detectors. The {{performance of the}} proposed algorithm has been demonstrated on the Lena image. The input of the proposed algorithm is a <b>grey</b> <b>level</b> <b>image.</b> The image was first processed by the mathematical morphological closing and dilation residue edge detector to enhance the edge features and sketch out the contour of the image, respectively. Then the adaptive weight SE operation {{was applied to the}} edge-extracted image to fuse edge gaps and hill up holds. Experimental results show it can not only primely extract detail edge, but also superbly preserve integer effect comparative to classical edge detection algorithm...|$|E
40|$|In this paper, an {{interactive}} technique for extracting cartographic features from aerial and spatial images is presented. The method is essentially {{an interactive}} method of image region segmentation based on pixel grey level and texture information. The underlying segmentation method is seeded region growing. The criterion for growing regions is based on both texture and grey level, where texture is quantified using cooccurrence matrices. The Kullback distance is utilised with co-occurrence matrices in order to describe the image texture, then the Theory of Evidence is applied to merge the information coming from texture and <b>grey</b> <b>level</b> <b>image</b> from the RGB bands. Several results from aerial and spatial images that support the technique are presente...|$|E
40|$|This report {{deals with}} the {{development}} of a parametric model based method to locate and characterize precisely important curved features such as ellipses and B-splines based curves. The method uses all the grey level information of the pixels contained within a window around the feature of interest and produces the complete parametric model that best approximates in a mean-square sense the observed <b>grey</b> <b>level</b> <b>image</b> intensities within the working area. Different solutions have been developed to reduce the computational time required by such approaches and a large number of experiments involving real images have been carried out in order to test and compare the reliability, the robustness and the efficiency of the different proposed approaches...|$|E
40|$|The {{performance}} of Legendre moments of colour images using the red, {{green and blue}} components is compared to the {{performance of}} Legendre moments using the hue, saturation and intensity components. In order to avoid large, unrealistic steps of the hue values, a modified definition of the hue is used in the calculations. The results show that the differences between those two colour models are small when the reconstructed images are compared. In many ways the Legendre moments behave much the same for colour <b>images</b> as for <b>grey</b> <b>level</b> <b>images.</b> ...|$|R
40|$|Loss of {{information}} in images undergoing fine-to-coarse transformations is analysed by using an approach based on the theory of irreversible processes. In the case of <b>grey</b> <b>level</b> <b>images,</b> entropy variation along scales is used to characterize basic, low-level information and to identify perceptual components of the image, such as shape and texture. Here {{an extension of the}} approach to colour images is proposed. Spatio-chromatic information is defined, which depends on cross-interactions between the different colour channels. Examples illustrating the use of spatio-chromatic information are presented, related to pattern recognition and active vision. Ó 2002 Elsevier Science B. V. All rights reserved...|$|R
40|$|In this paper, a novel data-hiding {{technique}} {{based on}} the Fibonacci representation of digital images is presented. A generalization of the classical Least Significant Bit (LSB) embedding method is performed. The Fibonacci representa-tion of <b>grey</b> <b>level</b> <b>images</b> requires 12 bit planes {{instead of the usual}} 8 planes of binary representation. Experimental results show that, such a redundant scheme outperforms the classi-cal LSB method resulting in marked images having less per-ceptual distortion even if different planes from the lowest bit plane are selected for embedding. The computational cost of the embedding scheme is compatible with the classical LSB data hiding scheme. 1...|$|R
