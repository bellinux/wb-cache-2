0|471|Public
3000|$|According to the {{expression}} of B, {{it is apparent that}} if B RR [...] holds <b>full</b> <b>column</b> rank, then B AA [...] must be of <b>full</b> <b>column</b> rank, provided P, Q measurements come in pairs. Therefore, the observability condition of MILP is that B RR [...] is of <b>full</b> <b>column</b> rank, provided that P, Q measurements come in pairs. Unobservable lines and their corresponding measurements should be removed in estimation by MILP.|$|R
3000|$|... {{are not all}} zero), Δ^SF 1 _N × 1 ⊗ I_N_t has <b>full</b> <b>column</b> rank, i.e., rank (Δ^SF 1 _N × 1 ⊗ I_N_t) = N_t. Hence, from (44), matrix φ has <b>full</b> <b>column</b> rank as well, i.e., rank(φ)=N [...]...|$|R
5000|$|Remark: A {{sufficient}} {{condition for the}} expected residuals to converge to 0 is [...] This can be achieved if [...] has a <b>full</b> <b>column</b> rank and under very mild conditions on [...] Convergence of the method can be established also without the <b>full</b> <b>column</b> rank assumption in a different way.|$|R
40|$|Thesis {{deals with}} {{design of a}} steel {{structure}} of multi-storey office building. It is a six-storey steel structure of square plan with dimensions of 40 x 48 meters and height of 21, 5 meters. Structural design report is made {{by a combination of}} manual calculations and calculations using software Scia engineer. Structural design report includes calculations of: truss girder, column, connection, bracing, beam, secondary beam and <b>column</b> <b>anchorage.</b> Thesis includes drawing documentation and report...|$|R
30|$|The {{matrices}} A_ 2, A_ 3 {{in problem}} (1) are both <b>full</b> <b>column</b> rank.|$|R
5000|$|... #Subtitle level 3: A has <b>full</b> <b>column</b> {{rank and}} B has full row rank ...|$|R
30|$|Necessary and {{sufficient}} uniqueness conditions more relaxed than the Kruskal’s one were established for third‐ and fourth‐order tensors, {{under the assumption}} that at least one matrix factor is <b>full</b> <b>column</b> rank[86, 87]. These conditions are complicated to apply. Other more relaxed conditions have been derived independently by Stegeman[88] and Guo et al.[89], for third‐order PARAFAC models with a <b>full</b> <b>column</b> rank matrix factor.|$|R
5000|$|Local identification. If g(Y,θ) is {{continuously}} differentiable in {{a neighborhood}} of , then matrix [...] must have <b>full</b> <b>column</b> rank.|$|R
5000|$|... has all <b>columns</b> linearly {{independent}} (<b>full</b> <b>column</b> rank) and [...] has all rows linearly independent (full row rank) or, ...|$|R
40|$|The {{objective}} of the diploma thesis is a design and a check of load-carrying steel structure of a shopping gallery. The project is designed in two options. The building is located in Vsetin. The total ground plan‘s dimensions are 51 x 54 metres. The height of the building in the highest point is 17 metres. The whole building {{is divided into two}} parts - entrance hall and free-storey building. For selected option the calculation of joints and <b>column</b> <b>anchorage</b> is performed as well as the design documentation...|$|R
5000|$|Decomposition: [...] where C is an m-by-r <b>full</b> <b>column</b> rank matrix and F is an r-by-n full row rank matrix ...|$|R
3000|$|From the {{condition}} (78), we {{can conclude that}} if two matrix factors (A and B) are <b>full</b> <b>column</b> rank (k [...]...|$|R
5000|$|The {{sample data}} matrix [...] must have <b>full</b> <b>column</b> rank.Otherwise [...] is not {{invertible}} and the OLS estimator cannot be computed.|$|R
5000|$|Since [...] has <b>full</b> <b>column</b> rank, [...] is {{invertible}} so [...]Similarly, since B has full row rank, [...] is invertible so [...]|$|R
5000|$|Since [...] is a <b>full</b> <b>column</b> rank matrix and [...] {{is a full}} row rank matrix, we {{can take}} [...] and [...]|$|R
5000|$|By {{assumption}} matrix X has <b>full</b> <b>column</b> rank, {{and therefore}} X'X is invertible {{and the least}} squares estimator for β is given by ...|$|R
30|$|G] has a <b>full</b> <b>column</b> rank. Obviously, x̂_jls is the BLUE, since n is the zero-mean white noise, {{according}} to the Gauss-Markov theorem [1].|$|R
5000|$|Consider the Gram-Schmidt process {{applied to}} the <b>columns</b> of the <b>full</b> <b>column</b> rank matrix , with inner product [...] (or [...] for the complex case).|$|R
5000|$|This is {{a special}} case of either <b>full</b> <b>column</b> rank or <b>full</b> row rank (treated above).If [...] has {{orthonormal}} columns (...) or orthonormal rows (...) , then: ...|$|R
3000|$|... has <b>full</b> <b>column</b> rank, {{the matrix}} A will be tall {{and also has}} <b>full</b> <b>column</b> rank, which {{guarantees}} {{the validity of the}} SVD operation in (15). Following Lemmas 1 and 2, we could make the following important observation; that is when M ≥ 2 Ncl, i.e., the number of antennas at the receivers is not less than two times of the number of channel clusters from each user, the validity of our method is guaranteed with L satisfying the condition in (25).|$|R
3000|$|... is <b>full</b> <b>column</b> rank. It {{implies that}} any K sub-matrices of A are {{guaranteed}} to yield a set of linearly independent columns so that [...]...|$|R
5000|$|... f is {{injective}} (or [...] "one-to-one") if {{and only}} if A has rank n (in this case, we say that A has <b>full</b> <b>column</b> rank).|$|R
2500|$|No linear dependence. The regressors in X {{must all}} be linearly independent. Mathematically, {{this means that}} the matrix X must have <b>full</b> <b>column</b> rank almost surely: ...|$|R
3000|$|... be two tall {{matrices}} of {{the same}} size with <b>full</b> <b>column</b> rank. Let P be a projection matrix of B, that is, P = B [...]...|$|R
3000|$|... has <b>full</b> <b>column</b> rank. Due {{to the way}} T is made in this design, γ is not used. The scalar γ is thus {{equal to}} 1.|$|R
5000|$|The first superdiagonal blocks [...] are <b>full</b> <b>column</b> rank [...] {{matrices}} in reduced row-echelon form (that is, {{an identity}} matrix followed by zero rows) for [...]|$|R
5000|$|No linear dependence. The regressors in X {{must all}} be linearly independent. Mathematically, {{this means that}} the matrix X must have <b>full</b> <b>column</b> rank almost surely: ...|$|R
40|$|Harshman (UCLA Working Papers in Phonetics 1972; 22 : 111 - 117) {{has given}} a proof of {{uniqueness}} (identification) of Parafac solutions, when {{two of the three}} component matrices are of <b>full</b> <b>column</b> rank, and the third satisfies a few other conditions. Kruskal has given more relaxed sufficient conditions, which do not require any of the component matrices to be of <b>full</b> <b>column</b> rank. However, even when two component matrices are of <b>full</b> <b>column</b> rank, Harshman's conditions on the third matrix are still less easily satisfied than Kruskal's. The present paper bridges the gap between the two sets of conditions by utilizing the possibilities of slice mixing in Harshman's approach. it offers an alternative uniqueness theorem that is sufficiently general for all practical purposes and easy to interpret, with a proof that is easy to understand. Copyright (C) 2008 John Wiley & Sons, Ltd...|$|R
3000|$|It {{is easy to}} {{find that}} the linear model for oblique {{projections}} is not a <b>full</b> <b>column</b> rank matrix, the model is a matrix belonging to [...]...|$|R
3000|$|... are column vectors and {{are both}} <b>full</b> <b>column</b> rank (rank is 1). If the {{polarized}} {{states of the}} target and the interference are different, then the subspaces [...]...|$|R
3000|$|... are equivalent, but if C {{is not a}} <b>full</b> <b>column</b> rank matrix, this {{implication}} may be not true. The following theorem {{shows that}} when the implication is true.|$|R
40|$|Situations often {{arise in}} which the matrix of {{independent}} variables is not of <b>full</b> <b>column</b> rank. That is, there are one or more linear dependencies among the independent variables. This paper covers in detail the situation {{in which the}} rank is one less than <b>full</b> <b>column</b> rank and extends this coverage to include cases of even greater rank deficiency. The emphasis is on the row geometry of the solutions based on the normal equations. The author shows geometrically how constrained-regression/generalized-inverses work in this situation to provide a solution in the face of rank deficiency...|$|R
5000|$|... for {{arbitrary}} vector w. Solution(s) exist if {{and only}} if [...] [...] If the latter holds, then the solution is unique if {{and only if}} A has <b>full</b> <b>column</b> rank, in which case [...] is a zero matrix. If solutions exist but A does not have <b>full</b> <b>column</b> rank, then we have an indeterminate system, all of whose infinitude of solutions are given by this last equation. This solution is deeply connected to the Udwadia-Kalaba equation of classical mechanics to forces of constraint that do not obey D'Alembert's principle.|$|R
3000|$|... {{that are}} {{decomposed}} into a sum of P Tucker models of rank‐ (L,M,N), which {{corresponds to the}} particular case where all the factor matrices are <b>full</b> <b>column</b> rank, with [...]...|$|R
50|$|The {{application}} of the Gram-Schmidt process to the column vectors of a <b>full</b> <b>column</b> rank matrix yields the QR decomposition (it is decomposed into an orthogonal and a triangular matrix).|$|R
5000|$|Drillers: These do {{what the}} name implies. It drills {{straight}} through a <b>full</b> <b>column,</b> destroying all pieces in that column. No points are awarded for Ka-Gloms or blocks so cleared.|$|R
