9|17|Public
40|$|Recent {{years have}} {{witnessed}} wide application of hashing for large-scale image retrieval. However, most existing hashing methods {{are based on}} hand-crafted features which might not be optimally compatible with the hashing procedure. Recently, deep hashing methods have been proposed to perform simultaneous feature learning and <b>hash-code</b> learning with deep neural networks, which have shown better performance than traditional hashing methods with hand-crafted features. Most of these deep hashing methods are supervised whose supervised information is given with triplet labels. For another common application scenario with pairwise labels, there have not existed methods for simultaneous feature learning and <b>hash-code</b> learning. In this paper, we propose a novel deep hashing method, called deep pairwise-supervised hashing(DPSH), to perform simultaneous feature learning and <b>hash-code</b> learning for applications with pairwise labels. Experiments on real datasets show that our DPSH method can outperform other methods to achieve the state-of-the-art performance in image retrieval applications. Comment: IJCAI 201...|$|E
40|$|Due to its low {{storage cost}} and fast query speed, cross-modal hashing (CMH) {{has been widely}} used for {{similarity}} search in multimedia retrieval applications. However, almost all existing CMH methods are based on hand-crafted features which might not be optimally compatible with the <b>hash-code</b> learning procedure. As a result, existing CMH methods with handcrafted features may not achieve satisfactory performance. In this paper, we propose a novel cross-modal hashing method, called deep crossmodal hashing (DCMH), by integrating feature learning and <b>hash-code</b> learning into the same framework. DCMH is an end-to-end learning framework with deep neural networks, one for each modality, to perform feature learning from scratch. Experiments on two real datasets with text-image modalities show that DCMH can outperform other baselines to achieve the state-of-the-art performance in cross-modal retrieval applications. Comment: 12 page...|$|E
40|$|A common {{operation}} in non-numerical analysis is {{the comparison of}} symbolic mathematical expressions. Often equivalence under the algebraic and trigonometric relations can be determined with the high probability by hash-coding the expressions using finite field arithmetic and then comparing the resulting <b>hash-code</b> numbers. The use of this scheme in a program for algebraic simplification is discussed...|$|E
40|$|In {{this paper}} {{trade-offs}} among certain computational factors in hash coding are analyzed. The paradigm problem con-sidered {{is that of}} testing a series of messages one-by-one for membership in a given set of messages. Two new <b>hash-coding</b> methods are examined and compared with a par-ticular conventional <b>hash-coding</b> method. The computational factors considered are {{the size of the}} hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency. The new methods are intended {{to reduce the amount of}} space required to contain the <b>hash-coded</b> information from that associated with conventional methods. The reduction in space is accomplished by exploiting the possibility that a small fraction of errors of commission may be tolerable in some applications, in particular, applications in which a larg...|$|R
40|$|Abstract. We {{examine the}} {{efficiency}} of <b>hash-coding</b> and tree-search algorithms for retrieving from a file of k-letter words all words which match a partially-specified input query word (for example, retrieving all six-letter English words of the form S**R*H where "* " is a "don’t care " character). We precisely characterize those balanced <b>hash-coding</b> algorithms with minimum average number of lists examined. Use of the first few letters of each word as a list index is shown to be one such optimal algorithm. A new class of combinatorial designs (called associative block designs) provides better hash functions with a greatly reduced worst-case number of lists examined, yet with optimal average behavior maintained. Another efficient variant involves storing each word in several lists. Tree-search algorithms are shown to be approximately as efficient as <b>hash-coding</b> algorithms, on the average. In general, these algorithms require time about O(n <k-s) /k) {{to respond to a}} query word with s letters specified, given a file of n k-letter words. Previous algorithms either required time O(s n/k) or else used exorbitant amounts of storage...|$|R
40|$|In {{analysing}} {{a well-known}} <b>hash-coding</b> method, Knuth gave an exact expression {{for the average}} number of rejections encountered by players of a variant of musical chairs. We study a variant more closely related to musical chairs itself and deduce the same expression by a purely combinatorial approach. Comment: December 1973, revised 197...|$|R
40|$|This paper {{presents}} a new framework for a biometric cryptosystem {{in which a}} cryptographic key is concealed with biometric modalities. In this paper, the candidate biometric modality is secured using two functions: 1) BCH encoding, which delivers the parity-code stored for the alignment of the query biometric template and 2) the Hash function to compute <b>hash-code</b> in order to safeguard its integrity. The cryptosystem is formed by creating two different cell-arrays. The <b>hash-code</b> is scattered on one cell-array by a randomly chosen column position, and the secret key is distributed over the second cell-array on the same position. The other cell-array locations are filled with the randomly generated chaff vectors. The parity-code is then jumbled up using a regenerative XORCoding in order to hide it from unauthorized access. At the unlocking stage, the parity-code is regenerated using XORcode and used to align the query template to the original one. If the hashed-code computed from the aligned template can locate the correct locations of the original hashed-code from the feature-array, it can get back the secret-key from the key-array and the correspondingly secret message. The proposed algorithm is implemented and evaluated in two modes: 1) unimodal and 2) multimodal. The experimental results from publicly available databases confirm {{the superiority of the}} multimodal cryptosystems over the unimodal cryptosystems. Department of Computin...|$|E
40|$|Hashing {{has been}} widely used for {{large-scale}} search due to its low storage cost and fast query speed. By using supervised information, supervised hashing can significantly outperform unsupervised hashing. Recently, discrete supervised hashing and deep hashing are two representative progresses in supervised hashing. On one hand, hashing is essentially a discrete optimization problem. Hence, utilizing supervised information to directly guide discrete (binary) coding procedure can avoid sub-optimal solution and improve the accuracy. On the other hand, deep hashing, which integrates deep feature learning and <b>hash-code</b> learning into an end-to-end architecture, can enhance the feedback between feature learning and <b>hash-code</b> learning. The key in discrete supervised hashing is to adopt supervised information to directly guide the discrete coding procedure in hashing. The key in deep hashing is to adopt the supervised information to directly guide the deep feature learning procedure. However, there have not existed works which can use the supervised information to directly guide both discrete coding procedure and deep feature learning procedure in the same framework. In this paper, we propose a novel deep hashing method, called deep discrete supervised hashing (DDSH), to address this problem. DDSH is the first deep hashing method which can utilize supervised information to directly guide both discrete coding procedure and deep feature learning procedure, and thus enhance the feedback between these two important procedures. Experiments on three real datasets show that DDSH can outperform other state-of-the-art baselines, including both discrete hashing and deep hashing baselines, for image retrieval...|$|E
40|$|Abstract—Due to {{intermittent}} connectivity {{and uncertain}} node mobility, opportunistic message forwarding algorithms {{have been widely}} adopted in delay tolerant networks (DTNs). While existing work proposes practical forwarding algorithms in terms of increasing the delivery rate and decreasing data overhead, {{little attention has been}} drawn to the control overhead induced by the algorithms. The control overhead could, however, make the forwarding algorithms infeasible when the network size scales. In this paper, we are interested in increasing scalability by reducing control overhead, while retaining the state-of-the-art forwarding performances. The basic idea is to use locality-sensitive hashing to map each node as a <b>hash-code,</b> and use these hash-codes to compute the pair-wise similarity that guides the forwarding decisions. The proposed SOFA algorithms have reduced control overhead and competitive forwarding performance, which are verified by extensive real trace-driven simulations. ...|$|E
40|$|The {{design of}} a syntax-directed {{interpreter}} for a subset of Algol is described. It is a conceptual design with sufficient details and completeness but as much independence of implementation as possible. The design includes {{a detailed description of}} a scanner, an analyzer described in the Floyd-Evans productions, a <b>hash-coded</b> symbol table, and an executor. Interpretation of sample programs is also provided to show how the interpreter functions...|$|R
40|$|The paper {{deal with}} {{problems}} of data security in the safety - related transmission systems defined within railway process. In details is analysed possibility od using <b>hash-coding</b> techniques, which are generally defined in norm EN 50159 - 2. According to specification of transmission in railway applications the type of secure hash keying and nonkeyng techniques are recommended. There are discussed the results of experimental part, which {{are related to the}} comparison of time relation of hash code determination...|$|R
40|$|Abstract:- Cerebellar Model Articulation Controllers (CMACs) are a biologically-inspired {{neural network}} system {{suitable}} for trajectory control. Traditionally, CMACs have been implemented using <b>hash-coding</b> for their memory allocation, requiring static allocation of fixed amounts of memory in advance to the training of the system. This paper presents a method for implementing CMACs using Binary Search Trees to provide dynamic memory allocation, allowing for lower memory usage without compromising the functionality of the CMAC. Key-Words:- CMAC, binary search trees, dynamic memory allocation, memory algorithm...|$|R
40|$|Min-Hash is {{a popular}} {{technique}} for efficiently estimating the Jaccard similarity of binary sets. Consistent Weighted Sampling (CWS) generalizes the Min-Hash scheme to sketch weighted sets and has drawn increasing interest from the community. Due to its constant-time complexity independent {{of the values of}} the weights, Improved CWS (ICWS) is considered as the state-of-the-art CWS algorithm. In this paper, we revisit ICWS and analyze its underlying mechanism to show that there actually exists dependence between the two components of the <b>hash-code</b> produced by ICWS, which violates the condition of independence. To remedy the problem, we propose an Improved ICWS (I$^ 2 $CWS) algorithm which not only shares the same theoretical computational complexity as ICWS but also abides by the required conditions of the CWS scheme. The experimental results on a number of synthetic data sets and real-world text data sets demonstrate that our I$^ 2 $CWS algorithm can estimate the Jaccard similarity more accurately, and also compete with or outperform the compared methods, including ICWS, in classification and top-$K$ retrieval, after relieving the underlying dependence...|$|E
40|$|Abstract — Distributed {{hash tables}} (DHTs), {{used in a}} number of current {{peer-to-peer}} systems, provide efficient mechanisms for resource location. Systems such as Chord, Pastry, CAN, and Tapestry provide strong guarantees that queries in the overlay network can be resolved in a bounded number of overlay hops, while preserving load balance among the peers. A key distinction in these systems is the way they handle locality in the underlying network. Topology-based node identifier assignment, proximity routing, and proximity neighbor selection are examples of heuristics used to minimize message delays in the underlying network. In this paper, we investigate the use of source IP addresses to enhance locality in overlay networks based on DHTs. We first show that a naive use of source IP address potentially leads to severe resource imbalance due to nonuniformity of peers over the IP space. We then present an effective caching scheme that combines a segment of the source IP with the queried <b>hash-code</b> to effectively localize access and affect replication. Using detailed experiments, we show that this scheme achieves performance gains of up to 41 %, when compared to Pastry in combination with the proximity neighbor selection heuristic. I...|$|E
30|$|As {{a kind of}} {{particular}} supervised hashing, similarity-preserving hashing is also a widely utilized method for large-scale image search tasks. In training, the input of similarity-preserving hashing {{is in the form}} of triplets or pairwise similar/dissimilar images. The binary codes are learned to keep the original similarities of the input tripltes/pairs. For example, Lai et al. [28] propose a “one-stage” supervised deep hashing architecture that has three parts: (1) shared stacked convolution layers to capture the image representations, (2) divide-and-encode modules to divide intermediate image features and map them into multiple hash codes, and (3) a triplet ranking loss function which is designed to keep triple relationship on images. Similarly using triple images, Zhang et al. propose a supervised learning framework to generate compact and bit-scalable hashing codes directly from raw images [70]. Besides the similarity-preserving, each bit of the hashing codes is unequally weighted so that the hashing framework can manipulate the code lengths by truncating the insignificant bits. Moreover, Li et al. propose a deep pairwise-supervised hashing to firstly perform simultaneous feature learning and <b>hash-code</b> learning for applications with pairwise labels [71]. Compared with [28, 70], the main difference of the deep pairwise-supervised hashing is that the triplet ranking loss is replaced by the pairwise ranking loss, which is similar to Siamese Neural Network. In addition, Zhu et al. extend the original pairwise rank loss to the pairwise cross-entropy loss and a pairwise quantization loss together [72]. Besides, Yao et al. present a novel deep semantic-preserving and ranking-based hashing architecture, which jointly learns projections from image representations to hash codes and classification [73].|$|E
40|$|A {{new design}} scheme for the Cerebellar Model Articular Controller (CMAC) is {{presented}} in this article. The controller is designed with a content addressable memory (CAM) that replaces {{the function of the}} <b>hash-coding</b> method, which is adopted by the traditional CMAC and consumes much computation power in memory space mapping. Therefore, address mapping in the proposed CMAC is different from the <b>hash-coding</b> method. The CAM, which is capable of fast comparison, can immediately determine in parallel if there is any identical data in memory as the input data to be compared. If a match does not occur, the activated address is then stored in a vacant cell of the CAM indexed by a circular incremental pointer. Memory collision can be avoided unless the memory is fully occupied. In this way, the memory utilization rate can be improved to 100 %, which is difficult to achieve using the conventional schemes. Meanwhile, control noise can be largely suppressed. In content addressing, the associated mask function, which is triggered when the CAM is full, can decrease the probability of collision and improve control performance. Simulation results of function approximation and truck backer-upper control indicate that the proposed CMAC is clearly superior to the conventional CMACs...|$|R
40|$|Chimaera {{is a model}} {{intended}} to secure data (messages) exchanges between applications. Chimaera uses the PEM (Privacy Enhanced Mail) format which enables the receiver to authentificate the sender identity using special informations (certificates, signature, <b>hash-coding)</b> held into the message. PEM is a model for e-mail authentification and certification. PEM complies to X 509 standard as for the handling of certificates. Chimaera provides a way to generate PEM formatted mails. Such mails will be fully read by users running a similar PEM application, not necessarily Chimaera. Basic Chimaera services are: The certification of public keys of individuals thru submission to the local certification authority...|$|R
5000|$|In {{semantic}} hashing [...] {{documents are}} mapped to memory addresses {{by means of}} a neural network {{in such a way that}} semantically similar documents are located at nearby addresses. Deep neural network essentially builds a graphical model of the word-count vectors obtained from a large set of documents. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of <b>hash-coding</b> to approximate matching is much faster than locality sensitive hashing, which is the fastest current method.|$|R
40|$|In {{this paper}} {{trade-offs}} among certain computational factors in hash coding are analyzed. The paradigm problem considered {{is that of}} testing a series of messages one-by-one for membership in a given set of messages. Two new hashcoding methods are examined and compared with a particular conventional <b>hash-coding</b> method. The computational factors considered are {{the size of the}} hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency. The new methods are intended {{to reduce the amount of}} space required to contain the <b>hash-coded</b> information from that associated with conventional methods. The reduction in space is accomplished by exploiting the possibility that a small fraction of errors of commission may be tolerable in some applications, in particular, applications in which a large amount of data is involved and a core resident hash area is consequently not feasible using conventional methods. In such applications, it is envisaged that overall performance could be improved by using a smaller core resident hash area in conjunction with the new methods and, when necessary, by using some secondary and perhaps time-consuming test to "catch " the small fraction of errors associated with the new methods. An example is discussed which illustrates possible areas of application for the new methods. Analysis of the paradigm problem demonstrates that allowing a small number of test messages to be falsely identified as members of the given set will permit a much smaller hash area to be used without increasing reject time. KEY WORDS AND PHRASES: hash coding, hash addressing, scatter storage, searching, storage layout, retrieval trade-ofFs, retrieval efficiency, storag...|$|R
40|$|We {{propose a}} new method for {{homology}} search of nucleic acids or proteins in databanks. All the possible subsequences {{of a specific}} length in a sequence are converted into a code and stored in an indexed file (<b>hash-coding).</b> This preliminary work of codifying an entire bank is rather long but it enables an immediate {{access to all the}} sequence fragments of a given type. With our method a strict homology pattern of twenty nucleotides can be found for example in the Los Alamos bank (GENBANK) in less than 2 seconds. We can also use this data storage to considerably speed up the non-strict homology search programs and to write a program to help in the selection of nucleic acid hybridization probes...|$|R
40|$|Bloom filters {{make use}} of a “probabilistic ” <b>hash-coding</b> method {{to reduce the amount}} of space {{required}} to store a hash set. A Bloom filter offers a trade-off between its size and the probability that the filter returns the wrong result. It does this without storing the entire set, at the cost of occasionally incorrectly answering yes to the question “is x a member of s?”. The paper discusses how Bloom filters can be used to speed up name to location resolution process in large scale distributed systems. The approach presented offers trade-offs between performance (the time taken to resolve an object’s name to its location) and resource utilisation (the amount of physical memory to store location information and the number of messages exchanged to obtain the object’s address). 1...|$|R
40|$|A file {{structure}} {{designed to}} provide rapid, random access with minimum storage overhead is presented. Storage and retrieval are achieved by direct attribute combination-to-address transfor-mation thereby negating the necessity for large file dictionaries or list-pointer structures. The attribute combination-to-address transformation is conceptually similar to key-to-address transfor-mation techniques, but the transformation {{is not limited to}} operations on a single key but operates on the combination of several independent keys (or any subset of the combination) describing an item or request. A storage and retrieval system utilizing the combinatorial file structure is developed. Stor-age and retrieval results derived from a simulated document library of 4000 items are presented. The new file organization is shown to have marked val-ue with respect to minimum storage overhead and high retrieval speed. KEY WORDS AND PHRASES file structures, file organization, combina-torial file, key-to-address transformation, multi-ple attribute retrieval, file access method, data structures, <b>hash-coding,</b> attribute combination-to-address transformation, storage and retrieval sys-tem I...|$|R
40|$|We {{show how}} to learn a deep {{graphical}} model of the word-count vectors obtained from a large set of documents. The values of the latent variables in the deepest layer are easy to infer and give a much better representation of each document than Latent Semantic Analysis. When the deepest layer is forced to use {{a small number of}} binary variables (e. g. 32), the graphical model performs “semantic hashing”: Documents are mapped to memory addresses {{in such a way that}} semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of <b>hash-coding</b> to approximate matching is much faster than locality sensitive hashing, which is the fastest current method. By using semantic hashing to filter the documents given to TF-IDF, we achieve higher accuracy than applying TF-IDF to the entire document set...|$|R
40|$|Abstract: After a brief {{introduction}} to <b>hash-coding</b> (scatter storage) {{and discussion of}} methods described in the literature, it is shown that for hash tables of length p> 2, prime, the primitive roots r of the cyclic group Z/p of prime residues mod p {{can be used for}} a simple collision strategy q(p,i) = r i mod p for f i(k) = f 0 (k) +q(p,i) mod p. It is similar to the strategy which uses quadratic residues q(p,i) = i 2 mod p in avoiding secondary clustering, but reaches all table positions for probing. A table of n primes for typical table lengths and their primitive roots is added. In cases where r = 2 j is such a primitive root, the collision strategy can be implemented simply by repeated shifts to the left (by j places in all). To make the paper self-contained and easy to read, the relevant definitions and the theorems used from the Theory of Numbers are included in the paper...|$|R
40|$|AbstractWe {{show how}} to learn a deep {{graphical}} model of the word-count vectors obtained from a large set of documents. The values of the latent variables in the deepest layer are easy to infer and give a much better representation of each document than Latent Semantic Analysis. When the deepest layer is forced to use {{a small number of}} binary variables (e. g. 32), the graphical model performs “semantic hashing”: Documents are mapped to memory addresses {{in such a way that}} semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of <b>hash-coding</b> to approximate matching is much faster than locality sensitive hashing, which is the fastest current method. By using semantic hashing to filter the documents given to TF-IDF, we achieve higher accuracy than applying TF-IDF to the entire document set...|$|R
5000|$|Chemists {{can search}} {{databases}} using parts of structures, {{parts of their}} IUPAC names as well as based on constraints on properties. Chemical databases are particularly different from other general purpose databases in their support for sub-structure search. This kind of search is achieved by looking for subgraph isomorphism (sometimes also called a monomorphism) and is a widely studied application of Graph theory. The algorithms for searching are computationally intensive, often of O (n3) or O (n4) time complexity (where n {{is the number of}} atoms involved). The intensive component of search is called atom-by-atom-searching (ABAS), in which a mapping of the search substructure atoms and bonds with the target molecule is sought. ABAS searching usually makes use of the Ullman algorithm or variations of it (i.e. SMSD [...] ). Speedups are achieved by time amortization, that is, some of the time on search tasks are saved by using precomputed information. This pre-computation typically involves creation of bitstrings representing presence or absence of molecular fragments. By looking at the fragments present in a search structure it is possible to eliminate the need for ABAS comparison with target molecules that do not possess the fragments that are present in the search structure. This elimination is called screening (not {{to be confused with the}} screening procedures used in drug-discovery). The bit-strings used for these applications are also called structural-keys. The performance of such keys depends on the choice of the fragments used for constructing the keys and the probability of their presence in the database molecules. Another kind of key makes use of <b>hash-codes</b> based on fragments derived computationally. These are called 'fingerprints' although the term is sometimes used synonymously with structural-keys. The amount of memory needed to store these structural-keys and fingerprints can be reduced by 'folding', which is achieved by combining parts of the key using bitwise-operations and thereby reducing the overall length.|$|R

