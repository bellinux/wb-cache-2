249|932|Public
5000|$|... #Caption: <b>Error</b> <b>surface</b> of {{a linear}} neuron with two input weights ...|$|E
5000|$|... #Caption: <b>Error</b> <b>surface</b> of {{a linear}} neuron {{for a single}} {{training}} case.|$|E
50|$|BPTT has {{difficulty}} with local optima. With recurrent neural networks, local optima {{is a much}} more significant problem than it is with feed-forward neural networks. The recurrent feedback in such networks tends to create chaotic responses in the <b>error</b> <b>surface</b> which cause local optima to occur frequently, and in very poor locations on the <b>error</b> <b>surface.</b>|$|E
40|$|Visualization of MLP <b>error</b> <b>surfaces</b> {{helps to}} {{understand}} the influence of network structure and training data on neural learning dynamics. PCA is used to determine two orthogonal directions that capture almost all variance in the weight space. 3 -dimensional plots show {{many aspects of the}} original <b>error</b> <b>surfaces...</b>|$|R
40|$|This journal {{article was}} {{published}} in the journal, The Photogrammetric Record [Remote Sensing and Photogrammetry Society and Blackwell Publishing Ltd / © The authors] and is available at: [URL] internal geometry of consumer grade digital cameras is generally considered unstable. Research conducted recently at Loughborough University indicated the potential of these sensors to maintain their internal geometry. It also identified residual systematic <b>error</b> <b>surfaces</b> or “domes”, discernible in digital elevation models (DEMs) (Wackrow et al., 2007), caused by slightly inaccurate estimated lens distortion parameters. This paper investigates these systematic <b>error</b> <b>surfaces</b> and establishes a methodology to minimise them. Initially, simulated data were used to ascertain the effect of changing the interior orientation parameters on extracted DEMs, specifically the lens model. Presented results demonstrate the relationship between “domes” and inaccurately specified lens distortion parameters. The stereopair remains important for data extraction in photogrammetry, often using automated DEM extraction software. The photogrammetric normal case is widely used, in which the camera base is parallel to the object plane and the optical axes of the cameras intersect the object plane orthogonally. During simulation, the <b>error</b> <b>surfaces</b> derived from extracted DEMs using the normal case, were compared with <b>error</b> <b>surfaces</b> created using a mildly convergent geometry. In contrast to the normal case, the optical camera axes intersect the object plane at the same point. Results of the simulation process clearly demonstrate that a mildly convergent camera configuration eradicates the systematic <b>error</b> <b>surfaces.</b> This result was confirmed through practical tests and demonstrates that mildly convergent imagery effectively improves the accuracies of DEMs derived with this class of sensor...|$|R
50|$|French, R. M. and Chater, N. (2002). Using Noise to Compute <b>Error</b> <b>Surfaces</b> in Connectionist Networks: A Novel Means of Reducing Catastrophic Forgetting. Neural Computation, 14(7), 1755-1769.|$|R
5000|$|In this analogy, {{the person}} {{represents}} the backpropagation algorithm, {{and the path}} taken down the mountain represents the sequence of parameter settings that the algorithm will explore. The steepness of the hill represents {{the slope of the}} <b>error</b> <b>surface</b> at that point. The instrument used to measure steepness is differentiation (the slope of the <b>error</b> <b>surface</b> can be calculated by taking the derivative of the squared error function at that point). The direction he chooses to travel in aligns with the gradient of the <b>error</b> <b>surface</b> at that point. The amount of time he travels before taking another measurement is the learning rate of the algorithm. See the limitation section {{for a discussion of the}} limitations of this type of [...] "hill climbing" [...] algorithm.|$|E
5000|$|Monotonic - When the {{activation}} function is monotonic, the <b>error</b> <b>surface</b> {{associated with a}} single-layer model is guaranteed to be convex.|$|E
50|$|By formalizing {{learning}} {{in such a}} way, connectionists have many tools. A very common strategy in connectionist learning methods is to incorporate gradient descent over an <b>error</b> <b>surface</b> in a space defined by the weight matrix. All gradient descent {{learning in}} connectionist models involves changing each weight by the partial derivative of the <b>error</b> <b>surface</b> {{with respect to the}} weight. Backpropagation (BP), first made popular in the 1980s, is probably the most commonly known connectionist gradient descent algorithm today.|$|E
40|$|This {{article was}} {{published}} in the journal, The Photogrammetric Record [© The Authors. The Photogrammetric Record © The Remote Sensing and Photogrammetry Society and Blackwell Publishing Ltd. ] The definitive version is available at wileyonlinelibrary. com: [URL] are increasing opportunities to use consumer-grade digital cameras, particularly if accurate spatial data can be captured. Research recently conducted at Loughborough University identified residual systematic <b>error</b> <b>surfaces</b> or domes discernible in digital elevation models (DEMs). These systematic effects are often associated with such cameras and are caused by slightly inaccurate estimated lens distortion parameters. A methodology that minimises the systematic <b>error</b> <b>surfaces</b> was therefore developed, using a mildly convergent image configuration in a vertical perspective. This methodology was tested through simulation and a series of practical tests. This paper investigates the potential of the convergent configuration to minimise the <b>error</b> <b>surfaces,</b> even if the geometrically more complex oblique perspective is used. Initially, simulated data was used to demonstrate that an oblique convergent image configuration can minimise remaining systematic <b>error</b> <b>surfaces</b> using various imaging angles. Additionally, practical tests using a laboratory testfield were conducted to verify results of the simulation. The need to develop a system to measure the topographic surface of a flooding river provided the opportunity to verify the findings of the simulation and laboratory test using real data. Results of the simulation process, the laboratory test and the practical test are reported in this paper and demonstrate that an oblique convergent image configuration eradicates the systematic <b>error</b> <b>surfaces</b> which result from inaccurate lens distortion parameters. This approach is significant because by removing the need for an accurate lens model it effectively improves the accuracies of digital surface representations derived using consumer-grade digital cameras. Carefully selected image configurations could therefore provide new opportunities for improving the quality of photogrammetrically acquired data...|$|R
50|$|SES is {{computationally}} {{very efficient}} {{as compared to}} TSS. However the peak signal-to-noise ratio achieved is poor as compared to TSS as the <b>error</b> <b>surfaces</b> are not strictly unimodal in reality.|$|R
40|$|Deep {{learning}} researchers commonly {{suggest that}} converged models {{are stuck in}} local minima. More recently, some researchers observed that under reasonable assumptions, {{the vast majority of}} critical points are saddle points, not true minima. Both descriptions suggest that weights converge around a point in weight space, be it a local optima or merely a critical point. However, it's possible that neither interpretation is accurate. As neural networks are typically over-complete, it's easy to show the existence of vast continuous regions through weight space with equal loss. In this paper, we build on recent work empirically characterizing the <b>error</b> <b>surfaces</b> of neural networks. We analyze training paths through weight space, presenting evidence that apparent convergence of loss does not correspond to weights arriving at critical points, but instead to large movements through flat regions of weight space. While it's trivial to show that neural network <b>error</b> <b>surfaces</b> are globally non-convex, we show that <b>error</b> <b>surfaces</b> are also locally non-convex, even after breaking symmetry with a random initialization and also after partial training...|$|R
50|$|Recently, highly {{accurate}} wavefront sensors {{have been}} applied for the characterization of the slope <b>error,</b> <b>surface</b> roughness and surface form of large optics such as X-ray mirrors. This is an alternative measurement technique to Long Trace Profilometer (LTP).|$|E
5000|$|The {{idea behind}} TSS {{is that the}} <b>error</b> <b>surface</b> due to motion in every macro block is {{unimodal}}. A unimodal surface is a bowl shaped surface such that the weights generated by the cost function increase monotonically from the global minimum. However a unimodal surface cannot have two minimums in opposite directions and hence the 8 point fixed pattern search of TSS can be further modified to incorporate this and save computations. SES [...] is the extension of TSS that incorporates this assumption.|$|E
50|$|RBF {{networks}} {{have the advantage}} of avoiding local minima {{in the same way as}} multi-layer perceptrons. This is because the only parameters that are adjusted in the learning process are the linear mapping from hidden layer to output layer. Linearity ensures that the <b>error</b> <b>surface</b> is quadratic and therefore has a single easily found minimum. In regression problems this can be found in one matrix operation. In classification problems the fixed non-linearity introduced by the sigmoid output function is most efficiently dealt with using iteratively re-weighted least squares.|$|E
40|$|In {{this work}} we {{address the problem}} of object {{recognition}} and localization from sparse range data. The method is based upon comparing the 7 -D <b>error</b> <b>surfaces</b> of objects in various poses, which result from the registration error function between two convolved surfaces. The objects and their pose values are encoded by a small set of feature vectors extracted from the minima of the <b>error</b> <b>surfaces.</b> The problem of object recognition is thus reduced to comparing these feature vectors to find the corresponding <b>error</b> <b>surfaces</b> between the runtime data and a preprocessed database. Specifically, we present a new approach to the problems of pose determination, object recognition and object class recognition. The algorithm has been implemented and tested on both simulated and real data. The experimental results demonstrate the technique to be both effective and efficient, executing at 122 frames per second on standard hardware and with recognition rates exceeding 97 % for a database of 60 objects. The performance of the proposed potential well space embedding (PWSE) approach on large size databases was also evaluated on the Princeton Shape Bench...|$|R
40|$|In this paper, {{simulation}} methods {{with improved}} surface generation techniques {{are used to}} study the accuracy of budding assembly processes. Our analysis shows that both the dimensional <b>error</b> and <b>surface</b> roughness together with surface waviness have {{significant effect on the}} budding assembly accuracy. In addition, we have found that even for the same parts with given dimensional <b>errors,</b> <b>surface</b> roughness and waviness, the budding assembly accuracy can still vary significantly depending on the different approaches of the alignment processes...|$|R
25|$|Shape <b>errors</b> - random <b>surface</b> <b>errors</b> in {{the shape}} of the {{reflector}} reduce efficiency. The loss is approximated by Ruze's Equation.|$|R
40|$|In error-driven {{distributed}} feedforward networks, {{new information}} typically interferes, sometimes severely, with previously learned information. We show how noise {{can be used}} to approximate the <b>error</b> <b>surface</b> of previously learned information. By combining this approximated <b>error</b> <b>surface</b> with the <b>error</b> <b>surface</b> associated with the new information to be learned, the network's retention of previously learned items can be improved and catastrophic interference significantly reduced. Further, we show that the noise-generated <b>error</b> <b>surface</b> is produced using only first-derivative information and without recourse to any explicit error information...|$|E
40|$|Research on {{neural network}} {{learning}} within the supervised learning paradigm {{has focused on}} efficient search (or optimization) over the <b>error</b> <b>surface.</b> Less {{attention has been given}} to the effect representation has on the <b>error</b> <b>surface.</b> One interesting question to ask is, how does the choice of data points affect learning time for a neural network on linearly separable problems. This paper examines the issue of class representation in the light of its affect on <b>error</b> <b>surface.</b> <b>Error</b> <b>surface</b> plots visually suggest that an equal representation of points for each class decreases learning time. This hypothesis is supported by simulation results for a simple classification problem. 1 Introduction In the supervised learning paradigm learning can be conceptualized as a search through weight space (guided by an <b>error</b> <b>surface)</b> for a region where the error is acceptably low. For backpropagation [10], the most commonly used network learning algorithm, search is performed by gradient descent on the [...] ...|$|E
40|$|In error-driven {{distributed}} feedforward networks {{new information}} typically interferes, sometimes severely, with previously learned information. We show how noise {{can be used}} to approximate the <b>error</b> <b>surface</b> of previously learned information. By combining this approximated <b>error</b> <b>surface</b> with the <b>error</b> <b>surface</b> associated with the new information to be learned, the network’s retention of previously learned items can be improved and catastrophic interference significantly reduced. Further, we show that the noise-generated <b>error</b> <b>surface</b> is produced using only first-derivative Everyone forgets but, thankfully, it is typically a gradual process. Neural networks, on the other hand, and especially those that develop highly distributed representations over a single set of weights, can suffer from severe and sudden forgetting. Almost all of the early solutions to this problem, called the problem of “catastrophic forgetting, ” relied on learning algorithm...|$|E
40|$|The Rprop {{algorithm}} {{proposed by}} Riedmiller and Braun {{is one of}} the best performing first-order learning methods for neural networks. We introduce modifications of the algorithm that improve its learning speed. The resulting speedup is experimentally shown for a set of neural network learning tasks as well as for artificial <b>error</b> <b>surfaces...</b>|$|R
40|$|Local minima in the <b>error</b> <b>surfaces</b> of {{feed-forward}} {{neural networks}} are significant {{because they may}} entrap gradient based training algorithms. Recent results have identified conditions under which local minima do not occur. The present paper considers three distinct definitions of local minimum, concluding that a new definition, called regional minimum, corresponds most closely to intuition. Using this definition, we analyse weight configurations in which a hidden node is ignored or redundant and show {{that these are not}} local minima. The practical implications of this result for gradient based learning are discussed. 1. Introduction Local minima in the <b>error</b> <b>surfaces</b> of feed-forward neural networks are of interest because they may entrap training algorithms based upon gradient descent (Rumelhart, Hinton and Williams, 1986). Global search techniques such as simulated annealing (Kirkpatrick, Gelatt and Vecchi, 1983) are often too computationally expensive for real-world problems. Prac [...] ...|$|R
40|$|As an {{integrated}} {{element of the}} manufacturing system of components, the machining fixture is a major contributing factor of the profile and orientation errors of component features. An effective way to control the accuracy of components is to decompose error sources and evaluate individual influential factors. This paper proposed a systematic method of error identification and calculation, in which locating error and machining error were studied. The locating error, which is the <b>surface</b> <b>error</b> generated before machining, is obtained from the calculation of the <b>surface</b> <b>error</b> based on tolerances of the locating positions and the decomposition of clamping deformation using finite element analysis (FEA). The machining <b>error,</b> the <b>surface</b> <b>error</b> generated from machining operations, is gained mainly from the coordinate measurement machine’s (CMMs) measurements. The <b>surface</b> <b>error</b> of multi-machining operations is investigated and the resultant <b>surface</b> <b>error</b> is evaluated against tolerance. The analysis of a sample feature of a turbine blade is provided as an example...|$|R
40|$|The Multi-Layer Perceptron (MLP) {{is one of}} {{the most}} widely applied and {{researched}} Artificial Neural Network model. MLP networks are normally applied to performing supervised learning tasks, which involve iterative training methods to adjust the connection weights within the network. This is commonly formulated as a multivariate non-linear optimization problem over a very high-dimensional space of possible weight configurations. Analogous to the field of mathematical optimization, training an MLP is often described as the search of an <b>error</b> <b>surface</b> for a weight vector which gives the smallest possible error value. Although this presents a useful notion of the training process, there are many problems associated with using the <b>error</b> <b>surface</b> to understand the behaviour of learning algorithms and the properties of MLP mappings themselves. Because of the high-dimensionality of the system, many existing methods of analysis are not well-suited to this problem. Visualizing and describing the <b>error</b> <b>surface</b> are also nontrivial and problematic. These problems are specific to complex systems such as neural networks, which contain large numbers of adjustable parameters, and the investigation of such systems in this way is largely a developing area of research. In this thesis, the concept of the <b>error</b> <b>surface</b> is explored using three related methods. Firstly, Principal Component Analysis (PCA) is proposed as a method for visualizing the learning trajectory followed by an algorithm on the <b>error</b> <b>surface.</b> It is found that PCA provides an effective method for performing such a visualization, as well as providing an indication of the significance of individual weights to the training process. Secondly, sampling methods are used to explore the <b>error</b> <b>surface</b> and to measure certain properties of the <b>error</b> <b>surface,</b> providing the necessary data for an intuitive description of the <b>error</b> <b>surface.</b> A number of practical MLP error surfaces are found to contain a high degree of ultrametric structure, in common with other known configuration spaces of complex systems. Thirdly, a class of global optimization algorithms is also developed, which is focused on the construction and evolution of a model of the <b>error</b> <b>surface</b> (or search spa ce) as an integral part of the optimization process. The relationships between this algorithm class, the Population-Based Incremental Learning algorithm, evolutionary algorithms and cooperative search are discussed. The work provides important practical techniques for exploration of the error surfaces of MLP networks. These techniques can be used to examine the dynamics of different training algorithms, the complexity of MLP mappings and an intuitive description of the nature of the <b>error</b> <b>surface.</b> The configuration spaces of other complex systems are also amenable to many of these techniques. Finally, the algorithmic framework provides a powerful paradigm for visualization of the optimization process and the development of parallel coupled optimization algorithms which apply knowledge of the <b>error</b> <b>surface</b> to solving the optimization problem...|$|E
40|$|Neural {{networks}} (NNs) have powerful computational {{abilities and}} {{could be used in}} a variety of applications; however, training these networks is still a difficult problem. With different network structures, many neural models have been constructed. In this report, a deeper neural networks (DNNs) architecture is proposed. The training algorithm of deeper neural network insides searching the global optimal point in the actual <b>error</b> <b>surface.</b> Before the training algorithm is designed, the <b>error</b> <b>surface</b> of the deeper neural network is analyzed from simple to complicated, and the features of the <b>error</b> <b>surface</b> is obtained. Based on these characters, the initialization method and training algorithm of DNNs is designed. For the initialization, a block-uniform design method is proposed which separates the <b>error</b> <b>surface</b> into some blocks and finds the optimal block using the uniform design method. For the training algorithm, the improved gradient-descent method is proposed which adds a penalty term into the cost function of the old gradient descent method. This algorithm makes the network have a great approximating ability and keeps the network state stable. All of these improve the practicality of the neural network...|$|E
40|$|Abstract—An {{application}} of numerical gradient (NG) to training of MLP networks is presented. Several {{versions of the}} algorithm {{and the influence of}} various parameters on the training process are discussed. Optimization of network parameters based on global search with numerical gradient is presented. Examples of two-dimensional projection of the <b>error</b> <b>surface</b> are shown and the influence of various numerical gradient parameters on the <b>error</b> <b>surface</b> is presented. The speed and accuracy of this method is compared with the search-based MLP training algorithm...|$|E
40|$|Abstract: This paper investigates cutting {{forces and}} <b>surface</b> <b>error</b> due to cutting force-induced tool {{deflections}} in peripheral milling of curved geometries. In machining workpiece geometries, where curvature varies continuously along the tool path, both cutting force and <b>surface</b> <b>error</b> vary from point to point. This {{is different from}} the case of machining straight and circular geometries where cutting forces and <b>surface</b> <b>error</b> are both invariant along the tool path. To study the effect of curvature, first force models for feed and normal cutting forces are derived from dynamometer measured forces. These force components are used to calculate tool deflection and resulting <b>surface</b> <b>error.</b> The results of this study show that both cutting forces and <b>surface</b> <b>error</b> are influenced strongly by workpiece curvature...|$|R
40|$|This paper {{constructs}} a geometrical {{perspective to}} justify the slow learning period and fast learning period during training. It plots the <b>error</b> <b>surfaces</b> and the solution spaces in the input space for a single neuron with two inputs. It records various training paths in this space using the back-propagation (BP) training algorithm [6]. It finds {{the relations between the}} learning curve and training path...|$|R
40|$|An {{adaptive}} method is employed {{to speed up}} computation of high accuracy surface modeling (HASM), for which an error indicator and an error estimator are developed. Root mean-square error (RMSE) is used as the error estimator that is formulated {{as a function of}} gully density and grid cell size. The error indicator is developed on the basis of <b>error</b> <b>surfaces</b> for different spatial resolutions, which are interpolated in terms of the absolute errors calculated at sampled points while paying attention to the landform characteristics. The <b>error</b> <b>surfaces</b> indicate the magnitude and distribution of errors in each step of adaptive refinement and make spatial changes to the errors in the simulation process visualized. The {{adaptive method}} of high accuracy surface modeling (HASM-AM) is applied to simulating elevation surface of the Dong-Zhi tableland with 27. 24 million pixels at a spatial resolution of 10 m 10 m. Test results show that HASM-AM has greatly speeded up computation by avoiding unnecessary calculations and saving memory. In addition, HASM-AM improves simulation accuracy...|$|R
40|$|We use {{sampling}} methods to analyse the "apparent minima" of the error surfaces of feedforward neural networks learning encoder problems. First and second-order statistics {{of a sample}} of these points of attraction are shown to provide qualitative statistical information about {{the structure of the}} <b>error</b> <b>surface,</b> allowing a simple description of this structure. Following methods previously used in the analysis of other complex configuration spaces (such as spin glass models and several combinatorial optimization problems), the third-order statistics of the points of attraction are examined and found to be arranged in a highly ultrametric way, using the normal Euclidean distance measure. The implications of this result are discussed. 1 Introduction 1. 1 Learning as <b>Error</b> <b>Surface</b> Optimization The <b>error</b> <b>surface</b> is essentially a cost function of the type that arises in general multivariate optimization problems. Given a vector of weights in a feedforward neural network or Multi-Layer Percept [...] ...|$|E
30|$|These algorithms, however, {{assume that}} the <b>error</b> <b>surface</b> of the minimum {{absolute}} difference increases monotonically as the search position {{moves away from the}} global minimum on the <b>error</b> <b>surface</b> [16]. This assumption would be reasonable in a small region near the global minimum, but not absolutely true for real video signals. To avoid trapped in undesirable local minimum, some adaptive search algorithms have been devised intending to achieve the global optimum or sub-optimum with adaptive search patterns. One of those algorithms is the adaptive rood pattern search (ARPS) [24].|$|E
40|$|We {{propose a}} new method for {{visualizing}} {{the learning process}} in artificial neural networks using Principal Component Analysis. The network weights constitute an evolving sequence of data in weight space that can be subjected to this form of analysis. We verify experimentally that {{the variance in the}} data is captured largely by the first few principal components. Our experimentation is applied to networks with various numbers of weights, and demonstrates that this approach is a useful technique for the visualization of learning in networks of practical size. Keywords: Visualization, learning, neural networks 1 Introduction In the training of neural networks, the concept of the <b>error</b> <b>surface</b> is of central importance. The training process is often described as a directed search of the <b>error</b> <b>surface,</b> attempting to locate a minimum point. For example, the commonly used back-propagation algorithm works by estimating the gradient of the <b>error</b> <b>surface</b> at the current position, and moves in [...] ...|$|E
40|$|The {{theory and}} design of linear {{adaptive}} filters based on FIR filter structures is well developed and widely applied in practice. However, the same is not true for more general classes of adaptive systems such as lin-ear infinite impulse response adaptive filters (IIR) and nonlinear adaptive systems. This situation results because both linear IIR structures and nonlinear structures tend to produce multi-modal <b>error</b> <b>surfaces</b> for which stochastic gradient optimization strategies may fail to reach the global minimum. After briefly dis-cussing {{the state of the}} art in linear adaptive filtering, the attention of this paper is turned to IIR and non-linear adaptive systems for potential use in echo cancellation, channel equalization, acoustic channel modeling, nonlinear prediction, and nonlinear system identification. Structured stochastic optimization algo-rithms that are effective on multimodal <b>error</b> <b>surfaces</b> are then introduced, with particular attention to the Particle Swarm Optimization (PSO) technique. The PSO algorithm is demonstrated on some representative IIR and nonlinear filter structures, and both performance and computational complexity are analyzed for these types of nonlinear systems...|$|R
40|$|In {{the design}} of {{adaptive}} IIR filters, the multi-modal nature of the <b>error</b> <b>surfaces</b> can limit the use of gradient-based and other iterative search methods. Stochastic learning automata have previously {{been shown to have}} global optimisation properties making them suitable for the optimisation of filter coefficients. Continuous action reinforcement learning automata are presented as an extension to the standard automata which operate over discrete parameter sets. Global convergence is claimed, and demonstration...|$|R
3000|$|... is a step-size {{parameter}} {{introduced to}} control {{how far we}} can move along the <b>error</b> function <b>surface</b> at each iteration. If [...]...|$|R
