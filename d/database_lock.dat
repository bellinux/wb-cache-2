44|83|Public
50|$|Segments {{of memory}} {{can be shared}} between virtual machines. There {{are two kinds of}} shared memory: public {{segments}} used by the operating system (which are present in all virtual machines), and global segments used for application-level shared data: this latter mechanism is used only when there is an application requirement for two virtual machines to communicate. For example, global memory segments are used for <b>database</b> <b>lock</b> tables. Hardware semaphore instructions are available to synchronise access to such segments. A minor curiosity is that two virtual machines sharing a global segment will use different virtual addresses for the same memory locations, which means that virtual addresses cannot safely be passed from one VM to another.|$|E
40|$|An {{increasing}} number of sponsors are submitting clinical trials data to the FDA in {{the format of the}} Clinical Data Interchange Standards Consortium (CDISC) Study Data Tabulation Model (SDTM). In many cases, however, the data were not collected, stored, or extracted from the database in the SDTM format. As a result, the data must be converted, and must meet a number of structure and content requirements that may be new to some sponsors. These requirements are described in the SDTM and the SDTM Implementation Guide (SDTMIG). This paper discusses some of the steps sponsors can take to facilitate the conversion of data collected in their traditional format(s) to data in the SDTM format. For maximum efficiency, many of these steps should be undertaken at study setup or during the study (prior to <b>database</b> <b>lock).</b> There are, however, some actions that sponsors can take that will facilitate even late-stage (after <b>database</b> <b>lock)</b> conversions...|$|E
40|$|Abstract. In {{parallel}} database systems, conflicts for accesses {{to objects}} are solved through object locking. In order to acquire and release locks, {{in the standard}} implementation of a lock manager small sections of the code may be executed only by a single thread. On massively parallel shared memory machines (SMM) the serialization of these critical sections leads to serious performance degradation. We eleminate the serialization by decomposing the complex <b>database</b> <b>lock</b> needed for granular locking into basic lock primitives. By doing so, we measured a speedup of a factor 200 on the SB-PRAM. Our method can be ported to any architecture supporting the used lock primitives, as most SMMs do. ...|$|E
5000|$|Resource usage via use of {{exclusion}} mechanisms and discipline in accessing serially reusable resources (including threads or <b>database</b> <b>locks)</b> ...|$|R
50|$|Other {{features}} of the language are intended to help MUMPS applications {{interact with each other}} in a multi-user environment. <b>Database</b> <b>locks,</b> process identifiers, and atomicity of database update transactions are all required of standard MUMPS implementations.|$|R
50|$|Using {{the virtual}} {{synchrony}} model, {{it is relatively}} easy to maintain fault-tolerant replicated data in a consistent state. One can then build higher level abstractions over the basic replication mechanisms. For example, many virtual synchrony libraries also support tools for building distributed key-value stores, replicating external files or <b>databases,</b> <b>locking</b> or otherwise coordinating the actions of group members, etc.|$|R
40|$|In {{this paper}} {{we present a}} {{concurrency}} control algorithm that allows co-existence of soft real-time, relational database transactions, and hard real-time database pointer transactions in real-time database management systems. The algorithm uses traditional pessimistic concurrencycontrol (i. e. locking) for soft transactions and versioning for hard transactions {{to allow them to}} execute regardless of any <b>database</b> <b>lock.</b> We provide formal proof that the algorithm is deadlock free and formally verify that transactions have atomic semantics. We also present an evaluation that demonstrates significant benefits for both soft and hard transactions when our algorithm is used. The proposed algorithm is suited for resource-constrained safety critical, real-time systems that have a mix of hard real-time control applications and soft real-time management, maintenance, or user-interface applications...|$|E
40|$|On-line {{services}} are making {{increasing use of}} dynamically generated Web content. Serving dynamic content {{is more complex than}} serving static content. Besides a Web server, it typically involves a server-side application and a database to generate and store the dynamic content. A number of standard mechanisms have evolved to generate dynamic content. We evaluate three specific mechanisms in common use: PHP, Java servlets, and Enterprise Java Beans (EJB). These mechanisms represent three different architectures for generating dynamic content. PHP scripts are tied to the Web server and require writing explicit database queries. Java servlets execute in a different process from the Web server, allowing them to be located on a separate machine for better load balancing. The database queries are written explicitly, as in PHP, but in certain circumstances the Java synchronization primitives can be used to perform locking, reducing <b>database</b> <b>lock</b> contention and the amount of communication between servlets and the database. Enterprise Java Beans (EJB) provide several services and facilities. In particular, many of the database queries can be generated automatically. We measure the performance of these three architectures using two application benchmarks: an online bookstore and an auction site. These benchmarks represent common applications for dynamic content and stress different parts of a dynamic content Web server. The auction site stresses the server front-end, while the online bookstore stresses the server back-end. For all measurements, we use widely available open-source software (the Apache Web server, Tomcat servlet engine, JOnAS EJB server, and MySQL relational database). While Java servlets are less efficient than PHP, their ability to execute on a different machine from the Web server and their ability to perform synchronization leads to better performance when the front-end is the bottleneck or when there is <b>database</b> <b>lock</b> contention. EJB facilities and services come at the cost of lower performance than both PHP and Java servlets...|$|E
40|$|Currently, {{hardware}} trends {{include a}} move toward multicore processors, cheap and persistent variants of memory, and even sophisticated hardware support for mutual exclusion {{in the form of}} transactional memory. These trends, coupled with a growing desire for extremely high performance on short database transactions, raise {{the question of whether the}} hardware primitives developed for mutual exclusion can be exploited to run database transactions. In this paper, we present a preliminary exploration of this question. We conduct a set of experiments on both a hardware prototype and a simulator of a multi-core processor with Transactional Memory (TM.) Our results show that TM is attractive under low contention workloads, while spinlocks can tolerate high contention workloads well, and that in some cases these approaches can beat a simple implementation of a traditional <b>database</b> <b>lock</b> manager by an order of magnitude. 1...|$|E
40|$|In a {{computing}} environment where access to system resources {{is controlled by}} an access control policy and execution of database transactions is dictated by <b>database</b> <b>locking</b> policy, interaction between the two policies can result in constraints restricting execution of transactions. We present a methodology for the verification of database transaction requirements in a Role Based Access Control (RBAC) environment. Specifically, we propose a step by step approach for the extraction of implicit requirements of a database transaction, and present a mechanism whereby these requirements can be verified against an RBAC policy representation. Based on the requirements of database transaction, we define feasible states of the access control policy which allow the transaction to be executed. We also illustrate the interaction of multiple database transactions executing in a single security environment. Finally, we define conditions in an access control policy, which allow the execution of a database transaction without relying on the underlying <b>database</b> <b>locking</b> policy for serializability and deadlock avoidance. 1...|$|R
5000|$|Before NAV 2013, Microsoft Dynamics NAV gave administrators {{the option}} of using either a native {{database}} server or Microsoft SQL Server, as the DBMS. SQL Server is now the exclusive database option for NAV. Retiring the old [...] "Native database" [...] {{has given way to}} long-awaited improvements in reducing/eliminating <b>database</b> <b>locking,</b> which can occur when hundreds or thousands of users are using the same data at once.|$|R
50|$|The XPC-L {{provides}} a communication path among the systems for coordination of actions. It also {{provides a}} very fast lock engine. Connection to the XPC-L is via a special I/O processor that operates with extremely low latencies. The lock manager in the XPC-L provides all the functions required for both file and <b>database</b> <b>locks.</b> This includes deadlock detection {{and the ability to}} free up locks of failed applications.|$|R
40|$|Often an {{important}} decision {{needs to be}} made based on anticipated data for a trial design or a determination of data handling rules. In this regard, simulation is a very useful method. For example, to prepare programs for statistical analyses and report generation before <b>database</b> <b>lock,</b> some SAS data has to be simulated. RAND, a new SAS function, is an easy-to-use general random number generator, and basically gives “standard distribution. ” Therefore, to obtain a random number for “non-standard distribution, ” some additional math work is needed to transform data from “standard ” to “non-standard. ” This paper demonstrates a SAS macro that generates simulation SAS data for clinical trials for a variety of “standard ” and “non-standard ” distributions. Moreover, the data is generated with different distribution parameters and the expected sample size for each treatment group that corresponds to the trial design...|$|E
40|$|Background: The BRIM- 3 trial showed {{improved}} progression-free survival (PFS) {{and overall}} survival (OS) for vemurafenib compared with dacarbazine in treatment-naive patients with BRAFV 600 mutation-positive metastatic melanoma. We present final OS data from BRIM- 3. Patients and methods: Patients {{were randomly assigned}} in a 1  :  1 ratio to receive vemurafenib (960 [*]mg twice daily) or dacarbazine (1000 [*]mg/m 2 every 3 [*]weeks). OS and PFS were co-primary end points. OS was assessed in the intention-to-treat population, with and without censoring of data for dacarbazine patients who crossed over to vemurafenib. Results: Between 4 January 2010 and 16 December 2010, a total of 675 patients were randomized to vemurafenib (n[*]=[*] 337) or dacarbazine (n[*]=[*] 338, of whom 84 crossed over to vemurafenib). At the time of <b>database</b> <b>lock</b> (14 August 2015), median OS, censored at crossover, was significantly longer for vemurafenib than for dacarbazine 13. 6 [*]months [95...|$|E
40|$|OBJECTIVE: The {{purpose of}} this multicenter, open label, {{randomized}} phase III {{study was to determine}} whether ixabepilone resulted in improved overall survival (OS) compared with commonly used single-agent chemotherapy (doxorubicin or paclitaxel) in women with locally advanced, recurrent, or metastatic endometrial cancer with at least one failed prior platinum-based chemotherapeutic regimen. METHODS: Patients were randomized 1 : 1 to ixabepilone (40 mg/m(2)), or either paclitaxel (175 mg/m(2)) or doxorubicin (60 mg/m(2)), every 21 days. Patients that had previously received an anthracycline were randomized to ixabepilone or paclitaxel; all other patients were randomized to ixabepilone or doxorubicin. An interim analysis of futility for OS was planned. RESULTS: At the time of <b>database</b> <b>lock,</b> 496 patients were randomized to receive ixabepilone (n= 248) or control (n= 248); nine patients in the control arm were not treated. The interim analysis of futility for OS (219 events) favored the control chemotherapy arm (hazard ratio= 1. 3 [95...|$|E
40|$|The {{concurrency}} control lock (e. g. file lock, table lock) {{has long been}} used as a canonical example of a covert channel in a <b>database</b> system. <b>Locking</b> is a fundamental {{concurrency control}} technique used in many kinds of computer systems besides <b>database</b> systems. <b>Locking</b> is generally considered to be interfering and hence unsuitable for multilevel systems. In this paper we show how such locks can be used for concurrency control, without introducing covert channels. 1...|$|R
40|$|Most {{performance}} evaluation studies of database systems are high level studies {{limited by the}} expressiveness of their modelling formalisms. In this paper, we illustrate the potential of Queueing Petri Nets as a successor of traditionally-adopted modelling formalisms in evaluating the complexities of database systems. This is demonstrated through the construction and analysis of a Queueing Petri Net model of table-level <b>database</b> <b>locking.</b> We show that this model predicts mean response times better than a corresponding Petri net model...|$|R
50|$|<b>Database</b> <b>locks</b> {{can be used}} as a {{means of}} {{ensuring}} transaction synchronicity. i.e. when making transaction processing concurrent (interleaving transactions), using 2-phased locks ensures that the concurrent execution of the transaction turns out equivalent to some serial ordering of the transaction. However, deadlocks become an unfortunate side-effect of <b>locking</b> in <b>databases.</b> Deadlocks are either prevented by pre-determining the locking order between transactions or are detected using waits-for graphs. An alternate to <b>locking</b> for <b>database</b> synchronicity while avoiding deadlocks involves the use of totally ordered global timestamps.|$|R
40|$|Abstract. On-line {{services}} are making {{increasing use of}} dynamically generated Web content. Serving dynamic content {{is more complex than}} serving static content. Besides a Web server, it typically involves a server-side application and a database to generate and store the dynamic content. A number of standard mechanisms have evolved to generate dynamic content. We evaluate three specific mechanisms in common use: PHP, Java servlets, and Enterprise Java Beans (EJB). These mechanisms represent three different architectures for generating dynamic content. PHP scripts are tied to the Web server and require writing explicit database queries. Java servlets execute in a different process from the Web server, allowing them to be located on a separate machine for better load balancing. The database queries are written explicitly, as in PHP, but in certain circumstances the Java synchronization primitives can be used to perform locking, reducing <b>database</b> <b>lock</b> contention and the amount of communication between servlets and the database. Enterprise Java Beans (EJB) provide several services and facilities. In particular, many o...|$|E
40|$|Being able {{to model}} {{contention}} for software resources (e. g., a critical section or <b>database</b> <b>lock)</b> is paramount to building performance models that capture {{all aspects of}} the delay encountered by a process as it executes. Several methods have been offered for dealing with software contention and with message blocking in client-server systems. This paper presents a general, straightforward, easy to understand and implement, approach to modeling software contention using queuing networks. The approach, called SQNHQN, consists of a two-level iterative process. Two queuing networks are considered: one represents software resources (SQN) and the other hardware resources (HQN). Multiclass models are allowed and any solution technique [...] -exact or approximate [...] -can be used at any of the levels. This technique falls in the general category of fixed-point approximate models and is similar in nature to other approaches. The main difference lies in its simplicity. The process converges very fast in the examples examined. The results were validated against global balance equation solutions and are very accurate...|$|E
40|$|A common {{approach}} to storage and retrieval of XML documents is to {{store them in}} a database, together with materialized views on their content. The advantage over ”native” XML storage managers {{seems to be that}} transactions and concurrency are for free, next to other benefits. But a closer look and preliminary experiments reveal that this results in poor performance of concurrent queries and updates. The reason is that <b>database</b> <b>lock</b> contention hinders parallelism unnecessarily. We therefore investigate concurrency control at the semantic, i. e., XML level and describe a respective transaction manager XMLTM. It features a new locking protocol DGLOCK. It generalizes the protocol for locking on directed acyclic graphs by adding simple predicate locking on the content of elements, e. g., on their text. Instead of using the original XML documents, we propose to take advantage of an abstraction of the XML document collection known as DataGuides. XMLTM allows to run XML processing at the underlying database at low ANSI isolation degrees and to release database locks early without sacrificing correctness in this setting. We have built a complete prototype system that is implemented on top of the XML Extender for IBM DB 2. Our evaluation shows that our approach consistently yields performance improvements by an order of magnitude. We stress that our approach can also be implemented within a native XML storage manager, and we expect even better performance...|$|E
40|$|Transaction {{processing}} workloads provide ample request level concurrency which highly parallel architectures can exploit. However, {{the resulting}} heavy utilization of core database services also causes resource contention within the database engine itself and limits scalability. Meanwhile, many database workloads consist of short transactions which access {{only a few}} database records each, often with stringent response time requirements. Performance of these short transactions is determined largely {{by the amount of}} overhead the database engine imposes for services such as logging, locking, and transaction management. This paper highlights the negative scalability impact of <b>database</b> <b>locking,</b> an effect which is especially severe for short transactions running on highly concurrent multicore hardware. We propose and evaluate Speculative Lock Inheritance, a technique where hot <b>database</b> <b>locks</b> pass directly from transaction to transaction, bypassing the lock manager bottleneck. We implement SLI in the Shore-MT storage manager and show that lock inheritance fundamentally improves scalability by decoupling the number of simultaneous requests for popular locks from the number of threads in the system, eliminating contention within the lock manager even as core counts continue to increase. We achieve this effect with only minor changes to the lock manager and without changes to consistency or other application-visible effects. 1...|$|R
40|$|This {{document}} {{describes the}} core software that resides in an Input/Output Controller (IOC), {{one of the}} major components of EPICS. The plan of the book is: EPICS overview, IOC test facilities, general purpose features, <b>database</b> <b>locking</b> - scanning - and processing, static database access, runtime database access, database scanning, record and device support, device support library, IOC database configuration, IOC initialization, and database structures. Other than the first chapter this document describes only core IOC software. Thus it does not describe other EPICS tools such as the sequencer. It also does not describe Channel Access, a major IOC component...|$|R
50|$|A {{process can}} then acquire <b>locks</b> on the <b>database</b> as a whole, {{and then on}} {{particular}} parts of the <b>database.</b> A <b>lock</b> must be obtained on a parent resource before a subordinate resource can be locked.|$|R
40|$|Aims: The {{effect on}} {{mortality}} and morbidity of pharmacist-led intervention to optimize pharmacological therapy {{in patients with}} systolic heart failure (HF) has not been tested in a large-scale, long-term, clinical trial. Methods: We describe the rationale and design of a UK, primary care-based, prospective cluster-randomized controlled trial of a pharmacist-led intervention in HF and report baseline characteristics of the patients randomized. Eighty-seven practices (1092 patients) {{were assigned to the}} intervention arm and 87 practices (1077 patients) to usual care. The average age of patients at baseline was 71 years, 70 % were male, 86 % were treated with an angiotensin-converting enzyme inhibitor or angiotensin receptor blocker and 62 % with a beta-blocker. Data for the primary outcome of death from any cause or hospitalization for HF will be available up to 31 December 2010, giving a mean follow-up of 5 years. More than 750 patients would have experienced the primary outcome during this period. The first secondary outcome is death from any cause or hospitalization for a cardiovascular reason. Deaths and hospitalizations are being identified using the Scottish National Health Service electronic patient record-linkage system (hence the delay between the end of follow-up and <b>database</b> <b>lock).</b> Conclusion: This trial is powered to provide a robust evaluation of the effect of pharmacist-led treatment optimization in patients with systolic HF in primary care...|$|E
40|$|Case {{report form}} (CRF) is a {{specialized}} document in clinical research. It should be study protocol driven, robust in content and have material {{to collect the}} study specific data. Though paper CRFs are still used largely, use of electronic CRFs (eCRFS) are gaining popularity due to the advantages they offer such as improved data quality, online discrepancy management and faster <b>database</b> <b>lock</b> etc. Main objectives behind CRF development are preserving and maintaining quality and integrity of data. CRF design should be standardized {{to address the needs}} of all users such as investigator, site coordinator, study monitor, data entry personnel, medical coder and statistician. Data should be organized in a format that facilitates and simplifies data analysis. Collection of large amount of data will result in wasted resources in collecting and processing it and in many circumstances, will not be utilized for analysis. Apart from that, standard guidelines should be followed while designing the CRF. CRF completion manual should be provided to the site personnel to promote accurate data entry by them. These measures will result in reduced query generations and improved data integrity. It is recommended to establish and maintain a library of templates of standard CRF modules as they are time saving and cost-effective. This article is an attempt to describe the methods of CRF designing in clinical research and discusses the challenges encountered in this process...|$|E
40|$|In the {{pharmaceutical}} industry, {{there is a}} regulatory responsibility, 21 CFR Part 11, to analyze only the clinical data that has passed data acceptance testing or is considered ‘clean data ’ after a <b>database</b> <b>lock.</b> Clinical data acceptance testing procedure involves confirming the validity of critical data variables. These critical data variables might need to be non-missing, consist only of valid values, be within a range, or be consistent with other variables. If incorrect clinical data is analyzed, then invalid study conclusions can be drawn about the drug’s safety and efficacy. In 2001, the Data Warehousing Institute conducted a survey of over 600 business professionals. Across all industries, the survey results estimate that data quality problems cost U. S. corporations more than $ 600 billion per year. Proactive steps {{need to be taken}} to identify, isolate and report clinical data issues using a system that is flexible, easy to update and facilitates good communication with the Clinical Data Management (CDM) department to help resolve these data quality problems. This paper will review an effective method to implement a clinical data acceptance testing procedure using edit check macros for creating an RTF file with minimum SAS ® expertise and maintenance. In addition, because all clinical studies have common issues, the edit check macros developed could easily be used to check similar data issues across other clinical studies...|$|E
50|$|Eswaran is a {{graduate}} of Stanford University and University of California, Berkeley. He was an architect of IBM System R, the precursor to DB2. Eswaran was one of the inventors of SQL language. The Eswaran principle relating to <b>database</b> <b>locking</b> and transactions is a contribution that he made along with Jim Gray and Irv Traiger while working as a scientist at IBM Research. Subsequently, he launched Esvel, Inc. (acquired by Cullinet Software in 1987, which itself was acquired by Computer Associates) and Kaps Corporation (technology acquired by subsidiary of BP, Hewlett-Packard and by Carlysle Library Systems). He is currently the CEO of Integrated Informatics Inc.|$|R
50|$|Locking (e.g., Two-phase locking - 2PL) - Controlling {{access to}} data by locks {{assigned}} to the data. Access of a transaction to a data item (<b>database</b> object) <b>locked</b> by another transaction may be blocked (depending on lock type and access operation type) until lock release.|$|R
50|$|In Computer Science, in {{the field}} of databases, non-lock {{concurrency}} control is a concurrency control method used in relational <b>databases</b> without using <b>locking.</b>|$|R
40|$|Data {{management}} has {{significant impact on}} the quality control of clinical studies. Every clinical study should have a data management plan to provide overall work instructions and ensure that all of these tasks are completed according to the Good Clinical Data Management Practice (GCDMP). Meanwhile, the data management plan (DMP) is an auditable document requested by regulatory inspectors and must be written {{in a manner that is}} realistic and of high quality. The significance of DMP, the minimum standards and the best practices provided by GCDMP, the main contents of DMP based on electronic data capture (EDC) and some key factors of DMP influencing the quality of clinical study were elaborated in this paper. Specifically, DMP generally consists of 15 parts, namely, the approval page, the protocol summary, role and training, timelines, database design, creation, maintenance and security, data entry, data validation, quality control and quality assurance, the management of external data, serious adverse event data reconciliation, coding, <b>database</b> <b>lock,</b> data management reports, the communication plan and the abbreviated terms. Among them, the following three parts are regarded as the key factors: designing a standardized database of the clinical study, entering data in time and cleansing data efficiently. In the last part of this article, the authors also analyzed the problems in clinical research of traditional Chinese medicine using the EDC system and put forward some suggestions for improvement...|$|E
40|$|Abstract BACKGROUND: Adrenocortical {{carcinoma}} is a rare, aggressive cancer {{for which}} few treatment options are available. Linsitinib (OSI- 906) is a potent, oral small molecule inhibitor of both IGF- 1 R and the insulin receptor, which has shown acceptable tolerability and preliminary evidence of anti-tumour activity. We assessed linsitinib against placebo to investigate efficacy {{in patients with}} advanced adrenocortical carcinoma. METHODS: In this international, double-blind, placebo-controlled phase 3 study, adult patients with histologically confirmed locally advanced or metastatic adrenocortical carcinoma were recruited at clinical sites in nine countries. Patients were randomly assigned (2 : 1) twice-daily 150 mg oral linsitinib or placebo via a web-based, centralised randomisation system and stratified according to previous systemic cytotoxic chemotherapy for adrenocortical carcinoma, Eastern Cooperative Oncology Group performance status, and use {{of one or more}} oral antihyperglycaemic therapy at randomisation. Allocation was concealed by blinded block size and permuted block randomisation. The primary endpoint was overall survival, calculated from date of randomisation until death from any cause. The primary analysis was done in the intention-to-treat population. This study is registered with ClinicalTrials. gov, number NCT 00924989. FINDINGS: Between Dec 2, 2009, and July 11, 2011, 139 patients were enrolled, of whom 90 were assigned to linsitinib and 49 to placebo. The trial was unblinded on March 19, 2012, based on data monitoring committee recommendation due to the failure of linsitinib to increase either progression-free survival or overall survival. At <b>database</b> <b>lock</b> and based on 92 deaths, no difference in overall survival was noted between linsitinib and placebo (median 323 days [95...|$|E
40|$|Rationale Aside from blood {{pressure}} lowering, treatment options for intracerebral haemorrhage remain limited and {{a proportion of}} patients will undergo early haematoma expansion with resultant significant morbidity and mortality. Tranexamic acid (TXA), an anti-fibrinolytic drug, {{has been shown to}} significantly reduce mortality in patients, who are bleeding following trauma, when given rapidly. TICH- 2 is testing whether TXA is effective at improving outcome in spontaneous intracerebral haemorrhage (SICH). Methods and design TICH- 2 is a pragmatic, phase III, prospective, double-blind, randomised placebo-controlled trial. Two thousand adult (aged[*]≥[*] 18 years) patients with an acute SICH, within 8 h of stroke onset, will be randomised to receive TXA or the placebo control. The primary outcome is ordinal shift of modified Rankin Scale score at day 90. Analyses will be performed using intention-to-treat. Results This paper and its attached appendices describe the statistical analysis plan (SAP) for the trial and were developed and published prior to <b>database</b> <b>lock</b> and unblinding to treatment allocation. The SAP includes details of analyses to be undertaken and unpopulated tables which will be reported in the primary and key secondary publications. The database will be locked in early 2018, ready for publication of the results later in the same year. Discussion The SAP details the analyses that will be done to avoid bias arising from prior knowledge of the study findings. The trial will determine whether TXA can improve outcome after SICH, which currently has no definitive therapy. Trial registration ISRCTN registry, ID: ISRCTN 93732214. Registered on 17 January 2013...|$|E
5000|$|In <b>database</b> {{management}} theory, <b>locking</b> is used {{to implement}} isolation among multiple database users. This is the [...] "I" [...] in the acronym ACID.|$|R
40|$|Abstract — Clinical Data Management (CDM) is a {{critical}} phase in clinical research, which leads to generation of high-quality, reliable, and statistically sound data from clinical trials. This helps to produce a drastic reduction in time from drug development to marketing. Team members of CDM are actively involved in all stages of clinical trial right from inception to completion. They should have adequate process knowledge that helps maintain the quality standards of CDM processes. Various procedures in CDM including Case Report Form (CRF) designing, CRF annotation, database designing, data-entry, data validation, discrepancy management, medical coding, data extraction, and <b>database</b> <b>locking</b> are assessed for quality at regular intervals during a trial. In the present scenario, there is an increased demand to improve the CDM standards to meet the regulatory requirements and {{stay ahead of the}} competition by means of faster commercialization of product. Wit...|$|R
40|$|There is {{a long-standing}} mantra {{in the world}} of industry-sponsored {{clinical}} trials that data must be absolutely clean before a <b>database</b> is <b>locked</b> and an analysis conducted. Tremendous time and resources are devoted to ensuring the quality of the data, both at the site and after the data are entered into the database. Data are checked for accuracy...|$|R
