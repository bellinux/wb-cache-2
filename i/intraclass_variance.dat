14|18|Public
30|$|Image classification, {{which is}} the task of {{determining}} the semantic class of un-labeled test samples, is a challenging task especially for real world images. Two issues challenge the classification accuracy in image classification. First, images are better described by several types of features; thus, the designed system {{should be able to}} merge heterogonous features. The second challenge comes from the large <b>intraclass</b> <b>variance</b> and interclass relationship in real world image databases.|$|E
30|$|The {{standard}} {{framework of}} MKL assigns fixed weights to kernels {{in the entire}} space. As discussed in section 1.2, {{because of the large}} <b>intraclass</b> <b>variance</b> and inter class relationship in complicated spaces, such as an image feature space, similar weights for kernels are not suitable. For example, in some cases the kernel based on color information is more informative than a texture based kernel. Therefore, a more accurate classifier will be achieved if variable weights are assigned to a kernel in different areas of the space.|$|E
30|$|In addition, we {{followed}} the One vs. All strategy in the training phase where we trained one classifier for each individual class. We should note that, generally compared to the One vs. One method, the One vs. All method suffers from high data imbalance between one class and the remaining classes. However, {{because of the high}} <b>intraclass</b> <b>variance</b> in real world image classification, the One vs. One method suffers from the same high data imbalance problem. The data imbalances both inside each class and between classes are addressed by dedicating variable weights to kernels as discussed in section 1.2.|$|E
40|$|Fuzzy vault scheme (FVS) {{is one of}} {{the most}} popular {{biometric}} cryptosystems for biometric template protection. However, error correcting code (ECC) proposed in FVS is not appropriate to deal with real-valued biometric <b>intraclass</b> <b>variances.</b> In this paper, we propose a multidimensional fuzzy vault scheme (MDFVS) in which a general subspace error-tolerant mechanism is designed and embedded into FVS to handle <b>intraclass</b> <b>variances.</b> Palmprint {{is one of the}} most important biometrics; to protect palmprint templates; a palmprint based MDFVS implementation is also presented. Experimental results show that the proposed scheme not only can deal with <b>intraclass</b> <b>variances</b> effectively but also could maintain the accuracy and meanwhile enhance security...|$|R
40|$|A new simple {{distance}} measure {{has been}} proposed in which each vector element is weighted in the distance calculation according to its importance as determined by taking its statistics into account. In order to reflect {{the characteristics of the}} class, the element-significance factors are calculated based on <b>intraclass</b> <b>variances</b> and mean values of vector elements and utilized in the distance measure. The proposed distance measure has been applied to the face detection system and the cephalometric landmarks identification system which we developed in other work. Improved performances in image classification have been demonstrated. 1...|$|R
40|$|Faces {{are highly}} {{challenging}} and dynamic {{objects that are}} employed as biometrics evidence in identity verification. Recently, biometrics systems {{have proven to be}} an essential security tools, in which bulk matching of enrolled people and watch lists is performed every day. To facilitate this process, organizations with large computing facilities need to maintain these facilities. To minimize the burden of maintaining these costly facilities for enrollment and recognition, multinational companies can transfer this responsibility to third-party vendors who can maintain cloud computing infrastructures for recognition. In this paper, we showcase cloud computing-enabled face recognition, which utilizes PCA-characterized face instances and reduces the number of invariant SIFT points that are extracted from each face. To achieve high interclass and low <b>intraclass</b> <b>variances,</b> a set of six PCA-characterized face instances is computed on columns of each face image by varying the number of principal components. Extracted SIFT keypoints are fused using sum and max fusion rules. A novel cohort selection technique is applied to increase the total performance. The proposed protomodel is tested on BioID and FEI face databases, and the efficacy of the system is proven based on the obtained results. We also compare the proposed method with other well-known methods...|$|R
40|$|A field {{spectrometer}} {{covering the}} range 0. 4 to 2. 5 microns was developed that acquires spectra in 2 seconds at 872 points within the spectrum. The Portable Instant Display and Analysis Spectrometer (PIDAS) can acquire spectra every 8 seconds and stores up to 288 spectra in bubble memory. A hand held display unit allows for {{display of the}} current spectrum acquired and superimposed on one of 128 permanently stored library spectra. PIDAS represents a major advance in the technology of field spectral data acquisition {{and for the first}} time makes possible the acquisition of enough spectra to characterize the mean and <b>intraclass</b> <b>variance</b> within a LANDSAT MSS or TM pixel...|$|E
30|$|Based on the {{outcomes}} of our preliminary experiments, we chose to apply Otsu's method [5] to compute the threshold values channel by channel and slice by slice. Assuming that the image to be thresholded contains two classes of pixels/voxels (e.g., object and background), Otsu's method computes the optimum threshold separating these two classes so that their combined spread (<b>intraclass</b> <b>variance)</b> is minimized. Although this method is efficient and works well for images with bimodal histograms, still it may not yield accurate segmentation results in our situation. Due to the scattering of many isolated red/green points, simple thresholding methods {{do not seem to}} be sufficient for identifying thrombi in our 2 -photon microscopic images. We need to combine the thresholding method with the density-based clustering approach, as to be discussed in detail below.|$|E
40|$|Rapid {{evaporative}} ionization {{mass spectrometry}} (REIMS) {{was used for}} the rapid mass spectrometric profiling of cancer cell lines. Spectral reproducibility was assessed for three different cell lines, and the extent of interclass differences and <b>intraclass</b> <b>variance</b> was found to allow the identification of these cell lines based on the REIMS data. Subsequently, the NCI 60 cell line panel was subjected to REIMS analysis, and the resulting data set was investigated for its distinction of individual cell lines and different tissue types of origin. Information content of REIMS spectral profiles of cell lines were found to be similar to those obtained from mammalian tissues although pronounced differences in relative lipid intensity were observed. Ultimately, REIMS was shown to detect changes in lipid content of cell lines due to mycoplasma infection. The data show that REIMS is an attractive means to study cell lines involving minimal sample preparation and analysis times in the range of seconds. © 2016 American Chemical Society...|$|E
5000|$|Experiments can be {{run with}} a similar setup to the one given in Table 1. Using {{different}} relationship groups, we can evaluate different intraclass correlations. Using [...] as the additive genetic variance and [...] as the dominance deviation <b>variance,</b> <b>intraclass</b> correlations become linear functions of these parameters. In general, ...|$|R
3000|$|To {{test the}} Moderation-Hypothesis, we {{specified}} regression models with students’ problem-solving score as the outcome and {{math and science}} scores as predictors {{for each of the}} 41 countries. Due to the clustering of data in schools, these models allowed for between-level <b>variance.</b> <b>Intraclass</b> correlations (ICC- 1) for math, science, and problem solving performance ranged between [...]. 03 and [...]. 61 for the school level (M[*]=[*]. 33, SD[*]=[*]. 16).|$|R
40|$|Identication of {{cartridge}} is very {{essential in}} the field of forensics, military or people who collect ammunitions. The cartridges can beidentied by their headstamps. This thesis presents work on identification and matching of cartridge headstamp from the image. The Libor Masek's open source iris recognition algorithm is considered for the identification of cartridge pattern from the image. The dataset is devoleped with the cartridge headstamp patterns and matching of cartridge headstamp patterns is implemented. For matching of the cartridge pattern the Hamming distance is considered as the metric to differentiate interclass and <b>intraclass</b> comparisons. <b>Variance</b> is used as a criteria to discard the unwanted areas of the cartridge headstamp pattern. Four distinct cartridge headstamp patterns are considered. Three cartridges of each headstamp pattern are considered for intra class comparisons. The validation of the method is performed...|$|R
40|$|We {{present a}} novel Affine-Gradient based Local Binary Pattern (AGLBP) {{descriptor}} for texture classification. It {{is very hard}} to describe complicated texture using single type information, such as Local Binary Pattern (LBP), which just utilizes the sign information of the difference between the pixel and its local neighbors. Our descriptor has three characteristics: 1) In order to make full use of the information contained in the texture, the Affine-Gradient, which is different from Euclidean-Gradient and invariant to affine transformation is incorporated into AGLBP. 2) An improved method is proposed for rotation invariance, which depends on the reference direction calculating respect to local neighbors. 3) Feature selection method, considering both the statistical frequency and the <b>intraclass</b> <b>variance</b> of the training dataset, is also applied to reduce the dimensionality of descriptors. Experiments on three standard texture datasets, Outex 12, Outex 10 and KTH-TIPS 2, are conducted to evaluate the performance of AGLBP. The results show that our proposed descriptor gets better performance comparing to some state-of-the-art rotation texture descriptors in texture classification. Comment: 11 pages, 4 page...|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimitedA computer processing technique is advanced {{which seeks to}} retain or improve data information context while reducing the dimensionality of data representation. Defining information context as the relative proximity of data points, a nonlinear transformation is analytically derived which utilizes Euclidean distance {{to one or more}} reference points to provide a measure of similarity "between data points. The nonarbitrary reference points are selectively manipulated to provide, given certain constraints, a unique mapping from high dimensional space to one or more dimensions for each point in space. The transformation process enhances class clustering and interclass separation in the lower dimensional representation. Computer processed experimental results are presented of reduction from 32, 10, and 3 space into 2 space for both synthetic and real world data. Utilizing a ratio of <b>intraclass</b> <b>variance</b> to interclass variance as a figure of merit and as one possible optimization criterion, this technique yielded a significant ratio improvement in mapping from higher dimensional space into 2 dimensional space for all cases examined. [URL] United States Nav...|$|E
30|$|Histogram shape-based {{thresholding}} {{methods are}} based on the shape property of the histograms. A commonly used thresholding algorithm in this category is due to Otsu [5] and aims to minimize the in-class variance and maximize the between-class variance. It assumes that the image to be thresholded contains two classes of pixels/voxels (e.g., the object and background), and computes the optimum threshold separating these two classes so that their combined spread (<b>intraclass</b> <b>variance)</b> is minimized. This is also equivalent to maximizing the inter-class variance. Sezan [6] performed the peak analysis by convolving the histogram function with a smoothing and differencing kernel and proposed the so-called peak-and-valley thresholding. Entropy-based thresholding algorithms exploit the entropy of the distribution of the gray levels. Johannsen and Bille [7] and Pal et al. [8] studied the Shannon entropy-based thresholding. Kapur et al. [9] strived to maximize the background and foreground entropies. Spatial thresholding methods utilize not only the gray value distribution but also the dependency of pixels in a neighborhood. Kirby and Rosenfeld [10] considered the local average gray levels for thresholding. Chanda and Majumder [11] used co-occurrence probabilities as indicators of the spatial dependency.|$|E
40|$|Background: Information {{regarding}} {{the variability of}} metabolite levels over time in an individual is required to estimate the reproducibility of metabolite measurements. In intervention studies, {{it is critical to}} appropriately judge changes that are elicited by any kind of intervention. The pre-analytic phase (collection, transport and sample processing) is a particularly important component of data quality in multi-center studies. Methods: Reliability of metabolites (within-and between-person <b>variance,</b> <b>intraclass</b> correlation coefficient) and stability (shipment simulation at different temperatures, use of gel-barrier collection tubes, freeze-thaw cycles) were analyzed in fasting serum and plasma samples of 22 healthy human subjects using a targeted LC-MS approach. Results: Reliability of metabolite measurements was higher in serum compared to plasma samples and was good in most saturated short-and medium-chain acylcarnitines, amino acids, biogenic amines, glycerophospholipids, sphingolipids and hexose. The majority of metabolites were stable for 24 h on cool packs and at room temperature in non-centrifuged tubes. Plasma and serum metabolite stability showed good coherence. Serum metabolite concentrations were mostly unaffecte...|$|R
40|$|We {{analyzed}} 50 sets of ankle radiographs {{to determine}} the interobserver and intraobserver reliability when ob-taining common linear and angular measurements. The radiographs {{were divided into two}} groups: one group included 25 normal ankles, and the second group in-cluded 25 fractured ankles. Each set of radiographs was evaluated independently by four different observers on two separate occasions under controlled conditions. Six radiographic parameters were measured on all 50 sets of films: syndesmosis A, syndesmosis B, syndesmosis C, the medial clear space, and the talocrural and bimalleo-lar angles. On the 25 sets of fracture films, four additional measurements of fracture displacement were included: displacement of the medial malleolus (mortise), dis-placement of the lateral malleolus (AP and lateral), and displacement of the posterior malleolus. Reliability was evaluated with an analysis of <b>variance</b> <b>intraclass</b> corre-lation coefficient. Among the examiners, 9 of the 10 pa-rameters could be measured reliably. Intraobserver reli-ability was found to increase with the experience of the examiner...|$|R
40|$|A {{barrier to}} assess the {{relative}} effectiveness of respiratory therapies has been insufficient accurate, reliable, and sensitive outcome measures. Lung sounds provide useful information for assessing and monitoring respiratory patients. However, standard auscultation is too subjective {{to allow them to}} be used as an outcome measure. In this paper, Computer Aided Lung Sound Analysis (CALSA) characterising crackles’ Initial Deflection Width (IDW) and Two Cycle Deflection (2 CD) is proposed as a potential objective, non-invasive, bedside outcome measure {{to assess the}} response to alveolar recruitment and airway clearance interventions. A preliminary ‘repeated measures’ experimental study was conducted. Seventeen participants with cystic fibrosis were recruited from out-patient clinics. Demographic, anthropometric and lung sound data were collected. The intra-subject reliability of crackles’ IDW and 2 CD was found to be ‘good’ to ‘excellent’, estimated by the Analysis of <b>Variance,</b> <b>Intraclass</b> Correlation Coefficient, Bland and Altman 95 % limits of agreement and Smallest Real Difference. It is concluded that crackle IDW and 2 CD detected by CALSA are reliable and stable measures. In future, CALSA may be useful for assessing and monitoring respiratory interventions in clinical settings...|$|R
40|$|This paper {{proposes a}} simple yet {{effective}} method {{to learn the}} hierarchical object shape model consisting of local contour fragments, which represents a category of shapes {{in the form of}} an And-Or tree. This model extends the traditional hierarchical tree structures by introducing the "switch" variables (i. e. the or-nodes) that explicitly specify production rules to capture shape variations. We thus define the model with three layers: the leaf-nodes for detecting local contour fragments, the or-nodes specifying selection of leaf-nodes, and the root-node encoding the holistic distortion. In the training stage, for optimization of the And-Or tree learning, we extend the concave-convex procedure (CCCP) by embedding the structural clustering during the iterative learning steps. The inference of shape detection is consistent with the model optimization, which integrates the local testings via the leaf-nodes and or-nodes with the global verification via the root-node. The advantages of our approach are validated on the challenging shape databases (i. e., ETHZ and INRIA Horse) and summarized as follows. (1) The proposed method is able to accurately localize shape contours against unreliable edge detection and edge tracing. (2) The And-Or tree model enables us to well capture the <b>intraclass</b> <b>variance.</b> Comment: 8 pages, 7 figures, CVPR 201...|$|E
40|$|Abstract—We {{present a}} new {{classification}} algorithm, principal component null space analysis (PCNSA), {{which is designed}} for classification problems like object recognition where different classes have unequal and nonwhite noise covariance matrices. PCNSA first obtains a principal components subspace (PCA space) for the entire data. In this PCA space, it finds for each class “, ” an-dimensional subspace along which the class ’ <b>intraclass</b> <b>variance</b> is the smallest. We call this subspace an approximate null space (ANS) since the lowest variance is usually “much smaller ” than the highest. A query is classified into class “ ” if its distance from the class ’ mean in the class ’ ANS is a minimum. We derive upper bounds on classification error probability of PCNSA and use these expressions to compare classification performance of PCNSA with that of subspace linear discriminant analysis (SLDA). We propose a practical modification of PCNSA called progressive-PCNSA that also detects “new ” (untrained classes). Finally, we provide an experimental comparison of PCNSA and progressive PCNSA with SLDA and PCA and also with other classification algorithms—linear SVMs, kernel PCA, kernel discriminant analysis, and kernel SLDA, for object recognition and face recognition under large pose/expression variation. We also show applications of PCNSA to two classification problems in video—an action retrieval problem and abnormal activity detection. I...|$|E
40|$|With the {{emergence}} of very high spatial resolution satellite images, the spatial resolution gap which existed between satellite images and aerial photographs has decreased. A study of the potential of these images for tree species in" monoculture stands" identification was conducted. Two Ikonos images were acquired, one in June 2000 {{and the other in}} October 2000, for an 11 - by 11 -km area covering the Sonian Forest in the southeastern part of the Brussels-Capital region (Belgium). The two images were orthorectified using a digital elevation model and 1256 geodetic control points. The identification of the tree species was carried out utilizing a supervised maximum-likelihood classification on a pixel-by-pixel basis. Classifications were performed on the orthorectified data, NDVI transformed data, and principal components imagery. In order to decrease the <b>intraclass</b> <b>variance,</b> a mean filter was applied to all the spectral bands and neo-channels used in the classification process. Training and validation areas were selected and digitized using detailed geographical databases of the tree species. The selection of the relevant bands and neo-channels was carried out by successive addition of information in order to improve the classification results. Seven different tree species of one to two different age classes were identified with an overall accuracy of 86 percent. The seven identified tree species or species groups are Oaks (Quercus sp.), Beech (Fagus sylvatica L.), Purple Beech (Fagus sylvatica purpurea), Douglas Fir (Pseudotsuga menziesii (Mirb.) Franco), Scots Pine (Pinus sylvestris L.), Corsican Pine (Pinus nigra Arn. subsp. laricio (Poir.) Maire var. corsican), and Larch (Larix decidua Mill.) ...|$|E
40|$|Background: For Comparative Effectiveness Research (CER) {{there is}} a need to develop scales for {{appraisal}} of available clinical research. Aims were to 1) test the feasibility of applying the pragmatic-explanatory continuum indicator summary tool and the six CER defining characteristics of the Institute of Medicine to RCTs of acupuncture for treatment of low back pain, and 2) evaluate the extent to which the evidence from these RCTs is relevant to clinical and health policy decision making. Methods: We searched Medline, the AcuTrialsTM Database to February 2011 and reference lists and included full-report randomized trials in English that compared needle acupuncture with a conventional treatment in adults with non-specific acute and/or chronic low back pain and restricted to those with $ 30 patients in the acupuncture group. Papers were evaluated by 5 raters. Principal Findings: From 119 abstracts, 44 full-text publications were screened and 10 trials (4, 901 patients) were evaluated. Due to missing information and initial difficulties in operationalizing the scoring items, the first scoring revealed inter-rater and inter-item <b>variance</b> (<b>intraclass</b> correlations 0. 02 – 0. 60), which improved after consensus discussions to 0. 20 – 1. 00. The 10 trials were found to cover the efficacy-effectiveness continuum; those with more flexible acupuncture and no placebo control scored closer to effectiveness...|$|R
40|$|Lung sounds {{provide useful}} {{information}} for assessing and monitoring respiratory patients, but standard auscultation is subjective. Computer aided lung sound analysis (CALSA) enables the quantification and characterisation of added lung sounds (e. g. crackles). At present, {{little is known}} about the reliability of these sound characteristics. Therefore, the aim of this study was to explore the reliability of crackle initial deflection width (IDW) and two-cycle deflection (2 CD) in a clinical population. Fifty-four subjects (37 bronchiectasis, 17 cystic fibrosis) were recruited from out-patient clinics. Three repeated lung sound recordings were taken at seven anatomical sites with a digital stethoscope connected to a laptop computer. The intra-subject reliability of crackle IDW and 2 CD was found to be 'good' to 'excellent', estimated by the analysis of <b>variance,</b> <b>intraclass</b> correlation coefficient (IDW 0. 76; 0. 85, 2 CD 0. 83; 0. 94), Bland and Altman 95 % limits of agreement (IDW ? 0. 50; 0. 47 ms, 2 CD ? 2. 12; 1. 87 ms) and smallest real difference (IDW 0. 30; 0. 66 ms, 2 CD 1. 57; 2. 42 ms). Crackle 2 CD was found to be more reliable than IDW. It is concluded that crackle IDW and 2 CD characterized by CALSA have good test–retest reliability. This technique requires further evaluation since CALSA has potential to diagnose or monitor respiratory conditions, and provide an objective physiological measure for respiratory interventions. <br/...|$|R
40|$|Sample size {{calculation}} {{for treatment}} effects in randomized trials with fixed cluster sizes and heterogeneous <b>intraclass</b> correlations and <b>variances</b> Math JJM Candel 1 and Gerard JP Van Breukelen 1 When comparing two {{different kinds of}} group therapy or two individual treatments where patients within each arm are nested within care providers, clustering of observations may occur in both arms. The arms may differ in terms of (a) the intraclass correlation, (b) the outcome variance, (c) the cluster size, and (d) the number of clusters, {{and there may be}} some ideal group size or ideal caseload in case of care providers, fixing the cluster size. For this case, optimal cluster numbers are derived for a linear mixed model analysis of the treatment effect under cost constraints as well as under power constraints. To account for uncertain prior knowledge on relevant model parameters, also maximin sample sizes are given. Formulas for sample size calculation are derived, based on the standard normal as the asymptotic distribution of the test statistic. For small sample sizes, an extensive numerical evaluation shows that in a two-tailed test employing restricted maximum likelihood estimation, a safe correction for both 80 % and 90 % power, is to add three clusters to each arm for a 5 % type I error rate and four clusters to each arm for a 1 % type I error rate...|$|R
40|$|Intensity {{modulated}} {{radiation therapy}} (IMRT) {{is a modern}} cancer therapy technique that aims to deliver a highly conformal radiation dose to a target tumor while sparing the surrounding normal tissues. The prescribed dose is specified by an intensity map (IM) matrix and often delivered by a multileaf collimator (MLC). In this thesis, we study a set of combinatorial optimization problems arising {{in the field of}} IMRT: 1) the auto-contouring problems using region properties, which aim to optimize the <b>intraclass</b> <b>variance</b> of the target objects; 2) the field decomposition problems, whose goal is to decompose a 2 ̆ 2 complex 2 ̆ 2 IM to the sum of two 2 ̆ 2 simpler 2 ̆ 2 sub-IMs such that the two sub-IMs are delivered in orthogonal directions to improve the delivery efficiency; 3) the field splitting problems, which seek to split a large IM that can not be directly delivered by MLC into several separate sub-IMs of size no larger than the given MLC size and the delivery effectiveness is optimized. Our algorithms are based on combinatorial techniques - mostly graph-based algorithms. We strive to find the globally optimal solution efficiently - in a linear or low polynomial time. In the case that the exact algorithm is not efficient enough, an approximation algorithm is also developed for solving the problem. We have implemented all the proposed algorithms and experimented on computer-generated phantoms and clinical data. Comparing with results supervised by experts, the auto-contouring algorithms yield highly accurate results for all tested datasets. The field decomposition and field splitting methods produce treatment plans of much better quality while comparing with the state-of-the-art commercial treatment planning system...|$|E
30|$|Recently, {{scholars}} have achieved various results in addressing this problem facing ancient murals by using intelligent protection. Izzo et al. [5] conducted in-depth research on various materials in Italian fresco painting techniques from different ages, specifically focusing on two main cases, i.e., the murals of Mario Sironi and Edmondo Bacci in Venice, and aimed at understanding the protection {{needs of these}} paintings and developing a sustainable protection plan. Sakr et al. [6] studied the influence of and solutions to streptomyces discoloration in mural paintings from ancient Egyptian tombs, thereby laying {{the foundation for the}} digital protection of ancient murals. Abdel-Haliem et al. [7] isolated and identified the streptomycetes that mainly affect the discoloration of paintings in ancient Egyptian tombs, thus providing new ideas for the removal of streptomyces. Li et al. [8] investigated the effectiveness of reinforcing mural paintings at the Mogao Grottoes in Dunhuang, China, by conducting on-site inspections to determine if new murals that had been previously repaired presented new forms of deterioration. These studies focused on ancient murals from the perspectives of aesthetics, biology, and chemistry, proposed the need for mural protection and laid the foundation for the protection of ancient murals using intelligent information processing technologies. In research on digital protection processes for ancient murals, Wan [9] studied ancient Chinese Dunhuang murals and designed a pigment color prototype that facilitates the digital conservation of ancient murals. Zhang et al. [10] developed a simulation platform for morphology analysis of ancient mural deterioration. By processing the surfaces of digital murals, Zhang et al. obtained three-dimensional data on murals and proposed a quantitative analysis method to analyze micro changes in the murals. Ren et al. [11] proposed a color clustering and masking algorithm to segment and extract damaged Dunhuang mural regions and designed a mural calibration and repair algorithm for fading and scratches. Tang et al. [12] proposed a method of clustered multi-instance learning to classify mural images from different ages, therein attempting to address the characteristics of Dunhuang mural images, which present strong <b>intraclass</b> <b>variance</b> and background noise. Luo et al. [13] extracted the scale-invariant feature transform (SIFT) and color features of mural images and used support vector machines to automate the classification of mural deterioration in Dunhuang. Li et al. [14] designed a visual analysis tool based on risk management to study single or multiple risks at the scales of sites, caves, walls, and specific risk areas from different perspectives at Mogao Grottoes in Dunhuang for facilitating computer intelligent repair processing. For the automatic calibration of murals, Wang et al. [15] extracted crack information using a top-hat transformation from the images of Tang dynasty mural paintings to realize automatic recognition and calibration. However, this method relied heavily on the selection of structural elements and thus experienced an overcalibration problem. Li et al. [16] proposed an automatic calibration method for mud spot disease in addressing the burial mural erosion problem. However, the texture feature used in this method is only ideal for the calibration of mud spot damage and experiences the problem of insufficient calibration for flaking deterioration. Wu et al. [17] improved the algorithm in [15], increased the scale of the structural elements, and used multiscale calibration of the deterioration; however, the analysis and calculation process of this algorithm is complex, and the time complexity is high. Huang et al. [18] proposed a multipath convolutional neural network (CNN) algorithm to detect erosion damage areas in 89 groups of ancient murals. This study reports the latest research results for the intelligent processing of ancient murals using machine learning methods. Scholars studying the intelligent processing of ancient murals are still in the beginning and exploration stages of this field.|$|E
40|$|Abstract – Photogrammetry is a {{valuable}} tool for the diagnosis and measurement of postural changes, {{but the lack of}} standardization of anatomical references and angular measures impairs the comparison between studies and compromises the reliability of the results. The objective {{of this study was to}} evaluate the inter- and intraexaminer reliability of angular measures proposed by the SAPO posture assessment software (v. 0. 68). Twenty [...] four subjects were photographed in the standing position according to the recommenda-tions of the SAPO software. Three examiners (A, B and C) experienced in the use of the software analyzed the images and repeated the analysis after 7 days. <b>Variance,</b> <b>intraclass</b> correlation coefficient (ICC), and t-test adopting a level of significance of 5 % were applied. With respect to interexaminer reliability among the 20 angles measured, two were classified as unacceptable (A 13 : ICC = 0. 623; A 14 : ICC = 0. 568), one as acceptable (A 19 : ICC = 0. 743), one as very good (A 20 : ICC = 0. 860), and 16 as excellent (ICC ≥ 0. 90). Evaluation of repeatability of the method by the same examiner showed that two angles measured by examiner A differed significantly between the two measurements (A 11 : p = 0. 015; A 12 : p = 0. 026), as did two angles measured by examiner B (A 2 : p = 0. 019; A 12 : p = 0. 015) and one angle measured by examiner C (A 16, p = 0. 011). In conclusion, comparison betwee...|$|R
40|$|For Comparative Effectiveness Research (CER) {{there is}} a need to develop scales for {{appraisal}} of available clinical research. Aims were to 1) test the feasibility of applying the pragmatic-explanatory continuum indicator summary tool and the six CER defining characteristics of the Institute of Medicine to RCTs of acupuncture for treatment of low back pain, and 2) evaluate the extent to which the evidence from these RCTs is relevant to clinical and health policy decision making. We searched Medline, the AcuTrials™ Database to February 2011 and reference lists and included full-report randomized trials in English that compared needle acupuncture with a conventional treatment in adults with non-specific acute and/or chronic low back pain and restricted to those with ≥ 30 patients in the acupuncture group. Papers were evaluated by 5 raters. From 119 abstracts, 44 full-text publications were screened and 10 trials (4, 901 patients) were evaluated. Due to missing information and initial difficulties in operationalizing the scoring items, the first scoring revealed inter-rater and inter-item <b>variance</b> (<b>intraclass</b> correlations 0. 02 - 0. 60), which improved after consensus discussions to 0. 20 - 1. 00. The 10 trials were found to cover the efficacy-effectiveness continuum; those with more flexible acupuncture and no placebo control scored closer to effectiveness. Both instruments proved useful, but need further development. In addition, CONSORT guidelines for reporting pragmatic trials should be expanded. Most studies in this review already reflect the movement towards CER and similar approaches can be taken to evaluate comparative effectiveness relevance of RCTs for other treatments...|$|R
40|$|Photogrammetry is a {{valuable}} tool for the diagnosis and measurement of postural changes, {{but the lack of}} standardization of anatomical references and angular measures impairs the comparison between studies and compromises the reliability of the results. The objective {{of this study was to}} evaluate the inter- and intraexaminer reliability of angular measures proposed by the SAPO posture assessment software (v. 0. 68). Twenty-four subjects were photographed in the standing position according to the recommendations of the SAPO software. Three examiners (A, B and C) experienced in the use of the software analyzed the images and repeated the analysis after 7 days. <b>Variance,</b> <b>intraclass</b> correlation coefficient (ICC), and t-test adopting a level of significance of 5 % were applied. With respect to interexaminer reliability among the 20 angles measured, two were classified as unacceptable (A 13 : ICC = 0. 623; A 14 : ICC = 0. 568), one as acceptable (A 19 : ICC = 0. 743), one as very good (A 20 : ICC = 0. 860), and 16 as excellent (ICC ≥ 0. 90). Evaluation of repeatability of the method by the same examiner showed that two angles measured by examiner A differed significantly between the two measurements (A 11 : p = 0. 015; A 12 : p = 0. 026), as did two angles measured by examiner B (A 2 : p = 0. 019; A 12 : p = 0. 015) and one angle measured by examiner C (A 16, p = 0. 011). In conclusion, comparison between different examiners showed that the angles proposed by the SAPO protocol are reliable for the measurement of body segments...|$|R
40|$|OBJECTIVE: The {{present study}} {{evaluated}} {{the reliability and}} concurrent validity of the new Tanaka B Intelligence Scale, which is an intelligence test that can be administered on groups within {{a short period of}} time. METHODS: The new Tanaka B Intelligence Scale and Wechsler Intelligence Scale for Children-Third Edition were administered to 81 subjects (mean age ± SD 15. 2 ± 0. 7 years) residing in a juvenile detention home; reliability was assessed using Cronbach's alpha coefficient, and concurrent validity was assessed using the one-way analysis of <b>variance</b> <b>intraclass</b> correlation coefficient. Moreover, receiver operating characteristic analysis for screening for individuals who have a deficit in intellectual function (an FIQ< 70) was performed. In addition, stratum-specific likelihood ratios for detection of intellectual disability were calculated. RESULTS: The Cronbach's alpha for the new Tanaka B Intelligence Scale IQ (BIQ) was 0. 86, and the intraclass correlation coefficient with FIQ was 0. 83. Receiver operating characteristic analysis demonstrated an area under the curve of 0. 89 (95 % CI: 0. 85 - 0. 96). In addition, the stratum-specific likelihood ratio for the BIQ≤ 65 stratum was 13. 8 (95 % CI: 3. 9 - 48. 9), and the stratum-specific likelihood ratio for the BIQ≥ 76 stratum was 0. 1 (95 % CI: 0. 03 - 0. 4). Thus, intellectual disability could be ruled out or determined. CONCLUSION: The present results demonstrated that the new Tanaka B Intelligence Scale score had high reliability and concurrent validity with the Wechsler Intelligence Scale for Children-Third Edition score. Moreover, the post-test probability for the BIQ could be calculated when screening for individuals who have a deficit in intellectual function. The new Tanaka B Intelligence Test is convenient and can be administered within a variety of settings. This enables evaluation of intellectual development even in settings where performing intelligence tests have previously been difficult...|$|R
40|$|A {{barrier to}} {{assessing}} {{the effectiveness of}} respiratory physiotherapy has been insufficient accurate, reliable and sensitive outcome measures. Lung sounds provide useful, specific information for assessing and monitoring respiratory patients. However, standard auscultation techniques are too subjective {{to allow them to}} be used as an outcome measure. In this research, Computer Aided Lung Sound Analysis (CALSA) was used to assess whether adventitious lung sounds’ characteristics could be quantified clinically and used as a new objective, non-invasive, bedside clinical outcome measure for physiotherapy alveolar recruitment and airway clearance techniques. Two experimental studies were conducted incorporating ‘before-and-after’ and ‘repeated measures’ components. Fifty four participants with productive lung disorders (cystic fibrosis and bronchiectasis) were recruited from out-patient clinics. Demographic, anthropometric, lung function, oxygen saturation, breathlessness and lung sound data were collected at baseline and after a single intervention (selfintervention in the first study and intervention applied by a physiotherapist in the second study). The intra-subject reliability of crackle frequency (f) within each session was found to be ‘good’ to ‘excellent’, estimated by the Analysis of <b>Variance,</b> <b>Intraclass</b> Correlation Coefficient, Smallest Real Difference and Bland and Altman 95 % limits of agreement. Crackle initial deflection width (IDW) and crackle two cycles deflection width (2 CD) were reliable over short time periods. The f of crackles increased in the majority of participants post interventions. Agreement on the number (N) and timing (T) of crackles between CALSA and a physiotherapist‘s auscultatory findings was found to be poor in anterior chest sites, but higher in posterior sites. Conclusion: the use of CALSA to identify the type and f of adventitious lung sounds collected clinically is feasible; crackle IDW and 2 CD are both reliable measures but crackle 2 CD is more consistent; crackle f was more responsive than the N or T of crackles per breathing cycle to the interventions. In future, CALSA may provide an objective and responsive tool for assessing and monitoring respiratory interventions in clinical settings. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|Objective To compare three simple {{methods of}} body {{composition}} {{analysis for the}} assessment of body fat in patients on long-term hemodialysis therapy. Design Cross-sectional study using the skinfold thickness, bioelectrical impedance analysis, and near-infrared interactance techniques after a hemodialysis session. Subjects/setting Ninety clinically stable patients (57 male/ 33 female) undergoing hemodialysis at the Dialysis Unit of the Federal University of São Paulo. Statistical analysis Analysis of <b>variance,</b> <b>intraclass</b> correlation coefficient, and Bland-Altman plot analysis were used for the comparative analysis between the methods. Results Body fat measurements obtained by skinfold thickness (13. 5 +/- 6. 2 kg) and bioelectrical impedance analysis (13. 7 +/- 6. 7 kg) were similar, whereas those measured by near-infrared interactance (11. 3 +/- 5. 1 kg) were significantly lower in comparison with skinfold thickness and bioelectrical impedance analysis (P <. 001). the strongest intraclass correlation coefficient was found between bioelectrical impedance analysis and skinfold thickness (r= 0. 87), and near-infrared interactance vs skinfold thickness and bioelectrical impedance analysis methods yielded r= 0. 78 and r= 0. 76, respectively. Near-infrared interactance showed a progressive underestimation of body fat values in comparison with the bioelectrical impedance analysis technique in patients with higher amount of adiposity. Conclusion in our study, we cannot consider that one method of body composition analysis is more accurate than the other because we did not apply a gold standard method. However, the most simple, long-established, and inexpensive method of skinfold thickness seems to be still very useful to the dietitians' routine for assessing body fat in patients on long-term hemodialysis therapy. Universidade Federal de São Paulo, Div Nephrol, São Paulo, BrazilUniversidade Federal de São Paulo, Nutr Program, São Paulo, BrazilUniversidade Federal de São Paulo, Dialysis Unit, São Paulo, BrazilUniversidade Federal de São Paulo, Renal Res Nutr Grp, São Paulo, BrazilUniversidade Federal de São Paulo, Div Nephrol, São Paulo, BrazilUniversidade Federal de São Paulo, Nutr Program, São Paulo, BrazilUniversidade Federal de São Paulo, Dialysis Unit, São Paulo, BrazilUniversidade Federal de São Paulo, Renal Res Nutr Grp, São Paulo, BrazilWeb of Scienc...|$|R

