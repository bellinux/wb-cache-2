667|31|Public
25|$|Research {{has shown}} that sirens mounted behind the engine grill or under the wheel arches {{produces}} less unwanted noise inside the passenger cabin and {{to the side and}} rear of the vehicle while maintaining noise levels to give adequate warnings. The inclusion of broadband sound to sirens has the ability to increase localisation of sirens, as in a directional siren, as a spread of frequencies makes use of the three ways the brain detects a direction of a sound: Interaural level difference, <b>interaural</b> <b>time</b> difference and head-related transfer function.|$|E
2500|$|Wilbanks, W.A.; Blodgett, H.C.; Jeffress, L.A. (1954), [...] "The {{effect of}} large <b>interaural</b> <b>time</b> {{differences}} upon {{the judgment of}} sidedness," [...] Journal of the Acoustical Society of America 26: 945, , WOS:A1954UD61900129 ...|$|E
5000|$|A {{well-known}} {{example of}} TDOA is the <b>interaural</b> <b>time</b> difference. The <b>interaural</b> <b>time</b> {{difference is the}} difference in arrival time of a sound between two ears. The <b>interaural</b> <b>time</b> difference is given bywhere ...|$|E
40|$|Sound {{localization}} {{is mostly}} performed by {{two sets of}} cues: <b>interaural</b> cues (<b>time</b> and level differences) for the horizontal plane and spectral cues (peaks and notches) for the vertical plane. The spectral localization cues have higher frequencies (4 - 16 kHz) than the most important speech information (< 4 kHz) ...|$|R
2500|$|McFadden, D.; Jeffress, L.A.; Russell, W.E. (1973), [...] "Individual {{differences}} in sensitivity to <b>interaural</b> {{differences in}} <b>time</b> and level," [...] Perceptual and Motor Skills 37: 755, WOS:A1973R662300017, PM 4764506 ...|$|R
50|$|To date, {{there has}} been no {{evidence}} provided that any acoustic emissions are used for sound localization. Holophonics, like binaural recording, instead reproduces the <b>interaural</b> differences (arrival <b>time</b> and amplitude between the ears), as well as rudimentary head-related transfer functions (HRTF). These create the illusion that sounds produced in the membrane of a speaker emanate from specific directions.|$|R
5000|$|<b>Interaural</b> <b>Time</b> Difference (ITD) Sound {{from the}} right side reaches the right ear earlier than the left ear. The {{auditory}} system evaluates <b>interaural</b> <b>time</b> differences from: (a) Phase delays at low frequencies and (b) group delays at high frequencies.|$|E
5000|$|Tonotopic maps: <b>interaural</b> <b>time</b> difference, {{frequency}} tonotopic {{maps of the}} cochlea ...|$|E
5000|$|The MSO's main {{function}} is detection of <b>interaural</b> <b>time</b> difference (ITD) cues to binaural lateralization.|$|E
40|$|In {{this thesis}} a {{binaural}} CASA localization algorithm is {{developed for the}} implementation in a binaural hearing aid with downstream speech enhancement. Two binaural CASA localization algorithms, based on the Albani model, are proposed to enhance the localization performance in noisy and reverberant acoustic environments. The Albani model is extended with a zero-lag <b>interaural</b> coherence (IC) <b>time</b> window pre-selection, detection of multiple sources per time-window, coincidence detection between <b>interaural</b> level and <b>time</b> differences (ILD and ITD) and a lagged time window comparison, in the proposed extended Albani algorithm. A further addition to the proposed extended Albani algorithm with a binaural cue selector based on an inhibition process, is proposed in the extended Albani algorithm with cue selection by inhibition. Performed simulations show that the extended Albani algorithm performs the best in noisy situations up to a SNR level of - 12 dB and the extended Albani algorithm with cue selection by inhibition performs the best in reverberant situations up to a reverberation time of 2. 0 s. These proposed localization algorithms show a better performance than the present known CASA methods in both noise and reverberation. Laboratory for Acoustical Imaging and Sound ControlImaging Science & TechnologyApplied Science...|$|R
40|$|Bilateral {{cochlear}} implants (BiCIs) {{have succeeded}} in improving the spatial hearing performance of bilateral CI users, albeit with considerable variability across implantees. Limited success {{can be caused by}} an interaural mismatch of the place-of-stimulation that arises from electrode arrays being inserted at different depths in each cochlea. In comparison to subjective pairing methods such as pitch matching, one promising objective measure based on electrically evoked auditory brainstem responses (EABR), the binaural interaction component (BIC), could be used to optimize the interaural electrode pairing (IEP) in BiCIs. Matched interaural electrodes are expected to facilitate binaural functions such as binaural fusion, localization, or detection of signals in noise. An IEP system, currently under development for clinical research, is proposed. The system offers subjective and objective IEP methods for BiCI: a psychoacoustic test module for pitch ranking and <b>interaural</b> pulse <b>time</b> difference (IPTD) sensitivity, and a binaural and monaural EABR recording module to derive the BIC. Psychoacoustic and IEP measures from one implantee are presented...|$|R
40|$|Dysfunctions of {{the organ}} of hearing are a {{significant}} limitation {{in the performance of}} occupations that require its full efficiency (vehicle driving, army, police, fire brigades, mining). Hearing impairment is associated with poorer understanding of speech and disturbed sound localization that directly affects the worker’s orientation in space and his/her assessment of distance and location of other workers or, even most importantly, of dangerous machines. Testing sound location abilities is not a standard procedure, even in highly specialized audiological examining rooms. It {{should be pointed out that}} the ability to localize sounds which are particularly loud, is not directly associated with the condition of the hearing organ, but is rather considered an auditory function of a higher level. Disturbances in sound localization are mainly associated with structural and functional disturbances of the central nervous system and occur also in patients with normal hearing when tested with standard methods. The article presents different theories explaining the phenomenon of sound localization, such as <b>interaural</b> differences in <b>time,</b> <b>interaural</b> differences in sound intensity, monaural spectrum shape and the anatomical and physiological basis of these processes. It also describes methods of measurement of disturbances in sound localization which are used in Poland and around the world, also by the author of this work. The author analyzed accessible reports on sound localization testing in occupational medicine and the possibilities of using such tests in various occupations requiring full fitness of the organ of hearing...|$|R
5000|$|... #Caption: <b>Interaural</b> <b>Time</b> Difference (ITD) between {{left ear}} (top) and right ear (bottom). 100 ms white noise ...|$|E
5000|$|... #Caption: [...] <b>Interaural</b> <b>time</b> {{difference}} (ITD) between left (top) {{and right}} (bottom) ears. (sound source: 100 ms white noise from 90&deg; azimuth, 0&deg; elevation) ...|$|E
5000|$|Localization {{accuracy}} is 1 degree for sources {{in front of}} the listener and 15 degrees for sources to the sides. Humans can discern <b>interaural</b> <b>time</b> differences of 10 microseconds or less.|$|E
40|$|We {{introduce}} a new technique for the blind localization of several sound sources from two binaural signals. First, the binaural signals are organized as two-dimensional data where each sound source appears as a line. Second, the Hough transform is used to recognize these lines. The slopes of the lines give the mixing coefficients and directions of arrival (azimuths). Two variants of our technique are proposed, based on {{only one of the}} <b>interaural</b> level or <b>time</b> differences, respectively. Although a rapid comparison to a well-known localization method as well as promising results are shown, they are clearly not exhaustive and this paper should rather be regarded as a feasibility demonstration of the new technique. 1...|$|R
40|$|Bilateral {{cochlear}} implants aim {{to improve}} sound localization compared to monaural implants, among other potential benefits. Monaural cochlear implants should not support localization in the horizontal plane {{as there are}} no <b>interaural</b> level and <b>time</b> difference cues available, although some previous studies have suggested limited capability. As background to other studies of bilateral implantation, the localization abilities of 18 monaural cochlear implantees were investigated experimentally in an anechoic chamber, using various sound stimuli with different amounts of temporal information. The effects of head movement and reverberation were also investigated. Localization performance {{was found to be}} close to chance for all stimuli. It is confirmed that monaural cochlear implants are unable to support useful auditory sound localization, even when head movements are allowed...|$|R
40|$|International audienceIn {{this paper}} we propose a {{complete}} computational system for Auditory Scene Analysis. This time-frequency system localizes, separates, and spatializes an arbitrary number of audio sources given only binaural signals. The localization is based on recent research frameworks, where <b>interaural</b> level and <b>time</b> differences are combined to derive a confident direction of arrival (azimuth) at each frequency bin. Here, the power-weighted histogram constructed in the azimuth space is modeled as a Gaussian Mixture Model, whose parameter structure is revealed through a weighted Expectation Maximization. Afterwards, a bank of Gaussian spatial filters is configured automatically to extract the sources with significant energy accordingly to a posterior probability. In this frequency-domain framework, we also inverse a geometrical and physical head model to derive an algorithm that simulates a source as originating from any azimuth angle...|$|R
5000|$|Viete, S. and Peña, J. L. and Konishi, M. (1997) The {{effects of}} {{interaural}} intensity difference on theprocessing of <b>interaural</b> <b>time</b> {{difference in the}} owl’s nucleus laminaris. J. Neurosci. 17: 1815 - 1824.|$|E
50|$|Interaural Phase Difference (IPD) {{refers to}} the {{difference}} in the phase of a wave that reaches each ear, and is dependent on the frequency of the sound wave and the <b>interaural</b> <b>time</b> differences (ITD).|$|E
50|$|Projections {{from the}} spherical bushy cells give {{excitatory}} input to the lateral and medial {{parts of the}} superior olive. Again their very close synaptic coupling suggest {{a part in the}} role of processing <b>interaural</b> <b>time</b> differences.|$|E
40|$|We {{investigated}} the {{mechanisms by which}} the barn owl (Tyto alba) determines the azimuth and elevation of a sound source. Our measure of localizing ability was the accuracy with which the owl oriented its head to a sound source. When localizing tonal signals, the owl committed the smallest errors at frequencies between 4 and 8 kHz. The azimuthal component of these errors was frequency independent from 1 to 8 kHz, but the elevational component increased dramatically for frequencies below 4 kHz. The owl's mean error when localizing wide band noise was nearly three times less than its mean error when localizing the optimal frequency for tonal localization (6 kHz). Occluding the right ear caused the owl to orient below {{and to the left}} of the sound source; occluding the left ear caused it to orient above and to the right of the sound source. With ruff feathers (facial ruff) removed, the owl continued to localize sounds accurately in azimuth, but failed to localize sounds in elevation. We conclude from these results that the barn owl uses interaural comparisons of sound spectrum to determine the elevation of a sound source. Both <b>interaural</b> onset <b>time</b> and <b>interaural</b> spectrum are used to identify the azimuth of the sound source. If onset time is not available (as in a continuous sound), the owl can derive the azimuth of the source from interaural spectrum alone, but its spatial resolution is poorer...|$|R
40|$|In a two-microphone approach, <b>interaural</b> {{differences}} in <b>time</b> (ITD) and <b>interaural</b> {{differences in}} sound intensity (IID) {{have generally been}} used for sound source localization. But those cues are not effective for vertical localization in the median plane (direct front). For that purpose, spectral cues based on features of head-related transfer functions (HRTF) have been investigated, {{but they are not}} robust enough against signal variations and environmental noise. In this paper, we use a “profile ” as a cue while using a combination of pinna specially designed for vertical localization. The observed sound is converted into a profile containing information about reflections as well as ITD and IID data. The observed profile is decomposed into signal and noise by using template profiles associated with sound source locations. The template minimizing the residual of the decomposition gives the estimated sound source location. Experiments show this method can correctly provide a rough estimate of the vertical location even in a noisy environment...|$|R
40|$|To {{determine}} {{the direction of}} a sound source in space, animals must process a variety of auditory spatial cues, including <b>interaural</b> level and <b>time</b> differences, as well as changes in the sound spectrum caused by the direction-dependent filtering of sound by the outer ear. Behavioural deficits observed when primary auditory cortex (A 1) is damaged have led to the widespread view that A 1 may have an essential role in this complex computational task. Here we show, however, that the spatial selectivity exhibited by the large majority of A 1 neurons is well predicted by a simple linear model, which assumes that neurons additively integrate sound levels in each frequency band and ear. The success of this linear model is surprising, given that computing sound source direction is a necessarily nonlinear operation. However, because linear operations preserve information, our results are consistent with the hypothesis that A 1 may also form a gateway to higher, more specialized cortical areas...|$|R
50|$|Binaural cues are {{generated}} by the difference in hearing between {{the left and right}} ears. These differences include the <b>interaural</b> <b>time</b> difference (ITD) and the interaural intensity difference (IID). Binaural cues are used mostly for horizontal localization.|$|E
5000|$|The sound {{information}} {{arriving at the}} left and right ears causes inter-aural time differences and interaural level differences. These small variations allow the brain and auditory system to calculate the direction and distance of the sound sources from the listener. <b>Interaural</b> <b>time</b> difference ...|$|E
50|$|Spherical bushy cells project ipsilaterally to the LSO, bilaterally to {{the medial}} {{superior}} olive (MSO) and LNTB, and contralaterally to the VNTB and VNLL. The most important {{purpose of these}} projections {{seems to be to}} imbue the MSO and LSO with their <b>interaural</b> <b>time</b> and level sensitivities (respectively).|$|E
40|$|Monaural Cochlear Implantation is {{a widely}} {{accepted}} and effective way to enable deaf people to achieve excellent speech understanding in quiet. Recently, it has been started to implant bilaterally to provide patients with the advantages of binaural information. We have {{conducted a series of}} basic experiments to study the availability of binaural cues to a bilateral CI-user of interaurally unsynchronized Cochlear Implant (CI) processors. The measurements included just-noticeable-difference (JND) for <b>interaural</b> level and <b>time</b> difference (ILD and ITD), lateralization by means of ILD and ITD, and speech-recognition in noise under different interaural amplitude or phase relationships of speech and noise. In the latter experiment we studied the efficiency of binaural masking level differences. The results indicate that the bilateral CI-user tested in this study was highly sensitive to ILD, comparable to normal hearing listeners, but less sensitive and consistent to ITD. We observed a small but significant degree of binaural unmasking of speech in noise when signal and masker had different amplitude or phase relationships at the two ears...|$|R
40|$|The {{long-term}} {{goal of this}} project is to understand the neural mechanisms that mediate the ability of normal-hearing people to understand speech and localize sounds in complex acoustic environments comprising reverberation and competing sound sources. In the past year, we focused on two research projects: (1) Physiological and psychophysical studies of sound localization in reverberant environments (Aim 1); (2) Spatio-temporal representation of pitch in the auditory nerve and cochlear nucleus (Aim 2). Sound localization in reverberant environments: Relating neural responses to human perception Reverberation, which is ubiquitous in everyday listening environments, poses a challenge to sound localization. In reverberant rooms, acoustic reflections interfere with the direct sound, leading to pronounced fluctuations in <b>interaural</b> differences in <b>time</b> (ITD) and level (ILD) {{over the course of}} a stimulus. These effects become more severe as the distance from sound source to listener increases, which causes the ratio of direct to reverberant energy (D/R) to decrease. We conducted two parallel studies aimed at characterizing the influence of acoustic reflections occurring in typical classrooms on both the directional sensitivity of low-frequency ITD-sensitiv...|$|R
40|$|The {{science of}} how we use {{interaural}} differences to localise sounds has been studied {{for over a century}} and in many ways is well understood. But in many of these psychophysical experiments listeners are required to keep their head still, as head movements cause changes in <b>interaural</b> level and <b>time</b> differences (ILD and ITD respectively). But a fixed head is unrealistic. Here we report an analysis of the actual ILDs and ITDs that occur as people naturally move and relate them to gyroscope measurements of the actual motion. We used recordings of binaural signals in a number of rooms and listening scenarios (home, office, busy street etc). The listener's head movements were also recorded in synchrony with the audio, using a micro-electromechanical gyroscope. We calculated the instantaneous ILD and ITDs and analysed them over time and frequency, comparing them with measurements of head movements. The results showed that instantaneous ITDs were widely distributed across time and frequency in some multi-source environments while ILDs were less widely distributed. The type of listening environment affected head motion. These findings suggest a complex interaction between interaural cues, egocentric head movement and the identification of sound sources in real-world listening situations...|$|R
50|$|Most mammals are {{adept at}} resolving the {{location}} of a sound source using <b>interaural</b> <b>time</b> differences and interaural level differences. However, no such time or level differences exist for sounds originating along the circumference of circular conical slices, where the cone's axis lies along the line between the two ears.|$|E
5000|$|Based on {{previous}} binaural sound localization methods, a hierarchical fuzzy {{artificial neural network}} system combines <b>interaural</b> <b>time</b> difference(ITD-based) and interaural intensity difference(IID-based) sound localization methods for higher accuracy {{that is similar to}} that of humans. Hierarchical Fuzzy Artificial Neural Networks [...] were used with the goal of the same sound localization accuracy as human ears.|$|E
50|$|Hornbostel {{also contributed}} to the theory of {{binaural}} hearing, propose the theory of <b>interaural</b> <b>time</b> difference as the main cue, and developing sound localization devices (for finding the directions to artillery, aircraft, submarines, etc.) for the German war effort during World War I. With Max Wertheimer, he developed a directional listening device that they referred to as the Wertbostel.|$|E
40|$|The tuning of {{auditory}} spatial {{attention with}} respect to <b>interaural</b> level and <b>time</b> difference cues (ILDs and ITDs) was explored using a rhythmic masking release (RMR) procedure. Listeners heard tone sequences defining one of two simple target rhythms, interleaved with arhythmic masking tones, presented over headphones. There were two conditions, which differed only in the ILD of the tones defining the target rhythm: For one condition, ILD was 0 dB and the perceived lateral position was central, and for the other, ILD was 4 dB and the perceived lateral position was to the right; target tone ITD was always zero. For the masking tones, ILD was fixed at 0 dB and ITDs were varied, giving rise {{to a range of}} lateral positions determined by ITD. The listeners' task was to attend to and identify the target rhythm. The data showed that target rhythm identification accuracy was low, indicating that maskers were effective, when target and masker shared spatial position, but not when they shared only ITD. A clear implication is that at least within the constraints of the RMR paradigm, overall spatial position, and not ITD, is the substrate for auditory spatial attention...|$|R
40|$|At present commercially {{available}} bilateral cochlear implants (CIs) improve their users’ speech understanding in noise but they employ two independent speech processors that cannot provide accurate and appropriate <b>interaural</b> level and <b>time</b> differences as seen binaurally in normal hearing (NH) listeners. Previous work suggests that binaural cues are accessible to bilateral CI users when presented to single pairs of pitch-matched electrodes, but the scope was limited and the mechanisms remained unclear. In this study, binaural masking level differences (BMLDs) were measured in five bilateral Nucleus- 24 CI users over multiple pairs of pitch-matched electrodes. Average BMLD was 4. 6 ± 4. 9 dB, but large individual variability prevented significance (p= 0. 09). Considering just the 125 Hz condition, as in previous work, phase (N 0 S 0 vs N 0 Sπ) and electrode effects were significant. Compared with simulated bilateral CI users, actual bilateral CI users had proportionally higher thresholds for N 0 Sπ than N 0 S 0. Together {{the present results}} suggest that the performance gap in BMLDs between CI and NH listeners is not {{due to a lack of}} sufficient acoustic cues in the temporal envelope domain but to a true binaural deficit related to a central mechanism in deprived binaural processing...|$|R
40|$|Listeners {{detected}} <b>interaural</b> {{differences of}} <b>time</b> (ITDs) or level (ILDs) carried by single 4000 -Hz Gabor clicks (Gaussian-windowed tone bursts) and trains of 16 such clicks repeating at an interclick interval (ICI) of 2, 5, or 10 ms. In separate conditions, target interaural differences favored {{the right ear}} by a constant amount for all clicks (condition RR), attained their peak value at onset and diminished linearly to 0 at offset (condition R 0), or grew linearly from 0 at onset to a peak value at offset (condition 0 R). Threshold ITDs and ILDs were determined adaptively in separate experiments {{for each of these}} conditions and for single clicks. ITD thresholds were found to be lower for 16 -click trains than for single clicks at 10 -ms ICI, regardless of stimulus condition. At 2 -ms ICI, thresholds in RR and R 0 conditions were similar to single click thresholds at 2 -ms ICI; thresholds in the 0 R condition were significantly worse than for single clicks at 2 -ms ICI, consistent with strong rate-dependent onset dominance in listeners’ temporal weighting of ITD. ILD thresholds, in contrast, were predominantly unaffected by ICI, suggesting little or no onset dominance for ILD of high-rate stimuli...|$|R
