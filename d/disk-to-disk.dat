34|0|Public
5000|$|EVault Software for <b>disk-to-disk,</b> {{on-premises}} backup and recovery.|$|E
5000|$|<b>Disk-to-disk</b> volume copying between disks of {{different}} sizes and types.|$|E
50|$|Rdiff-backup is an {{open source}} product that {{provides}} reverse-delta <b>disk-to-disk</b> backups.|$|E
50|$|On Mac OS X, Time Machine {{provides}} a <b>disk-to-disk</b> backup solution using reverse-deltas.|$|E
50|$|Attix5 is {{an online}} <b>disk-to-disk</b> backup solution. Attix {{is one of}} the oldest managed online backup companies, {{powering}} SMBs and enterprises.|$|E
50|$|Vess storage {{subsystems}} {{are designed}} for small to medium business and organizations of all sizes. The Vess storage system is used in surveillance applications and can handle unstructured data, <b>disk-to-disk</b> backup.|$|E
50|$|Unitrends Inc. is a US-based company {{specializing in}} backup and {{business}} continuity. The company’s portfolio of virtual, physical, and cloud solutions provides adaptive protection and network-based <b>disk-to-disk,</b> vertically integrated Backup and Disaster Recovery appliances for data protection and disaster recovery.|$|E
50|$|Dataram RAMDisk {{software}} {{is used to}} create a virtual RAM drive, or block of memory, which your computer treats {{as if it were a}} disk drive. It can speed up internet load times and <b>disk-to-disk</b> activities, accelerate databases and reduce compile times. The {{software is}} offered in both freeware and licensed versions.|$|E
5000|$|The term [...] "disk-to-disk", or [...] "D2D", {{generally}} {{refers to}} <b>disk-to-disk</b> backup. With D2D, a computer hard disk is backed up to another hard disk {{rather than to}} a tape or floppy. D2D is often confused with virtual tape, but differs in that it enables multiple backup and recovery operations to simultaneously access the disk directly by using a true file system.|$|E
50|$|At SC06 (Tampa, FL), {{the team}} {{transferred}} an astronomy dataset at 8Gbit/s <b>disk-to-disk</b> from Chicago, IL to Tampa, FL using UDT. At SC08 (Austin, TX), the team demonstrated {{the use of}} UDT in a complex high-speed data transfer involving various distributed applications over a 120-node system, across four data centers in Baltimore, Chicago (2), and San Diego. At SC09 (Portland, OR), a collaborative team from NCDM, Naval Research Lab, and iCAIR showcased UDT-powered wide area data intensive cloud computing applications.|$|E
50|$|Current Isilon {{hardware}} platforms {{include the}} S-Series nodes, which support high-performance, process-intensive, high-transaction applications; the X-Series nodes, which support high-throughput and high-concurrency application needs; the NL-Series nearline storage nodes, which support archiving, disaster recovery and <b>disk-to-disk</b> backup needs; and the HD-Series nodes, which support large-scale, high-density deep archive storage needs, {{as well as}} disaster recovery and, when combined with other nodes, an enterprise data lake foundation. Isilon also offers A-Series Accelerator nodes, which scale cluster performance and data backup processes.|$|E
5000|$|BackupPC {{is a free}} <b>disk-to-disk</b> backup {{software}} suite with a web-based frontend. The cross-platform server will run on any Linux, Solaris, or UNIX-based server. No client is necessary, as the server is itself a client for several protocols that are handled by other services native to the client OS. In 2007, BackupPC was mentioned {{as one of the}} three most well known open-source {{backup software}}, even though {{it is one of the}} tools that are [...] "so amazing, but unfortunately, if no one ever talks about them, many folks never hear of them".|$|E
50|$|Windows XP {{includes}} {{technology from}} Roxio which {{allows users to}} directly burn files to a compact disc through Windows Explorer. Previously, end users had to install CD burning software. In Windows XP, CD and DVD-RAM (FAT32 only for DVD-RAM) burning has been directly integrated into the Windows interface. Data discs are created using the Joliet and ISO 9660 file systems and audio CDs using the Redbook standard. To prevent buffer underrun errors, Windows XP premasters a complete image of files to be burnt and then streams it to the disc burner. Users can burn files to a CD {{in the same way}} they write files to a floppy disk or to the hard drive via standard copy-paste or drag and drop methods. The burning functionality is also exposed as an API called the Image Mastering API. Windows XP's CD burning support does not do <b>disk-to-disk</b> copying or disk images, although the API can be used programmatically to do these tasks. Creation of audio CDs is integrated into Windows Media Player. Audio CDs are burnt using track-at-once mode. CD-RW discs can be quick erased.|$|E
40|$|The Yottabyte NetStorage(TM) Company, today {{announced}} a new world record for TCP <b>disk-to-disk</b> data transfer using the company's NetStorager(R) System. The record-breaking demonstration transferred 5 terabytes of data between Chicago, Il. to Vancouver, BC and Ottawa, ON, at a sustained average throughput of 11. 1 gigabits per second. Peak throughput exceeded 11. 6 gigabits per second, more than 15 -times faster than previous records for TCP transfer from <b>disk-to-disk</b> (1 page) ...|$|E
40|$|In {{order to}} measure {{database}} performance of computer systems, {{a collection of}} companies and universities defined three benchmarks in 1985, including a <b>disk-to-disk</b> sort of one million records, which became a standard for database systems. After the first official result for this benchmark in 1986, performance steadily improved each year thereafter. In 1996 the best systems for the DatamationSort were multiprocessors, and since 1997 clusters of workstations have dominated these <b>disk-to-disk</b> sorting benchmarks. We explore the benefits, performance limitations, and key architectural issues involved in achieving high performance in <b>disk-to-disk</b> sorting. The testbed for this work was the NCSA NT-Supercluster, a cluster of Pentium-II workstations running Windows NT and the HPVM software suite. We selected the MinuteSort benchmark, which requires the sorting of as many records as possible in one minute, as the performance benchmark because it offered the clearest opportunity for compari [...] ...|$|E
40|$|A most {{commonly}} used method to fabricate accelerating tubes is machining, followed by stacking of disks, and then bonding such as diffusion bonding and brazing. In acceler-ating structures fabricated {{with this kind of}} method, surface currents associatedwithmagnetic fields flow across <b>disk-to-disk</b> junctions. On the other hand, we focus on a quadrant-type structure, where no surface currents flow across any junctions although naive such structures have some disad-vantages. In this paper, based on our new ideas to overcome all of the disadvantages, we present a fabrication process of a quadrant-type X-band accelerating structure {{in the form of a}} single-cell test cavity [1], together with results on machin-ing of quadrants and test of electron beam welding (EBW) ...|$|E
40|$|We present our {{experiences}} {{in developing and}} tuning the performance of NOW-Sort, a parallel, <b>disk-to-disk</b> sorting algorithm. NOW-Sort currently holds two world records in databaseindustry standard benchmarks. Critical to the tuning process was the setting of expectations, which tell the programmer both where to tune and when to stop. We found three categories of useful tools: tools that help set expectations and configure the application to different hardware parameters, visualization tools that animate performance counters, and search tools that track down performance anomalies. All such tools must interact well with all layers of the underlying software (e. g., the operating system), {{as well as with}} applications that leverage modern OS features, such as threads and memory-mapped I/O...|$|E
40|$|A 100 Gbps {{network was}} {{established}} between the California Institute of Technology conference {{booth at the}} Super Computing 2011 conference in Seattle, Washington and the computing center at the University of Victoria in Canada. A circuit was established over the BCNET, CANARIE and Super Computing (SCInet) networks using dedicated equipment. The small set of servers at the endpoints used a combination of 10 GE and 40 GE technologies, and SSD drives for data storage. The configuration of the network and the server configuration are discussed. We will show that the system was able to achieve <b>disk-to-disk</b> transfer rates of 60 Gbps and memory-to-memory rates in excess of 180 Gbps across the WAN. We will discuss the transfer tools, disk configurations, and monitoring tools used in the demonstration...|$|E
40|$|We present Tigris, a {{high-performance}} computation and I/O substrate for clusters of workstations that is implemented entirely in Java. Tigris automatically balances resource load across the cluster as a whole, shielding applications from asymmetries in CPU, I/O, and network performance. This is accomplished {{through the use}} of a dataflow programming model coupled with a work-balancing distributed queue. To meet the performance challenges of implementing such a system in Java, Tigris relies on Jaguar, a system that enables direct, protected access to hardware resources, including fast network interfaces and disk I/O. Jaguar yields an order-of-magnitude performance boost over the Java Native Interface for Java bindings to system resources. We demonstrate the applicability of Tigris through a one-pass, parallel, <b>disk-to-disk</b> sort exhibiting high performance...|$|E
40|$|Following the {{successful}} design and fabrication of Damped Detuned Structures (DDS), the JLC/NLC linear collider project advanced to Rounded Damped Detuned Structures (RDDS) with curved {{cross section of}} the cavity shape for increased shunt impedance. Various advanced techniques for fabricating RDDS 1 disks comparing to those for DDS were established to satisfy the dimension accuracy of +- 1 micron over the entire surface made by ultra-precision turning. These disks were assembled with almost the same stacking and bonding jigs and processes as those of DDS 3 assembly. In consequence, the assembly showed little <b>disk-to-disk</b> misalignment within 1 micron before and after the process. Though, it had 200 micron smooth bowing, which was subsequently corrected as DDS 3, and flares at both ends. Comment: 3 page...|$|E
40|$|Ideally, it is {{desirable}} to design and manufacture a transformer winding that can render all its internal resonances non-excitable. This study examines the effectiveness of an interleaved winding in achieving this goal. While investigating its effectiveness, {{it led to the}} establishment of a much desired theoretical basis that reinforces the reasons put forward in the literature to explain internal insulation failures observed in interleaved windings used in extra high voltage (EHV) transformers. Numerical calculations along with experimental verification on actual transformer windings are presented. This study reveals that most of the natural frequencies that are normally non-excitable in the line and neutral current responses of an interleaved winding have been rendered excitable in the <b>disk-to-disk</b> voltages, thus, providing favourable conditions for insulation overstress because of resonant overvoltages. Prevalence of such a condition is an inherent characteristic of interleaved windings...|$|E
40|$|As {{network data}} rates scale-up, an {{understanding}} of how and why traffic characteristics change is needed. We study the characteristics of bulk data transfer TCP/IP flows in a fully-switched Gigabit Ethernet network. Both <b>disk-to-disk</b> and memory-to-memory transfers are studied. We investigate flow characteristics as a function of link speed, server load, operating system (Linux and WindowsNT), and number of simultaneous clients accessing a server. Using trace-based simulations, we find that there is great variability in the "friendliness" of a flow to a single-server queue in terms of packet losses. Flows on Gigabit Ethernet tend to cause greater packet losses for a given link utilization and buffer size under certain conditions. We also find some unexplained inefficiencies in some of the flows. We investigate methods of application-level traffic smoothing to improve flow characteristics. A prototype packet-spacing socket send function is developed to reduce packet losses. 1...|$|E
40|$|Growth and usage {{trends for}} large {{decision}} support databases {{indicate that there}} is a need for architectures that scale the processing power as the dataset grows. To meet this need, several researchers have recently proposed Active Disk architectures which integrate substantial processing power and memory into disk units. In this paper, we evaluate Active Disks for decision support databases. First, we compare the performance of Active Disks with that of existing scalable server architectures: SMP-based conventional disk farms and commodity clusters of PCs. Second, we evaluate the impact of several design choices on the performance of Active Disks. We focus on the performance impact of interconnect bandwidth, amount of disk memory and <b>disk-to-disk</b> communication architecture on decision support workloads. Our results show that for identical disks, number of processors and I/O interconnect, Active Disks provide better price/performance than both SMPbased conventional disk farms and commo [...] ...|$|E
40|$|In {{this paper}} we {{introduce}} {{and describe the}} highly concurrent xDFS file transfer protocol and examine its cross-platform and cross-language implementation in native code for both Linux and Windows in 32 or 64 -bit multi-core processor architectures. The implemented xDFS protocol based on xDotGrid. NET framework is fully compared with the Globus GridFTP protocol. We finally propose the xDFS protocol as a new paradigm of distributed systems for Internet services, and data-intensive Grid and Cloud applications. Also, we incrementally consider different developmental methods of the optimum file transfer systems, and their advantages and disadvantages. The vision of this paper tries as possible to minimize the overhead concerned with the file transfer protocol itself and to examine optimal software design patterns of that protocol. In all <b>disk-to-disk</b> tests for transferring a 2 GB file with or without parallelism, the xDFS throughput at minimum 30 % and at most 53 % was superior to the GridFTP. Comment: 25 pages, 20 figure...|$|E
40|$|Flat Datacenter Storage (FDS) is a high-performance, fault-tolerant, large-scale, locality-oblivious blob store. Using a novel {{combination}} of full bisection bandwidth networks, data and metadata striping, and flow control, FDS multiplexes an application’s large-scale I/O across the available throughput and latency budget of every disk in a cluster. FDS therefore makes many optimizations around data locality unnecessary. Disks also {{communicate with each}} other at their full bandwidth, making recovery from disk failures extremely fast. FDS is designed for datacenter scale, fully distributing metadata operations that might otherwise become a bottleneck. FDS applications achieve single-process read and write performance of more than 2 GB/s. We measure recovery of 92 GB data lost to disk failure in 6. 2 s and recovery from a total machine failure with 655 GB of data in 33. 7 s. Application performance is also high: we describe our FDS-based sort application which set the 2012 world record for <b>disk-to-disk</b> sorting. ...|$|E
40|$|The GridFTP {{extensions}} to the File Transfer Protocol {{define a}} general-purpose mechanism for secure, reliable, high-performance data movement. We report {{here on the}} Globus striped GridFTP framework, a set of client and server libraries designed to support the construction of data-intensive tools and applications. We describe the design of both this framework and a striped GridFTP server constructed within the framework. We show that this server is faster than other FTP servers in both single-process and striped configurations, achieving, for example, speeds of 27. 3 Gbit/s memory-to-memory and 17 Gbit/s <b>disk-to-disk</b> over a 60 millisecond round trip time, 30 Gbit/s network. In another experiment, we show that the server can support 1800 concurrent clients without excessive load. We argue that this combination of performance and modular structure make the Globus GridFTP framework both a good foundation {{on which to build}} tools and applications, and a unique testbed for the study of innovative data management techniques and network protocols. ...|$|E
40|$|Abstract: The {{continued}} growth {{of data and}} high-continuity of application have raised a critical and mounting demand on storage-efficient and high-performance data protection. New technologies, especially the D 2 D (<b>Disk-to-Disk)</b> de-duplication storage are therefore getting wide attention both in academic and industry in the recent years. Existing de-duplication systems mainly rely on duplicate locality inside the backup workload to achieve high throughput but suffer from read performance degrading under conditions of poor duplicate locality. This paper presents the design and perform-ance evaluation of a D 2 D-based de-duplication file backup system, which employs caching techniques to improve write throughput while encoding files as graphs called BP-DAGs (Bi-pointer-based Directed Acyclic Graphs). BP-DAGs not only satisfy the 'unique ' chunk storing policy of de-duplication, but also help improve file read performance in case of poor duplicate locality workloads. Evaluation {{results show that the}} system can achieve comparable read performance than non de-duplication backup systems such as Bacula under representative workloads, and the metadata storage overhead for BP-DAGs are reasonably low...|$|E
40|$|We {{describe}} the porting, redesign, and tuning {{of a high}} performance <b>disk-to-disk</b> parallel sort on a general purpose Myrinet connected PC cluster running Windows NT. This cluster employs the high speed communication of Fast Messages (from the HPVM system). The study exposes the performance limitations and key architectural issues in achieving high performance. Key distinguishing features of our effort {{include the use of}} MPI, Windows NT and the HPVM software. Our effort builds on the Berkeley NOWSort, adapted and tuned for an HPVM NT-cluster. Using 60 nodes, we sorted 10. 3 GB in 56. 51 seconds breaking the fastest published Minute sort record of which we are aware 1. Each machine in our cluster consisted of dual processor, 300 MHz Pentium II's, with 512 MB of memory, making possible the use of a one pass sort, and a single 4. 5 GB 10000 RPM disks. 1 Introduction Clusters of commodity PC's or workstations have achieved success and popularity in the academic world and most recently in in [...] ...|$|E
40|$|Abstract—Motivated by {{the need}} to {{distribute}} large volumes of scientific data to large numbers of subscribers, we propose a reliable multicast transport protocol. Specifically, this protocol is developed for use on virtual circuits, since dynamic circuit services are now being offered by large providers, and virtual circuit networking is well suited to multicasting as it eliminates the data-plane congestion control problem of connectionless IP. The new protocol is called Virtual Circuit Multicast Transport Protocol (VCMTP). A key concept is to execute retransmissions (required due to flow control problems) at the end, i. e., after the message (file or memory data) is multicast. This leads to scalability, where the throughput for the receivers that can keep pace with the sending rate is independent of the number of receivers. A prototype was tested on Emulab, and measurements obtained. Our findings are that for <b>disk-to-disk</b> file transfers at a sending rate of 600 Mbps, the Emulab hosts can support multicasting with less than 0. 5 % retransmission rates, but at a 800 Mbps sending rate, the average throughput is only 650 Mbps because the retransmission rate increases to 9 %. Index Terms—Reliable multicast; transport protocols; high speed; virtual circuits I...|$|E
40|$|Abstract. In this study, CrZrN {{film was}} {{deposited}} on AISI 4130 cylinder barrel and valve plate using a unbalanced magnetron sputtering. The tribological properties between a coated steel cylinder barrel and coated steel valve plate {{were evaluated by}} a custom-built <b>disk-to-disk</b> tribometer {{and the results were}} compared with those between a coated steel cylinder barrel and a conventional bronze valve plate. The lowest friction coefficients were observed between a coated steel cylinder barrel and coated steel valve plate and they were approximately one half of those between a coated steel cylinder barrel and conventional bronze valve plate in the range of testing up to 1600 rpm. Coated valve plate and bronze valve plate showed a steady state of friction coefficient at approximately 0. 03 and 0. 06, respectively after about 1000 rpm, but steel valve plate without coating showed a continuous decrease in friction coefficient, which suggested that active wear is continuously occurring. By applying CrZrN coating on the commercial cylinder barrel and steel valve plate, much improved tribological results could be obtained, by a factor of approximately 2 in terms of friction coefficient than those from bronze valve plate. Under a tap water environment CrZrN coating appeared to perform better than CrSiN coating. Although further work to reduce the friction coefficient below 0. 13 in the steady state after initial drop is still needed, possibilities of applying CrZrN coating to realize a water hydraulic pump were demonstrated through the present work...|$|E
40|$|We {{present a}} novel process flow {{enabling}} prototyping of microfluidic cartridges {{made out of}} polymer films. Its high performance is proven by implementation of a microfluidic genotyping assay testing 22 DNA samples including clinical isolates from patients infected by methicilin-resistant Staphylococcus aureus (MRSA). The microfluidic cartridges (disks) are fabricated by a novel process called microthermoforming by soft lithography (microTSL). Positive moulds are applied allowing for higher moulding precision and very easy demoulding when compared to conventional microthermoforming. High replication accuracies with geometric <b>disk-to-disk</b> variations of less than 1 % are typical. We describe and characterise fabrication and application of microfluidic cartridges with wall thicknesses < 188 microm thus enabling efficient thermocycling during real-time polymerase chain reaction (PCR). The microfluidic cartridges are designed for operation in a slightly modified commercial thermocycling instrument. This approach demonstrates new opportunities for both microfluidic developments and well-established laboratory instruments. The microfluidic protocol is controlled by centrifugal forces and divides the liquid sample parallely into independent aliquots of 9. 8 microl (CV 3. 4 %, N = 32 wells). The genotyping assays are performed with pre-stored primers and probes for real-time PCR showing a limit of detection well below 10 copies of DNA per reaction well (N = 24 wells in 3 independent disks). The system was evaluated by 44 genotyping assays comprising 22 DNA samples plus duplicates {{in a total of}} 11 disks. The samples contained clinical samples of seven different genotypes of MRSA as well as positive and negative controls. The results are in excellent agreement with the reference in microtubes...|$|E
40|$|DotGrid {{platform}} is a Grid infrastructure integrated {{with a set}} of open and standard protocols recently implemented on the top of Microsoft. NET in Windows and MONO. NET in UNIX/Linux. DotGrid infrastructure along with its proposed protocols provides a right and solid approach to targeting other platforms, e. g., the native C/C++ runtime. In this paper, we propose a new file transfer protocol called DotDFS as a high-throughput distributed file transfer component for DotGrid. DotDFS introduces some open binary protocols for efficient file transfers on current Grid infrastructures. DotDFS protocol also provides mechanisms for multiple file streams to gain high-throughput file transfer similar to GridFTP protocol, but by proposing and implementing a new parallel TCP connection-oriented paradigm. In our LAN tests, we have achieved better results than Globus GridFTP implementation particularly in multiple TCP streams and directory tree transfers. Our LAN experiences in memory-to-memory tests show that DotDFS accesses to the 94 % bottleneck bandwidth while GridFTP is accessing 91 %. In LAN <b>disk-to-disk</b> tests, comparing DotDFS protocol with GridFTP protocol unveils a set of interesting and technical problems in GridFTP for both the nature of the protocol and its implementation by Globus. In the WAN experimental studies, we propose a new idea for analytical modeling of file transfer protocols like DotDFS inspired by sampling, experimentation and mathematical interpolation approaches. The cross-platform and open standard-based features of DotDFS provide a substantial framework for unifying data access and resource sharing in real heterogeneous Grid environments. Comment: 28 pages, 21 figure...|$|E
40|$|NASA in {{collaboration}} {{with a number of}} partners conducted a set of individual experiments and demonstrations during SC 10 that collectively were titled "Using 100 G Network Technology in Support of Petascale Science". The partners included the iCAIR, Internet 2, LAC, MAX, National LambdaRail (NLR), NOAA and SCinet Research Sandbox (SRS) as well as the vendors Ciena, Cisco, ColorChip, cPacket, Extreme Networks, Fusion-io, HP and Panduit who most generously allowed some of their leading edge 40 G/ 100 G optical transport, Ethernet switch and Internet Protocol router equipment and file server technologies to be involved. The experiments and demonstrations featured different vendor-provided 40 G/ 100 G network technology solutions for full-duplex 40 G and 100 G LAN data flows across SRS-deployed single-node fiber-pairs among the Exhibit Booths of NASA, the National Center for Data lining, NOAA and the SCinet Network Operations Center, as well as between the NASA Exhibit Booth in New Orleans and the Starlight Communications Exchange facility in Chicago across special SC 10 - only 80 - and 100 -Gbps wide area network links provisioned respectively by the NLR and Internet 2, then on to GSFC across a 40 -Gbps link. provisioned by the Mid-Atlantic Crossroads. The networks and vendor equipment were load-stressed by sets of NASA/GSFC High End Computer Network Team-built, relatively inexpensive, net-test-workstations that are capable of demonstrating greater than 100 Gbps uni-directional nuttcp-enabled memory-to-memory data transfers, greater than 80 -Gbps aggregate [...] bidirectional memory-to-memory data transfers, and near 40 -Gbps uni-directional <b>disk-to-disk</b> file copying. This paper will summarize the background context, key accomplishments and some significances of these experiments and demonstrations...|$|E
40|$|Magnetic tape {{has long}} been used to backup {{computer}} information, customarily during dedicated, low use, 'overnight windows' when tape backup programs could be run without interfering with user applications. Over time these windows have shrunk to near non-existence due to the globalization of business through its use of the Internet and the World Wide Web. While tape system speeds have accelerated and tape capacities have increased, they have not {{keep pace with the}} demand for shorter backup windows, quicker restoration requirements, and the rapidly escalating volume of disk drive data being backed up. At the same time, the cost of PC disk drives ("ATA" computer interface) has dropped to be competitive with tape, and <b>disk-to-disk</b> (D 2 D) backup has become popular, particularly using the new Serial ATA (SATA) PC drives. A D 2 D system can run at the full speed of disk, and can use the higher capacity of PC disk drives (up to 400 Gbytes today), because backup is primarily serial data storage not needing the high high random access speed of the enterprise storage systems being backed up (which get high random access speed by running many smaller capacity disks in parallel). The current interest in SATA D 2 D backup is of concern because these are PC drives sold with narrow profit margins that limit drive reliability. This paper addresses the fact that the reliability of SATA PC drives is inherently lower than enterprise-class drives (SCSI and Fiber Channel "SCSI/FC"). Methods are proposed for SATA storage system designers to achieve high system level reliability by requiring appropriate SATA drive reliability testing, by increased RAID redundancy, and by system management of drive failure warnings. A method is proposed for maintaining user data security in removable D 2 D archival drives...|$|E

