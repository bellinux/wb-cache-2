68|64|Public
5|$|The Gain {{disk space}} section {{itself is a}} disk cleanup {{component}} which replaces Windows Disk Cleanup. It also gives access to TuneUp Disk Space Explorer (a disk space analyzer) and TuneUp Shredder (a <b>data</b> <b>erasure</b> tool).|$|E
5|$|Symantec {{announced}} a Professional Edition on November 19, 2002. Data recovery tools in this version allow users to recover deleted or malware-damaged files. The {{inclusion of a}} <b>data</b> <b>erasure</b> tool allows users to delete files while minimizing the chance of recovery. Web Cleanup removes browser cache files, history, and cookies. To maintain dial-up connections, Connection Keep Alive simulates online activity during periods of user inactivity. Norton Productivity Control enables users to filter Internet content and block newsgroups. When used with the User Access Manager, multiple filtering profiles can be created, assigned to different users.|$|E
5000|$|Blancco Technology Group is {{a global}} {{provider}} of mobile device diagnostics and secure <b>data</b> <b>erasure</b> products. The company {{is divided into two}} distinct business units: Blancco (<b>data</b> <b>erasure)</b> and SmartChk (mobile diagnostics). Of all <b>data</b> <b>erasure</b> solution providers, Blancco Technology Group holds the highest number of certifications, approvals, and recommendations from national and international governing bodies, including the United States Department of Defense, NATO, TUV Saarland and DIPCOG.|$|E
50|$|The {{traditional}} {{scheme for}} transferring <b>data</b> across an <b>erasure</b> channel depends on continuous two-way communication.|$|R
5000|$|The {{input data}} {{is divided into}} blocks. Blocks are {{sequences}} of bits that {{are all the same}} size. Recovery data uses the same block size as the input <b>data.</b> The <b>erasure</b> of a block (input or recovery) is detected by some other means. (For example, a block from disk does not pass a CRC check or a network packet with a given sequence number never arrived.) ...|$|R
40|$|Abstract—Reliability {{control is}} a key concern on the {{evolution}} of real-time mobile multicast services. To this direction the use of forward error correction (FEC) on the application layer is widely adopted in several mobile multicast standards. FEC is a feedback free error control method where the transmitter introduces in advance redundant information to enable receivers recovering <b>data</b> <b>erasures.</b> On multicast delivery and especially on the time constrained streaming delivery where retransmission-based error recovery methods are not feasible, the most suitable error control method is the use of application layer forward error correction (AL-FEC) codes. In this work, we introduce a novel AL-FEC deployment policy over mobile multicast standards utilizing online algorithms. We aim on the efficient application of AL-FEC protection with RaptorQ codes over multicast streaming delivery in the context of competitive analysis. We provide a competitiveness analysis model of AL-FEC application over mobile multicast real-time environments and we introduce an innovative online algorithm adjusting the introduced redundancy of AL-FEC protection according to the FEC encoding parameters in order to satisfy the individual constraints of a multicast streaming delivery. Keywords-forward error correction, RaptorQ codes, mobile multicast networks, real-time delivery, online algorithms, competitive analysis I...|$|R
50|$|Permanent <b>data</b> <b>erasure</b> {{goes beyond}} basic file {{deletion}} commands, which only remove direct pointers {{to the data}} disk sectors and make the data recovery possible with common software tools. Unlike degaussing and physical destruction, which render the storage media unusable, <b>data</b> <b>erasure</b> removes all information while leaving the disk operable. New flash memory - based media implementations, such as solid-state drives or USB flash drives can cause <b>data</b> <b>erasure</b> techniques to fail allowing remnant data to be recoverable.|$|E
50|$|<b>Data</b> <b>erasure</b> {{software}} {{should provide}} the user with a validation certificate {{indicating that the}} overwriting procedure was completed properly. <b>Data</b> <b>erasure</b> software should also comply with requirements to erase hidden areas, provide a defects log list and list bad sectors {{that could not be}} overwritten.|$|E
50|$|<b>Data</b> <b>erasure</b> is {{term that}} refers to {{software-based}} methods of preventing file undeletion.|$|E
40|$|Deep space {{communications}} over noisy channels lead {{to certain}} packets {{that are not}} decodable. These packets leave gaps, or bursts of <b>erasures,</b> in the <b>data</b> stream. Burst <b>erasure</b> correcting codes overcome this problem. These are forward erasure correcting codes that allow one to recover the missing gaps of data. Much of the recent work on this topic concentrated on Low-Density Parity-Check (LDPC) codes. These are more complicated to encode and decode than Single Parity Check (SPC) codes or Reed-Solomon (RS) codes, and so far {{have not been able}} to achieve the theoretical limit for burst erasure protection. A block interleaved maximum distance separable (MDS) code (e. g., an SPC or RS code) offers near-optimal burst erasure protection, in the sense that no other scheme of equal total transmission length and code rate could improve the guaranteed correctible burst erasure length by more than one symbol. The optimality does not depend on the length of the code, i. e., a short MDS code block interleaved to a given length would perform as well as a longer MDS code interleaved to the same overall length. As a result, this approach offers lower decoding complexity with better burst erasure protection compared to other recent designs for the burst erasure channel (e. g., LDPC codes). A limitation of the design is its lack of robustness to channels that have impairments other than burst erasures (e. g., additive white Gaussian noise), making its application best suited for correcting <b>data</b> <b>erasures</b> in layers above the physical layer. The efficiency of a burst erasure code is the length of its burst erasure correction capability divided by the theoretical upper limit on this length. The inefficiency is one minus the efficiency. The illustration compares the inefficiency of interleaved RS codes to Quasi-Cyclic (QC) LDPC codes, Euclidean Geometry (EG) LDPC codes, extended Irregular Repeat Accumulate (eIRA) codes, array codes, and random LDPC codes previously proposed for burst erasure protection. As can be seen, the simple interleaved RS codes have substantially lower inefficiency over a wide range of transmission lengths...|$|R
50|$|Specialized {{forms of}} Reed-Solomon codes, {{specifically}} Cauchy-RS and Vandermonde-RS, {{can be used}} to overcome the unreliable nature of <b>data</b> transmission over <b>erasure</b> channels. The encoding process assumes a code of RS(N, K) which results in N codewords of length N symbols each storing K symbols of data, being generated, that are then sent over an erasure channel.|$|R
40|$|Layered {{multiple}} description codes allow robust {{transmission of}} scalable media <b>data</b> over packet <b>erasure</b> networks, while providing simple rate adaptation and bandwidth savings for shared bottleneck links. We show how to efficiently design layered multiple description codes for multicast and broadcast applications in memoryless packet erasure networks. Our approach offers a significantly better quality tradeoff among clients {{than the best}} previous solutio...|$|R
5000|$|<b>Data</b> <b>erasure</b> utility BCWipe {{to erase}} {{unprotected}} copies {{of data to}} complement encryption.|$|E
5000|$|Certified <b>Data</b> <b>Erasure</b> Software & Hardware: see Blancco Mobile Device Diagnostics Software & Business Intelligence: see SmartChk ...|$|E
5000|$|It {{is a way}} of <b>data</b> <b>erasure</b> -- {{removing}} {{sensitive data}} from a disk, or disk partition.|$|E
40|$|Abstract—Information erasure is {{a formal}} {{security}} require-ment that stipulates when sensitive data {{must be removed}} from computer systems. In a system that correctly enforces erasure requirements, an attacker who observes the system after sensitive data is required to have been erased cannot deduce anything about the data. Practical obstacles to enforcing information erasure include: (1) correctly determining which data requires erasure; and (2) reliably deleting potentially large volumes of data, despite untrustworthy storage services. In this paper, we present a novel formalization of language-based information erasure that supports cryptographic enforce-ment of <b>erasure</b> requirements: sensitive <b>data</b> is encrypted be-fore storage, and upon erasure, only a relatively small set of decryption keys needs to be deleted. This cryptographic technique {{has been used by}} a number of systems that imple-ment data deletion to allow the use of untrustworthy storage services. However, these systems provide no support to correctly determine which <b>data</b> requires <b>erasure,</b> nor have the formal semantic properties of these systems been explained or proven to hold. We address these shortcomings. Specifically, we study a programming language extended with primitives for public-key cryptography, and demonstrate how information-flow control mechanisms can automatically track <b>data</b> that requires <b>erasure</b> and provably enforce erasure requirements even when programs employ cryptographic techniques for erasure. I...|$|R
40|$|International audienceModern storage systems now {{typically}} combine plain replication and erasure {{codes to}} reliably store {{large amount of}} data in datacenters. Plain replication allows a fast access to popular <b>data,</b> while <b>erasure</b> codes, e. g., Reed-Solomon codes, provide a storage-efficient alternative for archiving less popular <b>data.</b> Although <b>erasure</b> codes are now increasingly employed in real systems, they experience high overhead during maintenance, i. e., upon failures, typically requiring files to be decoded before being encoded again to repair the encoded blocks stored at the faulty node. In this paper, we propose a novel erasure code system, tailored for networked archival systems. The efficiency of our approach relies on the joint use of random codes and a clustered placement strategy. Our repair protocol leverages network coding techniques to reduce by 50 % the amount of data transferred during maintenance, by repairing several cluster files simultaneously. We demonstrate both through an analysis and extensive experimental study conducted on a public testbed that our approach significantly decreases both the bandwidth overhead during the maintenance process and the time to repair lost data. We also show that using a non-systematic code does not impact the throughput, and comes only {{at the price of}} a higher CPU usage. Based on these results, we evaluate the impact of this higher CPU consumption on different configurations of data coldness by determining whether the cluster's network bandwidth dedicated to repair or CPU dedicated to decoding saturates first. on different configurations of data coldness by determining whether the cluster's network bandwidth dedicated to repair or CPU dedicated to decoding saturates first...|$|R
40|$|Abstract — Layered {{multiple}} description codes allow robust {{transmission of}} scalable media <b>data</b> over packet <b>erasure</b> networks, while providing simple rate adaptation and bandwidth savings for shared bottleneck links. We show how to efficiently design layered multiple description codes for multicast and broadcast applications in memoryless packet erasure networks. Our approach offers a significantly better quality trade-off among clients {{than the best}} previous solution. I...|$|R
50|$|To {{protect the}} data on lost or stolen media, some <b>data</b> <b>erasure</b> {{applications}} remotely destroy the data if the password is incorrectly entered. <b>Data</b> <b>erasure</b> tools can also target specific data on a disk for routine erasure, providing a hacking protection method that is less time-consuming than software encryption. Hardware/firmware encryption built into the drive itself or integrated controllers is a popular solution with no degradation in performance at all.|$|E
50|$|While {{there are}} many {{overwriting}} programs, only those capable of complete <b>data</b> <b>erasure</b> offer full security by destroying the data on all areas of a hard drive. Disk overwriting programs that cannot access the entire hard drive, including hidden/locked areas like the host protected area (HPA), device configuration overlay (DCO), and remapped sectors, perform an incomplete erasure, leaving some of the data intact. By accessing the entire hard drive, <b>data</b> <b>erasure</b> eliminates the risk of data remanence.|$|E
50|$|DBAN, {{like other}} methods of <b>data</b> <b>erasure,</b> is {{suitable}} for use prior to computer recycling for personal or commercial situations, such as donating or selling a computer.|$|E
40|$|Canetti et al. [11] {{recently}} {{proposed a}} new framework — termed Generalized Universal Composability (GUC) — for properly analyzing concurrent execution of cryptographic protocols {{in the presence}} of a global setup. While arguing that none of the existing solutions achieved the desired level of security in the GUCframework, the authors constructed the first known GUC-secure implementations of commitment (GUCC) and zero-knowledge (GUC ZK), which suffice to implement any two-party or multi-party functionality under several natural and relatively mild setup assumptions. Unfortunately, the feasibility results of [11] used rather inefficient constructions: the commitment scheme was bit-by-bit, while the zero-knowledge proof for a relation R was implemented using the generic Cook-Levin reduction to a canonical NP-complete problem. In this paper, we dramatically improve the efficiency of (adaptively-secure) GUCC and GUC ZK assuming <b>data</b> <b>erasures</b> are allowed. Namely, using the same minimal setup assumptions as those used by [11], we build • a direct and efficient constant-round GUC ZK for R from any “dense ” Ω-protocol [31] for R. As a corollary, we get a semi-efficient construction from any Σ-protocol for R (without doing the Cook-Levin reduction), and a very efficient GUC ZK for proving the knowledge of discrete log representation. • the first constant-rate (and constant-round) GUCC scheme. Additionally, we show how to properly model a random oracle (RO) in the GUC framework without losing deniability, which is one of the attractive features of the GUC framework. As an application, by adding the random oracle to the setup assumptions used by [11], we build the first two-round (which we show is optimal), deniable, straight-line extractable and simulatable ZK proof for any NP relation R...|$|R
50|$|Wear {{leveling}} {{attempts to}} work around these limitations by arranging <b>data</b> so that <b>erasures</b> and re-writes are distributed evenly across the medium. In this way, no single erase block prematurely fails due to {{a high concentration of}} write cycles. In flash memory, a single block on the chip is designed for longer life than the others so that the memory controller can store operational data with less chance of its corruption.|$|R
40|$|Abstract—Despite {{the many}} {{advantages}} provided by cloud-based storage services, {{there are still}} major concerns such as security, reliability and confidentiality of data stored in the cloud. In this paper, we propose a reliable and secure cloud storage schema using multiple service providers. Different from existing approaches to achieving data reliability using redundancy at the server side, we propose a reliable and secure cloud storage schema that can be implemented at the client side. In our approach, we view multiple cloud-based storage services as virtual independent disks for storing redundant <b>data</b> encoded using <b>erasure</b> codes. Since each independent cloud service provider has no access to a user’s complete data, the data stored in the cloud would not be easily compromised. Furthermore, the failure or disconnection of a service provider will not result {{in the loss of}} a user’s data as the missing data pieces can be readily recovered. To demonstrate the feasibility of our approach, we developed a prototype cloud-based storage system that breaks a data file into multiple data pieces, generates an optimal number of checksum pieces, and uploads them into multiple cloud storages. Upon the failure of a cloud storage service, the application can quickly restore the original data file from the available pieces of data. The experimental results show that our approach is not only secure and fault-tolerant, but also very efficient due to concurrent data processing. Keywords-Cloud storage; reliability; <b>data</b> security; <b>erasure</b> codes; cloud service provider; integer linear programming. I...|$|R
50|$|The {{mentioned}} {{security issues}} are not specific to crypto-shredding, but apply in general to encryption. In addition to crypto-shredding, <b>data</b> <b>erasure,</b> degaussing and physically shredding the disk can mitigate the risk.|$|E
50|$|The Gain {{disk space}} section {{itself is a}} disk cleanup {{component}} which replaces Windows Disk Cleanup. It also gives access to TuneUp Disk Space Explorer (a disk space analyzer) and TuneUp Shredder (a <b>data</b> <b>erasure</b> tool).|$|E
50|$|In 1997, Janne Tervo and Kim Väisänen co-founded Carelian Innovation Ltd. The company’s first <b>data</b> <b>erasure</b> product, Blancco Data Cleaner, was {{released}} in 1999. Then, in 2000, Carelian Innovations Ltd. {{changed its name to}} Blancco Ltd.|$|E
40|$|We {{describe}} SWIPE, {{an approach}} to reduce the life time of sensitive, memory resident data in large scale applications written in C. In contrast to prior approaches that used a delayed or lazy approach {{to the problem of}} erasing sensitive data, SWIPE uses a novel eager erasure approach that minimizes the risk of accidental sensitive data leakage. SWIPE achieves this by transforming a legacy C program to include additional instructions that erase sensitive data immediately after its intended use. SWIPE is guided by a highlyscalable static analysis technique that precisely identifies the locations to introduce erase instructions in the original program. The programs transformed using SWIPE enjoy several additional benefits: minimization of leaks that arise due to <b>data</b> dependencies; <b>erasure</b> of sensitive <b>data</b> with minimal developer guidance; and negligible performance overheads...|$|R
40|$|In {{this paper}} we address the {{reliability}} problem in storage-centric sensor networks deployed in hazardous environments. We use the nodes' extra flash memory to save distributed encoded blocks of <b>data.</b> We employ <b>erasure</b> coding to substantially improve the trade-off between storage reliability and required disk capacity. Moreover, efficient mechanisms are designed for spreading the encoded pieces over the network. Mathematical and experimental results show that erasure coding preserves the data from both crashes and memory overflow, substantially outperforming traditional schemes...|$|R
40|$|We {{study the}} {{performance}} of selected communication schemes based on fountain codes, under maximum-likelihood (ML) decoding, {{as a function of}} the Protocol <b>Data</b> Unit (PDU) <b>erasure</b> probability. In particular, we also consider a scheme that reduces the performance loss due to fragmentation of fountain coding symbols into several PDUs. For the presented schemes, tight upper bounds on the failure probabilities are derived and finite length decoding complexity analysis are provided. Furthermore, we propose an enhanced decoding technique for one of the selected schemes...|$|R
50|$|<b>Data</b> <b>erasure</b> may {{not work}} {{completely}} on flash based media, such as Solid State Drives and USB Flash Drives, as these devices can store remnant data which is inaccessible to the erasure technique, and data can be retrieved from the individual flash memory chips inside the device.Data erasure through overwriting only works on hard drives that are functioning and writing to all sectors. Bad sectors cannot usually be overwritten, but may contain recoverable information. Bad sectors, however, may be invisible to the host system and thus to the erasing software. Disk encryption before use prevents this problem. Software-driven <b>data</b> <b>erasure</b> could also be compromised by malicious code.|$|E
5000|$|BCWipe is {{commercial}} military-grade file {{eraser tool}} [...] <b>data</b> <b>erasure</b> utility for Windows, UNIX and Mac OS X. Developed by Jetico Inc, the software erases files beyond recovery and can erase free unused space on existing disks.|$|E
50|$|<b>Data</b> <b>erasure</b> is {{a method}} of {{software-based}} overwriting that completely destroys all electronic data residing on a hard drive or other digital media {{to ensure that no}} sensitive data is leaked when an asset is retired or reused...|$|E
40|$|Abstract. The {{cells of}} flash {{memories}} can only endure {{a limited number}} of write cycles, usually between 10, 000 and 1, 000, 000. Furthermore, cells containing data must be erased before they can store new <b>data,</b> and <b>erasure</b> operations erase large blocks of memory, not individual cells. To maximize the endurance of the device (the amount of useful data that can be written to it before one of its cells wears out), flash-based systems move data around in an attempt to reduce the total number of erasures and to level the wear of the different erase blocks. This data movement introduces interesting online problems called wear-leveling problems. We show that a simple randomized algorithm for one problem is essentially optimal. For a more difficult problem, we show that clever offline algorithms can improve upon naive approaches, but online algorithms essentially cannot. ...|$|R
30|$|During {{the last}} three decades, the theory of frames, which generalize the notion of bases by {{allowing}} redundancy yet still providing a reconstruction formula, has been growing rapidly, since several new applications such as nonlinear sparse approximation (e.g., image compression), coarse quantization, <b>data</b> transmission with <b>erasures,</b> and wireless communication, have been developed [1 – 7]. As a special class of frames, the multi-band wavelets have attracted considerable attention due to their richer parameter space, to give better energy compaction than 2 -band wavelets [8 – 16].|$|R
40|$|This paper {{considers}} a coding scheme for <b>data</b> transmission over <b>erasure</b> channels {{which is also}} known as multiple description coding. The LMMSE prefilter method of Romano [1] is reviewed and generalized to allow three different operational modes of the prefilter. They include the possibility to decrease or increase the number of descriptions to be transmitted. We derive explicitly the Hessian matrix for an efficient calculation of the prefilter. We also study the properties of the distortion measure theoretically. Index Terms — multiple description coding, transform coding, correlating transform 1...|$|R
