1|15|Public
40|$|Priority {{arbitration}} is {{an essential}} part of the ATM switches in order to support the integration of telecommunication services with difference characteristics. Service priority control selects the connection to output a cell among all connections destined to the same output port. <b>Discard</b> <b>priority</b> control selects the connection to discard a cell when the shared buffer is full. In this paper we present a VLSI design of a priority arbitrator for shared buffer ATM switches. This priority arbitrator is targeted to support our new service priority control scheme, reactive bandwidth arbitration (RBA), and new <b>discard</b> <b>priority</b> control scheme, local pushout discarding (LPD). The priority arbitrator is designed for an 8 Θ 8 shared buffer ATM switch with four priority classes per port and a link rate of 622 Mbps. The chip has 130 k gates in a chip area of 137. 88 mm 2 using 0. 6 m CMOS technology...|$|E
40|$|TCP {{flows from}} {{applications}} such as the web or ftp are well supported by a Guaranteed Minimum Throughput Service (GMTS), which provides a minimum network throughput to the flow and, if possible, an extra throughput. We propose a scheme for a GMTS using Admission Control (AC) that is able to provide different minimum throughput to different users and that is suitable for "standard" TCP flows. Moreover, we consider a multidomain scenario where the scheme is used {{in one of the}} domains, and we propose some mechanisms for the interconnection with neighbor domains. The whole scheme uses a small set of packet classes in a core-stateless network where each class has a different <b>discarding</b> <b>priority</b> in queues assigned to it. The AC method involves only edge nodes and uses a special probing packet flow (marked as the highest <b>discarding</b> <b>priority</b> class) that is sent continuously from ingress to egress through a path. The available throughput in the path is obtained at the egress using measurements of flow aggregates, and then it is sent back to the ingress. At the ingress each flow is detected using an implicit way and then it is admission controlled. If it is accepted, it receives the GMTS and its packets are marked as the lowest <b>discarding</b> <b>priority</b> classes; otherwise, it receives a best-effort service. The scheme is evaluated through simulation in a simple "bottleneck" topology using different traffic loads consisting of "standard" TCP flows that carry files of varying size...|$|R
40|$|Internet {{applications}} such as web or ftp will be satisfactorily supported by a guaranteed minimum throughput service. We propose to build this service in a core-stateless network such as Differentiated Services with an Admission Control (AC) method based on end-to-end measurements, using the first packets of the flow. The whole scheme uses a small set of packet classes (with different <b>discarding</b> <b>priorities),</b> and is therefore simple and scalable. We evaluate the scheme through simulations in several scenarios using TCP flows. Finally, we propose {{an extension of the}} method aimed at improving its performance...|$|R
30|$|With TCP-Casablanca [28], the TCP sender marks packets {{with two}} {{different}} <b>discard</b> <b>priorities</b> and intermediate routers drop the packets {{of the lower}} priority first in congestion. Thus the probability distribution of congestion losses will be different for {{the two types of}} packets while that of wireless losses may be similar for both of them assuming the uniform distribution of wireless losses. The TCP sender exploits such a difference when determining the cause of a packet loss. However, TCP-Casablanca requires the change of every router in networks and this makes it hard to be deployed. It discriminates the cause of a packet loss in a stochastic manner, therefore has higher detection failure probability of wireless losses than ELN schemes. Furthermore, under heavy congestion or bursty-loss wireless channel condition, the probability distribution of packet losses may not follow the presumed one and thus the detection failure probability will increase.|$|R
40|$|Abstract. TCP flows {{generated}} by {{applications such as}} the web or ftp require a minimum network throughput to satisfy users. To build this service, we propose a scheme with Admission Control (AC) using a small set of packet classes in a core-stateless network. At the ingress each flow packet is marked {{as one of the}} set of classes, and within the network, each class is assigned a different <b>discarding</b> <b>priority.</b> The AC method is based on edge-to-edge per-flow measurements, and it requires flows to be sent at a minimum rate. The scheme is able to provide different throughput to different flows and protection against nonresponsive sources. We evaluate the scheme through simulation in several network topologies with different traffic loads consisting of TCP flows that carry files of varying sizes. In the simulation, TCP uses a new algorithm to keep the short-term sending rate above a minimum value. The results prove that the scheme guarantees the throughput to accepted flows and achieves high utilization of resources, similar to the ideal results of a classical hop-by-hop AC. ...|$|R
40|$|TCP flows {{generated}} by {{applications such as}} the Web or FTP require a minimum network throughput and can also benefit from an extra throughput due to their rate-adaptive algorithms. To build a guaranteed minimum throughput service, the authors propose a scheme with Admission Control (AC) using a small set of packet classes in a core-stateless network. At the ingress of the network, each flow packet is marked {{as one of the}} set of classes, and within the network, each class is assigned a different <b>discarding</b> <b>priority.</b> The AC method is based on edge-to-edge per flow measurements and requires flows to be sent at a minimum rate. The scheme is able to provide different minimum throughput to different users and protection against nonresponsive sources. The authors evaluate the scheme through simulation using different traffic loads consisting of TCP flows that carry files of varying sizes. In the simulation, TCP uses a new algorithm that forces the source to keep the short-term sending rate above a desired minimum rate. The authors study the influence of several parameters on the performance of the scheme in different network topologies. The results show that the scheme guarantees the requested throughput to accepted flows and achieves a high utilization of network resources, similar to the ideal results of a classical hop-by-hop AC...|$|R
40|$|The Differentiated Services (DiffServ) model {{brings the}} {{flexibility}} of the definition of a variety of QoS services through PHBs (Per Hop Behaviours) and Traffic Conditioners. It integrates gracefully with the Integrated Services (IntServ) model, that can have functions of QoS signalling, admission control, channel management, resource estimation and dynamic traffic contract agreement. With this integration we can have a scalable, flexible and dynamic IP Core QoS environment. The transport network and switching solution used in the architecture proposed, DIPQoS (DTM IP QoS) architecture, is the DTM, a fast circuit switching technology. It’s simple, cost effective and have attractive QoS performance. Four simplex services are defined in DIPQoS: the DSG (DTM Strict Guarantee), that guarantees the signalled peak rate and delay tolerance; the DGF (DTM Guaranteed Forwarding), that guarantees a minimum forwarding of packets based on pre-signalled QoS specifications and SLAs (Service Level Agreements) between customer and provider. Another one is the DLF (DTM Loose Forwarding) that provides a loose forwarding of the packets based on <b>discard</b> <b>priorities</b> and also on guaranteed pre-signalled QoS specifications and SLAs. And the last one is the DBE (TM Best Effort), like the traditional Internet service, there isn’t any QoS support. The {{purpose of this paper is}} to present a functional architecture of QoS services based on DiffServ and IntServ models for a Core Internet network over the DTM technology...|$|R
40|$|This {{article is}} a first, limited attempt made to {{understand}} public knowledge, awareness, attitude and willingness to pay (WTP) in WEEE management {{in the context of}} Bangladesh, with a particular focus on the level of awareness, knowledge on WEEE, the reasons of <b>discarding,</b> <b>priority</b> choice, and disposal method. The {{purpose of this paper is}} to provide scientific knowledge basis promoting a policy agenda by assessing current WEEE management trend among households. Survey data has been collected by distributing questionnaires randomly in 7 residential areas in Dhaka, and interviewing 400 households. This study found a very limited knowledge on WEEE among households (i. e. only 9 %). Competitive price, warranty period, brands and installment facilities are found important economic factors during the new purchase of electronic products. The actual life cycle of mobile phone, personal computers and television sets varies from 2 to 6 years. With the presence of informal sector WEEE collection and recycling, this study found that 30 % of the households were selling WEEE items to scrap collectors for economic benefits. More than 20 % of the respondents have thrown WEEE with household waste. Only 5 – 10 % of the respondents are willing to pay for any new WEEE management system. Enacting effective regulations, developing national WEEE inventory database, environmental awareness building through education and stakeholders participation in creating economic value chain; are suggested through this study. This research is one of a kind in investigating the attitudes of the households of a developing nation towards WEEE management and will show the pattern of WEEE generation, disposal and management practice in the most densely populated city in the world, i. e., Dhaka, Bangladesh. Results found from this research are expected to create a primary basis for encouraging scientific discussion and understanding the situation at policy level in Bangladesh and will also pave the way for a successful WEEE management policy making in any similar social and economic conditions...|$|R
40|$|TCP (Transmission Control Protocol) {{with its}} {{well-established}} congestion control mechanism is the prevailing transport layer protocol for non-real time data in current IP (Internet Protocol) networks. It would be desirable to transmit {{any type of}} multimedia data using TCP {{in order to take}} advantage of the extensive operational experience behind TCP in the Internet. However, some features of TCP including retransmissions and variations in throughput and delay, although not catastrophic for non-real time data, may result in inefficiencies for video streaming applications. In this paper, we propose an architecture which consists of an input buffer at the server side, coupled with the congestion control mechanism of TCP at the transport layer, for efficiently streaming stored video in the best-effort Internet. The proposed buffer management scheme selectively <b>discards</b> low <b>priority</b> frames from its head-end, which otherwise would jeopardize the successful playout of high priority frames. Moreover, the proposed discarding policy is adaptive to changes in the bandwidth available to the video stream...|$|R
40|$|Networked {{applications}} require {{good performance}} from the network {{along with a}} high level of predictability. One of the factors that affects application performance is the packet dropping policy adopted in the switch or router. In this paper, we evaluate the absolute performance of various packet dropping policies, and also the predictability they can provide to an application. The packet dropping schemes studied are the Partial Packet Discard (PPD), Early Packet <b>Discard</b> (EPD), Age <b>Priority</b> Packet <b>Discarding</b> (APPD) and the Preemptive Partial Packet Discard (pPPD). Results are provided in multihop networks for different traffic types, including IP over ATM trace data...|$|R
40|$|Cataloged from PDF {{version of}} article. Transmission control {{protocol}} (TCP) with its well-established congestion control mechanism is the prevailing transport layer protocol for non-real time data in current Internet Protocol (IP) networks. It would be desirable to transmit {{any type of}} multimedia data using TCP {{in order to take}} advantage of the extensive operational experience behind TCP in the Internet. However, some features of TCP including retransmissions and variations in throughput and delay, although not catastrophic for non-real time data, may result in inefficiencies for video streaming applications. In this paper, we propose an architecture which consists of an input buffer at the server side, coupled with the congestion control mechanism of TCP at the transport layer, for efficiently streaming stored video in the best-effort Internet. The proposed buffer management scheme selectively <b>discards</b> low <b>priority</b> frames from its head-end, which otherwise would jeopardize the successful playout of high priority frames. Moreover, the proposed discarding policy is adaptive to changes in the bandwidth available to the video stream. 2004 Elsevier B. V. All rights reserved...|$|R
40|$|Transmission control {{protocol}} (TCP) with its well-established congestion control mechanism is the prevailing transport layer protocol for non-real time data in current Internet Protocol (IP) networks. It would be desirable to transmit {{any type of}} multimedia data using TCP {{in order to take}} advantage of the extensive operational experience behind TCP in the Internet. However, some features of TCP including retransmissions and variations in throughput and delay, although not catastrophic for non-real time data, may result in inefficiencies for video streaming applications. In this paper, we propose an architecture which consists of an input buffer at the server side, coupled with the congestion control mechanism of TCP at the transport layer, for efficiently streaming stored video in the best-effort Internet. The proposed buffer management scheme selectively <b>discards</b> low <b>priority</b> frames from its head-end, which otherwise would jeopardize the successful playout of high priority frames. Moreover, the proposed discarding policy is adaptive to changes in the bandwidth available to the video stream. © 2004 Elsevier B. V. All rights reserved...|$|R
40|$|In this paper, {{we propose}} an N × N high speed and {{non-blocking}} {{asynchronous transfer mode}} (ATM) switch with input and output buffers. In this switch, each buffer adopts a <b>priority</b> <b>discarding</b> scheme, which discards incoming cells of low-priority traffic when its queue length is greater than a predefined threshold value. Our switch also sup-ports broadcast/multicast functions without increasing the cost and imposing a signifi-cant performance penalty. We use the discrete-time Markov chain model to analyze cell delay and cell loss probability for each traffic class. An example 4 × 4 ATM switch has been described with VHDL. We have verified the functionality of the switch via VHDL simulation, and have synthesized the switch to evaluate its area and timing. Experimental results and synthesis results show that our proposed ATM switch can meet a requirement for high speed and support QOS...|$|R
40|$|Due to the {{character}} of the original source materials and the nature of batch digitization, quality control issues may be present in this document. Please report any quality issues you encounter to digital@library. tamu. edu, referencing the URI of the item. Includes bibliographical references. Issued also on microfiche from Lange Micrographics. Software based desktop videoconferencing tools are developed to demonstrate techniques necessary for video delivery in heterogeneous packet networks. Using the current network infrastructure and no network resource reservation, a one-to-many implementation is designed around a two-layer pyramidal video coder. During periods of congestion, the network routers give priority to the base layer, which by itself allows reconstruction of reasonable quality video. Receiver feedback is used to lower the output rate of the encoder's low priority pyramidal layer when all receivers are suffering high packet loss. Each of the two layers is transmitted on a separate multicast channel. Under persistent congestion, an individual receiver will <b>discard</b> the low <b>priority</b> pyramidal layer, which allows the network to prune the multicast tree 'd congestion. A new scheme is examined where if the other receivers back and avoi are agreeable, the source will respond to a receiver pruning its pyramidal layer by lowering its rate and allowing the receiver to quickly rejoin the pyramidal layer at a quality level higher than what the high priority base layer can provide by itself. Another new scheme is described where an agent on the receiver's local router provides spare capacity information to assist the receiver in its decision to rejoin the pyramidal layer...|$|R
40|$|Studying ontogenetic {{trends in}} diet and habitat use of {{endangered}} sharks and deep-water teleosts is logistically challenging and expensive, {{due to the}} remote and inaccessible nature of the pelagic realm, {{and the extent of}} many marine migrations. Chemical analysis of inert, organic, incrementally formed tissues represents a window to retrospectively study whole life-history ecology, however these tissues are rare or absent in many fishes. The fish eye lens is a unique tissue, formed via the sequential deposition of protein-filled fiber cells, which undergo no subsequent remodelling once formed. Despite having great potential to record chemical variations reflecting foraging behaviour, lenses have received relatively little analytical attention. In this thesis I have explored the suitability of fish lenses for recovery of retrospective ontogenetic chemical information, focusing particularly on pre-birth and early juvenile life histories in elasmobranchs. I have confirmed consistent relationships between the body size and lens diameter of four study species (Aphonopus carbo, Coryphaenoides rupestris, Lamna nasus and Squalus acanthias), which allows recovery of a body size-referenced lens samples. Growth relationships reveal that a large proportion of lens tissue in elasmobranchs is deposited pre-birth, opening a previously unrecognised opportunity to study maternal provisioning from tissues of the offspring as adults. I have confirmed that transects of stable isotope compositions across lenses show bilateral symmetry, reflecting the sequential deposition of eye lens tissue. Muscle-lens tissue offsets were examined in S. acanthias and C. rupestris, identifying the potential for species-specific differences, possibly driven by variability in taxon-specific lens protein expression. I have then applied the validated lens sampling protocol to recover cross-generational life history movement and diet ecology information in three study species. Focusing on pre-birth ecology requires some understanding of the isotopic relationship between maternal and embryonic tissues. With access to gravid female spiny dogfish, isotopic spacing between maternal and offspring tissues was investigated. However, the measured mother-embryo isotopic offsets are confounded by migration across isotopic provinces, introducing considerable temporal de-coupling between nutrient assimilation fuelling maternal tissue remodelling and yolk sac provisioning, particularly in species with a long gestation period. For the spiny dogfish Squalus acanthias recovered from the North Sea, lens-derived isotope histories revealed that, whilst the southern and central North Sea represent important foraging areas during mature life history, the northern North Sea is more important during gestation, pupping, and early life history. Similarly, porbeagle (L. nasus) lens chemistry indicates that the mothers of individuals caught in the Celtic Sea region forage over a wide area of the north Atlantic. Attracting mothers from a wide geographical area, this nursery area is likely to be important for the conservation of the endangered northeast Atlantic population, where mitigating <b>discarding</b> is a <b>priority</b> to fisheries managers. The isotopic variability of sequential A. carbo lens samples was also investigated in order to address trends in foraging and location throughout individual life history. Lens data provide further evidence of ontogenetic depth and latitudinal movements, consistent with the species’ ontogenetic migration theory. Obtaining a near whole life history record of information relating to an individual’s trophic and spatial ecology using traditional tagging methods is challenging, if not impossible. The lens therefore represents a valuable chemical repository for high-resolution ecological information that can be analysed retrospectively. <br/...|$|R
40|$|The {{quality of}} H. 264 /AVC {{compressed}} video delivery over time-varying and error-prone wireless channels {{is affected by}} packet losses. To support quality of service (QoS) for video delivery over wireless networks cross-layer schemes have been discussed in the literature. We introduce a cross-layer priority-aware packet fragmentation scheme at the medium access control (MAC) layer to {{enhance the quality of}} pre-encoded H. 264 /AVC compressed bitstreams over bit-rate limited error-prone links in wireless networks. Larger fragments {{are more likely to be}} in error but smaller fragments require more overhead. The H. 264 slices are classified in four priorities at the encoder based on their cumulative mean square error (CMSE) contribution towards the received video quality. The slices of a priority class in each frame are aggregated into video packets of corresponding priority at the application (APP) layer. We derive the optimal fragment size for each priority class which achieves the maximum expected weighted goodput at different encoded video bit rates, slice sizes and bit error rates. Priority-aware packet fragmentation invokes slice discard in the buffer due to channel bit rate constraints on allocating fragment header bits. We propose a slice discard scheme using frame importance and slice CMSE contribution to control error propagation effects. Packet fragmentation is then extended to slice fragmentation by modifying the conventional H. 264 decoder to handle partial slice decoding. Priority-aware slice fragmentation combined with the proposed slice discard scheme provides considerable peak signal-to-noise ratio (PSNR) and video quality metric gains as compared to priority-agnostic fragmentation. Distortion due to channel errors can be alleviated by assigning stronger channel code rates, at the cost of reduced rate for source coding. Besides MAC layer fragmentation, aggregating H. 264 /AVC slices at the APP layer to form video packets with sizes adapted to their importance can also improve transmission reliability. We present a cross-layer dynamic programming (DP) approach to minimize the expected received video distortion by jointly addressing the priority-adaptive packet formation at the APP layer and rate compatible punctured convolutional (RCPC) code rate allocation at the physical layer for pre-encoded prioritized slices of each group of pictures (GOP). Our scheme <b>discards</b> some low <b>priority</b> slices in order to improve protection to more important slices and meet the channel bitrate limitations, whenever necessary. Simulation results show that our proposed approach significantly improves received video quality compared to other error protection schemes. Further, we extend our cross-layer DP-based scheme to slices of each frame by predicting the expected channel bit budget per frame for real-time transmission. The prediction uses a generalized linear model developed over the parameters - CMSE per frame, channel SNR, and normalized compressed frame bit budget determined over a video dataset that spans high, medium and low motion complexity. This predicted frame bit budget is used to derive the packet sizes and their corresponding RCPC code rates for transmission using our DP-based approach. Simulation results show good correlation with the results of our DP-based scheme applied over the GOP. Unique characteristics of video traffic, such as the temporal and spatial dependencies between different video frames and their deadline constraints, pose a challenge in supporting the video quality rendered to the clients over time- varying, bandwidth-limited channels. Scalable Video Coding (H. 264 /SVC) enables the transmission and decoding of partial bit streams to provide video services with lower temporal or spatial resolutions or reduced fidelity while retaining a reconstruction quality that is high relative to the rate of the partial bit streams. We propose a sliding-window based flow control for scheduling the network abstraction layer (NAL) units in the post-encoding buffer of the streaming server for a real-time scalable video transmission scenario over a fast time-varying channel. Our scheduling scheme considers the importance of the NAL unit in terms of (i) its CMSE distortion contributed to the received video quality, (ii) its size in bits, and (iii) its time-to-expiry in seconds. The scheduling problem of determining the appropriate order of transmission is formulated as a 0 - 1 knapsack problem and a DP solution is proposed which runs in polynomial time. Our scheduling approach significantly reduces the number of whole frames discarded as compared to (a) a CMSE-based scheme which considers the importance of the NAL units only in terms of their CMSE contribution, and (b) the earliest deadline first scheme which minimizes the dwelling time of the NAL units in the post-encoding buffer. Simulation results show significant PSNR gains for different video sequences at different pre-roll delay...|$|R

