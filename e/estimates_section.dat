11|583|Public
50|$|Napoleon's {{large-scale}} {{program of}} public works, and his expensive foreign policy, had created rapidly mounting government debts; the annual deficit was about 100 million gold-francs, and the cumulative debt had reached nearly 1,000 million gold-francs (1 billion in US readings). The Emperor needed {{to restore the}} confidence of the business world, and to involve the legislature and have them share responsibility.On 24 December 1861, Napoleon III, against the opposition of his own ministers, issued a decree announcing that the legislature would have greater powers. The Senate and the assembly could, for the first time, give a response to the Emperor's program, ministers were obliged to defend their programs before the assembly, and the right of Deputies to amend the programs was enlarged. On 1 February 1861, further reforms were announced: Deputies could speak from the tribune, not just from their seats, and a stenographic record would be made and published of each session. Another even more important reform was announced on 31 December 1861: the budget of each ministry would be voted section by section, not in a block, and the government could no longer spend money by special decree when the legislature was not in session. He did retain the right to change the budget <b>estimates</b> <b>section</b> by section.|$|E
30|$|The {{outline of}} this paper is as follows. In Section 2, we give the {{definition}} of lower and upper functions to problems (1.1) and (1.2) and obtain some a priori <b>estimates.</b> <b>Section</b> 3 will be devoted {{to the study of the}} existence of solutions. In Section 4, we give an example to illustrate the conclusions.|$|E
40|$|The past {{thirteen}} {{years have seen}} the development of many algorithms for approximating matrix functions in O(N) time, where N is the basis size. These O(N) algorithms rely on assumptions about the spatial locality of the matrix function; therefore their validity depends {{very much on the}} argument of the matrix function. In this article I carefully examine the validity of certain O(N) algorithms when applied to hamiltonians of disordered systems. I focus on the prototypical disordered system, the Anderson model. I find that O(N) algorithms for the density matrix function can be used well below the Anderson transition (i. e. in the metallic phase;) they fail only when the coherence length becomes large. This paper also includes some experimental results about the Anderson model's behavior across a range of disorders. Comment: 12 pages, 5 figures. All code and configuration files necessary to reproduce the results will be made available at [URL]. Accepted by Numerical Linear Algebra with Applications. Changes in version two include correction of a substantial error in the error <b>estimates</b> <b>section,</b> changes of both the error <b>estimates</b> <b>section</b> and the numerical results section to give {{for the first time a}} good way of estimating the errors incurred by an O(N) algorithm, and other less important refinement...|$|E
40|$|ASJ RTN-Model 2003, {{which is}} {{generally}} used {{for the evaluation of}} 2 ̆ 7 the Environmental Quality Standards for Noise 2 ̆ 7 in Japan, estimates the averaged noise levels as the representative value in each <b>estimate</b> <b>section.</b> However, the noise level at each building is greatly changed by the arrangement of buildings. Therefore, it is necessary for the evaluation of the Standards to take the distribution of noise levels in the <b>estimate</b> <b>sections</b> into consideration. From the above viewpoint, we propose a new concept to adopt the most frequent level of insertion loss of road traffic noise as the representative value of each <b>estimate</b> <b>section,</b> and a simple method to predict the most frequent level of insertion loss is presented based on the simulation of the insertion losses caused by detached houses by the authors 2 ̆ 7 method...|$|R
6000|$|... "I {{have the}} designs here all ready," [...] Walter Tyrrel replied, holding them out. [...] "Plans, elevations, specifications, <b>estimates,</b> <b>sections,</b> figures, everything. Will {{you do me}} the favor to look at them? Then, perhaps, you'll {{be able to see}} whether or not the offer's genuine." ...|$|R
30|$|The present {{paper is}} {{organized}} as follows. The next section outlines {{the collection of}} methods used, both for yield curve estimation and for the ACM <b>estimates.</b> <b>Sections</b> 3 and 4 contain our empirical analysis, using Colombian data as an illustration for the methods before discussing the term premia estimates and their behavior. A final Sect. 5 concludes.|$|R
40|$|In {{the finite}} field setting, {{we show that}} the {{restriction}} conjecture associated {{to any one of}} a large family of d= 2 n+ 1 dimensional quadratic surfaces implies the n+ 1 dimensional Kakeya conjecture (Dvir's theorem). This includes the case of the paraboloid over finite fields in which - 1 is a square. We are able to partially reverse this implication using the sharp Kakeya maximal operator estimates of Ellenberg, Oberlin and Tao to establish the first finite field restriction estimates beyond the Stein-Tomas exponent in this setting. Comment: 52 pages, v 3 : minor corrections to the additive energy <b>estimates</b> (<b>section</b> 13) v 4 /v 5 : the statement of the necessary conditions for the full conjecture have been corrected, as pointed out by Doowon Ko...|$|E
40|$|AbstractIn {{this paper}} the author tries to give general {{conditions}} {{for the existence of}} Bayes estimates and for the consistency of sequences of Bayes estimates. In Section 3 we prove existence theorems for Bayes estimates, which contain those of DeGroot and Rao [3], as a special case. The proof is based on a theorem of Landers [5]. Section 4 gives a characterization of Bayes estimates with convex loss and linear decision space. This theorem is also a generalization of a similar theorem of DeGroot and Rao [3]. In Section 5 we generalize the theory of minimum contrast estimates (the foundations of which were laid by Huber [4], cf. Pfanzagl [6]) {{in such a way that}} we can apply it to the theory of Bayes <b>estimates.</b> <b>Section</b> 6 tries to give a general theory of consistency for Bayes estimates using the martingale argument of Doob [1] and the theory of minimum contrast estimates. Confer in this connection the results of Schwartz [8]. Section 7 contains some auxiliary results...|$|E
40|$|A {{two-phase}} {{feasibility study}} was initiated in late 1996 {{to identify a}} way to expedite the removal of SNM from the CPP- 651 vault. The first phase of this study provided preliminary information that appeared promising, but needed additional detailed planning and evaluate to validate the concepts and conclusions. The focus of Phase 2 was to provide the validation via resource-loaded schedules and more detailed cost <b>estimates.</b> <b>Section</b> 1 describes the purpose and objectives of the Phase 2 tasks and the programmatic drivers that influence related CPP- 651 high-enriched uranium (HEU) management issues. Section 2 identifies the evaluation criteria and methodology and the transfer issues and barriers preventing shipment. Section 3 provides site-specific background information for the CPP- 651 facility and the Idaho National Engineering and Environmental Laboratory (INEEL) and describes {{the development of the}} basic material removal schedule, the proposed base case plan for removal of SNM, and the proposed HEU material management/shipping issues and strategies. Section 4 identifies the proposed options for accelerated removal of SNM and how they were evaluated via detailed scheduling, resource histograms, and cost analysis. Section 5 summarizes principal tasks for implementing this plan and other related HEU CPP- 651 management issues that require continued planning efforts to assure successful implementation of this proposed early removal strategy...|$|E
40|$|Traffic state {{estimation}} {{is a key}} problem with considerable implications in modern traffic management. A simple, general, and complete approach {{to the design of}} urban network traffic state and phase estimator has been developed in this paper. A uniform traffic state dynamic estimation method structure is designed which consists of three steps. (1) Floating-car data and radio frequency identification data preprocessing method is proposed to remove the abnormal data and finish the map matching process. (2) Section speed estimation method is proposed based on the degree of confidence. (3) Traffic phase identification method is proposed based on the <b>estimated</b> <b>section</b> speed. A number of simulation and field investigations have been conducted to test the estimator performance. The investigation results indicate that the proposed approach is of high accuracy and smoothness on the section speed estimation and effectively eliminates the influence of abnormal data fluctuations and insufficient data. And the traffic phase identification method can effectively filter out the abnormal distortion of <b>estimated</b> <b>section</b> speed around the threshold value and modify the phase step of traffic status caused by abnormal data...|$|R
30|$|This stage votes by {{the results}} from the set of hints to compute the <b>estimated</b> age (<b>Section</b> 3.3. 2).|$|R
40|$|A major {{function}} of the Tank Waste Remediation System (TWRS) is to characterize waste in support of waste management and disposal activities at the Hanford Site. Analytical data from sampling and analysis and other available information about a tank are compiled and maintained in a tank characterization report (TCR). This report and its appendices serve as the TCR for single-shell tank 241 -AX- 102. The objectives of this report are: (1) to use characterization data in response to technical issues associated with tank 241 -AX- 102 waste, and (2) to provide a standard characterization of this waste {{in terms of a}} best-basis inventory <b>estimate.</b> <b>Section</b> 2. 0 summarizes the response to technical issues, Section 3. 0 shows the best-basis inventory <b>estimate,</b> <b>Section</b> 4. 0 makes recommendations about the safety status of the tank and additional sampling needs. The appendices contain supporting data and information. This report supports the requirements of the Hanford Federal Facility Agreement and Consent Order (Ecology et al. 1997), Milestone M- 44 - 15 c, change request M- 44 - 97 - 03 to ''issue characterization deliverables consistent with the Waste Information Requirements Document developed for FY 1999 '' (Adams et al. 1998) ...|$|R
40|$|The Oak Ridge National Laboratory (ORNL) {{provides}} {{research and}} development (R&D) support to the Department of Energy on {{issues related to the}} cost and performance of hybrid vehicles. ORNL frequently benchmarks its own research against commercially available hybrid components currently used in the market. In 2005 we completed a detailed review {{of the cost of the}} second generation Prius hybrid. This study examines the new 2007 Camry hybrid model for changes in technology and cost relative to the Prius. The work effort involved a detailed review of the Camry hybrid and the system control strategy to identify the hybrid components used in the drive train. Section 2 provides this review while Section 3 presents our detailed evaluation of the specific drive train components and their cost <b>estimates.</b> <b>Section</b> 3 also provides a summary of the total electrical drive train cost for the Camry hybrid vehicle and contrasts these estimates to the costs for the second generation Prius that we estimated in 2005. Most of the information on cost and performance were derived from meetings with the technical staff of Toyota, Nissan, and some key Tier I suppliers like Hitachi and Panasonic Electric Vehicle Energy (PEVE) and we thank these companies for their kind cooperation...|$|E
40|$|In {{order to}} measure {{industry}} {{total factor productivity}} accurately, we require reliable information {{not only on the}} outputs produced and the labour input utilized by the industry but we also require accurate information on eight additional classes of input used by the industry. One of these additional classes of input is intermediate input; i. e., inputs that are utilized by the industry but which are produced by other industries. Information on the real and nominal purchases of intermediate inputs by industry comes from the system of input-output tables published by Statistics Canada. In section 4, we explain why the estimates of real intermediate input utilization by industry that one can obtain from the real input-output tables of any country are likely to be inaccurate. In section 5, we go on {{to make the case that}} national productivity estimates are likely to be more accurate than subnational industry <b>estimates.</b> <b>Section</b> 6 concludes on an optimistic note. The total factor productivity of a firm, industry or group of industries is defined as the real output produced by the firm or industry over a period of time divided by the real input used by the same set of production units over the same time period. However, it turns out to be difficult to provide a meaningful definition of real output or real input du...|$|E
40|$|This paper {{introduces}} a simple method of price risk decomposition that determines {{the extent to}} which producer price risk is attributable to volatile inter-market margins, intra-day variation, intra-week (day of week) variation, or terminal market price variability. We apply the method to livestock markets in northern Kenya, a setting of dramatic price volatility where price stabilization is a live policy issue. In this particular application, we find that large, variable inter-market basis is {{the most important factor in}} explaining producer price risk in animals typically traded between markets. Local market conditions explain most price risk in other markets, in which traded animals rarely exit the region. Variability in terminal market prices accounts for relatively little price risk faced by pastoralists in the dry lands of northern Kenya although this is the focus of most present policy prescriptions under discussion. Producer price volatility concerns producers and governments in a wide range of industries and nations. In settings where producers have little or no access to financial markets through which they can effectively hedge against price risk, governments are often keen to find cost-effective means to reduce producer price volatility. Yet such volatility can arise from any of several sources, so identification of effective intervention strategies depends fundamentally on locating the source(s) of variability in producer prices. This paper {{introduces a}} simple method of price risk decomposition intended to serve as a policy analysis tool for precisely that purpose. This method determines {{the extent to which}} producer price risk is attributable to volatile inter-market margins, intra-day variation, intra-week (day of week) variation, or variability in terminal market price. We apply the method to livestock markets in northern Kenya, a setting of dramatic price volatility where price stabilization is a live policy issue. The remainder of the paper proceeds as follows. Section I introduces our price risk decomposition method. We then demonstrate its utility with an application to livestock markets in the drylands of northern Kenya in a series of three sections. Section II describes the context and some of the current policy debate surrounding livestock price stabilization in Kenya. Section III presents the data and key limitations of this particular sample. The empirical results appear in Section IV along with discussion of these <b>estimates.</b> <b>Section</b> V concludes. Demand and Price Analysis, O 1, Q 13, Q 18,...|$|E
30|$|It {{was noted}} that the orbital {{dimensions}} were more in females than in males. A multivariate function {{was derived from the}} discriminant function analysis. The <b>estimated</b> <b>sectioning</b> point or cut off score was calculated to be - 0.141. If the score of the function was more than - 0.141, then sex can be assigned as male. On the other hand, if it was less than - 0.141, then it was female. Likewise, sex could be correctly assigned in 68.5 % of the cases by this method.|$|R
30|$|This work is {{organized}} as follows: Section 2 provides {{background on the}} technical details to carry out multi-DOA estimation, as well as discusses {{the issue of the}} amount of microphones versus the number of sources <b>estimated.</b> <b>Section</b> 3 presents some background on the nature of simultaneous speech and how the proposed approach takes advantage of it. Section 4 details the proposed system. Section 5 presents the evaluation method we employed to measure the proposed system’s performance. Section 6 discusses the results, and Section 7 provides the conclusions and future work.|$|R
3000|$|... -QAM {{modulation}} formats. Subsequently, {{the minimum}} required SIR values, which allow the interference {{to be considered}} negligible, and the minimum distance among DAA protection zones are <b>estimated</b> in <b>Section</b> 3.3.|$|R
40|$|Summary Flexible {{inflation}} targeting {{has become}} the preferred policy among {{a growing number of}} central banks over the last decades. Due to the lag between interest rates and inflation, optimal monetary policy in this framework is essentially about forecasting inflation (Svensson and Woodford, 2003). The output gap, measuring the deviation of output from potential, has a key role in this regard. Through different transition mechanisms a positive output gap leads to inflation. For central banks aiming at a flexible inflation target, an appropriate policy response to the observed pressure in the economy will not only help stabilize inflation at a desired level, but also stabilize output (Svensson, 1997 and 2000). If the policy reactions are going to be proper, the measure of the output gap has to be adequate. As demonstrated in this and other analysis it seldom is (see for example Orphanides and van Norden (2002) and Bernhardsen, Eitrheim, Jore and Røisland (2004). There are basically two factors making the derivation of the output gap difficult. The first concerns the estimation procedure. Since one fails to reject the hypothesis of a unit root in macroeconomic time series, the long run trend of output can no longer be treated as deterministic; see e. g. Nelson and Plosser (1982). Accordingly, the computation of potential output has to take into consideration the estimation of a stochastic trend, which greatly complicates the measuring of potential output and the output gap. The second factor concerns the real-time nature at which central banks have to conduct monetary policy: Decisions are based on highly uncertain data, which are subjected to substantial revisions. This is especially true of the output. There are three main reasons for changes to official statistics. • The earliest estimates are based on preliminary and incomplete information. • Changes to the base year. • The national accounts are occasionally subject to major revisions. Real-time data is data as it was observed at each point in time, and typically categorized into different vintages describing their time of release, thus taking into account these data revision processes. In the spirit of Orphanides and van Norden (2005), this paper examines two different methods for extracting the output gap in real-time, and evaluates their performance in forecasting Norwegian inflation. Especially, I question whether the inclusion of the output gap gives any value added in forecasting Norwegian domestic inflation compared to simple autoregressive benchmark models. The answer clearly depends on factors as model specifications, evaluation criteria, the forecasting periods {{and the quality of the}} data: The output gap models evaluated are the Hodrick-Prescott filter and the Production function method. As a benchmark forecasting model I employ a linear AR(p) model of inflation. My main forecasting model is a Phillips curve relation including the output gap. These specifications make it possible to relate inflation to real activity. I have used root mean square forecast errors (RMSFE) to assess the forecasting performance, and the forecasting period has ranged from 94 q 1 to 06 q 2. By using real-time data this paper highlights the problems and the uncertainties brought forward by the data revision processes. To my knowledge real-time forecasting exercises of this kind has not been conducted on Norwegian data before. Bjørnland, Brubakk and Jore (2007) found that models including the output gap gave a better predictive power of inflation than models based on alternative indicators, and that they forecasted significantly better than simple benchmark models, but they did not use real-time data. Based on real-time data estimations my findings suggests that the inclusion of the output gap makes the out-of-sample forecasts less accurate than what would have been attained if the simpler benchmark models had been used, a finding that is consistent with results reported in Orphanides and van Norden (2005). Some output gap models computed in real-time do however forecast better than the benchmark models, but the results seems to be very sensitive to the chosen forecasting period. Further I find that there are considerable differences in forecasting performance between using real-time data, and final vintage data (the final vintage in the sample has been 06 q 2). The reminder of this paper is organized as follows: Section 2 describes the output gap concept, the output gap models and the real-time data sets that I have used. Sections 2 - 2. 2 follow Bjørnland, Brubakk and Jore (2004), and Frøyland and Nymoen (2000) closely. For a more thorough exposition of the output gap, and the different methods to extract it, I refer to the cited papers. Section 2. 4 illustrates clearly how the real-time issues affect the output gap <b>estimates.</b> <b>Section</b> 3 presents the forecasting methodology. Sections 4 and 5 present the results and conclusions. I have used Matlab computer software and the Econometrics Toolbox provided by James P. LeSage for my computations. Programming codes can be made available on request...|$|E
40|$|One major {{function}} of the Tank Waste Remediation System (IWRS) is to characterize wastes in support of waste management and disposal activities at the Hanford Site. Analytical data from sampling and analysis and other available information about a tank are compiled and maintained in a tank characterization report (CR). This report and its appendixes serve as the CR for single-shell tank 24 1 -C- 1 12. The objectives of this report are: 1) to use characterization data in response to technical issues associated with tank 24 1 -C- 1 12 waste, and 2) to provide a standard characterization of this waste {{in terms of a}} best-basis inventory <b>estimate.</b> <b>Section</b> 2. 0 summarizes the response to technical issues, Section 3. 0 shows the best-basis inventory <b>estimate,</b> and <b>Section</b> 4. 0 makes recommendations regarding safety status and additional sampling needs. The appendixes contain supporting data and information. This report supports the requirements of the Hanford Federal Facility Agreement and Consent Order, Milestone M- 44 - 05 (Ecology et al. 1996) ...|$|R
30|$|In {{order to}} get the fine and {{unambiguous}} estimates of the direction-cosines, the coarse <b>estimates</b> obtained in <b>Section</b> 1 will be used as the reference to disambiguate the fine <b>estimates</b> derived in <b>Section</b> 2. This disambiguation approach has been derived by Zoltowski and Wong [10], and has also been used in the other literature, i.e. [11, 45]. The main essence is summarized as follows [45].|$|R
2500|$|The Committee on Buildings {{described}} the first <b>section,</b> <b>estimated</b> to cost {{not more than}} $300,000, this way: ...|$|R
5000|$|Expected {{values for}} {{logarithmic}} transformations (useful for maximum likelihood <b>estimates,</b> see <b>section</b> titled [...] "Parameter estimation, Maximum likelihood" [...] below) {{are discussed in}} this section. The following logarithmic linear transformations {{are related to the}} geometric means GX and G(1−X) (see section titled [...] "Geometric mean"): ...|$|R
40|$|The present work {{is focused}} on {{neutronics}} research for LVR- 15 reactor, using SCALE and PARCS code. SCALE is a lattice code, which {{has been used to}} create geometries for reactor and of <b>estimate</b> cross <b>sections</b> for the modelled elements. These cross sections have been used to create the whole reactor model in PARCS code. PARCS is a neutronic code, that uses the cross <b>sections</b> <b>estimated</b> by lattice codes; it has been used to perform calculations on steady state, no-event transients and start-up transients for LVR- 15 reactors. The data obtained from PARCS are then compared with data coming from reactor operators...|$|R
2500|$|Expected {{values for}} {{logarithmic}} transformations (useful for maximum likelihood <b>estimates,</b> see <b>section</b> titled [...] "Parameter estimation, Maximum likelihood" [...] below) {{are discussed in}} this section. [...] The following logarithmic linear transformations {{are related to the}} geometric means GX and [...] G(1−X) (see section titled [...] "Geometric mean"): ...|$|R
40|$|A major {{function}} of the Tank Waste Remediation System (TWRS) is to characterize waste in support of waste management and disposal activities at the Hanford Site. Analytical data from sampling and analysis, {{in addition to other}} available information about a tank are compiled and maintained in a tank characterization report (TCR). This report and its appendices serve as the TCR for the single-shell tank series consisting of 241 -T- 201, -T- 202, -T- 203, and -T- 204. The objectives of this report are: (1) to use characterization data in response to technical issues associated with T- 200 series tank waste and (2) to provide a standard characterization of this waste in terms of a best-basis inventory <b>estimate.</b> <b>Section</b> 2. 0 summarizes the response to technical issues, Section 3. 0 shows the best-basis inventory <b>estimate,</b> <b>Section</b> 4. 0 makes recommendations about the safety status of the tank and additional sampling needs. The appendices contain supporting data and information. Appendix A contains historical information for 241 -T- 201 to T- 204, including surveillance information, records pertaining to waste transfers and tank operations, and expected tank contents derived from a process knowledge-based computer program. Appendix B summarizes sampling events, sample data obtained before 1989, and the most current sampling results. Appendix C reports the statistical analysis and numerical manipulation of data used in issue resolution. Appendix D contains the evaluation to establish the best-basis for the inventory estimate and the statistical analysis performed for this evaluation. Appendix E is a bibliography that resulted from an in-depth literature search of all known information sources applicable to tanks 241 -T- 201, -T- 202, -T- 203, and -T- 204. The reports listed in Appendix E are available in the Tank Characterization and Safety Resource Center...|$|R
30|$|This <b>section</b> <b>estimates</b> {{the energy}} outputs of CSG production, the energy inputs of CSG’s {{life-cycle}} stages, and CSG losses during transportation.|$|R
3000|$|The second interference-based {{algorithm}} additionally {{includes the}} speech probability <b>estimate</b> defined in <b>Section</b> 3.3. 2. Thus, {{in addition to}} the parameters [...]...|$|R
30|$|Vocabulary size {{in reading}} is {{assessed}} in section 4, where they read a Japanese equivalent and four English words and choose {{the word that}} has the same meaning. This section {{is based on the}} Mochizuki vocabulary size test and some {{attempts have been made to}} argue for validity of the test score interpretations (e.g., Koizumi & Mochizuki 2011). The vocabulary size <b>estimated</b> in this <b>section</b> is from JACET 1000 to 7000. This <b>section</b> <b>estimates</b> test-takers’ receptive vocabulary size.|$|R
40|$|We model J/psi {{using an}} {{effective}} lagrangian approach and calculate itsspectral {{function in a}} gas of light mesons at finite temperature. We explorethe hadronic landscape <b>estimating</b> cross <b>sections</b> for elastic and inelasticchannels. Effects of form factors are tested in a general, but consistentlygauge invariant manner. Relevance to heavy ion experiments is discussed...|$|R
3000|$|The {{performance}} of MIMO precoders presented in Section IV is evaluated {{in terms of}} BER {{in the presence of}} impulsive noise. The parameters A and Γ <b>estimated</b> in <b>Section</b> III-B were used to generate the corresponding noise. In measurement setup, single antenna is used to capture the impulsive noise, and we do not have yet measures for n [...]...|$|R
3000|$|... s {{measured}} {{between the}} centres {{of the two}} sections of the ring. σs is {{the area of the}} ring <b>section</b> <b>estimated</b> as the product of the length L [...]...|$|R
40|$|Experiments {{have been}} {{performed}} to validate and to supplement the intranuclear cascade model as a method for <b>estimating</b> cross <b>sections</b> of importance to spacecraft shield design. The experimental situation is inconclusive particularly for neutron-producing reactions, but is relatively sound for reaction cross sections and for proton spectra at several hundred MeV at medium forward angles. Secondary photon contributions are imprecisely known...|$|R
40|$|In this paper, we {{conduct an}} {{empirical}} comparison of travel time estimation methods based on single-loop detector data. The methods of concern are the regression method {{based on an}} intuitive stochastic model as proposed by Petty et al. in [7], and the conventional method of using an identity relating speed, flow and occupancy with the assumption of a common vehicle length. The analysis is tailored to fit in the limitations imposed by available field data sets. We also introduce several variations of the regression method and give examples which suggest directions for future work to further improve the regression method. The comparison is composed of three interrelated parts, each with a different focus:local comparison (concerning a single link of freeway), comparison of <b>estimated</b> <b>section</b> travel times over a prolonged stretch of freeway with multiple links and a visualized approach which enables investigation of performance patterns {{in time and space}} of the estimation methods. Travel time (Traffic engineering) [...] Mathematical models, Traffic estimation, intelligent transportation systems...|$|R
40|$|This note applies ECCEP {{estimates}} of service productivity curves, service prices, and service utilisation, and the {{predictions about the}} consequences of achieving perfect efficiency, in the discussion of three policy propositions. It deals primarily with only two of the seventeen outputs for which productivity curves have been <b>estimated.</b> <b>Section</b> I: defines two dimensions of productive efficiency analysed, explains and justifies in the context of reform argument the choice of three scenarios setting the framework for deducing the implications of productivities, prices and information about utilisation for what would be the best allocation of resources, and relates the targeting implications of making the best use of resources to targeting strategies for investment in efficiency improvement. Section II suggests what light the results throw on three policy propositions: allocate more to the less on the less dependent, if necessary, releasing resources by allocating less to the more dependent spend less on the older community services, and give higher priority to caregivers, less to users</li...|$|R
30|$|It {{should be}} noted that the {{constant}} T_ 0 depends on m. For a uniformly valid T_ 0 > 0 for every m∈N^*, we need the <b>estimates</b> in next <b>section.</b>|$|R
