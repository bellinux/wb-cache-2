273|1233|Public
5000|$|P(HFEijr): the {{probability}} of human <b>failure</b> <b>event</b> (HFEijr) occurring ...|$|E
5000|$|A human {{reliability}} analysis calculator {{to determine a}} human <b>failure</b> <b>event</b> probability based upon the task type and compounding performance shaping factors ...|$|E
5000|$|The largest {{credible}} single generation <b>failure</b> <b>event,</b> {{which is}} currently either Sizewell B nuclear power station (1,260 MW) or one cable of the HVDC Cross-Channel interconnector (1,000 MW) ...|$|E
5000|$|Define Human <b>Failure</b> <b>Events</b> (HFE’s) and/or unsafe actions (UAs) {{which may}} affect the task in {{question}} ...|$|R
5000|$|It {{increases}} the {{guarantee that the}} key risks associated with the Human <b>Failure</b> <b>Events</b> in question have been identified.|$|R
50|$|The {{function}} f {{is sometimes}} called the event density; it is the rate of death or <b>failure</b> <b>events</b> per unit time.|$|R
5000|$|The 62 Banqiao Dams <b>failure</b> <b>event</b> in China in 1975, due to Typhoon Nina. Approximately 86,000 {{people died}} from {{flooding}} and another 145,000 died from subsequent diseases, total of 231,000 deaths.|$|E
50|$|On October 16, 2008 the NTSB {{recommended}} that the FAA issue urgent new inspection procedures on the PW2037 model of the engine, following an uncontained turbine <b>failure</b> <b>event</b> in August 2008. The NTSB {{recommended that}} the FAA order PW2037 engines inspected beyond a threshold of flight hours or flight cycles {{less than that of}} the event engine, and be reinspected at regular intervals.|$|E
50|$|Data are {{entered by}} {{installation}} and by owner. Each {{piece of equipment}} (e.g. a gas turbine) constitutes a single database inventory record, which includes a technical description of the equipment, and of its environmental and operating conditions, along with all associated failure events. Every <b>failure</b> <b>event</b> is given a set of data including failure cause, date, effect, and mode. Corrective and preventive maintenance data are also included.|$|E
50|$|Subset {{simulation}} is {{a method}} used in reliability engineering to compute small (i.e., rare <b>event)</b> <b>failure</b> probabilities encountered in engineering systems. The basic idea is to express a small failure probability {{as a product of}} larger conditional probabilities by introducing intermediate <b>failure</b> <b>events.</b> This conceptually converts the original rare event problem into a series of frequent event problems that are easier to solve. In the actual implementation, samples conditional on intermediate <b>failure</b> <b>events</b> are adaptively generated to gradually populate from the frequent to rare event region. These 'conditional samples' provide information for estimating the complementary cumulative distribution function (CCDF) of the quantity of interest (that governs failure), covering the high as well as the low probability regions. They can also be used for investigating the cause and consequence of <b>failure</b> <b>events.</b> The generation of conditional samples is not trivial but can be performed efficiently using Markov Chain Monte Carlo (MCMC).|$|R
40|$|Specific {{criteria}} {{have been}} established to define the occurrence of myocardial infarction (MI) and stroke in cardiovascular clinical trials, {{but there is not}} a consistent definition for heart <b>failure.</b> Heart <b>failure</b> <b>events</b> appear to occur at a rate that is similar to stroke and MI in trials of hypertension, hyperlipidaemia, diabetes, and coronary heart disease, yet a consistent approach to defining heart <b>failure</b> <b>events</b> has not yet been realized. The wide range of definitions used in clinical trials makes it difficult to interpret new data in the context of existing literature. This inconsistency has led to challenges in determining the incidence of heart failure in cardiovascular studies and the effects of interventions on these endpoints. This paper examines issues related to defining heart <b>failure</b> <b>events</b> in cardiovascular clinical trials and presents a definition to formally address this issu...|$|R
40|$|Stochastic {{processes}} often exhibit sudden systematic {{changes in}} pattern {{a short time}} before certain <b>failure</b> <b>events.</b> Examples include increase in medical costs before death and decrease in CD 4 counts before AIDS diagnosis. To study such terminal behavior of stochastic processes, a natural and direct way is to align the processes using <b>failure</b> <b>events</b> as time origins. This paper studies backward stochastic processes counting time backward from <b>failure</b> <b>events,</b> and proposes one-sample nonparametric estimation of the mean of backward processes when follow-up is subject to left truncation and right censoring. We will discuss benefits of including prevalent cohort data to enlarge the identifiable region and large sample properties of the proposed estimator with related extensions. A SEER [...] Medicare linked data set is used to illustrate the proposed methodologies. Comment: Published in at [URL] the Annals of Applied Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
50|$|Mass wasting is the {{geomorphic process}} by which surface {{material}} move downslope typically as a mass, largely under {{the force of gravity}} As rivers flow down steeply sloping mountains, deep channel incision occurs as the river's flow wears away the underlying rock. Large channel incision progressively decreases the amount of gravitational force needed for a slope <b>failure</b> <b>event</b> to occur, eventually resulting in mass wasting. Removal of large amounts of surface mass in this fashion will induce an isostatic response resulting in uplift until equilibrium is reached.|$|E
50|$|A {{special case}} of train {{handling}} problems is overspeed on sharp curves. This generally arises when a driver fails {{to slow the}} train for a sharp curved section in a route that otherwise has higher speed conditions. In the extreme this results in the train entering a curve at a speed at which it cannot negotiate the curve, and gross derailment takes place. It takes roughly 6.976 × 106 N to completely derail a train traveling at such circumstances. The specific mechanism of this may involve bodily tipping (rotation) but is likely to involve disruption of the track structure and derailment as the primary <b>failure</b> <b>event,</b> followed by overturning.|$|E
50|$|On July 19, 1989, United Airlines Flight 232, a McDonnell Douglas DC-10-10, {{experienced}} an uncontained failure of its No. 2 engine stage 1 fan rotor disk assembly. The engine fragments severed the No. 1 and No. 3 hydraulic system lines. Forces {{from the engine}} failure fractured the No. 2 hydraulic system line. With the loss of all three hydraulic-powered flight control systems, safe landing was impossible. The lack of independence of the three hydraulic systems, although physically isolated, left them vulnerable to a single <b>failure</b> <b>event</b> due to their close proximity to one another. This was a zonal hazard. The aircraft crashed after diversion to Sioux Gateway Airport in Sioux City, Iowa, with 111 fatalities, 47 serious injuries and 125 minor injuries.|$|E
40|$|Abstract — In {{this paper}} we show how {{information}} contained in robust network codes {{can be used for}} passive inference of possible locations of link failures or losses in a network. For distributed randomized network coding, we bound the probability of being able to distinguish among a given set of <b>failure</b> <b>events,</b> and give some experimental results for one and two link failures in randomly generated networks. We also bound the required field size and complexity for designing a robust network code that distinguishes among a given set of <b>failure</b> <b>events.</b> I...|$|R
40|$|The <b>failure</b> <b>events</b> {{which can}} occur with the fastest {{timescales}} in the LHC are mainly {{associated with the}} injection and extraction processes. The possible worst case <b>failure</b> <b>events</b> are catalogued and the protection layers in place to prevent the failure or mitigate the consequences are described. Particular {{attention is paid to}} the beam dump system kickers, the energy tracking system, the injection kickers and the aperture/tune kickers. The requirements for the positioning of the dedicated absorbers with respect to the aperture will be recalled, and the implications for the setting-up and operational stability critically examined...|$|R
40|$|Abstract. Previous {{studies on}} {{inferring}} {{the origin of}} routing changes in the Internet are limited to <b>failure</b> <b>events</b> that generate {{a large number of}} routing changes. In this paper, we present a novel approach to origin inference of small <b>failure</b> <b>events.</b> Our scheme focuses on routing changes imposed on preferred paths of prefixes and not on transient paths trig-gered by path exploration. We first infer the preferred path of each prefix and measure the stability of each inter-AS link over this preferred path. The stability is measured based on routing changes of specific prefixes that regularly use the link and are advertised by the AS adjacent to the link. We then correlate the stability of other links over this path and infer the instability boundary as the origin. Our analysis using Oregon Route-Views data and trouble tickets from operational networks shows that our inference scheme can identify the origins of small <b>failure</b> <b>events</b> with very high accuracy. ...|$|R
50|$|The initial {{density of}} the {{sediment}} {{plays a key role}} in the mobilization into flows and the distances that the slide will travel. If the sediment is a soft, fluid material then the slide is likely to travel great distances and a flow is more likely to occur. However, if the sediment is stiffer then the slide will only travel a short distance and a flow is less likely to occur. Furthermore, the ability to flow may also be dependent upon the amount of energy transferred to the falling sediment throughout the <b>failure</b> <b>event.</b> Often large landslides on the continental margin are complicated and components of slide, debris flow and turbidity current may all be apparent when examining the remains of a submarine landslide.|$|E
50|$|On August 12, 1985, Japan Air Lines Flight 123, a Boeing 747-SR100, {{experienced}} {{cabin decompression}} 12 minutes after takeoff from Haneda Airport in Tokyo, Japan, at 24,000 feet. The decompression {{was caused by}} failure of a previously repaired aft pressure bulkhead. Cabin air rushed into the unpressurized fuselage cavity, overpressurizing the area and causing failure of the auxiliary power unit (APU) firewall and the supporting structure for the vertical fin. The vertical fin separated from the airplane. Hydraulic components located in the aft body were also severed, leading to a rapid depletion of all four hydraulic systems. The loss of the vertical fin, coupled {{with the loss of}} all four hydraulic systems, left the airplane extremely difficult, if not impossible, to control in all three axes. Lack of independence of four hydraulic systems from a single <b>failure</b> <b>event</b> was a zonal hazard. The aircraft struck a mountain at forty-six minutes after takeoff with 520 fatalities and 4 survivors.|$|E
40|$|New {{approach}} for studying failure mechanism is use of multispan-beam shear test, which puts some regions of specimen in almost pure shear and enables observation of location of initial failure and {{way in which}} damage propagates. Test stopped at any time, such as when first <b>failure</b> <b>event</b> occurs, for study of phenomenon or taking photographs of <b>failure</b> <b>event.</b> Individual plies studied easily with long-distance microscope or from photographs taken during test...|$|E
5000|$|Estimating the {{conditional}} probabilities {{by means of}} simulation requires the efficient generation of samples of X conditional on the intermediate <b>failure</b> <b>events,</b> i.e., {{the conditional}} samples. This is generally non-trivial.|$|R
25|$|Lambda {{denotes the}} failure rate of devices and systems in {{reliability}} theory, {{and it is}} measured in <b>failure</b> <b>events</b> per hour. Numerically, this lambda is also the reciprocal of the mean time between failures.|$|R
40|$|In {{large-scale}} networked computing systems, component failures become norms {{instead of}} exceptions. Failure prediction {{is a crucial}} technique for self-managing resource burdens. <b>Failure</b> <b>events</b> in coalition systems exhibit strong correlations {{in time and space}} domain. In this paper, we develop a spherical covariance model with an adjustable timescale parameter to quantify the temporal correlation and a stochastic model to describe spatial correlation. We further utilize the information of application allocation to discover more correlations among failure instances. We cluster <b>failure</b> <b>events</b> based on their correlations and predict their future occurrences. We implemented a failure prediction framework, called PREdictor of <b>Failure</b> <b>Events</b> Correlated Temporal-Spatially (hPrefects), which explores correlations among failures and forecasts the time-between-failure of future instances. We evaluate the performance of hPrefects in both offline prediction of failure by using the Los Alamos HPC traces and online prediction in an institute-wide clusters coalition environment. Experimental results show the system achieves more than 76 % accuracy in offline prediction and more than 70 % accuracy in online prediction during the time from May 2006 to April 2007...|$|R
3000|$|In a competing-risk {{model the}} <b>failure</b> <b>event</b> can occur {{for more than}} one reason. In this dataset {{terminating}} employment can lead to three different events: to another employment contract (T [...]...|$|E
30|$|Road-induced erosion events: we {{measured}} their position {{along the road}} and elevation using a Geographic Positioning System (GPS), their dimensions using a laser distance meter; we also noted <b>failure</b> <b>event</b> activity/age, the material involved, etc.|$|E
40|$|This {{multiple}} {{case study}} of eight entrepreneurial narratives of failed businesses examines how narratives that express different emotional states (folks) reflect different efforts {{to make sense of}} failure experiences (strokes). Our comparisons of the narratives 2 ̆ 7 emotional content (describing emotional states at the time of business failure and presently) revealed some new insights. First, high negative emotions motivate making sense of a loss, while high positive emotions provide cognitive resources to facilitate and motivate making sense of the <b>failure</b> <b>event.</b> Second, emotion-focused coping helped deal with negative emotions. Finally, sensemaking was also facilitated by cognitive strategies that focused attention on the <b>failure</b> <b>event</b> and promoted self-reflection...|$|E
40|$|In {{dental implant}} {{research}} studies, {{events such as}} implant complications including pain or infection may be observed recurrently before <b>failure</b> <b>events,</b> i. e. the death of implants. It is natural to assume that recurrent <b>events</b> and <b>failure</b> <b>events</b> are correlated to each other, since they happen on the same implant (subject) and complication times have strong effects on the implant survival time. On the other hand, each patient may {{have more than one}} implant. Therefore these recurrent <b>events</b> or <b>failure</b> <b>events</b> are clustered since implant complication times or failure times within the same patient (cluster) are likely to be correlated. The overall implant survival times and recurrent complication times are both interesting to us. In this paper, a joint modelling approach is proposed for modelling complication events and dental implant survival times simultaneously. The proposed method uses a frailty process to model the correlation within cluster and the correlation within subjects. We use Bayesian methods to obtain estimates of the parameters. Performance of the joint models are shown via simulation studies and data analysis. © 2013 Copyright Taylor and Francis Group, LLC...|$|R
40|$|Knowledge on <b>failure</b> <b>events</b> {{and their}} {{associated}} factors, gained from past construction projects, {{is regarded as}} potentially extremely useful in risk management. However, a number of circumstances are constraining its wider use. Such knowledge is usually scarce, seldom documented, and even unavailable when it is required. Further, there exists a lack of proven methods to integrate and analyze it in a cost-effective way. This article addresses possible options to overcome these difficulties. Focusing on limited but critical potential <b>failure</b> <b>events,</b> the article demonstrates how knowledge {{on a number of}} important potential <b>failure</b> <b>events</b> in tunnel works can be integrated. The problem of unavailable or incomplete information was addressed by gathering judgments from a group of experts. The elicited expert knowledge consisted of failure scenarios and associated probabilistic information. This information was integrated using Bayesian belief-networks-based models that were first customized {{in order to deal with}} the expected divergence in judgments caused by epistemic uncertainty of risks. The work described in the article shows that the developed models that integrate risk-related knowledge provide guidance as to the use of specific remedial measure...|$|R
40|$|Trend {{detection}} is {{an important}} task in failure data analysis in both reliability and maintenance. Usually, detecting certain possible trend in <b>failure</b> <b>events</b> of systems or items {{is the first step}} of data handling, and the result of trend detection indicates some directions for further statistical analysis of these data. Available statistical tests commonly used for detecting trends in <b>failure</b> <b>events</b> collected over time as well as the underlying hypotheses being tested are reviewed in this paper. Directions for future research on statistical trend tests are suggested {{at the end of the}} paper. These suggestions will address issues concerning analysis of failure data obtained from single and multiple systems. Department of Industrial and Systems Engineerin...|$|R
40|$|The core {{sense of}} {{accountability}} is the demanding and giving of accounts. Especially, when {{the outcome is}} negative, the management {{can be expected to}} account for the <b>failure</b> <b>event,</b> contextualizing the numerical outcome. In this paper the particular <b>failure</b> <b>event</b> of goodwill impairment, and the response of the management in the financial report is studied. Even though not necessarily being a <b>failure</b> <b>event,</b> {{it is reasonable to assume}} that an impairment will, if not explained, be perceived as such. Using an account typology the responses in financial reports have been scrutinised. In general the explanatory value of those responses is weak. Accountability does not seem to imply a requirement for the management to explain themselves in the financial report. The role of the financial report as a mean to hold the management accountable is likely weakened by the institutionalization of the reporting language, partly through the professionalization of report writing. Also, the possibilities of explaining complex cause and effect relationships might be accepted as limited. In this sense the accountability requirements of the financial report extend not much further than to the non-contextualized accounting language. Even though the performative quality of the language enhancing our understanding of the particular <b>failure</b> <b>event</b> is limited, the language used still performs as part of creating or protecting the discourse of “mergers and acquisitions”, deemphasising the rather few impairments made, describing them as being outside the control of the management or fairly insignificant. In this sense the language takes part in preserving the image of mergers and acquisitions as a prestigious and value creating business activity...|$|E
30|$|Furthermore, {{we observe}} {{a larger number}} of steps in the raw {{accelerations}} during the first and second orbit after an EDAC <b>failure</b> <b>event.</b> For some EDAC failure events, there is almost no effect visible after step correction, whereas for other EDAC failure events, the first and second orbit after an EDAC <b>failure</b> <b>event</b> seem to be of degraded quality. This is demonstrated in Fig.  4, where the blue curves show the step-corrected acceleration and the gray curves show the step-corrected acceleration shifted by one orbit (5623  s) into the future. By comparing the blue and gray curves, we see that the accelerations do not change much from one orbit to the next before the EDAC failure events. After the EDAC failure events, the left panel shows differences in the order of 200  nm/s^ 2 that are most likely an artifact of the EDAC <b>failure</b> <b>event,</b> whereas the right panel of Fig.  4 shows almost no changes from one orbit to the next in step-corrected accelerations. Thus, accelerations after EDAC failure events must be interpreted with caution. The reason for the different behavior is not known.|$|E
30|$|For {{landslide}} dam failure, it {{is found}} that landslide dam will easily breach or collapse, when the landslide-dam-deposits are fine. The Kaligandaki landslide dam <b>failure</b> <b>event</b> gave us a good lesson that geo-environmental disasters can be reduced by catching the precursory phenomena and necessary managements.|$|E
40|$|The {{purpose of}} this paper is to propose the method of {{reliability}} evaluation for structural systems using the higher-order moment standardization technique. In the reliability evaluation of structural systems, it is necessary to calculate the joint occurrence probability of multiple <b>failure</b> <b>events</b> that the system has. This paper formulates the joint occurrence probability of multiple <b>failure</b> <b>events</b> using the moments alone in a form that includes the non-Gaussian property of probability distribution. A method of reliability evaluation for structural systems is proposed by using the joint occurrence probability. Failure probabilities of several types of frame structure are calculated through the proposed method. The solutions by the proposed method agree with those by Monte Carlo simulation with sufficient precision...|$|R
40|$|A new {{simulation}} approach, called 'subset simulation', {{is proposed}} to compute small failure probabilities. The basic {{idea is to}} express the failure probability {{as a product of}} larger conditional failure probabilities by introducing intermediate <b>failure</b> <b>events.</b> With a proper choice of the intermediate <b>failure</b> <b>events,</b> the original problem of calculating a small failure probability, which is computationally demanding, is reduced to calculating a sequence of conditional probabilities, which are efficiently estimated by simulation using a special Markov chain. The proposed method is robust to the number of uncertain parameters and efficient in computing small probabilities. An example of calculating the first-excursion probability of a five-story shear building under uncertain seismic excitation is presented to demonstrate the efficiency of the method...|$|R
40|$|AbstractThe {{ultimate}} goal of prognostics within Through-life Engineering Services (TES) is to accurately predict the remaining useful life (RUL) of components. Prognostic frameworks inherently presume that there is predictability in the failure rate of the system, i. e. a system experiencing exclusively stochastic <b>failure</b> <b>events</b> cannot, by definition, be predictable. Prediction model uncertainties must be bound in some logical way. Therefore, to achieve an accurate prognostic model, uncertainty must first be reduced through the identification and elimination of {{the root causes of}} random <b>failure</b> <b>events.</b> This research investigates human error in maintenance activities as a major cause of random <b>failure</b> <b>events,</b> using a case study from the biopharmaceutical industry. Elastomer failures remain the number one contamination risk in this industry and data shows unexplained variability in the lifetime of real components when compared to accelerated lifetime testing in the lab environment. Technician error during installation and maintenance activities of elastomers is one possible cause for this and this research explores how these errors can be eliminated, reduced, or accounted for within the reliability modeling process. The initial approach followed was to improve technician training in order to reduce errors and thereby reduce the variability of random <b>failure</b> <b>events.</b> Subsequent data has shown an improvement in key metrics with failures now more closely matching data from lab testing. However, there is scope for further improvements and future research will explore the role of performance influencing factors in the maintenance task to identify additional causes of variation. These factors may then be incorporated as a process variable in a prognostics and health management (PHM) model developed for the system. The paper will present these data fusion approaches accounting for human factors as a roadmap to improving PHM model reliability...|$|R
