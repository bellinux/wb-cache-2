6|2256|Public
50|$|An image {{pipeline}} {{or video}} pipeline is {{the set of}} components commonly used between an image source (such as a camera, a scanner, or the rendering engine in a computer game), and an <b>image</b> <b>renderer</b> (such as a television set, a computer screen, a computer printer or cinema screen), or for performing any intermediate digital image processing consisting {{of two or more}} separate processing blocks. An image/video pipeline may be implemented as computer software, in a digital signal processor, on an FPGA, or as fixed-function ASIC. In addition, analog circuits can be used to do many of the same functions.|$|E
5000|$|Dr. Bahl {{began his}} {{professional}} career in 1988 {{as an engineer}} in the image processing research group at Digital Equipment Corporation in Maynard, Massachusetts. In 1990 he developed the computer industry’s first video compression and image rendering software library that shipped with every Ultrix and VMS computer. [...] Between 1990 and 1992, {{he worked on the}} Jvideo hardware prototype (and later the J300), a TURBOchannel based multimedia board for manipulating digital video on personal workstations. Following the success of these project, in 1993, he and his group shipped FullVideo and FullVideo Supreme, the IT industry’s first multimedia hardware product for VAX-, Alpha-, and Pentium-based workstations. FullVideo used two C-Cube CL 550 chips for simultaneous compression and decompression of JPEG streams, a Motorola DSP5001 for CD quality audio, and a propriety blue noise based <b>image</b> <b>renderer.</b> In 1994 Dr. Bahl was awarded a two-year doctoral fellowship from DEC, which enabled him to complete his PhD degree at the University of Massachusetts, Amherst. In 1997, he joined Microsoft Research in Redmond, Washington and developed the first Wi-Fi based indoor positioning system and the first public area Wi-Fi hot spot. In 2001 he formed the Networking Research Group and in 2010 the Mobility & Networking Research Group. Dr. Bahl advises Microsoft’s CEO and senior leadership team on long-term vision and strategy related to networking technologies. He and his group execute on the strategy through research, technology transfers to product groups, industry partnerships, and associated policy engagement with governments and research institutions around the world. Over the years his group has developed technologies such as campus-wide white space networking, wireless zero-configuration, Native Wi-Fi, Virtual Wi-Fi, firmware TPM, and the Xbox wireless controller protocol. His group is also known for re-thinking data center networking by redesigning its architecture, load balancer, management systems, network virtualization, and cloud services.|$|E
40|$|AbstractThere {{are some}} {{problems}} with the current popular search engines, namely filter bubble {{and the lack of}} visual representation. The result of a survey that we conducted shows that users were longing for a higher performing search engine with a holistic view of the search results. This work is aimed to elevate the effectiveness of web searching results by incorporating visual and social representation into the results. We have designed and developed a social and integrated search interface that utilizes three-dimensional <b>image</b> <b>renderer</b> to deal with the above issues. Social search results are incorporated with the basic search engine results, while giving users the capability of navigating and visualize through the search results using the three-dimensional <b>image</b> <b>renderer.</b> We tested the system by administering a survey to measure users’ experience and satisfaction. It can be concluded from the result of the survey that in general the users were happy with how the search interface works, including the social media search results and the three-dimensional visualization. The system was able to enhance the users’ experience when searching for some general topics by having a better visualization...|$|E
5000|$|... #Caption: K-3D, an {{animation}} and <b>image</b> <b>rendering</b> {{computer program}} ...|$|R
5000|$|... Methods and {{apparatus}} {{for performing}} <b>image</b> <b>rendering</b> and rasterization operations ...|$|R
5000|$|<b>Images</b> <b>Rendered</b> Bare. Vacant. Recognizable. at Stadium, New York, New York, 2012 ...|$|R
40|$|Image {{rendering}} is {{the process}} of creating realistic computer images from  geometric models and physical laws of light and reflection. This master thesis deals mainly with the numerical intricacies of implementing an <b>image</b> <b>renderer</b> using spherical harmonics. It investigates how to calculate the reflection of light in a surface using the Phong model, and employs ray tracing to create a realistic image of a geometric model. Further, it investigates different ways of calculating the spherical harmonic representation of a function defined on the sphere. The thesis also deals with the implementation of self-shadowing, and the effects of adding this component to the rendering equation...|$|E
40|$|In 1987, Robert Cook, Loren Carpenter, and Edwin Catmull {{released}} {{an article about}} the Reyes (Renders Everything You Ever Saw) image rendering pipeline [1]. This pipeline formed the basis for PRMan (Photo-realistic RenderMan), Pixar's ground-breaking <b>image</b> <b>renderer.</b> The basic Reyes pipeline proceeds in several steps: For each primitive in the scene: 1. Transform to Camera-Space 2. Bound - Eye-space bound of the primitive that will help cull primitives that are outside of the viewing area. 3. Split - A primitive can split itself into one or more smaller primitives. This will help to reduce the total number of polygons when the primitive gets diced. It will also allow parts of a primitive to be culled. 4. Dice - Convert the primitive into a grid of micropolygons. Each micropolygon {{is about the size of}} half of a pixel. 5. Shade - Perform lighting and shading calculations on each vertex in the micropolygon grid. 6. Draw - Scan convert and perform z-buffer calculations on the micropolygons of each primitive. ...|$|E
40|$|In a {{thin client}} {{computing}} architecture, application processing is delegated {{to a remote}} server rather than running the application locally. User input is forwarded to the server, and the rendered images are relayed through a dedicated remote display protocol to the user’s device. Existing remote display protocols have been successfully optimized for applications with only minor and low-frequent screen updates, such as a spreadsheet or a text editor. However, they are not designed {{to cope with the}} finegrained and complex color patterns of multimedia applications,leading to high bandwidth requirements and an irresponsive user interface. In this article, a hybrid remote display protocol approach is presented. The existing Remote FrameBuffer protocol of Virtual Network Computing (VNC-RFB) protocol is leveraged with a video streaming mode to transport the rendered images of multimedia applications to the client. Dependent on the amount of motion in the images to be presented, the images are relayed to the client either through the VNC-RFB protocol or through video streaming in the H. 264 format. The architecture of this hybrid <b>image</b> <b>renderer</b> is presented and the implementation is detailed. Furthermore, the decision heuristic to switch between the VNCRFB and the streaming mode is discussed. Experimental results clearly show the advantage of the hybrid approach in terms of client CPU and bandwidth requirements...|$|E
5000|$|... #Caption: An <b>image</b> <b>rendered</b> using path tracing, demonstrating notable {{features}} of the technique.|$|R
5000|$|... #Caption: Depiction of TPR repeat. <b>Image</b> <b>rendered</b> with King Software. PDB ID: 1NA0.|$|R
5000|$|Different {{spectral}} emissions / {{reflections of}} the object and of the <b>image</b> <b>render</b> process (e.g. a printer or monitor).|$|R
50|$|In August 2016, the iOS {{version of}} the app was updated to edit image offline by {{utilizing}} the phone's processor for <b>image</b> <b>rendering.</b>|$|R
5000|$|... #Caption: This <b>image</b> <b>rendering</b> {{suggests}} {{a type of}} formal crown (hokan) {{which would have been}} worn by a Japanese Imperial consort (published c. 1840).|$|R
5000|$|... #Caption: An <b>image</b> <b>rendered</b> using Mental Ray which {{demonstrates}} global illumination, photon maps, {{depth of}} field, ambient occlusion, glossy reflections, soft shadows and bloom ...|$|R
5000|$|Rendering features. Strata Design 3D CX {{is famous}} for its {{high-quality}} rendering ability - rendering being the creation of a finished, final <b>image.</b> <b>Rendering</b> features include: ...|$|R
3000|$|Figures  5 and 6 show {{image quality}} {{comparisons}} between our algorithm and TSM using multi-sampling {{with the same}} number of samples per pixel. Due to a small number of samples per pixel, <b>images</b> <b>rendered</b> by our algorithm and TSM have noise. However, <b>images</b> <b>rendered</b> by TSM have visual artifacts (a green highlighted inset in Fig.  5 and a red highlighted inset in Fig.  6) in the shadow areas while ours does not. The {{reason for this is that}} TSM uses two random times, t [...]...|$|R
5000|$|In 1982, Japan's Osaka University {{developed}} the LINKS-1 Computer Graphics System, a supercomputer that used up to 257 Zilog Z8001 microprocessors, used for rendering realistic 3D computer graphics. According to the Information Processing Society of Japan: [...] "The core of 3D <b>image</b> <b>rendering</b> is calculating the luminance of each pixel {{making up a}} rendered surface from the given viewpoint, light source, and object position. The LINKS-1 system was developed to realize an <b>image</b> <b>rendering</b> methodology in which each pixel could be parallel processed independently using ray tracing. By developing a new software methodology specifically for high-speed <b>image</b> <b>rendering,</b> LINKS-1 was able to rapidly render highly realistic images." [...] It was [...] "used to create the world's first 3D planetarium-like video of the entire heavens that was made completely with computer graphics. The video {{was presented at the}} Fujitsu pavilion at the 1985 International Exposition in Tsukuba." [...] The LINKS-1 was the world's most powerful computer, as of 1984.|$|R
5000|$|As {{the scene}} changes again, {{this time the}} <b>image</b> <b>rendered</b> will be of a surface {{changing}} shape, {{similar to that of}} water. This scene is rendered using a Voxel landscape rendering technique.|-| ...|$|R
5000|$|... 2001-2002: {{helped create}} Reach for the Stars, the first 3D <b>image</b> <b>{{rendered}}</b> in space, rendered in POV-Ray on the International Space Station (April 25 - May 5, 2002) by astronaut Mark Shuttleworth.|$|R
50|$|Configurations of BOXX Technologies {{systems are}} {{tailored}} for lower-end 2D graphic design, 3D animation and architectural CAD, and advanced 3D animation and modeling (multithreaded applications {{as well as}} digital compositing and <b>image</b> <b>rendering).</b>|$|R
5000|$|ENQUIRE {{was similar}} to Apple's HyperCard which also lacked {{clickable}} text and was not [...] "hypertext", but ENQUIRE lacked an <b>image</b> <b>rendering</b> system. The advantage {{was that it was}} portable and ran on different systems.|$|R
50|$|Many brands use LED {{backlighting}} technology, {{which offer}} the advantages over CCFL LCDs of reduced energy consumption, better contrast and brightness, greater color range, more rapid response {{to changes in}} scene and more accurate <b>image</b> <b>rendering.</b>|$|R
40|$|We {{present a}} {{versatile}} computational <b>image</b> <b>rendering</b> software of optically-acquired holograms. The reported software can process 4 Megapixel 8 -bit raw frames from a sensor array acquired at a sustained rate of 80 Hz. Video-rate <b>image</b> <b>rendering</b> {{is achieved by}} streamline image processing with commodity computer graphics hardware. For time-averaged holograms acquired in off-axis optical configuration with a frequency-shifted reference beam, wide-field imaging of one tunable spectral component is permitted. This software is validated by phase-stepped hologram rendering, and non-contact monitoring of surface acoustic waves by single and dual sideband hologram rendering. It demonstrates the suitability of holography for video-rate computational laser Doppler imaging in heterodyne optical configuration...|$|R
5000|$|Eric Haines is an American {{software}} engineer, and is {{an expert}} in computer graphics, specifically <b>image</b> <b>rendering.</b> [...] he is with Autodesk, Inc. as senior principal software engineer. He is a co-author of the book Real-Time Rendering.|$|R
50|$|During {{graduate}} school Blackwell joined Viaweb {{for which he}} wrote the <b>image</b> <b>rendering,</b> order processing and statistics software. The company was acquired by Yahoo in 1998, and Blackwell moved to Silicon Valley to lead the Yahoo Store development group.|$|R
50|$|A notable {{example of}} {{third-party}} node development is Denis Pontonnier's Additional Nodes. These free nodes enable modifying <b>images,</b> <b>renders,</b> procedural textures, Hypervoxels, object motions, animation channels, and volumetric lights. Also they enable particles and other meshes to drive node parameters.|$|R
50|$|A {{common use}} of this form of data {{structure}} is in efficient <b>image</b> <b>rendering.</b> The underlying method constructs a signed distance field that extends from the boundary, {{and can be used}} to solve the motion of the boundary in this field.|$|R
50|$|WebFX was {{developed}} for website content generation, and to this end includes various web-orientated tools such as image slicing, java rollovers and web optimisation, together with rotoscoping and onion-skinning features for animation. However, <b>image</b> <b>rendering</b> is limited to 1024x1024 pixels.|$|R
50|$|This gallery {{represents}} {{a collection of}} <b>images</b> <b>rendered</b> using high quality volume ray casting. Commonly the crisp appearance of volume ray casting images distinguishes them from output of texture mapping VR due to higher accuracy of volume ray casting renderings.|$|R
40|$|A {{new image}} {{appearance}} model, designated as iCAM 06, {{has been developed}} for the applications of high-dynamic-range (HDR) <b>image</b> <b>rendering</b> and color <b>image</b> appearance prediction. The iCAM 06 model, based on the iCAM framework, incorporates the spatial processing models in the human visual system for contrast enhancement, photoreceptor light adaptation functions that enhance local details in highlights and shadows, and functions that predict {{a wide range of}} color appearance phenomena. This paper reviews the concepts of HDR imaging and image appearance modeling, presents the specific implementation framework of iCAM 06 for HDR <b>image</b> <b>rendering,</b> and provides a number of examples of the use of iCAM 06 in HDR rendering and color appearance phenomena prediction...|$|R
50|$|Reference Rendering Transform (RRT): Converts the scene-referred {{colorimetry}} to display-referred, and resembles traditional film <b>image</b> <b>rendering</b> with an S-shaped curve. It has {{a larger}} gamut and dynamic range available {{to allow for}} rendering to any output device (even ones not yet in existence).|$|R
50|$|Gilles Tran is a {{contemporary}} French 3D artist mostly {{known for his}} <b>images</b> <b>rendered</b> in POV-Ray and multiple winnings of the Internet Ray Tracing Competition. Most notable is his web site project The Book of Beginnings with images accompanied with pieces of unfinished text.|$|R
50|$|The {{following}} {{image is}} a mosaic of four different indexed color <b>images</b> <b>rendered</b> with a single shared master palette of 6-8-5 levels RGB plus 16 additional grays. Note the limited range of colors used for every image, and how many palette entries are left unused.|$|R
5000|$|Core Video is {{the video}} {{processing}} model employed by macOS. It links {{the process of}} decompressing frames from a video source {{to the rest of}} the [...] Quartz technologies for <b>image</b> <b>rendering</b> and composition. Both QuickTime X and [...] QuickTime 7 depend on Core Video.|$|R
2500|$|In {{the field}} of {{realistic}} rendering, Japan's Osaka University developed the LINKS-1 Computer Graphics System, a supercomputer that used up to 257 Zilog Z8001 microprocessors, in 1982, {{for the purpose of}} rendering realistic 3D computer graphics. According to the Information Processing Society of Japan: [...] "The core of 3D <b>image</b> <b>rendering</b> is calculating the luminance of each pixel making up a rendered surface from the given viewpoint, light source, and object position. The LINKS-1 system was developed to realize an <b>image</b> <b>rendering</b> methodology in which each pixel could be parallel processed independently using ray tracing. By developing a new software methodology specifically for high-speed <b>image</b> <b>rendering,</b> LINKS-1 was able to rapidly <b>render</b> highly realistic <b>images.</b> It was used to create the world's first 3D planetarium-like video of the entire heavens that was made completely with computer graphics. The video was presented at the Fujitsu pavilion at the 1985 International Exposition in Tsukuba." [...] The LINKS-1 was the world's most powerful computer, as of 1984. Also in {{the field of}} realistic rendering, the general rendering equation of David Immel and James Kajiya was developed in 1986 - an important step towards implementing global illumination, which is necessary to pursue photorealism in computer graphics.|$|R
