2|592|Public
3000|$|... b {{shows that}} the {{achieved}} throughput of PowerNap is better than IEEE 802.11 PSM and SleepWell. In both PowerNap and IEEE 802.11 PSM, the beacon interval has a significant contribution in network throughput as the increased value of beacon interval produces less overhead and the network throughput increases. As stated in our algorithm, it increases the listen time and shortens the overhear time of clients. <b>During</b> <b>listen</b> time, an AP checks if the timer is matured, and during overhear time, an AP overhears the channel for packets. In standard PSM, after waking up, every client overhears the channel for the entire beacon interval; however, in PowerNap, one client overhears the channel only when the AP associated with it is active for transmission. Similarly, the SleepWell incurs much overhead for repeated migration operations, and thus, it keeps the users in overhear mode for longer duration. As expected theoretically, for increasing beacon intervals, the protocol operation overheads of the studied protocols are drecreased, as shown in Fig. 4 [...]...|$|E
40|$|Perception {{and actions}} can be tightly coupled; but does a perceptual event dissociated from action {{processes}} still engage the motor system? We conducted 2 functional {{magnetic resonance imaging}} studies involving rhythm perception and production to address this question. In experiment 1, on each trial subjects 1 st listened in anticipation of tapping, and then tapped along with musical rhythms. Recruitment of the supplementary motor area, mid-premotor cortex (PMC), and cerebellum was observed <b>during</b> <b>listen</b> with anticipation. To test whether this activation was related to motor planning or rehearsal, in experiment 2 subjects naively listened to rhythms without foreknowledge that they would later tap along with them. Yet, the same motor regions were engaged despite no action [...] perception connection. In contrast, the ventral PMC was only recruited during action and action-coupled per-ceptual processes, whereas the dorsal part was only sensitive to the selection of actions based on higher-order rules of temporal organization. These functional dissociations shed light on the nature of action [...] perception processes and suggest an inherent link between auditory and motor systems in the context of rhythm...|$|E
40|$|Emotion-related {{areas of}} the brain, such as the medial frontal cortices, amygdala, and striatum, are {{activated}} <b>during</b> <b>listening</b> to sad or happy music as well as <b>during</b> <b>listening</b> to pleasurable music. Indeed, in music, like in other arts, sad and happy emotions might co-exist and be distinct from emotions of pleasure or enjoyment. Here we aimed at discerning the neural correlates of sadness or happiness in music as opposed those related to musical enjoyment. We further investigated whether musical expertise modulates the neural activity <b>during</b> affective <b>listening</b> of music. To these aims, 13 musicians and 16 non-musicians brought to the lab their most liked and disliked musical pieces with a happy and sad connotation. Based on a listening test, we selected themos...|$|R
5000|$|Sleep timer {{stops the}} audio book {{playback}} <b>during</b> nighttime <b>listening</b> ...|$|R
50|$|With a paid {{subscription}} to Google Play Music, {{in addition to}} the standard features users get access to on-demand streaming of 40 million songs, without advertisements <b>during</b> <b>listening,</b> no limit on number of skips, and offline music playback on the mobile apps. A one-time 30-day free trial for a {{subscription to}} Google Play Music is offered for new users.|$|R
40|$|In {{the present}} study we used {{transcranial}} magnetic stimulation (TMS) to investigate the influence of phonological and lexical properties of verbal items on the excitability of the tongue’s cortical motor representation <b>during</b> passive <b>listening.</b> In particular, we aimed to clarify if the difference in tongue motor excitability found <b>during</b> <b>listening</b> to words and pseudo-words (Fadiga et al. 2002) is due to lexical frequency or {{to the presence of}} a meaning per se. In order to do this, we investigated the time-course of tongue motor-evoked potentials (MEPs) <b>during</b> <b>listening</b> to frequent words, rare words, and pseudo-words embedded with a double consonant requiring relevant tongue movements for its pronunciation. Results showed that at the later stimulation intervals (200 and 300 ms from the double consonant) listening to rare words evoked much larger MEPs than listening to frequent words. Moreover, by comparing pseudo-words embedded with a double consonant requiring much or less tongue movements, we found that a pure phonological motor resonance was present only 100 ms after the double consonant. Thus, while the phonological motor resonance appears very early, the lexical-dependent motor facilitation takes more time to appear and depends on the frequency of the stimuli. The present results indicate that the motor system responsible for phonoarticulatory movements during speech production is also involved <b>during</b> speech <b>listening</b> in a strictly specific way. This motor facilitation reflects both the difference in the phonoarticulatory characteristics and the difference in familiarity of the verbal material...|$|R
40|$|In {{order to}} {{investigate}} the neurobiological mechanisms accompanying emotional valence judgements <b>during</b> <b>listening</b> to complex auditory stimuli, cortical direct current (dc) -electroencephalography (EEG) activation patterns were recorded from 16 right-handed students. Students listened to 160 short sequences taken from the repertoires of jazz, rock-pop, classical music and environmental sounds (each n = 40). Emotional valence of the perceived stimuli were rated on a 5 -step scale after each sequence. Brain activation patterns <b>during</b> <b>listening</b> revealed widespread bilateral fronto-temporal activation, but a highly significant lateralisation effect: positive emotional attributions were accompanied {{by an increase in}} left temporal activation, negative by a more bilateral pattern with preponderance of the right fronto-temporal cortex. Female participants demonstrated greater valence-related differences than males. No differences related to the four stimulus categories could be detected, suggesting that the actual auditory brain activation patterns were more determined by their affective emotional valence than by differences in acoustical “fine ” structure. The results are consistent with a model of hemispheric specialisation concernin...|$|R
30|$|Mined During Listening Block (MDLB)—This set {{identifies}} all {{the blocks}} {{included on the}} Blockchain <b>during</b> the <b>listening</b> period and propagated by the peers before the next block was discovered. There are 1209 blocks discovered by 530 source nodes and spread through 11, 179 destination nodes. The maximum number of blocks discovered by a single node <b>during</b> the <b>listening</b> time is 86. These are the only blocks analysed.|$|R
5000|$|The BIA+ {{is one of}} many {{models that}} was defined based on data from psycholinguistic or {{behavioral}} studies which investigate how the languages of bilinguals are manipulated <b>during</b> <b>listening,</b> reading and speaking each of them; however, BIA+ is now being supported by neuroimaging data linking this model to more neurally inspired ones which have a greater focus on the brain areas and mechanisms involved in these tasks.|$|R
40|$|International audienceIn {{dialogue}} speaker's and listener's breathings synchronize at {{the time}} of turn-taking. This suggests a mutual adaptation of speaker's and listener's breathing, which could {{be a part of the}} interactive speaker/listener alignment process that occurs in communication. In this framework we investigated whether breathing variations <b>during</b> <b>listening</b> to speech depend on the speaker who produces the speech and/or on the loudness of the speech signal. We recorded acoustic and breathing signals from two native speakers of German (a male and a female) while they were reading short texts with either a normal or a loud acoustic level. The two readers had different breathing kinematics. Then, we monitored breathing for 26 native female speakers of German, while they were listening to the readers' speech signals played back via loudspeakers. They listened either to the male or to the female speaker, starting either with the normal (5 texts) or with the loud readings (5 texts). After listening to each text, listeners had to briefly summarize it. We analyzed breathing kinematics <b>during</b> <b>listening</b> according to the reader (male vs. female), to the loudness condition (normal vs. loud) and to the order (normal first vs. loud first). Altogether, these preliminary results show that listeners' breathing is sensitive to the reader and to the loudness of the reader's speech. This sensitivity could be a physiological reaction, as breathing is closely linked with heartbeats and emotional state. However, the changes in listeners' breathing could also be due to an adaptation to specific characteristics of the reader's voice and/or rhythms. It could also result from a speaker-listener's breathing coupling, as it has been observed for body movements in dialogue or for brain activity <b>during</b> <b>listening...</b>|$|R
40|$|In {{the present}} study, we used {{transcranial}} magnetic stimulation (TMS) to investigate the influence of phonological and lexical properties of verbal items on the excitability of the tongue's cortical motor representation <b>during</b> passive <b>listening.</b> In particular, we aimed to clarify if the difference in tongue motor excitability found <b>during</b> <b>listening</b> to words and pseudo-words [Fadiga, L., Craighero, L., Buccino, G., Rizzolatti, G., 2002. Speech listening specifically modulates the excitability of tongue muscles: a TMS study. European Journal of Neuroscience 15, 399 - 402] is due to lexical frequency or {{to the presence of}} a meaning per se. In order to do this, we investigated the time-course of tongue motor-evoked potentials (MEPs) <b>during</b> <b>listening</b> to frequent words, rare words, and pseudo-words embedded with a double consonant requiring relevant tongue movements for its pronunciation. Results showed that at the later stimulation intervals (200 and 300 ms from the double consonant) listening to rare words evoked much larger MEPs than listening to frequent words. Moreover, by comparing pseudo-words embedded with a double consonant requiring or not tongue movements, we found that a pure phonological motor resonance was present only 100 ms after the double consonant. Thus, while the phonological motor resonance appears very early, the lexical-dependent motor facilitation takes more time to appear and depends on the frequency of the stimuli. The present results indicate that the motor system responsible for phonoarticulatory movements during speech production is also involved <b>during</b> speech <b>listening</b> in a strictly specific way. This motor facilitation reflects both the difference in the phonoarticulatory characteristics and the difference in the frequency of occurrence of the verbal material...|$|R
40|$|Objectives: Recent {{studies with}} event-related brain {{potentials}} (ERPs) investigating music processing found (early) negativities with righthemispheric predominance {{as a response}} to inappropriate harmonies within sequences of chords. The stimuli used in those studies were fairly artificial in order to control the experimental factors (e. g. variations in tempo and loudness were eliminated). This {{raises the question of whether}} these ERPs can also be elicited <b>during</b> <b>listening</b> to more naturalistic stimuli...|$|R
40|$|Infant crying can elicit {{sensitive}} caregiving {{as well as}} {{hostility and}} harsh parenting responses. In the current study (N= 42 females) with a double-blind experimental design, we tested the effect of intranasal oxytocin administration {{on the use of}} excessive force using a hand-grip dynamometer <b>during</b> <b>listening</b> to infant cry sounds. Participants' experiences with harsh parental discipline during childhood were found to moderate the effect of oxytocin administration on the use of excessive force. Participants' whose parents did not discipline them harshly used less excessive force in the oxytocin condition, but for participants who were disciplined harshly {{there was no difference between}} the oxytocin and placebo condition. Such effects were not found <b>during</b> <b>listening</b> to infant laughter. We conclude that early caregiving experiences constitute an important moderator of the prosocial and/or stress-reducing effects of oxytocin. Oxytocin administration may increase trust and cooperation in individuals with supportive backgrounds, but not generate this effect in individuals who as a consequence of unfavorable early caregiving experiences may have a bias toward negative interpretation of social cues. © The Author (2011). Published by Oxford University Press...|$|R
5|$|While {{some other}} designs use clamps to couple the disc to the platter, Roksan shares the same {{philosophy}} as Linn in believing that rigid coupling would cause rumble to be transmitted {{from the main}} bearing, thus colouring the sound. To further increase isolation, the Xerxes possesses a spindle that allows the user to centre the record, but {{which is supposed to}} be removed <b>during</b> <b>listening</b> so that the disc rests solely on the felt mat.|$|R
40|$|The {{measurement}} of brain activation <b>during</b> music <b>listening</b> {{is a topic}} that is attracting increased attention from many researchers. Because of their high spatial accuracy, functional MRI measurements are often used for measuring brain activation {{in the context of}} music listening. However, this technique faces the issues of contaminating scanner noise and an uncomfortable experimental environment. Electroencephalogram (EEG), however, is a neural registration technique that allows the {{measurement of}} neurophysiological activation in silent and more comfortable experimental environments. Thus, it is optimal for recording brain activations during pleasant music stimulation. Using a new mathematical approach to calculate intracortical independent components (sLORETA-IC) on the basis of scalp-recorded EEG, we identified specific intracortical independent components <b>during</b> <b>listening</b> of a musical piece and scales, which differ substantially from intracortical independent components calculated from the resting state EEG. Most intracortical independent components are located bilaterally in perisylvian brain areas known to be involved in auditory processing and specifically in music perception. Some intracortical independent components differ between the music and scale listening conditions. The most prominent difference is found in the anterior part of the perisylvian brain region, with stronger activations seen in the left-sided anterior perisylvian regions <b>during</b> music <b>listening,</b> most likely indicating semantic processing <b>during</b> music <b>listening.</b> A further finding is that the intracortical independent components obtained for the music and scale listening are most prominent in higher frequency bands (e. g. beta- 2 and beta- 3), whereas the resting state intracortical independent components are active in lower frequency bands (alpha- 1 and theta). This new technique for calculating intracortical independent components is able to differentiate independent neural networks associated with music and scale listening. Thus, this tool offers new opportunities for studying neural activations <b>during</b> music <b>listening</b> using the silent and more convenient EEG technolog...|$|R
40|$|Emotion-related {{areas of}} the brain, such as the medial frontal cortices, {{amygdala}} and striatum are activated <b>during</b> <b>listening</b> to sad or happy music as well as <b>during</b> <b>listening</b> to pleasurable music. Indeed, in music, like in other arts, sad and happy emotions might co-exist and be distinct from emotions of pleasure or enjoyment. Here we aimed at discerning the neural correlates of sadness or happiness in music as opposed those related to musical enjoyment. We further investigated whether musical expertise modulates the neural activity <b>during</b> affective <b>listening</b> of music. To these aims, 13 musicians and 16 non-musicians brought to the lab their most liked and disliked musical pieces with a happy and sad connotation. Based on a listening test, we selected the most representative 18 -sec excerpts of the emotions of interest for each individual participant. Functional magnetic resonance imaging (fMRI) recordings were obtained while subjects listened to and rated the excerpts. The cortico-thalamo-striatal reward circuit and motor areas were more active during liked than disliked music, whereas only the auditory cortex and the right amygdala were more active for disliked over liked music. These results discern the brain structures responsible for the perception of sad and happy emotions in music from those related to musical enjoyment. We also obtained novel evidence for functional differences in the limbic system associated with musical expertise, by showing enhanced liking-related activity in fronto-insular and cingulate areas in musicians...|$|R
40|$|Irregular {{and complex}} signals are {{ubiquitous}} in nature. The principal {{aim of this}} paper is to develop an index, quantifying the complexity of such signals, which is based on the distribution of the strengths of its orthogonal oscillatory modes estimated by singular value decomposition. The index is first tested with simulated chaotic and/or stochastic maps and flows. Among neural data analysis, the index is first applied to a cognitive EEG data set recorded from two groups, musicians and non-musicians, <b>during</b> <b>listening</b> to music and resting state. In the gamma band (30 - 50  Hz), musicians showed robust changes in complexity, consistent over various scalp regions, <b>during</b> <b>listening</b> to music from resting condition, whereas such changes were minimal for non-musicians. Then the index is used to separate healthy participants from epileptic and manic patients based on spontaneous EEG analysis. Finally, it is used to track a tonic-clonic seizure EEG signal, and a conspicuous change was found in the complexity profiles of delta band (1 - 3. 5  Hz) oscillations at the onset of seizure. We conclude that this index would be useful for quantification of a wide range of time series including neural oscillations...|$|R
40|$|Current {{models of}} {{cortical}} speech and language processing include multiple regions within the temporal lobe of both hemispheres. Human communication, by necessity, involves complex interactions between regions subserving speech and language processing with {{those involved in}} more general cognitive functions. To assess these interactions, we utilized an ecologically salient conversation-based approach. This approach mandates that we first clarify activity patterns at the earliest stages of cortical speech processing. Therefore, we examined high gamma (70 - 150 Hz) responses within the electrocorticogram (ECoG) recorded simultaneously from Heschl’s gyrus (HG) and lateral surface of the superior temporal gyrus (STG). Subjects were neurosurgical patients undergoing evaluation for treatment of medically intractable epilepsy. They performed an expanded version of the Mini-mental state examination (MMSE), which included additional spelling, naming, and memory-based tasks. ECoG was recorded from HG and the STG using multicontact depth and subdural electrode arrays, respectively. Differences in high gamma activity <b>during</b> <b>listening</b> to the interviewer and the subject's self-generated verbal responses were quantified for each recording site and across sites within HG and STG. The expanded MMSE produced widespread activation in auditory cortex of both hemispheres. No {{significant difference was found}} between activity <b>during</b> <b>listening</b> to the interviewer's questions and the subject's answers in posteromedial HG (auditory core cortex). A different pattern was observed throughout anterolateral HG and posterior and middle portions of lateral STG (non-core auditory cortical areas), where activity was significantly greater <b>during</b> <b>listening</b> compared to speaking. No systematic task-specific differences in the degree of suppression during speaking relative to listening were found in posterior and middle STG. Individual sites could, however, exhibit task-related variability in the degree of suppression during speaking compared to listening. The current study demonstrates that ECoG recordings can be acquired in time-efficient dialog-based paradigms, permitting examination of language and cognition in an ecologically salient manner. The results obtained from auditory cortex serve as a foundation for future studies addressing patterns of activity beyond auditory cortex that subserve human communication...|$|R
40|$|AbstractElucidating the cognitive, affective, {{and reward}} {{processes}} {{that take place}} <b>during</b> music <b>listening</b> is the aim {{of a growing number}} of researchers. Several authors have used the Bayesian brain framework and existing models of reward to interpret neural activity observed <b>during</b> musical <b>listening.</b> The claims from Friston and colleagues regarding the role of dopamine, as well as the demonstration that salience-seeking behavior naturally emerges from minimizing free energy, will be of potential interest to those seeking to understand the general principles underlying our motivation to hear music...|$|R
40|$|How do we {{understand}} the actions of other individuals if we can only hear them? Auditory mirror neurons respond both while monkeys perform hand or mouth actions and while they listen to sounds of similar actions [1, 2]. This system might be critical for auditory action understanding and language evolution [1 - 6]. Preliminary evidence suggests that a similar system may exist in humans [7 - 10]. Using fMRI, we searched for brain areas that respond both during motor execution and when individuals listened {{to the sound of}} an action made by the same effector. We show that a left hemispheric temporo-parieto-premotor circuit is activated in both cases, providing evidence for a human auditory mirror system. In the left premotor cortex, a somatotopic pattern of activation was also observed: A dorsal cluster was more involved <b>during</b> <b>listening</b> and execution of hand actions, and a ventral cluster was more involved <b>during</b> <b>listening</b> and execution of mouth actions. Most of this system appears to be multimodal because it also responds to the sight of similar actions. Finally, individuals who scored higher on an empathy scale activated this system more strongly, adding evidence for a possible link between the motor mirror system and empathy...|$|R
5000|$|... #Caption: American Kindergarten {{students}} <b>listening</b> <b>during</b> story time ...|$|R
5000|$|At a {{few points}} in the album, {{different}} sounds have been [...] "hidden" [...] (i.e. recorded at extremely low volume, {{so that they are}} inaudible <b>during</b> normal <b>listening).</b>|$|R
40|$|Eighty {{students}} who spoke Spanish at home {{were randomly assigned}} one of four teachers. Two of the teachers used {{a great deal of}} music in their classrooms while the other two did not. The students and their teachers remained together for two years - kindergarten and first grade. Literacy achievement data suggests that music had a positive effect on oral language and reading scores. Differences focused on the use of music for morning opening, music and signing while working with words, and the use of music <b>during</b> <b>listening</b> stations...|$|R
40|$|This diploma thesis {{deals with}} the issue of visualisaion <b>during</b> <b>listening</b> to the music in the music lessons at the {{elementary}} school. The thesis contains suggestions for both: visualisation-based listening work and listening work without visualisation. Theoretical part contains theoretical background of the topic. It covers the development of the listening activities, describes their contemporary conception and {{deals with the}}ir psychological aspects. Practical part of this study comprises the research surveys with didactic materials. Part of the work is a CD with listening tests, presentations and videostreams, which are adapted for presentation during classes...|$|R
40|$|This {{study was}} {{designed}} {{to examine the effects of}} high versus low non-verbal teacher affect and active versus passive student activities <b>during</b> music <b>listening</b> on preschool children's attention, paired-comparison piece preference, time spent listening, and piece recognition. Three- through five-year-old subjects (N = 94) participated in four small-group listening lessons and subsequent individual posttests. Through the use of a modified multiple baseline design, each of four treatment conditions, representing different sequences of instructional events, was replicated three times. All lessons were videotaped on a split screen showing both teacher and students. Data obtained through observation of the videotaped lessons indicated that high teacher affect was associated with higher levels of group attending behavior than was low affect, and active listening activities elicited similar or higher on-task behavior than passive activities. No significant effects concerning teacher affect <b>during</b> <b>listening</b> or piece familiarity were found in analyses of posttest piece preferences, time spent listening, or piece recognition, although some differences between older and younger children were evident...|$|R
40|$|Interest in {{therapeutic}} {{applications of}} music has recently increased, {{as well as}} the effort to understand the relationship between music features and physiological patterns. In this study, we present a methodology for characterizing music-induced effects on the dynamics of the heart rate modulation. It consists of three steps: (i) the smoothed pseudo Wigner-Ville distribution is performed to obtain a time-frequency representation of HRV; (ii) a parametric decomposition is used to robustly estimate the time-course of spectral parameters; and (iii) statistical population analysis is used to continuously assess whether different acoustic stimuli provoke different dynamic responses. Seventy-five healthy subjects were repetitively exposed to pleasant music, sequences of Shepard tones with the same tempo as the pleasant music and unpleasant sounds overlaid with the same sequences of Shepard tones. Results show that the modification of HRV parameters are characterized by an early fast transient phase (15 - 20 s), followed by an almost stationary period. All kinds of stimuli provoked significant changes compared to the resting condition, while <b>during</b> <b>listening</b> to pleasant music the heart and respiratory rates were higher (for more than 80 % of the duration of the stimuli, p 10 (- 5)) {{and the power of the}} HF modulation was lower (for more than 70 % of the duration of the stimuli, p 0. 05) than <b>during</b> <b>listening</b> to unpleasant stimuli...|$|R
40|$|Data de publicació electrónica: 06 - 01 - 2018 Listening to speech {{has been}} shown to {{activate}} motor regions, as measured by corticobulbar excitability. In this experiment, we explored if motor regions are also recruited <b>during</b> <b>listening</b> to non-native speech, for which we lack both sensory and motor experience. By administering Transcranial Magnetic Stimulation (TMS) over the left motor cortex we recorded corticobulbar excitability of the lip muscles when Italian participants listened to native-like and non-native German vowels. Results showed that lip corticobulbar excitability increased for a combination of lip use during articulation and non-nativeness of the vowels. Lip corticobulbar excitability was further related to measures obtained in perception and production tasks showing a negative relationship with nativeness ratings and a positive relationship with the uncertainty of lip movement during production of the vowels. These results suggest an active and compensatory role of the motor system <b>during</b> <b>listening</b> to perceptually/articulatory unfamiliar phonemes. This research was supported by the European Community Grant POETICON ++ (STREP-Project ICT – 288382) awarded to L. F, and grants from the European Commission Seventh Framework Programme (FP 7 / 2007 - 2013) : ERG grant agreement number 323961 (UNDER CONTROL); Cooperation grant agreement number 613465 (AThEME), the Spanish Ministerio de Economía y Competitividad (PSI 2015 - 66918 -P) and the Catalan Government (SGR 2014 – 1210) awarded to N. S. G...|$|R
40|$|The endless scale illusion, {{obtained}} by cyclically repeating a chromatic scale {{made up of}} Shepard tones, {{has been used in}} a variety of musical works. Music psychology and neuroscience has been interested in this particular psychoacoustic phenomenon mainly for studying the cognitive processes of pitch perception involved. In the present study, we investigated the emotional states induced by the Shepard-Risset glissando, a variant of the Shepard scale. For this purpose we chose three musical stimuli: a Matlab-generated Shepard Risset glissando, Jean-Claude Risset’s Computer Suite from Little Boy, which presents a Shepard-Risset glissando integrated in the aesthetic context of a composition, and an ordinary orchestral glissando taken from the opening of Iannis Xenakis’ Metastasis. Seventy-three volunteers completed a <b>listening</b> experiment <b>during</b> which they rated their emotional response to these stimuli on a 7 -point Likert scale and indicated whether they had experienced a disruption of equilibrium. Personality was also measured with the Five-Factor Model of personality traits. The results show that negative emotions were most strongly evoked <b>during</b> <b>listening</b> to each of the stimuli. We also found that the Shepard-Risset glissando illusion, both within the aesthetic context of a musical composition and on its own, was capable of evoking disruption of equilibrium, frequently leading to the associated feeling of falling. Moreover, generally for the Shepard-Risset glissando illusion, higher negative emotional ratings were given by individuals who had experienced a feeling of disturbance of equilibrium relative to those who had not had this experience. Finally, we found a complex pattern of relationships between personality and the subjective experience of the glissando. Openness to experience correlated positively with positive emotion ratings for the Computer Suite, while agreeableness correlated negatively with positive emotion ratings for the Matlab stimulus. Moreover, results indicated higher (Bonferroni-uncorrected) neuroticism for those who experienced an equilibrium disturbance relative to subjects who did not have this experience <b>during</b> <b>listening</b> to the Computer Suite. These findings suggest that musical paradoxes may be of interest not only for the insights they provide on our perceptual system, but also for the richness of the emotional experience elicited <b>during</b> <b>listening...</b>|$|R
40|$|Listening {{selectively}} to one out {{of several}} competing speakers in a “cocktail party” situation is a highly demanding task. It relies on a widespread cortical network, including auditory sensory, but also frontal and parietal brain regions involved in controlling auditory attention. Previous work has shown that, <b>during</b> selective <b>listening,</b> ongoing neural activity in auditory sensory areas {{is dominated by the}} attended speech stream, whereas competing input is suppressed. The relationship between these attentional modulations in the sensory tracking of the attended speech stream and frontoparietal activity <b>during</b> selective <b>listening</b> is, however, not understood. We studied this question in young, healthy human participants (both sexes) using concurrent EEG-fMRI and a sustained selective listening task, in which {{one out of}} two competing speech streams had to be attended selectively. An EEG-based speech envelope reconstruction method was applied to assess the strength of the cortical tracking of the to-be-attended and the to-be-ignored stream <b>during</b> selective <b>listening.</b> Our results show that individual speech envelope reconstruction accuracies obtained for the to-be-attended speech stream were positively correlated with the amplitude of sustained BOLD responses in the right temporoparietal junction, a core region of the ventral attention network. This brain region further showed task-related functional connectivity to secondary auditory cortex and regions of the frontoparietal attention network, including the intraparietal sulcus and the inferior frontal gyrus. This suggests that the right temporoparietal junction is involved in controlling attention <b>during</b> selective <b>listening,</b> allowing for a better cortical tracking of the attended speech stream. SIGNIFICANCE STATEMENT Listening selectively to one out of several simultaneously talking speakers in a “cocktail party” situation is a highly demanding task. It activates a widespread network of auditory sensory and hierarchically higher frontoparietal brain regions. However, how these different processing levels interact <b>during</b> selective <b>listening</b> is not understood. Here, we investigated this question using fMRI and concurrently acquired scalp EEG. We found that activation levels in the right temporoparietal junction correlate with the sensory representation of a selectively attended speech stream. In addition, this region showed significant functional connectivity to both auditory sensory and other frontoparietal brain areas <b>during</b> selective <b>listening.</b> This suggests that the right temporoparietal junction contributes to controlling selective auditory attention in “cocktail party” situations...|$|R
40|$|AbstractWe {{evaluated}} {{the effect of}} stress-less and comfortable acoustic information on the prefrontal cortex (PFC) activity. We measured the blood flow and hemoglobin concentration changes by Near Infrared Spectroscopy (NIRS) <b>during</b> <b>listening</b> to acoustic information. We observed that right and center PFC activity increases against unpleasant acoustic information. Left and center PFC decreases for user's favorite and stress-less acoustic information. These results suggest that comfortable and stress-less acoustic information are discriminated by this observation method of asymmetric activities in PFC. The method {{can be applied to}} find stress-less acoustic resources and their stress-less and comfortable combinations...|$|R
40|$|Contemporary Christian music {{appeared}} in Hungary in the 1960 s. In a unique period of Hungary when religion was suppressed and {{rock and roll}} was denounced, the merging of the two, Christian rock music was even more disapproved during the communist regime. For many, contemporary Christian music was more than simply music; it indicated opposition against the political system. Therefore contemporary Christian music was an alternative music not only from a clerical but also from a political point of view. This article tries to analyze the function of contemporary Christian music in listeners’ religious folklife, their experience <b>during</b> <b>listening</b> to contemporary Christian music...|$|R
40|$|While music {{triggers}} many {{physiological and}} psychological reactions, the underlying neural basis of perceived and experienced emotions <b>during</b> music <b>listening</b> remains poorly understood. Therefore, using functional {{magnetic resonance imaging}} (fMRI), I conducted a comparative study of the different brain areas involved in perceiving and feeling emotions <b>during</b> music <b>listening.</b> I measured fMRI signals while participants assessed the emotional expression of music (perceived emotion) and their emotional responses to music (felt emotion). I found that cortical areas including the prefrontal, auditory, cingulate, and posterior parietal cortices were consistently activated by the perceived and felt emotional tasks. Moreover, activity in the inferior frontal gyrus increased more during the perceived emotion task than <b>during</b> a passive <b>listening</b> task. In addition, the precuneus showed greater activity during the felt emotion task than <b>during</b> a passive <b>listening</b> task. The findings reveal that the bilateral inferior frontal gyri and the precuneus are important areas for the perception of the emotional content of music {{as well as for the}} emotional response evoked in the listener. Furthermore, I propose that the precuneus, a brain region associated with self-representation, might be involved in assessing emotional responses...|$|R
40|$|International audienceBackground: In schizophrenia, perceptual {{inundation}} {{related to}} sensory gating deficit can be evaluated ‘‘off-line’’ with the sensory gating inventory (SGI) and ‘‘on-line’’ <b>during</b> <b>listening</b> tests. However, no {{study investigated the}} relation between ‘‘off-line evaluation’’ and ‘‘on-line evaluation’’. The present study investigates this relationship. Methods: A sound corpus of 36 realistic environmental auditory scenes was obtained from a 3 D immersive synthesizer. Twenty schizophrenic patients and twenty healthy subjects completed the SGI and evaluated the feeling of ‘‘inundation’’ from 1 (‘‘null’’) to 5 (‘‘maximum’’) for each auditory scene. Sensory gating deficit was evaluated in half of each population group with P 50 suppression electrophysiological measure. Results: Evaluation of inundation <b>during</b> sound <b>listening</b> was significantly higher in schizophrenia (3. 25) compared to the control group (2. 40, P <. 001). The evaluation of inundation <b>during</b> the <b>listening</b> test correlated significantly with the perceptual modulation (n = 20, rho =. 52, P =. 029) and the over- inclusion dimensions (n = 20, rho =. 59, P =. 01) of the SGI in schizophrenic patients and with the P 50 suppression for the entire group of controls and patients who performed ERP recordings (n = 20, rho = -. 49, P =. 027). Conclusion: An evaluation of the external validity of the SGI was obtained through listening tests. The ability to control acoustic parameters {{of each of the}} realistic immersive environmental auditory scenes might in future research make it possible to identify acoustic triggers related to perceptual inundation in schizophrenia...|$|R
5000|$|The {{movie was}} debuted at the 50th Edinburgh International Film Festival in [...] The {{following}} month, <b>during</b> outside <b>listening</b> parties in Atlanta, Boston, Chicago, Los Angeles, and New York, preview showings were projected onto buildings three stories high.|$|R
5000|$|... #Caption: U.S. and Moroccan members <b>listen</b> <b>during</b> a {{briefing}} on Block 52 F-16 Fighting Falcon capabilities ...|$|R
40|$|Previous {{research}} suggests that the human left planum temporale (PT) {{plays an important role in}} language. To test this hypothesis, functional MRI (fMRI) data were collected from 12 normal right-handed subjects during passive and active listening to words and tone sequences. Several left hemisphere areas, including the superior temporal sulcus, middle temporal gyms, angular gyrus and lateral frontal lobe showed stronger activation during the word conditions. This was not true of the PT, which responded equally to tones and words <b>during</b> passive <b>listening</b> and more strongly to tones <b>during</b> active <b>listening.</b> The PT is likely to be involved in early auditory processing, while specifically linguistic functions are mediated by multimodal association areas distributed elsewhere in the left hemisphere...|$|R
