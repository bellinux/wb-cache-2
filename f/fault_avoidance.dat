48|12|Public
5000|$|... {{to enable}} in-process <b>fault</b> <b>avoidance</b> and fault {{prevention}} through proper development ...|$|E
50|$|The primary {{methods for}} dealing with faults include <b>fault</b> <b>avoidance,</b> fault tolerance, and fault {{detection}} and recovery. <b>Fault</b> <b>avoidance</b> covers proactive measures taken to minimize the occurrence of faults. These proactive measures {{can be in the}} form of transactions, replication and backups. Fault tolerance is the ability of a system to continue operation {{in the presence of a}} fault. In the event, the system should detect and recover full functionality. In any event, any actions taken should make every effort to preserve the single system image.|$|E
40|$|<b>Fault</b> <b>avoidance</b> methods on {{dynamically}} reconfigurable {{devices have}} been proposed to extend device life-time, while their quantitative comparison has not been sufficiently presented. This paper shows results of quantitative life-time evaluation by simulating <b>fault</b> <b>avoidance</b> procedures of representative five methods under the same conditions of wearout scenario, application and device architecture. Experimental results reveal 1) MTTF is highly correlated {{with the number of}} avoided faults, 2) there is the efficiency difference of spare usage in five <b>fault</b> <b>avoidance</b> methods, and 3) spares should be prevented from wear-out not to spoil lifetime enhancement. 1...|$|E
5000|$|<b>Fault</b> {{prediction}} & <b>avoidance</b> - {{take action}} before something fails ...|$|R
40|$|This paper {{discusses}} {{the feasibility of}} employing reconfigurable hardware in the avionics systems of future generations of launch vehicles for space. Such technology {{has the potential of}} being orders of magnitude faster than conventional embedded computer technology for many applications. It can be argued that avionics architectures that use such reconfigurable elements may vastly improve the flexibility, efficiency, performance and reusability of the avionics system. It can also be argued that such a flexible environment will facilitate the implementation of advanced and mission-specific real-time <b>fault</b> tolerance, <b>avoidance,</b> and corrective techniques and would greatly facilitate sensor/data fusion operations. However, before this technology can be applied to such safety critical applications there are several issues that must be resolved. Such issues center around the suitability and robustness of reconfigurable hardware for in-flight operation. In this paper we investigate many of these issues, focussing upon the use of reconfigurable hardware to form a generic multi-functional launch controller element that can be replicated as needed and distributed throughout the launch vehicle...|$|R
40|$|While I partly {{agree with}} earlier {{responses}} to Wolff’s anarchist proposal, I find them inadequate or counter-productive {{in one way}} or another. Partly in contrast and partly to supplement, I object to Wolff’s argument that individual autonomy is incompatible with state authority by arguing that this incompatibility is negated if individual autonomy is represented by internal moral and practical convictions for obeying state command. I <b>fault</b> Wolff’s <b>avoidance</b> of contractualism and his presentation ofstate as alien, and show that his definition of authority is actually a definition of power. I object to Wolff’s assumption that men are purely rational animals,and that rationality is the only component of human decisions, which is foundational to his proposal that men can exist without state or social organization. I argue that Wolff’s de jure attack on state ricochets into individual autonomy: if there is no state in the ideal sense, it is because there is no individual autonomy in the ideal sense...|$|R
40|$|Errors in {{computer}} systems can cause abnormal behavior and degrade data integrity and system availability. <b>Fault</b> <b>avoidance</b> {{techniques such as}} radiation hardening and shielding have been the major approaches to protecting the system from transient errors, but these techniques are expensive. Recently, unhardened Commercial Off-The-Shelf (COTS) components have been investigated for a low cost alternative to <b>fault</b> <b>avoidance</b> techniques, and Software Implemented Hardware Fault Tolerance (SIHFT) has been proposed to increase the data integrity and availability of COTS systems. ED 4 I is...|$|E
40|$|Reliable {{computing}} {{in critical}} tasks is a log-term issue in computer systems. Two major fields of research are <b>fault</b> <b>avoidance</b> techniques and fault tolerance techniques. Even though these techniques {{can be used}} together, {{in many cases they}} are in opposing sides in system design. Fault tolerance techniques are also separated in two major fields, hardware redundancy and time redundancy. In this paper, I am going to compare the <b>fault</b> <b>avoidance</b> techniques with the purely software implemented fault tolerance techniques from with the experiments carried out in a special space mission, the ARGOS project...|$|E
40|$|This paper {{presents}} an innovative application of EDAA - European design and Automation Association 1149. 4 and the Integrated Diagnostic Reconfiguration (IDR) as tools {{for the implementation}} of an embedded test solution for an Automotive Electronic Control Unit implemented as a fully integrated mixed signal system. The paper described how the test architecture can be used for <b>fault</b> <b>avoidance</b> with results from a hardware prototype presented. The paper concludes that <b>fault</b> <b>avoidance</b> can be integrated into mixed signal electronic systems to handle key failure modes. Comment: Submitted on behalf of EDAA ([URL]...|$|E
40|$|A multi-ship {{collision}} avoidance decision-making and path planning formulation is studied in a distributed way. This paper proposes {{a complete set}} of solutions for multi-ship {{collision avoidance}} in intelligent navigation, by using a top-to-bottom organization to structure the system. The system is designed with two layers: the collision avoidance decision-making and the path planning. Under the general requirements of the International Regulations for Preventing Collisions at Sea (COLREGs), the performance of distributed path planning decision-making for anti-collision is analyzed for both give-way and stand-on ships situations, including the emergency actions taken by the stand-on ship in case of the give-way ship’s <b>fault</b> of collision <b>avoidance</b> measures. The Artificial Potential Field method(APF) is used for the path planning in details. The developed APF method combined with the model of ship domain takes the target ships’ speed and course in-to account, so that it can judge the moving characteristics of obstacles more accurately. Simulation results indicate that the system proposed can work effectiveness...|$|R
40|$|A novel, hybrid, agent {{architecture}} for (small) swarms of satellites has been developed. The software architecture for each satellite comprises a high-level rational agent {{linked to a}} low-level control system. The rational agent forms dynamic goals, decides how to tackle them and passes the actual implementation of these plans to the control layer. The rational agent also has access to a MatLabmodel of the satellite dynamics, thus allowing it to carry out selective hypothetical reasoning about potential options. This hybrid architecture has been implemented on simulated Earth orbiting satellites and hardware at Southampton’s satellite hardware simulation lab. While the deployment of satellites in formation within an Earth orbit is clearly both interesting and useful, this paper takes the spacecraft swarm further a?eld. In particular, we here investigate the rational agent autonomy problem, including collision <b>avoidance,</b> <b>fault</b> diagnosis and recovery, and cooperative behaviour of such spacecraft deployed to explore groups of Trojan Asteroids...|$|R
40|$|One of {{the major}} {{challenges}} in wide use of Grid workflow systems is <b>fault</b> tolerance and <b>avoidance.</b> Checkpointing schemes provide a way of fault detection and recovery. In our research, we focus on performance optimization of checkpointing schemes and DVS (Dynamic Voltage Scaling) for Grid workflow systems. We propose offline checkpointing schemes with DVS and online adaptive checkpointing schemes that dynamically adjust the checkpointing intervals by using store-checkpoints (SCPs) and compare-checkpoints (CCPs). When combined with DVS, offline adaptive checkpointing schemes not only are fault tolerant but also lead to reduce average execution time of tasks. These schemes can efficiently utilize comparison and storage operations and significantly improve the performance. Further, these schemes can calculate the optimal numbers of checkpoints by which minimize the mean execution time. We also expand the online adaptive checkpointing schemes from single-task execution scenarios to multi-task execution scenarios. Simulation results show these online schemes outstandingly {{increase the likelihood of}} timely task completion when faults occur...|$|R
40|$|This article {{presents}} an online and offline built-in self-test architecture implemented as an SRAM intellectual-property core for telecommunication applications. The architecture combines fault-latency reduction, code-based fault detection, and architecture-based <b>fault</b> <b>avoidance</b> to meet reliability constraint...|$|E
40|$|A {{reliable}} {{computer system}} needs to provide its normal {{level of service}} {{in the presence of}} hardware and software faults [Avi 86, Lap 92]. There are two philosophies of achieving this reliability: <b>fault</b> <b>avoidance,</b> wherein techniques prevent the occurrence of faults in the first place, and fault tolerance, wherein techniques allow the system to continue execution despite the occurrence o...|$|E
40|$|Parts of the Poukawa, Waipukurau and Tukituki Fault Zones in Central Hawke’s Bay District {{have been}} mapped in detail {{according}} to the Guidelines of the Ministry for the Environment’s “Planning for Development of Land on or Close to Active Faults”. Fault traces with associated <b>Fault</b> <b>Avoidance</b> Zones have been mapped to produce corridors surrounding the active fault traces at a scale suitable {{for the purposes of}} cadastral zoning, i. e. c. 1 : 5000. Particular {{attention has been paid to}} the urban areas of Otane, Waipawa and Waipukurau. Mapping of the <b>Fault</b> <b>Avoidance</b> Zones has been undertaken using a Geographic Information System (GIS) in conjunction with rectified aerial photographs, ortho-photographs and LiDAR imagery. Each of these media have a level of uncertainty for mapping. We found that a 5 -m pixel Shaded Relief LiDAR image produced particularly accurate micro-topography of the mapping areas, allowing us to map fault features to a level of accuracy not possible with the other more traditional methods...|$|E
40|$|Abstract- One of {{the most}} {{important}} and efficient factors in longevity of wireless sensor networks is their energy consumption. Therefore; the less energy consumption, the more networks lifetime. On another hand, <b>fault</b> tolerance and <b>avoidance</b> of Repeating operations, which are because of faults should be re-done will reduce energy consumption and cause the longevity of these networks. In this paper, we will review and introduce the fault tolerant clustering protocols like: Leach and DSC and also fault tolerant clustering protocols like: FT – LRC together with their weak points. Furthermore, by taking the advantage of these protocols, we’ll introduce a new protocol named FT-LRC, which has the whole benefits of previous protocols and has the least Energy Consumption Level. In this regard, fault tolerant is better than other previous protocols and other main advantages of these clustering protocols, which make them distinguished among the others is that it does re-clustering phase locally. While other protocols to do this matter globally, which cause procrastination and more energy consumption and reduction of network‘s total efficiency...|$|R
40|$|Background: Persistent {{pain and}} {{psychological}} distress are common after traumatic musculoskeletal injury (TMsI). Individuals sustaining a TMsI are often young, do not recover quickly, {{and place a}} large economic burden on society. Objectives: The aim of this systematic review is to determine (1) the incidence of persistent pain following TMsI, (2) the characteristics of pain, characterized by injury severity and type, and (3) risk and protective factors associated with persistent pain following TMsI. Methods: A systematic search of electronic databases (MEDLINE®, PubMed®, Embase, and PsycINFO®) was conducted for prospective, interventional, or noninterventional studies measuring the incidence of pain associated with TMsI. Results: The search revealed 4388 studies. Eleven studies examined persistent pain and met inclusion criteria. Pain was assessed using a validated measure of pain intensity or pain presence in six studies. Persistent pain was reported by all studies at variable time points up to 84 months postinjury, with wide variation among studies in pain intensity (ie, from mild to very severe) and pain incidence at each time point. The incidence of pain decreased over time within each study. Two studies found significant relationships between injury severity and persistent pain. Frequently cited predictive factors for persistent pain included: symptoms of anxiety and depression, patient perception that the injury was attributable to external sources (ie, they were not at <b>fault),</b> cognitive <b>avoidance</b> of distressing thoughts, alcohol consumption prior to trauma, lower educational status, being injured at work, eligibility for compensation, pain at initial assessment, and older age. Conclusion and implications: The evidence from the eleven studies included in this review indicates that persistent pain is prevalent up to 84 months following traumatic injury. Further {{research is needed to}} better evaluate persistent pain and other long-term posttraumatic outcomes. Joel Katz is supported by a Canadian Research Chair in Health Psychology at York University...|$|R
40|$|AbstractThe {{objective}} {{of this research is}} to develop a task scheduling algorithm for grid services with improved resource utilization, fault tolerance capability and to avoid deadlocks. The algorithm simulated using GridSim toolkit, which is used to measure the effectiveness of scheduling and related algorithms in grid environments with reduced complexity than the real imple-mentation. Two advanced methods, Two Phase Commit Protocol with novel back filling Technique (TPCNBF) and Two Phase Commit Protocol with novel back filling technique and task migration (TPCNBFM) were developed to schedule the tasks. The backfilling technique is used to improve resource utilization and fault tolerance in job scheduling. Two phase commit protocol prevents deadlocks in task scheduling. Fault tolerance in task scheduling is improved by task migration to protect tasks from any resource failure. TPCNBF and TPCNBFM methods give higher performance improvement than the current method called Cost Driven Work Flow Scheduling (CDWFS). In this method, the scheduling is based on deadline and the resource status. The experimental results showed that the TPCNBF and TPCNBFM have less makespan, cost, high success rate and throughput. The new algorithm can be used to schedule computing intensive tasks in grid services like Astronomy, high energy physics with deadlock <b>avoidance,</b> <b>fault</b> tolerance and improved resource utilization...|$|R
40|$|For the {{improvement}} of APR 1400 Diverse Protection System (DPS) design, the Advanced DPS (ADPS) has recently been developed to enhance the fault tolerance capability of the system. Major fault masking features of the ADPS compared with the APR 1400 DPS are the changes to the channel configuration and reactor trip actuation equipment. To minimize the fault occurrences within the ADPS, and to mitigate the consequences of common-cause failures (CCF) within the safety I&C systems, several <b>fault</b> <b>avoidance</b> design features have been applied in the ADPS. The <b>fault</b> <b>avoidance</b> design features include the changes to the system software classification, communication methods, equipment platform, MMI equipment, etc. In addition, the fault detection, location, containment, and recovery processes have been incorporated in the ADPS design. Therefore, {{it is expected that}} the ADPS can provide an enhanced fault tolerance capability against the possible faults within the system and its input/output equipment, and the CCF of safety systems...|$|E
40|$|<b>Fault</b> <b>avoidance,</b> fault {{removal and}} fault {{tolerance}} represent three successive lines {{of defense against}} the contingency of faults in software systems {{and their impact on}} system reliability. The law of diminishing returns advocates that these three sets of methods be put to bear to achieve effective software verification and validation: each method is used in the context where it is most effective. In this paper, we present an integrated approach to verification and validation, where we identify what aspects each set of methods is best adapted to deal with. Keywords <b>Fault</b> <b>avoidance,</b> Fault removal, Fault tolerance, Formal specifications, Verification and validation. 1 Successive Lines of Defense Despite three decades of intensive research, the verification and validation of software products remains an active research area. A great deal of progress has been achieved in this field, but the advent of new programming languages and new software development paradigms, combined with the incre [...] ...|$|E
40|$|Phase Change Memory (PCM) is an {{emerging}} non-volatile memory technology that could either replace or augment DRAM and NAND flash that are hindered by scalability challenges. PCM {{suffers from a}} limited endurance problem {{that needs to be}} alleviated before it can be endorsed into the memory stack. This thesis is based on the observation that the endurance problem and its ramification depend on the write data. Accordingly, a data-aware approach is applied to salvage the endurance of PCM at three different stages: pre-write <b>fault</b> <b>avoidance,</b> post-write fault tolerance and post-failure recovery. The pre-write <b>fault</b> <b>avoidance</b> stage aims at reducing the endurance cost of servicing write requests. To this end, Cost Aware Flip Optimization (CAFO) is presented as an efficient technique to lessen the endurance degradation. Essentially, CAFO relies on a cost model that captures the endurance cost of programming memory cells based on their already stored values. Subsequently,the write data is encoded into a form that incurs a lower endurance cost than the original write data. Overall, CAFO is capable of reducing the endurance cost by up to 65...|$|E
40|$|In many applications, the {{manipulations}} {{require only}} part of the degrees of freedom (DOFs) of the end-effector, or some DOFs are more important than the rest. We name these applications prioritized manipulations. The end-effector’s DOFs are divided into those which are critical and must be controlled as precisely as possible, and those which have loose specifications, so their tracking performance can be traded-off to achieve other needs. In this paper, for the class of general constrained rigid multibody systems (including passive joints and multiple closed kinematic loops), we derive a formulation for partitioning the task space into major and secondary task directions and finding the velocity and static force mappings that precisely accomplish the major task and optimize some secondary goals such as reliability enhancement, obstacle and singularity <b>avoidance,</b> <b>fault</b> tolerance, or joint limit avoidance. The major task and secondary goals need to be specified in term of velocities/forces. In addition, a framework is developed to handle two kinds of common actuator failures, torque failure and position failure, by reconfiguring the differential kinematics and static force models. The techniques are tested on a 6 -DOF parallel robot. Experimental results illustrate that the approach is practical and yields good performance. Index Terms — Robot kinematics, task decomposition, fault tolerance, parallel robots, multibody systems...|$|R
40|$|Brittany N Rosenbloom, 1 Sobia Khan, 2 Colin McCartney, 3 Joel Katz 41 Institute of Medical Science, University of Toronto, Toronto, Ontario, 2 School of Public Health and Health Systems, University of Waterloo, Waterloo, Ontario, 3 Department of Anesthesia, Sunnybrook Health Sciences Centre, Toronto, Ontario, 4 Department of Psychology, York University, Toronto, Ontario, CanadaBackground: Persistent {{pain and}} {{psychological}} distress are common after traumatic musculoskeletal injury (TMsI). Individuals sustaining a TMsI are often young, do not recover quickly, {{and place a}} large economic burden on society. Objectives: The aim of this systematic review is to determine (1) the incidence of persistent pain following TMsI, (2) the characteristics of pain, characterized by injury severity and type, and (3) risk and protective factors associated with persistent pain following TMsI. Methods: A systematic search of electronic databases (MEDLINE&reg;, PubMed&reg;, Embase, and PsycINFO&reg;) was conducted for prospective, interventional, or noninterventional studies measuring the incidence of pain associated with TMsI. Results: The search revealed 4388 studies. Eleven studies examined persistent pain and met inclusion criteria. Pain was assessed using a validated measure of pain intensity or pain presence in six studies. Persistent pain was reported by all studies at variable time points up to 84 months postinjury, with wide variation among studies in pain intensity (ie, from mild to very severe) and pain incidence at each time point. The incidence of pain decreased over time within each study. Two studies found significant relationships between injury severity and persistent pain. Frequently cited predictive factors for persistent pain included: symptoms of anxiety and depression, patient perception that the injury was attributable to external sources (ie, they were not at <b>fault),</b> cognitive <b>avoidance</b> of distressing thoughts, alcohol consumption prior to trauma, lower educational status, being injured at work, eligibility for compensation, pain at initial assessment, and older age. Conclusion and implications: The evidence from the eleven studies included in this review indicates that persistent pain is prevalent up to 84 months following traumatic injury. Further {{research is needed to}} better evaluate persistent pain and other long-term posttraumatic outcomes. Keywords: persistent pain, psychological outcomes, traumatic injury, musculoskeletal, systematic review, pain intensity, injury severity, risk and protective factor...|$|R
40|$|Soft {{automation}} employs multi-functional robotic manipulators where dexterity, versatility and reconfigurability are {{now becoming}} key issues. For manipulators {{used in these}} circumstances, hyper-redundancy in configuration is essential. A typical application envisaged, is manipulator movement in unpredictable surroundings such as navigation amongst branches and foliage in automated fruit picking. This requires motion of a manipulator in an unmapped dynamic environment to find {{its way to the}} goal i. e. the fruit. To date, centralized control architectures have proven impractical for the control of redundant systems because the full environment needs to be mapped, there are time constraints on the computation required and, simultaneously, algorithms for obstacle avoidance must be enacted. Decentralized control of manipulators complements modular construction because the need for a full environment reconstruction in one master controller is bypassed by having localized sub-goals for each module. Time constraints are removed because the control algorithms are much simpler. Obstacle avoidance is localized. Manipulators constructed modularly are effective because they allow for reconfiguration and ease of fault diagnosis. For modular manipulators to be a more effective option as a subclass of robots, the conditions under which the interactive movements of modules are stable become a major issue. When a general review of hyper-redundant manipulators was undertaken, no published implementation of Modular Decentralized Control (MDC) was discovered. This thesis explored the use of a modular decentralized technique to create stable control of a redundant manipulator system. The computational burden was minimized by restricting inverse kinematics to within each module. Advantages of the approach taken were the ease of implementation of obstacle <b>avoidance,</b> reconfigurability and <b>fault</b> tolerance. Having firstly simulated a MATLAB version of stable motion using MDC on a modular manipulator with up to six identical modules, the technique was extended with state space analysis to redefine the limits of stable control of a hyper-redundant manipulator. The MDC study mapped motion profile types that were dependant on the initial manipulator configuration and goal position and, thereby, investigated possible instabilities in the system. A two-link, single degree of freedom system was initially explicated followed by an extension of the stability analysis to an n-module two degree of freedom system. A stability theory utilizing decentralized control was formed. Simulation results showed dynamic motion, path generation and obstacle avoidance capabilities in unmapped environments to be stable. The modeling redefined the bounds of stable control, showing that classical stability via Root Locus, now required only two roots from the characteristic equation to be stable for a selection of path trajectory to the goal to be found. The remaining roots could be unstable in traversing to the goal and settling at a marginal stability point when the goal was reached. The marginal stability was a reflection of the pseudo-independence given to each module in seeking the goal and differed radically from a standard Root Locus analysis and interpretation of stability. A hyper-redundant Reconfigurable Modular Manipulator System (RMMS) was designed and built to implement the MDC technique in a real world environment. From an initial design, five modules were constructed and control algorithms embedded appropriate to their position in a five-segment robotic manipulator. A stereoscopic vision system was attached {{to the end of the}} manipulator which supplied real time data on a goal in 3 D Cartesian space. The data was supplied to the first module of the arm and subsequently to all others by localized homogenous transformation. The manipulator was tested for goal seeking, path following, obstacle <b>avoidance,</b> <b>fault</b> tolerance and reconfigurability. The arm produced stable motion and satisfied the criteria as hypothesized in the theory...|$|R
40|$|The DOE {{intends to}} design the Yucca Mountain {{high-level}} waste facility structures, systems and components (SSCs) for fault displacements to provide reasonable assurance that they will meet the preclosure safety performance objectives established by 10 CFR Part 60. To the extent achievable, fault displacement design of the facility will follow guidance provided in the NRC Staff Technical Position. <b>Fault</b> <b>avoidance</b> will be the primary design criterion, especially for spatially compact or clustered SSCs. When <b>fault</b> <b>avoidance</b> is not reasonably achievable, {{expected to be the}} case for most spatially extended SSCs, engineering design procedures and criteria or repair and rehabilitation actions, depending on the SSC`s importance to safety, are provided. SSCs that have radiological safety importance will be designed for fault displacements that correspond to the hazard exceedance frequency equal to their established seismic safety performance goals. Fault displacement loads are generally localized and may cause local inelastic response of SSCs. For this reason, the DOE intends to use strain-based design acceptance criteria similar to the strain-based criteria used to design nuclear plant SSCs for impact and impulsive loads...|$|E
40|$|This {{paper is}} {{composed}} of two sections. The first provides a conceptual framework for expressing the attributes of what constitutes dependable and reliable computing: a) the impairments to dependability (faults, errors, and failures), 6) the means for dependability (<b>fault</b> <b>avoidance,</b> tolerance, removal, and forecasting), and c) the measures of dependability (reliability, availability, safety). The second section focuses {{on one of the most}} challenging pro& /ems for dependable computing: coping with design faults...|$|E
40|$|We are {{designing}} scalable dynamic {{information flow}} tracking techniques and employing them {{to carry out}} tasks related to debugging (bug location and <b>fault</b> <b>avoidance),</b> security (software attack detection), and data validation (lineage tracing of scientific data). The focus of our ongoing work is on developing online dynamic analysis techniques for long running multithreaded programs that may be executed on a single core or on multiple cores to exploit thread level parallelism. 1. 1...|$|E
30|$|One of {{the many}} usages of the neural {{networks}} is in the field/area of computer networks for diagnosing faults. Some researchers (Charoenpornwattana et al. 2008) are applying the concept for detecting and diagnosing faults in grids for improving reliability. Charoenpornwattana et al. (2008), used neural network based approach for proactive <b>fault</b> <b>avoidance.</b> Calado and da Costa (2006), used neural network based fault identification and diagnosis using fuzzy approach to achieve reliability in high performance computing environments.|$|E
40|$|Software Reliability reviews some {{fundamental}} issues of software reliability {{as well as}} the techniques, models, and metrics used to predict the reliability of software. Topics covered include <b>fault</b> <b>avoidance,</b> fault removal, and fault tolerance, along with statistical methods for the objective assessment of predictive accuracy. Development cost models and life-cycle cost models are also discussed. This book is divided into eight sections and begins with a chapter on adaptive modeling used to predict software reliability, followed by a discussion on failure rate in software reliability growth m...|$|E
40|$|In {{the last}} decade the {{dominance}} of the general computing systems market has being replaced by embedded systems with billions of units manufactured every year. Embedded systems appear in contexts where continuous operation is of utmost importance and failure can be profound. Nowadays, radiation poses a serious threat to the reliable operation of safety-critical systems. <b>Fault</b> <b>avoidance</b> techniques, such as radiation hardening, have been commonly used in space applications. However, these components are expensive, lag behind commercial components with regards to performance and do not provide 100...|$|E
40|$|Areliability {{strategy}} {{is a set}} of softwareengineering practices defined for each project by combining different relia-bility achievement and assessment activi-ties and methods, according to the soft-ware reliability goal and project’s charac-teristics. In [1] is a description of a deci-sion-support system for reliability strategy selection based on a set of product, proj-ect, and resources decision factors. There are two main approaches to achieving high software reliability: 1. Avoiding defects in the final product. 2. Using fault tolerance methods. <b>Fault</b> <b>avoidance</b> can be achieved by using fault prevention and fault detection an...|$|E
40|$|The {{results are}} {{presented}} of an experimental study undertaken to assess the improvement in program quality by using formal specifications. Specifications in the Z notation were developed for a simple but realistic antimissile system. These specifications were then used to develop 2 versions in C by 2 programmers. Another set of 3 versions in Ada were independently developed from informal specifications in English. A comparison of the reliability {{and complexity of the}} resulting programs suggests the advantages of using formal specifications in terms of number of errors detected and <b>fault</b> <b>avoidance...</b>|$|E
40|$|Abstract. Traditionally, it {{is common}} to {{distinguish}} between three broad families of methods for dealing with the presence and manifestation of faults in digital (hardware or software) systems: <b>Fault</b> <b>Avoidance,</b> Fault Removal and Fault Tolerance. We focus on fault tolerance and submit that current techniques of fault tolerance would benefit from a better undersdtanding of recoverability preservation, i. e. a system’s ability to preserve recoverability even when / if it does not preserve correctness. In this extended abstract, we briefly introduce the concept of recoverability preservation, discuss some preliminary characterizations of it, then explore possible applications thereof...|$|E
40|$|A mask {{comparator}} {{is described}} {{to study the}} potentials of <b>fault</b> <b>avoidance</b> (calibration, guiding correction, compensation) and fault correction for positioning improvement of 7 -inch hard layer masks. Object structures are detected by a CCD camera and compared with the interferometrically measured table position. Using this experimental comparator the positioning was improved by one order of magnitude enabling thus measurements in the regions of u_ 9 _ 5 = 50 nm. Concepts for further improvement of comparator calibration are discussed including self-calibration and FEM aided correction of mask deflection. (WEN) SIGLEAvailable from TIB Hannover: RO 5064 (24) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekDEGerman...|$|E
40|$|Abstract. Reliability {{is one of}} {{the major}} issues in High Performance Comput-ing (HPC) systems today. It is {{expected}} to become even a greater challenge in the next generation peta-scale systems. The traditional fault tolerance mecha-nisms (e. g., checkpoint/restart mechanisms) may not be efficient in every sce-nario in such large scale systems due to the scalability and performance issues. In this paper, we aim to provide a distributed scalable Unified Fault Tolerance (UFT), which consists of Proactive <b>Fault</b> <b>Avoidance</b> (PFA) and traditional Re-active Fault Tolerance (RFT) for HPC systems based on fault prediction and virtualization technologies. The results from simulation suggest the perform-ance improvement over the existing solutions. 1...|$|E
40|$|Conference ABSTRACT: All known active faults in the Kapiti Coast District {{have been}} mapped {{according}} to the methodology presented in the Ministry for the Environment’s Interim Guidelines on planning for development of land on, or close to active faults. <b>Fault</b> <b>Avoidance</b> Zones are defined along all the faults based on the rupture complexity of the fault, and the precision to which its location can be constrained. These zones range in width from about 40 m to greater than 300 m, and are attributed as well defined, distributed, uncertain- constrained, or uncertain- poorly constrained. Based on existing data, a Recurrence Interval Class (RIC) is defined for each fault: Ohariu & Norther...|$|E
