37|247|Public
5000|$|CPU <b>fully</b> <b>associative</b> <b>cache</b> {{controllers}} and translation lookaside buffers ...|$|E
5000|$|The {{placement}} {{policy is}} a trade-off between direct mapped and <b>fully</b> <b>associative</b> <b>cache.</b>|$|E
50|$|Set {{associative}} cache is {{a trade-off}} between Direct mapped cache and <b>Fully</b> <b>associative</b> <b>cache.</b>|$|E
40|$|In modern micro-architectures, {{computation}} {{speed is}} often reduced by cache misses. Cache analysis is therefore imperative to obtain e#ective optimization. We present an analytical technique based on reuse distances {{that focuses on}} e#ciently determining the behavior of <b>fully</b> <b>associative</b> <b>caches</b> and extends to set-associative caches. In this technique, the number of cache misses is obtained by {{counting the number of}} integer points in a parameterized polytope...|$|R
50|$|Suppose a direct-mapped L1 cache with blocks A, B {{pointing}} to the same set. It is linked to a 2 entry <b>fully</b> <b>associative</b> victim <b>cache</b> with blocks C, D in it.|$|R
40|$|Cache {{behavior}} modeling is {{an important}} part of modern optimizing compilers. In this paper we present a method to estimate the number of cache misses, at compile time, using a machine independent model based on stack algorithms. Our algorithm computes the stack histograms symbolically, using data dependence distance vectors and is totally accurate when dependence distances are uniformly generated. The stack histogram models accurately <b>fully</b> <b>associative</b> <b>caches</b> with LRU replacement policy, and provides a very good approximation for set-associative caches and programs with non-constant dependence distances...|$|R
5000|$|<b>Fully</b> <b>associative</b> <b>cache</b> {{the best}} miss rates, but {{practical}} {{only for a}} small number of entries ...|$|E
5000|$|<b>Fully</b> <b>associative</b> <b>cache</b> {{structure}} {{provides us}} {{the flexibility of}} placing memory block {{in any of the}} cache lines and hence full utilization of the cache.|$|E
5000|$|A victim cache is a small, usually <b>fully</b> <b>associative</b> <b>cache</b> {{placed in}} the refill path of CPU cache, and it stores all the blocks evicted from that level of cache.|$|E
40|$|As {{processors}} become faster, {{memory hierarchy}} becomes a serious bottleneck. In recent years memory speeds {{have failed to}} keep up with processor speeds and the gap has been steadily increasing. Cache performance has become a critical parameter in system performance. Victim <b>caches</b> are small <b>fully</b> <b>associative</b> <b>caches</b> placed between a cache and its refill path. This results in the misses served by the victim caches, having only a very small miss penalty, typically one cycle, as opposed to several cycles for main memory. Small victim caches from one to five line sizes are sufficient to significantly improve the effective cache hit rate. This improvement however is sub-linear {{in the size of the}} victim cache and can be poorer for set <b>associative</b> <b>caches.</b> We propose to do a quantitative comparison of set-associative caches and direct-mapped caches together with a victim cache. In particular we aim to find the "associativity " of a direct-mapped cache with a victim cache. This would [...] ...|$|R
30|$|A {{scratchpad memory}} for JOP is {{implemented}} and the {{integration into the}} programming model is under investigation. We will add a small <b>fully</b> <b>associative</b> data <b>cache</b> to JOP. This cache will {{also serve as a}} buffer for a real-time transactional memory for the JOP CMP system. We will investigate whether a standard cache for static data is a practical solution for Java.|$|R
40|$|In modern micro-architectures, {{computation}} {{speed is}} often reduced by cache misses. Cache analysis is therefore imperative to obtain effective optimization. We present an analytical technique based on reuse distances {{that focuses on}} efficiently determining the behavior of <b>fully</b> <b>associative</b> <b>caches</b> and extends to set-associative caches. In this technique, the number of cache misses is obtained by {{counting the number of}} integer points in a parameterized polytope. It is well know that this parameterized count can be represented by an Ehrhart polynomial. Previously, interpolation was used to obtain these polynomials, but this technique has some disadvantages, most notably that under certain conditions it fails to produce a solution. Our main contribution is a novel method for calculating Ehrhart polynomials analytically. It extends an existing method, based on Barvinokâ€™s decomposition, for counting the number of points in a nonparameterized polytope. Our technique always produces a solution and is usually faster than interpolation. 1...|$|R
5000|$|The {{range of}} caches from direct mapped to fully {{associative}} is {{a continuum of}} levels of set associativity. (Direct mapped is one-way set associative and <b>Fully</b> <b>associative</b> <b>cache</b> with m blocks is m -way set associative.) ...|$|E
50|$|In a <b>Fully</b> <b>associative</b> <b>cache,</b> {{the cache}} is {{organized}} {{into a single}} cache set with multiple cache lines. A memory block can occupy any of the cache lines. The cache organization can be framed as (1*m) row matrix.|$|E
50|$|In {{the common}} case {{of finding a}} hit in the first way tested, a pseudo-associative cache is {{as fast as a}} direct-mapped cache, but it has a much lower {{conflict}} miss rate than a direct-mapped cache, closer to the miss rate of a <b>fully</b> <b>associative</b> <b>cache.</b>|$|E
40|$|As {{processors}} become faster, memory performance {{becomes a}} serious bottleneck. In recent years memory speeds {{have failed to}} keep up with processor speeds and the gap has been steadily increasing. Cache performance has become a critical parameter in system performance. Several methods have been proposed to improve cache performances. One such technique that aims at reducing the cache miss rate, without affecting the hit rate is using what are called "victim caches". Victim <b>caches</b> are small <b>fully</b> <b>associative</b> <b>caches</b> placed between a cache and its refill path. Misses served by the victim caches have only a very small miss penalty, typically one cycle, as opposed to several cycles for main memory. A small victim cache along with a direct mapped primary cache is expected to significantly improve the 1 cache hit-rate. This project was aimed at a quantitative comparison of set-associative caches and directmapped caches together with a victim cache. We verified Jouppi's [Jou 90] clai [...] ...|$|R
40|$|An area model {{suitable}} for comparing data buffers of different organizations and arbitrary sizes is described. The area model considers the supplied bandwidth of a memory cell and includes such buffer overhead as control logic, driver logic, and tag storage. The model gave less than 10 percent error when verified against real caches and register files. It is shown that, comparing caches and register files {{in terms of}} area for the same storage capacity, caches generally occupy more area per bit than register files for small caches because the overhead dominates the cache area at these sizes. For larger caches, the smaller storage cells in the cache provide a smaller total cache area per bit than the register set. Studying cache performance (traffic ratio) {{as a function of}} area, it is shown that, for small caches, direct-mapped caches perform significantly better than four-way set-associative caches and, for caches of medium areas, both direct-mapped and set-associative caches perform better than <b>fully</b> <b>associative</b> <b>caches...</b>|$|R
40|$|The {{gap between}} {{processor}} and memory speeds {{has been growing}} with each new generation of microprocessors. As a result, memory hierarchy response has become a critical factor limiting application performance. For this reason, {{we have been working}} to model the impact of memory access latency on program performance. We build upon our prior work on constructing machine-independent characterizations of application behavior [9] by improving instructionlevel modeling of the structure and scaling of data access patterns. This paper makes two contributions. First, it describes static analysis techniques that help us build accurate reference-level characterizations of memory reuse patterns in the presence of complex interactions between loop unrolling and multi-word memory blocks. Second, it describes a strategy for combining memory hierarchy response characterizations suitable for predicting behavior for <b>fully</b> <b>associative</b> <b>caches</b> with a probabilistic technique that enables us to predict misses for set-associative caches. We validate our approach by comparing our predictions (at loop, routine and program level) against measurements using hardware performance counters for several benchmarks on two platforms with different memory hierarchy characteristics over a large range of problem sizes...|$|R
50|$|Conflict misses {{occur when}} the data {{required}} was in the cache previously, but got evicted. These evictions occur because another request was mapped to the same cache line. Generally, conflict misses are measured by subtracting the number of misses in a cache with limited associativity {{by the number of}} misses of a <b>fully</b> <b>associative</b> <b>cache</b> of the same size and cache block size.|$|E
50|$|In {{order to}} avoid the problem of read/write order {{described}} above, the write buffer can be treated as a <b>fully</b> <b>associative</b> <b>cache</b> and added into the memory hierarchy of the device in which it is implemented.Adding complexity slows down the memory hierarchy so this technique is often only used for memory which does not need strong ordering (always correct) like the frame buffers of video cards.|$|E
50|$|A {{capacity}} miss occurs due to {{the limited}} size of a cache and not the cache's mapping function. When the working set, i.e. the data that is currently important to the program, is bigger than the cache, capacity misses will occur frequently. Out of the 3Cs capacity misses are the hardest to identify and {{can be thought of}} as non-compulsory misses in a <b>fully</b> <b>associative</b> <b>cache.</b> In a single processor system, the misses that exist after subtracting the number of compulsory misses and conflict misses can be categorized as capacity misses.|$|E
40|$|Abstract â€” Caches {{are known}} to consume {{up to half of}} all system power in {{embedded}} processors. Co-optimizing performance and power consumption of the cache subsystem is therefore an important step in the design of embedded systems, especially those employing application specific instruction processors. One of the main difficulty in such attempts is that cache behaviors are application as well as cache-structure specific. The two general approaches to tackling this problem is to either use exhaustive simulation or analytical modeling. The former is too time-consuming while the latter often lacks the required accuracy. In this paper, we introduce a novel third approach. We propose an analytical cache model that interpolates data from runs with direct mapped and <b>fully</b> <b>associative</b> <b>caches</b> so that the entire parameter space involving all set-associativity is spanned. Validation against full trace-driven simulations shows that our model has a very high degree of fidelity, but requires significantly less simulation time than exhaustive simulation. Furthermore, the model works for instruction, data or unified caches. Finally, we show how the model can be coupled with a power model for caches so that one can very quickly decide on pareto-optimal performance-power design points for rapid design space exploration...|$|R
40|$|The growing {{gap between}} {{processor}} and main memory speed makes {{it necessary to}} exploit the caches maximally {{in order to obtain}} reasonable program execution speed. Many program transformations have been proposed {{in order to make the}} cache behavior better. However, if one wants maximum e#ectivity from such optimizations, the cache behavior needs to be determined first. In contrast to profile-driven measurement of the cache behavior, this paper presents a method which derives it from the structure of the loops in the program. The lack of profiling makes the time needed to calculate the cache behavior independent of the programs input data. Furthermore, in contrast to other techniques which calculate cache behavior at compiler time, the presented technique is exact and is able to handle <b>fully</b> <b>associative</b> <b>caches</b> e#ciently. The e#ciency originates from the use of an intermediate data locality metric: the reuse distance. From the reuse distance, the hit/miss behavior of a memory access can be easily determined. Furthermore, the use of this analysis in a cache optimization phase in an EPIC-compiler for the Itanium processor is shown as an example of its practicality...|$|R
40|$|We propose in {{this paper}} a new {{approach}} to study the temporal and spatial locality of codes using a plot of cache miss bandwidth as a function of cache size and line size for a <b>fully</b> <b>associative</b> LRU <b>cache.</b> We apply this new approach to the study of locality for several High-Performance benchmarks. We show that this plot capture fine behavior of these benchmarks and explain some of the difficulties that recent attempts to characterize locality using a few parameters are facing: Codes can exhibit different levels of temporal or spatial locality for different cache sizes; averaging these different behavior requires to weight properly the cost of misses at different levels of the memory hierarchy. We propose such a scheme, for an average measure of temporal locality. ...|$|R
50|$|While {{measuring}} performance improvement {{by using}} victim cache, Joupii assumed a Level-1 direct-mapped cache augmented with a <b>fully</b> <b>associative</b> <b>cache.</b> For the test suite used by him, {{on an average}} 39% of the Level-1 data cache misses {{are found to be}} conflict misses, while on an average 29% of the Level-1 instruction misses are found to be conflict misses. Since conflict misses amount to large percentage of total misses, therefore providing additional associativity by augmenting the Level 1 cache with a victim cache is bound to improve total miss rate significantly.|$|E
5000|$|Experimental {{results are}} deduced by {{considering}} a 32-Kb Direct-Mapped, 2-way and <b>fully</b> <b>associative</b> <b>cache</b> augmented with a 256 block (8KB) victim cache and running on it 8 randomly selected SPEC95 Benchmarks. While the results cannot be generalized for all benchmarks, adding a victim cache provides a miss rate reduction ranging from 10% to 100% {{for all the}} cache configuration. The returns although seem to level off beyond victim cache size of 50 blocks, thus proving Joupii's observation that victim cache benefits reach a plateau {{after the first few}} victim blocks.|$|E
5000|$|In particular, in the {{idealized}} {{case of a}} <b>fully</b> <b>associative</b> <b>cache</b> {{consisting of}} [...] cache lines of [...] bytes each, the above algorithm is sub-optimal for [...] and [...] stored in row-major order. When , every iteration of the inner loop (a simultaneous sweep through a row of [...] and a column of [...] ) incurs a cache miss when accessing an element of [...] This means that the algorithm incurs [...] cache misses in the worst case. , the speed of memories {{compared to that of}} processors is such that the cache misses, rather than the actual calculations, dominate the running time for sizable matrices.|$|E
40|$|As DRAM access latencies {{approach}} a thousand instructionexecution times and on-chip caches grow to multiple megabytes, {{it is not}} clear that conventional cache structures continue to be appropriate. Two key featuresâŽ¯full associativity and software managementâŽ¯have been used successfully in the virtual-memory domain to cope with disk access latencies. Future systems will need to employ similar techniques to deal with DRAM latencies. This paper presents a practical, <b>fully</b> <b>associative,</b> software-managed secondary <b>cache</b> system that provides performance competitive with or superior to traditional caches without OS or application involvement. We see this structure as the first step toward OS- and application-aware management of large on-chip caches. This paper has two primary contributions: a practical design for a <b>fully</b> <b>associative</b> memory structure, the indirect index cache (IIC), and a novel replacement algorithm, generational replacement, that is specifically designed to work with the IIC. We analyze the behavior of an IIC with generational replacement as a drop-in, transparent substitute for a conventional secondary cache. We achieve miss rate reductions from 8 % to 85 % relative to a 4 way associative LRU organization, matching or beating a (practically infeasible) <b>fully</b> <b>associative</b> true LRU <b>cache.</b> Incorporating these miss rates into a rudimentary timing model indicates that the IIC/generational replacement cache could be competitive with a conventional cache at todayâ€™s DRAM latencies, and will outperform a conventional cache as these CPU-relative latencies grow. 1...|$|R
40|$|In this study, the {{performance}} of an e-learning environment is analyzed and evaluated in terms of average network traffic and upload/download rates under various cache memory organizations. In particular, we study the influence of three <b>cache</b> organizations, namely <b>fully</b> <b>associative,</b> direct, and set <b>associative</b> <b>caches.</b> As {{a result of this}} work, we recommend the set <b>associative</b> <b>cache</b> memory organization with the LFU replacement policy, as this led to optimal performance in e-learning environments with the highest hit ratio and upload/download rates...|$|R
40|$|Step caches are caches {{in which}} data entered to an cache array is kept valid only {{until the end}} of ongoing step of execution. Together with an {{advanced}} pipelined multi-threaded architecture they can be used to implement con-current read concurrent write (CRCW) memory access in shared memory multiprocessor systems on chip (MP-SOC) without cache coherency problems. Unfortunately obvious step cache architectures assume full associativity, which can become expensive since the size and thus associativity of caches equal the number of threads per processor being at least the square root of the number of processors. In this paper, we describe a technique to radically reduce the associativity and even size of step caches in CRCW opera-tion. We give a short performance evaluation of limited associativity step cache systems with different settings using simple parallel programs on a parametrical MP-SOC framework. According to the evaluation, the perform-ance of limited associativity step cache systems comes very close to that of <b>fully</b> <b>associative</b> step <b>cache</b> systems, while decreasing the size of caches decreases the performance gradually. 1...|$|R
50|$|While {{using the}} cache {{to improve the}} memory latency, it may not always result in the {{required}} improvement for the time taken to fetch data due to the way caches are organized and traversed. E.g. The same size direct mapped caches usually have more miss rate than the fully associative caches. This may also depend on upon the benchmark that we are testing the processor upon and the pattern of instructions. But always using the <b>fully</b> <b>associative</b> <b>cache</b> may result in more power consumption as it has to search the whole cache every time. Due to this, the trade-off between the power consumption {{and the size of}} the cache becomes critical in the cache design.|$|E
40|$|Energy {{consumption}} in caches {{depends on the}} number of enabled sets/ways/blocks. The optimal energy consumption is the case of one set/one way/one block enabled. This paper proposes an algorithm to map cache line to one block in <b>fully</b> <b>associative</b> <b>cache</b> by XORâ€™ing the address with constant. Bit selection is applied to the result and the block accessed. Only one block is accessed in this mapping. The proposed model is simulated with SPEC 2 K benchmarks. The average memory access time is comparable with traditional <b>fully</b> <b>associative</b> <b>cache</b> with energy savings...|$|E
30|$|The dual-access {{prediction}} mechanism {{used in the}} DAWP cache yields {{high accuracy}} since the scaled-up index table and the <b>fully</b> <b>associative</b> <b>cache</b> are used. Unlike the WP cache, the performance gain of the DAWP cache (against the conventional cache) increases with increasing set associativity of the cache.|$|E
40|$|Emerging {{computer}} architectures will feature drastically decreased flops/byte (ratio of peak processing rate to memory bandwidth) as highlighted by recent studies on Exascale architectural trends. Further, flops are getting cheaper while the energy cost of data movement is increasingly dominant. The understanding and characterization of data locality properties of computations {{is critical in}} order to guide efforts to enhance data locality. Reuse distance analysis of memory address traces is a valuable tool to perform data locality characterization of programs. A single reuse distance analysis {{can be used to}} estimate the number of cache misses in a <b>fully</b> <b>associative</b> LRU <b>cache</b> of any size, thereby providing estimates on the minimum bandwidth requirements at different levels of the memory hierarchy to avoid being bandwidth bound. However, such an analysis only holds for the particular execution order that produced the trace. It cannot estimate potential improvement in data locality through dependence preserving transformations that change the execution schedule of the operations in the computation. In this article, we develop a novel dynamic analysis approach to characterize the inherent locality properties of a computation and thereby assess the potential for data locality enhancement via dependence preserving transformations. The execution trace of a code is analyzed to extract a computational directed acyclic graph (CDAG) of the data dependences. The CDAG is then partitioned into convex subsets, and the convex partitioning is used to reorder the operations in the execution trace to enhance data locality. Th...|$|R
40|$|It is {{critical}} to provide high performance for scientific programs running on a Chip Multi-Processor (CMP). A CMP architecture often has a shared L 2 cache and lower storage hierarchy. The shared L 2 cache can {{reduce the number of}} cache misses if the data are commonly shared by several threads, but it can also lead to performance degradation due to resource contention. Sometimes running threads on all cores can cause severe contention and increase the number of cache misses greatly. To investigate how a threadâ€™s performance varies when it runs together with other threads on different cores, we develop an analytical model to predict the number of misses on the shared L 2 cache, especially for thread-parallel numerical codes. We assume that the parallel threads work on homogeneous tasks and share a <b>fully</b> <b>associative</b> L 2 <b>cache.</b> Stack processing technique and circular sequences are used to analyze the L 2 trace to predict the number of compulsory misses, capacity misses on shared data, and capacity misses on private data, respectively. It is the first work to predict the number of L 2 misses for threads that âˆ— This material is based upon work supported by the National Science Foundation under grant No. 0444363...|$|R
5000|$|Eight-way set <b>associative</b> <b>cache,</b> {{a common}} choice for later {{implementations}} ...|$|R
