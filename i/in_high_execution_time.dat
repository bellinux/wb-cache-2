8|10000|Public
3000|$|From Table 1, {{it can be}} {{observed}} that both MaxFIA [9] and SWA [10] are primary data hiding techniques offering multiple advantages over others in terms of simplicity, required number of database scans etc. Both these approaches identify maximum support item as a victim and remove it from the supporting transactions. For accommodating volumes of data, as discussed in [...] [...] "Traditional heuristic approaches" [...] section, MaxFIA is a novel scheme but when applied to a big dataset, it results <b>in</b> <b>high</b> <b>execution</b> <b>time</b> and non-scalability. Further, K-sliding window approach used in SWA resolves the scalability issue but these K-data windows {{still need to be}} processed in a sequential manner i.e. one after another which again results <b>in</b> <b>high</b> <b>execution</b> <b>time.</b>|$|E
3000|$|From the {{literature}} discussed in [...] "Related work" [...] and [...] "Traditional heuristic approaches" [...] sections, {{it can be}} stated {{that we need to}} preserve the data privacy in both ways i.e. for secure data storage/rule mining at a third party as well as by masking restricted sensitive information before data sharing. A number of traditional techniques exist in both categories which perform well with comparatively accommodating volumes of data; the current situation is not that much in-line and often results <b>in</b> <b>high</b> <b>execution</b> <b>time</b> and sometimes result in non-feasibility. Much work has been done to improvise outsourced techniques but data hiding techniques are still struggling with these limitations.|$|E
30|$|A {{number of}} data hiding {{techniques}} {{are used in}} [9 – 14] to mask sensitive knowledge before sharing or analyzing datasets. Existing data hiding techniques can be broadly divided into three categories: heuristic, border and exact. Heuristics based approaches are simple and provide high privacy level but process the data in sequential fashion [13]. These heuristics are fairly adequate for small/medium sized datasets though the current situation is not that much in-line. The exponential increase in data volume and sequential nature of conventional data hiding techniques often result <b>in</b> <b>high</b> <b>execution</b> <b>time</b> or sometimes even non-feasible. This new challenge of scalability paves a way of experimenting with Big data approaches (e.g. MapReduce framework) for parallel processing.|$|E
40|$|Object {{detection}}, such as face detection using supervised learning, {{often requires}} extensive {{training for the}} computer, which results <b>in</b> <b>high</b> <b>execution</b> <b>times.</b> If the trained system needs re-training {{in order to accommodate}} a missed detection, waiting several hours or days before the system is ready may be unacceptable in practical implementations. This dissertation presents a generalized object detection framework whereby the system can efficiently adapt to misclassified data and be re-trained within a few minutes. Our developed methodology is based on the popular AdaBoost algorithm for object detection. AdaBoost functions by iteratively selecting the best among weak classifiers, and then combining several weak classifiers in order to obtain a stronger classifier. Even though AdaBoost has proven to be very effective, its learning <b>execution</b> <b>time</b> can be <b>high</b> depending upon the application. For example, in face detection, learning can take several days. In our dissertation, we present two techniques that contribute to reducing to the learning <b>execution</b> <b>time</b> within the AdaBoost algorithm. Our first technique utilizes a highly parallel and distributed AdaBoost algorithm that exploits the multiple cores in a CPU via lightweight threads. In addition, our technique uses multiple machines in a web service similar to a map-reduce architecture in order to achieve a high scalability, which results in a training <b>execution</b> <b>time</b> of a few minutes rather than several days. Our second technique is a methodology to create an optimal training subset to further reduce the training <b>execution</b> <b>time.</b> We obtained this subset through a novel score-keeping of the weight distribution within the AdaBoost algorithm, and then removed the images that had a minimal effect on the overall trained classifier. Finally, we incorporated our parallel and distributed AdaBoost algorithm, along with the optimized training subset, into a generalized object detection framework that efficiently adapts and makes corrections when it encounters misclassified data. We demonstrated the usefulness of our adaptive framework by providing detailed testing on face and car detection, and explained how our framework applies to developing any other object detection task...|$|R
40|$|The {{work done}} in this thesis enhances the MMD {{algorithm}} in multi-core environments. The MMD algorithm, a transformation based algorithm for reversible logic synthesis, {{is based on the}} works introduced by Maslov, Miller and Dueck and their original, sequential implementation. It synthesises a formal function specification, provided by a truth table, into a reversible network and is able to perform several optimization steps after the synthesis. This work concentrates on one of these optimization steps, the template matching. This approach is used {{to reduce the size of}} the reversible circuit by replacing a number of gates that match a template which implements the same function and uses less gates. Smaller circuits have several benefits since they need less area and are not as costly. The template matching approach introduced in the original works is computationally expensive since it tries to match a library of templates against the given circuit. For each template at each position in the circuit, a number of different combinations have to be calculated during runtime resulting <b>in</b> <b>high</b> <b>execution</b> <b>times,</b> especially for large circuits. In order to make the template matching approach more efficient and usable, it has been reimplemented in order to take advantage of modern multi-core architectures such as the Cell Broadband Engine or a Graphics Processing Unit. For this work, two algorithmically different approaches that try to consider each multi-core architecture’s strengths, have been analyzed and improved. For the analysis these approaches have been cross-implemented on the two target hardware architectures and compared to the original parallel versions. Important metrics for this analysis are the <b>execution</b> <b>time</b> of the algorithm and the result of the minimization with the template matching approach. It could be shown that the algorithmically different approaches produce the same minimization results, independent of the used hardware architecture. However, both cross-implementations also show a significantly <b>higher</b> <b>execution</b> <b>time</b> which makes them practically irrelevant. The results of the first analysis and comparison lead to the decision to enhance only the original parallel approaches. Using the same metrics for successful enhancements as mentioned above, it could be shown that improving the algorithmic concepts and exploiting the capabilities of the hardware lead to better results for the <b>execution</b> <b>time</b> and the minimization results compared to their original implementations...|$|R
40|$|We {{present a}} case study {{concerning}} manipulation of [...] . -OBDDs based on a probabilistic equivalence test. Efficient Boolean function manipulation requires efficient algorithms for Boolean synthesis as well as testing the equivalence of two Boolean functions. Due {{to the fact that}} [...] . -OBDDs do not provide a canonical representation the equivalence test becomes an extensive operation. A recently introduced deterministic equivalence test performs only <b>in</b> <b>high</b> polynomial degree <b>execution</b> <b>time</b> and therefore, it is not qualified for practical purposes. Hence, we tried to work with a probabilistic equivalence test (with one-sided error probability) based on Boolean signatures. Due to our experimental results we can conclude that the application of the probabilistic equivalence test is well suited for working in the field of Computer Aided Design...|$|R
30|$|Expansion of Internet {{and its use}} for on-line {{activities}} such as E-Commerce and social networking are producing large volumes of transactional data. This huge data volume resulted from these activities facilitates the analysis and understanding of global trends and interesting patterns used for several decisive purposes. Analytics involved in these processes expose sensitive information present in these datasets, which is a serious privacy threat. To overcome this challenge, few sequential heuristics {{have been used in}} past where volumes of data were comparatively accommodating to these sequential heuristics; the current situation is not that much in-line and often results <b>in</b> <b>high</b> <b>execution</b> <b>time.</b> This new challenge of scalability paves a way for experimenting with Big Data approaches (e.g., MapReduce Framework). We have agglomerated the MapReduce framework with adopted heuristics to overcome this challenge of scalability along with much-needed privacy preservation and yields efficient analytic results within bounded execution times.|$|E
40|$|Instruction {{scheduling}} methods {{based on}} the construction of state diagrams (or automata) have been used for architectures involving deeply pipelined function units. However, the size of the state diagram is prohibitively large, resulting <b>in</b> <b>high</b> <b>execution</b> <b>time</b> and space requirement. In this paper, we present a simple method for reducing the size of the state diagram by recognizing unique paths of a state diagram. Our experiments show that the number of paths in the reduced state diagram is significantly lower — by 1 to 3 orders of magnitude — compared to the number of paths in the original state diagram. Using the reduced MS-state diagrams, we develop an efficient software pipelining method. The proposed software pipelining algorithm produced efficient schedules and performed better than Huff’s Slack Scheduling method, and the original Co-scheduling method, in terms of both the initiation interval (II) and the time taken to construct the schedule. ...|$|E
30|$|SWA is {{an improvised}} version of MaxFIA and {{requires}} only single database scan. Sliding window approach used in SWA make it quite scalable but, still {{in case of}} big data the sanitization time is huge. Hence, MapReduce implementation helps the approach to be fast enough such that even voluminous data can be sanitized in an adequate amount of time. Unlike MaxFIA, SWA sorts the identified supporting transactions against any sensitive itemset in increasing order of their ’length’ to minimize the side effects on non-sensitive information. The data is processed by considering the set of transactions within K-size window. It adds to scalability but still each data window needs to be processed one after another i.e. in a sequential fashion and often results <b>in</b> <b>high</b> <b>execution</b> <b>time.</b> MapReduce framework not only reduces the sanitization time but also mitigates the requirement of sliding window approach. This is possible since it directly divides data into small chunks and sanitizes them in parallel.|$|E
30|$|These new {{challenges}} of scalability and <b>high</b> <b>execution</b> <b>time</b> paves {{a way for}} experimenting with Big Data approaches (e.g. MapReduce framework). Thus, here we propose parallelized version of these conventional approaches by using MapReduce Framework.|$|R
40|$|The {{self-organizing}} map is a prominent {{unsupervised neural network}} model which lends itself {{to the analysis of}} high-dimensional input data. However, the <b>high</b> <b>execution</b> <b>times</b> required to train the map put a limit to its application in many high-performance data analysis application domains, where either very large datasets are encountered and/or interactive response times are required...|$|R
50|$|C to HDL {{techniques}} are most commonly applied to applications that have unacceptably <b>high</b> <b>execution</b> <b>times</b> on existing general-purpose supercomputer architectures. Examples include Bioinformatics, Computational fluid dynamics (CFD), financial processing, {{and oil and}} gas survey data analysis. Embedded applications requiring high performance or real-time data processing are also an area of use. System-on-a-chip design may also take advantage of C to HDL techniques.|$|R
40|$|Instruction {{scheduling}} methods {{based on}} the construction of state diagrams (or automatons) have been used for architectures involving deeply pipelined function units. However, the size of the state diagram is prohibitively large, resulting <b>in</b> <b>high</b> <b>execution</b> <b>time</b> and space requirement, which in turn, restrict the use of these methods. In this paper, we develop the underlying theory for reducing the size of state diagrams by identifying primary paths of a state diagram. We establish that a reduced state diagram consisting only primary paths is complete, i. e., it retains all the useful information represented by the original state diagram as far as scheduling of operations in the software pipelining method is concerned. Our experiments show that the number of paths in the reduced state diagram is significantly lower [...] - by 1 to 3 orders of magnitude [...] - compared to the number of paths in the original state diagram. Using the reduced MS-state diagrams, we develop an efficient software pipe [...] ...|$|E
40|$|Part 7 : Software Security and PrivacyInternational audienceUnsafe {{memory accesses}} in {{programs}} written using popular programming languages like C and C++ {{have been among}} the leading causes of software vulnerability. Memory safety checkers, such as Softbound, enforce memory spatial safety by checking if accesses to array elements are within the corresponding array bounds. However, such checks often result <b>in</b> <b>high</b> <b>execution</b> <b>time</b> overhead due to the cost of executing the instructions associated with the bound checks. To mitigate this problem, techniques to eliminate redundant bound checks are needed. In this paper, we propose a novel framework, SIMBER, to eliminate redundant memory bound checks via statistical inference. In contrast to the existing techniques that primarily rely on static code analysis, our solution leverages a simple, model-based inference to identify redundant bound checks based on runtime statistics from past program executions. We construct a knowledge base containing sufficient conditions using variables inside functions, which are then applied adaptively to avoid future redundant checks at a function-level granularity. Our experimental results on real-world applications show that SIMBER achieves zero false positives. Also, our approach reduces the performance overhead by up to 86. 94 % over Softbound, and incurs a modest 1. 7 % code size increase on average to circumvent the redundant bound checks inserted by Softbound...|$|E
30|$|Canny and DoG run {{at nearly}} real time {{due to the}} low {{complexity}} of these algorithms. The proposed approach took 3.645 seconds on average to process an image, whereas the average <b>execution</b> <b>time</b> for the FDoG algorithm was 3.235  seconds. The slightly <b>higher</b> <b>execution</b> <b>time</b> of the proposed approach, {{when compared to the}} FDoG, is partly explained by the pre- and post-processing steps of the proposed approach, which are not required in the other compared algorithms.|$|R
30|$|Wavelet-based image {{processing}} systems are typically implemented with memory-intensive algorithms and with <b>higher</b> <b>execution</b> <b>time</b> than other encoders based on other transforms like discrete cosine transform. In usual two-dimensional (2 D)-DWT implementations [4], image decomposition {{is computed by}} means of a convolution filtering process, and so its complexity rises as the filter length does. The image is transformed at every decomposition level, first column by column and then row by row.|$|R
30|$|Exhaustive {{solution}} {{techniques are}} efficient to reach optimal solutions but suffer from scalability issues and <b>higher</b> <b>execution</b> <b>time</b> (especially for medium/big-sized problem instances). On the other hand, meta-heuristics (such as PSO) seems more promising {{as they can}} produce near-optimal solutions faster by also achieving better scalability. However, they need proper configuration and modelling {{which can be a}} time-consuming task while it is not always guaranteed that near-optimal solutions can be produced.|$|R
30|$|Table  1 {{shows that}} the <b>execution</b> <b>times</b> for {{different}} tasks are not well balanced, and this situation has to be changed. Taking into account the time spent on each task, a new organization of tasks is proposed. The main reason supporting the redesign of the pipeline stages is the <b>high</b> <b>execution</b> <b>time</b> for the velocities function. The proposed solution is to parallelize the slower task (velocities) since it {{does not depend on}} other images, but on the derivatives with respect to t, x, and y, i.e., It, Ix, and Iy.|$|R
40|$|ABSTRACT A {{load and}} store reuse {{mechanism}} {{can be used}} for filtering memory references to reduce memory activity including onchip cache activity. The challenging aspect of this task is to ensure that energy savings achieved in memory are not offset by energy used by the reuse hardware. In this paper we present the design of a reuse mechanism which has been carefully tuned to achieve net energy savings. In contrast to traditional filter cache designs which trade-off energy reductions with <b>higher</b> <b>execution</b> <b>times,</b> our approach reduces both energy and <b>execution</b> <b>time.</b> 1...|$|R
40|$|Recently, a very {{versatile}} algorithm for {{the design}} of finite word length filters has been proposed. The algorithm makes use of a simulated annealing algorithm which searches over discrete values of the filter coefficients. It has very fast development time and produces filters with good performances, its only drawback is a quite <b>high</b> <b>execution</b> <b>time.</b> <b>In</b> this paper, {{a detailed description of the}} particular annealing algorithm is presented together with three examples of filter design...|$|R
30|$|Searching for a {{solution}} comparable to those made by humans could be of lesser interest if not obtained in lesser time. However, time-on-task reported in Table 2 outlines designers in the first group consistently experienced a shorter time in building {{a solution}}. Indeed, the manual method entails in all cases a <b>higher</b> <b>execution</b> <b>time,</b> while the automatic method provide a significant time saving. Higher time is justified by additional selection time with IGA and checking the contrast ratio in the manual approach, both not required by SGA.|$|R
40|$|Entity {{resolution}} {{is a crucial}} step for data quality and data integration. Learning-based approaches show high effectiveness {{at the expense of}} poor efficiency. To reduce the typically <b>high</b> <b>execution</b> <b>times,</b> we investigate how learningbased entity resolution can be realized in a cloud infrastructure using MapReduce. We propose and evaluate two efficient MapReduce-based strategies for pair-wise similarity computation and classifier application on the Cartesian product of two input sources. Our evaluation is based on real-world datasets and shows the high efficiency and effectiveness of the proposed approaches...|$|R
30|$|In {{favor of}} {{execution}} speed and hardware resources, a local, window-based disparity estimator is applied. Other, global methods can yield better results but also imply <b>higher</b> <b>execution</b> <b>times.</b> Possible future work could explore hierarchical global methods for hardware implementations. The rectification unit can produce aliasing artifacts {{in case of}} extreme minification, e.g., if the pixel distances are reduced {{by more than a}} half. Low-pass pre-filtering of the input video could resolve the problem. However, most stereo cameras employ the same lenses and cameras, and therefore, this extreme case can safely be neglected.|$|R
40|$|KOPPA (Kinetic Octree Parallel PolyAtomic) is a {{parallel}} numerical code for the simulation of rarefied gas dynamics. It {{is based on}} a library named PABLO (PArallel Balanced Linear Octree) used to manage octree grids in parallel. The main issue with such numerical codes is the very <b>high</b> <b>execution</b> <b>time</b> which can become prohibitive for some industrial applications. Thanks to the SHAPE project, important improvements have been achieved with respect to <b>execution</b> <b>time</b> and scalability. In particular, some parts of the code have been reimplemented to suit better a MIC (Multi Integrated Cores) architecture. So far, the computational time requirements have been decreased by a factor of almost 8 and a good scalability has been obtained up to 64 processors against 16 initially...|$|R
40|$|Digital designs can be {{mapped to}} {{different}} implemen-tations using diverse approaches, with varying cost crite-ria. Post-processing transforms, such as transistor sizing can drastically improve circuit performance, by optimizing critical paths to meet timing specifications. However, most transistor sizing tools have <b>high</b> <b>execution</b> <b>times,</b> and the attainable circuit delay {{can be determined}} only after run-ning the tool. In this paper, we present an approach for fast transistor sizing that can enable a designer to choose one among several functionally identical implementations. Our algorithm computes the minimum achievable delay of a cir-cuit with a maximum average error of 5. 5 % {{in less than a}} second for even the largest benchmarks. ...|$|R
40|$|A {{clinical}} trial {{is a study}} designed to demonstrate the efficacy and safety of a drug, procedure, medical device, or diagnostic test. Since {{clinical trial}}s involve research in humans, they must be carefully designed and must comply strictly {{with a set of}} ethical conditions. Logistical disadvantages, ethical constraints, costs and <b>high</b> <b>execution</b> <b>times</b> could {{have a negative impact on}} the execution of the clinical trial. This article proposes the use of a simulation tool, the MRSA-T-Simulator, to design and perform “virtual clinical trials” for the purpose of studying MRSA contact transmission among hospitalized patients. The main advantage of the simulator is its flexibility when it comes to configuring the patient population, healthcare staff and the simulation environment...|$|R
40|$|We present {{comparative}} {{reliability analysis}} between shared-bus AMBA and network-on-chip (NoC) {{in the presence}} of single-event upsets (SEUs) using MPEG- 2 video decoder as a case study. Employing SystemC-based cycle-accurate fault simulations, we investigate how the decoder reliability is affected when SEUs are injected into the computation cores and communication interconnects of the decoder. We show that for a given soft error rate, AMBA-based decoder experiences higher SEUs during computation due to <b>higher</b> <b>execution</b> <b>time.</b> On the other hand, NoC-based decoder experiences higher SEUs during inter-core communication due to higher channel latency and resource usage in the interconnects. Furthermore, we evaluate the impact of total SEUs at application-level for NoC- and AMBA-based decoders...|$|R
40|$|Digital designs can be {{mapped to}} {{different}} implementations using diverse approaches, with varying cost criteria. Post-processing transforms, such as transistor sizing, can significantly improve circuit performance by optimizing critical paths to meet timing specifications. However, most transistor sizing tools have <b>high</b> <b>execution</b> <b>times,</b> {{and the possible}} delay gains due to sizing, and the associated costs are not known prior to sizing. In this paper, we present two metrics for comparing different implementations—the minimum achievable delay {{and the cost of}} achieving a target delay—and show how these can be estimated without running a sizing tool. Using these fast and accurate performance estimators, a designer can determine the tradeoffs between multiple functionally identical implementations, and size only the selected implementation...|$|R
40|$|Every {{regression}} testing cycle of a software application results in new test cases being introduced {{in a test}} suite. Many test cases from previous {{regression testing}} cycles become unstable or unusable due to the removal/addition of the new functionalities. The execution {{of a large number}} of unusable test cases results in less test coverage and <b>higher</b> test <b>execution</b> <b>time.</b> The lower test coverage is due to the coverage of the non-existent code statements. The <b>higher</b> test <b>execution</b> <b>time</b> is due to the execution of unused and broken test cases. In this paper, we propose a new bipartite graph approach to eliminate the subset of test cases that are not relevant for the testing of the current version of a software application. The suggested approach helps in executing a minimal set of test cases that are required to cover more code statements...|$|R
50|$|The second {{priority}} level defines an execution processor resource group. <b>In</b> general <b>higher</b> <b>execution</b> group priorities typically get more processor time.|$|R
30|$|Our {{approach}} {{assumes that}} the initial solution is computed first before running the metaheuristic, {{and so it is}} expected that the time and effort required to calculate the greedy solution will be more than that of a random solution which would {{have a negative impact on}} the variants of metaheuristic that start with the greedy solution. However, the results show that the <b>high</b> <b>execution</b> <b>time</b> required to produce a greedy solution was not enough to counter the small number of function evaluations required by metaheuristic that start with greedy solutions to attain the target solutions. Therefore, our model-based algorithm when supported with metaheuristic that starts with greedy solutions would be suitable for handling large-scale cloud deployment projects that may have a significant number of interdependent components.|$|R
40|$|This paper {{presents}} a parallel {{implementation of a}} kind of discrete Fourier transform (DFT) : the vector-valued DFT. The vector-valued DFT is a novel tool to analyze the spectra of vector-valued discrete-time signals. This parallel implementation is developed in terms of a mathematical framework with a set of block matrix operations. These block matrix operations contribute to analysis, design, and implementation of parallel algorithms in multicore processors. In this work, an implementation and experimental investigation of the mathematical framework are performed using MATLAB with the Parallel Computing Toolbox. We found that there is advantage to use multicore processors and a parallel computing environment to minimize the <b>high</b> <b>execution</b> <b>time.</b> Additionally, speedup increases when the number of logical processors and length of the signal increase...|$|R
40|$|Text Classification is an {{important}} technique for handling the huge and increasing amount of text documents on the web. An important problem of text classification is features selection. Many feature selection techniques were used in order to solve this problem, such as chi-square (CHI). Rather than using these techniques, this paper proposes a method for feature selection based on text summarization. We demonstrate this method on Arabic text documents and use text summarization for feature selection. Support Vector Machine (SVM) is then used to classify the summarized documents and the ones processed by CHI. The classification indicators (precision, recall, and accuracy) achieved by text summarization are higher than the ones achieved by CHI. However, text summarization has negligible <b>higher</b> <b>execution</b> <b>time...</b>|$|R
40|$|Abstract. Smith {{normal form}} {{computation}} has many applications in group theory, module theory and number theory. As the entries {{of the matrix}} and of its corresponding transformation matrices can explode during the computation, {{it is a very}} difficult problem to compute the Smith normal form of large dense matrices. The computation has two main problems: the <b>high</b> <b>execution</b> <b>time</b> and the memory requirements, which might exceed the memory of one processor. To avoid these problems, we develop two parallel Smith normal form algorithms using MPI. These are the first algorithms computing the Smith normal form with corresponding transformation matrices, both over the rings Z and F[x]. We show that our parallel algorithms both have a good efficiency, i. e. by doubling the processes, the <b>execution</b> <b>time</b> is nearly halved, and succeed in computing the Smith normal form of dense example matrices over the rings Z and F 2 [x] with more than thousand rows and columns. ...|$|R
40|$|Segmentation of curled textlines from warped {{document}} {{images is}} one of the major issues in document image de-warping. Most of the curled textlines segmentation algo-rithms present in the literature today are sensitive to the degree of curl, direction of curl, and spacing between adja-cent lines. We present a new algorithm for curled textline segmentation which is robust to above mentioned problems at the expense of <b>high</b> <b>execution</b> <b>time.</b> We will demon-strate this insensitivity in a performance evaluation section. Our approach is based on the state-of-the-art image seg-mentation technique: Active Contour Model (Snake) with the novel idea of several baby snakes and their conver-gence in a vertical direction only. Experiment on publically available CBDAR 2007 document image dewarping contest dataset shows our textline segmentation algorithm accuracy of 97. 96 %. ...|$|R
40|$|Abstract — Digital designs can be {{mapped to}} {{different}} implementations using diverse approaches, with varying cost criteria. Post-processing transforms, such as transistor sizing can significantly improve circuit performance, by optimizing critical paths to meet timing specifications. However, most transistor sizing tools have <b>high</b> <b>execution</b> <b>times,</b> {{and the possible}} delay gains due to sizing, and the associated costs, are not known prior to sizing. In this paper, we present two metrics for comparing different implementations – the minimum achievable delay, {{and the cost of}} achieving a target delay, and show how these can be estimated without running a sizing tool. Using these fast and accurate performance estimators, a designer can determine the trade-offs between multiple functionally identical implementations, and has to size only the selected implementation. Index Terms — Estimation, Very-large-scale integration, CMOS digital integrated circuits, Digital integrated circuit...|$|R
