36|1347|Public
5000|$|On June 15, 2007, Mueller {{was named}} Los Angeles Dodgers interim hitting coach when Eddie Murray was fired. After {{a month on}} the <b>job,</b> <b>manager</b> Grady Little {{announced}} that the Dodgers would be removing the [...] "interim" [...] tag and that Mueller would remain the teams's hitting coach {{through the end of}} the season.|$|E
50|$|LeMel Humes studied Civil {{engineering}} at The University Of Buffalo, {{he went on}} to work for his father’s construction company Humes Construction and Sullivan - Humes Painting contractors. Some of the projects built were the Buffalo Hilton, The Buffalo Marriott and The Buffalo City Mission. During his time as a construction <b>job</b> <b>manager</b> building the Sullivan County Maximum correctional facility for his family business.|$|E
5000|$|McLemore's primary {{claim to}} fame is his {{nickname}} [...] "Supersub", which he earned due to his contributions to the -03 Seattle Mariners. The club's regular 2nd baseman during the 2000 season, in 2001 he was replaced by Bret Boone, who had been acquired during the offseason. With McLemore openly bitter about losing his <b>job,</b> <b>manager</b> Lou Piniella appeased him by using him regularly {{in a variety of}} infielder and outfielder positions (mainly LF, 3B and SS, but also 2B, CF, DH and RF), with remarkable results. During the 2001 Mariners' record-tying 116-win season, he racked up 409 at-bats, 117 hits, 69 walks, [...]286 batting average, [...]384 OBP and 39 stolen bases—all while playing without a regular position.|$|E
50|$|A {{month after}} {{arriving}} at Arsenal, Graham was {{linked with the}} <b>job</b> as <b>manager</b> of the Scotland national team, {{with the possibility of}} combining it with the Arsenal <b>manager's</b> <b>job,</b> but the job went to Andy Roxburgh instead.|$|R
5000|$|Jem The Bee - <b>Job</b> Entry <b>Manager</b> the Batch Execution Environment ...|$|R
50|$|Benny Gaughran {{took over}} the <b>job</b> as <b>manager</b> of St. Sylvester's {{football}} team in Malahide in September, 1993.|$|R
50|$|Tebis Version 3.0 was {{presented}} in 1993. The system was modularized and expanded for operation under the SCO UNIX, HP-UX, IRIX and AIX operating systems. Version 3.1 included the Milling Wizard, version V3.2 featured interactive CAD and version V3.3 offered the first integration of a tool library and parameterized administration for all NC calculations. In Version 3.4, modules for the simulation of machining at a virtual CNC machine, the design of electrodes for EDM, and 2.5D milling and drilling were added. The current Version 3.5 offers an option for automating NC programming with variable machining templates, {{and for the first}} time makes the <b>job</b> <b>manager</b> the central control element for all machining steps. The BREP design CAD module has been integrated into the software, so that Tebis software can now accompany the entire manufacturing process in die, mold and model making.|$|E
50|$|The Liberty {{profile of}} WebSphere Application Server is {{included}} {{with all the}} commercial editions of the server, providing a lightweight profile of the server for web, mobile and OSGi applications. In this release it is a functional subset of the full profile of WebSphere Application Server, for both development and production use, with an install size of under 50 MB, a startup time of around 3 seconds and a new XML-based server configuration {{which can be treated}} as a development artifact to aid developer productivity. Server capabilities are engaged through the set of features defined in the server configuration; features are added and removed dynamically through internal use of OSGi services. A new model is provided for moving applications through the pipeline from development to production as a packaged server; this is a complete archive of the server, server configuration and application for unzip deploy. A centralized managed install is optionally available through the <b>Job</b> <b>Manager</b> component of WebSphere Application Server Network Deployment edition.|$|E
50|$|Failure {{to achieve}} {{promotion}} back the following {{season and the}} departure of Astle in 1974 seemed to presage a gloomy future, and cost Howe his job. In his place, the club were able to secure the services of player-manager Johnny Giles. Giles was an instant hit and Albion clinched promotion at Oldham {{on the final day}} of the 1976 season in front of an army of more than 15,000 fans. However, he then dropped a bombshell on the club by submitting his resignation in order to give more attention to his other <b>job,</b> <b>manager</b> of the Irish national team. The board was able to persuade Giles to stay on for another year however, and he led them to a comfortable finish back in the First Division, after which the club reluctantly let him go. After being turned down by Lincoln manager Graham Taylor, the club appointed Ronnie Allen as Giles' successor. Allen led the club to a brilliant start to the season, but following a row over both transfer funds and his own pay packet, accepted an offer to manage in Saudi Arabia and left after only five months.|$|E
5000|$|August 30 - Michel Preud'homme quits {{his job as}} {{technical}} director {{to take up the}} <b>job</b> of <b>manager</b> with Standard Liège.|$|R
50|$|After {{he retired}} from playing, Vázquez became a {{football}} coach. His first <b>job</b> as <b>manager</b> was with Torneo Argentino A side Sportivo Desamparados in 2009.|$|R
50|$|Thompson {{claimed in}} his book that Souness {{believed}} Thompson was interested in his <b>job</b> as <b>manager</b> while Souness recovered from bypass surgery {{in the spring of}} 1992.|$|R
40|$|This {{viewgraph}} presentation {{provides a}} high-level {{design of the}} IPG <b>Job</b> <b>Manager,</b> and satisfies its Master Requirement Specification v 2. 0 Revision 1. 0, 01 / 29 / 2003. The presentation includes a Software Architecture/Functional Overview with the following: Job Model; <b>Job</b> <b>Manager</b> Client/Server Architecture; <b>Job</b> <b>Manager</b> Client (<b>Job</b> <b>Manager</b> Client Class Diagram and <b>Job</b> <b>Manager</b> Client Activity Diagram); <b>Job</b> <b>Manager</b> Server (<b>Job</b> <b>Manager</b> Client Class Diagram and <b>Job</b> <b>Manager</b> Client Activity Diagram); Development Environment; Project Plan; Requirement Traceability...|$|E
40|$|The goal is location-independent computing. Implementing {{a set of}} {{services}} to satisfy this goal, build upon the GLOBUS toolkit services, and implementing with OGSA. Current status includes: Event service, <b>Job</b> <b>manager,</b> Resource selector and Broker, Next versions in development. Development includes: Monitoring and testing, Portability manager, Performance prediction, Dynamic accounting, and MDS evaluation...|$|E
40|$|The Cascade Parallel Processing Framework (PPF) is a {{user level}} library that {{facilitates}} manual parallelization of complex C++ systems. In Cascade, processing {{duties of the}} system are enclosed in a Cascade Task. Tasks are linked by dependencies in a task dependency graph. The task graph is traversed at runtime by the Cascade <b>Job</b> <b>Manager</b> who assigns tasks to threads for execution. The <b>Job</b> <b>Manager</b> must correctly satisfy dependencies while maximizing performance. While a task-based PPF {{is not a new}} concept, Cascade’s unique goal is to address complex systems, such as video game engines. These systems are built as multiple interacting sub-systems, with non-trivial dependencies. Existing PPFs, while suitable for parallelization of individual sub-systems, do not solve the entire problem. In this paper we describe the early design and implementation of Cascade, present preliminary evaluation, and outline plans for future research. 1...|$|E
5000|$|... 6 December 1982: Mike Bailey is sacked {{from his}} <b>job</b> as <b>manager</b> at Brighton & Hove Albion, {{reportedly}} because Albion's supporters deemed the team [...] "too boring".|$|R
5000|$|November 7 - Thomas Caers quits his <b>job</b> as <b>manager</b> of Sint Truiden as {{he feels}} the players don't respect him anymore {{after a series of}} bad results.|$|R
50|$|Similarly, {{position}} players {{must accept}} facing both left-handed and right-handed pitching {{as part of}} their <b>job.</b> <b>Managers</b> will usually juggle batters who are exceptionally weak against one sort of pitcher so that they only face starting pitchers who offer favorable matchups, but it is impossible to shield a batter from every instance in which he will face a pitcher who has him at a disadvantage. As a result, a position player must be prepared at all times to face a lefty-righty switch in a situation where his team cannot afford to pinch hit for him.|$|R
40|$|Image {{restoration}} is {{the process}} of recovering the original image from the degraded or noisy image. We are using multiple images for processing, the identifier select the blur image for further process and restore after applying the algorithms. Our object is identifying the blur image and restores them by parallel distributed computing. We are using blind deconvolution algorithm for restoration process. In which PSF value is unknown that is the intensity value by which image is blurred. We are applying 3 approaches first one is blind deconvolution algorithm. Second is parallel computing and third one is distributed computing. In parallel computing approaches one local scheduler is working for distributing the job and restore result at once. In distributed computing each worker has their same program after running program send the result to the <b>job</b> <b>manager.</b> This overall architecture is depended upon the parallel distributed computing. <b>Job</b> <b>manager</b> handled the overall process done by the worker...|$|E
40|$|Beowulf-style cluster {{computers}} {{which have}} traditionally served batch jobs via resource managers such asPortable Batch System (PBS) {{are now under}} pressure from the scienti c community to support real time response to data stream applications. That is, applications supporting events streamed from remote sensors or instruments at high rates. Cluster management must be agile and timely in its response to resource demands prompted by the unpredictable occurrence of real-world events. Streaming applications di er in fundamental ways from interactive applications. While solutions exist to support interactive applications, the solutions are not well suited to streaming applications. In this paper we experimentally evaluate two approaches to supporting streaming applications on Linux clusters. Both are coexistence schemes, that is, streaming applications share nodes with traditional batch jobsonnodesscheduled by a batch <b>job</b> <b>manager.</b> One approach works with the batch <b>job</b> <b>manager</b> (i. e., PBS) to reallocate computational resources {{to meet the needs of}} real time data analysis. The second approach works outside of PBS utilizing Linux real time scheduling features. We alsointroduce a software architecture for streaming applications that we developed. Its purpose is a general reusable software structure for IU scientists requiring streaming solutions for acquisition of resources necessary for real time stream analysis. ...|$|E
40|$|A basic {{function}} of a computational grid such as the NASA Information Power Grid (IPG) is to allow users to execute applications on remote computer systems. The Globus Resource Allocation Manager (GRAM) provides this functionality in the IPG and many other grids at this time. While the functionality provided by GRAM clients is adequate, GRAM does not support useful features such as staging several sets of files, running more than one executable in a single job submission, and maintaining historical information about execution operations. This specification is intended to provide the environmental and software functional requirements for the IPG <b>Job</b> <b>Manager</b> V 2. 0 being developed by AMTI for NASA...|$|E
50|$|Dave Edwards {{lost his}} <b>job</b> as <b>manager</b> in 1992 after the club went into administration, but was {{instantly}} re-instated in 1993 when the club {{returned to the}} Isthmian League.|$|R
5000|$|In March 2011, Froger {{left his}} <b>job</b> as <b>manager</b> of African nation Togo for [...] "personal reasons", before {{taking charge of}} Ligue 2 side Nîmes just a day later.|$|R
50|$|Vinod (Kunal) and Manoj (Karan) {{are best}} friends. Manoj gives Vinod a <b>manager</b> <b>job</b> {{of his three}} star hotel. They both are in love with Neetha (Monal), but Neetha is {{actually}} in love with Vinod. This causes a strain in their friendship and Manoj fired Vinod from his <b>manager</b> <b>job.</b>|$|R
40|$|Abstract. In {{order to}} {{implement}} the spatial Inverse Distance Weighting (IDW) interpolation in parallel, we use the quadtree approach to decompose the spatial domain and execute the IDW interpolation on these quads through the parametric tasks on Windows HPC platform. The whole implementation includes the executables of QuadDC, IDW, output gathering and data visualization which should be run serially. Compared with the traditional method to integrate each executable manually, the <b>Job</b> <b>Manager</b> of Windows HPC utilizes multi-task job to serialize all the executables as a workflow. The experiments test two cases of IDW 0 and IDW 6 with different domain clustering regions, and show that our implementation improves the functionality integration and efficiency of the spatial IDW interpolation...|$|E
40|$|Abstract. One of {{the main}} goals of the CrossGrid Project [1] is to provide {{explicit}} support to parallel and interactive compute- and dataintensive applications. The CrossBroker <b>job</b> <b>manager</b> provides services {{as part of the}} CrossGrid middleware and allows execution of parallel MPI applications on Grid resources in a transparent and automatic way. This document describes the design and implementation of the key components responsible for an efficient and reliable execution of MPI jobs splitted over multiple Grid sites, executed either in an on-line or batch manner. We also provide details on the overheads introduced by our system, as well as an experimental study showing that our system is well-suited for embarrassingly parallel applications. ...|$|E
40|$|Existing {{techniques}} for allocating processors in parallel and distributed {{systems are not}} suitable for use in large distributed systems. In such systems, dedicated multiprocessors should exist as an integral component of the distributed system, and idle processors {{should be available to}} applications that need them. The Prospero Resource Manager (PRM) is a scalable resource allocation system that supports the allocation of processing resources in large networks and on multiprocessor systems. PRM employs three types of managers, the <b>job</b> <b>manager,</b> the system manager, and the node manager, to manage resources in a distributed system. Multiple independent instances of each type of manager exist, reducing bottlenecks. When making scheduling decisions each manager utilizes information most closely associated with the entities for which it is responsible...|$|E
50|$|After a {{successful}} {{start to the}} 2006-07 season, Laws was offered the <b>job</b> of <b>manager</b> at Sheffield Wednesday, which he accepted, ending almost a decade {{in charge of the}} Iron.|$|R
50|$|He {{left the}} <b>job</b> as Cavan <b>manager</b> in July 2010.|$|R
5000|$|A {{version in}} Swedish radio program Framåt fredag was called [...] "Han ringde Hamrén" [...] {{and was about}} Lars Lagerbäck leaving his <b>job</b> as <b>manager</b> for the Sweden men's {{national}} soccer team.|$|R
40|$|In {{this paper}} we propose a {{software}} architecture that allows for processing of large geospatial data sets in the cloud. Our system is modular and flexible and supports multiple algorithm design paradigms such as MapReduce, in-memory computing or agent-based programming. It contains a web-based user interface where domain experts (e. g. GIS analysts or urban planners) can define high-level processing workflows using a domain-specific language (DSL). The workflows are passed through a number of components including a parser, interpreter, and a service called <b>job</b> <b>manager.</b> These components use declarative and procedural knowledge encoded in rules to generate a processing chain specifying the execution of the workflows on a given cloud infrastructure according to the constraints defined by the user. The <b>job</b> <b>manager</b> evaluates this chain, spawns processing services in the cloud and monitors them. The services communicate with each other through a distributed file system that is scalable and fault-tolerant. Compared to previous work describing cloud infrastructures and architectures we focus on the processing of big heterogeneous geospatial data. In addition to that, we do not rely on only one specific programming model or a certain cloud infrastructure but support several ones. Combined with the possibility to control the processing through DSL-based workflows, this makes our architecture very flexible and configurable. We do not only see the cloud as a means to store and distribute large data sets but also as a way to harness the processing power of distributed computing environments for large-volume geospatial data sets. The proposed architecture design has been developed for the IQmulus research project funded by the European Commission. The paper concludes with the evaluation results from applying our solution to two example workflows from this project...|$|E
40|$|Abstract. The paper {{describes}} a parallel program checkpointing mechanism {{and its potential}} application in Grid systems in order to migrate applications among Grid sites. The checkpointing mechanism can automatically (without user interaction) support generic PVM programs created by the PGRADE Grid programming environment. The developed checkpointing mechanism is general enough {{to be used by}} any Grid <b>job</b> <b>manager</b> but the current implementation is connected to Condor. As a result, the integrated Condor/PGRADE system can guarantee the execution of any PVM program in the Grid. Notice that the Condor system can only guarantee the execution of sequential jobs. Integration of the Grid migration framework and the Mercury Grid monitor results in an observable Grid execution environment where the performance monitoring and visualization of PVM applications are supported even when the PVM application migrates in the Grid. ...|$|E
40|$|Using {{workstation}} clusters for {{distributed computing}} has become {{popular with the}} proliferation of inexpensive, powerful workstations. Workstation clusters offer both a cost effective alternative to batch processing and an easy entry into parallel computing. However, a number of workstations on a network does not constitute a cluster. Cluster management software is necessary to harness the collective computing power. A variety of cluster management and queuing systems are compared: Distributed Queueing Systems (DQS), Condor, Load Leveler, Load Balancer, Load Sharing Facility (LSF - formerly Utopia), Distributed <b>Job</b> <b>Manager</b> (DJM), Computing in Distributed Networked Environments (CODINE), and NQS/Exec. The systems differ in their design philosophy and implementation. Based on published reports on the different systems and conversations with the system's developers and vendors, a comparison of the systems are made on the integral issues of clustered computing...|$|E
5000|$|In 2001, {{while serving}} as the Reds third base coach, Oester was offered the <b>job</b> of <b>manager</b> of the club. As the offer was below the market average, Oester turned it down.|$|R
5000|$|Compass Group 03/2008 - 02/2009 - <b>Job</b> Title: Sales <b>Manager</b> Nordic, ...|$|R
40|$|International audienceIn this paper, {{we examine}} how the {{execution}} context of grid jobs can help to refine submission strategies on a production grid. On this kind of infrastructure, the latency highly impacts performances. We present experiments that quantify the dependencies between the grid latency and {{both internal and external}} context parameters on the EGEE grid infrastructure. We show how <b>job</b> submission <b>managers,</b> <b>job</b> execution sites and the submission date can be statistically correlated to grid performances...|$|R
