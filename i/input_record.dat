29|457|Public
5000|$|... : Keeps a {{count of}} the number of fields in an <b>input</b> <b>record.</b> The last field in the <b>input</b> <b>record</b> can be {{designated}} by $NF.|$|E
5000|$|Stages {{normally}} {{access the}} <b>input</b> <b>record</b> in locate mode, and produce the output records before consuming the <b>input</b> <b>record.</b> This lock-step approach not only avoids copying {{the data from}} one buffer to the next; it also {{makes it possible to}} predict the flow of records in multi-stream pipelines.|$|E
50|$|More specifically, let us {{consider}} a subarray of W channels, where W<<N, which slides over the input data as shown in Fig. 1. For every subarray position an optimum multichannel filter based on (9) can be designed so that the undesired interferences are suppressed from its corresponding output trace. It {{should be noted that}} in designing this filter we use W instead of N in expression (9). thus traces 1,2,...,W of the <b>input</b> <b>record</b> produce the first trace of the output record, traces K,K+1,...K+W-1 of the <b>input</b> <b>record</b> produce the Kth trace of the output record, and traces N-W+1,N-W+2,...,N of the <b>input</b> <b>record</b> produce the (N-W+1)st trace, which is the last trace, of the output record. For a large N and small W, as is typically the case in geophysical data, the output record can be viewed as comparable in dimensions to the <b>input</b> <b>record.</b> Clearly for such a scheme to work effectively W must be as small as possible; {{while at the same time}} it must be large enough to provide the necessary attenuation of the undesired signals.Note that a maximum of W-1 undesired interferences can be totally suppressed by such a scheme.|$|E
40|$|PATSTAGS {{computer}} program translates data from PATRAN finite-element mathematical model into STAGS <b>input</b> <b>records</b> used for engineering analysis. Reads data from PATRAN neutral file and writes STAGS <b>input</b> <b>records</b> into STAGS <b>input</b> file and UPRESS data file. Supports translations of nodal constraints, and of nodal, element, force, and pressure data. Written in FORTRAN 77...|$|R
5000|$|<b>Input</b> <b>records</b> can be {{automatically}} {{edited and}} verified using the editing and error-handling facilities ...|$|R
50|$|The MapReduce System would {{line up the}} 1100 Map processors, {{and would}} provide each with its {{corresponding}} 1 million <b>input</b> <b>records.</b> The Map step would produce 1.1 billion (Y,(N,1)) records, with Y values ranging between, say, 8 and 103. The MapReduce System would then line up the 96 Reduce processors by performing shuffling operation of the key/value pairs {{due to the fact}} that we need average per age, and provide each with its millions of corresponding <b>input</b> <b>records.</b> The Reduce step would result in the much reduced set of only 96 output records (Y,A), which would be put in the final result file, sorted by Y.|$|R
5000|$|... : Stores {{the current}} [...] "record {{separator}}" [...] character. Since, by default, an input {{line is the}} <b>input</b> <b>record,</b> the default record separator character is a [...] "newline".|$|E
5000|$|... : Contains the [...] "field separator" [...] {{character}} used {{to divide}} fields on the <b>input</b> <b>record.</b> The default, [...] "white space", includes any space and tab characters. FS can be reassigned to another character {{to change the}} field separator.|$|E
5000|$|The Map Input Contract {{works in}} the same way as in MapReduce. It has a single input and assigns each <b>input</b> <b>record</b> to its own subset. Hence, all records are {{processed}} independently from each other (see figure below). \\ ...|$|E
5000|$|In {{cinematography}} and photography, pre-flashing is {{the exposure}} of the film or other photosensor to uniform light prior to exposing it to the scene to be imaged. This adds a bias to the overall light <b>input</b> <b>recorded</b> by the sensor.|$|R
40|$|Data mining {{applications}} place {{special requirements}} on clustering algorithms including: {{the ability to}} find clusters embedded in subspaces of high dimensional data, scalability, end-user comprehensibility of the results, non-presumption of any canonical data distribution, and insensitivity {{to the order of}} <b>input</b> <b>records.</b> We present CLIQUE, a clustering algorithm that satisfies each of these requirements. CLIQUE identifies dense clusters in subspaces of maximum dimensionality. It generates cluster descriptions in the form of DNF expressions that are minimized for ease of comprehension. It produces identical results irrespective of the order in which <b>input</b> <b>records</b> are presented and does not presume any specific mathematical form for data distribution. Through experiments, we show that CLIQUE efficiently finds accurate clusters in large high dimensional datasets. ...|$|R
5000|$|The Lyra x2400 is a {{portable}} audio/video recorder and player with a 3.5" [...] LCD screen. It has a CompactFlash slot, audio out, built-in speaker and RCA A/V <b>inputs.</b> <b>Recorded</b> video is compressed with an XVID encoder. The included software, Blaze Media Encoder, can transcode from most popular {{video and audio}} formats.|$|R
5000|$|An early {{version of}} Job Entry Control Language for OS/360 Remote Job Entry (Program Number 360S-RC-536) used the {{identifier}} [...] in columns 1 - 2 of the <b>input</b> <b>record</b> and consisted of a single control statement: [...] (Job Entry Definition). [...] "Workstation Commands" [...] such as , , and [...] also began with [...]|$|E
50|$|The **Constant Fields** {{annotation}} marks {{fields that}} are not modified by the user code function. Note that for every <b>input</b> <b>record</b> a constant field may not change its content and position in any output record! In case of binary second-order functions such as Cross, Match, and CoGroup, the user can specify one annotation per input.|$|E
5000|$|The I or Input specs are next, and {{describe}} the data areas within files. RPG II permits redefinition of data areas so that a field named FLDA might occupy the same area as an array AR that contains 8 elements of 1 character each. Non-record areas such as data structures can be described. Depending on {{the values of the}} <b>input</b> <b>record,</b> indicators may be conditioned.|$|E
40|$|The Water Rights Analysis Package (WRAP) is {{documented}} by a Reference Manual and this Users Manual. The Reference Manual explains WRAP capabilities and methodologies. This Users Manual provides the operational logistics for applying the model. The Users Manual describes {{the organization of}} input and output files and the content and format of <b>input</b> <b>records...</b>|$|R
40|$|S 05 L is {{a general}} purpose data {{handling}} program designed to facilitate the task of transferring a data file from one type of medium to another. It also {{has the ability to}} combine two or more <b>input</b> <b>records</b> to form one new output record. The paper describes all the necessary processes to operate the program and gives examples...|$|R
30|$|Mansuri and Sarawagi [18] {{propose to}} enrich an {{existing}} database with unstructured records taken from web sources. By matching information {{extracted from the}} <b>input</b> <b>records</b> and the data already in the database, their system inserts new data into the database to represent new entities and new relationships. The tasks of extraction and matching are based on models that are previously trained.|$|R
5000|$|... {{function}} Map is input: integer K1 between 1 and 1100, {{representing a}} batch of 1 million social.person records for each social.person record in the K1 batch do let Y be the person's age let N be the number of contacts the person has produce one output record (Y,(N,1)) repeat end function [...] function Reduce is input: age (in years) Y for each <b>input</b> <b>record</b> (Y,(N,C)) do Accumulate in S the sum of N*C Accumulate in Cnew the sum of C repeat let A be S/Cnew produce one output record (Y,(A,Cnew)) end function ...|$|E
50|$|The {{system was}} mainly used for {{key-to-disk}} operations {{to replace the}} previously popular IBM card punches and more advanced key-to-tape systems manufactured for example by Mohawk Data Sciences (MDS) or Singer. In addition to the basic key-to-disk function, the proprietary operating system, called XLOS, supported indexed file operations for on-line transaction processing even with data journaling. The system was programmed in two different ways. The data entry was either described in several tables that specified {{the format of the}} <b>input</b> <b>record</b> with optional automatic data validation procedures or the indexed file operations were programmed in a special COBOL dialect with IDX and SEQ file support.|$|E
30|$|Furthermore, {{enrichment}} in TOC and TOCMAR do not {{coincide with}} maxima of planktonic δ 18 O values and aeolian <b>input</b> <b>record</b> calculated by Wienberg et al. (2010) which points to periods of intensified trade wind and resulting changes in upwelling intensity (Figure  4).|$|E
40|$|This was a rush job on a {{spontaneous}} trip. It {{was not possible}} to stay long or to meet with just one person. The whole recording is rather chaotic. There were too many people but I tried to speak just with a few. Sonny Djonler was there to help as interpreter and to provide <b>input.</b> <b>Recording</b> and scan of transcription included...|$|R
40|$|Joining {{multiple}} datasets in MapReduce may {{amplify the}} disk and network overheads because intermediate join results {{have to be}} written to the underlying distributed file system, or map output records have to be replicated multiple times. This paper proposes a method for applying filters based on the processing order of input datasets, which is appropriate for {{the two types of}} multiway joins: common attribute joins and distinct attribute joins. The number of redundant records filtered depends on the processing order. In common attribute joins, the <b>input</b> <b>records</b> {{do not need to be}} replicated, so a set of filters is created, which are applied in turn. In distinct attribute joins, the <b>input</b> <b>records</b> have to be replicated, so multiple sets of filters need to be created, which depend on the number of join attributes. The experimental results showed that our approach outperformed a cascade of two-way joins and basic multiway joins in cases where small portions of input datasets were joined...|$|R
50|$|A sort is {{a special}} case: all the <b>input</b> <b>records</b> must be read before the first output record can be written. Hence {{there can be no}} overlap between the input and output phases of a sort. But the input phase can be {{overlapped}} with the previous job's output phase. Similarly, the output phase of sort can be overlapped with a downstream job that reads the sorted data.|$|R
40|$|A Bayesian {{simulation}} {{model has been}} applied to a database developed for split liver transplantation on two adult recipients (SLT A/A) {{in the context of a}} macroregional project funded by the Italian Ministry of Health. The model was entered within Bayesian inference Using Gibbs Sampling (WinBUGS), a free software for Bayesian analysis of complex statistical models using Markov chain Monte Carlo techniques developed by the MRC Biostatistics Unit Cambridge jointly with the Imperial College School of Medicine at St Mary's, London. The model was built by using data entry performed from January 1, 2005 to August 5, 2005. In that period, 20 potential donors suitable for the SLT A/A procedure were entered into the database. We only selected the continuous and dichotomous donor-related variables (DRV, n = 62) for which almost one data entry procedure. The model assumed that a database user learned during data entry procedures for each donor, and that the probability of a successful input may depend on the number of previous errors and corrections. After binary transformation of the DRV (value 0 for each <b>input</b> <b>record,</b> value 1 for each no <b>input</b> <b>record),</b> we calculated an overall value of 0. 28 +/- 0. 27 (median: 0. 3; 95 % confidence interval: from 0. 18 to 0. 629). The transformed DRV were entered within the WinBUGS environment after model specification, assuming as success (y = 1) each procedure of <b>input</b> <b>record,</b> and as failure (y = 0) each procedure of no <b>input</b> <b>record.</b> A unequivocal convergence was obtained after 10, 000 iterations, and a simulation run was launched for a further 10, 000 updates. We obtained a negligible Monte Carlo error and a fine profile in the kernel density plot. This study supported the application of {{simulation model}}s to databases concerning liver transplantation as a useful strategy to identify a critical state in the data entry process...|$|E
30|$|The {{elements}} of Figure  1 represent the key information needed for individual-based modelling of forests (Weiskittel et al., 2011). These data may be read from file by the growth model, with each <b>input</b> <b>record</b> forming {{one of many}} tree records or cohorts in the model, a technique used widely in forest growth modelling (Vanclay, 1994 b; Porte and Bartelink, 2002). These cohorts retain the species identity, and progressively increase tree size to reflect growth, and reduce stocking to reflect mortality.|$|E
30|$|The Maldives {{sedimentary}} archive {{contains a}} valuable lithogenic <b>input</b> <b>record</b> reflecting on terrestrial climate {{changes in the}} source areas of the Indian-Asian landmass or from further afield. The Fe/K aridity record shows orbitally forced cycles reflecting on changes in {{the relative importance of}} aeolian (stronger winter monsoon) during dry glacial periods versus fluvial supply (stronger summer monsoon) during humid interglacial periods. The chronology is based on precessional cycles visually present in the Fe/K record, where Fe/K maxima correlate to precession minima (insolation maxima) with zero phase lag. The value of this approach results in an age model independent from the LR 04 record.|$|E
3000|$|The Map {{function}} task is {{to calculate}} the distance between each record and the center point and remark the focus clustering category. The <b>input</b> is all <b>recorded</b> data for clustering and iterated clustering center from the previous round, with the record data form of[*]<[*]key, value[*]>[*]pairs as[*]<[*]line number, recording line>; each Map function will read the described file of clustering center, and the Map function will calculate the nearest class center to the <b>input</b> <b>recording</b> point and make a new category marking; the form of output intermediate result[*]<[*]key, value[*]>[*]is[*]<[*]cluster category ID, record attribute vector >. The pseudo code of Map function is as follows: [...]...|$|R
30|$|The process {{coefficient}} matrix T, {{which is}} also known as product-by-process coefficient matrix (Heijungs and Suh 2002; Suh 2004), shows all <b>inputs</b> (<b>recorded</b> as negative values) and outputs (recorded as positive values) associated with the production of one functional unit of products in physical units. It is a symmetric matrix extended at the bottom with the environmental extension coefficient matrix R̃ that shows the amount of direct environmental interventions per functional unit.|$|R
40|$|Abstract. US {{input-output}} tables {{are of the}} type where intermediate <b>inputs</b> <b>record</b> the sum of imported and domestically produced goods. In modeling exercises this implies that imports are required to be specified exogenously. This has two consequences, which seriously restricts the usefulness of US-type tables. First, the multipliers can only be interpreted under the highly implausible assumption that the (changes in) imports are zero. Second, {{the results will be}} strongly overestimated in an empirical analysis, unless perfect foresight exists...|$|R
40|$|Today’s record {{matching}} infrastructure {{does not}} allow a flexible way to account for synonyms such as “Robert ” and “Bob ” which refer to the same name, and more general forms of string transformations such as abbreviations. We expand the problem of record matching to take such user-defined string transformations as input. These transformations coupled with an underlying similarity function are used to define the similarity between two strings. We demonstrate {{the effectiveness of this}} approach via a fuzzy match operation that is used to lookup an <b>input</b> <b>record</b> against a table of records, where we have an additional table of transformations as input. We demonstrate an improvement in record matching quality and efficient retrieval based on our index structure that is cognizant of transformations...|$|E
40|$|The Bibliographic Record Card {{serves as}} the basic <b>input</b> <b>record</b> for each item entered in the System. It {{consequently}} served as the link with all subsequent processes which are carried out after initial input. The Record Card which contains the bibliographic record or docu-ment profile, provides unique identification of each document, ensur-ing retrieval from various access points. The Physical Layout Although more detailed than the conventional catalogue card, the physical format has been structured to permit filing in a card cata-logue. The CARISPLAN national focal points, and participating centres all opted for the record card being prepared in the card form {{in which it is}} now used. The Record Card integrates information for the various types of literature being treated, so that the main subdivision is betwee...|$|E
40|$|A {{computer}} program was developed at Utah State University (USU) {{to aid in}} obtaining a more complete individual performance record keeping system for beef cattle in Utah. Some {{computer program}}s for beef cattle records presently exist but a program was needed that was readily available to the USU animal science extension and resident staff. The program was written in FORTRAN for use on the Burrogh 6700 computer located at the Utah State University Computer Center. It was designed to read input data for individual animals, perform various calculations (i. e. days of age, adjusted weaning weight and weaning weight ratio), print out the input data and results of the calculations for each animal {{as well as the}} average adjusted weight for each sex group (heifer, bull, steer). The computer program will manipulate weights in either the English or metric system and will convert weights from the English to metric system if desired. A unique feature of the program is the ranking of animals from highest to lowest based on the weaning weight ratio with accompanying animal number. The records can be evaluated to identify potential animals to use as replacements and those to be culled. The input data are collected on the ranch by a cooperative arrangement between the ranch operator and the USU Extension Staff. The ranch operator collects the preliminary data such as: birth date, tag number, tattoo number, dam, age of dam, and sire, and records it on the beef cattle performance <b>input</b> <b>record.</b> The extension specialist weights, gives a conformation score and records the information for each calf on the <b>input</b> <b>record.</b> The beef cattle performance <b>input</b> <b>record</b> is arranged in the same order as the data card is key punched thus facilitating the punching of the data cards. The staff can change from using the desk calculator {{to the use of the}} computer to improve efficiency and flexibility output as well as having more time available to spend with the public teaching that maintaining accurate records can help improve their herds for production and for inventory control. This can help the beef cattle industry to improve quality and type of beef animal produced in Utah and should improve the potential efficiency and profit. The rancher can transfer his records, with minor modifications, to one of the existing national computer programming organizations if desired. This computer program with or without modification has application for current research and university teaching. The computer program was designated for use with beef cattle, but could be modified to use for any class of livestock. This program is not an end in itself but is a foundation from which to build an improved record keeping system in Utah which could improve the production and quality of the beef cattle industry...|$|E
40|$|Abstract. Improvement of {{the expert}} system shell Pro/ 3 {{sentence}} <b>record</b> <b>input</b> function is introduced. The function of the Pro/ 3 inputting disposable a large quantities of sentence records corresponding to a sentence, i. e., batch <b>inputting</b> the sentence <b>records,</b> was implemented. The basic security condition assessment method of some desktop terminal computers in the power supply company was explored to achieve the expert system about these desktop terminal computers basic security condition assessment. Experimental {{results show that the}} batch <b>inputting</b> sentence <b>records</b> function is more effective and the desktop terminal computers basic security condition assessment expert system is feasible...|$|R
40|$|For {{derivative}} calculations, debugging {{and several}} other purposes one may need to reverse the execution of a computer program for given <b>inputs.</b> <b>Recording</b> on the way forward all the data required for the reversal and restoring it on the way backwards is a simple approach to implement the reversal. However, the memory requirement of this naive reversal strategy {{is proportional to the}} runtime of the computer program. Therefore, the practical application of this time-minimal technique is limited to the reversal of small problems. If an...|$|R
50|$|This compact camera {{comes with}} a 3.5mm audio input - not a {{balanced}} XLR <b>input.</b> It <b>records</b> up to 6 hours of full HD on 64Gb internal memory.|$|R
