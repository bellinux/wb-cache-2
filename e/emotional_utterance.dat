7|47|Public
6000|$|... "No. I didn't say {{anything}} about leaving you." [...] He {{put his arm around}} her. [...] "I'm not that kind of a man. You and I were built for each other--I felt that on that first ride. I guess it's up to me to take you out of this." [...] He broke off his <b>emotional</b> <b>utterance</b> and grew keen and alert.|$|E
6000|$|My {{conscious}} memory holds {{nothing of}} my mother's agony of waiting, {{nothing of the}} dark days when the baby was ill and the doctor far away--but into my subconscious ear her voice sank, and the words Grant, Lincoln, Sherman, [...] "furlough," [...] "mustered out," [...] ring like bells, deep-toned and vibrant. I shared dimly in every <b>emotional</b> <b>utterance</b> of the neighbors who came to call and {{a large part of}} what I am is due to the impressions of these deeply passionate and poetic years.|$|E
60|$|Who cares whether Mr Ruskin's {{views on}} Turner are sound or not? What does it matter? That mighty and majestic prose of his, so fervid and so fiery {{coloured}} in its noble eloquence, so rich in its elaborate symphonic music, so sure and certain, at its best, in subtle choice of word and epithet, is, at least, {{as great a}} work of art as any of those wonderful sunsets that bleach or rot on their corrupted canvases in England's gallery--greater, indeed, one is apt to think at times, not merely because its equal beauty is more enduring but on account of the fuller variety of its appeal--soul speaking to soul in those long, cadenced lines, not through form and colour alone, though through these, indeed, completely and without loss, but with intellectual and <b>emotional</b> <b>utterance,</b> with lofty passion and with loftier thought, with imaginative insight and with poetic aim--greater, I always think, even as literature is the greater art.|$|E
40|$|This paper {{provides}} {{a description of}} the structure and goals of the database of Russian Language Affective (<b>emotional)</b> <b>utterances.</b> It also reports some preliminary results of an experimental phonetic analysis of the acoustic characteristics of <b>emotional</b> <b>utterances</b> (surprise, happiness, anger, sadness and fear) vs. neutral ones in Russian. The study utilizes 600 database utterances by 10 speakers. Special focus of the study is placed on interactions between prosody and syntactic patterns of utterances. 1...|$|R
40|$|Communication is an {{essential}} part of our life. Though, not only communication is the key – it is all about emotional (prosodic) communication. Due to empirical research, people, who are augmentative communicators and speak with a voice output communication aid, want to express their emotions in the same way as everybody else – it is one of their deepest interests (Portnuff, 2006; Hoffmann and Wülfing, 2010). So far, current devices lack the opportunity of <b>emotional</b> <b>utterances.</b> This circumstance leads not only to a huge usability deficit, but furthermore, it is an obstacle to develop emotional competence and to behave as well as regulate one´s emotion adequately (Blackstone and Wilkins, 2009). This article aims to increase the sensitivity for the importance of emotional communication. Furthermore, it tries to give first suggestions for implementing an usable device that supports users with a voice output communication aid to express <b>emotional</b> <b>utterances.</b> This could be done by using phrase-generation, as mentiond by Vanderheyden and Pennigton (1998) ...|$|R
40|$|In {{this paper}} {{we report on}} first {{experiments}} {{for the detection of}} emotion and the use of this information in a complex speech understanding system like Verbmobil. We do not look at lexical information like swear words but rather try to find <b>emotional</b> <b>utterances</b> with the use of acoustic prosodic cues. We only want to classify angry versus neutral speaking style. 20 speakers were asked to produce 50 neutral and 50 angry utterances. With this data set we created one training set and two test sets. One test set with seen speakers, but new turns, the other with unseen speakers, but seen turns. Each word of the <b>emotional</b> <b>utterances</b> was labeled as belonging to the class "emotional", each word in the neutral utterances as belonging to the class "neutral". For each word 276 prosodic features were calculated and multi layer perceptrons were trained for the two classes. We achieved a precision of 87 % and a recall of 92 % for the one test set and 94 % respectively 84 % for the other (precision resp [...] ...|$|R
6000|$|GILBERT. Of {{course it}} is. Who cares whether Mr. Ruskin's views on Turner are sound or not? What does it matter? That mighty and majestic prose of his, so fervid and so fiery-coloured in its noble eloquence, so rich in its {{elaborate}} symphonic music, so sure and certain, at its best, in subtle choice of word and epithet, {{is at least}} as great a work of art as any of those wonderful sunsets that bleach or rot on their corrupted canvases in England's Gallery; greater indeed, one is apt to think at times, not merely because its equal beauty is more enduring, but on account of the fuller variety of its appeal, soul speaking to soul in those long-cadenced lines, not through form and colour alone, though through these, indeed, completely and without loss, but with intellectual and <b>emotional</b> <b>utterance,</b> with lofty passion and with loftier thought, with imaginative insight, and with poetic aim; greater, I always think, even as Literature is the greater art. Who, again, cares whether Mr. Pater has put into the portrait of Monna Lisa something that Lionardo never dreamed of? The painter may have been merely the slave of an archaic smile, as some have fancied, but whenever I pass into the cool galleries of the Palace of the Louvre, and stand before that strange figure 'set in its marble chair in that cirque of fantastic rocks, as in some faint light under sea,' I murmur to myself, 'She is older than the rocks among which she sits; like the vampire, she has been dead many times, and learned the secrets of the grave; and has been a diver in deep seas, and keeps their fallen day about her: and trafficked for strange webs with Eastern merchants; and, as Leda, was the mother of Helen of Troy, and, as St. Anne, the mother of Mary; and all this has been to her but as the sound of lyres and flutes, and lives only in the delicacy with which it has moulded the changing lineaments, and tinged the eyelids and the hands.' And I say to my friend, 'The presence that thus so strangely rose beside the waters is expressive of what in the ways of a thousand years man had come to desire'; and he answers me, 'Hers is the head upon which all [...] "the ends of the world are come," [...] and the eyelids are a little weary.' ...|$|E
6000|$|Who cares whether Mr. Ruskin's {{views on}} Turner are sound or not? What does it matter? That mighty and majestic prose of his, so fervid and so fiery-coloured in its noble eloquence, so rich in its {{elaborate}} symphonic music, so sure and certain, at its best, in subtle choice of word and epithet, {{is at least}} as great a work of art as any of those wonderful sunsets that bleach or rot on their corrupted canvases in England's Gallery; greater indeed, one is apt to think at times, not merely because its equal beauty is more enduring, but on account of the fuller variety of its appeal, soul speaking to soul in those long-cadenced lines, not through form and colour alone, though through these, indeed, completely and without loss, but with intellectual and <b>emotional</b> <b>utterance,</b> with lofty passion and with loftier thought, with imaginative insight, and with poetic aim; greater, I always think, even as Literature is the greater art. Who, again, cares whether Mr. Pater has put into the portrait of Monna Lisa something that Lionardo never dreamed of? The painter may have been merely the slave of an archaic smile, as some have fancied, but whenever I pass into the cool galleries of the Palace of the Louvre, and stand before that strange figure 'set in its marble chair in that cirque of fantastic rocks, as in some faint light under sea,' I murmur to myself, 'She is older than the rocks among which she sits; like the vampire, she has been dead many times, and learned the secrets of the grave; and has been a diver in deep seas, and keeps their fallen day about her: and trafficked for strange webs with Eastern merchants; and, as Leda, was the mother of Helen of Troy, and, as St. Anne, the mother of Mary; and all this has been to her but as the sound of lyres and flutes, and lives only in the delicacy with which it has moulded the changing lineaments, and tinged the eyelids and the hands.' And I say to my friend, 'The presence that thus so strangely rose beside the waters is expressive of what in the ways of a thousand years man had come to desire'; and he answers me, 'Hers is the head upon which all [...] "the ends of the world are come," [...] and the eyelids are a little weary.' ...|$|E
40|$|Mere {{exposure}} to faces increases attention to vocal affect: A cross-cultural investigation （顔の提示による声の情動への注意の増加：比較文化研究） In interpersonal communication, vocal affect often reveals the speaker’s relational attitudes. Because knowing partners ’ relational attitudes {{is crucial in}} subsequent social interaction, people may automatically allocate attention to vocal affect especially when they are relationally engaged. The present work examined cross-culturally whether automatic attention to vocal affect would be enhanced by a mere {{exposure to}} schematic faces, which are ubiquitous in social interaction and cues indicating social engagement. Japanese and American participants judged the verbal meaning of emotionally spoken emotional words while ignoring the vocal tone. Consistent with previous studies, interference by to-be-ignored vocal affect was significantly greater for Japanese than for Americans. Moreover, as predicted, it was also greater when participants were exposed to schematic human faces while listening to the stimulus utterance regardless of cultures, suggesting that attention to vocal tone increases in a much subtler, cross-cultural fashion and with mere exposure to schematic faces. Implications for future work are discussed. Key words: culture, face, attention, vocal tone, and <b>emotional</b> <b>utterance...</b>|$|E
40|$|Abstract — This paper aims {{to build}} an affect {{recognition}} system by analysing acoustic speech signals. A database of 391 authentic <b>emotional</b> <b>utterances</b> was collected from 11 speakers. Two emotions, angry and neutral, were considered. Features relating to pitch, energy and rhythm were extracted and used as feature vectors for a neural network. Forward selection was employed to prune redundant and harmful inputs. Initial results show a classification rate of 86. 1 %. I...|$|R
40|$|Emotional prosody {{difficulties}} {{have been}} found in recently detoxified alcoholics. Through three experiments, it was explored if these production and perception deficits per se continue even after a period of long-term abstinence. In Study one, 15 dry abstained alcoholics (AA) and 15 aged/educational matched healthy controls were asked to produce sentences in the six basic emotions plus neutral whilst being recorded. Results demonstrated that at an acoustic level pitch was a cue that AA struggled to modulate emotionally compared to healthy controls. The aim of Study 2 was to firstly explore on a perception level whether AA <b>emotional</b> <b>utterances</b> from Study 1 were perceived differently from those of healthy controls. A further goal was to explore how voice qualities of AA compared with healthy controls. To this aim, twenty-one naïve listeners heard randomly selected recordings from Study 1 and were asked to judge the emotion in a force-choice paradigm followed by a judgment of the speakers voice quality. Results showed naïve listeners find it more difficult to judge AA <b>emotional</b> <b>utterances</b> compared to those of healthy controls supporting acoustic results from Study 1. Listeners also rated AA voice quality as huskier, flat and less emotionally expressive than healthy controls. Finally in Study 3 abstained alcoholics perception of emotional prosody was investigated. Fifteen AA and 15 aged/educational matched healthy controls heard <b>emotional</b> <b>utterances</b> from Study 1 and were asked to identify the emotion heard in the tone of voice. Analyses showed that AA performed worse than healthy controls at judging emotional prosody. This applies to both stimuli uttered by AA or healthy controls. All these results combined demonstrate that abstained alcoholics show an emotional prosody deficit at the production and perception level. Potential reasons for this deficit are further discussed in this thesis...|$|R
40|$|An affective priming {{paradigm}} {{was used}} to test the hypothesis that when presented with an emotionally spoken emotional word, speakers of a high-context language (e. g., Japanese) process its vocal tone more thoroughly than its verbal content. Native Japanese speakers were presented with such an utterance (prime), immediately followed by a neutrally spoken emotionally valenced word (target). They were to judge the emotional meaning of the target as quickly as possible. In support of the foregoing hypothesis, Study 1 showed that this judgment is made more quickly if the prime is spoken in a congruous emotional vocal tone than if it is spoken in an incongruous tone. However, no comparable effect was found {{as a function of the}} emotional verbal meaning of the prime. This was the case despite the fact that emotional vocal tones of primes were considerably less extreme than their verbal meanings. Furthermore, Study 2 predicted and found that when a prime is spoken in a neutral tone of voice, the verbal meaning of the prime has a reliable priming effect. Implications for culture, communication, and cognition are discussed. Key words: Japanese, <b>emotional</b> <b>utterances,</b> vocal tone, affective priming. 3 Processing of <b>emotional</b> <b>utterances...</b>|$|R
40|$|Interpersonal {{communication}} {{involves the}} processing of multimodal emotional cues, particularly facial expressions (visual modality) and emotional speech prosody (auditory modality) which can interact during information processing. Here, we investigated whether the implicit processing of emotional prosody systematically influences gaze behavior to facial expressions of emotion. We analyzed the eye movements of 31 participants as they scanned a visual array of four emotional faces portraying fear, anger, happiness, and neutrality, while listening to an emotionally-inflected pseudo-utterance (Someone migged the pazing) uttered in a congruent or incongruent tone. Participants heard the <b>emotional</b> <b>utterance</b> during the first 1250 milliseconds of a five-second visual array and then performed an immediate recall decision about the face they had just seen. The frequency and duration of first saccades and of total looks in three temporal windows ([0 - 1250 ms], [1250 - 2500 ms], [2500 - 5000 ms]) were analyzed according to the emotional content of faces and voices. Results showed that participants looked longer and more frequently at faces that matched the prosody in all three time windows (emotion congruency effect), although this effect was often emotion-specific (with greatest effects for fear). Effects of prosody on visual attention to faces persisted over time and could be detected long after the auditory information was no longer present. These data imply that emotional prosody is processed automatically during communication and that these cues {{play a critical role}} in how humans respond to related visual cues in the environment, such as facial expressions...|$|E
40|$|Abstract. D-scripts {{model is}} {{originally}} developed for description of affective (emotional) mass media texts and with extension {{also applies to}} emotional speech synthesis. In this model we distinguish units for “rational ” inference (r-scripts) and units for “emotional ” processing of meaning (d-scripts). Basing on a psycholinguistics study we demonstrate relations between classes of <b>emotional</b> <b>utterances</b> in d-script model and psychological characteristics of informants. The study proposes a theoretical framework for an affective agent simulating given psychological characteristics in it's emotional speech behaviour. 1...|$|R
40|$|Abstract—Speaker {{verification}} {{suffers from}} significant per-formance degradation with emotion variation. In a previous study, we {{have demonstrated that}} an adaptation approach based on MLLR/CMLLR can provide a significant performance improvement for verification on emotional speech. This paper follows this direction and presents an emotional adaptive training (EAT) approach. This approach iteratively estimates the emotion-dependent CMLLR transformations and re-trains the speaker models with the transformed speech, which therefore can make use of emotional enrollment speech to train a stronger speaker model. This {{is similar to the}} speaker adaptive training (SAT) in speech recognition. The experiments are conducted on an emotional speech database which involves speech recordings of 30 speakers in 5 emotions. The results demonstrate that the EAT approach provides significant performance improvements over the baseline system where the neutral enrollment data are used to train the speaker models and the <b>emotional</b> test <b>utterances</b> are ver-ified directly. The EAT also outperforms another two emotion-adaptation approaches in a significant way: (1) the CMLLR-based approach where the speaker models are trained with the neutral enrollment speech and the <b>emotional</b> test <b>utterances</b> are transformed by CMLLR in verification; (2) the MAP-based approach where the emotional enrollment data are used to train emotion-dependent speaker models and the <b>emotional</b> <b>utterances</b> are verified based on the emotion-matched models. I...|$|R
40|$|AbstractThis paper {{introduces}} “Paralingua” - a new speech corpus created {{within a}} larger ongoing project whose primary aim {{was to develop a}} speaker recognition and identification system for forensics. The present corpus was designed for the purpose of analysis of selected paralinguistic features in continuous speech and for preliminary examination of the vocal display of affective states. The recorded (and annotated) data include conversational speech in the form of task-oriented dialogues, <b>emotional</b> <b>utterances</b> (realized as emotion portrayals), and an acted court scene. As a reference material, a short read text was provided by each of the speakers...|$|R
40|$|Abstract—We {{propose a}} novel {{real-time}} affect classification {{system based on}} features extracted from the acoustic speech signal. The proposed system analyses the speech signal and provides a real-time classification of the speaker’s perceived affective state. A neural network is trained and tested using a database of 391 authentic <b>emotional</b> <b>utterances</b> from 11 speakers. Two emotions, anger and neutral, are considered. The system {{is designed to be}} speaker and text-independent and is to be deployed in a call-centre environment to assist in the handling of customer inquiries. We achieve a success rate of 80. 1 % accuracy in our preliminary results. I...|$|R
40|$|We {{introduce}} a novel emotion recognition approach which integrates ranking models. The approach is speaker independent, {{yet it is}} designed to exploit information from utterances from the same speaker in the test set before making predictions. It achieves much higher precision in identifying <b>emotional</b> <b>utterances</b> than a conventional SVM classifier. Furthermore we test several possibilities for combining conventional classification and predictions based on ranking. All combinations improve overall prediction accuracy. All experiments are performed on the FAU AIBO database which contains realistic spontaneous emotional speech. Our best combination system achieves 6. 6 % absolute improvement over the Interspeech 2009 emotion challenge baseline system on the 5 -class classification tasks. Index Terms: emotion classification, ranking models, spontaneous speec...|$|R
40|$|Abstract. This paper brings {{together}} two {{important aspects of}} the human-machine interaction in cars: the psychological aspect and the engineering aspect. The psychologically motivated part of this study addresses questions such as why it is important to automatically assess the driver’s affective state, which states are important and how a machine’s response should look like. The engineering part studies how the emotional state of a driver can be estimated by extracting acoustic features from the speech signal and mapping them to an emotion state in a multidimensional, continuous-valued emotion space. Such a feasibility study is performed in an experiment in which spontaneous, authentic <b>emotional</b> <b>utterances</b> are superimposed by car noise of several car types and various road surfaces. ...|$|R
40|$|Incorporating {{multimodal}} {{information and}} temporal context from speakers during an emotional dialog {{can contribute to}} improving performance of automatic emotion recognition systems. Motivated by these issues, we propose a hierarchical framework which mod-els emotional evolution within and between <b>emotional</b> <b>utterances,</b> i. e., at the utterance and dialog level respectively. Our approach can incorporate a variety of generative or discriminative classifiers at each level and provides flexibility and extensibility in terms of multimodal fusion; facial, vocal, head and hand movement cues can be included and fused according to the modality and the emotion classification task. Our results using the multimodal, multi-speaker IEMOCAP database indicate that this framework is well-suited for cases where emotions are expressed multimodally and in context, as in many real-life situations. Index Terms — hierarchical HMM, multimodality, dialog mod-eling, discriminative training, emotion recognition 1...|$|R
40|$|Personal {{use of this}} {{material}} is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing {{this material}} for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. Bie, F., Wang, D., Zheng, T. F., Tejedor, J., Chen, R. "Emotional adaptive training for speaker verification", in Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2013 Asia-Pacific, 2013, pp. 1 - 4 Speaker verification suffers from significant performance degradation with emotion variation. In a previous study, we have demonstrated that an adaptation approach based on MLLR/CMLLR can provide a significant performance improvement for verification on emotional speech. This paper follows this direction and presents an emotional adaptive training (EAT) approach. This approach iteratively estimates the emotion-dependent CMLLR transformations and re-trains the speaker models with the transformed speech, which therefore can make use of emotional enrollment speech to train a stronger speaker model. This {{is similar to the}} speaker adaptive training (SAT) in speech recognition. The experiments are conducted on an emotional speech database which involves speech recordings of 30 speakers in 5 emotions. The results demonstrate that the EAT approach provides significant performance improvements over the baseline system where the neutral enrollment data are used to train the speaker models and the <b>emotional</b> test <b>utterances</b> are verified directly. The EAT also outperforms another two emotionadaptation approaches in a significant way: (1) the CMLLR-based approach where the speaker models are trained with the neutral enrollment speech and the <b>emotional</b> test <b>utterances</b> are transformed by CMLLR in verification; (2) the MAP-based approach where the emotional enrollment data are used to train emotion-dependent speaker models and the <b>emotional</b> <b>utterances</b> are verified based on the emotion-matched models. This work was supported by the National Natural Science Foundation of China under Grant No. 61271389 and the National Basic Research Program (973 Program) of China under Grant No. 2013 CB 329302...|$|R
40|$|The {{analysis}} of parameters extracted from speech data may contribute, {{together with other}} approaches, to the analysis and classification of a subject emotional status. Pitch value and variability {{have been shown to}} carry useful information to reach this goal. However the non stationarity of running speech and the short duration of utterances represent a difficulty for the estimation of these parameters. In this work a method based on a variation of the Sawtooth Waveform Pitch Estimator (SWIPE') to estimate pitch and jitter in vowel sound, is evaluated. The performances of the approach are assessed on simulated datasets with varying signal to noise ratios and jitter values. Issues related to data length are introduced and discussed through simulations. A comparison of the approach performances with the Simplified Inverse Filtering Technique (SIFT) is presented. Preliminary results on vowels extracted from a database of <b>emotional</b> <b>utterances</b> are introduced...|$|R
40|$|Psychosomatic Medicine {{is aiming}} at a {{comprehensive}} understanding of patient's requests. This requires patient-centred communication. During the two-year course for "Psychosomatic Medicine"of the Lower-Austrian Medical Chamber at the Danube-University Krems relevant techniques are trained. This paper {{reports on the}} analysis of 120 video-consultations with simulated patients (30 participants, two per participant {{before and after the}} training) using {{a modified version of the}} Roter Interaction Analysis System (RIAS). Results show a considerable increase in participant's ability to respond to <b>emotional</b> <b>utterances</b> and to use techniques of patient-centred communication: percentage of appropriate utterances related to the sum of all utterances: from 9. 78 ± 3. 5 to 13. 56 ± 4. 7 (ANOVA with repeated measures: p> 0. 001). Furthermore, participants allow patients longer stretches of uninterrupted speech: increase from 1. 76 ± 1. 4 to 2. 47 ± 2. 3 utterances (p> 0. 001) helping them into a narrative style of conversation...|$|R
40|$|In a {{production}} and a perception study, {{the relation between}} the emotion or attitude expressed in an utterance, and the intonation pattern realized on that utterance was investigated. In the production study, the pitch contours of <b>emotional</b> <b>utterances</b> were labelled in terms of the IPO intonation grammar. One intonation pattern, the 1 &A, was produced in all emotions studied. Some other patterns were specifically used in expressing some emotions. In the perception study checking the perceptual relevance of these findings, the role of the patterns present in the database was tested. A listening test provided converging evidence on the contribution of specific intonation patterns in the perception of some of the emotions and attitudes studied. Some intonation patterns, such as final 3 C and 12, which were specifically produced in some emotion, e. g., indignation, also introduced a perceptual bias towards that emotion. In that sense, the results from the perception study supported the results from the production study...|$|R
40|$|This paper studies articulatory, {{acoustic}} and perceptual {{characteristics of}} Mandarin Chinese <b>emotional</b> <b>utterances</b> as produced by two speakers, expressing Neutral, Angry, Sad and Happy emotions. Articulatory patterns were recorded using ElectroMagnetic Articulography (EMA), together with acoustic recordings. The acoustic and articulatory {{analysis revealed that}} Happy and Angry were generally higherpitched, louder, and produced with a more open mouth than Neutral or Sad. Sad is produced with low back tongue dorsum position and Happy, with a forward position, and for one speaker, duration was longer for Angry and Sad. Moreover, F 1 and F 2 are more dispersed (i. e., hyperarticulated) in emotional speech than Neutral speech. Perception tests conducted with 18 native listeners suggest that listeners were able to perceive the expressed emotions far above chance level. The louder and higher pitched the <b>utterance,</b> the more <b>emotional</b> the speech tends to be perceived. We also explore specific articulatory and acoustic correlates of each type of emotional speech, and how they impact perception...|$|R
5000|$|William Reddy {{includes}} {{the idea of}} sincerity as a key point in the effects of emotive. The concept of emotives forces a redefinition of sincerity. Because of the powerful and unpredictable effects of <b>emotional</b> <b>utterances</b> on the speaker, sincerity {{should not be considered}} the natural, best, or most obvious state toward which individuals strive. On the contrary, probably the most obvious orientation toward the power of emotives is a kind of fugitive instrumentalism (Reddy 1999). One might say that, just as a performative can be happy or unhappy, an emotive brings emotional effects appropriate to its content or effects that differ markedly from its content. If it does bring up appropriate effects, then the emotive, in Western context, might be said to be [...] "sincere"; if it does not, the emotive may be claimed, after the fact, to be hypocrisy, an evasion, a mistake, a projection, or a denial (Reddy 1997). Emotives are both self-exploring and self-altering (Reddy 1999).|$|R
40|$|The ways {{in which}} we move our faces and bodies are the source of much biologically {{important}} information. Such movements can be exaggerated by extending techniques developed for static facial caricature into the temporal domain. Spatial exaggeration of movement is accomplished by first time-normalising the sequences to be exaggerated and then exaggerating the differences between individual frames and an average frame. We can also exaggerate the temporal properties of movement by reversing and extrapolating the time-normalisation step. Previous findings from a variety of domains where exaggeration has been shown to enhance the perception of task-relevant information are reviewed, together with new data showing that spatial exaggeration of <b>emotional</b> <b>utterances</b> relative to an averaged utterance can enhance their perceived happiness, sadness, or angriness. Conversely, it is argued that exaggerating emotional or other individual differences may actually interfere with the recovery of information common to all the sequences being averaged, in this case the lexical content. Lastly, motion exaggeration, like facial caricature, may reflect general underlying principles involved in the encoding and discrimination of biological movement, an as yet poorly understood process...|$|R
40|$|We {{study the}} cross-database speech emotion {{recognition}} based on online learning. How {{to apply a}} classifier trained on acted data to naturalistic data, such as elicited data, remains a major challenge in today’s speech emotion recognition system. We introduce three types of different data sources: first, a basic speech emotion dataset which is collected from acted speech by professional actors and actresses; second, a speaker-independent data set which contains {{a large number of}} speakers; third, an elicited speech data set collected from a cognitive task. Acoustic features are extracted from <b>emotional</b> <b>utterances</b> and evaluated by using maximal information coefficient (MIC). A baseline valence and arousal classifier is designed based on Gaussian mixture models. Online training module is implemented by using AdaBoost. While the offline recognizer is trained on the acted data, the online testing data includes the speaker-independent data and the elicited data. Experimental results show that by introducing the online learning module our speech emotion recognition system can be better adapted to new data, which is an important character in real world applications...|$|R
40|$|The {{thesis is}} focused on the {{emotional}} states classification in the Matlab program, using neural networks and the classifier which is based on a combination of Gaussian density functions. It deals with the speech signal processing; the prosodic and spectral signs and the MFCC coefficients were extracted from the signal. The work also deals with the quality evaluation of individual signs of which the most suitable were chosen in order to provide the correct classification of emotional states. In order to identify the emotional states, two different methods were used. The first method of classification was the use of neural networks with differently selected parameters, and the second method was the use of the Gaussian mixture model (GMM). In both methods, a database of <b>emotional</b> <b>utterances</b> was divided into the training group and the test group. The testing was based on a method independent of the speaker. The work also includes the comparison of individual analyzed methods as well as the representation and comparison of the results. The conclusion comprises a proposition for the best parameters and the best classifier for the recognition of the speaker’s emotional state...|$|R
40|$|The Sewol ferry {{disaster}} severely shocked Korean society. The {{objective of}} this study was to explore how the public mood in Korea changed following the Sewol disaster using Twitter data. Data were collected from daily Twitter posts from 1 January 2011 to 31 December 2013 and from 1 March 2014 to 30 June 2014 using natural language-processing and text-mining technologies. We investigated the <b>emotional</b> <b>utterances</b> in reaction to the disaster by analyzing the appearance of keywords, the human-made disaster-related keywords and suicide-related keywords. This disaster elicited immediate emotional reactions from the public, including anger directed at various social and political events occurring in the aftermath of the disaster. We also found that although the frequency of Twitter keywords fluctuated greatly during the month after the Sewol disaster, keywords associated with suicide were common in the general population. Policy makers should recognize that both those directly affected and the general public still suffers from the effects of this traumatic event and its aftermath. The mood changes experienced by the general population should be monitored after a disaster, and social media data can be useful for this purpose...|$|R
40|$|International audienceThis paper {{presents}} algorithms {{that allow}} a robot to express its emotions by modulating the intonation of its voice. They are very simple and efficiently provide life-like speech {{thanks to the}} use of concatenative speech synthesis. We describe a technique which allows to continuously control both the age of a synthetic voice and the quantity of emotions that are expressed. Also, we present the first large-scale data mining experiment about the automatic recognition of basic emotions in informal everyday short utterances. We focus on the speaker-dependent problem. We compare a large set of machine learning algorithms, ranging from neural networks, Support Vector Machines or decision trees, together with 200 features, using a large database of several thousands examples. We show that the difference of performance among learning schemes can be substantial, and that some features which were previously unexplored are of crucial importance. An optimal feature set is derived {{through the use of a}} genetic algorithm. Finally, we explain how this study can be applied to real world situations in which very few examples are available. Furthermore, we describe a game to play with a personal robot which facilitates teaching of examples of <b>emotional</b> <b>utterances</b> in a natural and rather unconstrained manner. Keywords: Emotions; Speech; Robots; Emotion production; Emotion recognitio...|$|R
40|$|Concatenative speech {{synthesis}} is increasing in popularity, as it offers higher quality output than previous formant synthesisers. However, {{it is based}} on recorded speech units, concatenative synthesis offers a lesser degree of parametric control during resynthesis. Consequently, adding pragmatic effects such as different speaking styles and emotions at the synthesis stage is fundamentally more difficult than with formant synthesis. This paper describes the results of a preliminary attempt to add emotion to concatenative synthetic speech (using BT's Laureate synthesiser), initially using techniques already applied successfully to formant synthesis. A new intonation contour (including both pitch and duration changes) was applied to the concatenated segments during production of the final audible utterance, and some of the available synthesis parameters were systematically modified to increase the affective content. The output digital speech samples were then subject to further manipulation with a waveform editing package, to produce the final output utterance. The results of this process were a small number of manually-produced utterances, but which illustrated that affective manipulations were possible on this type of synthesiser. Further work has produced rule-based implementations which allow automatic production of <b>emotional</b> <b>utterances.</b> Development of these systems will be described, and some initial results from listener studies will be presented. 1. 1. Context 1...|$|R
40|$|In {{this article}} we report on two {{experiments}} about the perception of audiovisual cues to emotional speech. The article addresses two questions: (1) how do visual cues from a speaker's face to emotion relate to auditory cues, and (2) what is the recognition speed for various facial cues to emotion? Both experiments reported below are based on tests with video clips of <b>emotional</b> <b>utterances</b> collected via {{a variant of the}} well-known Velten method. More specifically, we recorded speakers who displayed positive or negative emotions, which were congruent or incongruent with the (emotional) lexical content of the uttered sentence. In order to test this, we conducted two experiments. The first experiment is a perception experiment in which Czech participants, who do not speak Dutch, rate the perceived emotional state of Dutch speakers in a bimodal (audiovisual) or a unimodal (audio- or vision-only) condition. It was found that incongruent emotional speech leads to significantly more extreme perceived emotion scores than congruent emotional speech, where the difference between congruent and incongruent emotional speech is larger for the negative than for the positive conditions. Interestingly, the largest overall differences between congruent and incongruent emotions were found for the audio-only condition, which suggests that posing an incongruent emotion has a particularly strong effect on the spoken realization of emotions...|$|R
40|$|This paper {{presents}} algorithms {{that allow}} a robot to express its emotions by modulating the intonation of its voice. They are very simple and efficiently provide life-like speech {{thanks to the}} use of concatenative speech synthesis. We describe a technique which allows to continuously control both the age of a synthetic voice and the quantity of emotions that are expressed. Also, we present the first large-scale data mining experiment about the automatic recognition of basic emotions in informal everyday short utterances. We focus on the speaker-dependent problem. We compare a large set of machine learning algorithms, ranging from neural networks, Support Vector Machines or decision trees, together with 200 features, using a large database of several thousands examples. We show that the difference of performance among learning schemes can be substantial, and that some features which were previously unexplored are of crucial importance. An optimal feature set is derived {{through the use of a}} genetic algorithm. Finally, we explain how this study can be applied to real world situations in which very few examples are available. Furthermore, we describe a game to play with a personal robot which facilitates teaching of examples of <b>emotional</b> <b>utterances</b> in a natural and rather unconstrained manner...|$|R
40|$|Vocal {{emotions}} are expressed either by speech or singing. The {{difference is that}} in singing the pitch is predetermined while in speech it may vary freely. It was of interest to study whether there were voice quality differences between freely varying and mono-pitched vowels expressed by professional actors. Given their profession, actors {{have to be able}} to express emotions both by speech and singing. Electroglottogram and acoustic analyses of <b>emotional</b> <b>utterances</b> embedded in expressions of freely varying vowels [a:], [i:], [u:] (96 samples) and mono-pitched protracted vowels (96 samples) were studied. Contact quotient (CQEGG) was calculated using 35 %, 55 %, and 80 % threshold levels. Three different threshold levels were used in order to evaluate their effects on emotions. Genders were studied separately. The results suggested significant gender differences for CQEGG 80 % threshold level. SPL, CQEGG, and F 4 were used to convey emotions, but to a lesser degree, when F 0 was predetermined. Moreover, females showed fewer significant variations than males. Both genders used more hypofunctional phonation type in mono-pitched utterances than in the expressions with freely varying pitch. The present material warrants further study of the interplay between CQEGG threshold levels and formant frequencies, and listening tests to investigate the perceptual value of the mono-pitched vowels in the communication of emotions. © 2014 Informa UK, Ltd...|$|R
40|$|The {{synthesis}} of emotional speech has wide {{applications in the}} field of human-computer interaction, medicine, industry and so on. In this work, an emotional speech synthesis system is proposed based on prosodic features modification and Time Domain Pitch Synchronous OverLap Add (TD-PSOLA) waveform concatenative algorithm. The system produces synthesized speech with four types of emotion: angry, happy, sad and bored. The experiment results show that the proposed emotional speech synthesis system achieves a good performance. The produced <b>utterances</b> present clear <b>emotional</b> expression. The subjective test reaches high classification accuracy for different types of synthesized <b>emotional</b> speech <b>utterances...</b>|$|R
