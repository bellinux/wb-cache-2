20|206|Public
40|$|Abstract: The paper {{presents}} {{a prototype of}} a biometric system based on human iris code matching. The system consists of the image acquisition, iris features <b>extraction,</b> <b>code</b> determination and the code-matching algorithm. A fast method of coding the iris features, based on Zak’s transform, is introduced. Performance of the system has been verified {{with the use of}} a proprietary database of human iris images...|$|E
40|$|Eilenberg {{machines}} {{have been}} introduced in 1974 {{in the field of}} formal language theory. They are finite automata for which the alphabet is interpreted by mathematical relations over an abstract set. They generalize many finite state machines. We consider in the present work a class of Eilenberg machines for which we provide an executable complete simulator. This program is described using the Coq specification language. The correction of the algorithm is also proved formally and mechanically verified using the Coq proof assistant. The Coq <b>extraction</b> <b>code</b> technology allows to translate the specification into executable OCaml code. The algorithm and proofs are inspired from the reactive engine of Gérard Huet...|$|E
40|$|This paper {{describes}} pycdec, a Python module for the cdec decoder. It enables Python code to use cdec 2 ̆ 7 s fast C++ {{implementation of}} core finite-state and context-free inference algorithms for decoding and alignment. The high-level interface allows developers to build integrated MT applications that {{take advantage of}} the rich Python ecosystem without sacrificing computational performance. We give examples of how to interact directly with the main cdec data structures (lattices, hypergraphs, sparse feature vectors), evaluate translation quality, and use the suffix-array grammar <b>extraction</b> <b>code.</b> This permits rapid prototyping of new algorithms for training, data visualization, and utilizing MT and related structured prediction tasks...|$|E
40|$|I {{present a}} brief {{overview}} of a variety of computational tools for supersymmetry calculations, including: spectrum generators, cross section and branching fraction calculators, low energy constraints, general purpose event generators, matrix element event generators, SUSY dark matter <b>codes,</b> parameter <b>extraction</b> <b>codes</b> and Les Houches interface tools. Comment: Chapter to appear in Perspectives on Supersymmetry, edited by G. Kane; 23 pages including one. eps figur...|$|R
30|$|The {{main problem}} with the BoVW {{approach}} is that the feature extraction, feature encoding, and classification are three separate problems. In order to counteract this problem, {{the size of the}} dictionary is increased to better divide the feature space, in some cases reaching hundreds of thousands of visual words. We created a neural network that is able to perform the same function as the BoVW but is able to learn jointly the feature <b>extraction,</b> <b>coding,</b> and classification functions.|$|R
30|$|Note {{that still}} {{we have the}} the {{permutation}} ambiguity issue on the estimation, i.e., unknown Δ. Since the user (column) identification along with timing and frequency estimation should be performed accordingly in the ranging process, therefore {{it is crucial to}} identify each column correctly and to remove the permutation ambiguity clearly. This can be done by the proposed <b>code</b> <b>extraction</b> and <b>code</b> detection process.|$|R
40|$|Abstract: We {{propose a}} {{binarization}} method based pigment in the ZIP code of 24 bmp image simulation and digital identification by CCD sensors, were extracted the grid binary image of zip code box and {{message of the}} two characters binary image; analyze the image processing, which includes code frame edge detection and separation of the image binarization, denoising smoothing, tilt correction, the <b>extraction</b> <b>code</b> number, position, normalization processing, digital image thinning, character recognition feature extraction. Through testing, the recognition rate of this method can be over 90 %. The recognition time of characters for character is less than 1. 3 second, which means the method is of more effective recognition ability and can better satisfy the real system requirements...|$|E
40|$|The {{objective}} of this work was to support the design of improved IUE NEWSIPS high dispersion extraction algorithms. The purpose of this work was to evaluate use of the Linearized Image (LIHI) file versus the Re-Sampled Image (SIHI) file, evaluate various extraction, and design algorithms for evaluation of IUE High Dispersion spectra. It was concluded {{the use of the}} Re-Sampled Image (SIHI) file was acceptable. Since the Gaussian profile worked well for the core and the Lorentzian profile worked well for the wings, the Voigt profile was chosen for use in the extraction algorithm. It was found that the gamma and sigma parameters varied significantly across the detector, so gamma and sigma masks for the SWP detector were developed. <b>Extraction</b> <b>code</b> was written...|$|E
40|$|In {{this paper}} {{we present a}} new {{algorithm}} for accelerating the potential calculation which occurs in the inner loop of iterative algorithms for solving electromagnetic boundary integral equations. Such integral equations arise, for example, in the extraction of coupling capacitances in three-dimensional (3 -D) geometries. We present extensive experimental comparisons with the capacitance <b>extraction</b> <b>code</b> FASTCAP [1] and demonstrate that, {{for a wide variety}} of geometries commonly encountered in integrated circuit packaging, on-chip interconnect and micro-electro-mechanical systems, the new "precorrectedFFT " algorithm is superior to the fast multipole algorithm used in FASTCAP in terms of execution time and memory use. At engineering accuracies, in terms of a speed-memory product, the new algorithm can be superior to the fast multipole based schemes by more than an order of magnitude...|$|E
5000|$|As {{of release}} 10gR2, Oracle Data Mining {{contains}} built-in SQL functions for scoring data mining models. These single-row functions support classification, regression, anomaly detection, clustering, and feature <b>extraction.</b> The <b>code</b> below illustrates a typical usage of a classification model: ...|$|R
40|$|PrimeArray is a Windows {{program that}} computes oligonuceotide primer pairs for genome-scale gene {{amplification}} by the Polymerase Chain Reaction (PCR). The program supports the automated <b>extraction</b> of <b>coding</b> sequences (CDS) from various input-file formats and allows highly automated primer pair-optimization...|$|R
40|$|Research in {{the area}} of {{cochlear}} prostheses to restore a level of hearing sensation to the profoundly deaf has been ongoing at a number of centers throughout the world since the 1960 's. 3, 4, 7, 8,. Work on a multichannel cochlear implant that utilizes a speech feature <b>extraction</b> <b>coding</b> strategy and multi-sited, sequential, bipolar stimulation to enhance pitch perception began at the University of Melbourne under the direction of Professor Graeme Clark in the 1970 's. Collaboration with Nucleus Limited, a multi-national biomedical corporation from Australia, {{led to the development of}} the current version of the prosthesis. The Nucleus 22 Channel Cochlear Implant System has been described in detail elsewhere. 1, 57 - 12 SeptemberOpen Acces...|$|R
40|$|This paper {{presents}} an original {{approach to the}} problem of camera calibration using a calibration pattern. It consists of directly searching for the camera parameters that best project three-dimensional points of a calibration pattern onto intensity edges in an image of this pattern, without explicitly extracting the edges. Based on a characterization of image edges as maxima of the intensity gradient or zero-crossings of the Laplacian, we express the whole calibration process as a one-stage optimization problem. A classical iterative optimization technique is used in order to solve it. Contrary to classical calibration techniques which involve two consecutive stages (extraction of image features and computation of the camera parameters), our approach does not require any customized feature <b>extraction</b> <b>code.</b> As a consequence, it can be directly used with any calibration pattern that produces image edges, and it is also more robust. First, we describe the details of the approach. The [...] ...|$|E
40|$|Abstract. We {{present the}} {{wavelength}} solution derived for the G 800 l grism with the Wide Field Channel from the spectra of two Galactic Wolf-Rayet stars, WR 45 and WR 96. The data were acquired in-orbit during the SMOV tests {{and the early}} INTERIM calibration program. We have obtained an average dispersion of 39. 2 ˚A/pix in the first order, 20. 5 ˚A/pix in the second and- 42. 5 ˚A/pix in the negative first order. We show that the wavelength solution is strongly field-dependent, with an amplitude of the variation of about 11 % {{from the center of}} the WFC aperture to the corners. The direction of the field-dependence is the diagonal from the image left top corner (amplifier A) to the bottom right corner (amplifier D). These trends are observed for all grism orders. We also describe the calibration files derived from the SMOV and INTERIM data which are used by the ST-ECF slitless <b>extraction</b> <b>code</b> aXe. 1...|$|E
40|$|In this paper, {{we provide}} a brief {{overview}} of the OASIS system, and then describe our recent successes in integrating with and using rover hardware. OASIS currently works in a closed loop fashion with onboard control software (e. g., navigation and vision) and has the ability to autonomously perform the following sequence of steps: analyze gray scale images to find rocks, extract the properties of the rocks, identify rocks of interest, retask the rover to take additional imagery of the identified target and then allow the rover to continue on its original mission. We also describe the early 2004 ground test validation of specific OASIS components on selected Mars Exploration Rover (MER) images. These components include the rockfinding algorithm, RockIT, and the rock size feature <b>extraction</b> <b>code.</b> Our team also developed the RockIT GUI, an interface that allows users to easily visualize and modify the rock-finder results. This interface has allowed us to conduct preliminary testing and validation of the rockfinder's performance...|$|E
40|$|Abstract. Feature <b>extraction,</b> <b>coding</b> and pooling, are {{important}} components on many contemporary object recognition paradigms. In this {{paper we explore}} novel pooling techniques that encode the second-order statistics of local descriptors inside a region. To achieve this effect, we introduce multiplicative second-order analogues of average and maxpooling that together with appropriate non-linearities lead to state-ofthe-art performance on free-form region recognition, without any type of feature coding. Instead of coding, we found that enriching local descriptors with additional image information leads to large performance gains, especially {{in conjunction with the}} proposed pooling methodology. We show that second-order pooling over free-form regions produces results superior to those of the winning systems in the Pascal VOC 2011 semantic segmentation challenge, with models that are 20, 000 times faster...|$|R
40|$|Designing a {{suitable}} image representation {{is one of}} the most fundamental issues of computer vision. There are three steps in the popular Bag of Words based image representation: feature <b>extraction,</b> <b>coding</b> and pooling. In the final step, current methods make an M × K encoded feature matrix degraded to a K-dimensional vector (histogram), where M is the number of features, and K is the size of the codebook: information is lost dramat-ically here. In this paper, a novel pooling method, based on 2 -D histogram representation, is proposed to retain more information from the encoded image features. This pooling method can be easily incorporated into state-of-the-art computer vision system frameworks. Experiments show that our approach improves current pooling methods, and can achieve satisfactory performance of image classification and image reranking even when using a small codebook and costless linear SVM...|$|R
30|$|In this section, we {{describe}} experimental results for determining whether <b>code</b> <b>extraction</b> schemes {{can be used}} successfully for analyzing malware samples that apply Bangcle and DexProtector.|$|R
40|$|We {{present the}} {{wavelength}} solution derived for the G 800 L grism with the Wide Field Channel from the spectra of two Galactic Wolf-Rayet stars, WR 45 and WR 96. The data were acquired in-orbit during the SMOV tests {{and the early}} INTERIM calibration program. We have obtained an average dispersion of 39. 2 A/pix in the first order, 20. 5 A/pix {{in the second and}} - 42. 5 A/pix in the negative first order. We show that the wavelength solution is strongly field-dependent, with an amplitude of the variation of about 11 % {{from the center of the}} WFC aperture to the corners. The direction of the field-dependence is the diagonal from the image left top corner (amplifier A) to the bottom right corner (amplifier D). These trends are observed for all grism orders. We also describe the calibration files derived from the SMOV and INTERIM data which are used by the ST-ECF slitless <b>extraction</b> <b>code</b> aXe. Comment: Proceeding for The 2002 HST Calibration Workshop, 9 page...|$|E
40|$|Cosmological probes {{come in a}} large {{variety of}} forms and unveil many {{characteristics}} of the Universe. The extraction of cosmological parameters from probes, such as the cosmic microwave background, is done by fitting predictions of cosmological models to data. This {{requires the use of}} Boltzmann codes that possess powerful and efficient mechanisms for producing cosmological predictions. In this thesis the cosmological parameter <b>extraction</b> <b>code</b> CLASS is used to constrain primarily the energy densities and the Hubble parameter of the Universe. Four independent datasets that together probe the Universe at a large range of scales are used for this purpose. The data analysis is done within the framework of two different LCDM cosmological models. A stepwise description of a data analysis within the framework of a LWDM model is also provided. It is shown how combining independent datasets can break degeneracies between cosmological parameters, which is necessary to explore signatures of beyond standard model physics. Furthermore, a consistency check between CLASS and a second, independent Boltzmann code is performed and discussed...|$|E
40|$|We {{investigate}} {{star formation}} and dust heating in the compact far-infrared (FIR) bright sources {{detected in the}} Herschel maps of M 83. We use the source <b>extraction</b> <b>code</b> getsources to detect and extract sources in the FIR, {{as well as their}} photometry in the mid-infrared and H alpha. By performing infrared spectral energy distribution fitting and applying an H alpha-based star formation rate (SFR) calibration, we derive the dust masses and temperatures, SFRs, gas masses and star formation efficiencies (SFEs). The detected sources lie exclusively on the spiral arms and represent giant molecular associations, with gas masses and sizes of 10 (6) - 10 (8) M-circle dot and 200 - 300 pc, respectively. The inferred parameters show little to no radial dependence and there is only a weak correlation between the SFRs and gas masses, which suggests that more massive clouds are less efficient at forming stars. Dust heating is mainly due to local star formation. However, although the sources are not optically thick, the total intrinsic young stellar population luminosity can almost completely account for the dust luminosity. This suggests that other radiation sources also contribute to the dust heating and approximately compensate for the unabsorbed fraction of UV light...|$|E
40|$|Summary: PrimeArray is a Windows {{program that}} computes oligonuceotide primer pairs for genome-scale gene {{amplification}} by the Polymerase Chain Reaction (PCR). The program supports the automated <b>extraction</b> of <b>coding</b> sequences (CDS) from various input-file formats and allows highly automated primer pair-optimization. Availability: The program is freely available for non-profit use via {{request from the}} authors. Contact: christoph. dehio@unibas. c...|$|R
40|$|An {{image is}} inevitably {{corrupted}} by noise in its acquisition or transmission. Noises in an image can degrade severely the follow-up image processing tasks, such as image feature <b>extraction,</b> <b>coding,</b> segmentation, and target detection. Thus noise reduction becomes {{a very important}} image pre-processing for {{improving the quality of}} image and meeting the needs of higher level processing tasks. In this paper we develop an algorithm for three different denoising methods: Median filter, wavelet domain based denoising and approximation part obtained by wavelet domain method is again filtered by median filter to get better denoised result. Image is considered as noised by Gaussian, speckle and salt & pepper noises. The results shows that if the image are affected by Gaussian and speckle noise then the method in which approximation part is filtered by median filter will perform better than individual median and wavelet domain method...|$|R
40|$|Abstract: To {{investigate}} the clinical utility of intra operative plain radiographs in cochlear implant surgery. Eighty consecutive adult and pediatric cochlear implant operations at a facility capable of intra operative radiographs were evaluated over 06 months. A carefully designed study {{to evaluate the}} performance of individuals who received the Nucleus 22 -channel cochlear implant. All patients were profound-totally deaf, adults with a post lingual onset of impairment. The preoperative evaluation, prosthesis fitting, training, and postoperative testing were consistent across clinics. Single- subject studies, where each patient acted as his/her own control, revealed that of the 80 subjects, 16 – 24 obtained significant improvement (P~ 0. 001) on unpracticed, unfamiliar recorded speech tests from the Minimal Auditory Capabilities (MAC) Battery, when using hearing alone (no lip-reading). In addition, virtually all patients showed improvement in recognition of speech material with lip-reading. The data support the efficacy of a feature <b>extraction</b> <b>coding</b> system where specific formant and amplitude information are transmitted via direct electrical stimulation to the cochlea. I...|$|R
40|$|We {{present a}} multi-scale, multi-wavelength source {{extraction}} algorithm called getsources. Although {{it has been}} designed primarily {{for use in the}} far-infrared surveys of Galactic star-forming regions with Herschel, the method can be applied to many other astronomical images. Instead of the traditional approach of extracting sources in the observed images, the new method analyzes fine spatial decompositions of original images across a wide range of scales and across all wavebands. It cleans those single-scale images of noise and background, and constructs wavelength-independent single-scale detection images that preserve information in both spatial and wavelength dimensions. Sources are detected in the combined detection images by following the evolution of their segmentation masks across all spatial scales. Measurements of the source properties are done in the original background-subtracted images at each wavelength; the background is estimated by interpolation under the source footprints and overlapping sources are deblended in an iterative procedure. In addition to the main catalog of sources, various catalogs and images are produced that aid scientific exploitation of the extraction results. We illustrate the performance of getsources on Herschel images by extracting sources in sub-fields of the Aquila and Rosette star-forming regions. The source <b>extraction</b> <b>code</b> and validation images with a reference extraction catalog are freely available. Comment: 31 pages, 27 figures, to be published in Astronomy & Astrophysic...|$|E
40|$|At CERN, a high {{performance}} negative ion (NI) source {{is required for}} the 160 MeV H- linear accelerator Linac 4. The source is planned to produce 80 mA of H- with an emittance of 0. 25 mm mradN-RMS which is technically and scientifically very challenging. The optimization of the NI source requires a deep understanding of the underling physics concerning the production and extraction of the negative ions. The extraction mechanism from the negative ion source is complex involving a magnetic filter in order to cool down electrons’ temperature. The ONIX (Orsay Negative Ion <b>eXtraction)</b> <b>code</b> is used to address this problem. The ONIX is a selfconsistent 3 D electrostatic code using Particles-in-Cell Monte Carlo Collisions (PIC-MCC) approach. It was written to handle the complex boundary conditions between plasma, source walls, and beam formation at the extraction hole. Both, the positive extraction potential (25 kV) and the magnetic field map are taken from the experimental set-up, in construction at CERN. This contribution focuses on the modeling of two different extractors (IS 01, IS 02) of the Linac 4 ion sources. The most efficient extraction system is analyzed via numerical parametric studies. The influence of aperture’s geometry and the strength of the magnetic filter field on the extracted electron and NI current will be discussed. The NI production of sources based on volume extraction and cesiated surface are also compared...|$|E
40|$|Far-infrared imaging {{surveys of}} Galactic star-forming regions with Herschel {{have shown that}} a {{substantial}} part of the cold interstellar medium appears as a fascinating web of omnipresent filamentary structures. This highly anisotropic ingredient of the interstellar material further complicates the difficult problem of the systematic detection and measurement of dense cores in the strongly variable but (relatively) isotropic backgrounds. Observational evidence that stars form in dense filaments creates severe problems for automated source extraction methods that must reliably distinguish sources not only from fluctuating backgrounds and noise, but also from the filamentary structures. A previous paper presented the multi-scale, multi-wavelength source extraction method getsources based on a fine spatial scale decomposition and filtering of irrelevant scales from images. Although getsources has performed very well in benchmarks, strong unresolved filamentary structures caused difficulties for reliable source extraction. In this paper, a multi-scale, multi-wavelength filament extraction method getfilaments is presented that solves this problem, substantially improving the robustness of source extraction with getsources in filamentary backgrounds. The main difference is that the filaments extracted by getfilaments are now subtracted by getsources from detection images during source extraction, greatly reducing the chances of contaminating catalogs with spurious sources. The getfilaments method shares its general philosophy and approach with getsources, and it {{is an integral part of}} the source <b>extraction</b> <b>code.</b> The intimate physical relationship between forming stars and filaments seen in Herschel observations demands that accurate filament extraction methods must remove the contribution of sources and that accurate sourc...|$|E
40|$|Mapping studies {{describe}} a broad body of literature, and differ from classical systematic reviews, which assess more narrowly-defined questions {{and evaluate the}} quality of the studies included in the review. While the steps involved in mapping studies have been described previously, a detailed qualitative account of the methodology could inform the design of future mapping studies.; Describe the perspectives of a large research team on the methods used and collaborative experiences in a study that mapped the literature published on maternal health interventions in low- and middle-income countries (2292 full text articles included, after screening 35, 048 titles and abstracts in duplicate).; Fifteen members of the mapping team, drawn from eight countries, provided their experiences and perspectives of the study in response to a list of questions and probes. The responses were collated and analysed thematically following a grounded theory approach.; The objectives of the mapping evolved over time, posing difficulties in ensuring a uniform understanding of the purpose of the mapping among the team members. Ambiguity of some study variables and modifications in data <b>extraction</b> <b>codes</b> were the main threats to the quality of data extraction. The desire for obtaining detailed information on a few topics needed to be weighed against the benefits of collecting more superficial data on a wider range of topics. Team members acquired skills in systematic review methodology and software, and a broad knowledge of maternal health literature. Participation in analysis and dissemination was lower than during the screening of articles for eligibility and data coding. Though all respondents believed the workload involved was high, study outputs were viewed as novel and important contributions to evidence. Overall, most believed there was a favourable balance between the amount of work done and the project's outputs.; A large mapping of literature is feasible with a committed team aiming to build their research capacity, and with a limited, simplified set of data <b>extraction</b> <b>codes.</b> In the team's view, the balance between the time spent on the review, and the outputs and skills acquired was favourable. Assessments of the value of a mapping need, however, {{to take into account the}} limitations inherent in such exercises, especially the exclusion of grey literature and of assessments of {{the quality of the}} studies identified...|$|R
30|$|After extracting user information, {{threads of}} question/answer can be extracted. It was {{necessary}} to extract 2.5 million threads of question/answer because Java forum does not allow access to a specific user's posts. Therefore, first, all threads of question/answer must be evacuated and then those posts related to specific user can be extracted from this threads. In posts <b>extraction,</b> source <b>code</b> and other quotes should be removed.|$|R
5000|$|Mission: Impossible - Ghost Protocol - Josh Holloway's {{character}} Hanaway's weaponized [...] "class ring" [...] has A113 {{emblazoned on}} the side. It is Tom Cruise's character Ethan Hunt's <b>extraction</b> access <b>code</b> given {{over the phone}} {{and is on the}} plate of a car in front of the Kremlin during the big explosion, time at which the bomb's deactivation button is pushed [...] "1.13 seconds".|$|R
40|$|Recent {{work has}} {{examined}} the design of wireless sensor network (WSN) systems for structural health monitoring (SHM). Wireless sensors enable dense monitoring of large physical structures and promise enormous ease and flexibility of deployment of instrumentation, as well as low maintenance and deployment costs. However, programming sensing applications on a network of wireless sensors remains a difficult and time-consuming endeavor. This {{is due in part}} to the complexity of such systems. Their limited battery resources, and the highly variable performance of wireless communication in different environments represent significant constraints that, if each application developer were forced to deal with, can significantly increase the time to develop robust applications. We have been developing a networked software system called TENET that simplifies the programming of wireless sensor actuator systems. A TENET system is a two-tier networked system consisting of two classes of nodes: a higher-tier with several nodes containing 32 -bit processors and IEEE 802. 11 b radios, and a lower-tier comprising battery-operated sensor nodes with less-capable processors, low-power radios. Our TENET software runs application code on the higher-tier nodes, and provides a generic interface for tasking sensors and actuators. This separation of functionality simplifies application development greatly, since developers can reuse networking and sensor data <b>extraction</b> <b>code,</b> thereby reducing application development time. We will report on the development of and experiences with structural data acquisition application for a long-span suspension bridge using TENET. We will report on our experiences in deploying a two-tier network of wireless sensors on the bridge. We will report on the performance of the TENET system in this setting as well...|$|E
40|$|International audienceFar-infrared imaging {{surveys of}} Galactic star-forming regions with Herschel {{have shown that}} a {{substantial}} part of the cold interstellar medium appears as a fascinating web of omnipresent filamentary structures. This highly anisotropic ingredient of the interstellar material further complicates the difficult problem of the systematic detection and measurement of dense cores in the strongly variable but (relatively) isotropic backgrounds. Observational evidence that stars form in dense filaments creates severe problems for automated source extraction methods that must reliably distinguish sources not only from fluctuating backgrounds and noise, but also from the filamentary structures. A previous paper presented the multi-scale, multi-wavelength source extraction method getsources based on a fine spatial scale decomposition and filtering of irrelevant scales from images. Although getsources performed very well in benchmarks, strong unresolved filamentary structures caused difficulties for reliable source extraction. In this paper, a multi-scale, multi-wavelength filament extraction method getfilaments is presented that solves this problem, substantially improving the robustness of source extraction with getsources in filamentary backgrounds. The main difference is that the filaments extracted by getfilaments are now subtracted by getsources from detection images during source extraction, greatly reducing the chances of contaminating catalogs with spurious sources. The getfilaments method shares its general philosophy and approach with getsources, and it {{is an integral part of}} the source <b>extraction</b> <b>code.</b> The intimate physical relationship between forming stars and filaments seen in Herschel observations demands that accurate filament extraction methods must remove the contribution of sources and that accurate source extraction methods must be able to remove underlying filamentary structures. Source extraction with the new version of getsources provides researchers not only with the catalogs of sources, but also with clean images of filamentary structures, free of sources, noise, and isotropic backgrounds...|$|E
40|$|This thesis {{takes place}} within the SuperNova factory experiment, which {{includes}} about 20 french and american searchers. The purpose {{is to improve the}} usage of type Ia supernovae in cosmology, both by improving our understanding of supernovae diversity and by increasing statistics for low redshift (between 0. 03 and 0. 08). After a brief introduction to these aspects, I focused more specifically the SNfactory experiment, which includes two components : a search of supernovae with data from QUEST-II camera on a telescope on the Mont Palomar (California) and a follow-up with the SNIFS instrument, specifically designed for this task and mounted permanently on the University of Hawaii telescope on the Mauna Kea. My task concerned SNIFS spectroscopic channels : a simulation of a datacube and a spectrum extraction method are thus presented here, with a discussion about the PSF to choose. The goal of 1 % precision in flux calibration in photometric nights seems to be reachable with my <b>extraction</b> <b>code.</b> I finally presented an analysis of 10 SNIFS supernovae, including a measurement of expanding velocity evaluated from blueshift of 4 spectrum features with a new method I designed which is more robust and reproducible method. These measures point out that 20 % of the sample is clearly different from the average behaviour. Cette these s'inscrit dans le cadre de l'experience SuperNova factory, une collaboration franco-americaine associant une vingtaine de chercheurs. Le but de cette experience est d'ameliorer l'utilisation des supernovae de type Ia en cosmologie en affinant notre comprehension de la diversite des SNIa et en augmentant la statistique disponible a bas redshift (entre 0. 03 et 0. 08). Apres avoir presente ces aspects, j'ai presente plus specifiquement l'experience SNfactory. Cette experience se compose de deux versants : l'un d'eux est la recherche de supernovae avec les donnees de la camera QUEST-II installee sur un telescope du Mont Palomar (Californie) et l'autre est le suivi grace a un instrument dedie nomme SNIFS installe en permanence sur le telescope de l'Universite de Hawaii au sommet du Mauna Kea. Mon travail a porte sur les voies spectroscopiques de SNIFS : une simulation d'un cube de donnees et une methode d'extraction de spectres par minimisation des moindres carres sont presentees ici, ainsi qu'une discussion sur le choix de la PSF a adopter. L'objectif de precision de la calibration en flux de 1 % lors de nuits photometriques peut etre atteint a l'aide d'une telle extraction. J'ai enfin presente une analyse d'un echantillon de dix supernovae, notamment en mesurant la vitesse d'expansion calculee a partir du decalage vers le bleu de 4 raies du spectre a l'aide d'une methode novatrice. Ces mesures montrent que 20 % de l'echantillon presente un comportement notablement different du comportement moyen...|$|E
40|$|Background: Mapping studies {{describe}} a broad body of literature, and differ from classical systematic reviews, which assess more narrowly-defined questions {{and evaluate the}} quality of the studies included in the review. While the steps involved in mapping studies have been described previously, a detailed qualitative account of the methodology could inform the design of future mapping studies. Objectives: Describe the perspectives of a large research team on the methods used and collaborative experiences in a study that mapped the literature published on maternal health interventions in low- and middle-income countries (2292 full text articles included, after screening 35, 048 titles and abstracts in duplicate). Methods: Fifteen members of the mapping team, drawn from eight countries, provided their experiences and perspectives of the study in response to a list of questions and probes. The responses were collated and analysed thematically following a grounded theory approach. Results: The objectives of the mapping evolved over time, posing difficulties in ensuring a uniform understanding of the purpose of the mapping among the team members. Ambiguity of some study variables and modifications in data <b>extraction</b> <b>codes</b> were the main threats to the quality of data extraction. The desire for obtaining detailed information on a few topics needed to be weighed against the benefits of collecting more superficial data on a wider range of topics. Team members acquired skills in systematic review methodology and software, and a broad knowledge of maternal health literature. Participation in analysis and dissemination was lower than during the screening of articles for eligibility and data coding. Though all respondents believed the workload involved was high, study outputs were viewed as novel and important contributions to evidence. Overall, most believed there was a favourable balance between the amount of work done and the project’s outputs. Conclusions: A large mapping of literature is feasible with a committed team aiming to build their research capacity, and with a limited, simplified set of data <b>extraction</b> <b>codes.</b> In the team’s view, the balance between the time spent on the review, and the outputs and skills acquired was favourable. Assessments of the value of a mapping need, however, {{to take into account the}} limitations inherent in such exercises, especially the exclusion of grey literature and of assessments of {{the quality of the}} studies identified...|$|R
40|$|In {{this paper}} a new {{automatic}} approach to road extraction from aerial images is proposed. The initialization strategies {{are based on}} the intensity, color, and Hough transform. After road elements <b>extraction,</b> chain <b>codes</b> are calculated. In the last step, using shadow, cars on the roads are detected. We implemented our method on the 25 images from "Google Earth" database. The experiments show an increase in both the completeness and the quality indexes for the extracted road...|$|R
40|$|Abstract- We have {{presented}} a desktop image compression algorithm for real-time {{applications such as}} remote desktop access by desktop image transmission. The desktop image is called as a complex image, because one 800 X 600 true color image has a size of approximately 1. 54 MB with pictures and text. The algorithm is called as group <b>extraction</b> and <b>coding</b> (GEC). Real-time image transmission requires that the compression algorithm should not only achieve high compression ratio, but also have excellent visual quality and low complexity. In the extraction/ separation phase of the complex desktop image, we have separated the blocks into picture and text/graphics blocks by thresholding the number of colors contained in each block. Text/graphics block classes have been compressed by using a wavelet based SPIHT lossless coding algorithm, while picture block classes, by JPEG algorithm. The compressed blocks have been combined to form a single bit stream. Experimental results {{have shown that the}} GEC has very low complexity and provides visually excellent lossless quality with very good compression ratios. Index Terms — GEC-group <b>extraction</b> and <b>coding,</b> wavelet based SPIHT coding, complex image group extraction, Complex image compression. I...|$|R
