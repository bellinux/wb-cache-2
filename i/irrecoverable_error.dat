5|9|Public
40|$|We {{present a}} pumping lemma {{for the class}} of epsilon-contractions of pushdown graphs of level n, for each n. A pumping lemma was {{proposed}} by Blumensath, {{but there is an}} <b>irrecoverable</b> <b>error</b> in his proof; we present a new proof. Our pumping lemma also improves the bounds given in the invalid paper of Blumensath...|$|E
40|$|We discuss an {{automatic}} method for constructing high-performance preclassification trees. The role of preclassifiers is to prune {{the number of}} classes to {{a fraction of the}} total (by contrast, classifiers pick exactly one class). Decision trees make fast preclassifiers; if they also prune strongly, they can increase speed of the overall system significantly when used with classifiers whose runtime increases with the number of classes. It is an <b>irrecoverable</b> <b>error</b> if the true class is pruned: unfortunately, such errors accumulate rapidly in decision trees. We attack this problem through massive simulation of precisely defined classification problems using a pseudo-random generator of labeled samples. We first build a tree using greedy entropy minimization, using as many training samples (e. g. 1. 1 M) as computing resources permit. This tree is then "populated" with many more samples (e. g. 180 M), driving the error rate down to an acceptable level. We present statistically rigorous stopping [...] ...|$|E
40|$|Transactional memory {{allows the}} user to declare {{sequences}} of instructions as speculative transactions that can either commit or abort. If a transaction commits, {{it appears to be}} executed sequentially, so that the committed transactions constitute a correct sequential execution. If a transaction aborts, none of its instructions can affect other transactions. The popular criterion of opacity requires that the views of aborted transactions must also be consistent with the global sequential order constituted by committed ones. This is believed to be important, since inconsistencies observed by an aborted transaction may cause a fatal <b>irrecoverable</b> <b>error</b> or waste of the system in an infinite loop. Intuitively, an opaque implementation must ensure that no intermediate view a transaction obtains before it commits or aborts can be affected by a transaction that has not started committing yet, so called deferred-update semantics. In this paper, we intend to grasp this intuition formally. We propose a variant of opacity that explicitly requires the sequential order to respect the deferred-update semantics. Unlike opacity, our property also ensures that a serialization of a history implies serializations of its prefixes. Finally, we show that our property is equivalent to opacity if we assume that no two transactions commit identical values on the same variable, and present a counter-example for scenarios when the “unique-write ” assumption does not hold. ...|$|E
5000|$|A kernel panic is used {{primarily}} by Unix and Unix-like operating systems: the Unix equivalent of Microsoft's Blue Screen of Death. It is a routine called when the kernel detects <b>irrecoverable</b> <b>errors</b> in runtime correctness; in other words, when continued operation may risk escalating system instability, {{and a system}} reboot is easier than attempted recovery.|$|R
40|$|Abstract—We {{investigate}} {{the impact of}} irrecoverable read errors—also known as bad blocks—on the MTTDL of mirrored disks, RAID level 5 arrays and RAID level 6 arrays. Our study {{is based on the}} data collected by Bairavasundaram et al. from a population of 1. 53 million disks over a period of 32 months. Our study indicates that <b>irrecoverable</b> read <b>errors</b> can reduce the mean time to data loss (MTTDL) of the three arrays by up to 99 percent, effectively canceling most of the benefits of fast disk repairs. It also shows the benefits of frequent scrubbing scans that map out bad blocks thus preventing future <b>irrecoverable</b> read <b>errors.</b> As an example, once-a-month scrubbing scans were found to improve the MTTDL of the three arrays by at least 300 percent compared to once-a-year scrubbing scans. Keywords-disk arrays; mirrored disks; RAID arrays. I...|$|R
40|$|Global {{optimization}} algorithms {{have shown}} impressive performance in data-association based multi-object tracking, but handling online data remains a difficult hurdle to overcome. In this paper, {{we present a}} hybrid data association framework with a min-cost multi-commodity network flow for robust online multi-object tracking. We build local target-specific models interleaved with global optimization of the optimal data association over multiple video frames. More specifically, in the min-cost multi-commodity network flow, the target-specific similarities are online learned to enforce the local consistency for reducing {{the complexity of the}} global data association. Meanwhile, the global data association taking multiple video frames into account alleviates <b>irrecoverable</b> <b>errors</b> caused by the local data association between adjacent frames. To ensure the efficiency of online tracking, we give an efficient near-optimal solution to the proposed min-cost multi-commodity flow problem, and provide the empirical proof of its sub-optimality. The comprehensive experiments on real data demonstrate the superior tracking performance of our approach in various challenging situations...|$|R
40|$|Contemporary {{and future}} speech {{telecommunication}} systems now utilise {{low bit rate}} (LBR) speech coding techniques in efforts to eliminate bandwidth expansion as a disadvantage of digital coding and transmission. These speech coders employ model-based approaches in compressing human speech {{into a number of}} parameters, using a well-known process known as linear predictive coding (LPC). However, a major side-effect observed in these coders is that errors in the model parameters have noticeable and undesirable consequences on the synthesised speech quality, and unless they are protected from such corruptions, the level of service quality will deteriorate rapidly. Traditionally, forward error correction (FEC) coding is used to remove these errors, but these require substantial redundancy. Therefore, a different perspective of the error control problems and solutions is necessary. In this thesis, emphasis is constantly placed on exploiting the constraints and residual redundancies present in the model parameters. It is also shown that with such source criteria in the LBR speech coders, varying degrees of error protection from channel corruptions are feasible. From these observations, error control requirements and methodologies, using both block- and parameter-orientated aspects, are analysed, devised and implemented. It is evident, that under the unusual circumstances which LBR speech coders have to operate in, the importance and significance of source reliant error control will continue to attract research and commercial interests. The work detailed in this thesis is focused on two LPC-based speech coders. One of the ideas developed for these two coders is an advanced zero redundancy scheme for the LPC parameters which is designed to operate at high channel error rates. Another concept proposed here is the use of source criteria to enhance the decoding capabilities of FEC codes to exceed that of maximum likelihood decoding performance. Lastly, for practical operation of LBR speech coders, lost frame recovery strategies are viewed to be an indispensable part of error control. This topic is scrutinised in this thesis by investigating the behaviour of a specific speech coder under <b>irrecoverable</b> <b>error</b> conditions. In all of the ideas pursued above, the effectiveness of the algorithms formulated here are quantified using both objective and subjective tests. Consequently, the capabilities of the techniques devised in this thesis can be demonstrated, examples of which are: (1) higher speech quality produced under noisy channels, using an improved zero-redundancy algorithm for the LPC filter coefficients; (2) as much as 50 % improvement in the residual BER and decoding failures of FEC schemes, through the utilisation of source criteria in LBR speech coders; and (3) acceptable speech quality produced under high frame loss rates (14 %), after formulating effective strategies for recovery of speech coder parameters. It is hoped that the material described here provide concepts which can help achieve the ideals of maximum efficiency and quality in LBR speech telecommunications...|$|E
40|$|Abstract—Mobile {{clinical}} applications {{designed for}} {{electronic medical record}} exchange depends on messaging protocols for reliable delivery of critical healthcare data. However, delivery of duplicate messages by these protocols in volatile communication environment often results in <b>irrecoverable</b> medical <b>errors.</b> The methodologies to solve the message duplication problem presented in different areas such as artificial intelligence, network protocols, software architecture, and message specifications address the re-transmission issue by TCP/IP in wired networks. They do not address wireless network issues explicitly. This paper presents a protocol that not only guarantees exact delivery but also checks the validity of messages to avoid duplication and assure exactly once medical record delivery in mobile communication environments...|$|R
40|$|Abstract—Fault-tolerant disk arrays rely on {{replication}} or erasure-coding {{to reconstruct}} lost data after a disk failure. As disk capacity increases, {{so does the}} risk of encountering <b>irrecoverable</b> read <b>errors</b> that would prevent the full recovery of the lost data. We propose a three-dimensional erasure-coding technique that reduces that risk by guaranteeing full recovery {{in the presence of}} all triple and nearly all quadruple disk failures. Our solution performs better than existing solutions, such as sets of disk arrays using Reed-Solomon codes against triple failures in each individual array. Given its very high reliability, it is especially suited to the needs of very large data sets that must be preserved over long periods of time. † Keywords-RAID arrays I...|$|R
40|$|Abstract—Disk {{scrubbing}} periodically {{scans the}} contents of a disk array to detect the presence of <b>irrecoverable</b> read <b>errors</b> and reconstitute {{the contents of}} the lost blocks using the builtin redundancy of the disk array. We address the issue of scheduling scrubbing runs in disk arrays that can tolerate two disk failures without incurring a data loss, and propose to start an urgent scrubbing run of the whole array whenever a disk failure is detected. Used alone or in combination with periodic scrubbing runs, these expedited runs can improve the mean time to data loss of disk arrays over a wide range of disk repair times. As a result, our technique eliminates the need for frequent scrubbing runs and the need to maintain spare disks and personnel on site to replace failed disks within a twentyfour hour interval. Keywords-irrecoverable read errors; RAID arrays; disk scrubbing. I...|$|R
40|$|This paper {{investigates the}} impact of carrier {{frequency}} offset (CFO) on Single Carrier wireless communication systems with Frequency Domain Equalization (SC-FDE). We show that CFO in SC-FDE systems causes <b>irrecoverable</b> channel estimation <b>error,</b> which leads to inter-symbol-interference (ISI). The impact of CFO on SC-FDE and OFDM is compared {{in the presence of}} CFO and channel estimation errors. Closed form expressions of signal to interference and noise ratio (SINR) are derived for both systems, and verified by simulation results. We find that when channel estimation errors are considered, SC-FDE is similarly or even more sensitive to CFO, compared to OFDM. In particular, in SC-FDE systems, CFO mainly deteriorates the system performance via degrading the channel estimation. Both analytical and simulation results highlight the importance of accurate CFO estimation in SC-FDE systems. ...|$|R
40|$|Mass {{spectrometry}} (MS) {{has shown}} great potential in detecting disease-related biomarkers for early diagnosis of stroke. To discover potential biomarkers from {{large volume of}} noisy MS data, peak detection must be performed first. This article proposes a novel automatic peak detection method for the stroke MS data. In this method, a mixture model is proposed to model the spectrum. Bayesian approach is used to estimate parameters of the mixture model, and Markov chain Monte Carlo method is employed to perform Bayesian inference. By introducing a reversible jump method, we can automatically estimate the number of peaks in the model. Instead of separating peak detection into substeps, the proposed peak detection method can do baseline correction, denoising and peak identification simultaneously. Therefore, it minimizes the risk of introducing <b>irrecoverable</b> bias and <b>errors</b> from each substep. In addition, this peak detection method {{does not require a}} manually selected denoising threshold. Experimental results on both simulated dataset and stroke MS dataset show that the proposed peak detection method not only has the ability to detect small signal-to-noise ratio peaks, but also greatly reduces false detection rate while maintaining the same sensitivity...|$|R
40|$|Polymeric {{materials}} have provided pathways to products {{that could not}} be manufactured otherwise. A new technology which merges the benefits of ceramics into these polymer products has created materials ideally suited to many different industries, like food packaging. Nano Scale Surface Systems, Inc. (NS 3), a company which coats polymers with ceramic oxides like SiO 2 through a process known as plasma enhanced chemical vapor deposition (PECVD), was interested in the feasibility of an in line measurement system for monitoring the deposited films on various polymer products. This project examined two different coated polymer products, polyethylene terephthalate (PET) beverage containers and biaxially oriented PET food packaging, commonly known as plastic wrap in an effort to determine the feasibility of an ellipsometry based measurement system for NS 3 ’s purpose. Due to its extensive use in the semiconductor industry for monitoring films deposited on silicon, a measurement systems known as ellipsometry, adept at monitoring the thickness and refractive index of thin films deposited on various substrates, appeared to be an ideal system for the measurement of ceramic oxides deposited on various polymer substrates. This project set out to determine the feasibility of using an ellipsometry based measurement system to monitor ceramic films, specifically silicon oxides (SiOX), deposited on polymer products. A preliminary experiment determined linearly polarized light could induce a discernible change in polarized light traversing a coated beverage container relative to an uncoated container. However, the experiment lacked repeatability due to the measurement apparatus’ cheap setup, prompting the construction of a null (conventional) ellipsometer for further research. The curved surface of the beverage containers under study unnecessarily complicated the feasibility study so further research examined PECVD SiOX on biaxially oriented PET instead. Characterization of the PECVD SiOX-PET material was divided into three experiments, with the first two analyzing the SiOX film and PET substrate separately while the third analyzed them together. To assist with the characterization experiments, NS 3 provided samples, both SiOX coated and uncoated, of various deposition thicknesses on silicon and biaxially oriented PET substrates. Null ellipsometry was used in conjunction with spectroscopic reflectometry to characterize the refractive index and thickness of the deposited films. The combined measurement systems found the refractive index of the deposited SiOX films to be between 1. 461 and 1. 465. The measured thicknesses resulting from the two measurement systems coincided well and were usually 10 - 20 nm thicker than the predicted thicknesses by the deposition processing parameters. Abeles’ method and monochromatic goniometry were attempted; however, the results had to be discarded due to <b>irrecoverable</b> <b>errors</b> discovered in the reflectance measurement. X-ray photoelectron spectroscopy (XPS) data provided by NS 3 showed the deposited SiOX films to be homogeneous with stoichiometries between 2. 15 and 2. 23. Characterization of the uncoated biaxially oriented PET required numerous measurement systems. From spectroscopic transmission, trirefringent anisotropy was discovered, intertwined with thickness variations in the PET foil. Goniometry measurements displayed distinct interference curves resulting from rear interface reflections interfering with front interface reflections from the PET sample. Subsequent goniometric models produced multiple solutions due to an unknown optical phenomenon, probably scattering, which degraded the reflection measurements. However, a combined measurement technique utilizing goniometry and differential scanning calorimetry (DSC) determined the refractive indices of the polymer to be NX = 1. 677, NY = 1. 632 and NZ = 1. 495 with a thickness of 11. 343 μm and a volume fraction crystallinity of 35 - 41 %. Utilizing the measured refractive indices, ellipsometric models produced only an adequate fit of the measured data due to the presence of depolarization caused by non-uniform PET thickness and scattering resulting from embedded microscopic crystallites. The majority of the error in the ellipsometric data was observed in the Δ measurement. XPS measurements of SiOX deposited on polypropylene (PP) provided by NS 3 showed a heterogeneous interphase layer between the deposited oxide and the polymer substrate where the composition of the layer was continually changing. A similar region, which violates the homogenous assumption the ellipsometric model relied on, was anticipated for the SiOX-PET samples under investigation. The use of an effective medium approximation (EMA) to represent the interphase region was attempted, but failed to provide a decent model fit of the measured data. Depolarization and high optical anisotropy caused by the polymer substrate in combination with a heterogeneous interphase region and the effects of the deposited SiOX layer all interacted to prevent ellipsometric modelling of the null ellipsometry measurements conducted. Goniometry measurements were conducted on the thickest deposited SiOX film (approximately 100 nm) which allowed for the refractive index of the film to be approximated through Abeles’ method (n = 1. 46); however the validity of this approximation was questionable given the presence of interference fringes resulting from interference between reflections at both the front and rear interfaces of the material. From the experiments conducted, it was concluded that null ellipsometry with conventional ellipsometric models could not adequately measure a SiOX film’s refractive index or thickness when deposited on biaxially oriented PET. The reasons for the failure were interactions between multiple sources of error which led to both measurement errors and inaccurate model assumptions. Use of generalized ellipsometry, possibly with spectroscopic ellipsometry, may overcome the failures of conventional ellipsometry when studying this complex optical material...|$|R

