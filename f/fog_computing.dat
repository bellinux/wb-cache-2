365|25|Public
25|$|On 19 November, Arm, {{alongside}} Cisco Systems, Dell, Intel, Microsoft, and Princeton University, {{founded the}} OpenFog Consortium, to promote interests {{and development in}} <b>fog</b> <b>computing.</b>|$|E
25|$|<b>Fog</b> <b>computing</b> is {{a viable}} {{alternative}} to prevent such large burst of data flow through Internet. The edge devices' computation power can be used to analyse and process data, thus providing easy real time scalability.|$|E
25|$|On November 19, 2015, Cisco, {{alongside}} ARM Holdings, Dell, Intel, Microsoft and Princeton University, {{founded the}} OpenFog Consortium, to promote interests {{and development in}} <b>fog</b> <b>computing.</b> Cisco Sr. Director of Innovation and Corporate Technology Helder Antunes became the consortium's first chairman.|$|E
30|$|<b>Fog</b> and cloud <b>computing</b> {{address a}} number of {{problems}} encountered in IoT however they also increase management complexity. Despite <b>fog</b> and cloud <b>computing</b> offering greater availability and resilience, they can also be viewed as vulnerabilities or potential points of failure. As such, in addition to edge device failure, attention must be paid to fog node and cloud infrastructure failures. While cloud and fog integration is relatively well known and shares common technologies, the integration/extension with IoT is a non-trivial task, mostly due to massive device heterogeneity and service requirements.|$|R
30|$|This section {{presents}} {{our understanding}} of basic concepts key to this paper including definitions of edge, <b>fog,</b> and cloud <b>computing</b> in Section 3. A {{brief description of the}} modeling approaches used in this work is presented in Sections 3 and 3.|$|R
30|$|According to Byers and Wetterwald [5], about 50 {{billion of}} devices will be {{connected}} to the Internet by 2020. One consequence of this trend is the production of an unprecedented volume of data in the most diverse segments. Such data can be used to provide new services for the improvement of various areas of the society (e.g. transport, health, economy). In this context, IoT, <b>Fog,</b> and Cloud <b>computing</b> paradigms of service provision stand out.|$|R
25|$|On November 19, 2015, Intel, {{alongside}} ARM Holdings, Dell, Cisco Systems, Microsoft, and Princeton University, {{founded the}} OpenFog Consortium, to promote interests {{and development in}} <b>fog</b> <b>computing.</b> Intel's Chief Strategist for the IoT Strategy and Technology Office, Jeff Faders, became the consortium's first president.|$|E
50|$|Even {{though both}} Cloud Computing and <b>Fog</b> <b>computing</b> provide to the {{end-users}} storage, applications and data, <b>Fog</b> <b>computing</b> has a bigger proximity to end-users and bigger geographical distribution.|$|E
5000|$|<b>Fog</b> <b>computing</b> - <b>Fog</b> <b>computing</b> {{is a term}} {{created by}} Cisco that refers to {{extending}} cloud computing {{to the edge of}} an enterprise's network. Also known as Edge Computing or fogging, <b>fog</b> <b>computing</b> facilitates the operation of compute, storage, and networking services between end devices and cloud computing data centers. It is a medium weight and intermediate level of computing power ...|$|E
30|$|The {{convergence}} of the distributed computing models is proposed {{to solve the}} tasks of collecting, processing and integration of big sensor data in the monitoring of distributed objects and processes. This approach includes: GRID convergence models, cloud, <b>fog</b> and mobile <b>computing</b> [13, 14]; association of computing clusters (grid, cloud and fog) in a single system; integration server application business logic, operating platforms, data warehouses; unification of administrative mechanism computing medium, information security on all data processing and storage levels.|$|R
30|$|For example, {{some authors}} {{describe}} <b>Fog</b> and Edge <b>computing</b> as distinct technologies [107, 108], while other authors interpret both as synonymous when considering Fog as a paradigm of computation [109, 110]. It {{is also possible}} to assume Mobile Edge Computing (MEC) [111] as an interpretation Fog environment but with a specific niche [16]: mobility. In MEC, Cloudlets are mobile devices that interact directly with an IoT or cellular layer. Such Cloudlets are orchestrated centrally in an upper layer (Cloud). Following this idea, this section presents some Fog-based architectures and some MEC-based architectures, to later on analyze them.|$|R
40|$|Internet of Things {{typically}} {{involves a}} significant number of smart sensors sensing information from the environment and sharing it to a cloud service for processing. Various architectural abstractions, such as <b>Fog</b> and Edge <b>computing,</b> have been proposed to localize some of the processing near the sensors and away from the central cloud servers. In this paper, we propose Edge-Fog Cloud which distributes task processing on the participating cloud resources in the network. We develop the Least Processing Cost First (LPCF) method for assigning the processing tasks to nodes which provide the optimal processing time and near optimal networking costs. We evaluate LPCF in a variety of scenarios and demonstrate its effectiveness in finding the processing task assignments. Comment: Published in IEEE 2 nd Conference on Cloudification of Internet of Things (CIoT) - 2016, Paris, Franc...|$|R
5000|$|Stolfo {{coined the}} term <b>FOG</b> <b>computing</b> (not to be {{confused}} with <b>fog</b> <b>computing)</b> where technology is used “to launch disinformation attacks against malicious insiders, preventing them from distinguishing the real sensitive customer data from fake worthless data.” [...] Stolfo’s proposed approach is to confuse and confound a traitor by leveraging uncertainty, to reduce the knowledge they ordinarily have of the systems and data they now gain access to without authorization. <b>FOG</b> <b>computing</b> systems integrate bait information with systems that generate alerts when a decoy is misused.|$|E
50|$|OpenFog {{released}} its reference architecture for <b>fog</b> <b>computing</b> on 13 February 2017.|$|E
5000|$|Dew computing—In the {{existing}} computing hierarchy, the Dew computing is positioned {{as the ground}} level for the cloud and <b>fog</b> <b>computing</b> paradigms. Compared to <b>fog</b> <b>computing,</b> which supports emerging IoT applications that demand real-time and predictable latency and the dynamic network reconfigurability, Dew computing pushes the frontiers to computing applications, data, and low level services away from centralized virtual nodes to the end users.|$|E
30|$|A {{number of}} extant works have {{proposed}} solutions {{to deal with}} IoT applications integrated with <b>fog</b> and cloud <b>computing.</b> For instance, an analytical model is used in [15] to decide where to process the data obtained from the IoT devices considering renewable energy consumption and the Quality of Service (QoS) of the application. To validate their model, the authors presented a video stream analysis application, where vehicles transmit data on road conditions, such as objects located on the road, to the cloud. This data is then analyzed and shared with other drivers as potential dangers. In [16], authors propose a QoS-aware service distribution strategy that takes in to account both service requirements and resource offerings in a fog-to-cloud scenario including parameters such as energy consumption balance and delay between the fog and cloud.|$|R
40|$|Distributed online data {{analytics}} has attracted significant research interest {{in recent years}} {{with the advent of}} <b>Fog</b> and Cloud <b>computing.</b> The popularity of novel distributed applications such as crowdsourcing and crowdsensing have fostered the need for scalable energy-efficient platforms that can enable distributed {{data analytics}}. In this paper, we propose CARDAP, a (C) ontext (A) ware (R) eal-time (D) ata (A) nalytics (P) latform. CARDAP is a generic, flexible and extensible, component- based platform that can be deployed in complex distributed mobile analytics applications e. g. sensing activity of citizens in smart cities. CARDAP incorporates a number of energy efficient data delivery strategies using real-time mobile data stream mining for data reduction and thus less data transmission. Extensive experimental evaluations indicate the CARDAP platform can deliver significant benefits in energy efficiency over naive approaches. Lessons learnt and future work conclude the paper...|$|R
40|$|Multi-agents systems {{communication}} is a technology, {{which provides a}} way for multiple interacting intelligent agents {{to communicate with each}} other and with environment. Multiple-agent systems are used to solve problems that are difficult for solving by individual agent. Multiple-agent communication technologies can be used for management and organization of <b>computing</b> <b>fog</b> and act as a global, distributed operating system. In present publication we suggest technology, which combines decentralized P 2 P BOINC general-purpose computing tasks distribution, multiple-agents communication protocol and smart-contract based rewards, powered by Ethereum blockchain. Such system can be used as distributed P 2 P computing power market, protected from any central authority. Such decentralized market can further be updated to system, which learns the most efficient way for software-hardware combinations usage and optimization. Once system learns to optimize software-hardware efficiency it can be updated to general-purpose distributed intelligence, which acts as combination of single-purpose AI. Comment: 18 pages, 8 figure...|$|R
5000|$|... #Caption: Antunes {{became the}} first {{chairman}} of the board of OpenFog, a <b>fog</b> <b>computing</b> consortium, in 2015.|$|E
5000|$|... #Caption: The OpenFog Consortium is an {{association}} of major tech companies aimed at standardizing and promoting <b>fog</b> <b>computing.</b>|$|E
5000|$|In February 2017, OpenFog Consortium {{published}} the OpenFog Reference Architecture. This discusses eight technical pillars for <b>fog</b> <b>computing.</b>|$|E
40|$|International audienceFog and edge {{computing}} leverage resources of end users and edge devices rather than centralized clouds. Isolation is a core security challenge for such paradigms: just like traditional clouds, fog and edge infrastructures {{are based on}} virtualization to share physical resources among several self-contained execution environments like virtual machines and containers. Yet, isolation may be threatened due to side-channels, created by the virtualization layer or due to the sharing of physical resources like the processor. Side-channel attacks (SCAs) exploit and use such leaky channels to obtain sensitive data. This paper aims to clarify {{the nature of this}} threat for fog and edge infrastructures. Current SCAs are local and exploit isolation challenges of virtualized environments to retrieve sensitive information. We introduce a new concept of distributed side-channel attack (DSCA) that is based on coordinating local attack techniques. We explore how such attacks can threaten isolation of any virtualized environments such as <b>fog</b> and edge <b>computing.</b> Finally, we study a set of different applicable countermeasures for attack mitigation...|$|R
40|$|The next {{generation}} wireless networks (i. e. 5 G and beyond), {{which would be}} extremely dynamic and complex due to the ultra-dense deployment of heterogeneous networks (HetNets), poses many critical challenges for network planning, operation, management and troubleshooting. At the same time, generation and consumption of wireless data are becoming increasingly distributed with ongoing paradigm shift from people-centric to machine-oriented communications, making the operation of future wireless networks even more complex. In mitigating the complexity of future network operation, new approaches of intelligently utilizing distributed computational resources with improved context-awareness becomes extremely important. In this regard, the emerging <b>fog</b> (edge) <b>computing</b> architecture aiming to distribute computing, storage, control, communication, and networking functions closer to end users, have a great potential for enabling efficient operation of future wireless networks. These promising architectures make the adoption of artificial intelligence (AI) principles which incorporate learning, reasoning and decision-making mechanism, as natural choices for designing a tightly integrated network. Towards this end, this article provides a comprehensive survey on the utilization of AI integrating machine learning, data analytics and natural language processing (NLP) techniques for enhancing the efficiency of wireless network operation. In particular, we provide comprehensive discussion on the utilization of these techniques for efficient data acquisition, knowledge discovery, network planning, operation {{and management of the}} {{next generation}} wireless networks. A brief case study utilizing the AI techniques for this network has also been provided. Comment: ITU Special Issue N. 1 The impact of Artificial Intelligence (AI) on communication networks and services, (To appear...|$|R
40|$|National audienceThe Cloud Computing {{approach}} concentrates the {{computing power}} in few datacenters. The high latency {{to reach the}} platform makes this architecture not well suited for the Internet of Things. The <b>Fog</b> and Edge <b>Computing</b> propose to place servers near the users. In this context, we propose a first-class object store service for Fog/Edge facilities. Our proposal is built with Scale-out Network Attached Storage systems (NAS) and IPFS, a BitTorrent-based object store spread throughout the Fog/Edge infrastructure. Without impacting the IPFS advantages particularly in terms of data mobility, {{the use of a}} Scale-out NAS on each site reduces the inter-site exchanges that are costly but mandatory for the metadata management in the original IPFS implementation. Several experiments conducted on Grid' 5000 testbed are analysed and confirmed, first, the benefit of using an object store service spread at the Edge and second, the importance of mitigating inter-site accesses. The paper concludes by giving a few directions to improve the performance and fault tolerance criteria of our Fog/Edge Object Store Service...|$|R
50|$|<b>Fog</b> <b>computing</b> can be {{perceived}} both in large cloud systems and big data structures, making {{reference to the}} growing difficulties in accessing information objectively. This results in a lack of quality of the obtained content. The effects of <b>fog</b> <b>computing</b> on cloud computing and big data systems may vary; yet, a common aspect that can be extracted is a limitation in accurate content distribution, {{an issue that has}} been tackled with the creation of metrics that attempt to improve accuracy.|$|E
50|$|On 19 November, Arm, {{alongside}} Cisco Systems, Dell, Intel, Microsoft, and Princeton University, {{founded the}} OpenFog Consortium, to promote interests {{and development in}} <b>fog</b> <b>computing.</b>|$|E
50|$|Fog {{networking}} {{consists of}} a control plane and a data plane. For example, on the data plane, <b>fog</b> <b>computing</b> enables computing services to reside {{at the edge of}} the network as opposed to servers in a data-center. Compared to cloud computing, <b>fog</b> <b>computing</b> emphasizes proximity to end-users and client objectives, dense geographical distribution and local resource pooling, latency reduction and backbone bandwidth savings to achieve better quality of service (QoS) and edge analytics/stream mining, resulting in superior user-experience and redundancy in case of failure.|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimitedPublished climatologies of marine-fog frequencies are in disagreement for common areas, although {{the nature and}} magnitude of errors are difficult to assess since actual frequencies for specific locations may be derived only from observations at Ocean Stations. The usual methods of <b>computing</b> <b>fog</b> frequencies, on a seasonal or monthly basis, are percent-of-reports-with-fog and number-of-fog-days. This study presents a method of synthesizing the elements of surface-ship synoptic reports into a computerized scheme {{for the purpose of}} deriving frequencies of marine fog occurrence. The program, based on a liberal interpretation of reporting guidelines in the Synoptic Code Manual, utilizes 16 combinations of present and past weather, and visibility, to identify fog in the reports. The program then objectively assigns the duration of fog for the three- or six-hour period represented by the synoptic report. A prototype climatology of marine-fog occurrence for July, over the eastern North Pacific Ocean, is derived from application of the method to a ten-year data base (1963 - 72). Results are compared to published and other defined marine-fog climatologies. [URL] United States Nav...|$|R
40|$|In {{this paper}} we propose a wet lab {{algorithm}} for prediction of radiation <b>fog</b> by DNA <b>computing.</b> The concept of DNA computing is essentially exploited for generating the classifier algorithm in the wet lab. The classifier is based on a new concept of similarity based fuzzy reasoning suitable for wet lab implementation. This new concept of similarity based fuzzy reasoning is different from conventional approach to fuzzy reasoning based on similarity measure and also replaces the logical aspect of classical fuzzy reasoning by DNA chemistry. Thus, we add a new dimension to existing forms of fuzzy reasoning by bringing it down to nanoscale. We exploit the concept of massive parallelism of DNA computing by designing this new classifier in the wet lab. This newly designed classifier is very much generalized in nature and apart from prediction of radiation fog this methodology can be applied to other types of data also. To achieve our goal we first fuzzify the given observed parameters in a form of synthetic DNA sequence which is called fuzzy DNA and which handles the vague concept of human reasoning. Comment: 36 page...|$|R
40|$|Current vision {{systems are}} {{designed}} to perform in clear weather. Needless to say, in any outdoor application, there is no escape from “bad ” weather. Ultimately, computer vision systems must include mechanisms {{that enable them to}} function (even if somewhat less reliably) in the presence of haze, fog, rain, hail and snow. We begin by studying the visual manifestations of different weather conditions. For this, we draw on what is already known about atmospheric optics, and identify effects caused by bad weather that can be turned to our advantage. Since the atmosphere modulates the information carried from a scene point to the observer, it {{can be viewed as a}} mechanism of visual information coding. We exploit two fundamental scattering models and develop methods for recovering pertinent scene properties, such as three-dimensional structure, from one or two images taken under poor weather conditions. Next, we model the chromatic effects of the atmospheric scattering and verify it for fog and haze. Based on this chromatic model we derive several geometric constraints on scene color changes caused by varying atmospheric conditions. Finally, using these constraints we develop algorithms for <b>computing</b> <b>fog</b> or haze color, depth segmentation, extracting three-dimensional structure, and recovering “clear day” scene colors, from two or more images taken under different but unknown weather conditions...|$|R
50|$|<b>Fog</b> <b>computing</b> is {{a viable}} {{alternative}} to prevent such large burst of data flow through Internet. The edge devices' computation power can be used to analyse and process data, thus providing easy real time scalability.|$|E
5000|$|The OpenFog Consortium (sometimes stylized as Open Fog Consortium) is a {{consortium}} of high tech industry companies and academic institutions across the world aimed at the standardization and promotion of <b>fog</b> <b>computing</b> in various capacities and fields.|$|E
50|$|On November 19, 2015, Cisco, {{alongside}} ARM Holdings, Dell, Intel, Microsoft, and Princeton University, {{founded the}} OpenFog Consortium, to promote interests {{and development in}} <b>fog</b> <b>computing.</b> Cisco Sr. Director of Innovation and Corporate Technology Helder Antunes became the consortium's first chairman.|$|E
40|$|Mobile Edge <b>Computing</b> (MEC) and <b>Fog</b> are {{emerging}} <b>computing</b> models that extend the cloud and its {{services to the}} edge of the network. The emergence of both MEC and fog introduce new requirements, which mean their supported deployment models must be investigated. In this paper, we point out the influence and strong impact of the extended cloud (i. e., the MEC and fog) on existing communication and networking service models of the cloud. Although the relation between them is fairly evident, there are important properties, notably those of security and resilience, that we study in relation to the newly posed requirements from the MEC and fog. Although security and resilience have been already investigated in the context of the cloud - to a certain extent - existing solutions may not be applicable in the context of the extended cloud. Our approach includes the examination of models and architectures that underpin the extended cloud, and we provide a contemporary discussion on the most evident characteristics associated with them. We examine the technologies that implement these models and architectures, and analyse them with respect to security and resilience requirements. Furthermore, approaches to security and resilience-related mechanisms are examined in the cloud (specifically, anomaly detection and policy based resilience management), and we argue that these can also be applied in order to improve security and achieve resilience in the extended cloud environment...|$|R
40|$|EEG-based Brain-computer {{interfaces}} (BCI) {{are facing}} grant challenges in their real-world applications. The technical difficulties in developing truly wearable multi-modal BCI {{systems that are}} capable of making reliable real-time prediction of users’ cognitive states under dynamic real-life situations may appear at times almost insurmountable. Fortunately, recent advances in miniature sensors, wireless communication and distributed computing technologies offered promising ways to bridge these chasms. In this paper, we report our attempt to develop a pervasive on-line BCI system by employing state-of-art technologies such as multi-tier <b>fog</b> and cloud <b>computing,</b> semantic Linked Data search and adaptive prediction/classification models. To verify our approach, we implement a pilot system using wireless dry-electrode EEG headsets and MEMS motion sensors as the front-end devices, Android mobile phones as the personal user interfaces, compact personal computers as the near-end fog servers and the computer clusters hosted by the Taiwan National Center for High-performance Computing (NCHC) as the far-end cloud servers. We succeeded in conducting synchronous multi-modal global data streaming in March and then running a multi-player on-line BCI game in September, 2013. We are currently working with the ARL Translational Neuroscience Branch and the UCSD Movement Disorder Center to use our system in real-life personal stress and in-home Parkinson’s disease patient monitoring experiments. We shall proceed to develop a necessary BCI ontology and add automatic semantic annotation and progressive model refinement capability to our system...|$|R
40|$|The {{concept of}} the {{augmented}} coaching ecosystem for non-obtrusive adaptive personalized elderly care is proposed {{on the basis of}} the integration of new and available ICT approaches. They include the multimodal user interface (MMUI), augmented reality (AR), machine learning (ML), Internet of Things (IoT), and machine-to-machine (M 2 M) interactions. The ecosystem is based on the Cloud-Fog-Dew computing paradigm services, providing a full symbiosis by integrating the whole range from low-level sensors up to high-level services using integration efficiency inherent in synergistic use of applied technologies. Inside of this ecosystem, all of them are encapsulated in the following network layers: Dew, <b>Fog,</b> and Cloud <b>computing</b> layer. Instead of the "spaghetti connections", "mosaic of buttons", "puzzles of output data", etc., the proposed ecosystem provides the strict division in the following dataflow channels: consumer interaction channel, machine interaction channel, and caregiver interaction channel. This concept allows to decrease the physical, cognitive, and mental load on elderly care stakeholders by decreasing the secondary human-to-human (H 2 H), human-to-machine (H 2 M), and machine-to-human (M 2 H) interactions in favor of M 2 M interactions and distributed Dew Computing services environment. It allows to apply this non-obtrusive augmented reality ecosystem for effective personalized elderly care to preserve their physical, cognitive, mental and social well-being. Comment: 6 pages, 2 figures, 40 th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO) Opatija, Croatia (2017...|$|R
