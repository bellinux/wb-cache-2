84|223|Public
50|$|Meaning any data {{outside the}} {{blockchain}} {{can be translated}} into a <b>deterministic</b> <b>value</b> {{that can be used}} in æternity smart contracts, making real-world data easily accessible and actionable.|$|E
3000|$|... {{during the}} {{observation}} period among the user, cluster header, and FC, {{which is a}} <b>deterministic</b> <b>value</b> but not a vector.|$|E
3000|$|We {{estimated}} large-scale spatial variation {{behavior of}} PWV from GNSS stations during March 2017. The <b>deterministic</b> <b>value</b> of PWV for TPGN network at six hours interval on 15 th of March 2017 is modeled as: [...]...|$|E
50|$|The {{most common}} {{formulation}} of the load flow problem specifies all input variables (PQ at loads, PV at generators) as <b>deterministic</b> <b>values.</b> Each set of specified values corresponds to one system state, which depends {{on a set of}} system conditions. When those conditions are uncertain, numerous scenarios must be analyzed.|$|R
40|$|Any {{reliability}} analysis based on just <b>deterministic</b> <b>values</b> of daily average or peak demands, cannot represent the overall system reliability realistically. Using a stochastic model {{in this paper}} variations {{of the system and}} nodal reliability values are investigated through a period of time. Considering probabilistic nature of demands, this paper combines the extended period simulation of water distribution networks with the head driven simulation based {{reliability analysis}} that presents the nodal and system reliabilities more realistically than conventional demand driven simulation based analysis. A sample network with possibility of one link failure is examined and diurnal profile of reliability values due to mechanical and hydraulic failures and known probabilities of demands is presented. It is seen that considering the probabilistic nature of demands the severity of mechanical and hydraulic failures on the hydraulic performance of the system are illustrated properly and realistically, especially at the critical times or nodes, in comparison with the <b>deterministic</b> <b>values...</b>|$|R
2500|$|Let [...] and [...] be a {{sequence}} of (<b>deterministic)</b> real <b>valued</b> functions. The following two properties are equivalent: ...|$|R
30|$|The initial {{corrosion}} time {{calculated by}} random environment and structure variables {{is not a}} <b>deterministic</b> <b>value,</b> but a random variable. Based on the statistical analysis, the initial corrosion time at the top surface over the supports {{is more likely to}} follow a lognormal distribution.|$|E
40|$|This paper {{develops}} two straightforward {{value of}} life models; {{one is a}} probabilistic value of life model {{and the second is}} a <b>deterministic</b> <b>value</b> of time model. Simplifying assumptions allow both models to be solved analytically. Constant relative risk aversion utility functions are used, and both value of life and value of time are solved for as functions of the relative risk-aversion parameter...|$|E
3000|$|... in the {{expression}} (15) and using this <b>deterministic</b> <b>value</b> of Y {{to replace the}} random variable Z in (17). The IC- κESM is an approximation that only applies to delayed CSI. For this reason, we will compare the accuracy of these three metrics for the scenario where the transmitter only has delayed CSI available (see the “Delayed CSI” section in the Appendix). The following simulation parameters are used: SNR= 10 dB, I_q/σ _w^ 2 = 0 dB, {{and the value of}} f [...]...|$|E
30|$|Initialize the {{population}} of the EA corresponding to the number of new added lines. Note that the quality of initial population has great impacts on the final solution. Therefore, in this process, uncertain features are neglected and some <b>deterministic</b> <b>values</b> are assigned. To be specific, carbon price is set to its mean value, wind power output is set to the installed capacity scaled by its capacity factor, all components are set as available, and load is set as (μ +  3 σ).|$|R
50|$|Evaluating an {{availability}} {{requirement is}} a challenge for manufacturers and supporters of systems because determining how to deliver a specific availability is not trivial. The required availability can be defined over the instance of system or over the fleet. It can be defined over different time windows or in different geographical boxes. However availability optimization approaches provide solutions only at selected points in time (not all times), using {{mean time to failure}} (or fixed rate demand) and mean time to repair as <b>deterministic</b> <b>values</b> as part of convex optimization.|$|R
40|$|Quality of Service (QoS) {{prediction}} and aggregation for composite {{services is}} one of the key issues in service computing. Existing solutions model service QoSs either as <b>deterministic</b> <b>values</b> or probabilistic distributions. However, these works overlooked an important aspect in QoS modeling, time. Most QoS metrics, such as response time, availability, are time-dependent. We believe time variation should be explicitly reflected in QoS modeling as well as aggregation. In this paper, we propose a dynamic web service QoS model to capture the time based QoS patterns, based on which QoS of composite services are aggregated. 9 page(s...|$|R
30|$|Monte Carlo {{simulation}} {{as a tool}} {{to perform}} risk analysis can be very beneficial to assist in successfully completing the projects and companies should consider implementing it. It helps in determining the required contingency to be added into the cost estimate and the float needed in the plan. Further, it gives ranges of the possible project cost and duration instead of a single <b>deterministic</b> <b>value.</b> This will help in planning the resources efficiently, thereby giving the management team realistic projections.|$|E
30|$|If {{the search}} for the global minimum can be {{performed}} in each OFDM symbol, then the CCDF curve improves to some degree. In our test, each OFDM symbol has been processed 100 times with different initial guesses and the one with the smallest PAPR is selected. The result in Figure 7 (advanced SQP) shows an overall improvement of about 0.5 dB. In this case, the PAPR of the system can almost be considered as a <b>deterministic</b> <b>value</b> since the CCDF curve is almost vertical.|$|E
40|$|This paper {{evaluates the}} {{electromagnetic}} field deviation from its <b>deterministic</b> <b>value</b> when the electrical {{parameters of the}} scenery under investigation are not known in a deterministic form but can be considered statistically distributed. First, a statistical model for the electrical parameters {{that will be used}} in the electromagnetic procedure is defined. Then, the analysis is performed by employing an efficient deterministic RT simulation in conjunction with a Monte Carlo procedure, allowing us to evaluate the electromagnetic field amplitude variations caused by the random electrical parameter...|$|E
40|$|This paper {{considers}} the static displacement bounds of structures modelled using uncertain (but non-random) para-meters. In the analysis, the uncertain parameters {{are assumed to}} take <b>deterministic</b> <b>values</b> within a specified interval, and the bounds of the displacement are obtained by solving interval linear equations. Two new methods for solving these equations are proposed. The first is {{a modified version of}} interval perturbation analysis, while the second {{is based on the assumption}} that the displacement surface is monotonic. Numerical results indicate that both methods are more accurate than standard interval perturbation analysis and more ecient than the combinatorial approach. Ó 200...|$|R
40|$|Stochastic model {{updating}} must {{be considered}} for quantifying uncertainties inherently existing in real-world engineering structures. By this means the statistical properties,instead of <b>deterministic</b> <b>values,</b> of structural parameters can be sought indicating the parameter variability. However, the implementation of stochastic model updating is {{much more complicated than}} that of deterministic methods particularly in the aspects of theoretical complexity and low computational efficiency. This study attempts to propose a simple and cost-efficient method by decomposing a stochastic updating process into a series of deterministic ones with the aid of response surface models and Monte Carlo simulation. The response surface models are used as surrogates for original FE models in the interest of programming simplification, fast response computation and easy inverse optimization. Monte Carlo simulation is adopted for generating samples from the assumed or measured probability distributions of responses. Each sample corresponds to an individual deterministic inverse process predicting the <b>deterministic</b> <b>values</b> of parameters. Then the parameter means and variances can be statistically estimated based on all the parameter predictions by running all the samples. Meanwhile, the analysis of variance approach is employed for the evaluation of parameter variability significance. The proposed method has been demonstrated firstly on a numerical beam and then a set of nominally identical steel plates tested in the laboratory. It is found that compared with the existing stochastic model updating methods, the proposed method presents similar accuracy while its primary merits consist in its simple implementation and cost efficiency in response computation and inverse optimization...|$|R
30|$|Although the {{polynomial}} coefficients {{depend on}} the instantaneous channel realizations, we {{have shown that the}} per-user SINRs converge to <b>deterministic</b> <b>values</b> in the large (M,K) regime. This enabled us to compute asymptotically optimal coefficients using merely the statistics of the channels. The simulations revealed that the difference in performance between RZF and TPE is small at low SNRs and for large CSI errors. The TPE order J can be chosen very small in these situations, and in general, it does not need to scale with the system dimensions. However, to maintain a fixed per-user rate-loss compared to RZF, J should increase with the SNR or as the CSI quality improves.|$|R
40|$|Critical exponents are {{calculated}} exactly {{at the onset}} of an instability, using asymptotic expansiontechniques. When the unstable mode is subject to multiplicative noise whose spectrum at zero frequency vanishes, we show that the critical behavior can be anomalous, i. e. the mode amplitude X scales with departure from onset μ as μ^β with an exponent β different from its <b>deterministic</b> <b>value.</b> This behavior is observed in a direct numerical simulation of the dynamo instability and our results provide a possible explanation to recent experimental observations...|$|E
3000|$|From the results, we can {{conclude}} that CSMA/CA presents a behaviour that {{is very similar to}} the one we have observed in the ideal channel. This is not the case of CSMA/ECA that presents an impaired behaviour, because it cannot differentiate between channel errors and collisions. This problem is partly alleviated by CSMA/E 2 CA. Nevertheless, if we want to obtain collision-free operation after a short transient state, the number of competing stations has to be around one half of the capacity of the system. As an example, for a <b>deterministic</b> <b>value</b> after successes [...]...|$|E
40|$|We {{consider}} {{the effect of}} heterogeneity on estimation {{of the time that}} is necessary to reclaim an aquifer by means of a constant rate pumping well. We derive the predictor of resident time (rendered by its mean) together with the associated prediction error (rendered by its variance) for non reactive solute particles under mean radial flow conditions in a heterogeneous aquifer. The solutions are obtained numerically following a Monte Carlo procedure, and compared with a newly developed first-order analytical approach. The natural logarithm, Y, of aquifer transmissivity, T, is modelled as a statistically homogeneous Gaussian random field with a Gaussian spatial correlation function. The agreement between analytical and numerical solutions is very good for all the cases considered. We analyzed the effect on the travel time statistical moments of (i) distance from the injection point to the well, (ii) heterogeneity of the random field, (iii) domain size, in terms of correlation length of Y. One of the main results is that the mean travel time is always larger than the <b>deterministic</b> <b>value</b> obtained assuming a homogeneous media. Our analysis can be a useful tool in planning water resources protection strategies, since {{it would be possible to}} obtain an estimate of the maximum clean-up time, associated with a design probability, which would substitute the use of a single (underestimated) <b>deterministic</b> <b>value...</b>|$|E
30|$|The present {{literature}} {{has focused on}} identifying the most important criteria and indicators, {{as well as on}} explaining the main steps of various MCDM methods applied for planning and design of linear transportation infrastructure. The majority of the literature utilizes MCDM methods taking <b>deterministic</b> <b>values</b> on preferences of route alternatives conducted among mostly quantitative criteria. Only a few papers (e.g. Ballestero et al. [12]) considers the project evaluation under uncertainty. In fact, the problem is simplified as it neglects the uncertainty issues that in reality affect infrastructure projects. These uncertainty issues commonly refer to different limited resources, lack of information or uncertain project outputs.|$|R
40|$|Methods for {{predicting}} {{the onset of}} flutter during an experiment are traditionally applied treating the data as <b>deterministic</b> <b>values.</b> Uncertainty and variation in the data is often glossed over by using best-fit curves to represent the information. This paper applies stochastic treatments to wind tunnel data obtained for the Piezoelectric Aeroelastic Response Tailoring Investigation model. These methods include modal amplitude tracking, modal frequency tracking and several applications of the flutter margin method. The flutter margin method was developed by Zimmerman and Weissenburger, and extended by Poirel, Dunn and Porter to incorporate uncertainty. Much of the current work follows the future work recommendations of Poirel, Dunn and Porter...|$|R
40|$|Confidence in the {{long-term}} use of jack-up platforms in deep water and harsh environments requires appropriate models for their assessment under dynamic loading conditions. In this paper probabilistic models are used to develop further understanding of this assessment. Particular {{emphasis is placed on}} achieving a balanced approach in considering the non-linearities and uncertainties in the structure, foundations and wave loading. A method of calculating short-term extreme response statistics, while including variability in parameters, is briefly outlined, and a numerical experiment for typical central North Sea conditions detailed. Long-term extreme response statistics are also evaluated, and the quantitative influence of the probabilistic formulations of the variables (as opposed to <b>deterministic</b> <b>values)</b> shown...|$|R
40|$|Policy {{gradient}} {{methods have}} had great success in solving continuous control tasks, yet the stochastic nature of such problems makes <b>deterministic</b> <b>value</b> estimation difficult. We propose an approach which instead estimates a distribution by fitting the value function with a Bayesian Neural Network. We optimize an α-divergence objective with Bayesian dropout approximation {{to learn and}} estimate this distribution. We show that using the Monte Carlo posterior mean of the Bayesian value function distribution, rather than a deterministic network, improves stability and performance of policy gradient methods in continuous control MuJoCo simulations. Comment: Accepted to Bayesian Deep Learning Workshop at NIPS 201...|$|E
40|$|This paper {{concerns}} a propagation mechanism {{in an economy}} where many individuals follow a threshold rule and interact with a positive feedback. We derive an asymptotic distribution of the propagation size {{when the number of}} the agents tends to infinity. The propagation distribution exhibits a slower convergence to a <b>deterministic</b> <b>value</b> than it would if the agents followed a smooth adjustment policy. This gives rise to significant aggregate fluctuations in a finite lumpy-adjusting economy even when the agents are hit by small independent shocks. The result is applied to a standard sectoral business cycle model. JEL classification: E 1, E...|$|E
40|$|We {{perform an}} {{asymptotic}} {{study on the}} performance of filter bank multicarrier (FBMC) in the context of massive multi-input multi-output (MIMO). We show that the signal-to-interference-plus-noise ratio (SINR) cannot grow unboundedly by increasing the number of base station (BS) antennas, and is upper bounded by a certain <b>deterministic</b> <b>value.</b> This {{is a result of the}} correlation between the multi-antenna combining tap values and the channel impulse responses between the terminals and the BS antennas. To solve this problem, we introduce a simple FBMC prototype filter design method that removes this correlation, enabling us to achieve arbitrarily large SINR values by increasing the number of BS antennas...|$|E
40|$|Literature on {{turbulence}} modeling {{is rich in}} empirical, semi-empirical {{and theoretical}} spectral equations whose parameters assume <b>deterministic</b> <b>values.</b> Starting from a critical review {{of the state of}} the art, this paper proposes a unified model of atmospheric turbulence especially suited to determine the 3 -D gust-excited response of structures. Unlike classical models, all parameters are assigned through first and second order statistical moments derived from a wide set of selected experimental measurements. A general discussion is also provided about model errors and other sources of randomness. Due to these properties the model proposed is suitable for carrying out reliability analyses which take into account the propagation of the uncertainties...|$|R
40|$|We study {{a random}} {{resistors}} network model on a euclidean geometry Z d. We formulate {{the model in}} terms of a variational principle and show that, under appropriate boundary conditions, the thermodynamic limit of the dissipation per unit volume is finite almost surely and in the mean. Moreover, we show that for a particular thermodynamic In this paper we to study a model of random resistors networks (RRN) on a euclidean geometry Z d. A RRN model is a classical electrical circuit in which the nodes are the sites of a lattice and are connected with resistors that do not take <b>deterministic</b> <b>values</b> but are random variables. RRN model...|$|R
40|$|The below {{overview}} {{is designed}} to give the reader a limited understanding of Bayesian and Maximum Likelihood (MLE) estimation; {{a basic understanding of}} some of the mathematical tools to evaluate the quality of an estimation; an introduction to energy methods and a limited discussion of damage potential. This discussion then goes on to presented a limited presentation as to how energy methods and Bayesian estimation are used together to qualify components. Example problems with solutions have been supplied as a learning aid. Bold letters are used to represent random variables. Un-bolded letter represent <b>deterministic</b> <b>values.</b> A concluding section presents a discussion of attributes and concerns...|$|R
40|$|Life-cycle cost (LCC) {{analysis}} {{can be used}} to select the best alternative among a set of candidates. The LCC is often treated as a random variable due to uncertainties. As a result, it is often represented by a distribution (or its first four moments) rather than a <b>deterministic</b> <b>value.</b> It is a challenge how to select the best alternative based on a set of given distributions. Mathematically, this deals with comparing magnitudes of the random variables representing the LCCs of alternatives. This paper develops an analytical framework to tackle this problem. The approach is illustrated by a numerical example. Department of Industrial and Systems Engineerin...|$|E
40|$|A {{model is}} {{considered}} to calculate effects of genetic drift on the expected proportion of new mutants amongst males affected by a sex-linked recessive lethal. We show how to relate {{the number of cases}} of the disorder in males to the expected deviations from the <b>deterministic</b> <b>value</b> of the proportion of new mutants. For small values of alpha (= 3 N mu), where N is the size of the female population, and mu is the mutation rate from wild-type to lethal allele, the standard deviation (SD) of the proportion of new mutants is large. However, if alpha more than 50, the potential effect of genetic drift is probably less important than the many other sources of error and bias...|$|E
30|$|The PWV {{variation}} {{on the basis of}} geographical coordinates has been modeled by using an ordinary least square estimator (OLSE) which can estimate the residuals as well. The OLSE residuals are very useful to construct the variance and covariance matrix. The OLSE can represent the variability of the tropospheric process and can be modeled by a polynomial. Let us assume <b>deterministic</b> <b>value</b> of PWV is expressed {{as a function of the}} independent variables (ϕ, λ and t). It is clear from the Fig.  1, that longitude range is more than double the latitude range for covering Turkey. Hence, we used first order function for latitude and second order for longitude to model the PWV (Ansari et al. 2017 c).|$|E
40|$|The {{cause of}} the long-suspected {{excessive}} conservatism in the prevailing structural deterministic safety factor {{has been identified as}} an inherent violation of the error propagation laws when reducing statistical data to <b>deterministic</b> <b>values</b> and then combining them algebraically through successive structural computational processes. These errors are restricted to the applied stress computations, and because mean and variations of the tolerance limit format are added, the errors are positive, serially cumulative, and excessively conservative. Reliability methods circumvent these errors and provide more efficient and uniform safe structures. The document is a tutorial on the deficiencies and nature of the current safety factor and of its improvement and transition to absolute reliability...|$|R
30|$|Among {{the whole}} set of {{possible}} random parameters, κ, ῡ and γ {{have an important role}} as they are involved in the Feller condition. As a first approach and in order to simplify the results to visualize some conclusions, only parameters κ and γ have been considered. Thus, we obtain a two-dimensional problem for UQ. Tests where Feller condition is satisfied or not have been carried out. The numerical results shown in this section will give us an insight in the impact of the variability of κ and γ on the output of the stated problem. All the computations have been done for time τ= 2 and the <b>deterministic</b> <b>values</b> for the parameters can be seen in Table  1.|$|R
40|$|General {{deterministic}} optimization {{does not}} consider the uncertainties which can happen after the design process. The Probabilistic approach, which conduced in this research, does not treat the design parameter and external load as <b>deterministic</b> <b>values</b> but statistical value having probabilistic distribution. So with this concept, Finding design parameters which minimizes the object function and satisfies the target reliability in the same time. In this thesis, author conducted Reliability Based Topology Optimization design process applicable for initial design, assuming the uncertainties is in the external loading, so to speak the magnitude of applied loading, the direction angle of applied loading, {{and the location of}} applied loading. Some examples show the effectiveness of the proposed method...|$|R
