6|10000|Public
40|$|The article gives a {{detailed}} description of difficulties that emerge in the process of teaching Russian students to make a speech in Japanese at the stage of the speech text preliminary development. The conducted research shows that in the process of acquiring speech making skills students have considerable difficulties and make a number of various mistakes at different stages (phases) of oral speech production. We suppose that for <b>developing</b> <b>an</b> <b>efficient</b> <b>model</b> of teaching students to make a speech in Japanese it is necessary to introduce corrections into the speech production model which are directly connected with the peculiarities of making an oral presentation in Japanese at the initial stage of education...|$|E
40|$|The {{increasing}} concern regarding chronic care, {{which is}} a consequence of the current demographic progression, and the need to decrease the costs associated with its care, places a focus on social care homes caring for highly dependent patients. Simultaneously, the residential facilities are progressing in order to care for fragile patients with increasingly complexity, even though, with some exceptions, it is in parallel with the healthcare system. Within this reality, pharmaceutical care is developing very differently in all the autonomic regions, and has become a reason for controversy. In this sense, diverse factors related with the patient care setting, but also linked to different pharmaceutical levels, make it difficult to implement a patient care model. Faced with this scenario, it seems reasonable to analyze the situation {{from the perspective of the}} healthcare requirements of the institutionalized persons in these facilities, and in terms of the patient care that we can and should provide; subsequently, and based on this, we should be able to propose the basic guidelines for <b>developing</b> <b>an</b> <b>efficient</b> <b>model</b> of pharmaceutical care integrated within this patient care settin...|$|E
40|$|Early stage cost {{estimate}} plays {{a significant role}} in any initial construction project decisions, despite the project scope has not yet been finalized and still very limited information regarding the detailed design is available during these early stages. This study aimed at <b>developing</b> <b>an</b> <b>efficient</b> <b>model</b> to estimate the cost of building construction projects at early stages using artificial neural networks. A database of 71 building projects is collected from the construction industry of the Gaza Strip. Several significant parameters were identified for the structural skeleton cost of the project and yet can be obtained from available engineering drawings and data at the pre-design stage of the project. The input layer of the Artificial Neural Networks (ANN) model comprised seven parameters, namely: ground floor area, typical floor area, number of storeys, number of columns, type of footing, number of elevators and number of rooms. The developed ANN model had one hidden layer with seven neurons. One neuron representing the early {{cost estimate}} of buildings formed the output layer of the ANN model. The results obtained from the trained models indicated that neural networks reasonably succeeded in predicting the early stage cost estimation of buildings using basic information of the projects and without the need for a more detailed design. The performed sensitivity analysis showed that the ground floor area, number of storeys, type of foundation and number of elevators in the buildings are the most effective parameters influencing the early estimates of building cost...|$|E
40|$|International audienceWe <b>developed</b> <b>an</b> <b>efficient</b> <b>model</b> to {{evaluate}} the electromagnetic scattering from large scenes composed by targets (metallic objects) placed in natural environment (dielectric object). Our model, named DEMOS, is a hybrid volume/surface model that integrate both metallic and dielectric scatterers. In this paper we compare the scattered field obtained with DEMOS with measurements done in an anechoïc chamber on scaled models composed of dielectric and metallic structures...|$|R
40|$|Recording, mapping, 3 D {{modeling}} and visualization for structures of architectural significance and objects {{of importance to}} cultural heritage have been received attention for digital archives or VR museum. With this motive, the authors have been <b>developing</b> <b>an</b> <b>efficient</b> <b>modeling</b> system for recreating historical city. However, reliable descriptions of ancient landscape are currently requested {{from the viewpoint of}} environmental archaeology. This paper reports 3 D modeling system for recreating ancient landscape, and shows landscape animation for Cheng-Tou-Shan in China. 1...|$|R
40|$|This paper {{presents}} {{an evaluation of}} the security, safety, and privacy of selected Online Collaborative Groupware (OCG) tools such as Skype, Facebook, Wikis and Gmail (SWFG) used to support learning activities, and with a particular focus on the impact of their usage on student trust. A case study was conducted with two groups of students at University of Bahrain to identify and <b>develop</b> <b>an</b> <b>efficient</b> <b>model</b> for using SWFG tools securely within learning. The overall finding {{of this study is that}} there are differences between two groups with respect to security, privacy, and trust for SWFG tools...|$|R
40|$|This paper {{presents}} a parametric study {{to investigate the}} behavior of eccentrically loaded concrete encased steel composite columns (SRC). The artificial neural network (ANN) technique was adopted in this study by <b>developing</b> <b>an</b> <b>efficient</b> <b>model</b> to predict the behavior of such composite columns, depending on a total of 105 experimental tests for such composite columns with concrete rectangular section encased I-shape structural steel section and subjected to eccentric loads producing bending moment {{about one of the}} column section axes. The developed model was used to investigate the effects on the structural behavior of the eccentrically loaded composite columns owing to the steel contribution ratio, the axis of the applied bending, the concrete strength, and the structural steel yield stress by analyzing of 36 SRC specimens with different structural properties. Generally, it is shown that the effect of the axis of applied bending moment on the strength of SRC specimens is directly proportional to steel contribution ratio. It was observed, also, that in spite of the strength of the analyzed composite columns were increased with the increase in the strength of concrete, but the both effects, the axis of the applied bending moment and the increase of structural steel yield stress, are inversely proportional to the increase of concrete strength. The Predicted strengths of SRC specimens from ANN analysis were compared with that calculated using the EC 4, giving good agreement reached to a ratio around 0. 96...|$|E
40|$|The {{aim of this}} {{research}} is firstly to determine the key risk factors of Supply Chain Management (SCM) and <b>developing</b> <b>an</b> <b>efficient</b> <b>model</b> to assess them. In this work, first the risks involved in SCM has been identified and arranged in a systematic hierarchical structure. Questionnaire surveys have been used for data collection from a managerial decision-making group of a case industry. Next, based on the obtained linguistic data, a fuzzy logic based assessment module has been designed for the evaluation of aggregated SC risks. Finally, various risk factors have been categorized; then ranked using ‘fuzzy maximizing and minimizing fuzzy set theory’ in order to identify/assess the major risk factors that need to be managed or controlled. The present trend in the market is no longer the competition among the enterprises but the supply chain. Supplier selection is the most critical decision of the whole procuring department. Selection of supplier is a complicated decision involving many criteria to take into consideration. In later part, this study tries to rank the suppliers centred on different risks and draw a compromise solution. In order to achieve this, understanding risks is of utmost important. In this work, risks associated with the supplier selection have been recognized and analyzed to rank candidate suppliers based on their affinity to risk using fuzzy based VIKOR method. These risks have varied probability of occurrence and impact on the supply chain. Risks have been represented by linguistic variables and then parameterized by Triangular Fuzzy Number (TFN). Fuzzy risk extent has been calculated and thereby Fuzzy Best Value (FBV) and Fuzzy Worst Value (FWV) have been determined. Fuzzy Utility value has been calculated and utilizing this, ranking has been made by closeness to FBV and farness to FWV. Best alternative has been preferred by maximizing utility group and minimizing regret group...|$|E
40|$|Advanced in the {{fabrication}} technology of integrated circuits (ICs) {{over the last}} couple of years has resulted in an unparalleled expansion of the functionality of microelectronic systems. Today’s ICs feature complex deep-submicron mixed-signal designs and have found numerous applications in industry due to their lower manufacturing costs and higher performance levels. The tendency towards smaller feature sizes and increasing clock rates is placing higher demands on signal integrity design by highlighting previously negligible interconnect effects such as distortion, reflection, ringing, delay, and crosstalk. These effects if not predicted in the early stages of the design cycle can severely degrade circuit performance and reliability. The objective of this thesis is to develop new model order reduction (MOR) techniques to minimize the computational complexity of non-linear circuits and electronic systems that have delay elements. MOR techniques provide a mechanism to generate reduced order models from the detailed description of the original modified nodal analysis (MNA) formulation. The following contributions are made in this thesis: 1. The first project presents a methodology for reduction of Partial Element Equivalent Circuit (PEEC) models. PEEC method is widely used in electromagnetic compatibility and signal integrity problems in both the time and frequency domains. The PEEC model with retardation has been applied to 3 -D analysis but often result in large and dense matrices, which are computationally expensive to solve. In this thesis, a new moment matching technique based on Multi-order Arnoldi is described to model PEEC networks with retardation. 2. The second project deals with <b>developing</b> <b>an</b> <b>efficient</b> <b>model</b> order reduction algorithm for simulating large interconnect networks with nonlinear elements. The proposed methodology is based on a multidimensional subspace method and uses constraint equations to link the nonlinear elements and biasing sources to the reduced order model. This approach significantly improves the simulation time of distributed nonlinear systems, since additional ports are not required to link the nonlinear elements to the reduced order model, yielding appreciable savings in the size of the reduced order model and computational time. 3. A parameterized reduction technique for nonlinear systems is presented. The proposed method uses multidimensional subspace and variational analysis to capture the variances of design parameters and approximates the weakly nonlinear functions as a Taylor series. An SVD approach is presented to address the efficiency of reduced order model. The proposed methodology significantly improves the simulation time of weakly nonlinear systems since the size of the reduced system is smaller than the original system and a new reduced model is not required each time a design parameter is changed...|$|E
30|$|In {{recent years}} many new methods for {{modeling}} and forecasting {{the stock market}} have been developed. Despite these efforts, the forecasting accuracy of these models remains an issue in stock market research. To address this challenge, we <b>developed</b> <b>an</b> <b>efficient</b> <b>model</b> for stock market forecasting that proposes a condensed polynomial neural network (PNN) architecture for predicting stock index closing prices. The model includes partial descriptions (PDs) and is limited to only two layers for the PNN architecture. The outputs of these PDs, along with the original features, are fed to the output layer, which has one neuron. The weight vectors and biases of the CPNN are explored by a GA.|$|R
40|$|Robust {{extraction}} of consensus sets from noisy data {{is a fundamental}} problem in robot vision. Existing multimodel estimation algorithms have shown success on large consensus sets estimations. One remaining challenge is to extract small consensus sets in cluttered multimodel data set. In this article, we present an effective multimodel extraction method to solve this challenge. Our technique is based on smallest consensus set random sampling, which we prove can guarantee to extract all consensus sets larger than the smallest set from input data. We then <b>develop</b> <b>an</b> <b>efficient</b> <b>model</b> competition scheme that iteratively removes redundant and incorrect model samplings. Extensive experiments on both synthetic data and real data with high percentage of outliers and multimodel intersections demonstrate the superiority of our method...|$|R
40|$|International audienceConstraint {{programming}} is a technology {{which is now}} widely used to solve com- binatorial problems in industrial applications. However, using it requires consid- erable knowledge and expertise {{in the field of}} constraint reasoning. This paper introduces a framework for automatically learning constraint networks from sets of instances that are either acceptable solutions or non-desirable assignments of the problem we would like to express. Such an approach {{has the potential to be}} of assistance to a novice who is trying to articulate her constraints. By restricting the language of constraints used to build the network, this could also assist <b>an</b> expert to <b>develop</b> <b>an</b> <b>efficient</b> <b>model</b> of <b>a</b> given problem. This paper provides a theoretical framework for a research agenda in the area of interactive constraint acquisition, automated modelling and automated constraint programming...|$|R
40|$|The {{purpose of}} this paper is to <b>develop</b> <b>an</b> <b>efficient</b> {{computational}} <b>model</b> for Abelian categories of coherent sheaves over certain classes of varieties. These categories are naturally described as Serre quotient categories. Hence, our approach relies on describing general Serre quotient categories in a constructive way which leads to <b>an</b> <b>efficient</b> computer implementation. Comment: updated bibliograph...|$|R
40|$|Abstract—Systems-on-Chips (SoCs) {{integrate}} {{more and}} more heterogeneous components: analog/RF/digital circuits, sensors, actuators, software. For the design of these systems very different description formalisms, or Models of Computation (MoCs), and tools are used for the different subblocks and design stages, which often create interoperability problems. Additionally the verification of a complete SoC is difficult due to huge performance problems. The goal of this Ph. D. work is to <b>develop</b> <b>an</b> <b>efficient</b> <b>modeling</b> and simulation platform that supports the design of mixed-signal SoCs using component models written in different design languages and using different MoCs. One component of this work {{is the development of}} a web-based platform for collecting behavioral models and supporting the design of Analog and Mixed-Signal (AMS) SoCs. Its current state and an outlook on its further development is the focus of this paper. I...|$|R
40|$|In reality {{there are}} many {{relational}} datasets in which both features of instances and the relationships among the instances are recorded, such as hyperlinked web pages, scientific literature with citations, and social networks. Collective classification has been widely used to classify a group of related instances simultaneously. Recently {{there have been several}} studies on statistical relational learning for collective classification, including relational dependency networks, relational Markov networks, and Markov logic networks. In statistical relational learning models, collective classification is usually formulated as an inference problem over graphical models. Hence the existing collective classification methods are expensive due to the iterative inference procedure required for general graphical models. Procedures that learn collective classifiers are also expensive, especially if they are based on iterative optimization of an expensive iterative inference procedure. Our goal is to <b>develop</b> <b>an</b> <b>efficient</b> <b>model</b> for collectiv...|$|R
40|$|The rapid {{advances}} in 3 D scanning and acquisition techniques have {{given rise to}} the explosive increase of volumetric digital models in recent years. This dissertation systematically trailblazes a novel volumetric modeling framework to represent 3 D solids. The need to explore more efficient and robust 3 D modeling framework has gained the prominence. Although the traditional surface representation (e. g., triangle mesh) has many attractive properties, it is incapable of expressing the interior space and materials. Such a serious drawback overshadows many potential modeling and analysis applications. Consequently volumetric modeling techniques become the well-known solution to this problem. Nevertheless, many unsolved research issues remain when <b>developing</b> <b>an</b> <b>efficient</b> <b>modeling</b> paradigm for existing 3 D models: complex geometry (fine details and extreme concaveness), arbitrary topology, heterogenous materials, large-scale data storage and processing, etc. Comment: Master Thesis, Computer Science Department, Stony Brook Universit...|$|R
40|$|As {{storage density}} increases, the {{performance}} of volume holographic storage channels is degraded, because intersymbol interference and noise also increase. Equalization and detection methods must be employed to mitigate the effects of intersignal interference and noise. However, the output detector array in a holographic storage system detects {{the intensity of the}} incident light's wave front, leading to loss of sign information. This sign loss precludes the applicability of conventional equalization and detection schemes. We first address channel modeling under quadratic nonlinearity and <b>develop</b> <b>an</b> <b>efficient</b> <b>model</b> named the discrete magnitude-squared channel model. We next introduce an advanced equalization method called the iterative magnitude-squared decision feedback equalization (IMSDFE), which takes the channel nonlinearity into account. The performance of IMSDFE is quantified for optical-noise-dominated channels as well as for electronic-noise-dominated channels. Results indicate that IMSDFE is a good candidate for a high-density, high-intersignal-interference volume holographic storage channel...|$|R
40|$|Container {{terminal}} (CT) {{operations are}} often bottlenecked by slow YC (yard crane) movements. PM (prime mover) queues {{in front of}} the YCs are common. Hence, efficient YC scheduling to reduce the PM waiting time is critical in increasing a CT&# 039;s throughput. We <b>develop</b> <b>an</b> <b>efficient</b> <b>model</b> for YC scheduling by taking into account realistic operational constraints such as inter-crane interference, fixed YC separation distances and simultaneous container storage/retrievals. Among them, only inter-crane interference has ever been considered in the literature. The model requires far fewer integer variables than the literature by using bi-index decision variables. We show how the model can be solved quickly using heuristics and rolling-horizon algorithm, yielding close to optimal solutions in seconds. The solution quality and solution time are both better than the literature even with additional constraints considered. The proposed formulations and algorithms can be extended to other problems with time windows and space constraints...|$|R
40|$|Abstract: The {{prediction}} of rain fade slope on satellite link is currently an essential parameter used by system designers {{for the development}} of communication power control and error-correction schemes, which mini-mize effects of the link outages for their systems. It is also useful for the implementation of computer generated received signal for primary experiments. Due to its time variation and dependence on the rain type and characteristics, it will be useful to <b>develop</b> <b>an</b> <b>efficient</b> <b>model</b> that fits the fade slope evaluated from the link. Fade slope indicates the rate of change of rain attenuation. The knowledge of fade slope of attenuation caused by rain or other meteorological events are very important for fade mitigation techniques. Knowledge of this parameter is important for determining the required tracking speed of fade mitigation techniques. The fade slope depends on attenuation level, on sampling time and on climatic parameters (drop size distribution and therefore on the type of rain). I...|$|R
40|$|This paper {{applies the}} Quadrature-Method-of-Moments (QMOM) to the polydispersed {{droplets}} spectrums typical in low pressure steam turbines. Various modes of nonequilibrium phase transition {{are present in}} steam turbines, starting with primary and secondary homogeneous nucleation as {{the main source of}} moisture followed by heterogeneous nucleation and surface entrainment sources. The range of phase transition possibilities leads {{to a wide range of}} droplet sizes, which are present under various combinations of inertial and thermal nonequilibrium. Given the extensive prevalence of CFD in turbomachinery design, it is of interest to <b>develop</b> <b>an</b> <b>efficient</b> <b>modeling</b> approach for polydispersed droplet flows that avoids solving an excessive number of equations to represent the droplet size distribution. Methods based on QMOM have shown promise in this regard in other applications areas of two-phase flow, and this paper attempts to quantify its potential for steam turbine applications by applying the method to supersonic nozzle studies with homogeneous and heterogeneous phase transitions...|$|R
40|$|Scattering from {{man-made}} objects in SAR imagery exhibits aspect and frequency dependencies {{which are not}} always well modeled by standard SAR imaging techniques based on the ideal point scattering model. This is particularly the case for high-resolution wide-band and wide-aperture data where model deviations are even more pronounced. If ignored, these deviations will reduce recognition performance due to the model mismatch, but when appropriately accounted for, these deviations from the ideal point scattering model can be exploited as attributes to better distinguish scat-terers and their respective targets. With this in mind, this thesis <b>develops</b> <b>an</b> <b>efficient</b> <b>modeling</b> framework based on a sub-aperture pyramid to utilize scatterer anisotropy {{for the purpose of}} target classification. Two approaches are presented to exploit scatterer anisotropy using the sub-aperture pyramid. The first is a nonparametric classifier that learns the azimuthal dependencies within an image and makes a classification decision based on the learned dependencies. The second approach is a parametric attribution of the observed anisotropy characterizing the azimuthal location and concentration o...|$|R
40|$|Rationale Developing {{models to}} {{efficiently}} explore the {{mechanisms by which}} stress can mediate reinstatement of drug-seeking behavior {{is crucial to the}} development of new pharmacotherapies for alcohol use disorders. Objectives We examined the effects of multiple reinstatement sessions using the pharmacological stressor, yohimbine, in ethanol- and sucrose-seeking rats in order to <b>develop</b> <b>a</b> more <b>efficient</b> <b>model</b> of stress-induced reinstatement. Methods Long–Evans rats were trained to self-administer 10...|$|R
40|$|The {{increasing}} {{incidence of}} hepatocellular carcinoma in Western countries {{has led to}} an expanding interest of scientific research in this field. Therefore, a vast need of experimental models that mimic the natural pathogenesis of hepatocellular carcinoma (HCC) in a short time period is present. The goal of our study was (1) to <b>develop</b> <b>an</b> <b>efficient</b> mouse <b>model</b> for HCC research, in which tumours <b>develop</b> in <b>a</b> natural background of fibrosis and (2) to assess the time-dependent angiogenic changes in the pathogenesis of HCC...|$|R
40|$|A reduced-order-based {{representation}} of the Schrödinger equation is investigated for electron wave functions in semiconductor nanostructures. In this representation, the Schrödinger equation is projected onto an eigenspace described by {{a small number of}} basis functions that are generated from the proper orthogonal decomposition (POD). The approach substantially reduces the numerical degrees of freedom (DOF’s) needed to numerically solve the Schrödinger equation for the wave functions and eigenstate energies in a quantum structure and offers an accurate solution as detailed as the direct numerical simulation of the Schrödinger equation. To <b>develop</b> such <b>an</b> approach, numerical data accounting for parametric variations of the system are used to perform decomposition in order to generate the POD eigenvalues and eigenvectors for the system. This approach is applied to develop POD models for single and multiple quantum well structure. Errors resulting from the approach are examined in detail associated with the selected numerical DOF’s of the POD model and quality of data used for generation of the POD eigenvalues and basis functions. This study investigates the fundamental concepts of the POD approach to the Schrödinger equation and paves <b>a</b> way toward <b>developing</b> <b>an</b> <b>efficient</b> <b>modeling</b> methodology for large-scale multi-block simulation of quantum nanostructures...|$|R
40|$|Toward <b>efficient</b> <b>modeling</b> of fuzzy expert systems: {{a survey}} O zvýšení účinnosti modelování fuzzy expertních systémů S. ALY, I. VRANA Czech University of Agriculture, Prague, Czech Republic Abstract: <b>Efficient</b> <b>modeling</b> of the {{artificial}} intelligence tools {{has become a}} necessity in order to cut down the development and maintenance cost associated with building application systems in the business, industrial and agriculture sectors that are frequently amendable to sudden unexpected environmental and economic conditions changes. This can be accomplished through <b>developing</b> <b>an</b> <b>efficient</b> <b>modeling</b> language which exploits the beneficial features of the emerging object-oriented technology. This research is aimed at reviewing the recent scientific aspects of the research concerning conceptual modeling of fuzzy knowledge-based system, which exhibits a large extent of applicability in last few decades due to its capability to deal with vagueness, uncertainty and subjectivity, those are inherent in real world problems. The most recent researches and applications of fuzzy expert system are surveyed. The existing knowledge modeling techniques are reviewed and the prominent ones are pinpointed. This paper is intended to identify the main and common bottlenecks of the existing knowledge modeling tools to overcome it in <b>developing</b> <b>a</b> reliable conceptual model of fuzzy expert system. Key words: fuzzy expert systems, knowledge modeling, object-orientatio...|$|R
40|$|International audienceA supply {{planning}} for multilevel serial production systems under lead time uncertainties is considered. The techniques {{used in industry}} are often {{based on the assumption}} that the lead times are known. However, in a supply chain the lead times are often random variables. Therefore, it is necessary to find the best values of the planned lead times minimising the total cost. It is supposed that the demand for the finished product and its due date are known. It is assumed also that the component lead time at each level is a random discrete variable. No restrictive hypothesis is made on such random variables; only that one supposes that the distribution probabilities are known. Therefore, we <b>develop</b> <b>an</b> <b>efficient</b> <b>model</b> to aid in MRP parameterization under lead time uncertainties, more precisely to calculate planned lead times when the component procurement times are random. The aim is to find the values of planned lead times which minimize the total cost and satisfy customers. We consider the criterion to minimize the average holding and tardiness costs at each level while respect the customer service level. The proposed approach is based on a mathematical model of this problem with discrete decision variables. Several properties of the objective function are proven...|$|R
40|$|The aims of {{this work}} are to <b>develop</b> <b>an</b> <b>efficient</b> <b>modeling</b> method for {{establishing}} dynamic output probability density function (PDF) models using measurement data and to investigate predictive control strategies for controlling the full shape of output PDF rather than the key moments. Using the rational square-root (RSR) B-spline approximation, a new modeling algorithm is proposed in which the actual weights are used instead of the pseudo weights in the weights dynamic model. This replacement can reduce computational load effectively in data-based modeling of a high-dimensional output PDF model. The use of the actual weights in modeling and control has been verified by stability analysis. A predictive PDF model is then constructed, based on which predictive control algorithms are established with the purpose to drive the output PDF towards the desired target PDF over the control process. An analytical solution is obtained for the non-constrained predictive PDF control. For the constrained predictive control, the optimal solution is achieved via solving a constrained nonlinear optimization problem. The integrated method of data-based modeling and predictive PDF control is applied to closed-loop control of molecular weight distribution (MWD) in an exemplar styrene polymerization process, through which the modeling efficiency and the merits of predictive control over standard PDF control are demonstrated and discussed...|$|R
5|$|An {{improved}} {{mathematical analysis}} of Cassini data by astrophysicists Matthew Holman and Matthew Payne tightened the constraints on possible locations of Planet Nine. Holman and Payne <b>developed</b> <b>a</b> more <b>efficient</b> <b>model</b> {{that allowed them}} to explore a broader range of parameters than the previous analysis. The parameters identified using this technique to analyze the Cassini data was then intersected with Batygin and Brown's dynamical constraints on Planet Nine's orbit. Holman and Payne concluded that Planet Nine is most likely to be located within 20° of RA = 40°, Dec = −15°, in an area of the sky near the constellation Cetus.|$|R
40|$|Abstract. Scattering from {{man-made}} objects in SAR imagery exhibits aspect and frequency dependencies {{which are not}} well modeled by standard SAR imaging techniques. If ignored, these deviations will reduce recognition performance due to the model mismatch, but when appropriately accounted for, these deviations from the ideal point scattering model can be exploited as attributes to better distinguish scatterers and their respective targets. With this premise in mind, we have <b>developed</b> <b>an</b> <b>efficient</b> <b>modeling</b> framework that incorporates scatterer anisotropy. One of the products of our analysis is the assignment of an anisotropy label to each scatterer conveying the degree of anisotropy. Anisotropic behavior is commonly predicted for geometric scatterers (scatterers with a simple geometric structure), {{but it may also}} arise from volumetric scatterers (random arrangements of interfering point scatterers). Analysis of anisotropy arising from these two modalities shows a clear source-dependent relationship between the anisotropy classification and parameters of the scatterer. In particular, the degree of anisotropy is closely related {{to the size of the}} scatterer, and increasing the aperture size reduces the incidence of volumetric anisotropy but preserves the detection rate for geometric anisotropy. This result helps to address the question in the SAR community regarding the utility of wide-aperture SAR data for ATR since wide-aperture data reveals geometric anisotropy while resolving volumetric anisotropy into individual isotropic scatterers...|$|R
40|$|Data stream {{management}} systems {{need to control}} their resources adaptively since stream characteristics as well as query workload vary over time. In this paper we investigate an approach to adaptive resource management for continuous sliding window queries that adjusts window sizes and time granularities to keep resource usage within bounds. In order to quantify the impact of both methods on a query plan, we <b>develop</b> <b>an</b> <b>efficient</b> cost <b>model</b> for estimating the resource allocation in terms of memory usage and processing costs. A thorough experimental study not only validates the accuracy of our cost model but also demonstrates the efficacy and scalability of the applied methods. ...|$|R
40|$|Abstract—This paper {{presents}} a discrete-time model for simulation of woodwind toneholes {{in a musical}} sound synthesis context. Starting from a lumped element approximation of the partially open tonehole, we <b>develop</b> <b>an</b> <b>efficient</b> digital tonehole <b>model</b> with dynamically adjustable tonehole state. The model, that covers {{a wider range of}} woodwind toneholes than those previously reported, is discretised with the use of wave digital filter techniques...|$|R
40|$|Social {{networks}} play {{a fundamental}} {{role in the}} diffusion of infor-mation. However, there are two different ways of how information reaches a person in a network. Information reaches us through con-nections in our social networks, as well as through the influence external out-of-network sources, like the mainstream media. While most present models of information adoption in networks assume information only passes from a node to node via the edges of the underlying network, the recent availability of massive online social media data allows us to study this process in more detail. We present a model in which information can reach a node via the links of the social network or through the influence of external sources. We then <b>develop</b> <b>an</b> <b>efficient</b> <b>model</b> parameter fitting tech-nique and apply the model to the emergence of URL mentions in the Twitter network. Using a complete one month trace of Twitter we study how information reaches the nodes of the network. We quantify the external influences over time and describe how these influences affect the information adoption. We discover that the in-formation tends to “jump ” across the network, which can only be explained as an effect of an unobservable external influence on the network. We find that only about 71 % of the information volume in Twitter can be attributed to network diffusion, and the remaining 29 % is due to external events and factors outside the network...|$|R
40|$|As {{the stock}} market data is non-stationary and {{volatile}} the investors feel insecure during investing. In the recent years lots of attention {{has been devoted to}} the analysis and prediction of future values and trends of the financial market. In recent years, mathematical methodology has been used by financial experts and brokers. This study presented Neural Network (NN) approach to <b>develop</b> <b>an</b> <b>efficient</b> <b>model</b> for stock price prediction. Financial ratios were included Earnings Per Share (EPS), Prediction Earnings Per Share (PEPS), Dividend Per Share (DPS), price-earnings ratio (P/E) and earnings-price ratio (E/P) which were extracted from Tehran stock exchange during a decennial period (2000 - 2009). The training and testing sets were used to develop the NN model. The developed models were subjected to a sensitivity analysis test to assess the relative importance of input variable on model output. Quantitative examination of the goodness of fit for the predictive models was made using R 2 and error measurement indices commonly used to evaluate forecasting models. Statistical performance of the developed NN model revealed close agreement between observed and predicted values of stock price, indicates that MLP type NN appears as a promising method for modeling the relationship between financial indices and stock price. The sensitivity analysis indicated that the stock price was more sensitive to DPS followed by EPD, PEPS, E/P and P/E, respectively. In conclusion, the developed NN model could satisfactorily predicted the stock price based on financial indices. Moreover, these models can serve as useful option in determining the relative importance of input variables on model output...|$|R
40|$|Joint Air Operations (JAO) are {{traditionally}} orchestrated using static vehicle roles assigned from command and control. With {{recent advances in}} information and communication technology and the increased need for a dynamic and flexible response, vehicles are expected to assume multiple roles {{over the course of}} a mission. In addition, this level of flexibility requires the capability for a vehicle to determine when to facilitate network communication. In this study, we <b>develop</b> <b>an</b> <b>efficient</b> mathematical <b>model</b> that can be used to dynamically assign vehicles to roles including the role of communication in a threat-filled environment. We compute rewards for role assignment based on the marginal benefit to the system and the risk to the individual vehicle. These rewards are utilized within <b>an</b> <b>efficient</b> network optimization formulation to allocate vehicle roles. Key words...|$|R
40|$|A {{biological}} {{sequence is}} a single, continuous molecule of nucleic acid or protein. Classical methods for the Multiple Longest Common Subsequence problem (MLCS) problem {{are based on}} dynamic programming. The Multiple Longest Common Subsequence problem (MLCS) is used to find the longest subsequence shared between two or more strings. For over 30 years, significant efforts {{have been made to}} find efficient algorithms for the MLCS problem. Many of them have been proposed for the general case of any given number of strings. They could benefit greatly from improving their computation times. (Qingguo et al.,) proposed a new algorithm for the general case of Multiple LCS problem, which is finding the LCS of any number of strings. This algorithm is based on the dominant point approach and employs a fast divide-and-conquers technique to compute the dominant points. From this existing work, it is observed that, when this approach is applied to a case of three strings, this algorithm demonstrates the same performance as the fastest existing MLCS algorithm. When applied to more than three strings, this technique is significantly faster than the existing sequential methods, reaching up to 2 - 3 orders of magnitude faster speed on large-size problems. However, from our experimental results, it is observed that as the size of the Data Set is increasing, its performance decreases in terms of execution time. To overcome this major issue, we have <b>developed</b> <b>an</b> <b>efficient</b> <b>model</b> called Cache Oblivious based Multiple Longest Common Subsequence (CMLCS). From our experimental results, it is observed that our proposed work performs better as compared with the existing system in terms of Execution Time and Memory Usage...|$|R
40|$|We show {{experimentally}} that a cheap glass microcapillary can accumulate λ-phage DNA at its tip {{and deliver}} the DNA into the capillary {{using a combination}} of electro-osmotic flow, pressure-driven flow, and electrophoresis. We <b>develop</b> <b>an</b> <b>efficient</b> simulation <b>model</b> for this phenomenon based on the electrokinetic equations and the finite-element method. Using our model, we explore the large parameter space of the trapping mechanism by varying the salt concentration, the capillary surface charge, the applied voltage, the pressure difference, and the mobility of the analyte molecules. Our simulation results show that this system can be tuned to capture a wide range of analyte molecules, such as DNA or proteins, based on their electrophoretic mobility. Our method for separation and pre-concentration of analytes has implications for the development of low-cost lab-on-a-chip devices. Comment: 9 pages, 4 figure...|$|R
