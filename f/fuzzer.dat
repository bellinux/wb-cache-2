95|50|Public
2500|$|John Regehr – {{developed}} the C compiler <b>fuzzer</b> Csmith, the Clang C compiler integer overflow sanitizer, and widely-read blog ...|$|E
5000|$|BFF (Basic <b>Fuzzer</b> Framework) -- a mutational file <b>fuzzer</b> for Linux ...|$|E
50|$|A generation-based <b>fuzzer</b> generates inputs from scratch. For instance, a smart generation-based <b>fuzzer</b> {{takes the}} input model that was {{provided}} by the user to generate new inputs. Unlike mutation-based fuzzers, a generation-based <b>fuzzer</b> does not depend on the existence or quality of a corpus of seed inputs.|$|E
30|$|Generally, OS kernels are fuzzed by {{randomly}} calling kernel API functions with randomly generated parameter values. According to {{the focus}} of <b>fuzzers,</b> kernel <b>fuzzers</b> could {{be divided into two}} categories: knowledge based <b>fuzzers</b> and coverage guided <b>fuzzers.</b>|$|R
30|$|<b>Fuzzers</b> {{could be}} {{classified}} as dumb fuzz and smart fuzz according to whether there is a feedback between the monitoring of program execution state and testcase generation. Smart <b>fuzzers</b> adjustment the generation of testcases according to the collected information that how testcases affect the program behavior. For mutation based <b>fuzzers,</b> feedback information could be used to determine which part of testcases should be mutated and the way to mutate them. Dumb <b>fuzzers</b> acquires a better testing speed, while smart <b>fuzzers</b> generate better testcases and gain a better efficiency.|$|R
30|$|Traditional <b>fuzzers</b> usually utilize {{a random}} based fuzzing {{strategy}} in practice. Limitations of program analysis techniques {{result in a}} present situation that <b>fuzzers</b> are not smart enough. Thus fuzzing test still faces many challenges. We list some key challenges as follows.|$|R
50|$|A white-box <b>fuzzer</b> leverages program {{analysis}} to systematically increase code coverage or to reach certain critical program locations. For instance, SAGE leverages symbolic execution to systematically explore different paths in the program.If the program's specification is available, a whitebox <b>fuzzer</b> might leverage techniques from model-based testing to generate inputs {{and check the}} program outputs against the program specification.A whitebox <b>fuzzer</b> can be very effective at exposing bugs that hide deep in the program. However, the time used for analysis (of the program or its specification) can become prohibitive. If the whitebox <b>fuzzer</b> takes relatively too long to generate an input, a blackbox <b>fuzzer</b> will be more efficient. Hence, there are attempts to combine the efficiency of blackbox fuzzers {{and the effectiveness of}} whitebox fuzzers.|$|E
5000|$|FOE (Failure Observation Engine) -- a mutational file <b>fuzzer</b> for Windows ...|$|E
50|$|A gray-box <b>fuzzer</b> leverages {{instrumentation}} {{rather than}} program analysis to glean {{information about the}} program. For instance, AFL and libFuzzer utilize lightweight instrumentation to trace basic block transitions exercised by an input. This leads to a reasonable performance overhead but informs the <b>fuzzer</b> about the increase in code coverage during fuzzing, which makes gray-box fuzzers extremely efficient vulnerability detection tools.|$|E
30|$|<b>Fuzzers</b> can be {{classified}} in various ways.|$|R
50|$|Some <b>fuzzers</b> {{have the}} {{capability}} to do both, to generate inputs from scratch and to generate inputs by mutation of existing seeds.|$|R
30|$|In this paper, {{we try to}} {{summarize}} the state-of-the-art fuzzing solution, and how they improve the effectiveness and efficiency of vulnerability discovery. Besides, we show how traditional techniques can help improving the effectiveness and efficiency of fuzzing, and make <b>fuzzers</b> smarter. Then, we give an overview of how state-of-the-art <b>fuzzers</b> detect vulnerabilities of different targets, including file format applications, kernels, and protocols. At last, we try to point out new trends of how fuzzing technique develops.|$|R
5000|$|A dumb <b>fuzzer</b> {{does not}} require the input model and can thus be {{employed}} to fuzz a wider variety of programs. For instance, AFL is a dumb mutation-based <b>fuzzer</b> that modifies a seed file by flipping random bits, by substituting random bytes with [...] "interesting" [...] values, and by moving or deleting blocks of data. However, a dumb <b>fuzzer</b> might generate a lower proportion of valid inputs and stress the parser code rather than the main components of a program. The disadvantage of dumb fuzzers can be illustrated by means of the construction of a valid checksum for a cyclic redundancy check (CRC). A CRC is an error-detecting code that ensures that the integrity of the data contained in the input file is preserved during transmission. A checksum is computed over the input data and recorded in the file. When the program processes the received file and the recorded checksum does not match the re-computed checksum, then the file is rejected as invalid. Now, a <b>fuzzer</b> that is unaware of the CRC is unlikely to generate the correct checksum. However, there are attempts to identify and re-compute a potential checksum in the mutated input, once a dumb mutation-based <b>fuzzer</b> has modified the protected data.|$|E
5000|$|A black-box <b>fuzzer</b> {{treats the}} program as a black box and is unaware of {{internal}} program structure. For instance, a random testing tool that generates inputs at random is considered a blackbox <b>fuzzer.</b> Hence, a blackbox <b>fuzzer</b> can execute several hundred inputs per second, can be easily parallelized, and can scale to programs of arbitrary size. However, blackbox fuzzers may only scratch the surface and expose [...] "shallow" [...] bugs. Hence, there are attempts to develop blackbox fuzzers that can incrementally learn about the internal structure (and behavior) of a program during fuzzing by observing the program's output given an input. For instance, LearnLib employs active learning to generate an automaton that represents {{the behavior of a}} web application.|$|E
5000|$|A <b>fuzzer</b> can be dumb or smart {{depending}} on whether it is aware of input structure, and ...|$|E
30|$|<b>Fuzzers</b> {{monitor the}} {{execution}} state during {{the execution of}} target programs, expecting exception and crashes. Common used exception monitoring methods includes monitoring on specific system signals, crashes, and other violations. For violations without intuitive program abnormal behaviors, lots of tools could be used, including AddressSanitizer (Serebryany et al. 2012), DataFlowsanitizer (The Clang Team 2017 a), ThreadSanitizer (Serebryany and Iskhodzhanov 2009), LeakSanitizer (The Clang Team 2017 b), etc. When violations are captured, <b>fuzzers</b> store the corresponding testcase for latter replay and analysis.|$|R
30|$|The {{challenge}} {{of how to}} mutate seed inputs. Mutation based generation strategy is widely used by state-of-the-art <b>fuzzers</b> for its convenience and easy set up. However, how to mutate and generate testcases that capable to cover more program paths and easier to trigger bugs is a key challenge (Yang et al. 2007). Specifically, mutation based <b>fuzzers</b> need to answer two questions when do mutation: (1) where to mutate, and (2) how to mutate. Only mutation on a few key positions would affect the control flow of the execution. Thus how to locate these key positions in testcases is of great importance. Besides, the way <b>fuzzers</b> mutate the key positions is another key problem,i.e, how to determine the value that could direct the testing to interesting paths of programs. In short, blind mutation of testcases result in serious waste of testing resource and better mutation strategy could significantly improve the efficiency of fuzzing.|$|R
30|$|We {{implemented}} fuzzing swarms by parallel {{execution of}} multiple individual <b>fuzzers</b> which are clustered and relocated {{according to the}} algorithm described in Section 6. For clustering, we apply the Lloyd k-means algorithm.|$|R
5000|$|A <b>fuzzer</b> can be white-, grey-, or black-box, {{depending}} on whether it is aware of program structure.|$|E
50|$|A smart (model-based, grammar-based, or protocol-based) <b>fuzzer</b> leverages {{the input}} model to {{generate}} {{a greater proportion of}} valid inputs. For instance, if the input can be modelled as an abstract syntax tree, then a smart mutation-based <b>fuzzer</b> would employ random transformations to move complete subtrees from one node to another. If the input can be modelled by a formal grammar, a smart generation-based <b>fuzzer</b> would instantiate the production rules to generate inputs that are valid w.r.t. the grammar. However, generally the input model must be explicitly provided which is difficult when it is proprietary, unknown, or very complex. If a large corpus of valid and invalid inputs are available, a grammar induction technique, such as Angluin's L* algorithm would be able {{to generate a}}n input model.|$|E
5000|$|A <b>fuzzer</b> can be generation-based or mutation-based {{depending}} on whether inputs are generated from scratch or by modifying existing inputs, ...|$|E
30|$|During {{the process}} of fuzzing, <b>fuzzers</b> track the {{execution}} via various methods. Basically, <b>fuzzers</b> track the execution for two purposes, the code coverage and security violations. The code coverage information is used to pursue a thorough program state exploration, and the security violation tracking is for better bug finding. As detailed in the previous subsections, AFL tracks the code coverage through code instrumentation and AFL bitmap. Security violations tracking could be processed {{with the help of}} lots of sanitizers, such as AddressSanitizer (Serebryany et al. 2012), ThreadSanitizer (Serebryany and Iskhodzhanov 2009), LeakSanitizer (The Clang Team 2017 b), etc.|$|R
40|$|International audienceFuzzing (aka Fuzz-Testing) {{consists}} of automatically creating and evaluating inputs towards discovering vulnerabilities. Traditional undirected fuzzing may get stuck into one direction and thus {{may not be}} efficient in finding {{a broad range of}} local optima. In this work, we combine artificial intelligence and security testing techniques to guide the fuzzing via an evolutionary algorithm. Our work is the first application of a genetic algorithm for black-box fuzzing for vulnerability detection. We designed heuristics for fuzzing PDF interpreters searching for memory corruption vulnerabilities and for fuzzing websites for cross site scripting. Our evolutionary <b>fuzzers</b> ShiftMonkey and KameleonFuzz outperform traditional black-box <b>fuzzers</b> both in vulnerability detection capabilities and efficiency...|$|R
30|$|A fuzzing test {{starts from}} the {{generation}} {{of a bunch of}} program inputs, i.e., testcases. The quality of generated testcases directly effects the test effects. The inputs should meet the requirement of tested programs for the input format as far as possible. While on the other hand, the inputs should be broken enough so that processing on these inputs would very likely to fail the program. According to the target programs, inputs could be files with different file formats, network communication data, executable binaries with specified characteristics, etc. How to generate broken enough testcases is a main challenge for <b>fuzzers.</b> Generally, two kind of generators are used in state-of-the-art <b>fuzzers,</b> generation based generators and mutation based generators.|$|R
50|$|Typically, a <b>fuzzer</b> is {{considered}} more effective if it achieves {{a higher degree}} of code coverage. The rationale is, if a <b>fuzzer</b> does not exercise certain structural elements in the program, then it is also not able to reveal bugs that are hiding in these elements. Some program elements are considered more critical than others. For instance, a division operator might cause a division by zero error, or a system call may crash the program.|$|E
50|$|In 1995, a <b>fuzzer</b> {{was used}} to test GUI-based tools (such as the X Window System), network protocols, and system library APIs.|$|E
5000|$|John Regehr - {{developed}} the C compiler <b>fuzzer</b> Csmith, the Clang C compiler integer overflow sanitizer, and widely-read blog Embedded in Academia ...|$|E
30|$|This {{paper is}} an {{extension}} of Hunting Bugs with Lévy Flight Foraging [19]. The prevalent method used for binary vulnerability detection is random test generation, also called fuzzing. Here, inputs are randomly generated and injected into the target program with the aim to gain maximal code coverage in the execution graph and drive the program to an unexpected and exploitable state. There is a rich diversity of fuzzing tools available, each focusing on specialized approaches. Multiple taxonomies for random test generation techniques have been proposed, and the most common is classification into mutational or generational fuzzing. Mutation <b>fuzzers</b> are unaware of the input format and mutate the whole range of input variables blindly. In contrast, generation <b>fuzzers</b> take the input format into account and generate inputs according to the format definition. For example, generation <b>fuzzers</b> can be aware of the file formats accepted by a program under test or the network protocol definition processed by a network stack implementation. We can further classify random test generation methods into black-box or white-box fuzzing, depending on the awareness of execution traces of generated inputs. We refer to [6, 7] for a comprehensive account.|$|R
30|$|Fuzzing {{has been}} used to detect {{vulnerabilities}} on massive applications since its appearance. According to characteristics of different target applications, different <b>fuzzers</b> and different strategies are used in practice. In this section, we present and summarize several mainly fuzzed types of applications.|$|R
30|$|C. How {{to select}} seed from the pool? <b>Fuzzers</b> {{repeatedly}} select seed from seed pool to mutate {{at the beginning}} of a new round test in the main fuzzing loop. How to select seed from the pool is another important open problem in fuzzing. Previous work has prove that good seed selection strategy could significantly improve the fuzzing efficiency and help find more bugs, faster (Rawat et al. 2017; Böhme et al. 2017, 2017; Wang et al. 2017). With good seed selection strategies, <b>fuzzers</b> could (1) prioritize seeds which are more helpful, including covering more code and be more likely to trigger vulnerabilities, (2) reduce the waste of repeatedly execution of paths and save computing resource, (3) optimally select seeds that cover deeper and more vulnerable code and help identifying hidden vulnerabilities faster. AFL prefers smaller and faster testcases to pursue a fast testing speed.|$|R
50|$|Csmith is a free, open source, permissively {{licensed}} C compiler <b>fuzzer</b> {{developed by}} researchers at the University of Utah. It was previously called Randprog.|$|E
50|$|A mutation-based <b>fuzzer</b> leverages an {{existing}} corpus of seed inputs during fuzzing. It generates inputs by modifying (or rather mutating) the provided seeds. For example, when fuzzing the image library libpng, the user {{would provide a}} set of valid PNG image files as seeds while a mutation-based <b>fuzzer</b> would modify these seeds to produce semi-valid variants of each seed. The corpus of seed files may contain thousands of potentially similar inputs. Automated seed selection (or test suite reduction) allows to pick the best seeds {{in order to maximize}} the total number of bugs found during a fuzz campaign.|$|E
5000|$|To make a <b>fuzzer</b> more {{sensitive}} to failures other than crashes, sanitizers {{can be used to}} inject assertions that crash the program when a failure is detected. There are different sanitizers for different kinds of bugs: ...|$|E
40|$|This paper {{presents}} an instrumentation framework for assessing and improving fuzzing, a powerful technique to rapidly detect software vulnerabilities. We address the major current limitation of fuzzing techniques, namely {{the absence of}} evaluation metrics {{and the absence of}} automated quality assessment techniques for fuzzing approaches. We treat the fuzzing process as a signal and show how derived measures like power and entropy can give an insightful perspective on a fuzzing process. We demonstrate how this perspective can be used to compare the efficiency of several <b>fuzzers,</b> derive stopping conditions for a fuzzing process, or help to identify good candidates for input data. We show through the Linux implementation of our instrumentation framework how the approach was successfully used to assess two different <b>fuzzers</b> on real applications. Our instrumentation framework leverages a tainted data approach and uses data lifetime tracing with an underlying tainted data graph structure...|$|R
50|$|Code {{injection}} vulnerabilities (injection flaws) {{occur when}} an application sends untrusted data to an interpreter. Injection flaws {{are most often}} found in SQL, LDAP, XPath, or NoSQL queries; OS commands; XML parsers, SMTP headers, program arguments, etc. Injection flaws tend to be easier to discover when examining source code than via testing. Scanners and <b>fuzzers</b> can help find injection flaws.|$|R
5000|$|In {{addition}} to [...] and tools {{that can be}} used for binary instrumentation, american fuzzy lop features utility programs meant for monitoring of the fuzzing process. Apart from that, there is [...] and , which can be used for test case and test corpus minimization. This can be useful when the test cases generated by [...] would be used by other <b>fuzzers.</b>|$|R
