171|0|Public
50|$|Another {{approach}} {{followed was}} the <b>generate-and-test</b> approaches where a trial route is generated and tested for satisfaction of constraints. This also leads to unpredictable results where each route {{has to be}} validated.|$|E
5000|$|This {{is another}} example of the <b>generate-and-test</b> organization. The choice rule in Line 1 [...] "generates" [...] all sets {{consisting}} of [...] vertices. The constraint in Line 2 [...] "weeds out" [...] the sets that are not cliques.|$|E
50|$|In {{parallel}} {{to work in}} ILP, Koza proposed genetic programming in the early 1990s as a <b>generate-and-test</b> based approach to learning programs. The idea of genetic programming was further developed into the inductive programming system ADATE and the systematic-search-based system MagicHaskeller. Here again, functional programs are learned from sets of positive examples together with an output evaluation (fitness) function which specifies the desired input/output behavior of the program to be learned.|$|E
5000|$|... {{provided}} the first {{polynomial time algorithm}} for finding a minimum weight basis, in graphs for which every edge weight is positive. His algorithm uses this <b>generate-and-test</b> approach, but restricts the generated cycles to a small set of [...] cycles, called Horton cycles. A Horton cycle is a fundamental cycle of a shortest path tree of the given graph. There are n different shortest path trees (one for each starting vertex) and each has fewer than m fundamental cycles, giving the bound on {{the total number of}} Horton cycles. As Horton showed, every cycle in the minimum weight cycle basis is a Horton cycle. Using Dijkstra's algorithm to find each shortest path tree and then using Gaussian elimination to perform the testing steps of the greedy basis algorithm leads to a polynomial time algorithm for the minimum weight cycle basis.Subsequent researchers have developed improved algorithms for this problem, reducing the worst-case time complexity for finding a minimum weight cycle basis in a graph with [...] edges and [...] vertices to [...]|$|E
40|$|This paper {{concerns}} {{the exploitation of}} user transparent inherent parallelism of pure Prolog programs using program transformation. We describe a novel paradigm enumerate-and-filter for transforming <b>generate-and-test</b> programs for execution under the committed-choice model extended to incorporate multiple solutions based on set enumeration. The paradigm simulates OR-parallelism by stream AND-parallelism integrating OR-parallelism, AND-parallelism, and stream parallelism. <b>Generate-and-test</b> programs are classified into three categories:simple <b>generate-and-test,</b> recursively embedded <b>generate-and-test,</b> and deeply intertwined <b>generate-and-test.</b> The intermediate programs are further transformed to reduce structure copying and metacalls. Algorithms are presented and demonstrated by transforming the representative examples of different classes of <b>generate-and-test</b> programs to Flat Concurrent Prolog equivalents. Statistics show that the techniques are efficient...|$|E
40|$|<b>Generate-and-test</b> {{algorithms}} {{to solve}} constraint satisfaction problems are often inefficient, {{but can be}} constructed fairly easily by knowledge compilation techniques that convert declarative problem knowledge and domain knowledge into a procedural format [Liew and Tong, 1987]. Current research is focusing on methods to improve the efficiency of <b>generate-and-test</b> algorithms by completely incorporating local constraints into generators of parts of composite solutions [Braudaway, 1988]. More global constraints on multiple parts cannot necessarily {{be incorporated into the}} part generators. Their satisfaction must be ensured in a different way. We describe an (unimplemented) method for transforming a <b>generate-and-test</b> algorithm into a generate-test-and-patch algorithm that efficiently hillclimbs toward a solution satisfying a particular global constraint. Our method is based on constructing an evaluation function from the global constraint, that reflects the &quot;degree&quot; to which the constraint has been satisfied. Some of the steps in this method rely on categorizing the global constraint into a generic class. In this paper, the constraint classes on which we focus are quota-meeting and covering constraints. We illustrate the general approach by applying it to a simple <b>generate-and-test</b> algorithm for house floorplanning. We provide empirical results that corroborate our claim that the efficiency of the algorithm has been significantly improved...|$|E
40|$|Previous work {{identifying}} depth-optimal n-channel sorting {{networks for}} 9 ≤ n ≤ 16 {{is based on}} exploiting symmetries {{of the first two}} layers. However, the naive <b>generate-and-test</b> approach typically applied does not scale. This paper revisits the problem of generating two-layer prefixes modulo symmetries. An improved notion of symmetry is provided and a novel technique based on regular languages and graph isomorphism is shown to generate the set of non-symmetric representations. An empirical evaluation demonstrates that the new method outperforms the <b>generate-and-test</b> approach by orders of magnitude and easily scales until n= 40...|$|E
40|$|Inductive {{programming}} {{is concerned with}} the automated construction of declarative, often functional, recursive programs from incomplete specifications such as input/output examples. The inferred program must be correct with respect to the provided examples in a generalising sense: it should be neither equivalent to it, nor inconsistent. Inductive programming algorithms are guided explicitly or implicitly by a language bias (the class of programs that can be induced) and a search bias (determining which generalised program is constructed first). Induction strategies are either <b>generate-and-test</b> or example-driven. In <b>generate-and-test</b> approaches, hypotheses about candidate programs are generated independently from the given specifications. Program candidates are tested against the given specification and {{one or more of the}} best evaluated candidates are developed further. In analytical approaches, candidate programs are constructed in an example-driven way. While <b>generate-and-test</b> approaches can – in principle – construct any kind of program, analytical approaches have a more limited scope. On the other hand, efficiency of induction is much higher in analytical approaches. Inductive {{programming is}} still mainly a topic of basic research, exploring how the intellectual ability of humans to infer generalised recursive procedures from incomplete evidence can be captured in the form of synthesis methods. Intended applications are mainly in the domain of programming assistanc...|$|E
40|$|Popular {{algorithms}} for feature matching {{and model}} extraction {{fall into two}} broad categories, <b>generate-and-test</b> and Hough transform variations. However, both methods suffer from problems in practical implementations. <b>Generate-and-test</b> methods are sensitive to noise in the data. They often fail when the generated model fit is poor due to error in the selected features. Hough transform variations are somewhat less sensitive the noise, but implementations for complex problems suffer from large time and space requirements and the detection of false positives. This paper describes a general method for solving problems where a model is extracted from or fit to data that draws benefits from both <b>generate-and-test</b> methods and those based on the Hough transform, yielding a method superior to both. An {{important component of the}} method is the subdivision of the problem into many subproblems. This allows efficient generateand-test techniques to be used, including the use of randomization {{to limit the number of}} subproblems that must be examined. Each subproblem is solved using pose space analysis techniques similar to the Hough transform, which lowers the sensitivity of the method to noise. This strategy is easy to implement and results in practical algorithms that are efficient and robust. We apply this method to object recognition, geometric primitive extraction, robust regression, and motion segmentation. ...|$|E
40|$|This paper {{studies the}} problem of {{frequent}} pattern mining with uncertain data. We will show how broad classes of algorithms can be extended to the uncertain data setting. In particular, we will study candidate <b>generate-and-test</b> algorithms, hyper-structure algorithms and pattern growth based algorithms. One of our insightful observations is that the experimental behavior of different classes of algorithms is very different in the uncertain case {{as compared to the}} deterministic case. In particular, the hyper-structure and the candidate <b>generate-and-test</b> algorithms perform much better than tree-based algorithms. This counter-intuitive behavior is an important observation from the perspective of algorithm design of the uncertain variation of the problem. We will test the approach on a number of real and synthetic data sets, and show the effectiveness of two of our approaches over competitive techniques...|$|E
30|$|Evolutionary {{algorithms}} are heuristic <b>generate-and-test</b> algorithms {{that mimic}} biological evolution {{by natural selection}} (Eiben and Smith 2015, p.  5). The task of creating a fuzzy rule set that enables us to violate group anonymity is a complex one, therefore utilizing evolutionary algorithms is a suitable approach to solving this problem.|$|E
40|$|Abstract. Popular {{algorithms}} for feature matching {{and model}} extraction {{fall into two}} broad categories: generateand-test and Hough transform variations. However, both methods suffer from problems in practical implementations. <b>Generate-and-test</b> methods are sensitive to noise in the data. They often fail when the generated model fit is poor due to error in the data used to generate the model position. Hough transform variations are less sensitive to noise, but implementations for complex problems suffer from large time and space requirements and from the detection of false positives. This paper describes a general method for solving problems where a model is extracted from, or fit to, data that draws benefits from both <b>generate-and-test</b> methods and those based on the Hough transform, yielding a method superior to both. An {{important component of the}} method is the subdivision of the problem into many subproblems. This allows efficient <b>generate-and-test</b> techniques to be used, including the use of randomization {{to limit the number of}} subproblems that must be examined. Each subproblem is solved using pose space analysis techniques similar to the Hough transform, which lowers the sensitivity of the method to noise. This strategy is easy to implement and results in practical algorithms that are efficient and robust. We describe case studies of the application of this method to object recognition, geometric primitive extraction, robust regression, and motion segmentation...|$|E
40|$|Generally, in DNA {{sequences}} {{used for}} the computation should be critically designed {{in order to reduce}} error that could occur during computation. In order to design a set DNA sequences for Direct-Proportional Length-Based DNA Computing (DPLB-DNAC), a Population-based Ant Colony Optimization (P-ACO) method is proposed. Previously, the DNA sequences for DPLB-DNAC are designed using graph method and <b>Generate-and-Test</b> approach, respectively. The both of methods are without the optimized objective functions process. The proposed method used four objective functions in their process to obtain the best solutions. The results obtained from the proposed method are compared with the sequences generated by graph and <b>Generate-and-Test</b> methods. The results show that P-ACO approach can generate relatively better DNA sequences in some objectives than others. It can be concluded that proposed algorithm can obtain relatively a better set of DNA sequences for DPLB-DNAC...|$|E
40|$|Exploration and {{exploitation}} {{are the two}} cornerstones of problem solving by search. Evolutionary Algorithms (EAs) are search algorithms that explore the search space by the genetic search operators, while exploitation is done by selection. During the history of EAs different operators have emerged, mimicing asexual and sexual reproduction in Nature. Here we give {{an overview of the}} variety of these operators, review results discussing the (dis) advantages of asexual and sexual mechanisms and touch on a new phenomenon: multi-parent reproduction. 1 Introduction <b>Generate-and-test</b> search algorithms obtain their power from two sources: exploration {{and exploitation}}. Exploration means the discovery of new regions in the search space, exploitation amounts to using collected information in order to direct further search to promising regions. Evolutionary Algorithms (EAs) are stochastic <b>generate-and-test</b> search algorithms having a number of particular properties. The standard pseudo-code for an [...] ...|$|E
40|$|This article {{details the}} SFX-EH {{architecture}} for handling sensing failures in autonomous mobile robots. The SFX-EH uses novel extensions to the <b>generate-and-test</b> method to classify failures {{with only a}} partial causal model of the sensor/environment/task interactions for the robot. The <b>generate-and-test</b> methodology exploits {{the ability of the}} robot as a physically situated agent to actively test assumptions about the state of sensors, condition of the environment, and validity of task constraints. The SFX-EH uses the type of failure to determine the appropriate recovery strategy: reconfiguration of the logical sensor or logical behavior, recalibration of the sensor or actuator, and corrective actions. The system bypasses classification if all hypotheses lead to the same recovery strategy. Results of the SFX-EH running on two robots with different sensor suites and tasks are presented, demonstrating intelligent failure recovery within a modular, portable implementation. 1...|$|E
40|$|A general {{synthesis}} method {{based on}} a parallel grammar for design of mechanical systems has been improved {{through the use of}} Perturbation Rules (P-Rules) that allow parametric design changes while upholding topologic and parametric constraints. The P-Rules have been used with a <b>generate-and-test</b> algorithm to produce preferred mechanical clock and wristwatch designs based on various design criteria...|$|E
40|$|Test {{incorporations}} are program transformations {{that improve}} {{the performance of}} <b>generate-and-test</b> procedures by moving information out of the "test" and into the "generator. " The test information {{is said to be}} "incorporated" into the generator so that items produced by the generator are guaranteed to satisfy the incorporated test. This article proposes and investigates the hypothesis that a general theory of AI methods can be constructed using only test incorporations. Starting from an initial <b>generate-and-test</b> algorithm, we attempt to derive the weak methods of heuristic search, hill climbing, and avoiding duplicates via a series of test incorporations. The derivations show that test incorporations are very powerful but that occasionally other program reformulations are required. Nevertheless, we conclude that test incorporation provides a good foundation upon which to construct a general theory of methods. Submitted to AAAI- 86, Philadelphia, PA a The authors have chosen to list [...] ...|$|E
40|$|We want {{to solve}} (k, 2) -CSP; that is, {{we are given}} a set of n {{variables}} xi (1 ≤ i ≤ n), each {{of which can be}} given one of k values. We are also given a set of constraints; each constraint specifies a pair of values from different variables. The goal is to find an assignment of values to all the variables, such that no constraint has both its specified values used. Tomás Feder and Rajeev Motwani of Stanford Univ., in an unpublished manuscript dated September 1998, gave a very simple randomized algorithm for this problem. Its analysis, however, is not so simple. Their algorithm is still the best known when k is sufficiently large (k ≥ 11). It is open, and would be of interest, to find a way of derandomizing their algorithm, giving a deterministic algorithm with a similar running time. The set cover based methods we’ve seen for derandomizing other randomized <b>generate-and-test</b> algorithms don’t seem to work here. Algorithm Like the simple randomized (3, 2) -CSP algorithm we saw last time, this algorithm combines ideas from both <b>generate-and-test</b> and backtracking algorithms. The outer loop is like a <b>generate-and-test</b> algorithm, repeating an inner loop enough times {{to make up for the}} low probability of finding an answer. The inner loop is similar to a recursive backtracking search, but only follows a single randomly chosen path through the searc...|$|E
40|$|The {{emphasis}} {{of this paper}} is on a particular type of images, density images. Because of the density interpretation of images, a procedure to segmentate the density images has been developed. This procedure is based on a <b>generate-and-test</b> idea. The hypotheses are generated by ordinary segmentation methods. The generated regions are then tested and combined by inference. The results have been promising...|$|E
40|$|Datalog systems {{evaluate}} joins over arithmetic (in) equalities as a naive <b>generate-and-test</b> of Cartesian products. We exploit aggregates in a source-to-source transformation {{to reduce}} the size of Cartesian products and to improve performance. Our approach approximates the well-known propagation technique from Constraint Programming. Experimental evaluation shows good run time speed-ups on a range of non-recursive as well as recursive programs. status: publishe...|$|E
40|$|This paper {{investigates the}} {{application}} of various efficiency-motivated search techniques to <b>generate-and-test</b> problems {{of the sort that}} arise in principle-based parsing. In particular, various means of eliminating redundant computation and performing operations in parallel are examined, and performance improvements are compared with those resulting from the use of more deterministic theories of language processing. Subject Areas: parallel search, parsing Word Count: XXX INTRODUCTIO...|$|E
40|$|Abstraction {{techniques}} {{are important for}} solving constraint satisfaction problems with global constraints and low solution density. In the presence of global constraints, backtracking search is unable to prune partial solutions. It therefore operates like pure <b>generate-and-test.</b> Abstraction improves on <b>generate-and-test</b> by enabling entire subsets of the solution space to be pruned early in a backtracking search process. This paper describes how abstraction spaces can be characterized in terms of approximate symmetries of the original, concrete search space. It defines two special types of approximate symmetry, called &quot;range symmetry &quot; and &quot;domain symmetry&quot;, which apply to function finding problems. It also presents algorithms for automatically synthesizing hierarchic problem solvers based on range or domain symmetry. The algorithms operate by analyzing declarative descriptions of classes of constraint satisfaction problems. Both algorithms have been fully implemented. This paper concludes by presenting data from experiments testing the two synthesis algorithms and the resulting problem solvers on NP-hard scheduling and partitioning problems. 1...|$|E
40|$|The parsing {{component}} of previous principle-based machine translation systems are inefficient since {{they tend to}} adopt a <b>generate-and-test</b> paradigm. We combine {{the benefits of a}} message-passing paradigm with the benefits of a parametric approach in the implementation of a parser that avoids overgeneration and is easily ported to multiple languages. The algorithm has been implemented in C++ and successfully tested on well-known, translationally divergent sentences in a MT system called PRINCITRAN...|$|E
40|$|There {{are three}} parts to this paper. First, I present what I hope is a conclusive, worst-case, {{complexity}} analysis of two well-known formulations of the Minimal Diagnosis problem — those of [Reiter 87] and [Reggia et al., 85]. I then show that Reiter 2 ̆ 7 s conflict-sets {{solution to the}} problem decomposes the single exponential problem into two problems, each exponential, that need be solved sequentially. From a worst case perspective, this only amounts to a factor of two, in which case I see no reason to prefer it over a simple <b>generate-and-test</b> approach. This is only emphasized with the results of the third part of the paper. Here I argue for a different perspective on algorithms, that of expected, rather than worst-case performance. From that point of view, a sequence of two exponential algorithms has lesser probability to finish early than a single such algorithm. I show that the straightforward <b>generate-and-test</b> approach may in fact be somewhat attractive as it has high probability to conclude in a polynomial time, given a random problem instance...|$|E
40|$|Abstract. Datalog systems {{evaluate}} joins over arithmetic (in) equalities as a naive <b>generate-and-test</b> of Cartesian products. We exploit aggre-gates in a source-to-source transformation {{to reduce}} the size of Cartesian products and to improve performance. Our approach approximates the well-known propagation technique from Constraint Programming. Experimental evaluation shows good run time speed-ups on a range of non-recursive as well as recursive programs. Furthermore, our technique improves upon the previously reported in the literature constraint magic set transformation approach. ...|$|E
40|$|Uncertain {{data sets}} have become popular {{in recent years}} because of {{advances}} {{in recent years in}} hardware data collection technology. In uncertain data sets, the values of the underlying data sets may not be fully specified. In this chapter, we will discuss the frequent pattern mining for uncertain data sets. We will show how the broad classes of algorithms can be extended to the uncertain data setting. In particular, we will discuss the candidate <b>generate-and-test</b> algorithms, hyper-structure algorithms and the pattern growth based algorithms. One of our insightful and interesting observations is that the experimental behavior of different classes of algorithms is very different in the uncertain case as compared to the deterministic case. In particular, the hyper-structure and the candidate <b>generate-and-test</b> algorithms perform much better than the tree-based algorithms. This counter-intuitive behavior compared to the case of deterministic data is an important observation from the perspective of frequent pattern mining algorithm design in the case of uncertain data. We will test the approach on a number of real and synthetic data sets, and show the effectiveness of two of our approaches over competitive techniques. ...|$|E
40|$|A {{learning}} {{program has}} been developed which combines 'learning by example' with the <b>generate-and-test</b> paradigm to furnish a robust learning environment capable of handling error-prone data. The problem is shown {{to be capable of}} learning class descriptions from positive and negative training examples of spectral and directional reflectance data taken from soil and vegetation. The program, which used AI techniques to automate very tedious processes, found the sequence of relationships that contained the most important information which could distinguish the classes...|$|E
40|$|We {{show that}} MapReduce, {{the de facto}} {{standard}} for large scale data-intensive parallel programming, can be equipped with a programming theory in calculational form. By integrating the <b>generate-and-test</b> program-ming paradigm and semirings for aggregation of results, we propose a novel parallel programming framework for MapReduce. The framework consists of two important calculation theorems: the shortcut fusion theorem of semiring homomorphisms bridges the gap between specications and efficient implementations, and the lter-embedding theorem helps to develop parallel programs in a systematic and incremental way...|$|E
40|$|AbstractConstraint {{propagation}} {{can often}} be conveniently expressed by rules. In recent years, a number of techniques for automatic generation of rule-based constraint solvers have been developed, most of them using a <b>generate-and-test</b> approach. We examine a generation method {{that is based on}} deduction. A solver (i. e. a set of rules) for a complex constraint is obtained from one or several weaker solvers for simple constraints. We describe incremental solver constructions for several types of constraint modifications, including conjunction, existential and universal quantification...|$|E
40|$|In this paper, we {{show how}} {{well-known}} graph-theoretic techniques can be successfully exploited to efficiently reason about partially ordered events in Kowalski and Sergot's Event Calculus {{and in its}} skeptical and credulous modal variants. We replace the traditional <b>generate-and-test</b> strategy of (Modal) Event Calculus by a generate-only strategy that operates on the transitive closure and reduction of the underlying directed acyclic graph of events. We prove the soundness and completeness of the proposed strategy, and thoroughly analyze its computational complexity...|$|E
40|$|Three general {{techniques}} are discussed for the source-to-source translation of sequential logic programs to AND-parallel logic programs. Producer-consumer information {{is used to}} translate deterministic programs with a single solution, utilizing stream-parallelism. Operationally nondeterministic programs with a single solution are translated by simulated OR-parallelism. Enumeration is used for <b>generate-and-test</b> programs. Enumeration needs a programming idiom for AND-parallel programs, called enumerate-and-filter. The enumerate-and-filter paradigm utilizes AND-parallelism along with stream-parallelism and pipelining. The {{techniques are}} illustrated with examples of Prolog programs translated to Flat Concurrent Prolog...|$|E
40|$|This article {{describes}} a technique for analyzing relational specifications. The underlying idea is very simple. Both simulation and checking amount to finding models of a relational formula, i. e., assignments {{for which the}} formula is true. For simulation the formula is {{the description of the}} operation; for checking, the formula is the negation of an assertion about an operation. Models are found by a <b>generate-and-test</b> strategy: the formula is repeatedly evaluated for a series of assignments until one is found for which the formula is tru...|$|E
40|$|Problem statement: A {{number of}} DNA {{computing}} models to solve mathematical graph problem {{such as the}} Hamiltonian Path Problem (HPP), Traveling Salesman Problem (TSP), and the Shortest Path Problem (SPP), have been proposed and demonstrated. Normally, the DNA sequences used for the computation should be critically designed {{in order to reduce}} error that could occur during a computation. We have proposed a DNA computing readout method tailored specifically to HPP in DNA computing using real-time Polymerase Chain Reaction (PCR). The DNA sequences were designed based on a procedure and DNASequenceGenerator was employed to generate the sequences required for the experiment. The drawback of the previous approach is that a pool of DNA sequences need to be generated by DNASequenceGenerator before the selection is done manually, based on several design constraints. Hence, an automatic and systematic approach is needed to generate the DNA sequences based on design constraints. Approach: In this study, a <b>generate-and-test</b> approach was proposed for the same problem subjected to several design constraints. The <b>generate-and-test</b> algorithm consists of two main levels. The first level considered the basic constraints of DNA sequence design, which were melting temperature, GC-percentage, similarity, continuity, hairpin, and H-measure. This was followed by the second level that includes specific constraints formulated base...|$|E
40|$|We {{propose a}} new method of {{automatically}} finding periodic modes of locomotion for arbitrary articulated figures. Cyclic pose control graphs {{are used as}} our control representation. These specifically constrain the controller synthesis process to only those controllers producing periodic driving functions. It is shown that stochastic <b>generate-and-test</b> techniques work well with this representation. Several choices that arise in this synthesis technique are explored. The impact of {{the design of the}} physical models upon the motions produced is examined. Lastly, the motions produced are analysed by looking at their bifurcation diagrams...|$|E
40|$|In this paper, {{we propose}} a new framework, called XLogic- Miner, to mine {{association}} rules from XML data. We consider the <b>generate-and-test</b> and the frequent-pattern growth approaches. In XLogic-Miner, we propose an novel method {{to represent a}} frequent-pattern tree in an object-relational table and exploit a new join operator developed in the paper. The principal focus {{of this research is}} to demonstrate that association rule mining can be expressed in an extended datalog program and be able to mine XML data in a declarative way. We also consider some optimization and performance issues. ...|$|E
40|$|This paper {{complements}} the previous paper Exhaustive Search Programs Deterministic" which showed a systematic method for compiling a Horn-clause program for exhaustive search into a GHC program or a Prolog program with no backtracking. This time {{we present a}} systematic method for deriving a deterministic logic program that simulates coroutining execution of a <b>generate-and-test</b> logic program. The class of compilable programs is suciently general, and compiled programs proved to be ecient. The method can also be viewed as suggesting a method of compiling a naive logic program into (very) low-level languages...|$|E
