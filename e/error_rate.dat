10000|8422|Public
5|$|Eisenhower {{would leave}} office with an {{agreement}} out of reach, as Eisenhower's technical advisors, upon whom he relied heavily, became {{mired in the}} complex technical questions of a test ban, driven {{in part by a}} strong interest among American experts to lower the <b>error</b> <b>rate</b> of seismic test detection technology. Some, including Kistiakowsky, would eventually raise concerns about the ability of inspections and monitors to successfully detect tests. The primary product of negotiations under Eisenhower was the testing moratorium without any enforcement mechanism. Ultimately, the goal of a comprehensive test ban would be abandoned in favor of a partial ban due to questions over seismic detection of underground tests.|$|E
5|$|In 1983, Mansour {{was offered}} {{money for the}} film rights {{to the story of}} her life. She said that she would accept on {{condition}} that the money was enough to balance Michigan's state deficit, $900,000,000 at that time. No film rights were obtained. Mansour remained director of DSS until 1987. Under her leadership, the department's <b>error</b> <b>rate</b> dropped to its lowest levels in awarding food stamps, Medicaid funds, and Aid to Families with Dependent Children (AFDC). She increased the investigation and conviction of fraud cases, and she achieved the highest national record of locating deadbeat parents for collecting child support. She streamlined office procedures, and she initiated programs to curtail teenage pregnancy and to assist teenage mothers. She broadened the state program benefiting victims of domestic abuse.|$|E
25|$|The {{estimated}} <b>error</b> <b>rate</b> {{of standard}} next-generation sequencing platforms is 10−2 - 10−3 per base call. With this <b>error</b> <b>rate</b> billions of base calls that {{are produced by}} NGS will results in millions of the errors. The errors are introduced during sample preparation and sequencing such as polymerase , sequencing and image analysis errors. While the NGS platforms <b>error</b> <b>rate</b> is admissible to some applications such as detection of clonal variants, {{it is a major}} limit for applications that require higher accuracy for detection of low frequency variants such as detection of intra-organismal mosaicism, subclonal variants in genetically heterogeneous cancers or circulating tumor DNA.|$|E
50|$|New Reno {{performs}} {{as well as}} SACK at low packet <b>error</b> <b>rates,</b> and substantially outperforms Reno at high <b>error</b> <b>rates.</b>|$|R
3000|$|The {{results of}} the type I <b>error</b> <b>rates</b> for the three methods are shown {{graphically}} in Figure  2. The three methods have comparable type I <b>error</b> <b>rates</b> across each of the trials and event distribution scenarios. The methods in general have nominal or close-to-nominal type I <b>error</b> <b>rates</b> when the event distribution probabilities are equivalent between treatment groups or when the experimental treatment group events occurred earlier in the trial compared with the standard group. However, under these same scenarios, slightly greater-than-nominal type I <b>error</b> <b>rates</b> are seen in the trials where (π [...]...|$|R
50|$|Finally, if {{the code}} had an {{asterisk}} next to it, a final FAC would {{be drawn to}} check for error. This was done by comparing the error range on the FAC to the <b>error</b> <b>rating</b> of the fielder. For example, if the fielder's <b>error</b> <b>rating</b> was E2 and the <b>error</b> <b>rating</b> on the FAC was F8 to F10, {{then there would be}} no error.|$|R
25|$|High <b>error</b> <b>rate</b> (0.01-0.001) of {{standard}} NGS platforms that introduced during sample preparation or sequencing {{is a major}} limitation for detection of variants present in small fraction of cells. Due to the duplex tagging system and use of information in both strands of DNA, duplex sequencing has significantly decreased the <b>error</b> <b>rate</b> of sequencing about 10 million fold using both SSCS and DCS method.|$|E
25|$|The {{successful}} {{hypothesis test}} {{is associated with}} a probability and a type-I <b>error</b> <b>rate.</b> The conclusion might be wrong.|$|E
25|$|If the <b>error</b> <b>rate</b> {{is small}} enough, {{it is thought}} to be {{possible}} to use quantum error correction, which corrects errors due to decoherence, thereby allowing the total calculation time to be longer than the decoherence time. An often cited figure for required <b>error</b> <b>rate</b> in each gate is 10−4. This implies that each gate must be able to perform its task in one 10,000th of the coherence time of the system.|$|E
40|$|An {{investigation}} of the accuracy of general practitioner and Executive Council files was approached by {{a comparison of the}} two. High <b>error</b> <b>rates</b> were found, including both file errors and record errors. On analysis it emerged that file <b>error</b> <b>rates</b> could not be satisfactorily expressed except in a time-dimensioned way, and we were unable to do this within the context of our study. Record <b>error</b> <b>rates</b> and field <b>error</b> <b>rates</b> were expressible as proportions of the number of records on both the lists; 79 · 2 % of all records exhibited non-congruencies and particular information fields had <b>error</b> <b>rates</b> ranging from 0 · 8 % (assignation of sex) to 68 · 6 % (assignation of civil state). Many of the errors, both field errors and record errors, were attributable to delayed updating of mutable information...|$|R
50|$|The <b>error</b> <b>rates</b> quoted {{here are}} those in {{additive}} white Gaussian noise (AWGN). These <b>error</b> <b>rates</b> are lower than those computed in fading channels, hence, are a good theoretical benchmark to compare with.|$|R
40|$|International audienceNoninvasive DNA {{sampling}} allows {{studies of}} natural populations without disturbing the target animals. Unfortunately, high genotyping <b>error</b> <b>rates</b> often make noninvasive studies difficult. We report low <b>error</b> <b>rates</b> (0. 0 - 7. 5 %/locus) when genotyping 18 microsatellite loci in only 4 multiplex {{polymerase chain reaction}} amplifications using fecal DNA from bighorn sheep (Ovis canadensis). The average locus-specific <b>error</b> <b>rates</b> varied significantly between the 2 populations (0. 13 % vs. 1. 6 %; P < 0. 001), as did multi-locus genotype <b>error</b> <b>rates</b> (2. 3 % vs. 14. 1 %; P < 0. 007). This illustrates the importance of quantifying <b>error</b> <b>rates</b> in each study population (and for each season and sample preservation method) before initiating a noninvasive study. Our <b>error</b> <b>rates</b> are among the lowest reported for fecal samples collected noninvasively in the field. This and other recent studies suggest that noninvasive fecal samples {{can be used in}} species with pellet-form feces for nearly any study (e. g., of population structure, gene flow, dispersal, parentage, and even genome-wide studies to detect local adaptation) that previously required high-quality blood or tissue samples...|$|R
25|$|Reverse {{transcriptase}} {{has a high}} <b>error</b> <b>rate</b> when transcribing RNA into DNA since, {{unlike most}} other DNA polymerases, it has no proofreading ability. This high <b>error</b> <b>rate</b> allows mutations to accumulate at an accelerated rate relative to proofread forms of replication. The commercially available reverse transcriptases produced by Promega are quoted by their manuals as having error rates {{in the range of}} 1 in 17,000 bases for AMV and 1 in 30,000 bases for M-MLV.|$|E
25|$|The power {{efficiency}} describes {{the ability of}} communication system to preserve bit <b>error</b> <b>rate</b> (BER) of the transmitted signal at low power levels.|$|E
25|$|Barcode {{scanners}} {{are relatively}} low cost and extremely accurate compared to key-entry, with only about 1 substitution error in 15,000 to 36 trillion characters entered. The exact <b>error</b> <b>rate</b> {{depends on the}} type of barcode.|$|E
40|$|Restriction-enzyme-based {{sequencing}} methods {{enable the}} genotyping {{of thousands of}} single nucleotide polymorphism (SNP) loci in nonmodel organisms. However, in contrast to traditional genetic markers, genotyping <b>error</b> <b>rates</b> in SNPs derived from restriction-enzyme-based methods remain largely unknown. Here, we estimated genotyping <b>error</b> <b>rates</b> in SNPs genotyped with double digest RAD sequencing from Mendelian incompatibilities in known mother-offspring dyads of Hoffman's two-toed sloth (Choloepus hoffmanni) {{across a range of}} coverage and sequence quality criteria, for both reference-aligned and de novo-assembled data sets. Genotyping <b>error</b> <b>rates</b> were more sensitive to coverage than sequence quality and low coverage yielded high <b>error</b> <b>rates,</b> particularly in de novo-assembled data sets. For example, coverage >= 5 yielded median genotyping <b>error</b> <b>rates</b> of >= 0. 03 and >= 0. 11 in reference-aligned and de novo-assembled data sets, respectively. Genotyping <b>error</b> <b>rates</b> declined to = 30, but remained >= 0. 04 in the de novo-assembled data sets. We observed approximately 10 - and 13 -fold declines in the number of loci sampled in the reference-aligned and de novo-assembled data sets when coverage was increased from >= 5 to >= 30 at quality score >= 30, respectively. Finally, we assessed the effects of genotyping coverage on a common population genetic application, parentage assignments, and showed that the proportion of incorrectly assigned maternities was relatively high at low coverage. Overall, our results suggest that the trade-off between sample size and genotyping <b>error</b> <b>rates</b> be considered prior to building sequencing libraries, reporting genotyping <b>error</b> <b>rates</b> become standard practice, and that effects of genotyping errors on inference be evaluated in restriction-enzyme-based SNP studies...|$|R
30|$|High bit <b>error</b> <b>rates.</b>|$|R
40|$|Abstract: The major {{objective}} {{of this study was}} to investigate the effects of non-normality on Type III <b>error</b> <b>rates</b> for ANOVA F its three commonly recommended parametric counterparts namely Welch, Brown-Forsythe, and Alexander-Govern test. Therefore these tests were compared in terms of Type III <b>error</b> <b>rates</b> across the variety of population distributions, mean difference (effect size), and sample sizes. At the end of 100, 000 simulation trials it was observed that the Type III <b>error</b> <b>rates</b> for four tests were affected by the effect size (δ) and sample size, whereas Type III errors were not affected from distribution shapes. Results of the simulation also indicated that increases in sample size and population mean difference decreased Type III error, and increased statistical test power. Across the all distributions, sample sizes and population mean differences (δ), the Alexander-Govern test obtained higher estimates for power, lower estimates of Type III error (γ). Key words: Type I <b>error</b> <b>rates,</b> power of test, Type III <b>error</b> <b>rates,</b> normality, ANOVA 1...|$|R
25|$|When a {{photodiode}} {{is used in}} {{an optical}} communication system, all these parameters contribute to {{the sensitivity of the}} optical receiver, which is the minimum input power required for the receiver to achieve a specified bit <b>error</b> <b>rate.</b>|$|E
25|$|The {{validity}} of forensic fingerprint {{evidence has been}} challenged by academics, judges and the media. While fingerprint identification was an improvement on earlier anthropometric systems, the subjective nature of matching, despite a very low <b>error</b> <b>rate,</b> has made this forensic practice controversial.|$|E
25|$|For comparison, 10−18 to 10−15 is the uncorrectable bit <b>error</b> <b>rate</b> of {{a typical}} hard disk. In theory, 128-bit hash functions, such as MD5, should stay within that range until about 820 billion documents, even if its {{possible}} outputs are many more.|$|E
40|$|Achieving <b>error</b> <b>rates</b> {{that meet}} or exceed the {{fault-tolerance}} threshold {{is a central}} goal for quantum computing experiments, and measuring these <b>error</b> <b>rates</b> using randomized benchmarking is now routine. However, direct comparison between measured <b>error</b> <b>rates</b> and thresholds {{is complicated by the}} fact that benchmarking estimates average <b>error</b> <b>rates</b> while thresholds reflect worst-case behavior when a gate is used as part of a large computation. These two measures of error can differ by orders of magnitude in the regime of interest. Here we facilitate comparison between the experimentally accessible average <b>error</b> <b>rates</b> and the worst-case quantities that arise in current threshold theorems by deriving relations between the two for a variety of physical noise sources. Our results indicate that it is coherent errors that lead to an enormous mismatch between average and worst case, and we quantify how well these errors must be controlled to ensure fair comparison between average error probabilities and fault-tolerance thresholds. Comment: 5 pages, 2 figures, 13 page appendi...|$|R
30|$|The {{results for}} inflected {{parts of speech}} are mixed. While the <b>error</b> <b>rates</b> for verbs, adjectives, and pronouns decreased, the <b>error</b> <b>rates</b> for nouns and numerals {{increase}}d. Although the increase WER for nouns is rather small, the increase for numerals is more significant.|$|R
40|$|The authors {{describe}} algorithms for the LTE uplink which {{implement the}} functionalities of random access preamble detection, time and frequency synchronisation, channel estimation and equalization (including MIMO) and error correction decoding. Evaluation {{is done by}} means of preamble detection and false detection <b>rates,</b> <b>error</b> vector magnitude and bit <b>error</b> <b>rates</b> as well as packet <b>error</b> <b>rates</b> after turbo decoding...|$|R
25|$|A common phylogenetic {{marker for}} {{microbial}} community diversity studies is the 16S ribosomal RNA gene. Both MinION and PacBio's SMRT platform {{have been used}} to sequence this gene. In this context the PacBio <b>error</b> <b>rate</b> was comparable to that of shorter reads from 454 and Illumina's MiSeq sequencing platforms.|$|E
25|$|In {{order to}} be able to analyse more {{chromosomes}} on the same sample, up to three consecutive rounds of FISH can be carried out. In the case of chromosome rearrangements, specific combinations of probes have to be chosen that flank the region of interest. The FISH technique is considered to have an <b>error</b> <b>rate</b> between 5 and 10%.|$|E
25|$|LAMSTAR {{has been}} applied to many domains, {{including}} medical and financial predictions, adaptive filtering of noisy speech in unknown noise, still-image recognition, video image recognition, software security and adaptive control of non-linear systems. LAMSTAR had a much faster learning speed and somewhat lower <b>error</b> <b>rate</b> than a CNN based on ReLU-function filters and max pooling, in 20 comparative studies.|$|E
40|$|More {{and more}} noninvasive genetic data are being {{produced}} but a general methodology to quantify genotyping <b>error</b> <b>rates</b> from non-pilot data remains lacking. Here we propose a mathematical approach to estimate genotyping <b>error</b> <b>rates</b> by exploring {{the relationship between}} errors and PCR replicates. This method {{can be used to}} quantify the <b>error</b> <b>rates</b> for either the multi-tubes approach designed by Taberlet et al. (Nucleic Acids Res 24 : 3189 – 3194, 1996) or the pilot method by Prugh et al. (Mol Ecol 14 : 1585 – 1596, 2005) ...|$|R
40|$|The {{predictive}} {{power of}} logistic regression, support vector machines and bootstrap-aggregated classification trees (bagging, double-bagging) is compared using misclassification <b>error</b> <b>rates</b> on independent test data sets. Based on a resampling approach {{that takes into}} account spatial autocorrelation, <b>error</b> <b>rates</b> for predicting 'present' and 'future' landslides are estimated within and outside the training area. In a case study from the Ecuadorian Andes, logistic regression with stepwise backward variable selection yields lowest <b>error</b> <b>rates</b> and demonstrates the best generalization capabilities. The evaluation outside the training area reveals that tree-based methods tend to overfit the data...|$|R
30|$|When using {{different}} loci in studies involving samples {{that have been}} obtained non-invasively, the researcher is keen to know the <b>error</b> <b>rates</b> and amplification success rate. We tested {{the applicability of the}} recommended panel with noninvasive samples (scat) and blood from the same individuals and estimated the frequency of occurrence of genotyping <b>error</b> <b>rates.</b> The values of the mean genotyping <b>error</b> <b>rates</b> were low and considerable for non-invasive genetic studies (allele dropout, 0.004 [*]±[*] 0.002 SD; false allele, 0.004 [*]±[*] 0.002 SD and scoring error, 0.006 [*]±[*] 0.003 SD). These relatively low <b>error</b> <b>rates</b> {{may be due to the}} use of locus-specific profile characteristics, which leads to correct decisions in allele calling. We also did not observe any change or discrepancy in the genetic data compared with the data generated from blood samples.|$|R
25|$|Moderate {{clinical}} probability. If negative D-dimer, PE is excluded. However, {{the authors}} were not concerned that a negative MDCT with negative D-dimer {{in this setting}} has a 5% probability of being false. Presumably, the 5% <b>error</b> <b>rate</b> will fall as 64 slice MDCT is more commonly used. If positive D-dimer, obtain MDCT and based treatment on results.|$|E
25|$|Recent {{approaches}} {{consider the}} use of high-quality microscopy images over which a statistical classification algorithm is used to perform automated cell detection and counting as an image analysis task. Generally performs with a constant <b>error</b> <b>rate</b> as an off-line (batch) type process. A range of image classification techniques can be employed for this purpose.|$|E
25|$|In a block tapping task {{participants}} {{are asked to}} type a sequence of five numbers with their dominant or non-dominant hand (specified in experiment), for an allotted period of time, followed by a rest period. A number of these trials occur and the computer records the number of sequences completed to assess speed and the <b>error</b> <b>rate</b> to assess accuracy.|$|E
40|$|We {{investigate}} {{the performance of}} MLPs with four risk functionals: the classical mean square error (MSE), the cross-entropy (CE), a generalized exponential risk (EXP), and the Shannon entropy of the classifier’s output error (HS). The performance is compared with an SVM with RBF kernel in terms of average balanced and unbalanced <b>error</b> <b>rates,</b> and their generalization, on practical classification tasks. For this purpose we carried out experiments on 35 public real-world datasets. A battery of statistical tests applied to the experimental results showed no significant difference among the classifiers in terms of unbalanced <b>error</b> <b>rates.</b> However, in terms of balanced <b>error</b> <b>rates</b> SVM-RBF performed significantly worse than MLP-CE and MLP-EXP. Regarding generalization, SVM-RBF and MLP-EXP scored as the classification methods with significantly better generalization, {{both in terms of}} balanced and unbalanced <b>error</b> <b>rates.</b> ...|$|R
40|$|There is {{debate whether}} {{clinical}} trials with suboptimal power are justified and whether results from large studies are {{more reliable than}} the (combined) results of smaller trials. We quantified the <b>error</b> <b>rates</b> for evaluations based on single conventionally powered trials (80 % or 90 % power) versus evaluations based on the random-effects meta-analysis {{of a series of}} smaller trials. When a treatment was assumed to have no effect but heterogeneity was present, the <b>error</b> <b>rates</b> for a single trial were increased more than 10 -fold above the nominal rate, even for low heterogeneity. Conversely, for meta-analyses on a series of trials, the <b>error</b> <b>rates</b> were correct. When selective publication was present, the <b>error</b> <b>rates</b> were always increased, but they still tended to be lower for a series of trials than single trials. We conclude that evidence of efficacy based on a series of (smaller) trials, may lower the <b>error</b> <b>rates</b> compared with using a single well-powered trial. Only when both heterogeneity and selective publication can be excluded, a single trial is able to provide conclusive evidence...|$|R
40|$|The {{congestion}} control mechanism in TCP introduces idle transmit periods {{as a result}} of its small transmission window and exponential retransmission timer. This behavior occurs while recovering from packet losses, due to wireless LAN link errors. Therefore, TCP suffers from low throughput due to poor utilization of the available bandwidth, while recovering from high bit <b>error</b> <b>rates.</b> TCP Noor`s Aggressive Error Recovery (AER) avoids the idle transmit periods while recovering from high Bit <b>Error</b> <b>Rates</b> (BER). Enhanced AER (EAER) applies further enhancement to AER which provides higher averaged Throughput during variable bit <b>error</b> <b>rates</b> on the channel. EAER applies a dynamic mechanism, which allows TCP`s transmission behavior to adjust based on the channel conditions. Simulation results indicate that TCP Noor`s EAER gains up to two 250 % increase in throughput over TCP Reno while recovering from variable bit <b>error</b> <b>rates...</b>|$|R
