268|95|Public
25|$|A {{representation}} of a chordal graph as an intersection of subtrees forms a tree decomposition of the graph, with treewidth equal to one less than {{the size of the}} largest clique in the graph; the tree decomposition of any graph G can be viewed in this way as a {{representation of}} G as a subgraph of a chordal graph. The tree decomposition of a graph is also the <b>junction</b> <b>tree</b> of the <b>junction</b> <b>tree</b> algorithm.|$|E
25|$|This dynamic {{programming}} approach {{is used in}} machine learning via the <b>junction</b> <b>tree</b> algorithm for belief propagation in graphs of bounded treewidth. It also {{plays a key role}} in algorithms for computing the treewidth and constructing tree decompositions: typically, such algorithms have a first step that approximates the treewidth, constructing a tree decomposition with this approximate width, and then a second step that performs {{dynamic programming}} in the approximate tree decomposition to compute the exact value of the treewidth.|$|E
5000|$|A clique tree or <b>junction</b> <b>tree</b> is a tree of cliques, {{used in the}} <b>junction</b> <b>tree</b> algorithm.|$|E
40|$|In {{this paper}} we {{prove that the}} {{well-known}} correspondence between the forward-backward algorithm for hidden Markov models (HMMs) and belief propagation (BP) applied to HMMs can be generalized to one between BP for <b>junction</b> <b>trees</b> and the generalized inside-outside probability computation for probabilistic logic programs applied to <b>junction</b> <b>trees.</b> ...|$|R
40|$|Belief {{propagation}} over <b>junction</b> <b>trees</b> {{is known}} to be computationally challenging in the general case. One way of addressing this computational challenge is to use node-level parallel computing, and parallelize the computation associated with each separator potential table cell. However, this approach is not efficient for <b>junction</b> <b>trees</b> that mainly contain small separators. In this paper, we analyze this problem, and address it by studying a new dimension of node-level parallelism, namely arithmetic parallelism. In addition, on the graph level, we use a clique merging technique to further adapt <b>junction</b> <b>trees</b> to parallel computing platforms. We apply our parallel approach to both marginal and most probable explanation (MPE) inference in <b>junction</b> <b>trees.</b> In experiments with a Graphics Processing Unit (GPU), we obtain for marginal inference an average speedup of 5. 54 x and a maximum speedup of 11. 94 x; speedups for MPE inference are similar. ...|$|R
25|$|In machine learning, tree decompositions {{are also}} called <b>junction</b> <b>trees,</b> clique trees, or join trees; they play an {{important}} role in problems like probabilistic inference, constraint satisfaction, query optimization, and matrix decomposition.|$|R
5000|$|Construct a <b>junction</b> <b>tree</b> {{from the}} {{triangulated}} graph (we {{will call the}} vertices of the <b>junction</b> <b>tree</b> [...] "supernodes") ...|$|E
5000|$|Generalizations of the Chow-Liu tree are the {{so-called}} t-cherry junction trees. It is {{proved that the}} t-cherry junction trees provide a better {{or at least as}} good approximation for a discrete multivariate probability distribution as the Chow-Liu tree gives.For the third order t-cherry <b>junction</b> <b>tree</b> see , for the kth-order t-cherry <b>junction</b> <b>tree</b> see [...] The second order t-cherry <b>junction</b> <b>tree</b> is in fact the Chow-Liu tree.|$|E
50|$|So, to {{construct}} a <b>junction</b> <b>tree</b> {{we just have to}} extract a maximum weight spanning tree out of the clique graph. This can be efficiently done by, for example, modifying Kruskal's algorithm.The last step is to apply belief propagation to the obtained <b>junction</b> <b>tree.</b>|$|E
5000|$|... 3. Artificial intelligenceThe {{notion of}} <b>junction</b> <b>trees</b> {{has been used}} to solve many {{problems}} in AI. Also the concept of bucket elimination used many of the concepts.|$|R
40|$|Dawid, Kjærulff & Lauritzen (1994) {{provided}} a preliminary {{description of a}} hybrid between Monte-Carlo sampling methods and exact local computations in <b>junction</b> <b>trees.</b> Utilizing the strengths of both methods, such hybrid inference methods {{has the potential of}} expanding the class of problems which can be solved under bounded resources as well as solving problems which otherwise resist exact solutions. The paper provides a detailed description of a particular instance of such a hybrid scheme; namely, combination of exact inference and Gibbs sampling in discrete Bayesian networks. We argue that this combination calls for an extension of the usual message passing scheme of ordinary <b>junction</b> <b>trees...</b>|$|R
50|$|Some theoreticians have {{suggested}} {{that it would be}} productive to merge certain features of junction grammar with other models. Millett and Lonsdale, in fact, have proposed an expansion of Tree Adjoining Grammar (TAG) to create <b>junction</b> <b>trees.</b>|$|R
50|$|The <b>junction</b> <b>tree</b> {{algorithm}} (also {{known as}} 'Clique Tree') {{is a method}} used in machine learning to extract marginalization in general graphs. In essence, it entails performing belief propagation on a modified graph called a <b>junction</b> <b>tree.</b> The basic premise is to eliminate cycles by clustering them into single nodes.|$|E
5000|$|Propagate the probabilities {{along the}} <b>junction</b> <b>tree</b> (via belief propagation) ...|$|E
5000|$|There is {{not much}} we can do {{when it comes to}} the {{construction}} of the <b>junction</b> <b>tree</b> except that we may have many maximal weight spanning tree and we should choose the spanning tree with the least [...] and sometimes this might mean adding a local domain to lower the <b>junction</b> <b>tree</b> complexity.|$|E
40|$|Graphical {{modeling}} is {{a method}} that is increasingly being used to model real-life applications such as medical diagnosis, security and emergency response systems, and computer vision. A graphical model represents random variables and dependencies between them {{by means of a}} graph, in which some variables may not be observable in practice. An important problem in graphical models is that of inference, the process of finding the probability distributions of hidden variables given those of the observed variables. A general technique for inference in graphical models uses <b>junction</b> <b>trees.</b> Inference on <b>junction</b> <b>trees</b> is computationally intensive and has a lot of data-level parallelism. However, the large number and lack of locality of memory accesses limits exploitable parallelism on CPU platforms. Graphical Processing Units(GPUs) are well suited to exploit data-level parallelism, and are being increasingly used for scientific computations. In this work, we exploit the support for data-level parallelism on GPUs to speedup exact inference on <b>junction</b> <b>trees</b> and rely on multi-threading to hide memory latency. We achieve two orders of magnitude speedup on a CPU/GPU system using an NVIDIA GeForce 8800 part over a standalone CPU system...|$|R
40|$|Sum-product {{networks}} (SPNs) are a deep prob-abilistic representation {{that allows}} for efficient, exact inference. SPNs generalize many other tractable models, including thin <b>junction</b> <b>trees,</b> latent tree models, and many types of mixtures. Previous work on learning SPN structure has mainly focused on using top-down or bottom-up clustering to find mixtures, which capture vari-able interactions indirectly through implicit la-tent variables. In contrast, most work on learning graphical models, thin <b>junction</b> <b>trees,</b> and arith-metic circuits has focused on finding direct in-teractions among variables. In this paper, we present ID-SPN, a new algorithm for learning SPN structure that unifies the two approaches. In experiments on 20 benchmark datasets, {{we find that the}} combination of direct and indirect interactions leads to significantly better accuracy than several state-of-the-art algorithms for learn-ing SPNs and other tractable models. 1...|$|R
5000|$|To {{the west}} at Penrhos {{junction}} (on {{the line from}} [...] <b>Junction</b> to Walnut <b>Tree</b> <b>Junction)</b> with the Pontypridd, Caerphilly and Newport Railway ...|$|R
50|$|For {{the given}} set of local domains as input, {{we find out}} if we can create a <b>junction</b> <b>tree,</b> either by using the set {{directly}} or by adding dummy domains to the set first and then creating the <b>junction</b> <b>tree,</b> if construction junction is not possible then algorithm output {{that there is no}} way to reduce the number of steps to compute the given equation problem, but once we have <b>junction</b> <b>tree,</b> algorithm will have to schedule messages and compute states, by doing these we can know where steps can be reduced, hence will be discusses this below.|$|E
50|$|<b>Junction</b> <b>Tree</b> Algorithm used in machine {{learning}} to extract marginalization in general graphs.|$|E
5000|$|The {{following}} is the complexity for solving the <b>junction</b> <b>tree</b> using message passing ...|$|E
40|$|Latent {{variable}} {{models are}} an elegant framework for capturing rich probabilistic dependencies in many applications. However, current approaches typically parametrize these models using conditional probability tables, and learning relies predominantly on local search heuristics such as Expectation Maximization. Using tensor algebra, we propose an alternative parameterization of latent variable models (where the model structures are <b>junction</b> <b>trees)</b> that still allows for computation of marginals among observed variables. While this novel representation {{leads to a}} moderate {{increase in the number}} of parameters for <b>junction</b> <b>trees</b> of low treewidth, it lets us design a local-minimum-free algorithm for learning this parameterization. The main computation of the algorithm involves only tensor operations and SVDs which can be orders of magnitude faster than EM algorithms for large datasets. To our knowledge, this is the first provably consistent parameter learning technique for a large class of low-treewidth latent graphical models beyond trees. We demonstrate the advantages of our method on synthetic and real datasets. Comment: Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence (UAI 2012...|$|R
50|$|Here we try {{to explain}} the {{complexity}} of solving the MPF problem {{in terms of the}} number of mathematical operations required for the calculation. i.e. We compare the number of operations required when calculated using the normal method (Here by normal method we mean by methods that do not use message passing or <b>junction</b> <b>trees</b> in short methods that do not use the concepts of GDL)and the number of operations using the generalized distributive law.|$|R
40|$|Latent {{variable}} {{models are}} an elegant framework for capturing rich probabilistic dependencies in many applications. However, current approaches typically parametrize these models using conditional probability tables, and learning relies predominantly on local search heuristics such as Expectation Maximization. Using tensor algebra, we propose an alternative parameterization of latent variable models (where the model structures are <b>junction</b> <b>trees)</b> that still allows for computation of marginals among observed variables. While this novel representation {{leads to a}} moderate {{increase in the number}} of parameter...|$|R
5000|$|Similarly {{for these}} set of domains, the <b>junction</b> <b>tree</b> looks like shown below: ...|$|E
5000|$|For {{the above}} given set of local domains, one can {{organize}} {{them into a}} <b>junction</b> <b>tree</b> as shown below: ...|$|E
50|$|A {{representation}} of a chordal graph as an intersection of subtrees forms a tree decomposition of the graph, with treewidth equal to one less than {{the size of the}} largest clique in the graph; the tree decomposition of any graph G can be viewed in this way as a {{representation of}} G as a subgraph of a chordal graph. The tree decomposition of a graph is also the <b>junction</b> <b>tree</b> of the <b>junction</b> <b>tree</b> algorithm.|$|E
40|$|AbstractAs {{intelligent}} systems are being applied to larger, open {{and more complex}} problem domains, many applications {{are found to be}} more suitably addressed by multiagent systems. Multiply sectioned Bayesian networks provide one framework for agents to estimate what is the true state of a domain so that the agents can act accordingly. Existing methods for multiagent inference in multiply sectioned Bayesian networks are based on linked junction forests. The methods are extensions of message passing in <b>junction</b> <b>trees</b> for inference in single-agent Bayesian networks. Many methods other than message passing in <b>junction</b> <b>trees</b> have been proposed for inference in single-agent Bayesian networks. It is unclear whether these methods can also be extended for multiagent inference. This paper presents the first investigation on this issue. In particular, we consider extending loop cutset conditioning, forward sampling and Markov sampling to multiagent inference. They are compared with the linked junction forest method in terms of off-line compilation, inter-agent messages during communication, consistent local inference, and preservation of agent privacy. The results reveal issues to be considered in investigating other single-agent oriented inference methods. The analysis provides those who implement multiagent probabilistic inference systems with a guide on {{the pros and cons of}} alternative methods...|$|R
40|$|We {{present a}} novel {{approach}} to morph between two isometric poses of the same non-rigid object given as triangular meshes. We model morphs as linear interpolations in a suitable shape space S. For triangulated 3 D polygons, we prove that interpolating linearly in this shape space corresponds to the most isometric morph in R 3. We then extend this shape space to arbitrary triangulations in 3 D using a heuristic approach that utilizes belief propagation on <b>junction</b> <b>trees</b> and show the practical use of the approach in preliminary experiments. ...|$|R
40|$|In {{this paper}} {{we present a}} novel generic mapping between Graphical Games and Markov Random Fields so that pure Nash equilibria in the former can be found by {{statistical}} inference on the latter. Thus, the problem of deciding whether a graphical game has a pure Nash equilibrium, a well-known intractable problem, can be attacked by well-established algorithms such as Belief Propagation, <b>Junction</b> <b>Trees,</b> Markov Chain Monte Carlo and Simulated Annealing. Large classes of graphical games become thus tractable, including all classes already known, but also new classes such as the games with O(log n) treewidth...|$|R
5000|$|For simplicity, we {{describe}} the algorithm on hidden Markov models. It can be easily generalized to dynamic Bayesian networks by using a <b>junction</b> <b>tree.</b>|$|E
5000|$|Thus, by triangulating a graph, we {{make sure}} that the {{corresponding}} <b>junction</b> <b>tree</b> exists. A usual way to do this, is to decide an elimination order for its nodes, and then run the Variable elimination algorithm. This will result to adding more edges to the initial graph, {{in such a way that}} the output will be a chordal graph.The next step is to construct the <b>junction</b> <b>tree.</b> To do so, we use the graph from the previous step, and form its corresponding clique graph. Now the next theorem gives us a way to find a junction tree: ...|$|E
5000|$|For Example, Lets {{consider}} a <b>junction</b> <b>tree</b> constructed {{from the set}} of local domains given above i.e. the set from example 1, Now the Scheduling table for these domains is (where the target vertex is [...] ).|$|E
40|$|Abstract—Genetic Algorithms (GAs) are {{emerging}} as a promis-ing instrument for quality of service (QoS) routing in Mobile Ad hoc Networks (MANETs). They implement an iterative process that can solve the NP search problem of routing with multiple QoS constraints. In each iteration new solutions are found through the mutation and crossover genetic operations. In this paper we focus on an existing GA tree-based application for QoS routing in MANETs which applies the genetic operations on a tree built from the network topology and uses fixed length chromosomes. The chromosome encodes <b>junctions</b> <b>tree</b> crossed by routes. This application suffers in convergence speed because its mutation operator does not allow deep exploration of the search space. The inefficiency of the adopted mutation operation is more evident in networks with big size and high network connectivity, i. e., networks with a larger search space. In this paper, we elaborate on the tree-based application with the main objective of improving its performance. We first introduce a criterion for <b>junctions</b> <b>tree</b> sorting based on their distance from the root. Later, we use chromosome properties due to the sorting criterion to design a sequential mutation technique with adaptive probability that allows faster convergence. We provide simulation results showing {{the effectiveness of the}} proposed enhancements while increasing the MANET size and connectivity. Keywords-Genetic Algorithm; QOS routing; junctions tree; MANE...|$|R
40|$|Multiply {{sectioned}} Bayesian networks (MSBNs) {{provide one}} framework for agents {{to estimate the}} state of a domain. Existing methods for multi-agent inference in MSBNs are based on linked junction forests (LJFs). The methods are extensions of message passing in <b>junction</b> <b>trees</b> for inference in singleagent Bayesian networks (BNs). We consider extending other inference methods in single-agent BNs to multi-agent inference in MSBNs. In particular, we consider distributed versions of loop cutset conditioning and forward sampling. They are compared with the LJF method in terms of off-line compilation, inter-agent messages during communication, consistent local inference, and preservation of agent privacy...|$|R
40|$|International audienceIn {{this paper}} we address {{efficient}} decentralised coordination for cooperative multi-agent systems (framed as DCOPs) by {{taking into account the}} communication and computational resources of the system. We focus on techniques that exploit structural independence among agents' actions to provide optimal solutions to the coordination problem, and, in particular, we use the Generalized Distributed Law (GDL) algorithm. In this settings, we propose a novel resource aware heuristic to build <b>junction</b> <b>trees</b> and to schedule GDL computations across the agents. Our approach aims at minimising directly the total running time of the coordination process, rather than the theoretical complexity of the computation, by considering computational and communication capabilities of agents...|$|R
