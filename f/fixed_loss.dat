23|129|Public
6000|$|In the one {{classic and}} perfect {{literary}} product that ever {{came out of}} Germany--I do not mean [...] "Faust," [...] but Grimm's Fairy Tales--there is a gorgeous story about a boy who went {{through a number of}} experiences without learning how to shudder. In one of them, I remember, he was sitting by the fireside and a pair of live legs fell down the chimney and walked about the room by themselves. Afterwards the rest fell down and joined up; but this was almost an anti-climax. Now that is very charming, and full of the best German domesticity. It suggests truly what wild adventures the traveller can find by stopping at home. But it also illustrates in various ways how that great German influence on England, which is the matter of these essays, began in good things and gradually turned to bad. It began as a literary influence, in the lurid tales of Hoffmann, the tale of [...] "Sintram," [...] and so on; the revisualising of the dark background of forest behind our European cities. That old German darkness was immeasurably livelier than the new German light. The devils of Germany were much better than the angels. Look at the Teutonic pictures of [...] "The Three Huntsmen" [...] and observe that while the wicked huntsman is effective in his own way, the good huntsman is weak in every way, a sort of sexless woman with a face like a teaspoon. But there is more in these first forest tales, these homely horrors. In the earlier stages they have exactly this salt of salvation, that the boy does not shudder. They are made fearful that he may be fearless, not that he may fear. As long as that limit is kept, the barbaric dreamland is decent; and though individuals like Coleridge and De Quincey mixed it with worse things (such as opium), they kept that romantic rudiment upon the whole. But the one disadvantage of a forest is that one may lose one's way in it. And the one danger is not that we may meet devils, but that we may worship them. In other words, the danger is one always associated, by the instinct of folk-lore, with forests; it is enchantment, or the <b>fixed</b> <b>loss</b> of oneself in some unnatural captivity or spiritual servitude. And in the evolution of Germanism, from Hoffmann to Hauptmann, we do see this growing tendency to take horror seriously, which is diabolism. The German begins to have an eerie abstract sympathy with the force and fear he describes, as distinct from their objective. The German is no longer sympathising with the boy against the goblin, but rather with the goblin against the boy. There goes with it, as always goes with idolatry, a dehumanised seriousness; the men of the forest are already building upon a mountain the empty throne of the Superman. Now it is just at this point that I for one, and most men who love truth as well as tales, begin to lose interest. I am all for [...] "going out into the world to seek my fortune," [...] but I do not want to find it--and find it is only being chained for ever among the frozen figures of the Sieges Allees. I {{do not want to be}} an idolator, still less an idol. I am all for going to fairyland, but I am also all for coming back. That is, I will admire, but I will not be magnetised, either by mysticism or militarism. I am all for German fantasy, but I will resist German earnestness till I die. I am all for Grimm's Fairy Tales; but if there is such a thing as Grimm's Law, I would break it, if I knew what it was. I like the Prussian's legs (in their beautiful boots) to fall down the chimney and walk about my room. But when he procures a head and begins to talk, I feel a little bored. The Germans cannot really be deep because they will not consent to be superficial. They are bewitched by art, and stare at it, and cannot see round it. They will not believe that art is a light and slight thing--a feather, even if it be from an angelic wing. Only the slime is at the bottom of a pool; the sky is on the surface. We see this in that very typical process, the Germanising of Shakespeare. I do not complain of the Germans forgetting that Shakespeare was an Englishman. I complain of their forgetting that Shakespeare was a man; that he had moods, that he made mistakes, and, above all, that he knew his art was an art and not an attribute of deity. That is what is the matter with the Germans; they cannot [...] "ring fancy's knell"; their knells have no gaiety. The phrase of Hamlet about [...] "holding the mirror up to nature" [...] is always quoted by such earnest critics as meaning that art is nothing if not realistic. But it really means (or at least its author really thought) that art is nothing if not artificial. Realists, like other barbarians, really believe the mirror; and therefore break the mirror. Also they leave out the phrase [...] "as 'twere," [...] which must be read into every remark of Shakespeare, and especially every remark of Hamlet. What I mean by believing the mirror, and breaking it, can be recorded in one case I remember; in which a realistic critic quoted German authorities to prove that Hamlet had a particular psycho-pathological abnormality, which is admittedly nowhere mentioned in the play. The critic was bewitched; he was thinking of Hamlet as a real man, with a background behind him three dimensions deep--which does not exist in a looking-glass. [...] "The best in this kind are but shadows." [...] No German commentator has ever made an adequate note on that. Nevertheless, Shakespeare was an Englishman; he was nowhere more English than in his blunders; but he was nowhere more successful than in the description of very English types of character. And if anything is to be said about Hamlet, beyond what Shakespeare has said about him, I should say that Hamlet was an Englishman too. He was as much an Englishman as he was a gentleman, and he had the very grave weaknesses of both characters. The chief English fault, especially in the nineteenth century, has been lack of decision, not only lack of decision in action, but lack of the equally essential decision in thought--which some call dogma. And in the politics of the last century, this English Hamlet, as we shall see, played a great part, or rather refused to play it.|$|E
50|$|The film {{chronicles the}} Twenty One quiz show scandals of the 1950s, {{the rise and}} fall of popular {{contestant}} Charles Van Doren after the <b>fixed</b> <b>loss</b> of Herb Stempel, and Congressional investigator Richard Goodwin's subsequent probe. Goodwin co-produced the film. The film received generally positive reviews and was nominated for several awards, including a Best Picture Oscar nomination and several Golden Globes. It had a disappointing box office return.|$|E
40|$|In their classic paper, Rothschild and Stiglitz (1976) analyze a {{competitive}} insurance market under asymmetric information with two policyholder risk types, or probabilities {{of having a}} <b>fixed</b> <b>loss</b> amount. Their model is a screening model where uninformed insurers move first offering a menu of contracts, defined as price-quantity pairs, to informed policyholders. The...|$|E
3000|$|... where IQ,classG is the {{additional}} quiescent current in class-G mode and LclassG[W] refers to further <b>fixed</b> <b>losses</b> in the class-G circuitry.|$|R
40|$|Abstract—. An {{experimental}} {{investigation has}} been carried out into the factors that affect the energy consumption of an escalator. The type of energy consumed by the escalator is classified into <b>fixed</b> <b>losses</b> and variable losses. The two main factors that affect the energy consumption have been identified as the mechanical design of the escalator (that affects the <b>fixed</b> <b>losses)</b> and the number of passengers using the escalator (that affects the variable losses). Experimental and survey work have been carried out on a large number of heavy duty escalators to find the relationship between the mechanical design aspects, the number of passengers using the escalator and the energy it consumes. The generic mechanical and electrical design features of the escalators were extracted to allow a grouping of the escalator in order to find general rules. Surveys were also carried out on the total daily number of passengers using an escalator and the corresponding daily total energy consumed. The relation between the various parameters has been analysed with good correlation. Index Terms—Escalator, energy consumption, power, mechanical design, passenger, walking. I...|$|R
40|$|This work {{introduces}} {{a new class}} of PT-symmetry grating assisted devices for switching or modulation applications. Their operation is based on a four-wave interaction, thus marking a step in the further development of PT-symmetry devices which currently are essentially based on two-wave interactions. A remarkable feature of the new device is that all their properties also hold for the case of imperfect PT-symmetry operation, corresponding to the important practical case of <b>fixed</b> <b>losses...</b>|$|R
40|$|Possible future {{changes of}} {{clustering}} and return periods (RPs) of European storm series with high potential losses are quantified. Historical storm series are identified using 40 winters of reanalysis. Time series of top events (1, 2 or 5 year return levels (RLs)) {{are used to}} assess RPs of storm series both empirically and theoretically. Additionally, 800 winters of general circulation model simulations for present (1960 – 2000) and future (2060 – 2100) climate conditions are investigated. Clustering is identified for most countries, and estimated RPs are similar for reanalysis and present day simulations. Future changes of RPs are estimated for fixed RLs and <b>fixed</b> <b>loss</b> index thresholds. For the former, shorter RPs are found for Western Europe, but changes are small and spatially heterogeneous. For the latter, which combines the effects of clustering and event ranking shifts, shorter RPs are found everywhere except for Mediterranean countries. These changes are generally not statistically significant between recent and future climate. However, the RPs for the <b>fixed</b> <b>loss</b> index approach are mostly beyond the range of pre-industrial natural climate variability. This is not true for fixed RLs. The quantification of losses associated with storm series permits a more adequate windstorm risk assessment in a changing climate...|$|E
40|$|We {{consider}} a stochastic model of an ECN/RED gateway with competing TCP sources sharing the capacity. As {{the number of}} competing flows becomes large, the queue behavior at the gateway can be described by a two-dimensional recursion and the throughput behavior of individual TCP flows becomes asymptotically independent. The steady-state regime of the limiting behavior can be calculated from a well-known TCP throughput model with <b>fixed</b> <b>loss</b> probability. In addition, a Central Limit Theorem is presented, yielding insight into {{the relationship between the}} queue fluctuation and the marking probability function. We confirm the results by simulations and discuss their implications for network dimensioning...|$|E
40|$|Abstract. Consider {{the problem}} of {{learning}} how long {{to wait for a}} bus before walking, experimenting each day and assuming that the bus arrival times are independent and identically distributed random variables with an unknown distribution. Similar uncertain optimal stopping problems arise when devising power-saving strategies, e. g., learning the optimal disk spin-down time for mobile computers, or speeding up certain types of satisficing search procedures by switching from a potentially fast search method that is unreliable, to one that is reliable, but slower. Formally, the problem can be described as a repeated game. In each round of the game an agent is waiting for an event to occur. If the event occurs while the agent is waiting, the agent suffers a loss that is the sum of the event’s “arrival time ” and some <b>fixed</b> <b>loss.</b> If the agents decides to give up waiting before the event occurs, he suffers a loss that is the sum of the waiting time and some other <b>fixed</b> <b>loss.</b> It is assumed that the arrival times are independent random quantities with the same distribution, which is unknown, while the agent knows the loss associated with each outcome. Two versions of the game are considered. In the full information case the agent observes the arrival times regardless of its actions, while in the partial information case the arrival time is observed only if it does not exceed the waiting time. After some general structural observations about the problem, we present a number of algorithms for both cases that learn the optimal weighting time with nearly matching minimax upper and lower bounds on their regret. ...|$|E
5000|$|<b>Fix</b> a <b>loss</b> {{function}} , for example, {{the square}} loss [...] For a given distribution [...] on , the expected {{risk of a}} hypothesis (a function) [...] is ...|$|R
5000|$|EG711 (Enhanced G.711) — <b>fixed</b> bit rate, <b>loss</b> tolerant, {{narrowband}} ...|$|R
30|$|We {{can assume}} that points A and A' are in {{the origins of the}} {{respective}} coordinate systems. One of the sides of the quadrilateral A'B'C'D' can be <b>fixed</b> without <b>loss</b> of generality. We will fix the side A'D' to be vertical.|$|R
40|$|Bronchial asthma is {{very common}} in {{childhood}} but the occurrence of wheeze with viral infections makes asthma difficult to diagnose in the pre-school child. Longitudinal studies {{suggest that there is}} a loss of airway function associated with early childhood asthma. Extrapolating from adult disease and the few tissue-based studies of children, this would appear to be related to abnormal postnatal development or remodelling of the airway walls. This appears to be associated with persistent airway inflammation without clinical evidence of airways obstruction. Abnormally thickened airways may be the mechanism underlying both bronchial hyper-responsiveness and <b>fixed</b> <b>loss</b> of respiratory function. The challenges for the future are to identify those children among the pre-school wheezers who will become asthmatic and to construct trials of therapies that may potentially prevent the development of clinical asthma or ameliorate the associated loss of airway function...|$|E
40|$|Evidence {{from recent}} studies of brain {{function}} in schizophrenia confirms that the frontal underactivity reported in earlier studies is a feature only of cases with decreased psychomotor activity, while patients with different symptom profile have different patterns of cerebral activity. Each {{of the three major}} groups of schizophrenic symptoms reflects a specific pattern of aberrant cerebral activity in association cortex of frontal, parietal and temporal lobes, and in related subcortical nuclei. These patterns indicate imbalances between neuronal activity at diverse interconnected brain sites, rather than abnormal function at a single location. Furthermore, during the performance of executive tasks, schizophrenic subjects exhibit impaired frontal activation, while during memory tasks there a tendency towards impaired temporal lobe activation. The difficulty activating a particular region does not reflect <b>fixed</b> <b>loss</b> of function. Rather the evidence indicates that the characteristic feature of schizophrenia is disturbed connectivity between cerebral areas. Postal address...|$|E
40|$|We analyze {{cooperative}} {{behavior of}} participants who faced a loss. In particular, we extend the Public Good Game by a <b>fixed</b> <b>loss</b> {{in the beginning of}} every period. We show that humans change their behavior compared to corresponding studies with gains only. First, in contrast to literature on gains, we observe significant order effects. When participants first play a treatment with punishment, they cooperate less and face higher punishment costs than when first playing a treatment without punishment. The changes are that drastic that punishment does not pay in the first case, while it does in the later. Second, for participants first playing without punishment the contributions in the very first period of play determine the contributions throughout both treatments of the game, yielding higher contributions in the punishment treatment than when playing with gains. Participants punishing first, show no comparable behavior. [...] public good,punishment,losses,experiment...|$|E
3000|$|... 0 -term) on {{the optimal}} solution. The coordinate-descent {{approach}} updates {{the estimate of}} each element of the sparse vector x, while keeping the others <b>fixed.</b> Without <b>loss</b> of generality, the solution for the one-dimensional version of (37) is given by the following theorem.|$|R
2500|$|Hobson, Chris. Vietnam Air <b>Losses,</b> USAF/Navy/Marine, <b>Fixed</b> Wing Aircraft <b>Losses</b> in Southeast 1961-1973. North Branch, Minnesota: Specialty Press, 2001[...]|$|R
40|$|International audienceThe {{so-called}} PT symmetric devices, which feature ε(−x) =ε(x) * {{associated with}} parity-time symmetry, incorporate both gain and loss and can present a singular eigenvalue behaviour around a critical transition point. The scheme, typically based on co-directional coupled waveguides, is here transposed {{to the case}} of variable gain on one arm with <b>fixed</b> <b>losses</b> on the other arm. In this configuration, the scheme exploits the full potential of plasmonics by making a beneficial use of their losses to attain a critical regime that makes switching possible with much lowered gain excursions. Practical implementations are discussed based on existing attempts to elaborate coupled waveguide in plasmonics, and based also on the recently proposed hybrid plasmonics waveguide structure with a small low-index gap, the PIROW (Plasmonic Inverse-Rib Optical Waveguide) ...|$|R
40|$|The {{behavior}} of connection transmitting packets into a network {{according to a}} general additive-increase multiplicative-decrease (AIMD) algorithm is investigated. It is assumed that loss of packets occurs in clumps. When a packet is lost, {{a certain number of}} subsequent packets are also lost (correlated losses). The stationary {{behavior of}} this algorithm is analyzed when the rate of occurrence of clumps becomes arbitrarily small. From a probabilistic point of view, it is shown that exponential functionals associated to compound Poisson processes play a key role. A formula for the fractional moments and some density functions are derived. Analytically, to get the explicit expression of the distributions involved, the natural framework of this study {{turns out to be the}} q-calculus. Different loss models are then compared using concave ordering. Quite surprisingly, it is shown that, for a <b>fixed</b> <b>loss</b> rate, the correlated loss model has a higher throughput than an uncorrelated loss model...|$|E
40|$|Abstract – The {{probability}} {{of losing a}} customer in M/G/n/ 0 and GI/M/n/ 0 loss queuing systems with heterogeneous servers is minimized. The first system uses a queue discipline in which a customer who arrives when there are free servers chooses {{any one of them}} with equal probability, but is lost otherwise. Provided that the sum of the servers rates are <b>fixed,</b> <b>loss</b> probability in this system attains minimum value when all the service rates are equal. The second system uses queue discipline, in which a customer who enters into the system is assigned to the server with the lowest number. Loss probability in this system takes the minimum value in the case when the fastest server rule is used in which an incoming customer is served by the free server with the shortest mean service time. If the mean of the arrival distribution is fixed, then loss probability is minimized by deterministic arrival distribution...|$|E
40|$|ABSTRACT. The {{behavior}} of connection transmitting packets into a network {{according to a}} general additive-increase multiplicative-decrease (AIMD) algorithm is investigated. It is assumed that loss of packets occurs in clumps. When a packet is lost, {{a certain number of}} subsequent packets are also lost (correlated losses). The stationary {{behavior of}} this algorithm is analyzed when the rate of occurrence of clumps becomes arbitrarily small. From a probabilistic point of view, it is shown that exponential functionals associated to compound Poisson processes play a key role. A formula for the fractional moments and some density functions are derived. Analytically, to get the explicit expression of the distributions involved, the natural framework of this study {{turns out to be the}} q-calculus. Different loss models are then compared using concave ordering. Quite surprisingly, it is shown that, for a <b>fixed</b> <b>loss</b> rate, the correlated loss model has a higher throughput than an uncorrelated loss model. CONTENT...|$|E
3000|$|... [...]. A {{comparison}} between nonfixed and <b>fixed</b> intercept path <b>loss</b> models {{is made to}} examine which approach provides a better fit to experimental data.|$|R
40|$|International audiencePT-symmetric structures, {{such as a}} pair of coupled waveguides with {{balanced}} loss/gain, {{exhibit a}} singularity of their eigenvalues around an exceptional point, hence a large apparent differential gain. In the case of <b>fixed</b> <b>losses</b> and variable gain, typical of plasmonic systems, a similar behavior emerges but the singularity is smoothened, especially in more confined structures. This reduces the differential gain around the singular point. Our analysis ascribes the origin of this behavior to a complex coupling between the waveguides once gain is present in an unsymmetrical fashion, even if guides feature the same modal gains in isolation. We demonstrate that adjunction of a real index variation to the variable waveguide heals the singularity nearly perfectly, as it restores real coupling. We illustrate the success of the approach with two geometries, planar or channel, and with different underlying physics, namely dielectric or plasmonic...|$|R
50|$|Candidatus Scalindua wagneri is a Gram-negative coccoid-shaped {{bacterium}} {{that was}} first isolated from a wastewater treatment plant. This bacterium is an obligate anaerobic chemolithotroph that undergoes anaerobic ammonium oxidation (anammox). It {{can be used}} in the wastewater treatment industry in nitrogen reactors to remove nitrogenous wastes from wastewater without contributing to <b>fixed</b> nitrogen <b>loss</b> and greenhouse gas emission.|$|R
40|$|The {{recently}} developed notion of TCP-compatibility {{has led to}} a number of proposals for alternative congestion control algorithms whose long-term throughput as a function of a steady-state loss rate {{is similar to that of}} TCP. Motivated by the needs of some streaming and multicast applications, these algorithms seem poised to take the current TCP-dominated Internet to an Internet where many congestion control algorithms co-exist. An important characteristic of these alternative algorithms is that they are slowly-responsive, refraining from reacting as drastically as TCP to a single packet loss. However, the TCP-compatibility criteria explored so far in the literature considers only the static condition of a <b>fixed</b> <b>loss</b> rate. This paper investigates the behavior of slowly-responsive, TCPcompatible congestion control algorithms under more realistic dynamic network conditions, addressing the fundamental question of whether these algorithms are safe to deploy in the public Internet. We study persistent loss rates, long- and short-term fairness properties, bottleneck link utilization, and smoothness of transmission rates. 1...|$|E
40|$|The {{behavior}} of a connection transmitting packets into a network according to a general additive-increase multiplicative-decrease (AIMD) algorithm is investigated. It is assumed that loss of packets occurs in clumps. When a packet is lost, {{a certain number of}} subsequent packets are also lost (correlated losses). The stationary {{behavior of}} this algorithm is analyzed when the rate of occurrence of clumps becomes arbitrarily small. From a probabilistic point of view, it is shown that exponential functionals associated to compound Poisson processes play a key role. A formula for the fractional moments and some density functions are derived. Analytically, to get the explicit expression of the distributions involved, the natural framework of this study {{turns out to be the}} q-calculus. Different loss models are then compared using concave ordering. Quite surprisingly, it is shown that, for a <b>fixed</b> <b>loss</b> rate, the correlated loss model has a higher throughput than an uncorrelated loss model. 1. Introduction. TCP (Transmission Control Protoco...|$|E
40|$|In {{this work}} we {{investigate}} {{the effectiveness of}} continuous-variable (CV) entangled states, transferred through high-loss atmospheric channels, {{as a means of}} viable quantum key distribution (QKD) between terrestrial stations and low-Earth orbit (LEO) satellites. In particular, we investigate the role played by the Gaussian CV states as compared to non-Gaussian states. We find that beam-wandering induced atmospheric losses lead to QKD performance levels that are in general quite different from those found in fixed-attenuation channels. For example, circumstances can be found where no QKD is viable at some <b>fixed</b> <b>loss</b> in fiber but is viable at the same mean loss in fading channels. We also find that, in some circumstances, the QKD relative performance of Gaussian and non-Gaussian states can in atmospheric channels be the reverse of that found in fixed-attenuation channels. These findings show that the nature of the atmospheric channel can have a large impact on the QKD performance. Our results should prove useful for emerging global quantum communications that use LEO satellites as communication relays. Comment: 7 pages, 5 figure...|$|E
30|$|Efficiency of {{domestic}} ventilation {{waste heat recovery}} systems (WHRS) depends {{not only on the}} amount of waste heat recovered, but also on the energy involved in running fans to drive air through the system. Computational fluid dynamics (CFD) can be a powerful tool for analysing WHRS losses (thus predicting fan energy usage), but the computational effort involved can limit the value of CFD as a practical design tool. This study presents a range of assumptions and simplifications that can be applied to reduce the computational effort associated with the CFD analysis of a WHRS. The importance of experimental validation to assess the effect of errors introduced by the simplifying assumptions is discussed. In an example case, application of the methods presented have allowed total pressure <b>losses</b> (excluding the <b>fixed</b> <b>losses</b> through the heat exchanger) to be reduced by over 50  % in comparison with an initial prototype design, with proportional reduction in fan energy usage. This highlights the value of sufficiently simplified CFD analyses within a typical WHRS product development cycle.|$|R
40|$|The {{free cooling}} {{behavior}} of a wet granular gas is studied in one dimension. We employ a particularly simple model {{system in which the}} interaction of wet grains is characterized by a <b>fixed</b> energy <b>loss</b> assigned to each collision. Macroscopic laws of energy dissipation and cluster formation are studied on the basis of numerical simulations and mean-field analytical calculations. We find a number of remarkable scaling properties which may shed light on earlier unexplained results for related systems...|$|R
40|$|Small, three- and five-stage {{depressed}} collectors {{were evaluated}} {{in conjunction with}} a 4. 8 - to 9. 6 -GHz TWT of 325 - to 675 -W power output and a beam of 0. 5 microperv. The multistage depressed collector (MDC) performed well even though its design had been optimized for a TWT of identical design but considerably less output power. Despite large, <b>fixed</b> <b>losses</b> significant efficiency enhancement was demonstrated with both the three- and five-stage depressed collectors. At saturated rf power output, the improvement in the overall efficiency ranged from a factor of 2. 5 to 3. 0 for the three-stage collector and a factor of 3. 0 to 3. 5 for the five-stage collector. At saturation three-stage collector efficiencies of 77 to 80 percent and five-stage collector efficiencies of 81 to 84 percent were obtained across the frequency band. An overall efficiency of 37. 0 to 44. 3 percent across the frequency band of 4. 8 to 9. 6 GHz was demonstrated with the use of harmonic injection. For operation below saturation, even larger relative improvements in the overall TWT efficiency were demonstrated. Collector performance was relatively insensitive to the degree of regulation of the collector power supply...|$|R
40|$|Streaming {{video is}} {{becoming}} the predominant type of traffic over the Internet with reports forecasting the video content to account for 80 % of all traffic by 2019. With significant investment on Internet backbone, the main bottleneck remains at the edge servers (e. g., WiFi access points, small cells, etc.). In this work, we propose and prove the optimality of a multiuser resource allocation mechanism operating at the edge server that minimizes the probability of stalling of video streams due to buffer under-flows. Our proposed policy utilizes Media Presentation Description (MPD) files of clients that are sent in compliant to Dynamic Adaptive Streaming over HTTP (DASH) protocol to be cognizant of the deadlines {{of each of the}} media file to be displayed by the clients. Then, the policy schedules the users in the order of their deadlines. After establishing the optimality of this policy to minimize the stalling probability for a network with links associated with <b>fixed</b> <b>loss</b> rates, the utility of the algorithm is verified under realistic network conditions with detailed NS- 3 simulations...|$|E
40|$|Abstract- In this work, the {{behaviour}} {{of systems}} with three {{degrees of freedom}} is modelled for a <b>fixed</b> <b>loss</b> factor. The loss factor used in this work is 0. 075. Four models are studied and the displacements {{as well as the}} energies stored in the systems are computed using the simulations. The governing equations are numerically solved using MATLAB. The dampers are varied in position and in numbers to derive four models. The simulation results show that among the four models with three degrees of freedom, the model with four dampers added to the system yields the lowest displacement, while the model with no dampers has the largest displacement for oscillator on which the force is acting. The models with dampers attached at different positions yield the displacements in between these two cases. The energy stored in the oscillator becomes large after the attached dampers are removed. Most of the energy is absorbed by the damper attached to the oscillator on which the force is acting. The magnitudes of displacements, velocities and energies for models with different positions of dampers are presented in this paper. Keywords [...] Vibration, 3 degrees of freedom, dampers, loss factor. I...|$|E
40|$|The {{constrained}} compartmentalized {{knapsack problem}} {{can be seen as}} an extension of the constrained knapsack problem. However, the items are grouped into different classes so that the overall knapsack has to be divided into compartments, and each compartment is loaded with items from the same class. Moreover, building a compartment incurs a fixed cost and a <b>fixed</b> <b>loss</b> of the capacity in the original knapsack, and the compartments are lower and upper bounded. The objective is to maximize the total value of the items loaded in the overall knapsack minus the cost of the compartments. This problem has been formulated as an integer non-linear program, and in this paper, we reformulate the non-linear model as an integer linear master problem with a large number of variables. Some heuristics based on the solution of the restricted master problem are investigated. A new and more compact integer linear model is also presented, which can be solved by a branch-and-bound commercial solver that found most of the optimal solutions for the constrained compartmentalized knapsack problem. On the other hand, heuristics provide good solutions with low computational effort. (C) 2011 Elsevier BM. All rights reserved. FapespFundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP) Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq) CNP...|$|E
50|$|BRS(Binary Reed-Solomon) Coding, which {{belongs to}} a RS code, {{is a way of}} {{encoding}} that can <b>fix</b> node data <b>loss</b> in a distributed storage environment, It has MDS’s(Maximum Distance Separable) encoding properties，Its encoding and decoding rate outperforms conventional RS coding and optimum CRS coding.|$|R
40|$|This paper studies {{different}} auctions {{of supply}} functions {{in a local}} market and a simple network market of a homogeneous good with two nodes and a <b>fixed</b> transmission <b>loss</b> per unit of the good. We study problems of existence, uniqueness and computation of Nash equilibria for these models. We also obtain the estimate of Nash equilibria deviation from the Walrasian equilibrium for each variant. We consider the problem of optimal auction organization {{from the point of}} view of the social welfare maximization. Russia, supply function auction, Cournot, Vickrey, Russian electricity market...|$|R
40|$|The high {{penetration}} of {{distributed energy resources}} (DER) in distribution networks and the competitive environment of electricity markets impose the use of new approaches in several domains. The network cost allocation, traditionally used in transmission networks, should be adapted and used in the distribution networks considering the specifications of the connected resources. The main goal {{is to develop a}} fairer methodology trying to distribute the distribution network use costs to all players which are using the network in each period. In this paper, a model considering different type of costs (<b>fixed,</b> <b>losses,</b> and congestion costs) is proposed comprising the use of a large set of DER, namely distributed generation (DG), demand response (DR) of direct load control type, energy storage systems (ESS), and electric vehicles with capability of discharging energy to the network, which is known as vehicle-to-grid (V 2 G). The proposed model includes three distinct phases of operation. The first phase of the model consists in an economic dispatch based on an AC optimal power flow (AC-OPF); in the second phase Kirschen's and Bialek's tracing algorithms are used and compared to evaluate the impact of each resource in the network. Finally, the MW-mile method is used in the third phase of the proposed model. A distribution network of 33 buses with large {{penetration of}} DER is used to illustrate the application of the proposed model...|$|R
