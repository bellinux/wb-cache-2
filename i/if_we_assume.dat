2980|10000|Public
25|$|Adaptive estimation. <b>If</b> <b>we</b> <b>assume</b> that error {{terms are}} {{independent}} from the regressors , the optimal estimator is the 2-step MLE, {{where the first}} step is used to non-parametrically estimate the distribution of the error term.|$|E
25|$|<b>If</b> <b>we</b> <b>assume</b> the {{validity}} of Kepler's laws, then the obvious way to resolve this discrepancy is {{to conclude that the}} mass distribution in spiral galaxies is not {{similar to that of the}} solar system. In particular, there is a lot of non-luminous matter in the outskirts of the galaxy ("dark matter").|$|E
25|$|Through its {{slope and}} y-intercept we can obtain vmon and K, which are {{constants}} for each adsorbent/adsorbate pair {{at a given}} temperature. vmon {{is related to the}} number of adsorption sites through the ideal gas law. <b>If</b> <b>we</b> <b>assume</b> that the number of sites is just the whole area of the solid divided into the cross section of the adsorbate molecules, we can easily calculate the surface area of the adsorbent.|$|E
30|$|In this context, <b>if</b> <b>we</b> <b>assumed</b> {{that players}} are selfish, the unique subgame perfect Nash {{equilibrium}} {{would result in}} the inefficient outcome where all players contribute zero at every period.|$|R
6000|$|... "Ah, yes, I remember; {{she was a}} Reynolds, {{for both}} the little boys could be excited to fury <b>if</b> <b>we</b> <b>assumed</b> {{that she was a}} fox. You don't mean that she went wrong?" ...|$|R
50|$|He {{went on to}} {{say that}} indulging in fantasy {{operations}} such as Operation Chico would only obscure the real dangers that threaten mankind <b>if</b> <b>we</b> <b>assumed</b> that exercises like this would provide protection against nuclear disaster.|$|R
25|$|A {{useful and}} {{straightforward}} calculation {{can be done}} <b>if</b> <b>we</b> <b>assume</b> that interest, after expenses, taxes, and inflation is zero. Assume that in real (after-inflation) terms, your salary never changes during your w years of working life. During your p years of pension, you have a living standard that costs a replacement ratio R {{times as much as}} your living standard in your working life. Your working life living standard is your salary less the proportion of salary Z that you need to save. Calculations are per unit salary (e.g., assume salary = 1).|$|E
25|$|For example, {{consider}} the deterministic sorting algorithm quicksort. This solves {{the problem of}} sorting a list of integers that is given as the input. The worst-case is when the input is sorted or sorted in reverse order, and the algorithm takes time O(n2) for this case. <b>If</b> <b>we</b> <b>assume</b> that all possible permutations of the input list are equally likely, the average time taken for sorting is O(n log n). The best case occurs when each pivoting divides the list in half, also needing O(n log n) time.|$|E
25|$|That is, <b>if</b> <b>we</b> <b>assume</b> that NP, {{the class}} of nondeterministic {{polynomial}} time problems, can be contained in the non-uniform polynomial time complexity class P/poly, then this assumption implies {{the collapse of the}} polynomial hierarchy at its second level. Such a collapse is believed unlikely, so the theorem is generally viewed by complexity theorists as evidence for the nonexistence of polynomial size circuits for SAT or for other NP-complete problems. A proof that such circuits do not exist would imply that P ≠ NP. As P/poly contains all problems solvable in randomized polynomial time (Adleman's theorem), the Karp–Lipton theorem is also evidence that the use of randomization does not lead to polynomial time algorithms for NP-complete problems.|$|E
3000|$|When {{one talks}} of a q-extension, q is {{variously}} considered as an indeterminate, a complex number q∈C or a p-adic number q∈C_p. <b>If</b> q∈C, <b>we</b> <b>assume</b> that [...] q < 1. <b>If</b> q∈C_p, <b>we</b> <b>assume</b> that [...] 1 -q _p< 1 (see, for details, [1 – 16]).|$|R
3000|$|Maximization of (4), using {{optimisation}} algorithms {{such as the}} Newton-Raphson, yields consistent {{estimates of}} θ, given correct specification of the model; that is, the true data generating process (DGP) is NB 2 -Logit. Alternatively, <b>if</b> <b>we</b> <b>assumed</b> that [...]...|$|R
30|$|Given a {{knowledge}} base, {{we developed a}} rule-based algorithm to generate biology questions and developed a method to collect biology questions by searching the web with keywords. When 12 biology students judged the pedagogical merit, depth, relevance, fluency, and ambiguity of the {{two different types of}} questions to professional human-made questions, the pattern of results depended on whether <b>we</b> <b>assumed</b> the judgements were a ratio scale or an ordinal scale. <b>If</b> <b>we</b> <b>assumed</b> a ratio scale, then professionally generated questions were rated as deeper and more pedagogically beneficial than questions generated from the knowledge base but there were no differences in fluency and ambiguity. On the other hand, <b>if</b> <b>we</b> <b>assumed</b> an ordinal scale, then there were differences in fluency and ambiguity but no differences in depth or pedagogy except that depth of the questions from the web was smaller than that of the machine-generated questions. Correlations of the measures suggest that depth and pedagogical merit were highly correlated but no other pair of measures were.|$|R
25|$|Some {{scholars}} claim, {{that while}} acceptance of Jordanes {{at face value}} may be too naive, a totally skeptical view is not warranted. For example, Jordanes says that the Goths originated in Scandinavia 1490 BC. Austrian historian Herwig Wolfram, believes {{that there might be}} a kernel of truth in that claim, <b>if</b> <b>we</b> <b>assume</b> that a clan of the Gutae left Scandinavia long before the establishment of the Amali in the leadership of the Goths. This clan might have contributed to the ethnogenesis of the Gutones in east Pomerania (see Wielbark culture). Another example is the name of the king Cniva which David S. Potter thinks is genuine because, since it doesn't appear in the fictionalized genealogy of Gothic kings given by Jordanes, he must have found it in a genuine 3rd-century source.|$|E
25|$|Sumati Shatakam, {{which is}} a neeti ("moral"), {{is one of the}} most famous Telugu Shatakams. Shatakam is {{composed}} of more than a 100 padyalu (poems). According to many literary critics Sumati Shatakam was composed by Baddena Bhupaludu (Telugu: బద్దెన భూపాల) (CE 1220–1280). He was also known as Bhadra Bhupala. He was a Chola prince and a vassal under the Kakatiya empress Rani Rudrama Devi, and a pupil of Tikkana. <b>If</b> <b>we</b> <b>assume</b> that the Sumati Shatakam was indeed written by Baddena, it would rank as one of the earliest Shatakams in Telugu along with the Vrushadhipa Satakam of Palkuriki Somanatha and the Sarveswara Satakam of Yathavakkula Annamayya. The Sumatee Shatakam is also one of the earliest Telugu works to be translated into a European language, as C. P. Brown rendered it in English in the 1840s.|$|E
25|$|In his Ph.D. dissertation, John Nash {{proposed}} two {{interpretations of}} his equilibrium concept, {{with the objective}} of showing how equilibrium points ``(...) can be connected with observable phenomenon. One interpretation is rationalistic: <b>if</b> <b>we</b> <b>assume</b> that players are rational, know the full structure of the game, the game is played just once, and there is just one Nash equilibrium, then players will play according to that equilibrium. This idea was formalized by Aumann, R. and A. Brandenburger, 1995, ``Epistemic Conditions for Nash Equilibrium, Econometrica, 63, 1161-1180 who interpreted each player's mixed strategy as a conjecture about the behaviour of other players and have shown that if the game and the rationality of players is mutually known and these conjectures are commonly know, then the conjectures must be a Nash equilibrium (a common prior assumption is needed for this result in general, but not in the case of two players. In this case, the conjectures need only be mutually known).|$|E
5000|$|... and <b>if</b> <b>we</b> further <b>assume</b> that [...] is a constant, {{we arrive}} at the simple form ...|$|R
30|$|<b>If</b> <b>we</b> {{additionally}} <b>assume</b> that w(z)/z^p is univalent in {{the unit}} disc, {{then we have the}} following result.|$|R
5000|$|<b>If</b> <b>we</b> also <b>assume</b> {{that the}} {{solutions}} {{are independent of}} y as , then they also satisfy Burgers' equation: ...|$|R
25|$|However, {{the time}} average of a {{function}} of the form cos(ωt+k) is zero provided that ω is nonzero. Therefore, the only product terms that have a nonzero average are those where the frequency of voltage and current match. In other words, it is possible to calculate active (average) power by simply treating each frequency separately and adding up the answers. Furthermore, <b>if</b> <b>we</b> <b>assume</b> the voltage of the mains supply is a single frequency (which it usually is), this shows that harmonic currents are a bad thing. They will increase the rms current (since there will be non-zero terms added) and therefore apparent power, but they will have no effect on the active power transferred. Hence, harmonic currents will reduce the power factor. Harmonic currents can be reduced by a filter placed at the input of the device. Typically this will consist of either just a capacitor (relying on parasitic resistance and inductance in the supply) or a capacitor-inductor network. An active power factor correction circuit at the input would generally reduce the harmonic currents further and maintain the power factor closer to unity.|$|E
500|$|If e1, …, en is the {{standard}} basis for Rn, then y(t) can also be written as [...] <b>If</b> <b>we</b> <b>assume</b> that the derivative of a vector-valued function retains the linearity property, then the derivative of y(t) must be ...|$|E
500|$|<b>If</b> <b>we</b> <b>assume</b> that v {{is small}} and that the {{derivative}} varies continuously in a, then [...] is approximately equal to , and therefore the right-hand side is approximately zero. The left-hand side can be rewritten {{in a different way}} using the linear approximation formula with [...] substituted for v. The linear approximation formula implies: ...|$|E
5000|$|Now, <b>if</b> <b>we</b> also <b>assume</b> that f(xi, β) {{takes the}} log-linear Cobb-Douglas form, the {{model can be}} written as: ...|$|R
5000|$|<b>If</b> <b>we</b> further <b>assume</b> (as seems reasonable) {{that there}} are no {{long-term}} supply shocks, this can be simplified to become: ...|$|R
5000|$|<b>If</b> <b>we</b> also <b>assume</b> that [...] is {{a smooth}} closed {{manifold}} and [...] is a -function, the following useful property holds: ...|$|R
500|$|Cochrane goes on, {{in chapter}} six, to {{consider}} Marxism. Unlike the other political theories explored in An Introduction to Animals and Political Theory, Marxism is purportedly not a normative account but a scientific theory which predicts and explains {{the end of}} the state and the beginning of communism. This is understood as the inevitable conclusion of the history of the changing forms of economic relationships. Cochrane outlines the discontinuities between humans and animals that exist for Karl Marx and considers the extent to which animal-rights thinking is an example of bourgeois morality. These analyses serve to illustrate how Marxist thinking can be used to exclude animals, but counterarguments are offered. Cochrane then draws upon the work of Catherine Perlow and Barbara Noske, who have argued that animals may represent an exploited group in a Marxist sense, but he is critical of the argument that this exploitation is caused by capitalism and that overthrowing capitalism would be a necessary step for achieving justice. He next considers the work of David Sztybel and Ted Benton, who have drawn upon the adage of [...] "From each according to his ability, to each according to his need" [...] in relation to animals; Cochrane is wary about the use of the phrase for three reasons. First, it is unclear how central the idea is to Marxist thought; second, it is a principle only for societies in advanced stages of communism; and third, even <b>if</b> <b>we</b> <b>assume</b> we can know the needs of animals, the principle would entail the extension of justice beyond sentient animals, which is an idea that Cochrane rejects. Finally, Cochrane considers Benton's proposal that liberal rights-based approaches to animal justice cannot achieve their goal, and that Marxism can be used as a resource for political achievement. This is, for Cochrane, Marxism's most important contribution in the area.|$|E
2500|$|<b>If</b> <b>we</b> <b>assume</b> that {{filtering}} and differentiation commute, then ...|$|E
2500|$|The camera matrix derived above can be {{simplified}} {{even further}} <b>if</b> <b>we</b> <b>assume</b> that f = 1: ...|$|E
3000|$|Note that <b>if</b> <b>we</b> further <b>assume</b> that nodes {{are aware}} of the C, M {{parameters}} of other nodes, then we can replace [...]...|$|R
3000|$|<b>If</b> <b>we</b> now <b>assume</b> {{further that}} a control or {{reaction}} mechanism is imposed, {{a slightly more}} complicated nonhomogeneous model {{such as the following}} [...]...|$|R
5000|$|... where <b>we</b> <b>assumed</b> all {{contributions}} to our measurement uncertainty statistically independent and thus got sum uncertainty by summation of standard deviations. <b>If</b> <b>we</b> further <b>assume</b> that all light pulses are similar {{and have the}} same phase uncertainty, thence [...]|$|R
2500|$|... {{we put the}} {{expression}} of the electric field in the equation and make some calculations. <b>If</b> <b>we</b> <b>assume</b> the slowly varying envelope approximation: ...|$|E
2500|$|The above {{expression}} can {{be integrated}} by parts. <b>If</b> <b>we</b> <b>assume</b> that there is periodic motion, the boundary term in the integral by parts disappears: ...|$|E
2500|$|In other words, [...] is an {{unbiased}} estimator {{of the first}} moment. <b>If</b> <b>we</b> <b>assume</b> that the mean [...] lies in the interval , then Arg will be a (biased) estimator of the mean [...]|$|E
30|$|Instead of the {{condition}} p≥ 2, <b>if</b> <b>we</b> only <b>assume</b> that p> 1, then we also can obtain {{the uniqueness of the}} solutions in some cases.|$|R
3000|$|..., {{the minimum}} {{eigenvalue}} {{of the sample}} covariance matrix can be a reasonable estimate of the noise power. <b>If</b> <b>we</b> further <b>assume</b> {{to know the difference}} [...]...|$|R
40|$|OBJECTIVE: To {{assess the}} reporting, extent, and {{handling}} of loss to follow-up {{and its potential}} impact on the estimates {{of the effect of}} treatment in randomised controlled trials. DESIGN: Systematic review. We calculated the percentage of trials for which the relative risk would no longer be significant under a number of assumptions about the outcomes of participants lost to follow-up. DATA SOURCES: Medline search of five top general medical journals, 2005 - 07. ELIGIBILITY CRITERIA: Randomised controlled trials that reported a significant binary primary patient important outcome. RESULTS: Of the 235 eligible reports identified, 31 (13 %) did not report whether or not loss to follow-up occurred. In reports that did give the relevant information, the median percentage of participants lost to follow-up was 6 % (interquartile range 2 - 14 %). The method by which loss to follow-up was handled was unclear in 37 studies (19 %); the most commonly used method was survival analysis (66, 35 %). When we varied assumptions about loss to follow-up, results of 19 % of trials were no longer significant <b>if</b> <b>we</b> <b>assumed</b> no participants lost to follow-up had the event of interest, 17 % <b>if</b> <b>we</b> <b>assumed</b> that all participants lost to follow-up had the event, and 58 % <b>if</b> <b>we</b> <b>assumed</b> a worst case scenario (all participants lost to follow-up in the treatment group and none of those in the control group had the event). Under more plausible assumptions, in which the incidence of events in those lost to follow-up relative to those followed-up is higher in the intervention than control group, results of 0 % to 33 % trials were no longer significant. CONCLUSION: Plausible assumptions regarding outcomes of patients lost to follow-up could change the interpretation of results of randomised controlled trials published in top medical journals...|$|R
