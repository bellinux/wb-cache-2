393|5086|Public
25|$|With {{the arrival}} of Mac OS X 10.4, Apple {{extended}} this foundation further by introducing the Core <b>Data</b> <b>framework,</b> which standardizes change tracking and persistence in the model layer. In effect, the framework greatly simplifies {{the process of making}} changes to application data, undoing changes (if necessary), saving data to disk, and reading it back in.|$|E
5000|$|ISO/TS 19129:2009 Geographic {{information}} -- Imagery, gridded {{and coverage}} <b>data</b> <b>framework</b> ...|$|E
50|$|GRC vendors with an {{integrated}} <b>data</b> <b>framework</b> are {{now able to}} offer custom built GRC data warehouse and business intelligence solutions. This allows high value data from any number of existing GRC applications to be collated and analysed.|$|E
30|$|An {{attractive}} {{approach for}} scaling {{the problem with}} data is to use Big <b>data</b> <b>frameworks.</b> More strictly, Big <b>data</b> <b>frameworks</b> go beyond the issue of data volume and address much wider issues covering augmented V’s of data, for instance volume, velocity, variety, value and veracity [13]. Big data framework-based solutions are process-centric: the programmer describes the algorithm {{in a way that}} enables the framework itself to understand (and attempt to exploit) the potential to distribute the data and processing 6. The result of this delegation of the optimisation for speed to the framework is that, while many of today’s Big <b>data</b> <b>frameworks</b> can handle large volumes of data, none can match the runtime performance of conventional HPC systems [14].|$|R
40|$|Big <b>data</b> {{processing}} <b>frameworks</b> (MapReduce, Hadoop, Dryad) are {{hugely popular}} today because they greatly simplify the deployment {{and execution of}} big data analysis jobs requir-ing the use of many machines in parallel. A strong selling point of these frameworks is their built-in failure resilience support. Big <b>data</b> <b>frameworks</b> can run computations to comple-tion despite occasional failures in the system. However, an important but overlooked point has been the efficiency of their failure resilience. The vision of this thesis is that big <b>data</b> <b>frameworks</b> should not only be failure resilient but that they should provide the resilience in an efficient manner. This means both minimizing the impact of failures on computations as well as minimizing the cost of running proactive failure resilience algorithms during failure-free periods. Towards the end goal of enabling efficient failure resilience for big <b>data</b> <b>frameworks,</b> this thesis makes two contributions. The {{first part of the}} thesis presents the first in-depth analysis of the efficiency of the failure resilience provided by Hadoop, the most popular big <b>data</b> processing <b>framework</b> today. The results show that even single machine failures ca...|$|R
5000|$|Universal <b>Data</b> Element <b>Framework</b> Forum - {{merged with}} Open Platform 3.0 in 2015; {{now known as}} O-DEF (Open <b>Data</b> Element <b>Framework)</b> ...|$|R
50|$|A {{portion of}} the software, a {{distributed}} <b>data</b> <b>framework</b> for the DCGS integration backbone (DIB) version 4, was submitted to an open-source software repository of the Codice Foundation on GitHub.The framework was new for DIB version 4, replacing the legacy DIB portal with an Ozone Widget Framework interface.It {{was written in the}} Java programming language.|$|E
50|$|With {{the arrival}} of Mac OS X 10.4, Apple {{extended}} this foundation further by introducing the Core <b>Data</b> <b>framework,</b> which standardizes change tracking and persistence in the model layer. In effect, the framework greatly simplifies {{the process of making}} changes to application data, undoing changes (if necessary), saving data to disk, and reading it back in.|$|E
5000|$|In econometrics, when {{endogeneity}} is {{a concern}} in a dynamic panel <b>data</b> <b>framework,</b> {{it is possible to}} exploit the panel data structure of the data to deal with this issue. Examples include (but are not limited to) data over time on capital investment or wage equations. The first differencing approach to instrumental variables, also referred to as first-difference two-stage least squares (FD2SLS), was first proposed by Anderson and Hsiao (1982) [...] and later extended by Arellano and Bond (1991). The key problem the attempt address, is the problem of as predetermined regressors.|$|E
3000|$|The need {{of native}} {{integration}} of (big) <b>data</b> processing <b>frameworks</b> into the DMSs arises {{along with the}} number of recently advanced Big <b>Data</b> processing <b>frameworks,</b> such as Hadoop MapReduce, Apache Spark, and their specific internal data models. Hence, the DMSs need to provide native drivers for Big <b>Data</b> processing <b>frameworks</b> which can automate the transformation of DMS data models into the respective Big <b>Data</b> processing <b>framework</b> storage models. Further, these native drivers can exploit data locality features of the DMSs as well. Please note that such a feature is also needed based on the respective DLMS architecture that has been presented in [...] "Data lifecycle management (DLM)" [...] section as a Big <b>Data</b> processing <b>framework</b> needs to be placed on top of the data management component.|$|R
50|$|We {{still get}} the {{benefits}} of dimensional models on Hadoop and similar big <b>data</b> <b>frameworks.</b> However, some features of Hadoop require us to slightly adopt the standard approach to dimensional modelling.|$|R
30|$|Different big <b>data</b> <b>frameworks</b> {{were used}} for some data-sampling methods, making {{comparative}} conclusions unreliable, i.e., SMOTE implementation is done in Apache Hadoop, while RUS and ROS implementations are done in Apache Spark.|$|R
5000|$|IQLECT {{was founded}} in 2012 in Bangalore, India by Sachin Sinha {{who is also the}} company's CEO. In its early stages, the company began {{building}} a NoSQL database with an elastic <b>data</b> <b>framework</b> called BangDB. In late 2012, the company was named one of Venture Burn's 14 [...] "hottest mobile startups" [...] in India and one of YourStory.in's [...] "Top 19 Startups and Apps in India." [...] In July 2015, Exfinity Venture Partners invested $2 million (Rs 12 crore) into IQLECT. This was the first instance of Exfinity investing in an analytics company.|$|E
5000|$|During {{his tenure}} as Director of DIA, Vice Admiral Jacoby {{initiated}} a dramatic improvement {{in the way the}} agency collected, shared and used the information its many components generated. In his statement to the Joint Congressional 9/11 Inquiry in October, 2002, then newly DIA Director, Vice Admiral Jacoby stated, [...] "We must move toward a common <b>data</b> <b>framework</b> and set of standards and will allow interoperability at the data, not system, level." [...] This seemingly innocuous statement, far afield from many similar efforts in the federal government, set DIA on a course toward the interoperability it needed, focusing on the information elements themselves, and avoiding the organizational resistance normally generated by technology mandates.|$|E
50|$|A {{canonical}} {{model is}} a design pattern used to communicate between different data formats. A form of enterprise application integration, {{it is intended to}} reduce costs and standardize on agreed data definitions associated with integrating business systems. A canonical {{model is a}}ny model that is canonical in nature, i.e. a model which is in the simplest form possible based on a standard, application integration (EAI) solution. Most organizations also adopt a set of standards for message structure and content (message payload). The desire for consistent message payload results in the construction of an enterprise or business domain canonical model common view within a given context. Often the term canonical model is used interchangeably with integration strategy and often entails a move to a message-based integration methodology. A typical migration from point-to-point canonical data model, an enterprise design pattern which provides common data naming, definition and values within a generalized <b>data</b> <b>framework.</b>|$|E
40|$|Recently, {{increasingly}} {{large amounts}} of data are generated {{from a variety of}} sources. Existing data processing technologies are not suitable to cope with the huge amounts of generated data. Yet, many research works focus on Big Data, a buzzword referring to the processing of massive volumes of (unstructured) <b>data.</b> Recently proposed <b>frameworks</b> for Big <b>Data</b> applications help to store, analyze and process the data. In this paper, we discuss the challenges of Big Data and we survey existing Big <b>Data</b> <b>frameworks.</b> We also present an experimental evaluation and a comparative study of the most popular Big <b>Data</b> <b>frameworks.</b> This survey is concluded with a presentation of best practices related to the use of studied frameworks in several application domains such as machine learning, graph processing and real-world applications...|$|R
25|$|Cocoa {{consists}} of the Foundation Kit, Application Kit, and Core <b>Data</b> <b>frameworks,</b> as included by the Cocoa.h header file, and the libraries and frameworks included by those, such as the C standard library and the Objective-C runtime.|$|R
30|$|Qualitative DBMS {{selection}} guidelines need to {{be extended}} with respect to operational and adaptation features of current DBMS (i.e., support for orchestration frameworks to enable automated operation and adaptation and the integration support into Big <b>Data</b> <b>frameworks).</b>|$|R
40|$|Abstract—Recognizing the {{importance}} of product lifecycle management in current manufacturing environment, we propose an integrated <b>data</b> <b>framework</b> of product and process families. The integrated <b>data</b> <b>framework</b> organizes all data of a product and the associated process families as a generic structure. Such integrated modeling lends itself to assist manufacturing companies in achieving design and production integration through managing product and process variety coherently. The unified modeling language is adopted in this research {{to shed light on}} the various elements of both product and process types and the complex relationships {{to be included in the}} integrated <b>data</b> <b>framework.</b> The framework has been tested in a local electronics company that produces a high variety of vibration motors for mobile phones. I...|$|E
40|$|Keeping {{track of}} the data that {{academic}} libraries capture is a massive task. The University of Nevada - Las Vegas (UNLV) University Libraries developed a <b>data</b> <b>framework</b> as a tracking tool for data points. This framework is both a data dictionary and a manual that records data-gathering procedures. This ensures that the data is continually gathered and reported in the same way, and also ensures that institutional memory of those procedures is preserved, regardless of staff turnover. Additionally, the revised <b>Data</b> <b>Framework,</b> and the revision process, transformed staff attitudes about data reporting and strengthened the libraries 2 ̆ 7 culture of assessment...|$|E
40|$|The Flight Research Services Directorate at the NASA Langley Research Center (LaRC) {{provides}} {{development and}} operations services associated with three general aviation (GA) aircraft used for research experiments. The GA aircraft includes a Cessna 206 X Stationair, a Lancair Colombia 300 X, and a Cirrus SR 22 X. Since 2004, the GA <b>Data</b> <b>Framework</b> software {{was designed and}} implemented to gather data from a varying set of hardware and software sources as well as enable transfer of the data to other computers or devices. The key requirements for the GA <b>Data</b> <b>Framework</b> software include platform independence, the ability to reuse the framework for different projects without changing the framework code, graphics display capabilities, {{and the ability to}} vary the interfaces and their performance. Data received from the various devices is stored in shared memory. This paper concentrates on the object oriented software design patterns within the General Aviation <b>Data</b> <b>Framework,</b> and how they enable the construction of project specific software without changing the base classes. The issues of platform independence and multi-threading which enable interfaces to run at different frame rates are also discussed in this paper...|$|E
5000|$|Cocoa {{consists}} of the Foundation Kit, Application Kit, and Core <b>Data</b> <b>frameworks,</b> as included by the [...] header file, and the libraries and frameworks included by those, such as the C standard library and the Objective-C runtime.|$|R
30|$|There are {{a growing}} number of {{different}} programming models that are used to describe algorithms within Big <b>data</b> <b>frameworks.</b> These models include MapReduce [15], Stream Processing [11, 12, 16] and Query-based techniques [17, 18]. Here, we focus on one such programming model, MapReduce.|$|R
30|$|This paper {{provides}} multidisciplinary {{scope to}} {{the utilization of}} geospatial <b>data</b> <b>frameworks</b> for urban disaster management with accentuation on particular events. The emergency management events presented in this review are universally known and represent high risk for {{different parts of the}} world.|$|R
40|$|Observed {{choices in}} Social Dilemma Games usually {{take the form}} of bounded integers. We propose a doubly-truncated count <b>data</b> <b>{{framework}}</b> to process such data. We compare this framework to past approaches based on ordered outcomes and truncated continuous densities using Bayesian estimation and model selection techniques. We find that all three frameworks (i) support the presence of unobserved heterogeneity in individual decision-making, and (ii) agree on the ranking of regulatory treatment effects. The count <b>data</b> <b>framework</b> exhibits superior efficiency and produces more informative predictive distributions for outcomes of interest. The continuous framework fails to allocate adequate probability mass to boundary outcomes, which are often of pivotal importance in these games. Social dilemma games; Hierarchical modeling; Bayesian simulation; Common property resource...|$|E
40|$|In recent years, {{software}} packages {{for the management}} of biological data have rapidly been developing. However, currently, there is no general information system available for managing molecular data derived from both Sanger sequencing and microsatellite genotyping projects. A prerequisite to implementing such a system is to design a general data model which can be deployed {{to a wide range of}} labs without modification or customization. Thus, this paper aims to (1) suggest a uniform solution to efficiently store data items required in different labs, (2) describe procedures for representing data streams and data items (3) and construct a formalized <b>data</b> <b>framework.</b> As a result, the <b>data</b> <b>framework</b> has been used to develop an integrated information system for small labs conducting biodiversity studies...|$|E
40|$|This paper derives a nonparametric {{procedure}} for testing poolability in a panel <b>data</b> <b>framework.</b> This is contrasted with alternative parametric tests {{which are not}} robust to functional misspecification. This test is illustrated with an earning equation {{using data from the}} Panel Study of Income Dynamics (PSID) ...|$|E
30|$|Interoperability of big <b>data</b> {{analytics}} <b>frameworks.</b> The service-oriented paradigm allows running large-scale {{distributed applications}} on cloud heterogeneous platforms along with software components developed using different programming languages or tools. Cloud service paradigms must {{be designed to}} allow worldwide integration of multiple <b>data</b> analytics <b>frameworks.</b>|$|R
50|$|Spring's <b>data</b> access <b>framework</b> {{addresses}} common difficulties developers {{face when}} working with databases in applications. Support is provided for all popular <b>data</b> access <b>frameworks</b> in Java: JDBC, iBatis/MyBatis, Hibernate, Java Data Objects (JDO), Java Persistence API (JPA), Oracle TopLink, Apache OJB, and Apache Cayenne, among others.|$|R
5000|$|RPI's TWC pursues {{disciplinary}} {{research and education}} themes centered on the World Wide Web. Peter Fox is the lead professor for the research areas: <b>Data</b> <b>Frameworks,</b> <b>Data</b> Science, Semantic eScience and XInformatics. Since 2009, the team has three applications themes: Open Government Data, Environmental Informatics, and Health Care and Life Science Informatics. Fox's primary appointment is in the Department of Earth and Environmental Sciences [...] in the School of Science. Appointments in the Departments of Computer Science (2008) and Cognitive Science (2014) followed.|$|R
40|$|This {{paper is}} {{concerned}} explicitly with spatial aspects of primary school performance results in northern England. In particular, {{the effect of}} the <b>data</b> <b>framework</b> (e. g. the area of each individual observation and the spatial arrangement of observations as an ensemble) on observed relationships between school performance and catchment characteristics is investigated. This work is particularly relevant to the interpretation of more substantive studies involving observed relationships that necessarily depend upon the <b>data</b> <b>framework.</b> Five geometric approaches to catchment area definition are evaluated in conjunction with a simple regression model. These approaches to the spatial linkage problem are compared to an autoregressive modelling approach, in which the structure of the spatial data is treated as part of the modelling process rather than a separate spatial allocation problem. <br/...|$|E
40|$|This {{thesis is}} focused on {{development}} of framework for spatial data management. As proof of the concept, an Objective-C application based on the framework was developed. Further, Core <b>Data</b> <b>framework</b> including theoretical and practical functions for development is introduced. Research part of thesis deals with open source library SpatiaLite and their useful functions...|$|E
40|$|Testing Big Data Processing {{systems is}} a {{challenging}} task as these systems are usually distributed on various virtual machines (potentially hosted by remote servers). In this poster {{we present a}} platform for testing non-functional properties of Big <b>Data</b> <b>framework</b> and a first implementation with Hadoop, a well known big data management and processing platform...|$|E
5000|$|A Java {{implementation}} {{is included}} in the ELKI <b>data</b> mining <b>framework.</b>|$|R
5000|$|DataMelt - a Java-based <b>data</b> {{analysis}} <b>framework</b> {{based on}} FreeHEP libraries ...|$|R
5000|$|DataMelt Java data {{analysis}} and <b>data</b> mining <b>framework</b> that supports scripting languages ...|$|R
