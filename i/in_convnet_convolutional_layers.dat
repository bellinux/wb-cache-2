0|713|Public
30|$|In {{the first}} <b>convolutional</b> <b>layer,</b> the input image patch is {{continuously}} filtered with 48 feature maps of 3 × 11 × 11 and 3 × 7 × 7 kernels with a stride of 2. The second <b>convolutional</b> <b>layer</b> continuously filters {{the output of}} the first <b>convolutional</b> <b>layer</b> with 128 feature maps of 3 × 9 × 9 and 3 × 5 × 5 kernels. The third <b>convolutional</b> <b>layer</b> filters {{the output of the}} second <b>convolutional</b> <b>layer</b> with 128 feature maps of 3 × 7 × 7 and 3 × 3 × 3 kernels, sequentially. The following fully connected layer has 512 neurons.|$|R
40|$|We {{present a}} new deep network <b>layer</b> called “Dynamic <b>Convolutional</b> <b>Layer</b> ” {{which is a}} {{generalization}} of the con-volutional <b>layer.</b> The conventional <b>convolutional</b> <b>layer</b> uses filters that are learned during training and are held constant during testing. In contrast, the dynamic <b>convolutional</b> <b>layer</b> uses filters that will vary from input to input during testing. This is achieved by learning a function that maps the input to the filters. We apply the dynamic <b>convolutional</b> <b>layer</b> to the application of short range weather prediction and show performance improvements compared to other baselines. 1...|$|R
30|$|<b>Convolutional</b> <b>layer</b> 1. There are 48 kernels (size 17 × 17, stride of 2) in {{the first}} <b>convolutional</b> <b>layer,</b> which is {{combined}} with one maxout operator and one max pooling layer.|$|R
30|$|<b>Convolutional</b> <b>layer</b> 2. 128 kernels of size 3 × 5 × 5 (with a stride of 2) {{are applied}} to the input banana image in the second layer {{combined}} with the ReLU. A max pooling <b>layer</b> follows this <b>convolutional</b> <b>layer.</b>|$|R
30|$|When {{comparing}} padded vs. non-padded <b>convolutional</b> <b>layers,</b> {{we found}} {{little difference in}} classification performance; however, using padding to enable creation of deeper networks increased training time by 2.72 ×, and using padding while doubling the number of <b>convolutional</b> <b>layers</b> increased training time by 3.26 ×.|$|R
30|$|<b>Convolutional</b> <b>layer</b> 1. 48 kernels of size 3 × 7 × 7 (3 {{represents}} {{the number of}} RGB channels, with a stride of 2) are applied to the input banana image in the first layer combined with the ReLU. A max pooling <b>layer</b> follows this <b>convolutional</b> <b>layer.</b>|$|R
30|$|Convolutional {{neural network}} (CNN) {{is a classic}} visual {{learning}} method. With the development of large-scale computing and GPU accelerating, huge CNN frameworks can be set with tens of <b>convolutional</b> <b>layers.</b> The newest development of residual network [12] applied in image classification even used more than 150 <b>convolutional</b> <b>layers.</b>|$|R
30|$|As noted above, {{when using}} log-m {{embedding}} for our two alphabet sizes, {{we have an}} input volume of 8 × l. Without padding, each <b>convolutional</b> <b>layer</b> reduces the dimension by 2. Thus, after three layers {{we are looking at}} an (8 - 3 * 2) × (l- 3 * 2) or 2 × (l- 6) volume. Since we have a dimension of 2 in one direction, no further <b>convolutional</b> <b>layers</b> with 3 × 3 filters can be applied. This sets a limit of three for the maximum number of 3 × 3 <b>convolutional</b> <b>layers</b> that can be used with this embedding. By using padding, this limit can be removed. Since padding adds a border of zeroes around our input volume to prevent its dimension from being reduced by each <b>convolutional</b> <b>layer,</b> we no longer have a limit to the number of layers employed. Thus, we can design a network similar to AlexNet [7], one of the highest performing network architectures for image classification. This architecture consists of stacking three 3 × 3 <b>convolutional</b> <b>layers,</b> followed by a 2 × 2 max pooling layer. Again, our small dimension (8) imposes a limit on the number of layer stacks we can have due to max pooling. After our first max pooling layer, we have a 4 × (l/ 2) volume, after the second we have a 2 × (l/ 4) volume and no further 3 × 3 convolutions can be performed. Thus, for the deepest network, we can construct an architecture consisting of three <b>convolutional</b> <b>layers</b> followed by a max pooling <b>layer,</b> three more <b>convolutional</b> <b>layers,</b> a second max pooling layer, then the fully connected layers.|$|R
40|$|In this note, we want {{to focus}} on aspects related to two {{questions}} most people asked us at CVPR about the network we presented. Firstly, What is the relationship between our proposed layer and the deconvolution layer? And secondly, why are convolutions in low-resolution (LR) space a better choice? These are key questions we tried to answer in the paper, but {{we were not able to}} go into as much depth and clarity as we would have liked in the space allowance. To better answer these questions in this note, we first discuss the relationships between the deconvolution layer in the forms of the transposed convolution <b>layer,</b> the sub-pixel <b>convolutional</b> <b>layer</b> and our efficient sub-pixel <b>convolutional</b> <b>layer.</b> We will refer to our efficient sub-pixel <b>convolutional</b> <b>layer</b> as a <b>convolutional</b> <b>layer</b> in LR space to distinguish it from the common sub-pixel <b>convolutional</b> <b>layer.</b> We will then show that for a fixed computational budget and complexity, a network with convolutions exclusively in LR space has more representation power at the same speed than a network that first upsamples the input in high resolution space. Comment: This is a note to share some additional insights for our the CVPR pape...|$|R
30|$|We use VGG- 16 as the {{baseline}} network, which is pre-trained with ImageNet [23] dataset. It has 13 <b>convolutional</b> <b>layer</b> and three fully connected layers. In {{order to improve}} detection speed and reduce the parameters, we use <b>convolutional</b> <b>layer</b> instead {{of the last three}} fully connected layers. It has been proved to be effective in paper [8].|$|R
5000|$|... #Caption: Neurons of a <b>convolutional</b> <b>layer</b> (blue), {{connected}} to their receptive field (red) ...|$|R
40|$|We propose local binary {{convolution}} (LBC), {{an efficient}} alternative to <b>convolutional</b> <b>layers</b> in standard <b>convolutional</b> neural networks (CNN). The design principles of LBC {{are motivated by}} local binary patterns (LBP). The LBC layer comprises {{of a set of}} fixed sparse pre-defined binary convolutional filters that are not updated during the training process, a non-linear activation function and a set of learnable linear weights. The linear weights combine the activated filter responses to approximate the corresponding activated filter responses of a standard <b>convolutional</b> <b>layer.</b> The LBC layer affords significant parameter savings, 9 x to 169 x in the number of learnable parameters compared to a standard <b>convolutional</b> <b>layer.</b> Furthermore, the sparse and binary nature of the weights also results in up to 9 x to 169 x savings in model size compared to a standard <b>convolutional</b> <b>layer.</b> We demonstrate both theoretically and experimentally that our local binary convolution layer is a good approximation of a standard <b>convolutional</b> <b>layer.</b> Empirically, CNNs with LBC layers, called local binary convolutional neural networks (LBCNN), achieves performance parity with regular CNNs on a range of visual datasets (MNIST, SVHN, CIFAR- 10, and ImageNet) while enjoying significant computational savings. Comment: To appear in CVPR 2017 as Spotligh...|$|R
50|$|Alexnet {{contained}} only 8 layers, first 5 were <b>convolutional</b> <b>layers</b> {{followed by}} fully connected layers.|$|R
30|$|Convolutional neural {{networks}} consist of sparsely connected <b>convolutional</b> <b>layers</b> followed by fully connected dense layers (these dense layers are {{equivalent to a}} multilayer perceptron neural network). <b>Convolutional</b> <b>layers</b> are sparsely connected. Neurons in these layers are connected to a small region of the previous layer known as the receptive field, instead of the entire previous layer as is found in a dense layer. The most common receptor field sizes are small, such as 3 × 3 or 5 × 5 neurons. By being sparsely connected CNNs view and learn local correlations. The <b>convolutional</b> <b>layers</b> in the network are followed by several densely connected layers with the last layer containing one neuron for each possible classification outcome.|$|R
30|$|Figure  7 {{shows that}} in the first <b>convolutional</b> <b>layer</b> (the first column in Fig.  7), the global {{features}} including shape and edge of the refrigerator are extracted. In the two following <b>convolutional</b> <b>layers</b> (the second and third columns in Fig.  7), local features are extracted hierarchically. Notable that these features, which are different from the handcrafted features exploited in [5 – 8], are automatically extracted through our proposed CNN.|$|R
30|$|We use a multi-scale {{network for}} the {{deblurring}} problem {{as shown in}} Fig.  2. This network has two scales which share weight between each level. The basic model for each scale begins with three convolutional layers; we add six ResBlocks and three <b>convolutional</b> <b>layers</b> after the beginning unit. Especially, except the first <b>convolutional</b> <b>layer</b> sets the kernel size to 11 × 11 to increase the receptive field, all the <b>convolutional</b> <b>layers</b> have the kernel size of 5 × 5 with 64 channels. The input of each scale contains six channels, that is to say, we combine the blurred image and deblurred result of lower scale as the input of the upper scale. Here, note that we copy the blurred image (i.e., six channels) as the input of the first scale. In this multi-scale model, we utilize a deconvolutional layer to achieve upsampling. For this multi-task framework, except the last three <b>convolutional</b> <b>layers</b> which utilized to perform reconstruction, the basic deblurring network shares the weight between two tasks (i.e., structure deblurring sub-network and image deblurring sub-network).|$|R
40|$|Visual object {{tracking}} {{is challenging}} as target objects often undergo significant appearance changes caused by deformation, abrupt motion, background clutter and occlusion. In this paper, we exploit features extracted from deep convolutional neural networks trained on object recognition datasets to improve tracking accuracy and robustness. The outputs {{of the last}} <b>convolutional</b> <b>layers</b> encode the semantic information of targets and such representations are robust to significant appearance variations. However, their spatial resolution is too coarse to precisely localize targets. In contrast, earlier <b>convolutional</b> <b>layers</b> provide more precise localization but are less invariant to appearance changes. We interpret the hierarchies of <b>convolutional</b> <b>layers</b> as a nonlinear counterpart of an image pyramid representation and exploit these multiple levels of abstraction for visual tracking. Specifically, we adaptively learn correlation filters on each <b>convolutional</b> <b>layer</b> to encode the target appearance. We hierarchically infer the maximum response of each layer to locate targets. Extensive experimental results on a largescale benchmark dataset show that the proposed algorithm performs favorably against state-of-the-art methods. Chao Ma, Jia-Bin Huang, Xiaokang Yang, Ming-Hsuan Yan...|$|R
3000|$|... e {{shows the}} results for {{different}} numbers of layers. The most accurate result is provided when the <b>convolutional</b> <b>layer</b> Ψ [...]...|$|R
40|$|Deep {{convolutional}} {{neural networks}} are {{generally regarded as}} robust function approximators. So far, this intuition is based on perturbations to external stimuli such as the images to be classified. Here we explore the robustness of convolutional neural networks to perturbations to the internal weights and architecture of the network itself. We show that convolutional networks are surprisingly robust {{to a number of}} internal perturbations in the higher <b>convolutional</b> <b>layers</b> but the bottom <b>convolutional</b> <b>layers</b> are much more fragile. For instance, Alexnet shows less than a 30 % decrease in classification performance when randomly removing over 70 % of weight connections in the top <b>convolutional</b> or dense <b>layers</b> but performance is almost at chance with the same perturbation in the first <b>convolutional</b> <b>layer.</b> Finally, we suggest further investigations which could continue to inform the robustness of convolutional networks to internal perturbations. Comment: under review at ICML 201...|$|R
30|$|Though padding {{increases}} training time, {{it allowed}} us to build a network with six <b>convolutional</b> <b>layers</b> resulting in a 2.2 – 2.4 % increase in classification accuracy. We observed using six <b>convolutional</b> <b>layers</b> required less than 20 % more time to train than the padded CNN with three layers. Additionally, the reduction in training time when using log-m embedding more than offsets the cost of padding. Thus, the combination of log-m embedding with padded <b>convolutional</b> <b>layers</b> allows networks using popular deep CNN architectures, such as AlexNet, to be trained with higher classification accuracy {{in less time than}} 1 -of-m embedding. Additionally, these results should apply to more recent networks architectures, such as the inception model [27], as the use of padding enables arbitrarily deep networks to be constructed from any input layer size.|$|R
30|$|The aim of {{this layer}} is to {{gradually}} reduce the dimension of the representation feature and thus further {{reduce the number of}} parameters and the computational complexity of the model. It is often placed between two <b>convolutional</b> <b>layers</b> or <b>convolutional</b> <b>layer</b> and fully connected layer. In our model, the max pooling is used for two reasons: (1) By eliminating non-maximal values, it reduces computation for the upper layers. (2) It provides a form of translation invariance.|$|R
30|$|Next, part 2 {{marked in}} gray is an {{inception}} module which contains one max pooling <b>layer</b> and four <b>convolutional</b> <b>layers</b> with different kernel sizes. Inception module is introduced by Szegedy et al. [25], {{which can be}} seen as a logical culmination of network in network (NIN). They use variable filter sizes to capture different visual patterns of different sizes and approximate the optimal sparse structures by the inception module. Pool 3 in the first line of this inception is a max pooling layer with a 3 [*]×[*] 3 pixel window; the stride of Pool 3 is set to be 2  pixels. <b>Convolutional</b> <b>layer</b> 31 (Conv 31) has 3 [*]×[*] 3 filters, with stride 1. The upper line of this inception contains three <b>convolutional</b> <b>layers</b> (Conv 321, Conv 322, and Con 323) with different kernel sizes, and strides of these convolution layers are all fixed to 1. Specifically, in one of the configuration, we use the 1 [*]×[*] 1 convolutional filter which {{can be seen as}} a linear transformation of the input of the lower layer. Finally, the outputs of the three <b>convolutional</b> <b>layers</b> are connected together by a concat layer (Conc 1).|$|R
50|$|A major {{drawback}} to Dropout {{is that it}} does not have the same benefits for <b>convolutional</b> <b>layers,</b> where the neurons are not fully connected.|$|R
30|$|For our {{experiments}} comparing padded vs. non-padded <b>convolutional</b> <b>layers,</b> {{the full}} sentiment 140 corpus was used. Thus, classifier accuracy {{is higher than}} was observed when testing the four embedding approaches. Results, presented in Table 7, {{show that there is}} little difference in accuracy between using padded vs non-padded <b>convolutional</b> <b>layers</b> (0.2 %); however, doubling the number of layers result in 2.2 – 2.4 % increase in classification accuracy. Thus, using padding to enable deeper networks to be trained is beneficial.|$|R
30|$|After this {{inception}} module, part 3 {{marked in}} green in Fig.  2 with a max pooling <b>layer</b> and a <b>convolutional</b> <b>layer</b> is inserted. <b>Convolutional</b> <b>layer</b> 4 (Conv 4) has 3 [*]×[*] 3 filters, {{and the depth}} is 40. The filter size of the max pooling layer (Pool 4) in this inception is set to be 2 [*]×[*] 2 to have a same output size with Conv 4. Following the inception is also a concat layer (Conc 2) as part 2.|$|R
40|$|Visual {{tracking}} {{is challenging}} as target objects often undergo significant appearance changes caused by deformation, abrupt motion, background clutter and occlusion. In this paper, we propose {{to exploit the}} rich hierarchical features of deep convolutional neural networks to improve the accuracy and robustness of visual tracking. Deep neural networks trained on object recognition datasets consist of multiple <b>convolutional</b> <b>layers.</b> These layers encode target appearance with different levels of abstraction. For example, the outputs of the last <b>convolutional</b> <b>layers</b> encode the semantic information of targets and such representations are invariant to significant appearance variations. However, their spatial resolutions are too coarse to precisely localize the target. In contrast, features from earlier <b>convolutional</b> <b>layers</b> provide more precise localization but are less invariant to appearance changes. We interpret the hierarchical features of <b>convolutional</b> <b>layers</b> as a nonlinear counterpart of an image pyramid representation and explicitly exploit these multiple levels of abstraction to represent target objects. Specifically, we learn adaptive correlation filters on the outputs from each <b>convolutional</b> <b>layer</b> to encode the target appearance. We infer the maximum response of each layer to locate targets in a coarse-to-fine manner. To further handle the issues with scale estimation and target re-detection from tracking failures caused by heavy occlusion or {{moving out of the}} view, we conservatively learn another correlation filter that maintains a long-term memory of target appearance as a discriminative classifier. Extensive experimental results on large-scale benchmark datasets show that the proposed algorithm performs favorably against the state-of-the-art tracking methods. Comment: Project page at [URL]...|$|R
30|$|This section {{provides}} {{details on}} our data, system environment and experimental design for evaluating our character embedding {{and the impact}} of padding in <b>convolutional</b> <b>layers.</b>|$|R
30|$|We {{propose a}} novel CNN {{architecture}} with triple input and the doubly <b>convolutional</b> <b>layer</b> which both are {{adapted to the}} characteristics of refrigerator front-view images.|$|R
30|$|<b>Convolutional</b> <b>layer</b> 4. There are 128 kernels (size 11 × 11, stride of 2) in {{the fourth}} layer, which is {{combined}} with one maxout operator.|$|R
30|$|<b>Convolutional</b> <b>layer</b> 7. There are 384 kernels (size 5 × 5, stride of 2) in {{the seventh}} layer, which is {{combined}} with one maxout operator.|$|R
30|$|FV-CNN {{framework}} extract descriptors {{from the}} last <b>convolutional</b> <b>layer</b> of a CNN to replace hand-crafted features, and the rest steps are similar to classical BOW model.|$|R
30|$|As we {{have noted}} in the {{previous}} section, it is intriguing to examine different ways of coalescing the <b>convolutional</b> <b>layers.</b> Both summing and concatenated <b>convolutional</b> <b>layers</b> of the RCC-Net surpass the other methods. Nevertheless, the concatenated version of RCC-Net has advantages over the summing one. One interesting result is the pedestrian segmentation of the summing version of the RCC-Net achieves the highest accuracy (70.6 %). This fact {{may lead to a}} promising application in the future research, e.g., to determine the salient regions for the pedestrian detection.|$|R
30|$|The CEN is a {{specially}} crafted network composed of three distinct layers: a contrast normalising <b>convolutional</b> <b>layer,</b> a generic <b>convolutional</b> <b>layer,</b> and {{a mixture of}} experts (ME) <b>convolutional</b> <b>layer.</b> Upon providing the network with a localised image patch, the contrast normalising layer performs Z score normalisation on the image prior to convolving with a ReLU-based kernel of the generic convolution layer. The first two layers of the network essentially prepare {{the data for the}} third and final ME-layer which produces the final alignment probabilities for the image patch. The ME-layer is a <b>convolutional</b> <b>layer</b> with a kernel using a sigmoid activation function which produces the response map or probabilities of landmark alignment within an image patch. The response maps are then combined with a non-negative weight final layer which too uses a sigmoid activation function to compute Di(xi;I). Zadeh et al. noted that a 1 × 1 kernel size with no pooling layers was selected to increase the resolution of the landmark prediction space and that the ME-layer {{had a significant impact on}} the overall performance. Unlike other CNNs, increasing the depth of the CEN did not improve the performance of the network while changes to the ME-layer such as removing the constraint of non-negative weights result in a significant reduction in performance.|$|R
40|$|Deep neural {{networks}} are playing {{an important role}} in state-of-the-art visual recognition. To represent high-level visual concepts, modern networks are equipped with large <b>convolutional</b> <b>layers,</b> which use a large number of filters and contribute significantly to model complexity. For example, {{more than half of the}} weights of AlexNet are stored in the first fully-connected layer (4, 096 filters). We formulate the function of a <b>convolutional</b> <b>layer</b> as learning a large visual vocabulary, and propose an alternative way, namely Deep Collaborative Learning (DCL), to reduce the computational complexity. We replace a <b>convolutional</b> <b>layer</b> with a two-stage DCL module, in which we first construct a couple of smaller <b>convolutional</b> <b>layers</b> individually, and then fuse them at each spatial position to consider feature co-occurrence. In mathematics, DCL can be explained as an efficient way of learning compositional visual concepts, in which the vocabulary size increases exponentially while the model complexity only increases linearly. We evaluate DCL on a wide range of visual recognition tasks, including a series of multi-digit number classification datasets, and some generic image classification datasets such as SVHN, CIFAR and ILSVRC 2012. We apply DCL to several state-of-the-art network structures, improving the recognition accuracy meanwhile reducing the number of parameters (16. 82 % fewer in AlexNet). Comment: Submitted to CVPR 2017 (10 pages, 5 figures...|$|R
40|$|The {{goal of a}} Knowledge Base–supported Question Answering (KB-supported QA) {{system is}} to answer a query natural {{language}} by obtaining the answer from a knowledge database, which stores knowledge {{in the form of}} (entity, relation, value) triples. QA systems understand questions by extracting entity and relation pairs. This thesis aims at recognizing the relation candidates inside a question. We define a multi-label classification problem for this challenging task. Based on the word 2 vec representation of words, we propose two convolutional neural networks (CNNs) to solve the multi-label classification problem, namely Parallel CNN and Deep CNN. The Parallel CNN contains four parallel <b>convolutional</b> <b>layers</b> while Deep CNN contains two serial <b>convolutional</b> <b>layers.</b> The <b>convolutional</b> <b>layers</b> of both the models capture local semantic features. A max over time pooling layer is placed {{on the top of the}} last <b>convolutional</b> <b>layer</b> to select global semantic features. Fully connected layers with dropout are used to summarize the features. Our experiments show that these two models outperform the traditional Support Vector Classification (SVC) –based method by a large margin. Furthermore, we observe that Deep CNN has better performance than Parallel CNN, indicating that the deep structure enables much stronger semantic learning capacity than the wide but shallow network...|$|R
30|$|Speeding up the {{computation}} in the <b>convolutional</b> <b>layer</b> is {{a common}} method to accelerate the deep neural network [30 – 33, 74]. For example, Lebedev et al. propose a two-step framework to speed up convolution layers based on tensor decomposition and discriminative fine-tuning [32]. The tensor decomposition uses non-linear squares to compute a low-rank CP-decomposition to decompose the full kernel tensor. Then the original <b>convolutional</b> <b>layers</b> are replaced by four <b>convolutional</b> <b>layers</b> with small kernels. After that, the new network will be fine-tuned on the training dataset again. The evaluations show that the new network achieves a 8.5 × CPU speedup of the whole network with only very little accurate drop. Moreover, Zhang et al. try to accelerate the very deep convolutional networks with the nonlinear asymmetric reconstruction, which achieves a 4 × speedup with merely a 0.3 percent increase of top- 5 center-view error [33].|$|R
