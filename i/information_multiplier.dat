0|62|Public
50|$|Direct-to-users {{marketers}} {{are also}} recognizing {{the need to}} shift to digital channels as audiences become more fragmented {{and the number of}} access points for news, entertainment and <b>information</b> <b>multiplies.</b> Standard television, radio and print direct-to-users (DTC) advertisements are less relevant than in the past, and companies are beginning to focus more on digital marketing efforts like product websites, online display advertising, search engine marketing, social media campaigns, place-based media and mobile advertising to reach the over 145 million U.S. adults online for health information.|$|R
40|$|In this {{research}} I explored {{the impact of}} information delays in a simple model of negotiation through an electronic market system. I found that a market can accurately reflect buyers' and sellers' preferences only if the rate of injection of price <b>information</b> <b>multiplied</b> by the rate of transfer of price information falls between 0 and 2. It is argued that markets adjust themselves to this constraint in practice. The alternatives are to experience chaotic and catastrophic volatility in prices or {{to go out of}} operation. Thus, electronic commerce can provide value beyond merely speeding up operations and increasing capacity. It also helps avoid misleading behavior by both buyer and seller and allows markets to operate in a wider range of trading environments...|$|R
40|$|A Cramer-Rao bound (CRB) {{study is}} {{developed}} in onedimensional (1 D) space which sheds fundamental insight onto the <b>information</b> about <b>multiply</b> scattering point-like scatterers that {{is contained in}} scattering field data corresponding to transmissive, reflective, and combined transmissive plus reflective sensing geometries, and single- and multi-frequency measurements. KEY WORDS Cramer-Rao bound, scattering, estimation, inverse problem, one-dimensional, Fisher information. ...|$|R
40|$|We {{consider}} convex quadratic programs wilh {{large numbers}} of constraints. We distribute these constraints among several parallel processors and modify the objective function {{for each of these}} subproblems with Lagrange <b>multiplier</b> <b>information</b> from the other processors. New Lagrange <b>multiplier</b> <b>information</b> is aggregated in a master processor and Ihe whole process is repeated. Linear convergence is established for strongly convex quadratic programs by formulating the algorithm in an appropriate dual space. The algorithm corresponds to a step of an iterative matrix splitting algorithm for a symmetric linear complementarity problem followed by a projection onto a subspace. 1. Introduction. W...|$|R
40|$|My {{research}} {{focuses on}} Information Mediators which collect content from various sources and present a unified view to users. In particular, I focus on Web search engines, arguably the most influential information mediation systems. As the amount of online <b>information</b> <b>multiplies,</b> the role of information mediators is likely to grow further. In my research I study the key challenges that are vital for their sustained success. In particular, I study the two main tasks that they perform: • Acquisition: collect content from many external sources into a central repository. For example, search engines acquire pages and advertisements from Web sites and advertiser feeds. • Presentation: present a unified view of the content to users. For example, search engines present ranked lists of pages and advertisements in response to user queries. I propose new techniques for both acquisition and presentation. Having worked in academia (CMU) as well as industry (Yahoo!), my research {{focus has been on}} developing techniques that are theoretically sound while being simple and practical. Some of my techniques have been adopted by commercial search engines. My research encompasses ideas from various fields of Computer Science including Informatio...|$|R
40|$|AbstractWe report here surface-induced {{dissociation}} {{spectra of}} three multiply charged peptides: doubly protonated angiotensin I, doubly protonated renin substrate, and triply protonated melittin. For comparison, the collision-activated dissociation spectra of renin substrate and melittin are also presented. The spectra show that surface-induced dissociation provides structural <b>information</b> on <b>multiply</b> charged peptides at the picomole per microliter sample concentrations compatible with electrospray ionization. For multiply protonated angiotensin I, renin substrate, and melittin, surface collisions (100 – 165 eV) favor {{a limited number}} of fragmentation pathways, which are the same as those favored in collision-activated dissociation experiments...|$|R
40|$|Abstract. Voltage sag is {{the major}} power quality problem and {{receives}} wide attention. Although wavelet analysis works well for detecting voltage sag features, the existence of noise can reduce the advantages of wavelet method or even make it ineffective. To solve the problem, the paper uses multi-scale wavelet <b>information</b> by <b>multiplying</b> the results of several scales, and then searches the local maxima from the product to find the transition moment of voltage sag. The proposed method can suppress the noise and improve the accuracy for detecting voltage sag features. Simulation result validates {{the effectiveness of the}} proposed method. ...|$|R
40|$|A {{number of}} mixed-valence {{complexes}} containing Ru”‘(NH,),(X) (X=NI-&, py) units cyano bridged to -Ru”(bpy), or -Os”(bpy), moieties {{have been examined}} by positive electrospray (ES) and Cs’ fast ion bombardment (FAB) mass spectrometIy. Both sets of data yielded a substantial [M-PF,]+ signal {{and a number of}} charactkristic fragment ions involving the multiple loss of the PF,- anion. ES data provided clear <b>information</b> on <b>multiply</b> charged ions as well as a strong indication of Ru-NH, bond weakening when Ru”‘(NI-&), units are cyano bridged to OS”. The advantages of ES mass spectrometry for this class of compounds and the gas-phase redox chemistry of the investigated ions are discussed...|$|R
40|$|By {{carefully}} classifying the NDVI {{spatial information}} retrieved from MODIS 13 over the Aksu Basin (China) into seven categories based on fractional vegetation cover, with a careful {{division of the}} whole study region (WS) into man-made sub-region (MMS) and natural sub-region (NS), and with special consideration of the seasonal difference between summer and winter, a new index, called the man-made oasis index (MMOI), to describe the extent of man-made oasis (EMMO), is proposed. It is expressed as the linear weighted combination of the area ratio of each class from III to VI to the total area, with the higher the class number the higher the weight. The reason to choose classes from III to VI is that in winter they can be only found in MMS. MMOI in winter in MMS shows an increasing trend over the last 10 years, which matches well {{with the increase of}} EMMO found from the documented study. A transfer function between MMOI in winter in MMS and EMMO is then proposed to calculate EMMO based on MMOI. As paddy field was only found located in MMS, evapotranspiration over the paddy field (ETP) simulated by the VIP distributed eco-hydrological dynamic model was chosen as the rate representative of water consumption by man-made oasis (WCMMO) per unit of EMMO. WCMMO is then calculated yearly based on the ETp <b>information</b> <b>multiplied</b> with EMMO based on the index MMOI. The simulated results of yearly WCMMO are useful in exploring the effects of the oasis on the hydrological regime of the Aksu River. </span...|$|R
40|$|International audienceThe {{central issue}} of this paper {{concerns}} improvements for radargrammetric synthetic aperture radar image processing for high-relief reconstruction, and {{we focus on the}} matching step, {{which is one of the}} most important points of the radargrammetric processing. Thus, we propose original methods using different correlation windows. On the one hand, we take the advantages of a multiwindow approach to combine relevant <b>information</b> by <b>multiplying</b> the correlation surfaces obtained for each correlation window size during the matching operation. On the other hand, the second improvement is based on the expansion of windows on foreshortened areas, particularly because of the side-looking radar view. These methods allow us to achieve reliable image matching and to improve the accuracy of the DEM...|$|R
40|$|One of {{the main}} {{problems}} which arises {{in the field of}} software evolution is the sheer amount of information to be dealt with. Compared to reverse engineering where the main goal is the main understanding of one single system. In the field of software evolution this <b>information</b> is <b>multiplied</b> by the number of versions of the system one wants to understand. To counter this problem we {{have come up with a}} flexible query engine which can perform queries on the different versions of a system. In this paper we give an outlook on our current work in the field of software evolution and focus particularly on the concepts behind the query engine we have built...|$|R
40|$|Here we descibe a tool {{to analyze}} {{molecular}} sequences utilizing the internet and existing computational resources for molecular biology. The computer program SeqHelp organizes information from database searches, gene structure prediction, and other <b>information</b> to generate <b>multiply</b> aligned, hypertext-linked reports to allow for fast analysis of molecular sequences. The efficient and economical strategy in this program can be employed to study molecular sequences for gene cloning, mutation analysis, and identical sequence search projects...|$|R
50|$|In {{the late}} 1950s the US Bureau of Standards brought {{together}} {{all that was}} then known about thermophysical properties of various materials. <b>Information</b> had <b>multiplied</b> since that time and Touloukian and others worked in whatever space was available in the School of Mechanical Engineering to bring the information up to date. The project was handled in three stages: 1. Retrieving of data on the thermophysical properties of about 10,000 different materials. 2. Reconciliation of theory and fact and filling in the gaps by means of research. 3. Assembling and publishing the information. After reaching a publishing agreement with McGraw-Hill and increasing support from the Space Agency, Air Force, Army, Wright Field and National Bureau of Standards, the project had grown {{to the point where}} it was too big for its operation in Mechanical Engineering. In 1962, the Trustees approved the construction of a new facility in McClure Park for the TPRC.|$|R
40|$|In this paper, I {{study the}} {{application}} of various specification tests to ordered logit and probit models with heteroskedastic errors, with the primary focus on the ordered probit model. The tests are Lagrange <b>multiplier</b> tests, <b>information</b> matrix tests, and chi-squared goodness of fit tests. The alternatives are omitted variables in the regression equation, omitted varaibles in the equation describing the heteroskedasticity, and non-logistic/non-normal errors. The alternative error distributions include a generalized logistic distribution in the ordered logit model and the Pearson family in the ordered. Lagrange <b>multiplier,</b> <b>Information</b> matrix, Chisquared,...|$|R
40|$|Correctly {{interpreting}} complex scattered wavefields {{to recover}} meaningful {{information about a}} medium {{is one of the}} most fundamental issues broached by seismologists. Such studies encompass the full spectrum of frequencies presented by seismic signals, and methods to identify coherency from otherwise chaotic looking signatures vary as broadly as do the studied scales and media. This body of work investigates the use of two different branches of scattered wavefield experiments conducted on very different scales. Works detailing high frequency coda-related seismic interferometry applied to Erebus volcano on Ross Island, Antarctica, and P-wave receiver functions applied to the whole of West Antarctica are developed, with a variety of innovations and implications for future imaging efforts. Chapters 1 through 3 detail the results of a novel pseudo-reflection technique based on recently identified theoretical principles pertaining to the recovery of specular <b>information</b> from <b>multiply</b> scattered wavefields. Wavefield modal equipartitioning in the coda of high frequency transient signals such as icequake...|$|R
40|$|The {{production}} and consumption of news in the digital era is blurring the boundaries between professionals, citizens and activists. Actors producing <b>information</b> are <b>multiplying,</b> but still media companies hold central position. Journalism research faces important challenges to capture, examine, and understand the current news environment. The SAGE Handbook of Digital Journalism starts from the pressing need for a thorough and bold debate to redefine the assumptions {{of research in the}} changing field of journalism. The 38 chapters, written by a team of global experts, are organised into four key areas: Section A: Changing Contexts Section B: News Practices in the Digital Era Section C: Conceptualizations of Journalism Section D: Research Strategies By addressing both institutional and non-institutional news {{production and}} providing ample attention to the question ‘who is a journalist?’ and the changing practices of news audiences in the digital era, this Handbook shapes the field and defines the roadmap for the research challenges that scholars will face in the coming decades. info:eu-repo/semantics/publishe...|$|R
40|$|We {{propose a}} data-based {{extremum}} formulation that extends theempirical-likelihood and information-theoretic methods of estimation andinference. It is demonstrated how this method {{may be used}} in a general linearmodel context to mitigate the problem of an ill-conditioned design matrix. Adual loss criterion function, which can be biased in finite samples, producesan estimator that is consistent and asymptotically normal. Limiting chi-squaredistributions are obtained that may be used for hypothesis testing andconfidence intervals. Empirical-risk sampling experiments suggest theestimator has excellent finite-sample properties under a squared error lossmeasure. Copyright Kluwer Academic Publishers 2003 empirical-likelihood, semiparametric models, extended estimating equations, Kullback–Leibler <b>information</b> criterion, Lagrange <b>multiplier,</b> pseudo-likelihood ratio tests,...|$|R
40|$|By using {{time-of-flight}} <b>information</b> {{encoded in}} <b>multiply</b> scattered light, {{it is possible}} to reconstruct images of objects hidden from the camera’s direct line of sight. Here, we present a non-line-of-sight imaging system that uses a single-pixel, single-photon avalanche diode (SPAD) to collect time-of-flight information. Compared to earlier systems, this modification provides significant improvements in terms of power requirements, form factor, cost, and reconstruction time, while maintaining a comparable time resolution. The potential for further size and cost reduction of this technology make this system a good base for developing a practical system {{that can be used in}} real world applications...|$|R
40|$|Direct {{reciprocity}} {{means to}} respond in kind to another person whereas indirect reciprocity is understood here as rewarding someone else. We perform corresponding experiments which use a similar underlying structure as the reciprocity experiment of Berg, Dickhaut, and McCabe (1995). Another variation concerns the <b>information</b> about the <b>multiplier</b> of donations where we compare the benchmark case with a commonly known multiplier to a condition where the multiplier is known for sure only by donators. Questions which we try to answer are: Will indirect reciprocity induce higher or lower donations?, will donators with the high multiplier "hide behind the small one?", how do receivers respond to the different situations...|$|R
40|$|The Digitalization of {{television}} is a technological fact that basically affects the codification of the audiovisual signal. However, {{due to the}} degree of transformation that it supposes for the rest of its facets (creation – production – distribution – reception), it can be stated that digitalization changes television as we know it. Digitalization implies a wide and complex evolutionary process for this communication means. Television changes its nature and is coded in a binary system, which allows computer processing. Firstly, it increases the possibilities of content designing. Secondly, it makes feasible the merger {{of television}} with other sources of <b>information,</b> which <b>multiplies</b> the opportunities to create enhanced and interactive services. In addition, digitalization improves the quality of image and sound, which is appreciated by the viewers. Furthermore, the computer compressing techniques allow a more efficient use of the radioelectric spectrum. This fact supposes an increase of the number of channels being broadcasted. Together with the improvement of the encrypting and conditional access systems, it makes the pay-tv model more flexible and brings new business opportunities. From the perspective of the viewer, digitalization of televisio...|$|R
40|$|As {{a result}} of the International Human Genome Project genetic <b>information</b> is rapidly <b>multiplying.</b> To avoid some of the {{problems}} regarding the availability and use of genetic information, it is sometimes suggested to apply the concept of ownership. This article focuses on the clarification of the status of genetic material and genetic information, obtained as {{a result of}} screening and counseling of individual patients. First, some philosophical theories of ownership are examined for a justification of the use of the concept of ownership with regard to the human body. Next, arguments with regard to ownership of the human body are examined. The results of this analysis are applied to genetic material and genetic information. ownership property genetic material genetic information...|$|R
50|$|The {{invention}} of printing {{in the fifteenth}} century revolutionised <b>information</b> culture, vastly <b>multiplying</b> the number of books in circulation. It had a transforming impact on the intellectual culture of the Renaissance. The invention attracted enormous attention, and the art of printing spread quickly through the European continent. In the next 150 years publishers brought out a huge number of texts in a large range of disciplines. These included thousands of Bibles as well as milestones of scientific publication. Printing also stimulated the production of new types of books, such as news pamphlets, and the influential propaganda works of the Protestant Reformation. Overall this amounted to a huge volume of books: at least 350,000 separate editions, a total of around two hundred million printed items.|$|R
40|$|This is a {{preprint}} {{of a paper}} published (with {{a slightly}} different title: Spatial semantics and individual differences {{in the perception of}} shape in information space) in the Journal of the American Society for Information Science, 51 (6), 521 - 528. Abstract: User problems with large <b>information</b> spaces <b>multiply</b> in complexity when we enter the digital domain. Virtual information environments can offer 3 -D representations, reconfigurations and access to large databases that can overwhelm many usersâ abilities to filter and represent. As a result, users frequently experience disorientation in navigating large digital spaces to locate and use information. To date, the research response has been predominantly based on the analysis of visual navigational aids that might support users' bottom-up processing of the spatial display. In the present paper an emerging alternative is considered that places greater emphasis on the top-down application of semantic knowledge by the user gleaned from their experiences within the socio-cognitive context of information production and consumption. A distinction between spatial and semantic cues is introduced and existing empirical data are reviewed that highlight the differential reliance on spatial or semantic information as domain expertise of the user increases. The conclusion is reached that interfaces for shaping information should be built on an increasing analysis of users' semantic processing...|$|R
40|$|We develop nonparametric {{tests for}} the null {{hypothesis}} that a function has a prescribed form, to apply to data sets with missing observations. Omnibus nonparametric tests {{do not need to}} specify a particular alternative parametric form, and have power against a large range of alternatives, the order selection tests that we study are one example. We extend such order selection tests to be applicable in the context of missing data. In particular, we consider likelihood-based order selection tests for multiply- imputed data. A simulation study and data analysis illustrate the performance of the tests. A model selection method in the style of Akaike's <b>information</b> criterion for <b>multiply</b> imputed datasets results along the same lines. Akaike information criterion; Hypothesis test; Multiple imputation; lack-of-fit test; Missing data; Omnibus test; Order selection;...|$|R
40|$|Antibody PG 9 is a prototypical {{member of}} a class of V 1 /V 2 -directed {{antibodies}} that effectively neutralizes diverse strains of HIV- 1. We analyzed strain-specific resistance to PG 9 using sequence and structural <b>information.</b> For <b>multiply</b> resistant strains, mutations in a short segment of V 1 /V 2 resulted in gain of sensitivity to PG 9 and related V 1 /V 2 neutralizing antibodies, suggest-ing both a commonmechanism of HIV- 1 resistance to and a commonmode of recognition by this class of antibodies. Monoclonal antibodies (MAbs) capable of effectively neutral-izing diverse strains of HIV- 1 have been isolated {{from a number of}} HIV- 1 -infected individuals. One recently identified class of such antibodies recognizes an epitope primarily in the V 1 /V 2 region of HIV- 1 gp 120, requires an N-linked glycan at residue 160, and generally binds with much higher affinity to membrane-associated trimeric forms of Env than to monomeric forms of gp 120 (4, 15). Members of this class include PG 9 and the somatically related PG 16, as well as antibodies CH 01 to CH 04 and PGT 141 to PGT 145 from two other donors (1, 14, 15). Antibody PG 9 is one of themost broadly cross-reactivemembers of the class and neutralizes 70 to 80 % of diverse HIV- 1 isolates (3, 7, 15). Th...|$|R
40|$|International audienceThis {{research}} {{proposed a}} method for adaptive Lagrange multiplier determination for rate-distortion optimization with dynamic texture in High Efficiency Video Coding (HEVC). Inspired by the experimental results of the Lagrange multiplier selection test, the presented approach adaptively predicts the optimum Lagrange multiplier for different dynamic texture sequences, based on {{the features of the}} dynamic texture sequences such as normal flow and spatial-temporal <b>information.</b> The Lagrange <b>multiplier</b> among the given values will be chosen based on the Bjontegaard delta measurements. After that, the data of training dynamic texture will be used for Support Vector Machine (SVM) in machine learning for getting the predicting results. The proposed algorithm has been fully integrated into HEVC reference codec. The result shows that the proposed method can improve 0. 5 in Structural Similarity Metric (SSIM) and 2 in Peak Signal-to-Noise Ratio (PSNR) ...|$|R
40|$|Multicore {{architecture}} {{has dramatically}} changed {{the general direction}} of software development dedicated for personal computers. As such, it is important for software designers {{to keep pace with the}} evolving challenges that happen in the hardware side, for example in this context of multicore architecture, so that they can leverage on the advantages of multicore technology as much as possible while developing software. As one of the well-known techniques, Divide and Conquer has a natural adaptation with the multicore technology. The technique needs to be further developed to fit into this new environment. In this paper, we present a new concurrent multithreaded Colored Petri Nets model that provides a new approach for scheduling Divide and Conquer problems on a multicore environment. Two new schedulers have been developed to control the actions of the model. The Multi Stealing Scheduler (MSS) has been designed to redistribute threads among the modelled cores. The MSS is general, scalable and it can be used for any Divide and Conquer problem. The second scheduler is the Local Threads Scheduler (LTS) that has the duty of threads creation and division inside each modelled core. In addition, the LTS introduces a new recursive method to provide the necessary <b>information</b> to <b>multiply</b> two matrices. Two main things have been achieved: First, workload among the modelled cores becomes well balanced; second, the technique produces a high level of concurrency between the elements of the model, which greatly minimise the execution time...|$|R
40|$|Abstract—The most {{intuitive}} way {{to extract}} depth information from remote sensing images is stereogrammetry, {{in which a}} digital elevation model (DEM) is achieved by computing stereoscopic radar images. When only the amplitude of the radar images is considered, this computation is called radargrammetry. The main idea {{of which is to}} match stereopair radar images {{in order to create a}} disparity map from one image to the other and, finally, to compute the elevation. Therein, we present our studies on the extraction of 3 -D information from radar images. We examine a way to produce a DEM of a challenging area of the French Alps. The central issue of this paper concerns improvements for radargrammetric synthetic aperture radar image processing for high-relief reconstruction, and we focus on the matching step, which is one of the most important points of the radargrammetric processing. Thus, we propose original methods using different correlation windows. On the one hand, we take the advantages of a multiwindow approach to combine relevant <b>information</b> by <b>multiplying</b> the correlation surfaces obtained for each correlation window size during the matching operation. On the other hand, the second improvement is based on the expansion of windows on foreshortened areas, particularly because of the side-looking radar view. These methods allow us to achieve reliable image matching and to improve the accuracy of the DEM. Index Terms—Digital elevation model (DEM), matching correlation, radar, radargrammetry, stereoscopy, synthetic aperture radar (SAR). I...|$|R
40|$|Much {{has been}} written on {{shortest}} path problems with weight, or resource, constraints. However, relatively little of it has provided systematic computational comparisons for a representative selection of algorithms. Furthermore, there has been almost no work showing numerical performance of scaling algorithms, although worst-case complexity guarantees for these are well known, nor has the effectiveness of simple preprocessing techniques been fully demonstrated. Here, we provide a computational comparison of three scaling techniques and a standard label-setting method. We also describe preprocessing techniques which {{take full advantage of}} cost and upper-bound information that can be obtained from simple shortest path information. We show that integrating information obtained in preprocessing within the label-setting method can lead to very substantial improvements in both memory required and run time, in some cases, by orders of magnitude. Finally, we show how the performance of the label-setting method can be further improved by making use of all Lagrange <b>multiplier</b> <b>information</b> collected in a Lagrangean relaxation first step...|$|R
40|$|Indirect {{information}} on the conformation of highly charged molecular ions may be obtained by monitoring their collisional cross sections and the course of simple gas-phase reactions such as hydrogen-deuterium exchange. In this work, another indirect but more visually oriented approach is explored: electrosprayed protein ions are accelerated toward a highly oriented pyrolytic graphite surface and the resulting single-ion defects are imaged by scanning force and tunneling microscopy. All protein impacts generated shallow hillocks: the shapes depended on the identity and charge state of the incident protein. Lysozyme and myoglobin, both compact, globular proteins in the native state, produced compact, almost circular hillocks. However, hillocks generated by myoglobin that had been denatured in the solution phase were elongated, and the elongation {{was positively correlated with}} the charge state of the ion. It appears that structural <b>information</b> about gas-phase <b>multiply</b> charged proteins can be derived from imprints generated by energetic protein impacts on surfaces...|$|R
40|$|NEWA {{operated}} {{and maintained}} the electronic weather network in 2006 with funding {{support from the}} NYS IPM Program. There are now 829 users in the NEWA database compared to the 512 users in 2005. As a result of continued free access, NEWA usage in 2006 is up over 400 % compared to 2001. NEWA <b>information</b> delivery is <b>multiplied</b> via dissemination in Cornell Cooperative Extension Educator and private consultant newsletters and crop updates. The National Weather Service ( NWS ) provided weather forecasts and weather radar images. Improvements to the NEWA website spanned apple, tomato and potato pest forecast model information and a comprehensive NEWA Site Map was written. New in 2006, the Northeast Regional Climate Center (NRCC) prepared daily degree day forecasts for various base temperatures based on NWS max/min temperature forecasts.   The NEWA network successfully transitioned to RainWise, Inc. instruments and installed, tested and supported their less expensive wireless weather station plus software for Internet delivery of data to the NEWA network...|$|R
40|$|Digital Convergence {{is having}} {{a big impact on}} the journalism profession. It alters the {{patterns}} of information production, information distribution and information consumption. For some, it’s also generating what could be called a new version of journalism, the civic journalism. All this has a direct impact on the credibility crisis suffered by traditional journalism. This paper tries to answer two questions. First, does digital convergence strengthen or weaken the values of journalism?; Second, do new technologies favour the creation and consolidation of a new journalism and of a Fifth Power or info-communicative platform, able to <b>multiply</b> <b>information</b> transparency by becoming the watcher of the watchers? According to the author, the current crisis of values of journalism will only have a chance to be overcame by protecting the professional practice, due to its social responsibility, or assuming the lost of its democratical function —altogether with the consequences that this may have on our sociopolitical systems...|$|R
40|$|In {{this paper}} we report {{experimental}} results {{that relate to}} the reciprocity experiment of Berg, Dickhaut, and McCabe (1995). We consider direct reciprocity, which means to respond in kind to another person, and indirect reciprocity, understood as rewarding someone else. Another variation concerns the <b>information</b> about the <b>multiplier</b> of donations where we compare the benchmark case with a commonly known multiplier to a condition where the multiplier is known with certainty only to donors. Questions which we try to answer are: Will indirect reciprocity induce higher or lower donations?, will donors with the high multiplier "hide behind the small one?", how do receivers respond to different situations? JEL codes: C 72, C 92 Key words: Trust, reciprocity, experiment, game * This paper was conceived while we were all at or visiting the CentER for Economic Research at Tilburg University. We thank Harmut Kliemt for his encouraging and inspiring comments. Martin Dufwenberg and Eric van Dam [...] ...|$|R
40|$|In econometrics, most null {{hypotheses}} are composite, {{dividing the}} parameters into parameters {{of interest and}} nuisance parameters. Typically, a composite hypothesis can be tested using two or more testing procedures. Competing testing procedures are commonly compared using size-corrected powers. What is often overlooked is that the size-corrected critical value of a test can {{be sensitive to the}} set of admissible values of the nuisance parameters, and hence its size-corrected power. As a result, different choices for the admissible set can produce different conclusions about which test is best. This fact complicates the interpretation of Monte Carlo power studies because in many cases there is no natural definition of the set of admissible values. We find this fact to be crucial when choosing a Lagrange Multiplier test {{in the case of a}} logit model. A theoretical explanation for this effect is developed using large parameter asymptotics. Composite hypotheses, finite sample power, Hessian <b>information</b> matrix, Lagrange <b>multiplier</b> test, logit model, nuisance parameter, outer product information matrix. ...|$|R
40|$|The Third Way, as {{the central}} ideological project of {{contemporary}} social democracy – particularly its British strain – raises objections to the very term “Utopia. ” The idea of Utopia defies the claim of the Third Wayers: namely, that “new” social democracy stands for a “new politics” defined precisely by its location beyond ideology and beyond utopian thinking. Indeed the term “utopia” is pejorative, for it denotes impossibility and not desirability. I argue in this essay that this Third Way position {{is related to the}} conceptualization, in British political discourse, of “new times” defined by economic and technological forces and the spread of knowledge and information. The logic of change in this new context is defined by the warp speed of creativity and by the logic of the silicon revolution, whereby the speed of <b>information</b> transmission constantly <b>multiplies.</b> In such a knowledge-based and individualized economic and social order, politics can no longer be about endpoints because these would soon be overtaken by the pace of progress. ...|$|R
40|$|The primal {{problem of}} multinomial {{likelihood}} maximization restricted to a convex closed {{subset of the}} probability simplex is studied. Contrary to widely held belief, a solution of this problem may assign a positive mass to an outcome with zero count. Related flaws in the simplified Lagrange and Fenchel dual problems, which arise because the recession directions are ignored, are identified and corrected. A solution of the primal problem {{can be obtained by}} the PP (perturbed primal) algorithm, that is, as the limit of a sequence of solutions of perturbed primal problems. The PP algorithm may be implemented by the simplified Fenchel dual. The results permit us to specify linear sets and data such that the empirical likelihood-maximizing distribution exists and {{is the same as the}} multinomial likelihood-maximizing distribution. The multinomial likelihood ratio reaches, in general, a different conclusion than the empirical likelihood ratio. Implications for minimum discrimination information, compositional data analysis, Lindsay geometry, bootstrap with auxiliary <b>information,</b> and Lagrange <b>multiplier</b> tests are discussed...|$|R
