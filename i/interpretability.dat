3624|1|Public
25|$|The <b>interpretability</b> {{method is}} often used to {{establish}} undecidability of theories. If an essentially undecidable theory T is interpretable in a consistent theory S, then S is also essentially undecidable. This is closely related to the concept of a many-one reduction in computability theory.|$|E
25|$|Note that {{examples}} (3)-(8) {{are open}} to <b>interpretability</b> as intuition of grammaticality is somewhat based on intuition so the degrees of grammaticality may range from individual to individual. Some linguists believe that the informal use of these diacritics is problematic because the exact meaning of the symbols have never been properly defined, and their usage is riddled with inconsistencies.|$|E
25|$|Fitzpatrick {{continues}} to develop her {{ideas of the}} importance of community with an analysis of digital text preservation in which she proposes that current technical issues with digital text preservation will require social solutions. This proposal is based on Fitzpatrick’s reasoning that difficulties in the preservation of digital texts are not caused by any quality inherent to digital artifacts but, rather, stem from our understanding of digital products and our social practices concerning their use. It is often assumed that issues with digital preservation are due to the ephemeral quality of digital artifacts. Fitzpatrick points out that this is not entirely correct, illustrating that print text is by no means permanent and digital text is far more permanent than is commonly thought. The loss of access to digital texts or their <b>interpretability,</b> sometimes due to incompatibility between older media formats and newer platforms, is mistakenly perceived as the loss of digital texts themselves. Establishing this fact, Fitzpatrick argues that digital preservation efforts should not focus entirely on technical solutions to technical issues, but instead should concentrate on developing socially organized preservation systems. In addition to focusing on the development of preservation practices through community organization, Fitzpatrick argues that creators of digital artifacts must take steps to ensure the compatibility of their work with preservation efforts, stating: “…planning for the persistent availability of digital resources as {{part of the process of}} their creation will provide the greatest stability of the resources themselves at the least possible cost”.|$|E
2500|$|The {{issue of}} World Englishes was first raised in 1978 to examine {{concepts}} of regional Englishes globally. Pragmatic {{factors such as}} appropriateness, comprehensibility and <b>interpretability</b> justified the use of English as an international and intra-national language. In 1988, at a Teachers of English to Speakers of Other Languages (TESOL) conference in Honolulu, Hawaii, the International Committee of the Study of World Englishes (ICWE) was formed. In 1992, the ICWE formally launched the International Association for World Englishes (IAWE) at a conference of [...] "World Englishes Today", at the University of Illinois, USA. There is now an academic journal devoted {{to the study of}} this topic, titled World Englishes.|$|E
5000|$|<b>Interpretability</b> logics {{comprise}} {{a family of}} modal logics that extend provability logic to describe <b>interpretability</b> or various related metamathematical properties and relations such as weak <b>interpretability,</b> Π1-conservativity, cointerpretability, tolerance, cotolerance, and arithmetic complexities.|$|E
50|$|It must {{be pointed}} out that <b>interpretability</b> of the Mamdani-type neuro-fuzzy systems can be lost. To improve the <b>interpretability</b> of neuro-fuzzy systems, certain {{measures}} must be taken, wherein important aspects of <b>interpretability</b> of neuro-fuzzy systems are also discussed.|$|E
50|$|In {{mathematical}} logic, weak <b>interpretability</b> is {{a notion}} of translation of logical theories, introduced together with <b>interpretability</b> by Alfred Tarski in 1953.|$|E
50|$|<b>Interpretability</b> logic.|$|E
50|$|<b>Interpretability</b> logics and Japaridze's polymodal logic present natural {{extensions}} of provability logic.|$|E
5000|$|<b>Interpretability</b> of scores: {{the scores}} often no longer convey a {{semantic}} meaning ...|$|E
5000|$|A {{generalization}} of weak <b>interpretability,</b> tolerance, {{was introduced by}} Giorgi Japaridze in 1992.|$|E
50|$|Other {{research}} in provability logic {{has focused on}} first-order provability logic, polymodal provability logic (with one modality representing provability in the object theory and another representing provability in the meta-theory), and <b>interpretability</b> logics intended to capture the interaction between provability and <b>interpretability.</b> Some very recent research has involved applications of graded provability algebras to the ordinal analysis of arithmetical theories.|$|E
50|$|The {{strength}} of neuro-fuzzy systems involves two contradictory requirements in fuzzy modeling: <b>interpretability</b> versus accuracy. In practice, {{one of the}} two properties prevails. The neuro-fuzzy in fuzzy modeling research field is divided into two areas: linguistic fuzzy modeling that is focused on <b>interpretability,</b> mainly the Mamdani model; and precise fuzzy modeling that is focused on accuracy, mainly the Takagi-Sugeno-Kang (TSK) model.|$|E
50|$|Proper data {{processing}} and quality control {{are critical to}} the validity and <b>interpretability</b> of gene chip analysis.|$|E
50|$|Generally sparse {{multiple}} kernel {{learning is}} particularly useful {{when there are}} many kernels and model selection and <b>interpretability</b> are important.|$|E
50|$|In {{mathematical}} logic, <b>interpretability</b> is {{a relation}} between formal theories that expresses {{the possibility of}} interpreting or translating one into the other.|$|E
5000|$|G. Japaridze, [...] "A {{generalized}} {{notion of}} weak <b>interpretability</b> {{and the corresponding}} modal logic". Annals of Pure and Applied Logic 61 (1993), pages 113-160.|$|E
5000|$|That said, {{there are}} other, more {{powerful}} ways {{to compare the}} strength of theories, {{the most important of}} which is defined in terms of the notion of <b>interpretability.</b> It can be shown that, if one theory T is interpretable in another B, then T is consistent if B is. (Indeed, this is a large point of the notion of <b>interpretability.)</b> And, assuming that T is not extremely weak, T itself will be able to prove this very conditional: If B is consistent, then so is T. Hence, T cannot prove that B is consistent, by the second incompleteness theorem, whereas B may well be able to prove that T is consistent. This is what motivates the idea of using <b>interpretability</b> to compare theories, i.e., the thought that, if B interprets T, then B is at least as strong (in the sense of 'consistency strength') as T is.|$|E
50|$|This concept, {{together}} with weak <b>interpretability,</b> {{was introduced by}} Alfred Tarski in 1953. Three other related concepts are cointerpretability, logical tolerance, and cotolerance, introduced by Giorgi Japaridze in 1992-1993.|$|E
5000|$|Lack of <b>interpretability</b> of measures/incorporation into {{clinical}} practice: Clinicians must {{be educated}} about {{the usefulness of}} outcome measures, and outcome measures must be easy to include into daily practice.|$|E
50|$|Many model-theoretic {{properties}} are preserved under <b>interpretability.</b> For example if {{the theory of}} N is stable and M is interpretable in N, then the theory of M is also stable.|$|E
50|$|This concept, {{in a sense}} dual to <b>interpretability,</b> was {{introduced}} by , who also proved that, for theories of Peano arithmetic and any stronger theories with effective axiomatizations, cointerpretability is equivalent to -conservativity.|$|E
5000|$|... {{of these}} {{theories}} with each [...] interpretable in [...] Tolerance naturally generalizes from sequences of theories to trees of theories. Weak <b>interpretability</b> {{can be shown}} to be a special, binary case of tolerance.|$|E
50|$|Direct oblimin {{rotation}} is {{the standard}} method when one wishes a non-orthogonal (oblique) solution - that is, {{one in which the}} factors are allowed to be correlated. This will result in higher eigenvalues but diminished <b>interpretability</b> of the factors. See below.|$|E
50|$|Note that scaling {{does not}} affect the psychometric {{properties}} of a test, it is something that occurs after the assessment process (and equating, if present) is completed. Therefore, it is not an issue of psychometrics, per se, but an issue of <b>interpretability.</b>|$|E
50|$|Silverman {{introduced}} a bootstrap method {{for the number}} of modes. The test uses a fixed bandwidth which reduces the power of the test and its <b>interpretability.</b> Under smoothed densities may have an excessive number of modes whose count during bootstrapping is unstable.|$|E
50|$|The <b>interpretability</b> {{method is}} often used to {{establish}} undecidability of theories. If an essentially undecidable theory T is interpretable in a consistent theory S, then S is also essentially undecidable. This is closely related to the concept of a many-one reduction in computability theory.|$|E
50|$|The {{initial set}} of raw {{features}} can be redundant and {{too large to}} be managed. Therefore, a preliminary step in many applications of machine learning and pattern recognition consists of selecting a subset of features, or constructing a new and reduced set of features to facilitate learning, and to improve generalization and <b>interpretability.</b>|$|E
5000|$|Interpretability: A good {{generalization}} {{accuracy is}} not always the primary objective, as clinicians would like to understand which parts of anatomy are affected by the disease. Therefore, <b>interpretability</b> of the results is very important; methods that ignore the image structure are not favored. Alternative methods based on feature selection have been proposed,.|$|E
5000|$|Romanticism {{had four}} basic principles: [...] "the {{original}} unity {{of man and}} nature in a Golden Age; the subsequent separation of man from nature and the fragmentation of human faculties; the <b>interpretability</b> {{of the history of}} the universe in human, spiritual terms; and the possibility of salvation through the contemplation of nature." ...|$|E
50|$|Note that {{examples}} (3)-(8) {{are open}} to <b>interpretability</b> as intuition of grammaticality is somewhat based on intuition so the degrees of grammaticality may range from individual to individual. Some linguists believe that the informal use of these diacritics is problematic because the exact meaning of the symbols have never been properly defined, and their usage is riddled with inconsistencies.|$|E
50|$|Robert Tibshirani {{introduced}} lasso {{in order}} to improve the prediction accuracy and <b>interpretability</b> of regression models by altering the model fitting process to select only a subset of the provided covariates for use in the final model rather than using all of them. It is based on Breiman’s Nonnegative Garrote, which has similar goals, but works somewhat differently.|$|E
5000|$|... • Compared to {{classical}} test theory, IRT parameter estimates {{are not as}} confounded by sample characteristics.• Statistical properties of items can be expressed with greater precision which increases the interpretation accuracy of DIF between two groups.• These statistical properties of items can be expressed graphically, improving <b>interpretability</b> and understanding of how items function differently between groups.|$|E
50|$|The core MBE tenet is that {{models are}} used to drive {{all aspects of the}} product {{lifecycle}} and that data is created once and reused by all downstream data consumers. Data reusability requires computer <b>interpretability,</b> where an MBD can be processed directly by downstream applications, and associativity of PMI to specific model features within the MBD.|$|E
5000|$|The MMPI was {{designed}} as an adult measure of psychopathology and personality structure in 1939. Many additions and changes to the measure have been made over time to improve <b>interpretability</b> of the original Clinical Scales. Additionally, there have been changes {{in the number of}} items in the measure, and other adjustments. The most historically significant developmental changes include: ...|$|E
5000|$|... where Cs is the spherical {{aberration}} coefficient, λ is the electron wavelength, and Δf is the defocus. In TEM, defocus {{can easily be}} controlled and measured to high precision. Thus one can easily alter {{the shape of the}} CTF by defocusing the sample. Contrary to optical applications, defocusing can actually increase the precision and <b>interpretability</b> of the micrographs.|$|E
