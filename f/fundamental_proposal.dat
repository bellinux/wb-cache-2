6|29|Public
40|$|The <b>fundamental</b> <b>proposal</b> for {{comprehensive}} individual income tax reform in Pakistan {{is to provide}} an integrated income tax structure that pertains to income of all individuals (including non-incorporated businesses) and which otherwise dramatically simplifies the taxation of individual income in Pakistan. An integrated individual income tax would treat individuals (salaried employees, self-employed businesses, and other non-corporate entities) {{in a similar way}} by applying a given tax rate structure to taxable income. Pakistan, Pakistan taxation, individual income tax, income tax...|$|E
40|$|Representational {{shifts in}} memory {{have been a}} recent topic of {{interest}} and debate (Blanco & Gureckis 2012; Lupyan, 2008; Richler, Gauthier & Palmeri, 2011; Richler, Palmeri & Gauthier, 2012). Whether there are true systematic biases in memory due to a stimulus being labeled has been proposed and contested. The <b>fundamental</b> <b>proposal</b> that representations shift toward the prototype has not previously been demonstrated. In the present experiment, participants judged colored silhouettes by color category or by preference, then were asked to remember the hue of the original silhouette among five narrowly distinct options. By using the single dimension of hue, {{we are able to}} show prototypical representational shifts in memory for colored silhouettes after a few minutes. We did not observe a difference between color labeled and preference judged silhouettes, refuting the claim that labeling is the source of prototypical representational shifts...|$|E
40|$|There {{has been}} much recent {{interest}} in questions of value in epistemology (for a thorough overview of this work, see Pritchard 2007), and in what follows I argue that epistemologists concerned with the value of true beliefs and knowledge {{would do well to}} devote attention to the enduring nature of beliefs, and in particular to the essential role that they play in constituting agents themselves. I begin by considering an analogy commonly drawn by epistemologists between acts in the domain of ethics and beliefs in the domain of epistemology, and argue that it is flawed in important respects. I propose that a better, more fruitful analogue for belief would be desire, or a similarly enduring state of an agent. This revision of a commonly used analogy may be of some value in itself, but I further consider how focusing excessively upon the belief-act analogy (with its implicit emphasis on the process of belief-formation) can lead to flaws or shortcomings in our epistemic value theorizing. Still, this initial work on analogies is intended as a preliminary step, providing an entryway to the more <b>fundamental</b> <b>proposal</b> that we ought to devote greater attention to the enduring nature of beliefs – a nature not captured by the belief-act analogy. I argue that our enduring beliefs help t...|$|E
50|$|Some {{have argued}} for radical {{economic}} {{change in the}} system. There are several <b>fundamental</b> <b>proposals</b> for restructuring existing economic relations, {{and many of their}} supporters argue that their ideas would reduce or even eliminate poverty entirely if they were implemented. Such proposals have been put forward by both left-wing and right-wing groups: socialism, communism, anarchism, libertarianism, binary economics and participatory economics, among others.|$|R
40|$|This paper makes {{a number}} of <b>fundamental</b> <b>proposals</b> to reconsider {{economics}} by putting human wellbeing at the centre. It emerges from a pluralist perspective in economics and the ontological, conceptual, axiomatic and methodological propositions that are made lead {{to the construction of}} what we call an inclusive economy matrix (IEM). In particular, the paper draws on heterodox economics to redefine the scope of economics, economic agency, rational behaviour and put emphasis on wellbeing rather than welfare. Furthermore, from the acknowledgement of human wellbeing as a three-dimensional concept, the economic aggregation problem is reconsidered and the methodological implications discussed. The IEM is proposed as a comprehensive and robust analytical framework that gives space to bring social equity and sustainable development considerations forward as a priori concerns for economic development. As such, the IEM can serve as a point of departure for formulating new research questions, exploring new relationships between human wellbeing and economic development, and building economic models that bring us closer to people’s realities on the ground...|$|R
30|$|Understanding {{outcomes}} and trajectories of care in patients submitted to TCT was <b>fundamental</b> for the <b>proposal</b> of a decannulation protocol. Further {{studies are needed}} to evaluate {{the quality of life in}} TCT patients and provide further insights on this protocol optimization.|$|R
40|$|The <b>fundamental</b> <b>proposal</b> in {{this article}} is that logical {{formulas}} of the form (f ~f) are not contradictions, and that formulas of the form (t t) are not tautologies. Such formulas, wherever they appear in mathematics, are instead reason to conclude that f and t have a third truth value, different from true and false. These formulas are circular definitions of f and t. We can interpret the implication formula (f ~f) as a rule, a procedure, to find the truth value of f on the left side: we just need to find the truth value of f on the right side. When we use the rules to ask if f and t are true or false, we need to keep asking if they are true or false over and over, forever. Russell's paradox and the liar paradox have the form (f ~f). The truth value provides a straightforward means of avoiding contradictions in these problems. One broad consequence is that the technique of proof by contradiction involving formulas of the form (f ~f) becomes invalid. One such proof by contradiction is one form of proof that the halting problem is uncomputable. The truth value also appears in Cantor's diagonal argument, Berry's paradox, and the Grelling-Nelson paradox. Comment: 7 page...|$|E
40|$|This {{dissertation}} {{investigates the}} geminate consonant phenomena known as integrity and inalterability {{with an eye}} toward providing a general characterization of geminate behavior as well as a deeper understanding of geminates in a principled and systematic way under the Optimality Theoretic framework. The <b>fundamental</b> <b>proposal</b> made in this dissertation is to have the range of surface geminate patterns follow from varying the ranking of key constraints. Depending on the ranking of the key constraints, languages select different output forms from the same input form. Thus, the key constraints not only conspire to produce anti-integrity/anti-inalterability effects, but they also determine what a language does "do" with its input geminates (i. e. integrity/inalterability), giving rise to different resolutions to the geminate puzzle. A chapter is devoted to an indepth discussion of integrity effects in geminates. For this purpose, seven key constraints are proposed: MAX-IO, DEP-IO, ONS, PROSHIER, ALIGN(WD-R, M-R), PLONS, NOBREAKING. By varying the key constraints, we can make several predictions about the possible geminate patterns according to the positions in which they occur. Several patterns are exemplified in this dissertation. We have also provided a discussion of the gaps between what is predicted to exist and what cases are attested. Another chapter is devoted to a more detailed analysis of inalterability effects in geminates. In particular, it is claimed that geminate inalterability matters only when we deal with weakening processes (e. g. spirantization, sonorantization, etc.). It is also proposed that the constraints IDENT-IO(μSF) and NOBREAKING play a pivotal role in explaining typological differences between weakening and assimilation, and other types of inalterability/anti-inalterability concerning geminates. The most interesting part of this dissertation is that we can explain both integrity/inalterability and anti-integrity/anti-inalterability cases uniformly depending on the ranking of the key constraints, without assuming any ad hoc conditions or procedures. Thus, those anti-integrity and anti-inalterability effects are produced as a natural consequence of the interaction of the constraints, just as in the cases of integrity and inalterability. Finally, unlike previous rule-based approaches, our theory allows for a unified account of integrity and inalterability through the interaction of a set of key constraints, making predictions available about both phenomena...|$|E
40|$|Resumo: O objetivo desta pesquisa foi demonstrar que é possível solicitar e integrar a criança cega no contexto da educação préescolar. A intervenção pedagógica visou estimular sua atividade espontânea, possibilitando-lhe a consecução de objetivos que envolvem todos os aspectos do desenvolvimento físico, social, afetivo e intelectual. o proposto {{fundamental}} consistiu em considerar todos os deficientes visuais portadores de reais capacidades de crescimento e desenvolvimento e que em decorrência disso, são também passíveis de receber estímulos do meio e conseqüentemente apresentar respostas adequadas. Para termos uma base comum, tanto da compreensão dessas adaptações sensório-motoras essenciais, quanto das limitações que a falta de visão causa nessas crianças, foi necessário refletirmos sobre os processos pelos quais os deficientes visuais percebem e experimentam o mundo. Para explicarmos os processos cognitivos, utilizamos os conceitos teóricos e as técnicas de pesquisa piagetiana, que proporcionam à Educação Especial uma estrutura de referência para a compreensão das manifestações comportamentais e do funcionamento cognitivo da criança deficiente visual. Assim, por intermédio da metodologia utilizada nesta pesquisa, concluímos que a criança cega pode e deve desenvolver-se satisfatoriamente na pré-escola comum, mas para que isto aconteça, a escola deverá proporcionar meios que permitam a construção do conhecimento, criando relações entre acontecimentos e objetos com os quais deverá cotidianamente interagirAbstract: The {{objective of}} this researche was to demonstrate {{that it is possible}} to propose the integration of de blind child in the mainstreaming of the pre school education. The pedagogical intervention used in this study looked for stimulating the blind child whit natural activity, giving them the possibility to achieve the goals related whit aspects of physical, social, affective and intellectual development. The <b>fundamental</b> <b>proposal</b> of the study was to consider that every visual handicapped child prossess (to hold) the capacity of growing and development as other normal children and because of this, they are also passive to receive stimulus of the environment and consequently present adequate (positive) answers. In order to have a common bases for the comprehension of the esscntial sensory motor adaptations, as the limitations that visual impairment caused to these children, it was necessary to think about the process through which the visually handicapped child has perception and experiences the world. To the explanation of the cognitive process, it was used the Piagetian theoretical concepts and research technique, that provides to the Special Education the structure of reference to the comprehension of the behavioral manifestation and the cognitive process of the visually handicapped child. So, making use of these techniques gaves to this research the possibility to conclude that the blind child can and must satisfactorily develop in the normal pre-school setting. Sut for this happen the school must give the conditions to construct the knowledge, looking for the relationship between happenings and the object which the blind child will interact dail...|$|E
40|$|The recent {{debate about}} a new {{international}} financial architecture, i. e. a reform of the international financial system, is strongly influenced by current events. In contrast to this the paper puts the problem into the framework of Ordnungspolitik. Beginning {{with the development of}} the recent discussion and shortcomings in the international financial system, we discuss <b>fundamental</b> reform <b>proposals</b> in brief and reform steps already realized at greater length. Copyright Verein fü Socialpolitik und Blackwell Publishers Ltd 2000...|$|R
5000|$|This precise Compton {{periodicity}} of {{a matter}} wave {{is said to}} be the necessary condition for a clock, with the implication that any such matter particle may be regarded as a <b>fundamental</b> clock. This <b>proposal</b> has been referred to as [...] "A rock is a clock." ...|$|R
50|$|On February 25, 2016 the Institute sent {{a written}} {{proposal}} to establish within Odessa Regional State Administration {{the post of}} a Commissioner on Human Rights {{with the purpose of}} organizing legal education, formation of legal culture and creating on the territory of the region a network of organizations to protect human rights and <b>fundamental</b> freedoms. The <b>proposal</b> was rejected.|$|R
40|$|We {{present a}} simple {{agent-based}} model {{to study how}} the proximate triggering factor of a crash or a rally might relate to its fundamental mechanism, and vice versa. Our agents form opinions and invest, based on three sources of information, (i) public information, i. e. news, (ii) information from their “friendship” network, promoting imitation and (iii) private information. Agents use Bayesian learning to adapt their strategy according to the past relevance of the three sources of information. We find that rallies and crashes occur as amplifications of random lucky or unlucky streak of news, due to the feedback of these news on the agents’ strategies into collective transient herding regimes. These ingredients provide a simple mechanism for the excess volatility documented in financial markets. Paradoxically, it is the attempt for investors to learn the level of relevance of the news on the price formation {{which leads to a}} dramatic amplification of the price volatility due to their collective search for the “truth”. A positive feedback loop is created by the two dominating mechanisms (Bayesian learning and imitation) which, by reinforcing each other, result in rallies and crashes. The model offers a simple reconciliation of the two opposite (herding versus <b>fundamental)</b> <b>proposals</b> for the origin of crashes within a single framework and justifies the existence of two populations in the distribution of returns, exemplifying the concept that rallies and crashes are qualitatively {{different from the rest of}} the price moves. stock market, crash, rallies, bubble, herding, news...|$|R
40|$|There {{is growing}} concern that {{presently}} dominant frameworks in economics no longer provide {{a way of}} adequately addressing and analysing the problems of today’s globalising and rapidly changing world. This article makes a number of <b>fundamental</b> <b>proposals</b> about how we might reframe economics to move it towards a clearer focus on human well-being. It develops arguments {{for a change in}} the basic ontological proposition and for the need to see ‘the economy’ as an instituted process of resource allocation. From this viewpoint, economics is then the study of resource allocation decisions and processes and the forces that guide these: from a human perspective it is about understanding who gets what, under what conditions and why? The paper argues that a pluralist approach to understanding the economy is necessary for political, analytical and technical reasons. Drawing on a range of contributions to heterodox economics, the paper argues that if we are to understand current crises and challenges, then our understanding of resource allocation in society must have a broader scope than is present in mainstream economics; it proposes a rethinking of economic agency and provides a critique of rational behaviour that is founded in shifting the emphasis from a narrow conception of welfare to well-being. Acknowledging human well-being as a multidimensional concept, the relationship between the well-being of the person and the collective is reconsidered and the methodological implications for the issue of aggregation are discussed. The article seeks to serve as a point of departure for formulating new research questions, exploring the relationships between human well-being and economic development and analysing economic behaviour and interactions {{in such a way as}} to bring us closer to peoples’ realities on the ground...|$|R
5000|$|... “We {{recommend}} that Congress act promptly to authorize {{and to establish}} a federally chartered, nonprofit, nongovernmental corporation, {{to be known as}} the [...] "Corporation for Public Television." [...] The Corporation should be empowered to receive and disburse governmental and private funds in order to extend and improve Public Television programming. The Commission considers the creation of the Corporation <b>fundamental</b> to its <b>proposal</b> and would be most reluctant to recommend the other parts of its plan unless the corporate entity is brought into being.” ...|$|R
40|$|This article {{examines}} a major UK value-for-money study by Sir Roy McNulty {{in the context}} of the neoliberal public policy environment. This environment favoured rail's privatization, and subsequent reform attempts, which maintained the privatization model, have done little to address rail's <b>fundamental</b> problems. McNulty's <b>proposals</b> are examined in terms of their likely effects on the infrastructure authority and the train companies. The article concludes that, although McNulty correctly identified fragmentation as a key cause of rail's cost escalation, and noted the less fragmented nature of other European railways, its neoliberal focus meant that it missed the opportunity to reverse the process...|$|R
40|$|Recent {{theoretical}} and experimental work has suggested the tantalizing possibility of opening a topological gap upon driving the surface states of a three-dimensional strong topological insulator (TI) with circularly polarized light. With this motivation, we study {{the response of}} TIs to a driving field that couples to states near the surface. We unexpectedly find coherent oscillations between the surface and the bulk and trace their appearance to unavoidable resonances caused by photon absorption from the drive. We show how these resonant oscillations may be captured by the Demkov- Osherov model of multi-level Landau-Zener physics, leading to non-trivial consequences such as the loss of adiabaticity upon slow ramping of the amplitude. We numerically demonstrate that these oscillations are observable in the time-dependent Wigner distribution, which is directly measurable in time-resolved ARPES experiments. Our results apply generically to any system with surface states {{in the presence of}} a gapped bulk, and thus suggest experimental signatures of a novel surface-bulk coupling mechanism that is <b>fundamental</b> for <b>proposals</b> to engineer non-trivial states by periodic driving. Comment: 10 pages + appendices, 8 figures, published versio...|$|R
50|$|<b>Fundamental</b> to Moussavi's <b>proposal</b> is that, due to {{the speed}} at which technology, the {{environment}} and culture are changing, the rate of change in contemporary architecture has shifted from a process of overhaul and replacement to a mode of continuous and incremental change. This rapid rate of change is the consequence of multiple intersecting causes which are rooted in human (social, subjective, sensorial) as well as nonhuman (natural, objective, technical) spheres. In order to be compatible with these mutating and diverse values, architecture cannot be limited to the representation of a-priori concepts or singular causes and must evolve through constantly producing, enriching and reinventing its environment.|$|R
40|$|After a {{short history}} of the Λ-term it is {{explained}} why the (effective) cosmological constant is expected to obtain contributions from short-distance physics, corresponding to an energy at least {{as large as the}} Fermi scale. The actual tiny value of the cosmological constant by particle physics standards represents, therefore, one of the deepest mysteries of present-day <b>fundamental</b> physics. Recent <b>proposals</b> of an approach to the cosmological constant problem which make use of (large) extra dimensions are briefly discussed. Cosmological models with a dynamical Λ, which attempt to avoid the disturbing cosmic coincidence problem, are also reviewed. Comment: 15 pages with one figure, uses special proceedings style. Invited talk at the third international conference on Dark Matter in Astro- and Particle Physics, DARK 2000, Heidelberg, Germany, July 10 - 16 (2000...|$|R
40|$|The {{issue of}} this PhD thesis is a Web Mapping {{platform}} {{on a global scale}} based on XML interchange protocols and accepted standards. The focus was on the minimum specifications that the archaeological data should have in order to be uniform and interoperable, and above all, on the geometric and cartographic characteristics that would allow the production of a homogeneous archaeological mapping. Until this moment, the proposals for the production of archaeological cartography have been developed at local, regional, national or, with rare exceptions, continental scale. Thanks to recent advances in information technology, it is now possible to create an open platform for the implementation, storage, exchange, discussion and verification of spatial archaeological data on a global scale. We have identified the primary categories for the acquisition of archaeological data, by defining the minimum standards of compliance of the data, without, however, trivialize the data themselves, thus avoiding a dangerous loss of historical-topographical information. The identification of the four coordinates of objects acquired and a minimum data set of attributes, plus a set of metadata was <b>fundamental.</b> The <b>proposal</b> is therefore the development of a Web Mapping platform, open and collaborative, for positioning and representation of archaeological remains, a sort of "cadaster", and an analytical and detailed knowledge base to assist, support and address each territorial study...|$|R
40|$|This essay {{offers an}} {{alternative}} account of Leibniz’s views on substance and <b>fundamental</b> ontology. The <b>proposal</b> {{is driven by}} three main ideas. First, that Leibniz’s treatment should be understood {{against the backdrop of}} a traditional dispute over the paradigmatic nature substance as well as his own overarching conciliatory ambitions. Second, that Leibniz’s metaphysics is intended to support his conciliatory view that both traditional views of substance are tenable in at least their positive and philosophical respects. Third, that the relationship between immaterial substances, corporeal substances, and ordinary bodies in Leibniz’s metaphysics is best understood as one of “material” constitution. The interpretation as a whole thus suggests that Leibniz needn’t be read as offering either an exclusive defense of corporeal substance realism, nor of immaterial substance idealism, nor as being deeply torn (at a time or over time) between two such views. He may instead be seen as offering a carefully presented, consistent, and sophisticated conciliatory account of substance...|$|R
40|$|Ratites (ostriches, emus, rheas, cassowaries, and kiwis) are large, flightless {{birds that}} have long fascinated biologists. Their current {{distribution}} on isolated southern land masses is believed to reflect {{the breakup of the}} paleocontinent of Gondwana. The prevailing view is that ratites are monophyletic, with the flighted tinamous as their sister group, suggesting a single loss of flight in the common ancestry of ratites. However, phylogenetic analyses of 20 unlinked nuclear genes reveal a genome-wide signal that unequivocally places tinamous within ratites, making ratites polyphyletic and suggesting multiple losses of flight. Phenomena that can mislead phylogenetic analyses, including long branch attraction, base compositional bias, discordance between gene trees and species trees, and sequence alignment errors, have been eliminated as explanations for this result. The most plausible hypothesis requires at least three losses of flight and explains the many morphological and behavioral similarities among ratites by parallel or convergent evolution. Finally, this phylogeny demands <b>fundamental</b> reconsideration of <b>proposals</b> that relate ratite evolution to continental drift. © 2008 by The National Academy of Sciences of the USA. Articl...|$|R
40|$|URL] audienceWe {{propose the}} Semantic Networking concept as a {{candidate}} for the Internet of the Future. Re-thinking of the architectural and functional paradigms is needed to face scalability and complexity issues in the current Internet developments. A <b>fundamental</b> of our <b>proposal</b> is to reconsider all the networking and service operations based on the flow granularity, thus beyond packet or circuit paradigms. This is enabled by the awareness of the transported traffic, thanks to a combined Deep Packet Inspection and Behavioral Analysis approach. Together with the flow-based and traffic-aware features, Autonomic Networking is considered as a pillar of this concept which leads in turn to specific requirements. This paper is an introduction to autonomic features which should be instantiated as per the Semantic Networking goals, within the traffic-aware data plane ("Semantic Analysis", "Elastic Fluid Switching"), the flow-based control plane ("Flow Admission Control", "Flow Policing", "Traffic Aware Routing"), and the self-management plane ("Network Mining", "Knowledge Plane"). We describe each of these functional building blocks, their interactions, the requirements for their autonomic (or self-*) features, and their localization in transport network nodes to transform them into "semantic network nodes"...|$|R
40|$|AbstractThe {{concept of}} Design for All {{emphasizes}} {{the impact of}} the surrounding environment in the individual's functionality. The International Classification of Functioning, Disability and Health brought the concepts of functionality and disability into a comprehensive whole of multiple dimensions of human functioning, such as biological, psychological, social and environmental. In order to contribute to a greater overall functionality of the individual, the use of software and complex systems can be decisive to assist the people with special needs in all areas of life. The paradigm introduced with ICF is inclusive and universal, so it favors not only the old people, but all others, whether they have a limitation or not. The characteristics of Ambient Assisted Living (AAL) are appropriate to fulfill elderly needs. However, the current state of development is still mostly oriented to a technological perspective, where the individual's functionality has not been fully addressed. Under the Living Usability Lab project we have defined a methodology and created some evaluation tools for assessment of AAL services according to a Living Lab perspective, based on the ICF. In this paper we intend to describe the base <b>fundamentals</b> of this <b>proposal,</b> as well as present some results concerning a practical implementation of this methodology...|$|R
40|$|The gas {{industry}} is perhaps Russia’s least reformed major sector. Prices are regulated, exports are monopolised and {{the domestic market}} is dominated by a state-controlled, vertically integrated monopolist, OAO Gazprom. Gazprom combines commercial and regulatory functions, and maintains tight control over the sector’s infrastructure and over information flows within it. The sector as it is currently constituted is highly unlikely {{to be able to}} sustain sufficient output growth to satisfy both rising export commitments and domestic demand. There is significant potential for accelerating the growth of non-Gazprom production and making gas supply in Russia more competitive, but this will require <b>fundamental</b> reform. The <b>proposals</b> for reform advanced in the paper address two sets of issues. First, there is an urgent need to increase transparency in the sector and transfer many of the regulatory functions now performed by Gazprom to state bodies. Secondly, there is a longer-term need for a considerable degree of unbundling of Gazprom. In particular, it would be desirable to remove control of the sector’s transport infrastructure from the company and to revise the arrangements governing gas exports to non-CIS states, which are currently monopolised by Gazprom. At the same time, recent increases in domestic gas tariffs must continue until internal gas prices rise above full, long-term cost-recovery levels...|$|R
40|$|Shadows {{have a great}} {{influence}} on the looks of a computer generated image. They are not only necessary for an authentic illumination of the scene, they also give cues about the position, {{size and shape of}} an object. The generation of shadows requires solving the problem of visibility, i. e. deciding whether the light source is visible from a point in the scene or not. Established methods like raytracing or radiosity are capable of rendering shadows, but are not suitable for real-time applications like games. Algorithms for real-time shadowing have been presented in the past, the two most popular and common being shadow volumes and shadow maps. Based on these <b>fundamental</b> algorithms many <b>proposals</b> have been made to improve them in terms of performance and appearance, because both algorithms have their drawbacks in either way. The recent appearance of programmable graphics hardware lead to completely new extensions of the existing algorithms previously not possible, especially hybrid algorithms that combine strengths or reduce drawbacks of shadow maps and shadow volumes. Two of those hybrid algorithms, shadow volume reconstruction and shadow silhouette maps, are described in this thesis along with possible implementations, with a focus on how they can benefit from graphics hardware...|$|R
40|$|Includes bibliographical references. It {{is common}} to think of our {{universe}} according to the "block universe" idea, which says that spacetime consists of many "stacked" 3 -surfaces varied {{as a function of}} some kind of proper time &# 428;. Standard ideas do not distinguish past and future, but Ellis' "evolving block universe" tries to make a <b>fundamental</b> distinction. One <b>proposal</b> for this proper time is the proper time measured along the timelike Ricci eigenlines, starting from the big bang. The main idea of this work is to investigate the shape of the {&# 428;=constant} surfaces relative to the the null surfaces, and determine what makes these surfaces timelike or spacelike. We use the Lemaître-Tolman metric as our inhomogeneous spacetime model, and we find the necessary and sufficient conditions for these {&# 428;=constant} surfaces to be spacelike or timelike. Furthermore, we indicate whether or not timelike surfaces appear inside black holes and other strong gravity domains, by determining the location of the timelike regions relative to the apparent horizon. Based on this idea, we find that the regions where these surfaces become timelike are often close to the apparent horizons, but always outside them, and in particular timelike regions occur outside black holes. They are always spacelike near the big bang, and at late times (near the crunch or the extreme far future), they are only timelike under special circumstances...|$|R
40|$|It {{is common}} to think of our {{universe}} according to the "block universe" concept, which says that spacetime consists of many "stacked" 3 -surfaces, labelled by some kind of proper time, τ. Standard ideas do not distinguish past and future, but Ellis "evolving block universe" tries to make a <b>fundamental</b> distinction. One <b>proposal</b> for this proper time is the proper time measured along the timelike Ricci eigenlines, starting from the big bang. This work investigates {{the shape of the}} "Ricci time" surfaces relative to the the null surfaces. We use the Lemaitre-Tolman metric as our inhomogeneous spacetime model, and we find the necessary and sufficient conditions for these {τ= constant} surfaces, S(τ), to be spacelike or timelike. Furthermore, we look at the effect of strong gravity domains by determining the location of timelike S regions relative to apparent horizons. We find that constant Ricci time surfaces are always spacelike near the big bang, while at late times (near the crunch or the extreme far future), they are only timelike under special circumstances. At intermediate times, timelike S regions are common unless the variation of the bang time is restricted. The regions where these surfaces become timelike are often adjacent to apparent horizons, but always outside them, and in particular timelike S regions do not occur inside black holes. Comment: 24 pages, 17 figures (many with multiple plots). Author list now complete...|$|R
40|$|This paper {{proposes a}} new {{approach}} to discourse directionality, a phenomenon which, as is well known, is neither well defined nor adequately accounted for. Directionality is the property of (a part of) a discourse to be directed towards a 'goal', usually implying asymmetric functional relations between the discourse units involved. The direction of such an asymmetric discourse relation depends on whether the unit that provides the goalsatisfying value precedes or follows the unit which is subservient to it. <b>Fundamental</b> to our <b>proposal</b> is an analysis of directionality in terms of the topic-comment distinction. Within this framework, directionality is defined as a recursive property assigned to higher-order and lower-order discourse relations central to which is the assumption that they are realized by explicit or implicit topic-forming questions. It will be shown that the distinction that can be made between three types of directionality is precisely a function of three different ways of quantitative/qualitative subordination realized by subquestioning. Apart from the resulting theory providing a solution to the definition problem, it also provides an answer to the determination problem which implies that we attribute a criterion to distinguish dominant discourse units from subservient ones. In addition, the theory contributes to the much discussed issue of an adequate formalization of those discourse elaboration processes that do not involve a new partial value but merely support an already introduced 'subject matter'. ...|$|R
40|$|Recent {{years have}} seen the {{introduction}} of <b>fundamental</b> tax reform <b>proposals</b> that {{call into question the}} meaning of Article I 2 ̆ 7 s 2 ̆ 2 direct tax 2 ̆ 2 clauses: 2 ̆ 2 direct Taxes shall be apportioned among the several states 2 ̆ 2 and 2 ̆ 2 No Capitation, or other direct, Tax shall be laid, unless in Proportion to the Census. 2 ̆ 2 Professor Ackerman argues that these clauses should be narrowly construed, and should not serve as constitutional bars to any of the wide range of reform proposals now under discussion. His essay emphasizes the tainted origins of the direct tax clauses. At the Founding, they served as an essential component of the larger compromise over slavery that was the price paid for the formation of our 2 ̆ 2 more perfect Union. 2 ̆ 2 In recognition of this fact, the clauses were narrowly interpreted by a series of Supreme Court opinions handed down during the first century of the Republic. But in 1895, the Court broke with this tradition of restraint in Pollock v. Farmers 2 ̆ 7 Loan 2 ̆ 6 Trust Co., holding that an income tax statute violated the direct tax clauses. Professor Ackerman traces the Court s gradual return to the pre-Pollock tradition of restraint {{during the course of the}} twentieth century after the enactment of the Sixteenth Amendment. On the basis of this historical review, he urges the rejection of recent academic calls to revive and broaden the scope of the direct tax clauses. Americans should be focusing on the future of tax reform without supposing that past constitutional texts and court decisions profoundly constrain their on-going pursuit of social justice...|$|R
40|$|The {{purpose of}} this paper is to provide a broad {{overview}} of the strengths of the Japanese economy and to consider some of its major problems. First, however, the paper begins with two <b>fundamental</b> economic policy <b>proposals.</b> It ends with a discussion of some effects of the March 11 earthquake triple disaster. First, Japan should implement a major macroeconomic stimulus package of more aggressive monetary policy, taking on a lot more credit risk and further fiscal stimulus, and then implement comprehensive tax increases and reforms. Second, it should institute a major program of comprehensive deregulation of the economy. Predictions about Japan's economic decline are off-base. Japan is a major, high income, technologically advanced, sophisticated economy and society, and will be among the top five economies for at least the next two to three decades. Some tend to believe that China's rise means Japan's decline, but though this may be true in relative terms, economic power is a positive-sum game; it is better to do business with rich, growing economies than poor, stagnant ones. Japan has three important strengths: its people and cohesive society; its high level of technology; and its very large stock of real capital. Of course Japan has problems as well. One is Japan's mediocre economic performance for the past two decades. It has been very difficult for Japan to shift from a model of investment-led growth to consumption-led growth; this has had a number of interrelated adverse effects, including a sustained deflation of prices of goods and services and a depressed job market (though the unemployment rate is far lower than in the U. S. and Europe). A second problem is the very low productivity and lack of international competitiveness of agriculture, and in forestry and fishing, due to small amounts of arable land, in a high wage, high labor cost economy. Agricultural interests are very well organized and effective in protecting their entitlements; this is why Japan has not been able to negotiate any serious free trade agreements, and why TPP poses such a difficult political problem. Third, Japan's corporate governance system is weak, principally because of the dearth of independent directors. The management continues to give higher priority to the interests of its regular employees than its shareholders, often cannot effectively manage their foreign operations, and perpetuates their management system for its own self-preservation. Finally, I consider the effects of the March 11, 2011 triple disaster in Tohoku. This was a regional, not nationwide, disaster. It showed the impressive response of the Japanese people, as well as the strength of Japanese supply chain systems. The outpouring of support from America and around the world has demonstrated how much goodwill – soft power – Japan has built up over the years. However, it brings into serious question Japan's pre-earthquake energy policy of heavy reliance on nuclear energy. At this point, it is politically difficult to reopen these plants soon, but if they're not, there will be electricity shortages this winter, and again next summer. So either short- or long-term, Japan will probably have to either revert to more coal energy, which will increase greenhouse gas emissions as well as health-related diseases, or more sources of still expensive renewable energy...|$|R
30|$|Although {{currently}} not {{a primary}} aim, {{launch of the}} NanoBob CubeSat in a slightly elliptic orbit will enable {{the investigation of the}} gravitational potential on entanglement. The finite speed of light and the description of gravity as space-time curvature are both manifestations of the role of locality in the theory of General Relativity. Quantum theory on the other hand is fundamentally non-local, as manifested by quantum entanglement. These two theories seem difficult to reconcile. (Still, in a controversial paper, it has recently been proposed that entanglement and space-time are linked [73, 74].) Quantum entanglement can be considered to be a linear superposition of two states that is maintained over large distances. General Relativity on the other hand is highly non-linear. The consequences for the interaction of General relativity and quantum theory are currently a hot topic in <b>fundamental</b> physics. Several <b>proposals</b> have appeared in the literature that aim to reconcile the two. A number of papers have suggested that the Schrodinger equation should be replaced by a non-linear equation in the presence of gravity. This would imply that entanglement needs to break down. The proposal by Ralph and Pienaar [75] is particularly attractive and has led the Space-QUEST consortium to propose an entangled photon experiment involving the ISS [4, 21]. In the ISS configuration the theory predicts a significantly different coincidence rate normalized to the single photon rate compared to standard quantum theory. The experiment can in principle also be carried out using NanoBob, provided the satellite is in a slightly elliptic orbit and a sufficiently high photon rate and short coherence time of the source can be achieved. It is estimated that a difference in gravitational field gradient corresponding to an orbital height difference of less than 100  km is needed in order to see an appreciable difference in the decoherence factor for realistic cases of the coherence time (0.8 to 3  ps) [4]. The effect is also predicted to increase with orbital height, making it easier to observe from the 550  km SSO proposed for NanoBob than the ISS orbit at 400  km. Alternatively, the launch of two NanoBob satellites into different circular orbits may still present an economically attractive alternative to the use of an elliptical orbit, given that circular orbits see more and cheaper commercial launch opportunities. It may even be possible to combine data obtained by a single NanoBob satellite with those obtained in a future Space-QUEST experiment on board of the ISS. A limiting factor is likely the required much higher photon rate in order to achieve an adequate signal-to-noise ratio. Increasing the brightness of the source would benefit from larger non-linear crystals, which is already an active area of research. This in turn may require that the photon flux arriving at Alice be distributed over a large number of individual detectors—a costly exercise as it is estimated that roughly a hundred-fold higher photon flux is required. Without reducing the atmospheric losses, or increasing the entanglement efficiency, this implies installing about hundred conventional detector units or using advanced nanowire detectors (about 16 of them) for each polarization direction in the OGS [4]. The space segment is likely not the limiting factor in this experiment. If necessary, the increased data rate could be handled by transferring the data to the ground station during multiple (optical or RF) communication sessions.|$|R
40|$|A {{generation}} ago, {{when the}} law schools {{of our state}} universities were first founded, the dominion of law appeared to be complete. Almost every phase of public and of individual activity was subject to judicial review. It was taken to be an axiom that the people themselves were subject to certain fundamental limitations, running back of all constitutions and inherent in {{the very nature of}} free government, and it was assumed without serious question that the scope and the extent of these limitations were questions of law. Administration was subjected to strict judicial control, and almost every measure of police encountered an injunction as a matter of course. We were proud to have achieved a government of laws and not of men, and we looked down complacently upon the bureau-ridden peoples of Europe without a suspicion of being law-ridden ourselves. So important was the role of law in connection with every aspect of social and governmental activity that one need not wonder that in the West the state itself undertook to provide for public instruction in law as a part of its broad programme of popular education In the interval a great change has gone forward. While the generation that established state universities was proud of the American doctrine of the judicial power over unconstitutional legislation, the present generation seems eager to reject the idea of a <b>fundamental</b> law; and <b>proposals</b> to transform constitutionality from a question of pure law into a question of pure politics find support even in the legal profession. Where the generation that founded the state universities of the West conceived it a postulate of liberty that administration must be confined to the inevitable minimum and sought through judicial review complete elimination of the personal equation in all matters affecting the life, liberty, or fortune of the citizen; the present generation is eager to unshackle administration, to take away judicial review of administrative action wherever possible, and to cut it down to the minimum where it cannot be avoided. Where yesterday we relied upon courts, to-day we rely upon boards and commissions. Even in criminal causes, which the lawyer regards, before all things, as the domain of the common law, Juvenile Courts, probation commissions, and other attempts to individualise the treatment of off enders—these, as well as the desire of the medical profession to take questions of expert opinion out of the forum and commit them to a sort of medical referee, bid fair to introduce an administrative element into punitive justice which our fathers would have abhorred. Yesterday, when the project of state colleges of law was first announced, the courts and the law played the chief role in the practical conduct of affairs. To-day, when the execution of that project is complete, it might seem that there is danger that nothing of real moment will much longer be committed to them. The most significant feature of twentieth century thought is faith in the efficacy of conscious social effort and of intelligently directed social control. For it is not physical nature alone that may be harnessed to man’s use. The laws by which mind combines its work with mind and with the non-sentient factors of human conditions are no less a part of nature and are no less to be learned and put to use. Not the least part of these laws consists of those determining the standards of conduct in the relations of man with man and of man with society which will advance civilisation and will make for the best and noblest society. And the administration of justice as far as reason and principle may insure conformity to such standards, not arbitrarily or in the conscious interest of any man or any class—-this is the justice according to law of our Germanic, our Anglo-American tradition, the sighing of the creature for the justice and truth of his creator, which marked the German law of the Middle Ages, the rule of the king under God, and the law of which Bracton spoke, and the fundamental law running back of all states and constitutions which our fathers sought to express in bills of rights...|$|R
40|$|The {{aim of this}} {{research}} project was to curb {{the environmental impact of}} chicken feathers, a waste from the poultry industry, by value-adding and development of bio-composites with improved biodegradability and thermo-mechanical properties, and to extract keratin from the feathers for inclusion in biomaterials for potential consumer applications. The first step in the application of chicken feathers involved thorough cleaning and disinfection since plucked chicken feathers impose severe microbiological hazards. Therefore, the design of a proper purification method in respect to the final application was necessary. Different surfactants including anionic, non-ionic, and cationic; bleach such as ozone and chlorine dioxide; ethanol extraction; and a combined method comprising surfactant–bleach–ethanol extraction were applied to chicken feathers and their bactericidal performance was investigated via a) Standard Plate Count, b) the enumeration of Escherichia coli, Pseudomonas species, coagulase positive Staphylococcus, aerobic and anaerobic spore-formers and c) Salmonella and Campylobacter detection tests. Among all practices, only the ethanol extraction and combined method eliminated Salmonella from the feathers. Although ethanol-extraction showed superior bactericidal decontamination compared with the combined method, the feathers purified with the latter method showed better morphological and mechanical properties. Scanning electron microscopy-energy dispersive spectroscopy confirmed the presence of sodium lauryl sulphate remnants in the feathers after applying the combined method. Fourier-transform infrared spectroscopy was adopted for the qualitative characterisation of the feathers before and after purification. Chicken feather characterisation including a-helix conformation in the feather wool, and pleated sheet in barbs and rachis, are presented herein. The pH, visual observation, optical microscopy under visible and ultraviolet lights, scanning electron microscopy, micro X-ray diffraction, wide-angle X-ray scattering, infrared spectroscopy, vibrational spectroscopy and thermogravimetry were used to characterise the feathers before and after purification and residues after extraction. The next consideration was to find a use for waste feathers. Two polyurethane based polymers were combined with chicken feather fibres, to form bio-composites. Thermoplastic polyether–polyurethane was used via solvent–casting–evaporation–compression moulding method at 10, 20, 30, 40, 50, 60 and 70 %·w/w of chicken feather fibres; and thermoplastic polysiloxane–polyurethane was used via solvent–casting–evaporation–compression moulding, and solvent–precipitation–evaporation–compression moulding methods to create new bio-composites incorporating 10 and 20 %·w/w of chicken feather fibres into the polyurethane. Compatibility of polyurethanes with the feather fibres and the thermo-mechanical properties of the resulting bio-composites were determined and using thermogravimetry, dynamic mechanical analysis and stress–strain measurements with hysteresis loops. The uniformity of the feather fibres dispersion in the polyurethane matrix was investigated via macro-photography. Scanning electron microscopy of fractured surfaces of the bio-composites was used to verify that the adhesion between fibre and polymer was effective. Molecular modelling visualisation predicted the existence of hydrogen bonding between fibres and polyurethane molecules and this result was supported by Fourier-transform infrared analysis of the composite. The addition of chicken feather fibres to the polyurethane matrixes was found to decrease the glass transition temperature, recovery strain and thermal mass loss of the composites, but increase the elastic modulus (hardness), storage modulus and char level on thermal decomposition. The thermo-mechanical properties of these polymers were enhanced by addition of keratin feather fibres. The utilization of ecofriendly, bio-based composites has been reported in many areas including, but not limited to, the packaging, insulation, automotive, building and roofing industries, as well as for separation membranes for water treatment. The applications of the produced bio-composites are steps towards more environmentally-friendly and more cost effective products. Keratin was then extracted from different segments of disposable chicken feathers including whole feathers, calamus and rachis (composed mainly of beta-pleated sheet structures), barbs and barbules (composed mainly of alpha-helix), using sodium sulfide or L‑cysteine. The extraction process involved dissolving the chicken feathers by reducing its disulfide links, then separating the protein from the medium by centrifugation. Once the feathers were dissolved, the pH of solution was adjusted to the isoelectric point using hydrochloric acid, to precipitate the proteins, and the yield of extracted keratin with sodium sulfide (88 ± 3 %) was higher than with L-cysteine (66 ± 4 %). The precipitated keratin was washed three times with distilled water. The presence of protein obtained from different methods was confirmed using the biuret test, and the Bradford assay enabled the concentration of keratin to be determined. The precipitated keratin was characterised using gel electrophoresis, which confirmed soluble protein of molar mass 11 kg/mol and estimated its purity to be over 95 %. Liquid chromatography-mass spectrometry verified the molar mass of the extracted material matched that of chicken keratin. Vibrational and nuclear magnetic resonance spectroscopy confirmed the structure of keratin was retained following extraction. Thermogravimetry of original purified chicken feather and keratin extracted via sodium sulfide treatment showed virtually identical decomposition behaviour, proving the purity of the keratin. In contrast, thermogravimetry of keratin extracted with L-cysteine indicated it may contain residual L-cysteine. The structure of keratins extracted from different segments of waste chicken feathers via sodium sulfide and L‑cysteine, have been subjected to further nuclear magnetic resonance spectroscopy and analysed for their antibacterial properties on Staphylococcus aureus and Escherichia coli as Gram-positive and Gram-negative species, respectively. The goal of this section was to produce an extract and to characterise several aspects of its behaviour that may have implications for its use as a biomaterial. Hence, the keratin extracted using sodium sulfide was incorporated into hair conditioner and cream, and used in hair and leather treatments to determine their interactions with animal tissues. These experiments confirmed and expanded earlier findings that keratin demonstrated excellent compatibility in biological systems, as the highest keratin concentration experimental cream and conditioner, had the best outcomes. Finally, this study presents suggestions for future <b>fundamental</b> studies and <b>proposals</b> for the development of keratin-based materials for biomedical and consumer product applications...|$|R
40|$|Definitions {{are one of}} {{the most}} {{important}} components of any high-quality terminological resource as well as a privileged medium for knowledge representation since they offer a direct natural-language explanation of the content of a concept. The adequacy of the definitions thus largely determines the overall usefulness of the terminological resource for the user. This study has been motivated by the observation that terminological definitions often do not meet the needs of users. This is partly due to certain preconceptions about the purpose of definitions as well as the nature of meaning itself. These preconceptions thus affect how terminological definitions are created. Traditionally, defining a term has been seen as stating the necessary and sufficient characteristics that make up the meaning of the term. This approach, known as the Aristotelian definition (§ 3. 2), presupposes the existence of a stable meaning, independent of the context in which the term is used. In addition, it is assumed that meaning (or semantic knowledge) is independent of world knowledge (or encyclopedic knowledge). The key premises on which the traditional approach to definitions is based have been refuted in the field of cognitive linguistics (§ 2. 1) : i) it is not possible to determine the necessary and sufficient features of a concept because conceptual boundaries are fuzzy; ii) concepts have prototypical features not shared by all members of the category; iii) it is not possible to make a distinction between semantic and encyclopedic knowledge, nor between semantic and pragmatic knowledge. From a cognitive point of view, encyclopedic knowledge plays a central role in the study of meaning because concepts always appear embedded in frames, which are structures based on encyclopedic knowledge which attribute sense to concepts (Fillmore 1982 a). Moreover, meaning is not considered a stable entity. It is constructed in each usage event in accordance with the context (§ 2. 1. 2). As a consequence, meaning and context are inseparable. In this doctoral thesis, we apply these premises of cognitive linguistics to terminological definitions and present a proposal called the flexible terminological definition (§ 3. 6). This consists of a set of definitions of the same concept made up of a general definition (in this case, one encompassing the entire environmental domain) along with additional definitions describing the concept from the perspective of the subdomains in which it is relevant. Our proposal assumes that by eliminating the artificial boundaries between semantic and pragmatic knowledge, the representation of contextual variation in the terminological definition will no longer be a mere possibility. Given the ubiquity of context and its effects on cognition and language, the representation of the traits activated by concepts in accordance with the context becomes a necessity if one aspires to fully meet the user’s needs. This also entails the inclusion of prototypical characteristics in the definition, i. e. characteristics that are not always applicable to the concept, but which are relevant in a given context. Similarly, encyclopedic knowledge in the terminological definition is no longer forbidden. It now forms an integral part of the definition. The role that the defined concept plays in the frames it activates should be, as far as possible, part of the definition. Our proposal is specifically based on frame-based terminology (§ 2. 2), in addition to the theories of grounded cognition (§ 2. 1. 1), frame semantics (§ 2. 1. 2. 2), prototype theory (§ 2. 1. 3. 2), and the theory theory (§ 2. 1. 3. 3). We took as a starting point the application of frame-based terminology to the representation of specialized knowledge and to terminological definitions in EcoLexicon (a terminological knowledge base on the environment created in accordance with frame-based terminology). In fact, our proposal for a flexible definition was inspired by the recontextualization of EcoLexicon (§ 3. 6. 1), as a result of which conceptual maps only show the relevant information for the subdomain of the environment chosen by the user. Recontextualization in EcoLexicon represents contextual variation and avoids information overload, thus increasing knowledge acquisition by the users. EcoLexicon follows the principle proposed by Meyer, Bowker, and Eck (1992 : 159) according to which, for a terminological knowledge base to be truly useful, it must reflect the same conceptual organization as in the human brain. Since terminological definitions are a kind of knowledge representation (Faber 2002), in this doctoral thesis, we assume that the creation of terminological definitions should also be based on the organization of the human conceptual system. Since context is a determining factor in the construction of the meaning of lexical units (including terms), we assume that terminological definitions can, and should, reflect the effects of context, even though definitions have traditionally been treated as the expression of meaning void of any contextual effect. The main objective of this thesis is to analyze the effects of contextual variation on specialized environmental concepts with a view to their representation in terminological definitions. Specifically, we focused on contextual variation based on thematic restrictions (§ 3. 5. 3. 4), i. e. how the different areas of knowledge comprising the vast domain of the environment conceptualize differently the same concepts, and how this can be reflected in the definition. One of the main <b>fundamentals</b> of our <b>proposal</b> is the notion in cognitive linguistics that lexical units only have meaning in real use events (§ 2. 1. 2). Outside of any use event, a lexical unit does not have any meaning, only semantic potential. A term’s semantic potential is the raw material for its definition, not its object. The semantic potential is not the object because this would mean that defining a term would involve describing all the conceptual content that the term could activate. This is not viable since a term’s semantic potential corresponds to a vast, immeasurable quantity of information that is never fully activated in real events. Lexical units have not only semantic potential, but also associated conventional and contextual constraints. These constraints cause some conceptual content to be activated more often than others, giving rise to what Croft and Cruse (2004 : 110) call pre-meanings. Pre-meanings are conceptual units that appear between the semantic potential and the meaning in the conceptualization process. The object of the definition is thus a subset of the semantic potential that corresponds to a pre-meaning. The pre-meaning that becomes the object of a given definition depends on the contextual constraints applied to the definition. In all cases, this subset always corresponds to a portion of a single concept and the frames that it can activate. By contextual constraints, we mean any situational factors that affect meaning construction and, indirectly, the content of terminological definitions. Given that the object of the definition (the pre-meaning) is an abstraction of the meanings that a lexical unit has under certain contextual constraints, we can state that the context associated with a pre-meaning is also a sort of abstraction from real contexts. As a consequence, we gave the name pre-context to the set of contextual restrictions that limit the semantic potential of a lexical unit in a relatively predictable way, giving rise to pre-meanings. Context comprises the linguistic context, discursive context, sociocultural context, and spatial-temporal context. The pre-context for terminological definitions includes linguistic constraints, thematic constraints, cultural constraints, ideological constraints, and diachronic constraints. Thematic constraints (i. e. discourse topic) allow for more accurate predictions about the way that the semantic potential of a given lexical unit is restricted than other contextual factors. Thematic constraints reduce the semantic potential of a lexical unit according to the topic at issue during a communicative act and the point of view taken. Our proposal of a flexible terminological definition relies on these types of constraint. Domains, in terms of a knowledge field, allow for the systematic characterization of thematic constraints in terminological definitions. They can be understood as macroframes that guide knowledge organization and categorization in a given conceptual area. In this doctoral thesis, we have used a simplified version of the domain classification that was created specifically for EcoLexicon. This work focuses on the phenomenon of contextual variation as opposed to lexical ambiguity (polysemy and homonymy). Contextual variation is the phenomenon that occurs when a concept does not always activate the same traits in use events and the relevance of these traits varies. For its part, lexical ambiguity is the phenomenon that occurs when a lexical unit is associated with more than one concept (Cruse 2011 : 100). To accomplish the objectives of this doctoral thesis, we conducted an empirical study (§ 5) consisting of the analysis of a set of contextually variable concepts and the creation of a flexible definition for two of them. Each of these two concepts presented different contextual profiles. To select the concepts to be analyzed, a terminological extraction was performed on 14 corpora of different environmental subdomains, specifically compiled for this doctoral thesis (§ 4. 1. 2. 1). The extraction was limited to simple nouns, and the results were compared so as to retain only those terms appearing (with a set frequency) in more than three domains. Polysemic terms were discarded manually. To extract the knowledge needed for the conceptual analysis and the writing of the flexible terminological definitions, the methodology of frame-based terminology (with certain additions) was followed (§ 4. 2. 2). This methodology consists of a combined top-down and bottom-up approach. The top-down method includes mainly the analysis of definitions from other terminological resources, whereas the bottom-up approach comprises corpus analysis. For more efficient knowledge extraction from the corpora, we employed hypernymic knowledge patterns (Meyer 2001 : 290) coded as word-sketches for SketchEngine. This allows for the extraction of superordinate concept candidates for the choice of genus in definitions. Moreover, we created a word-sketch for the extraction of contextonyms, which are the lexical units that tend to co-occur with a given lexical unit in linguistic contexts (Ji, Ploux, and Wehrli 2003; Ji and Ploux 2003). In this work, the analysis of contextonyms was used to determine the semantic traits activated by a concept in a given domain. As a result of the first part of our empirical study (the analysis of all the terms in our working list) (§ 5. 1), we divided our notion of domain-dependent contextual variation into three different phenomena: i) modulation (similar to Cruse’s modulation (§ 3. 5. 3. 5. 2)); ii) perspectivization (related to Cruse’s ways-of-seeing (§ 3. 5. 3. 5. 3)); iii) subconceptualization (akin to Cruse’s microsenses and local sub-senses (§ 3. 5. 3. 5. 4)). These phenomena are additive in that all concepts experience modulation, some concepts also undergo perspectivization, and finally, a small number of concepts are additionally subjected to subconceptualization. Modulation (§ 5. 2. 1) is the type of contextual variation that only alters minor characteristics of a concept which are neither necessary nor prototypical. These alterations are not represented in a terminological definition. For its part, perspectivization (§ 5. 2. 2) results in the change in the level of prototypicality of certain traits for a concept in relation to the general environmental premeaning. Finally, subconceptualization (§ 5. 2. 3) is the type of contextual variation in which the extension of the concept in relation to the general environmental premeaning is modified. In the second part of our empirical study (§ 5. 4), we created two flexible terminological definitions, one for a concept with subconceptualizations (POLLUTANT) and another for a concept with perspectives (CHLORINE). In this section, we presented guidelines on how to build them, from the extraction of knowledge to the actual writing of the definition. These guidelines ensure that the definition actually reflects how the defined concept is construed in different environmental domains, which might differ from the viewpoint adopted in the environment as a whole or other environmental subdomains. This doctoral thesis contributes to the improvement of the quality of terminological definitions because, with our approach, the user is presented with a definition tailored to the domain that he/she has chosen, thus multiplying the probabilities that the definition will offer him/her the information he/she needs. Furthermore, flexible terminological definitions provide a knowledge representation that better resembles the human conceptual system than traditional terminological definitions. As a consequence, a flexible definition not only provides more relevant information, but it also accomplishes this in a way that potentially facilitates and enhances knowledge acquisition...|$|R

