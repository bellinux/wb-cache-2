1|1392|Public
40|$|Plasma glow {{discharge}} corona reactor {{system has been}} made and tested to stimulate mangrove growth. Plasma {{glow discharge}} corona reactor consists of DC high voltage system and point to plate electode system. Reactor performance test has done on Rhizophora apiculata mangrove species. High voltage system mainly develop using TV fly back that been triggered by pulsed frequency using oscillator circuit. Point to plane electrode system was designed to maintain two electrode range remains 3 cm with number of point electrode plugged parallel with plane electrode are 63 pieces. Plasma glow discharge corona reactor tested was done on radiation process of mangrove seed by <b>divided</b> <b>sample</b> groups into 6 sample which have different time radiation and 1 sample that not been radiated as a control data. Mangrove growth was analyzed by measure the growth of mangrove plumulae after 17 days of cultivation. Research results shows that in developing plasma glow discharge corona to radiate mangrove, plasma reactor system works in 8 kV DC plasma voltage development. Mangrove growth after 6 time variation of plasma radiation shows a good effect of growth stimulation along with longer radiation time duration. The increase of mangrove growth was hypothetically caused by an increase of nitrogen concentration in mangrove seeds because of ion nitrogen fixation in the seeds during plasma radiation proces...|$|E
50|$|Nair and Shrivastava in 1942 {{suggested}} a similar idea but instead advocated <b>dividing</b> the <b>sample</b> into three equal parts before calculating {{the means of}} the subsamples. Brown and Mood in 1951 proposed the idea of using the medians of two subsamples rather the means. Tukey combined these ideas and recommended <b>dividing</b> the <b>sample</b> into three equal size subsamples and estimating the line based on the medians of the subsamples.|$|R
30|$|Star with <b>dividing</b> the <b>sample</b> DMUs into m̄ ordered sample standards. Let t _ 1 = 1, t _ 2 =m̅.|$|R
30|$|The {{industry}} coding {{changed in}} 2004 and 2009 in the BSWS. Thus, {{the sample is}} limited to the following industries that can be coded consistently: manufacturing, trade, finance, and services (medical and nursing + other services). For manufacturing and services, I further <b>divide</b> the <b>sample</b> into 11 regions. 11 For finance and trade industries, I do not <b>divide</b> the <b>sample</b> into regions because some of the cohort–region–industry cells have no observations. Thus, in total, there are 24 industry–region cells.|$|R
30|$|In this paper, SVM {{method is}} used, face feature vector {{is taken as}} input feature, Jackknife [22] method is used as test sample {{partition}} method, and RBF is taken as SVM kernel function. The Jackknife method is a method of <b>dividing</b> <b>samples.</b> Each time, a sample is randomly deleted from the sample set, and the remaining samples are taken as input samples. The final calculation result takes the mean of multiple classification results, thereby avoiding the specificity of the sample.|$|R
40|$|Past test of {{differences}} in economic efficiency between small and large farms in developing countries used arbitrary criteria to <b>divide</b> <b>samples</b> of farms into the two size groups. Given the inferential danger from such arbitrary groupings, this paper presents a method for conducting the efficiency tests where the cut-off point between small and large farms is determined statistically. The test is applied to dryland food grain farming in Morocco. The estimated cut-off point was {{significantly different from the}} conventional one, but the efficiency test results were similar. ...|$|R
3000|$|Step 6 : Use the FBH {{algorithm}} to <b>divide</b> the <b>samples</b> (y_i)_i = 1, [...]..., n in R^k into k classes C 1, C 2, …, C [...] k [...].|$|R
40|$|This paper aims to {{shed light}} on the {{relationship}} between CEO compensation and management efficiency. It reports the results of an empirical study of a sample of Fortune 500 firms. My hypothesis {{is that there is a}} positive relationship between CEO compensation and management efficiency. In order to try to prove my hypothesis, I conducted several tests. The tests were performed by <b>dividing</b> the <b>sample</b> in a number of ways and then performing a regression analysis on the data. The first test was conducted by taking the sample as a whole, the second by <b>dividing</b> the <b>sample</b> by industry, third by <b>dividing</b> the <b>sample</b> by market value (size), and finally by <b>dividing</b> the <b>sample</b> by salary-to- asset ratio. The results of the first test showed that, overall, no relationship exists between CEO compensation and the efficiency of management. The second test showed that a relationship does exist between CEO compensation and management efficiency in some industries. The next test showed that a positive relationship exists between the two variables for small firms, and a negative relationship exists for large firms. The final test also provides support for the idea that CEO compensation is related to the efficiency of management. Although my hypothesis was not entirely correct, it did prove to be accurate in several cases. B. S. (Bachelor of Science...|$|R
30|$|There are 200 {{images of}} life {{taken by the}} mobile MI 4, and the image size is 92 [*]×  112. The image is {{converted}} into a BMP (Bitmap) format by a JPEG (Joint Photographic Experts Group). In order to avoid specific features, we <b>divided</b> the subjects’ <b>samples</b> into 10 -cross fold methods and <b>divided</b> 200 <b>samples</b> into 10 samples. One sample {{was used as a}} test sample, and 9 samples were used as training samples.|$|R
3000|$|Step 6 : Use the K-mean {{algorithm}} to <b>divide</b> the <b>samples</b> [...] (y_i)_i = 1, [...]..., n in R^k into k classes C 1, C 2, …, C [...] k [...].|$|R
5000|$|An {{alternative}} {{method is to}} <b>divide</b> the <b>sample</b> into g groups each of size p with n = pg. Let ri be the estimate of the ith group. Then the estimator ...|$|R
40|$|Classification {{of patient}} samples {{is a crucial}} aspect of cancer {{diagnosis}} and treatment. The present a method for classifying samples by computational analysis of gene expression data. the classification problem in two parts: class discovery and class prediction. Class discovery refers {{to the process of}} <b>dividing</b> <b>samples</b> into reproducible classes that have similar behavior or properties, while class prediction places new samples into already known classes. a method for per-forming class prediction and illustrate its strength by correctly classifying bone marrow and blood samples from acute leukemia patients. if it describe to use our predictor to validate newly discovered classes...|$|R
40|$|In this paper, the time-series {{developments}} of covariations of returns between the Japanese stock {{markets and the}} European stock markets are empirically examined. We analyze these comovements by <b>dividing</b> <b>sample</b> periods into several terms that are {{before and after the}} Lehman Shock in the US. In this study, it is firstly clarified that the linkage of stock returns of the Japanese markets and the European markets recently gradually increased. Moreover, it is secondly identified that in the period right after the US Lehman Shock, the covariations between stock returns in Japan and in several European countries highly increased. </p...|$|R
30|$|A {{method is}} clearly needed to {{efficiently}} <b>divide</b> the <b>sampling</b> area into clusters, {{in order to}} run a parallel AS algorithm with multiple robots. Here, we propose an approach to efficiently <b>divide</b> the <b>sampling</b> area for parametric distributions using Fuzzy c-means clustering (FCM) and Centroidal Voronoi Tessellation (CVT) diagrams.FCM has frequently {{been used in the}} past for the classification of numerical data. CVT diagrams[24] have also been used for forming non-uniform size grids to better explore high-variance areas for non-parametric distributions[7]. Here, we employ a scheme to efficiently <b>divide</b> the <b>sampling</b> areas for parametric distributions using both FCM and CVT. In this approach, FCM clusters samples based on the estimated centers of the approximating Gaussians used to map the field. Note here that we have assumed that the partitioning is performed once only {{at the beginning of the}} Fusion filter. For a time-varying field, further accuracy can be obtained by re-partitioning the field (and hence repositioning the Gaussians) after some samples to account for the field evolution in time.|$|R
50|$|It is a {{weighted}} {{average of the}} variance. The authors suggest that this parameter {{can be used as}} the optimisation target to <b>divide</b> a <b>sample</b> into two subpopulations. No statistical justification for this suggestion was given.|$|R
30|$|Energy measure: Sum of {{the squares}} <b>samples</b> <b>divided</b> {{by the number}} of samples in a frame.|$|R
30|$|To further {{analyze the}} results, we {{separated}} the firms by company scale and ownership. First, we <b>divided</b> the <b>sample</b> {{into two groups}} {{based on the number}} of employees: large and small. Companies with more than 10, 000 employees were categorized as large, while firms with less than 10, 000 employees were considered small. Based on this criterion, there were eight large firms (ABC, BC, BOC, CCB, CITIC, CMB, GDD, and ICBC) and six small firms (CEB, HX, IB, MSB, SPD, and SZD). Second, we <b>divided</b> the <b>sample</b> into state-owned commercial banks and nationwide joint-stock commercial banks to delineate ownership characteristics. Using the banks listed in Table  1, there were 4 state-owned bank firms and 10 nationwide banks.|$|R
50|$|Phi can be {{computed}} {{by finding}} the square {{root of the}} chi-squared statistic <b>divided</b> by the <b>sample</b> size.|$|R
40|$|Small {{structures}} are checked by monitoring samples for acoustical signal count. Flaws are located by observing relatively high acoustical activity within given area. Acoustical monitoring {{has been extended}} to large structures by <b>dividing</b> large <b>samples</b> into small areas and then monitoring each area separately...|$|R
40|$|It {{is often}} {{necessary}} to make sampling-based statistical inference about many probability distributions in parallel. Given a finite computational resource, this article addresses how to optimally <b>divide</b> <b>sampling</b> effort between the samplers of the different distributions. Formally approaching this decision problem requires both the specification of an error criterion to assess how well each group of samples represent their underlying distribution, and a loss function to combine the errors into an overall performance score. For the first part, a new Monte Carlo divergence error criterion based on Jensen-Shannon divergence is proposed. Using results from information theory, approximations are derived for estimating this criterion for each target based on a single run, enabling adaptive sample size choices to be made during sampling...|$|R
40|$|Due to {{the fine}} delay {{resolution}} of typical ultra wideband (UWB) systems, Rake receivers need to combine samples from {{a large number of}} multipath components (MPCs) in order to collect sufficient signal energy for reliable decisions. In addition, these samples need to be combined optimally in order to minimize bit error probability. The optimal linear minimum mean square error (MMSE) combining scheme might require inversion of a large matrix depending on channel and system parameters. Therefore, suboptimal algorithms with lower computational complexity but close-to-optimal performance are desirable. In this paper, a low-complexity combining scheme is proposed for that purpose, which <b>divides</b> <b>samples</b> into a number of groups and performs MMSE combining in two steps. Performance of this two-step combining scheme is investigated theoretically and by simulations...|$|R
40|$|This paper uses panel {{data from}} Japan {{to explore how}} the Great East Japan Earthquake {{influenced}} the intention to leave one's place of residence by comparing the same individuals' responses {{before and after the}} earthquake. Controlling for unobserved individual fixed effects and various individual characteristics, we found that (1) people were more willing to leave their place of residence after the disaster when they lived nearer to Fukushima, (2) the effect of the disaster on intention to leave was reinforced when respondents had a small child, and (3) after <b>dividing</b> <b>sample</b> by gender, such tendencies were observed among women but not among men. From the last finding, we conclude that {{differences between men and women}} in perceived risk lead to differences in mobility intentions...|$|R
5000|$|... label {{density is}} the number of labels per <b>sample</b> <b>divided</b> {{by the total number of}} labels, {{averaged}} over the samples: [...] where [...]|$|R
5000|$|Cramér's V is {{computed}} {{by taking}} the square root of the chi-squared statistic <b>divided</b> by the <b>sample</b> size and the minimum dimension minus 1: ...|$|R
3000|$|... 12 I {{disregard}} {{information on}} divorce and second marriage in this analysis, although provided by SOEP, because only few migrants {{in the sample}} report divorcing or marrying a second time. Also, considering the small <b>sample</b> sizes, <b>dividing</b> the <b>sample</b> into too many groups is likely to reduce {{the reliability of the}} results.|$|R
40|$|This paper {{investigates the}} nature of {{volatility}} spillovers between stock returns {{and a number of}} exchange rates in six Latin American countries and one European economy in the 1998 - 2006 period. We <b>divide</b> our <b>sample</b> into sub periods, prior to and after thStock returns, exchange rates, integration, volatility spillovers, EGARCH modelling...|$|R
3000|$|The {{actual time}} {{difference}} (in seconds) between samples Y(t + τ) and Y(t) is calculated {{as the time}} lag τ <b>divided</b> by the <b>sampling</b> frequency F [...]...|$|R
30|$|In the experiments, we <b>divide</b> the <b>sample</b> {{into two}} parts: the {{training}} set that contains 1000 positive samples and 2000 negative samples {{and the test}} set that contains 500 positive samples and 1000 negative samples. In order to improve {{the efficiency of the}} program, we normalize each sample data to 100 [*]×[*] 100 pixels.|$|R
50|$|Exhaustive {{cross-validation}} {{methods are}} cross-validation methods which learn and test on all possible ways to <b>divide</b> the original <b>sample</b> into a training and a validation set.|$|R
5000|$|The Apdex {{formula is}} the number of {{satisfied}} samples plus half of the tolerating samples plus none of the frustrated <b>samples,</b> <b>divided</b> by all the samples: ...|$|R
3000|$|We {{therefore}} {{account for}} as much as 80 % of the cross-country variation in gender unemployment gaps adjusted for gender differences in pre-market human capital by focusing separately on individuals with and without children, by <b>dividing</b> the <b>sample</b> into the two types of countries, and by applying a single explanatory factor, a different one in each group. 39 [...]...|$|R
30|$|In (4) and (12), {{there are}} two {{parameters}} {{in the process of}} parameter optimization: penalty factor C and kernel function parameter γ. The Grid Search method is to give a range of parameter C and γ, divide the grid under the given step size, traverse each point in the grid by cross validation and select the highest accuracy as the optimal parameter. The method of cross validation is to train samples of SVM model in groups, one is used as a real training set and the other is verified by validation set. The most commonly used is the cross validation of N-CV. The method <b>divides</b> <b>samples</b> into N groups, with one group as a test set and the rest as training sets, to evaluate the accuracy of this parameter after all the results are synthesized. After traversing the grid, the classification accuracy of all the parameters is collected.|$|R
40|$|An {{optical path}} switch <b>divides</b> <b>sample</b> path {{radiation}} into a time series of alternating first polarized components and second polarized components. The first polarized components are transmitted along a first optical path {{and the second}} polarized components along a second optical path. A first gasless optical filter train filters the first polarized components to isolate at least a first wavelength band thereby generating first filtered radiation. A second gasless optical filter train filters the second polarized components to isolate at least a second wavelength band thereby generating second filtered radiation. A beam combiner combines {{the first and second}} filtered radiation to form a combined beam of radiation. A detector is disposed to monitor magnitude of at least a portion of the combined beam alternately at the first wavelength band and the second wavelength band as an indication of the concentration of the substance in the sample path...|$|R
3000|$|... in each subband is obtained. The {{number of}} bins in the {{histogram}} {{has been set}} equal to the square root {{of the number of}} <b>samples</b> <b>divided</b> by two.|$|R
30|$|There are {{different}} approaches to <b>divide</b> the <b>sample</b> firms into financially constrained and unconstrained type. These approaches generally include asset size, annual payout distribution, commercial paper ratings, bond ratings, costs of external financing, the interest coverage ratio, and Wu and Whited index (Whited & Wu (2006)). We use the following three criteria for classifying firm-year observations as financially constrained and unconstrained.|$|R
40|$|We {{report on}} {{compositions}} obtained by instrumental {{neutron activation analysis}} on three new lunar meteorites, MET 01210 (Meteorite Hills, Antarctica; 23 g), NEA 001 (Northeast Africa, Sudan; 262 g), and NWA 3136 (Northwest Africa, Algeria or Morocco; 95 g). As in previous similar studies, we <b>divided</b> our <b>samples</b> into many (8 - 9) small (approximately 30 mg) subsamples prior to analysis...|$|R
