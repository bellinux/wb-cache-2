23|388|Public
5000|$|The <b>dataset</b> <b>structure</b> for {{observations}} is a {{flat file}} representing {{a table with}} one or more rows and columns. Normally, one dataset is submitted for each domain. Each row of the dataset represents a single observation and each column represents one of the variables. Each dataset or table is accompanied by metadata definitions that provide information about the variables used in the dataset. The metadata are described in a data definition document named Define that is submitted along with the data to regulatory authorities.|$|E
30|$|As {{stated in}} Section 5, the whole dataset {{will be made}} {{available}} {{at no cost to}} the research community at the following web address [URL] with the goal of providing the content needed to assess the performance of next generation of image and video forensic tools. In addition to the dataset we release a guide to the <b>dataset</b> <b>structure</b> and several csv files containing names and technical information, such as metadata tags, of all the collected media.|$|E
30|$|Due to, for our purposes, {{undesirable}} artefacts {{in these}} rotation texture datasets (periodic stripes in Outex and JPEG compression in Mondial Marmi), a new dataset with rotations of textures was acquired. The Kylberg Sintorn Rotation Dataset is a generic texture dataset with similar types of textured surfaces {{as in the}} Outex dataset but with the same general <b>dataset</b> <b>structure</b> (many samples of each texture-rotation combination) as the Mondial Marmi dataset. Furthermore, the acquisition setup used in the Kylberg Sintorn Rotation Dataset avoids the aforementioned limitations and artefacts.|$|E
50|$|Define-XML {{can also}} be used to {{describe}} proprietary, non-CDISC <b>dataset</b> <b>structures.</b> The Define-XML model is implemented using extensions to the CDISC Operational Data Model (ODM) XML schema. The current version is 2.0 published on the CDISC web site.|$|R
40|$|This paper {{presents}} {{the current state}} of development of the DBnary dataset. DBnary is a RDF <b>dataset,</b> <b>structured</b> using the LEMON vocabulary, that is extracted from twelve different Wiktionary language editions. DBnary also contains additional relations from translation pairs to their source word senses. The extracted data is registered a...|$|R
40|$|Abstract. A new {{general and}} {{efficient}} architecture {{for working with}} pattern structures, an extension of FCA for dealing with “complex ” descriptions, is introduced and implemented in a subsystem of Formal Concept Analysis Research Toolbox (FCART). The architecture is universal in terms of possible <b>dataset</b> <b>structures</b> and formats, techniques of pattern structure manipulation...|$|R
40|$|PRSD Studio是一款专为研究解决模式识别问题而开发的MATLAB工具箱软件，与传统的模式识别工具箱PRTools相比，它操作方便，运行速度快，可视化程度高，且能调度到matlab环境外部并嵌入到自定义程序中使用。鉴于此，在介绍模式识别方法及几种分类器后，以鸢尾花数据文件iris. data为例，详细阐述了PRSD Studio模式识别工具箱的功能及使用方法，其中包括数据集构造、特征选择、分类器的设计及性能评价等。 PRSD Studio is a MATLAB toolbox {{developed}} for studying and solving pattern recognition problems. And {{compared with the}} traditional pattern recognition PRTools, PRSD Studio has a few special features, such as bedienbarkeit, high speed, high visualization as well as embeding classifiers in your applications outside of the matlab environment. Therefore, after introducing the methods of pattern recognition and several classifiers, it elaborates the function and basic operations of the PRSD Studio pattern recognition toolbox by taking the iris. data as example, including <b>dataset</b> <b>structure,</b> feature selection, classifier design and perfor-mance evaluation...|$|E
40|$|Tabular data is {{difficult}} to analyze and to search through, yielding for new tools and interfaces that would allow even non tech-savvy users to gain insights from open datasets without resorting to specialized data analysis tools or even without having to fully understand the <b>dataset</b> <b>structure.</b> The goal of our demonstration is to showcase answering natural language questions from tabular data, and to discuss related system configuration and model training aspects. Our prototype is publicly available and open-sourced (see [URL] Full version of the demo paper accepted at SEMANTiCS 201...|$|E
30|$|In this paper, {{we propose}} a novel RF-approach that {{combines}} various recent trends on interactive image retrieval systems, such as semi-supervised learning, ranked-based methods, and collaborative image retrieval. The proposed method is semi-supervised since {{it not only}} uses the supervised relevance feedback information but also exploits the unlabeled data. The method is inspired by the recent Pairwise Recommendation [9] algorithm, which considers the intrinsic <b>dataset</b> <b>structure</b> through a recommendation simulation model. Additionally, the approach presents other advantages, as the low computational cost {{and the use of}} an unified recommendation model for representing positive and negative feedback and collaborative image retrieval.|$|E
40|$|International audienceThis paper {{presents}} {{the current state}} of development of the DBnary dataset. DBnary is a RDF <b>dataset,</b> <b>structured</b> using the LEMON vocabulary, that is extracted from twelve different Wiktionary language editions. DBnary also contains additional relations from translation pairs to their source word senses. The extracted data is registered at [URL]...|$|R
30|$|Regardless of the {{modality}} used {{to acquire}} the 3 D <b>datasets,</b> <b>structures</b> of interest have to be segmented and translated into a surface model to enable 3 D printing. Segmentation is the key process herein [33]. In some cases, the vessel wall is too thin to segment; extra thickness then should {{be added to the}} model since 3 D printers have minimum thickness requirements [20].|$|R
40|$|New {{applications}} of data mining, {{such as in}} biology, bioinformatics, or sociology, are faced with large <b>datasets</b> <b>structured</b> as graphs. We present an efficient algorithm for mining associations between tree queries in a large graph. Tree queries are powerful tree-shaped patterns featuring existential variables and data constants. Our algorithm applies the theory of conjunctive database queries to make the generation of association rules efficient. We propose a practical, database-oriented implementation in SQL, and show that the approach works in practice through experiments on data about food webs, protein interactions, and citation analysis. ...|$|R
40|$|An {{accurate}} {{cost model}} {{that accounts for}} dataset size and structure can help optimize geoscience data analysis. We develop and apply a computational model to estimate data analysis costs for arithmetic operations on gridded datasets typical of satellite- or climate model-origin. For these dataset geometries our model predicts data reduction scalings that agree with measurements of widely used geoscience data processing software, the netCDF Operators (NCO). I/O performance and library design dominate throughput for simple analysis (e. g. dataset differencing). <b>Dataset</b> <b>structure</b> can reduce analysis throughput ten-fold relative to same-sized unstructured datasets. We demonstrate algorithmic optimizations which substantially increase throughput for more complex, arithmetic-dominated analysi...|$|E
40|$|Monitoring {{security}} {{and trust in}} on-line personalised recommendation systems is now recognised as a key challenge. Noisy data, or maliciously biased data, can significantly skew the system’s output. This paper out-lines our research goals, which aim to tackle this issue along a number of lines. Game theoretic techniques are applied to determining bounds {{on the effect of}} robust-ness attacks on recommender systems. Graph theoretic techniques are used to analyse the <b>dataset</b> <b>structure</b> and identify influential users in the application user-group, for filtering purposes. A user profile database is a key component of most on– line e–commerce systems. The database stores user infor-mation such as the history of access patterns to the site, prod...|$|E
30|$|In this paper, {{we focus}} on the FCE method using the K-medoids {{clustering}} method that divides all the nodes into functionally similar nodes by the greedy maximization of the objective function. Typical sampling algorithms like Jiang et al. (2002) and Aggarwal et al. (2009) can cluster large datasets. Another previous work (Jiang et al. 2002) focused on the fractal structure of the dataset and extracted a subset of significant size that holds the entire <b>dataset</b> <b>structure.</b> However, because the approximate centers or clusters are computed from stochastically chosen, relatively small objects, the accuracy of the results is not guaranteed. Since our proposed method focuses on the similar structure of road networks, we expect to obtain relatively higher accuracy than the above sampling methods.|$|E
5000|$|U.S. Geological Survey. Best Practices National <b>Structures</b> <b>Dataset.</b> http://bpgeo.cr.usgs.gov/. 30-Jul-2008.|$|R
5000|$|Auto-extraction - Automatically {{extract data}} from web pages into a <b>structured</b> <b>dataset</b> ...|$|R
40|$|Analyzing {{data from}} {{multiple}} complex datasets requires that analysts have knowledge of each <b>dataset’s</b> <b>structure.</b> Comparing variable names, labels, and formats between {{more than two}} complex datasets can be challenging. A data dictionary and SAS procedures such as PROC COMPARE are commonly used to complete this task. However, PROC COMPARE only allows comparison of two datasets at a time. We present relatively simple procedures that data analysts can use to identify {{the changes in the}} structures of two or more than two datasets which can pinpoint areas in which existing SAS code need to be modified to account for changes in the variable names, labels, or formats...|$|R
40|$|Large datasets, on {{the order}} of GB and TB, are {{increasingly}} common as abundant computational resources allow practitioners to collect, produce and store data at higher rates. As dataset sizes grow, it becomes more challenging to interactively manipulate and analyze these datasets due to the large amounts of data that need to be moved and processed. Application-independent caches, such as operating system page caches and database buffer caches, are present throughout the memory hierarchy to reduce data access times and alleviate transfer overheads. We claim that an applicationaware cache with relatively modest memory requirements can effectively exploit <b>dataset</b> <b>structure</b> and application information to speed access to large datasets. We demonstrate this idea {{in the context of a}} system named the tree cache, to reduce query latency to large octree datasets by an order of magnitude. 1...|$|E
40|$|Objective: {{to develop}} a {{prototype}} of a Colorectal Cancer records management system, aiming to store an embracing structured dataset for later application of intelligent data analysis methods. Material and Method: the prototype was developed in five stages: Colorectal Cancer domain analysis, definition of basic requirements (operational and informational), system design, system construction using open source technologies and system evaluation supported by domain experts. Results and Discussion: during development, the prototype <b>dataset</b> <b>structure,</b> coherence of information structure, validity of functions implemented and the attendance to the basic requirements define, were evaluated by domain experts. Conclusion: according to the experts evaluation, the prototype development completed {{the first stage of}} the project of developing the system with a 100 % satisfaction rate and is now being used into recording real data. In the next stage it will be finished and deployed the complete system, attending to security requirements and multiuser support...|$|E
30|$|The {{interaction}} of users with search services {{has been recognized}} as an important mechanism for expressing and handling user information needs. One traditional approach for supporting such interactive search relies on exploiting relevance feedbacks (RF) in the searching process. For large-scale multimedia collections, however, the user efforts required in RF search sessions is considerable. In this paper, we address this issue by proposing a novel semi-supervised approach for implementing RF-based search services. In our approach, supervised learning is performed taking advantage of relevance labels provided by users. Later, an unsupervised learning step is performed {{with the objective of}} extracting useful information from the intrinsic <b>dataset</b> <b>structure.</b> Furthermore, our hybrid learning approach considers feedbacks of different users, in collaborative image retrieval (CIR) scenarios. In these scenarios, the relationships among the feedbacks provided by different users are exploited, further reducing the collective efforts. Conducted experiments involving shape, color, and texture datasets demonstrate the effectiveness of the proposed approach. Similar results are also observed in experiments considering multimodal image retrieval tasks.|$|E
40|$|Parallel {{scientific}} applications {{store and}} retrieve very large, <b>structured</b> <b>datasets.</b> Directly supporting these structured accesses {{is an important}} step in providing high-performance I/O solutions for these applications. High-level interfaces such as HDF 5 and Parallel netCDF provide convenient APIs for accessing <b>structured</b> <b>datasets,</b> and the MPI-IO interface also supports efficient access to structured data. However, parallel file systems do not traditionally support such access. In this work we present an implementation [...] ...|$|R
40|$|Abstract. Hierarchical Multi-Label Classification is {{a complex}} classifi-cation problem where the classes are hierarchically structured. This task is very common in protein {{function}} prediction, where each protein can {{have more than one}} function, which in turn can have more than one sub-function. In this paper, we propose a novel hierarchical multi-label classification algorithm for protein function prediction, namely HMC-PC. It is based on probabilistic clustering, and it makes use of cluster membership probabilities in order to generate the predicted class vec-tor. We perform an extensive empirical analysis in which we compare our new approach to four different hierarchical multi-label classification algorithms, in protein function <b>datasets</b> <b>structured</b> both as trees and di-rected acyclic graphs. We show that HMC-PC achieves superior or com-parable results compared to the state-of-the-art method for hierarchical multi-label classification...|$|R
40|$|The Mathematics Subject Classification (MSC) is {{a widely}} used scheme for {{classifying}} documents in mathematics by subject. Its traditional, idiosyncratic conceptualization and representation makes the scheme hard to maintain and requires custom implementations of search, query and annotation support. This limits uptake e. g. in semantic web technologies {{in general and the}} creation and exploration of connections between mathematics and related domains (e. g. science) in particular. This paper presents the new official implementation of the MSC 2010 as a Linked Open Dataset, building on SKOS (Simple Knowledge Organization System). We provide a brief overview of the <b>dataset's</b> <b>structure,</b> its available implementations, and first applications. Comment: Conference on Intelligent Computer Mathematics, July 9 - 14, Bremen, Germany. Published as number 7362 in Lecture Notes in Artificial Intelligence, Springe...|$|R
40|$|Abstract—A Type- 2 fuzzy {{clustering}} algoritm that integreates Type- 2 fuzzy sets with Gustafson-Kessel {{algorithm is}} proposed in this paper. The proposed Type- 2 Gustafson-Kessel algorithm (T 2 GKA) is essentially a combination of probabilistic and possibilistic clustering schemes. It will be shown that the T 2 GKA is less susceptive to noise than the Type- 1 GKA. The T 2 GKA ignores the inlier and outlier interruptions. The clustering results show the robustness of the proposed T 2 GKA since {{a reasonable amount of}} noise data does not affect its clustering performance. A drawback of the conventional GKA is that it can only find clusters of approximately equal volume. To overcome this difficulty, this work uses an algorithm called The Directed Evaluation Ellipsoid Cluster Volume (DEECV) to effectively evaluate the proper ellipsoid volume. The proposed T 2 GKA is essentially a DEECV based learning algorithm integrated with T 2 GKA. The experimental results show that the T 2 GKA can learn suitable sized cluster volume along with a varying <b>dataset</b> <b>structure</b> volume. Keywords-ellipsoids; probabilistic; possibilistic; fuzzy c-means; Gustafson-Kessel algorithm; Type- 2 fuzzy clustering I...|$|E
40|$|Protection of one’s {{intellectual}} property {{is a topic}} with important technological and legal facets. The significance of this issue is amplified nowadays due to the ease of data dissemination through the internet. Here, we provide technological mechanisms for establishing the ownership of a dataset consisting of multiple objects. The objects that we consider in this work are shapes (i. e., two dimensional contours), which abound in disciplines such as medicine, biology, anthropology and natural sciences. The protection of the dataset is achieved through means of embedding of an imperceptible ownership ‘seal’, that imparts only minute visual distortions. This seal needs to be embedded in the proper data space so that its removal or destruction is particularly difficult. Our technique is robust to many common transformations, such as data rotation, translation, scaling, noise addition and resampling. In addition to that, the proposed scheme also guarantees that important distances between the dataset shapes/objects are not distorted. We achieve this by preserving the geodesic distances between the dataset objects. Geodesic distances capture {{a significant part of}} the <b>dataset</b> <b>structure,</b> and their usefulness is recognized in many machine learning, visualization and clustering algorithms. Therefore, if a practitioner uses the protected dataset as input to a variety of mining, machine learning, or database operations, the output will be the same as on the original dataset. We illustrate and validate the applicability of our methods on image shapes extracted from anthropological and natural science data. 1...|$|E
40|$|Abstract The {{detection}} of gene - gene and gene - environment interactions associated with complex human disease or pharmacogenomic endpoints {{is a difficult}} challenge for human geneticists. Unlike rare, Mendelian diseases {{that are associated with}} a single gene, most common diseases are caused by the non-linear interaction of numerous genetic and environmental variables. The dimensionality involved in the evaluation of combinations of many such variables quickly diminishes the usefulness of traditional, parametric statistical methods. Multifactor dimensionality reduction (MDR) is a novel and powerful statistical tool for detecting and modelling epistasis. MDR is a non-parametric and model-free approach that has been shown to have reasonable power to detect epistasis in both theoretical and empirical studies. MDR has detected interactions in diseases such as sporadic breast cancer, multiple sclerosis and essential hypertension. As this method is more frequently applied, and was gained acceptance in the study of human disease and pharmacogenomics, it is becoming increasingly important that the implementation of the MDR approach is properly understood. As with all statistical methods, MDR is only powerful and useful when implemented correctly. Concerns regarding <b>dataset</b> <b>structure,</b> configuration parameters and the proper execution of permutation testing in reference to a particular dataset and configuration are essential to the method's effectiveness. The detection, characterisation and interpretation of gene - gene and gene - environment interactions are expected to improve the diagnosis, prevention and treatment of common human diseases. MDR can be a powerful tool in reaching these goals when used appropriately. </p...|$|E
40|$|Abstract. Formulating a query on a Linked Open Data (LOD) {{source is}} not an easy task; a {{technical}} knowledge of the query language, and, the awareness of the <b>structure</b> of the <b>dataset</b> are essential to create a query. We present a revised version of LODeX that provides the user an easy way for building queries in a fast and interactive manner. When a user decides to explore a LOD source, he/she {{can take advantage of the}} Schema Summary produced by LODeX (i. e. a synthetic view of the <b>dataset’s</b> <b>structure)</b> and he/she can pick graphical elements from it to create a visual query. The tool also supports the user in browsing the results and, eventually, in refining the query. The prototype has been evaluated on hundreds of public SPARQL endpoints (listed in Data Hub 1) and it is available online a...|$|R
30|$|The ClueWeb dataset is a semi-synthetic dataset {{generated}} from the ClueWeb Category B dataset. The original dataset was very small. Thus, we randomly generated new pages each with 10 outgoing links, and added them to the original <b>dataset</b> graph <b>structure</b> to make it larger. The total size of the new dataset is 30 GB.|$|R
40|$|Formulating a query on a Linked Open Data (LOD) {{source is}} not an easy task; a {{technical}} knowledge of the query language, and, the awareness of the <b>structure</b> of the <b>dataset</b> are essential to create a query. We present a revised version of LODeX that provides the user an easy way for building queries in a fast and interactive manner. When a user decides to explore a LOD source, he/she {{can take advantage of the}} Schema Summary produced by LODeX (i. e. a synthetic view of the <b>dataset’s</b> <b>structure)</b> and he/she can pick graphical elements from it to create a visual query. The tool also supports the user in browsing the results and, eventually, in refining the query. The prototype has been evaluated on hundreds of public SPARQL endpoints (listed in Data Hub) and it is available online at [URL] A survey conducted on 27 users has demonstrated that our tool can effectively support both unskilled and skilled users in exploring and querying LOD datasets...|$|R
40|$|Large {{volumes of}} marine geoscientific {{datasets}} have been gathered by various institutes {{over the past}} number of years. In order to add value to these very costly and valuable products and "improve the quality of scientific advice, " an effort must be spent on providing integrated management and access to these datasets. This will allow a more holistic or "ecosystem " approach {{in the analysis of}} marine and geoscientific data. The objective of the GeoDI (Geological & Geophysical Data Integration) project is to derive maximum value from the national data acquisition effort to date and to allow future data to be integrated easily. As part of GeoDI a database is being designed and implemented for integrating marine geoscientific datasets using a common structure and common semantics. A key issue that is addressed by GeoDI is populating the database using the datasets that are continuously being collected. As a matter of fact, data collection procedures are continuously evolving, resulting in a variety of data formats, structures and semantics. GeoDI is designing and developing an automatic ontology-based ETL system for marine geoscientific data. The system automatically (i) extracts the structure and semantics of a new dataset to be integrated, (ii) matches the <b>dataset</b> <b>structure</b> and semantics to those of the integrated database, (iii) transforms the dataset according to the integrated database schema, and (iv) loads it. The GeoDI ETL system uses ontologies as a way to represent data structure and semantics. It is based on an extensible multi-strategy learning approach wherein different matchers (learners) are trained separately to match new schemas to the integrated database schema. Given a new dataset to be integrated into the geoscientific database, each learner maps the schema of the dataset to that of the integrated database. Decisions of the various learners are then combined by a meta-matcher...|$|E
40|$|Abstract Background With {{the advance}} of {{microarray}} technology, several methods for gene classification and prognosis have been already designed. However, under various denominations, some of these methods have similar approaches. This study evaluates the influence of gene expression variance structure {{on the performance of}} methods that describe the relationship between gene expression levels and a given phenotype through projection of data onto discriminant axes. Results We compared Between-Group Analysis and Discriminant Analysis (with prior dimension reduction through Partial Least Squares or Principal Components Analysis). A geometric approach showed that these two methods are strongly related, but differ in the way they handle data structure. Yet, data structure helps understanding the predictive efficiency of these methods. Three main structure situations may be identified. When the clusters of points are clearly split, both methods perform equally well. When the clusters superpose, both methods fail to give interesting predictions. In intermediate situations, the configuration of the clusters of points has to be handled by the projection to improve prediction. For this, we recommend Discriminant Analysis. Besides, an innovative way of simulation generated the three main structures by modelling different partitions of the whole variance into within-group and between-group variances. These simulated datasets were used in complement to some well-known public datasets to investigate the methods behaviour in a large diversity of structure situations. To examine the structure of a dataset before analysis and preselect an a priori appropriate method for its analysis, we proposed a two-graph preliminary visualization tool: plotting patients on the Between-Group Analysis discriminant axis (x-axis) and on the first and the second within-group Principal Components Analysis component (y-axis), respectively. Conclusion Discriminant Analysis outperformed Between-Group Analysis because it allows for the <b>dataset</b> <b>structure.</b> An a priori knowledge of that structure may guide the choice of the analysis method. Simulated datasets with known properties are valuable to assess and compare the performance of analysis methods, then implementation on real datasets checks and validates the results. Thus, we warn against the use of unchallenging datasets for method comparison, such as the Golub dataset, because their structure is such that any method would be efficient. </p...|$|E
40|$|Groundwater {{monitoring}} networks typically yield large, multivariate datasets. Analysis {{and interpretation}} of these datasets starts with an exploratory data analysis in order to summarize the available data, extract useful information and formulate hypotheses for further research. Exploratory data analysis is mostly focussed on finding related variables and groupings of similar observations. Traditionally multivariate statistical techniques like principal component analysis (PCA) are used for this purpose. In PCA a linear dimensionality reduction of the original, high dimensional dataset is carried out {{in order to identify}} orthogonal directions (principal components) of maximum variance in the dataset based on linear combinations of correlated variables. Projections of the original data in the subspace defined by the principal components can be used to identify groups in the data and to reveal relationships between variables (Davis, 1986). In this study, principal component analysis is compared to Kohonen's self-organizing map (SOM) algorithm. The SOM-algorithm is an artificial neural network technique designed to carry out a non-parametric regression process that is mainly used to represent high-dimensional, nonlinearly related data items in a topology-preserving, often two-dimensional display, and to perform unsupervised classification and clustering (Kohonen, 1995). Both PCA and SOM are applied to a hydrochemical dataset from a monitoring network in two sandy, phreatic aquifers in Central Belgium. The monitoring network consists of 47 monitoring wells each equipped with three filters at different depths, in which 14 variables are measured. The first aquifer, the Diest sands aquifer is of Late Miocene age and consists of coarse, glauconiferous sands and sandstones (Laga et al., 2001). The second aquifer, the Brussels sands aquifer, is of Middle Eocene age and is an heterogeneous formation consisting of an alteration of highly and poorly calcareous sands, locally silicified (Laga et al., 2001). Both techniques succeed in distinguishing between both aquifers and reveal the relationships between variables. The main advantage of PCA is the mathematical quantification of correlation between variables and the expression of the original data in the subspace defined by the principal components. The visualization of the SOM-analysis on the other hand allows a straightforward interpretation of the <b>dataset</b> <b>structure</b> in which even non-linear relationships between variables can be identified. Additionally, the SOM-algorithm can handle a limited amount of missing values in the dataset, contrary to PCA...|$|E
40|$|CZSaw [1] is {{a visual}} {{analytics}} tool for sense-making across entities, entity collections, and relations {{with a focus}} on augmenting the analysis process. It uses a variety of flexible data visualizations to represent, explore, and compute networks of entities and relations from different perspectives. CZSaw is designed to provide a replayable record of the analysis process and to generate a reusable model of the analysis logic, structured as a dependency graph. To support these goals, semantically meaningful interactions are captured into a script. Replaying this script replays the analysis process, and editing it allows fine control and reuse of the process. Specialized viewers are also provided for the dependency graph and for the user’s history, to provide more visual interaction. This demo shows how CZSaw can be used to analyze different types of <b>datasets</b> (<b>structured</b> and unstructured data), as well as some strategies (e. g. divide and conquer) used on analysis tasks...|$|R
50|$|Dieselpoint, Inc. is {{a privately}} held {{enterprise}} search company headquartered in Chicago, IL. The company {{was founded in}} 1999 and develops software to search large <b>datasets</b> with both <b>structured</b> and unstructured elements.|$|R
40|$|The Listening Experience Database (LED) is {{a project}} that gathers {{documented}} evidence of listening to music across cultural and historical contexts. Its underlying information system relies on the principles and practices of Linked Data, including a knowledge base that is itself a linked <b>dataset</b> <b>structured</b> according to common vocabularies for media and bibliographies such as Bibo and the Music Ontology. The data management workflow fully supports crowd-sourced input and incorporates data reuse from various sources right {{from the point of}} data collection, including the British National Bibliography dataset. The LED system gives contributors and moderators the tools to enhance these data by modelling divisions of bibliographc entries, providing information beyond the BNB schema, or aligning with other datasets by simple data reconciliation procedures. Vocabularies for music genre and instrument classes are also crowd-sourced on top of a baseline taxonomy from the DBpedia dataset. The Web frontend uses the Drupal content management system to encapsulate listening experiences into digital objects that incorporate textual information with references to literary, musical and other classes of related entities...|$|R
