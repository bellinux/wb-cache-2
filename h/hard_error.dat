35|176|Public
50|$|It {{follows from}} the above {{definition}} that any prediction that misses a single token, includes a spurious token, or has the wrong class, is a <b>hard</b> <b>error</b> and does not contribute to either precision or recall.|$|E
5000|$|Single-event snapback, {{similar to}} SEL but not {{requiring}} the PNPN structure, can be induced in N-channel MOS transistors switching large currents, when an ion hits near the drain junction and causes avalanche multiplication {{of the charge}} carriers. The transistor then opens and stays opened. <b>Hard</b> <b>error,</b> irreversible.|$|E
5000|$|Single-event induced burnout (SEB) {{may occur}} in power MOSFETs when the {{substrate}} {{right under the}} source region gets forward-biased and the drain-source voltage {{is higher than the}} breakdown voltage of the parasitic structures. The resulting high current and local overheating then may destroy the device. <b>Hard</b> <b>error,</b> irreversible.|$|E
30|$|<b>Hard</b> <b>errors</b> {{occur when}} the data value stored in one cell cannot be changed in the next {{programming}} cycle. There {{are two types of}} <b>hard</b> <b>errors</b> in PRAM: stuck-RESET failure and stuck-SET failure[20]. Stuck-SET or stuck-RESET means that the value of stored data in PRAM cell is stuck in SET or RESET state no matter what value has been written into the cell. These errors increase as the number of programming cycles increases.|$|R
40|$|This article {{presents}} a chip multiprocessor (CMP) design that mixes coarse- and fine-grained reconfigurability to increase core availability of safety-critical embedded {{systems in the}} presence of <b>hard</b> <b>errors.</b> The authors conducted a comprehensive design-space exploration to identify the granularity mixes that maximize CMP fault tolerance and minimize performance and energy overheads. The authors added fine-grained reconfigurable logic to a coarse-grained sparing approach. Their resulting design can tolerate 3 times more <b>hard</b> <b>errors</b> than core redundancy and 1. 5 times more than any other purely coarse-grained solution...|$|R
40|$|The {{incidence}} of <b>hard</b> <b>errors</b> in CPUs {{is a challenge}} for future multicore designs due to increasing total core area. Even if the location and nature of <b>hard</b> <b>errors</b> are known a priori, either at manufacture-time or in the field, cores with such errors must be disabled {{in the absence of}} hard-error tolerance. While caches, with their regular and repetitive structures, are easily covered against <b>hard</b> <b>errors</b> by providing spare arrays or spare lines, structures within a core are neither as regular nor as repetitive. Previous work has proposed microarchitectural core salvaging to exploit structural redundancy within a core and maintain functionality in the presence of <b>hard</b> <b>errors.</b> Unfortunately microarchitectural salvaging introduces complexity and may provide only limited coverage of core area against <b>hard</b> <b>errors</b> {{due to a lack of}} natural redundancy in the core. This paper makes a case for architectural core salvaging. We observe that even if some individual cores cannot execute certain operations, a CPU die can be instruction-set-architecture (ISA) compliant, that is execute all of the instructions required by its ISA, by exploiting natural cross-core redundancy. We propose using hardware to migrate offending threads to another core that can execute the operation. Architectural core salvaging can cover a large core area against faults, and be implemented by leveraging known techniques that minimize changes to the microarchitecture. We show it is possible to optimize architectural core salvaging such that the performance on a faulty die approaches that of a fault-free die [...] assuring significantly better performance than core disabling for many workloads and no worse performance than core disabling for the remainder...|$|R
5000|$|Single-event gate rupture (SEGR) was {{observed}} in power MOSFETs when a heavy ion hits the gate region while a high voltage {{is applied to the}} gate. A local breakdown then happens in the insulating layer of silicon dioxide, causing local overheat and destruction (looking like a microscopic explosion) of the gate region. It can occur even in EEPROM cells during write or erase, when the cells are subjected to a comparatively high voltage. <b>Hard</b> <b>error,</b> irreversible.|$|E
5000|$|Single-event latchup (SEL) {{can occur}} in any chip with a {{parasitic}} PNPN structure. A heavy ion or a high-energy proton passing {{through one of the}} two inner-transistor junctions can turn on the thyristor-like structure, which then stays [...] "shorted" [...] (an effect known as latchup) until the device is power-cycled. As the effect can happen between the power source and substrate, destructively high current can be involved and the part may fail. <b>Hard</b> <b>error,</b> irreversible. Bulk CMOS devices are most susceptible.|$|E
50|$|When {{a device}} is {{operating}} in full duplex {{while the other}} one operates in half duplex, the connection works only with at a very low throughput when both devices attempt to send frames {{at the same time}}. This is because data can be sent in both directions {{at the same time in}} full-duplex mode, but only in one direction at a time in half-duplex mode. As a result, a full-duplex device may transmit data while it is receiving. However, if the other device is working in half duplex, it does not expect to receive data (because it is currently sending); therefore, it senses a collision and attempts to resend the frame it was sending. Depending on timing, the half-duplex device may sense a late collision, which it will interpret as a <b>hard</b> <b>error</b> rather than a normal consequence of CSMA/CD and may not attempt to resend the frame. On the other hand, the full-duplex device does not detect any collision and does not resend the frame, even if the other device has discarded it as corrupted by collision. Still, the full-duplex device, not expecting incoming frames to be truncated by collision detection, will report frame check sequence errors from the aborted frames the half-duplex device attempted to send. This combination of (late) collisions reported at the half-duplex end and FCS errors reported by the full-duplex end can be used as an indication that a duplex mismatch is present.|$|E
40|$|A new {{architecture}} for {{matching the}} data protected with an error-correcting code (ECC) {{is presented in}} this brief to reduce latency and complexity. In this paper, by making use of the property that soft errors are rare and <b>hard</b> <b>errors</b> increase gradually {{with the number of}} writes, a novel scheme is proposed to correct both soft and <b>hard</b> <b>errors</b> through integrating 2 -error-correcting BCH codes and ECPs (error-correcting pointers). In particular, we propose an efficient technique for partial self-checking FINITE STATE MACHINES (FSM) design based on on-line monitoring of FSM state transitions...|$|R
40|$|Technology scaling {{leads to}} burn-in phase out and {{increasing}} post-silicon test complexity, which increases in-the-field error rate due to both latent defects and actual errors. As a consequence, {{there is an}} increasing need for continuous on-line testing techniques to cope with <b>hard</b> <b>errors</b> in the field. Similarly, those techniques are needed for detecting soft errors in logic, whose error rate is expected to raise in future technologies. Cache memories, which occupy most of {{the area of the}} chip, are typically protected with parity or ECC, but most of the wires as well as some combinational blocks remain unprotected against both soft and <b>hard</b> <b>errors.</b> This paper presents a set of techniques to detect and confine <b>hard</b> and soft <b>errors</b> in cache memories in combination with parity/ECC at very low cost. By means of hard signatures in data rows and error tracking, faults can be detected, classified properly and confined for hardware reconfiguration. Peer ReviewedPostprint (published version...|$|R
30|$|In this section, we {{describe}} {{the basic structure of}} the PRAM cell including read and write operations (see “Background” section), characterization of its soft <b>errors</b> and <b>hard</b> <b>errors</b> (see “PRAM error model” section), and a circuit-level technique to reduce these errors (see “Circuit-level techniques for reducing soft and hard errors” section).|$|R
30|$|The <b>hard</b> <b>error</b> rate of 2 -bit MLC PRAM {{is due to}} the {{resistance}} drop of ‘ 00 ’ state to the ‘ 01 ’ state as shown in Figure 7. It is a function of Rth(01, 00), and {{the resistance}} distribution of state 00. Due to multiple pulse write strategy for intermediate states, there is no resistance drop from ‘ 01 ’ state to ‘ 10 ’ state, and thus Rth(10, 01) has no impact on the <b>hard</b> <b>error</b> rate.|$|E
30|$|In {{order to}} {{counteract}} the effect of resistance drift, dynamic Rth(01, 00) and Rth(10, 01) tuning has been proposed in[11]. Here, a time tag is used to record the storage time information for each memory block or page and this information is {{used to determine the}} threshold resistance that minimizes the BER. The technique in[11] considers the effect of resistance drift on soft errors. The threshold resistance value affects the <b>hard</b> <b>error</b> rate as well and so the choice of threshold resistance has to be determined by both soft and <b>hard</b> <b>error</b> rates as will be described next.|$|E
40|$|Applications {{of highly}} scaled devices in space {{applications}} are {{shown to be}} limited by hard errors from cosmic rays. Hard errors were first observed in 0. 8 (micro) m DRAMs. For feature sizes below 0. 5 (micro) m, scaling theory predicts that low power devices will have much lower <b>hard</b> <b>error</b> rates than devices optimized for high speed...|$|E
40|$|Main {{memory is}} one of the leading {{hardware}} causes for machine crashes in today’s datacenters. Designing, evaluating and modeling systems that are resilient against memory errors requires a good understanding of the underlying characteristics of errors in DRAM in the field. While there have recently been a few first studies on DRAM errors in production systems, these have been too limited in either the size of the data set or the granularity of the data to conclusively answer many of the open questions on DRAM errors. Such questions include, for example, the prevalence of soft <b>errors</b> compared to <b>hard</b> <b>errors,</b> or the analysis of typical patterns of <b>hard</b> <b>errors.</b> In this paper, we study data on DRAM errors collected on a diverse range of production systems in total covering nearly 300 terabyte-years of main memory. As a first contribution, we provide a detailed analytical study of DRAM error characteristics, including both <b>hard</b> and soft <b>errors.</b> We find that a large fraction of DRAM errors in the field can be attributed to <b>hard</b> <b>errors</b> and we provide a detailed analytical study of their characteristics. As a second contribution, the paper uses the results from the measurement study to identify a number of promising directions for designing more resilient systems and evaluates the potential of different protection mechanisms in the light of realistic error patterns. One of our findings is that simple page retirement policies might be able to mask a large number of DRAM errors in production systems, while sacrificing only a negligible fraction of the total DRAM in the system...|$|R
40|$|This paper {{presents}} an {{experimental study of}} the sensitivity to 15 -MeV neutrons at low bias voltage of advanced low-power SRAMs by Renesas Electronics. The most interesting results are the occurrence of clusters of bitflips, <b>hard</b> <b>errors</b> only visible at low voltage, appearing along with single event upsets. The physical mechanisms are briefly discussed...|$|R
30|$|There {{are several}} {{factors that affect}} the failure in STT-RAM memories: access {{transistor}} manufacturing errors such as those due to RDFs, channel length, and width modulations, geometric variations in MTJ such as area and thickness variation, and thermal fluctuations that are modeled by the initial magnetization angle variation[15]. Note that all these variations cause <b>hard</b> <b>errors.</b>|$|R
40|$|Radiation {{test data}} {{submitted}} by many testers is collated {{to serve as}} a reference for engineers who are concerned with and have some knowledge {{of the effects of the}} natural radiation environment on microcircuits. Total dose damage information and single event upset cross sections, i. e., the probability of a soft error (bit flip) or of a <b>hard</b> <b>error</b> (latchup) are presented...|$|E
40|$|This {{databank}} is the collation {{of radiation}} test data submitted by many testers {{and serves as}} a reference for engineers who are concerned with and have some knowledge {{of the effects of the}} natural radiation environment on microcircuits. It contains radiation sensitivity results from ground tests and is divided into two sections. Section A lists total dose damage information, and section B lists single event upset cross sections, I. E., the probability of a soft error (bit flip) or of a <b>hard</b> <b>error</b> (latchup) ...|$|E
40|$|Testing is a {{difficult}} process that becomes more difficult with scaling. With smaller and faster devices, tolerance for errors shrinks and devices may act correctly under certain condition and not under others. As such, hard errors may exist but are only exercised by very specific machine state and signal pathways. Targeting these errors is difficult, and creating test cases that cover all machine states and pathways is not possible. In addition, new complications during burn-in may mean latent hard errors are not exposed in the fab and reach the customer before becoming active. To address this problem, we propose an architecture we call BlackJack that allows hard errors to be detected using redundant threads running on a single SMT core. This technique provides a safety-net that catches hard errors that were either latent during test or just {{not covered by the}} test cases at all. Like SRT, our technique works by executing redundant copies and verifying that their resulting machine states agree. Unlike SRT, BlackJack is able to achieve high <b>hard</b> <b>error</b> instruction coverage by executing redundant threads on different front and backend resources in the pipeline. We show that for a 15 % performance penalty over SRT, BlackJack achieves 97 % <b>hard</b> <b>error</b> instruction coverage compared to SRT’s 35 %. ...|$|E
40|$|Rights to {{individual}} papers {{remain with the}} author or the author's employer. Permission is granted for noncommercial reproduction of the work for educational or research purposes. This copyright notice must {{be included in the}} reproduced paper. USENIX acknowledges all trademarks herein. and will continue to be, the most probable cause of loss of availability. While such failures are clearly common-place, especially in desktop environments, the probabil-ity of certain hardware errors is increasing. Hardware errors can be classified as <b>hard</b> <b>errors</b> and transient (soft) <b>errors.</b> <b>Hard</b> <b>errors</b> are those that require replacement (or otherwise relinquished use) of the com-ponent. They are typically caused by physical damage to a component, e. g. by damage to connectors. Transient errors are those that result in an invalid state that can be corrected, for example, by overwriting a corrupt mem-ory location. Ziegler et al. [21, 22] have shown that fac...|$|R
40|$|The {{relentless}} {{scaling of}} CMOS technology {{has provided a}} steady increase in processor performance {{for the past three}} decades. However, increased power densities (hence temperatures) and other scaling effects have an adverse impact on long-term processor lifetime reliability. This paper represents a first attempt at quantifying the impact of scaling on lifetime reliability due to intrinsic <b>hard</b> <b>errors,</b> taking workload characteristics into consideration...|$|R
40|$|Abstract—Programmers {{generally}} {{attempt to}} perform useful work. If they performed an action, {{it was because}} they believed it served some purpose. Redundant operations violate this belief. However, in the past, redundant operations have been typically regarded as minor cosmetic problems rather than serious errors. This paper demonstrates that, in fact, many redundancies are as serious as traditional <b>hard</b> <b>errors</b> (such as race conditions or null pointer dereferences). We experimentally test this idea by writing and applying five redundancy checkers {{to a number of}} large open source projects, finding many errors. We then show that, even when redundancies are harmless, they strongly correlate with the presence of traditional <b>hard</b> <b>errors.</b> Finally, we show how flagging redundant operations gives a way to detect mistakes and omissions in specifications. For example, a locking specification that binds shared variables to their protecting locks can use redundancies to detect missing bindings by flagging critical sections that include no shared state. Index Terms—Extensible compilation, error detection, program redundancy, software quality. æ...|$|R
40|$|The {{number of}} CPUs in chip multiprocessors {{is growing at}} the Moore’s Law rate, due to {{continued}} technology advances. However, new technologies pose serious reliability challenges, such as more frequent occurrences of degraded or even nonoperational devices, and they threaten the cost-effectiveness and dependability of future computing systems. This work studies how to protect the on-chip coherence directory from fault occurrences. In a chip multiprocessor, cache coherence mechanisms such as directory memory are critical for offering consistent data view to all CPUs. We propose a novel online fault detection and correction scheme to enhance yield and resilience to runtime errors at a small performance cost. The proposed scheme uses smart encoding and coherence protocol adaptation strategies to salvage faulty directory entries. We also develop an online error recovery scheme that protects the directory memory from soft errors. We call our fault-tolerant directory memory architecture PERFECTORY. Evaluation results show that PERFECTORY achieves very high fault resilience: Over 99 percent chip yield at 0. 05 percent <b>hard</b> <b>error</b> ratio and 1, 934 years MTTF at 1, 000 FIT using a 100 -processor cluster configuration. PERFECTORY limits performance degradation to less than 1 percent at 0. 05 percent <b>hard</b> <b>error</b> ratio and requires significantly smaller area overheads than existing redundancy approaches...|$|E
30|$|In the {{previous}} section, {{we have shown}} that the soft error rate increases with data storage time and that the <b>hard</b> <b>error</b> rate increases with the number of programming cycles. In this section, we show how the error rate can be controlled by tuning the threshold resistance Rth(00, 01) for a specific data storage time. Recall that threshold resistance can be tuned by changing the current reference of the sense amplifier. Data storage time is set to 105 s, which is typical of storage systems such as those for daily backup.|$|E
40|$|As DRAM {{is facing}} the scaling {{difficulty}} in terms of energy cost and reliability, some nonvol-atile storage materials were proposed to be the substitute or supplement of main memory. Phase Change Memory (PCM) {{is one of the}} most promising nonvolatile memory that could be put into use in the near future. However, before becoming a qualified main memory tech-nology, PCM should be designed reliably so that it can ensure the computer system’s stable running even when errors occur. The typical wear-out errors in PCM have been well studied, but the transient errors, that caused by high-energy particles striking on the complementary metal-oxide semiconductor (CMOS) circuit of PCM chips or by resistance drifting in multi-level cell PCM, have attracted little focus. In this paper, we propose an innovative mecha-nism, Local-ECC-Global-ECPs (LEGE), which addresses both soft errors and hard errors (wear-out errors) in PCMmemory systems. Our idea is to deploy a local error correction code (ECC) section to every data line, which can detect and correct one-bit errors immedi-ately, and a global error correction pointers (ECPs) buffer for the whole memory chip, which can be reloaded to correct more <b>hard</b> <b>error</b> bits. The local ECC is used to detect and correct the unknown one-bit errors, and the global ECPs buffer is used to store the corrected value of hard errors. In comparison to ECP- 6, our method provides almost identical lifetimes, but reduces approximately 50 % storage overhead. Moreover, our structure reduces approxi-mately 3. 55 % access latency overhead by increasing 1. 61 % storage overhead compared to PAYG, a <b>hard</b> <b>error</b> only solution...|$|E
40|$|Programmers {{generally}} {{attempt to}} perform useful work. If they performed an action, {{it was because}} they believed it served some purpose. Redundant operations violate this belief. However, in the past redundant operations have been typically regarded as minor cosmetic problems rather than serious errors. This paper demonstrates that in fact many redundancies are as serious as traditional <b>hard</b> <b>errors</b> (such as race conditions or null pointer dereferences). We experimentally test this idea by writing and applying five redundancy checkers {{to a number of}} large open source projects, finding many errors. We then show that even when redundancies are harmless they strongly correlate with the presence of traditional <b>hard</b> <b>errors.</b> Finally we show how flagging redundant operations gives a way to detect mistakes and omissions in specifications. For example, a locking specification that binds shared variables to their protecting locks can use redundancies to detect missing bindings by flagging critical sections that include no shared state...|$|R
5000|$|Predictive failure {{analysis}} to predict which intermittent correctable errors will lead eventually to <b>hard</b> non-correctable <b>errors.</b>|$|R
40|$|Low-power modes {{in modern}} {{microprocessors}} rely on low frequencies and low voltages {{to reduce the}} energy budget. Nevertheless, manufacturing induced parameter variations can make SRAM cells unreliable producing <b>hard</b> <b>errors</b> at supply voltages below Vccmin. Recent proposals provide a rather low fault-coverage due to the fault coverage/overhead trade-off. We propose a new fault- tolerant L 1 cache, which combines SRAM and eDRAM cells in L 1 data caches to provide 100...|$|R
40|$|Abstract — Today’s {{embedded}} systems integrate multiple IP cores for processing, communication and sensing {{on a single}} die as Systems on a Chip (SoCs). Aggressive transistor scaling, decreased voltage margins and increased processor power and temperature have made reliability assessment a much more significant issue. Although reliability of devices and interconnect has been broadly studied, in this work we study a tradeoff between reliability and power consumption for component-based SoC designs. We specifically focus on <b>hard</b> <b>error</b> rates as they cause a device to permanently stop operating. We also present a joint reliability and power management optimization problem whose solution is an optimal management policy. When careful joint policy optimization is performed, we obtain a significant improvement in energy consumption (40 %) in tandem with meeting a reliability constraint for all SoC operating temperatures...|$|E
40|$|Today's {{embedded}} systems integrate multiple IP cores for processing, communication, and sensing {{on a single}} die as systems-on-chip (SoCs). Aggressive transistor scaling, decreased voltage margins and increased processor power and temperature have made reliability assessment a much more significant issue. Although reliability of devices and interconnect has been broadly studied, in this work, we study a tradeoff between reliability and power consumption for component-based SoC designs. We specifically focus on <b>hard</b> <b>error</b> rates as they cause a device to permanently stop operating. We also present a joint reliability and power management optimization problem whose solution is an optimal management policy. When careful joint policy optimization is performed, we obtain a significant improvement in energy consumption (40 %) in tandem with meeting a reliability constraint for all SoC operating temperatures...|$|E
40|$|Abstract—Today’s {{embedded}} systems integrate multiple IP cores for processing, communication, and sensing {{on a single}} die as systems-on-chip (SoCs). Aggressive transistor scaling, decreased voltage margins and increased processor power and temperature have made reliability assessment a much more sig-nificant issue. Although reliability of devices and interconnect has been broadly studied, in this work, we study a tradeoff between reliability and power consumption for component-based SoC designs. We specifically focus on <b>hard</b> <b>error</b> rates as they cause a device to permanently stop operating. We also present a joint reliability and power management optimization problem whose solution is an optimal management policy. When careful joint policy optimization is performed, we obtain a significant improve-ment in energy consumption (40 %) in tandem with meeting a reliability constraint for all SoC operating temperatures. Index Terms—Optimal control, power consumption, reliability management. I...|$|E
50|$|While most RAID {{levels can}} provide good {{protection}} against and recovery from hardware defects or defective sectors/read <b>errors</b> (<b>hard</b> <b>errors),</b> {{they do not}} provide any protection against data loss due to catastrophic failures (fire, water) or soft errors such as user error, software malfunction, malware infection. For valuable data, RAID is only one building block of a larger data loss prevention and recovery scheme - it cannot replace a backup plan.|$|R
30|$|The rest of {{the article}} is {{organized}} as follows. “PRAM reliability” section describes the sources of soft and <b>hard</b> <b>errors</b> for 2 -bit MLC PRAM and proposes circuit-level techniques to reduce them. “STT-RAM reliability” section describes the causes of failures in STT-RAM and proposes circuit parameter tuning to address them. “ECC schemes” section focuses {{on the details of}} the ECC schemes for PRAM and STT-RAM with hardware overhead. Finally, the article concludes with some conclusions.|$|R
40|$|This paper {{explores the}} idea that {{redundant}} operations, like type errors, commonly flag correctness errors. We experimentally test this idea by writing and applying four redundancy checkers to the Linux operating system, finding many errors. We then use these errors to demonstrate that redundancies, even when harmless, strongly correlate {{with the presence of}} traditional <b>hard</b> <b>errors</b> (e. g., null pointer dereferences, unreleased locks). Finally we show that how flagging redundant operations gives a way to make specifications "fail stop " by detecting dangerous omissions...|$|R
