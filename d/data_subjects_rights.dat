0|10000|Public
50|$|Concerning the {{obligations}} and {{duties of the}} operator of a search engine, the Court held that {{in the present case}} Article 7(f) of the Directive, relating to legitimacy of processing, requires a balancing of the opposing rights and interests of the <b>data</b> <b>subject</b> (González) and the data controller (Google), taking into account the <b>data</b> <b>subject's</b> <b>rights</b> deriving from Articles 7 (respect for private and family life) and 8 (protection of personal data) of the Charter of Fundamental Rights of the European Union. Article 14(a) of the Directive, relating to the <b>data</b> <b>subject's</b> <b>rights,</b> allows the <b>data</b> <b>subject,</b> at least in the cases covered by Articles 7(e) and 7(f), to object at any time on compelling legitimate grounds relating to his particular situation to the processing of data relating to him, save where otherwise provided by national legislation. Article 12(b) of the Directive, relating to the <b>data</b> <b>subject's</b> <b>right</b> of access to the data, allows the <b>data</b> <b>subject</b> to request erasure of the data. Such request may be made directly of the controller, who must then duly examine the merits of the request. If the request is not granted, the <b>data</b> <b>subject</b> may then direct the request to a supervisory authority or the judicial authority so that it carries out the necessary checks and orders the controller to take specific measures accordingly.|$|R
40|$|Recent UK {{legislation}} {{facilitating the}} credit scoring {{of small and}} medium-sized enterprises using Big Data techniques and Open Data sources threatens to further hollow out information management norms and <b>data</b> <b>subject</b> <b>rights</b> enshrined in privacy and data protection law just as it is gathering unprecedented momentum in courts and on statute books across the EU. After examining the economic rationale for such measures and their envisaged impact on the credit risk industry, we argue that the associated regulatory re-shuffling and privacy-related safeguards are highly unlikely to address adequately the serious accuracy, transparency, and accountability concerns of individual <b>data</b> <b>subjects.</b> Would the effective, full enforcement of data protection principles and <b>data</b> <b>subject</b> <b>rights</b> really cripple the credit reference industry {{to the detriment of the}} nascent economic recovery, or is there a middle path and will the forthcoming EU General Data Protection Regulation provide it...|$|R
50|$|Every {{resident}} of Chile {{over the age}} of 18 must have and carry at all times their ID Card called Cédula de Identidad issued by the Civil Registry and Identification Service. It contains the full name, gender, nationality, date of birth, photograph of the <b>data</b> <b>subject,</b> <b>right</b> thumb print, ID number, and personal signature.|$|R
50|$|The {{decision}} confirms {{a so-called}} {{right to be}} forgotten mooted in the proposed General Data Protection Regulation, due to take effect in late 2014, although the Court did not explicitly grant such a right, depending instead on the <b>data</b> <b>subject's</b> <b>rights</b> deriving from Article 7 (respect for private and family life) and Article 8 (protection of personal data) of the Charter of Fundamental Rights of the European Union.|$|R
50|$|Scope of {{application}} of Russian Data Protection legislation: Russian laws apply when the operator uses his own or third-party {{data processing equipment}} located in Russia. As well as {{in cases where the}} data has been already transferred outside Russia, but there has been a violation of personal <b>data</b> <b>subject’s</b> <b>rights</b> prior to or during such transfer. If the data is transferred outside Russia duly, it will be subsequently regulated by the laws of country of destination and implications of Russian law will not apply thereto.|$|R
40|$|In April 2016, the European {{commission}} {{adopted a}} new General Data Protection Regulation. This regulation {{is based on}} fundamental principles of the existing legislation and elaborates it further into the set of rather detailed legal instructions. The suggested paper aims to summarize {{the parts of the}} new regulation that are most relevant for operators of gray literature repositories, who will have to adapt within the next two years. The paper will focus on the issues of consent to the processing of personal data, the legal regime of processing personal data without the consent of the <b>data</b> <b>subjects,</b> the <b>data</b> <b>subject’s</b> <b>right</b> to information as well as the legal regime of acquisition of personal data by third parties and exercise of the newly formulated right to be forgotten...|$|R
40|$|This master’s thesis {{analyzes}} {{the development of}} <b>data</b> <b>subject’s</b> consent as a legal basis due to the General Data Protection Regulation (GDPR). The research method of this master’s thesis is legal dogmatic. The provisions of the GDPR are analyzed against {{the provisions of the}} Data Protection Directive (DPD). The focus of the research is on defining whether the GDPR will change <b>data</b> <b>subject’s</b> consent as a legal basis for processing personal data in comparison to the situation under the DPD. The requirements for a valid consent provided for in the GDPR have previously to a quite large extent existed in the opinions of the Article 29 Working Party, the case law of the Court of Justice of the European Union and legal literature. Nevertheless, the GDPR changes <b>data</b> <b>subject’s</b> consent as a legal basis through at least two mechanisms. Firstly, {{the context in which the}} requirements for a valid consent are interpret has changed, inter alia, as the GDPR emphasizes protection of <b>data</b> <b>subject’s</b> <b>rights</b> and sets forth strong accountability and transparency principles. Secondly, the GDPR clarifies and makes legally binding more requirements for a valid consent in comparison to the DPD. For example, it explicitly forbids the use of opt-out based consents and provides <b>data</b> <b>subjects</b> with the <b>right</b> to withdraw consent at any time. Thus, the GDPR tightens the requirements for a valid consent and data controllers need to ensure that all consents they base their processing on meet these GDPR’s requirements...|$|R
40|$|We {{consider}} the likelihood ratio test for a changepoint or lag {{of the effect}} of some covariates, e. g. treatment, in regression modelling of survival <b>data</b> <b>subject</b> to <b>right</b> cen-soring. Asymptotic distributions under null hypotheses are derived. Numerical examples are used to illustrate the implementation of the procedure. Finally we apply the theory to data from a large randomised cancer prevention trial...|$|R
5000|$|The ruling {{balances}} {{the right to}} privacy and data protection in European law with the legitimate interest of the public to access such information, and it does not mandate that information is instantly removed upon request. It distinguishes between public figures and private persons. The Court stressed that Internet search engines profile individuals in modern society in an ubiquitous manner, in a way that could not otherwise have been obtained formerly save only with the greatest difficulty. The <b>data</b> <b>subject's</b> <b>rights</b> must therefore in general override [...] "as a rule, not only the economic interest of the operator of the search engine but also the interest of the general public in finding that information upon a search relating to the <b>data</b> <b>subject's</b> name", but that would not be the case if the role played by the <b>data</b> <b>subject</b> in public life is such [...] "that the interference with his fundamental rights is justified by the preponderant interest of the general public in having, on account of inclusion in the list of results, access to the information in question".|$|R
5000|$|The <b>data</b> <b>subject</b> has the <b>right</b> to be {{informed}} when his personal data is being processed. The controller must provide his name and address, the purpose of processing, the recipients of the data and all other information required to ensure the processing is fair. (art. 10 and 11) ...|$|R
40|$|Article 21 in the General Data Protection Regulation (GDPR) grants <b>data</b> <b>subjects</b> the <b>right</b> to object. Already {{present in}} Directive 95 / 46, {{the right to}} object has been {{fine-tuned}} and solidified. The legislator has also clarified the right’s murky relationship {{with the right to}} erasure. This blogpost aims first to tease out the key characteristics and legal innovations in the GDPR’s right to object, and secondly to elucidate how it interacts with the right to erasure in Article 17. status: publishe...|$|R
30|$|Since the era {{of paper}} and film, the {{obligations}} to keep records archived and accessible for a defined period of time have usually been defined, according to medical or medico-legal purposes, by national or local authorities. Over the past few years, storage of large digital data such as images on PACS has been greatly facilitated by the price decay of storage capacity. The adequate storage period for digital data on PACS and EPR depends first of all on {{the potential benefits of}} this information for the individual’s personal health and may be 10  years or longer, e.g. in the context of chronic conditions originating before or during childhood, or neoplastic conditions with a propensity for late recurrence. As long as confidentiality and data safety are guaranteed and access rules are defined with regard to the purpose of individual healthcare, {{there appears to be no}} obvious reason and no requirement to erase data such as medical images routinely after defined periods without the <b>data</b> <b>subject’s</b> request. Interpretations may differ with regard to derogations from the <b>data</b> <b>subject’s</b> <b>right</b> to erasure upon his/her request in the context of health data (Articles 9, 17 and 89) [2, 3]. Specific requirements in this context may still have to be clarified by national legislation of member states [3].|$|R
40|$|We {{develop a}} unified {{approach}} for classification and regression support vector machines for <b>data</b> <b>subject</b> to <b>right</b> censoring. We provide finite sample bounds on the generalization error of the algorithm, prove risk consistency {{for a wide}} class of probability measures, and study the associated learning rates. We apply the general methodology to estimation of the (truncated) mean, median, quantiles, and for classification problems. We present a simulation study that demonstrates {{the performance of the}} proposed approach. Comment: In this version, we strengthened the theoretical results and corrected a few mistake...|$|R
5000|$|... {{processing}} {{is necessary}} {{for the purposes of the}} legitimate interests pursued by the controller or by the third party or parties to whom the data are disclosed, except where such interests are overridden by the interests for fundamental rights and freedoms of the <b>data</b> <b>subject.</b> The <b>data</b> <b>subject</b> has the <b>right</b> to access all data processed about him. The <b>data</b> <b>subject</b> even has the right to demand the rectification, deletion or blocking of data that is incomplete, inaccurate or not being processed in compliance with the data protection rules. (art. 12) ...|$|R
5000|$|In May 2016, South Korea’s Korea Communications Commission (KCC) {{announced}} {{citizens will}} be able to request search engines and website administrators to restrict their own postings from being publicly accessible. The KCC released “Guidelines on the Right to Request Access Restrictions on Personal Internet Postings”, which will take effect in June 2016, which will not apply to third party contents. [...] To the extent that the right to be forgotten concerns a <b>data</b> <b>subject's</b> <b>right</b> to limit the searchability of third party postings about him/her, the Guideline does not constitute a right to be forgotten. [...] Also, as to the right to withdraw one's own posting, critics have noted that people have been able to delete their own postings before the Guideline {{as long as they have}} retained their login credentials, and that people who have misplaced their login credentials were permitted to retrieve or receive new ones. [...] The only services significantly affected by the Guideline are Wiki-type services where people's contributions make logical sense only in response to or in conjunction with one another's contributions and therefore the postings are made permanent part of the mass-created content, but KCC made sure that the Guideline applies to these services only when the posting identifies the authors.|$|R
40|$|Transparency is an {{integral}} part of European data protection. In particular, the right of access allows the <b>data</b> <b>subject</b> to verify if his personal data is processed in a lawful manner. The data controller has the full obligation to provide all information on personal data processing in an easily accessible way. Privacy dashboards are promising tools for this purpose. However, there is not yet any privacy dashboard available which allows full access to all personal data. Particularly, information flows remain unclear. We present the next generation privacy dashboard PrivacyInsight. It provides full access to all personal data along information flows. Additionally, it allows exercising the <b>data</b> <b>subject’s</b> further <b>rights.</b> We evaluate PrivacyInsight in comparison with existing approaches by means of a user study. Our results show that PrivacyInsight is the most usable and most feature complete existing privacy dashboard...|$|R
40|$|At time of writing, {{the policy}} of DNA profile {{retention}} for Constabularies within England and Wales {{is determined by the}} Association of Chief Police Officer’s (ACPO) 2006 Retention Guidelines for Nominal Records on the Police National Computer (PNC), which was developed following the passing of the Criminal and Police Act 2001 and the Criminal Justice Act 2003. The former of these legislations ended the requirement for Constabularies to destroy DNA records relating to persons acquitted or who had their case discontinued, whilst the latter extended powers so as to permit the taking of DNA records without consent from any individual arrested for a recordable offence. These Retention Guidelines detail a governing principle that all records held on the PNC should be maintained until the person in question reaches 100 years of age, regardless of status of conviction, caution, acquittal, or No Further Action (NFA). As such, by 2010 there were over 5 million persons with profiles on the UK National DNA database, with approximately 1 million of these having no record of conviction or caution. This policy was challenged, first unsuccessfully through the UK judicial system, before success-fully being appealed in the European Court of Human Rights (ECtHR) in the case of S and Marper v. UK (2008) ECHR 1581, where it was found to violate the <b>data</b> <b>subject’s</b> <b>rights</b> under Article 8 o...|$|R
40|$|Since {{survival}} data occur over time, often important covariates that {{we wish to}} consider also change over time. Such covariates are referred as time-dependent covariates. Quantile regression offers flex-ible modeling of {{survival data}} by allowing the covariates to vary with quantiles. This paper provides a novel quantile regression model ac-commodating time-dependent covariates, for analyzing survival <b>data</b> <b>subject</b> to <b>right</b> censoring. Our simple estimation technique assumes the existence of instrumental variables. In addition, we present a doubly-robust estimator {{in the sense of}} Robins & Rotnitzky (1992). The asymptotic properties of the estimators are rigorously studied. Finite-sample properties are demonstrated by a simulation study. The utility of the proposed methodology is demonstrated using the Stan-ford heart transplant dataset. 1. Introduction. Quantil...|$|R
30|$|Key {{elements}} of the new regulation include, for example: the need for clear and affirmative consent by the <b>data</b> <b>subject</b> concerned, destruction of data if storage is no longer necessary for the initial purpose or after withdrawal of consent by the <b>data</b> <b>subject</b> (‘the <b>right</b> to be forgotten’); the right to obtain rectification of his/her personal data; the <b>right</b> of the <b>data</b> <b>subject</b> to transfer personal data to another service provider (‘data portability’); the <b>right</b> of the <b>data</b> <b>subject</b> to be informed when his/her data have been hacked. The new GDPR implies that all organisations processing personal data {{must be able to}} prove that they comply with the rules. It also proposes a European Data Protection Board and requires institutions which process certain types or volumes of data to have a designated data protection officer (DPO), who is in contact with the national data protection authorities. The legislation also specifies that ‘effective, proportionate and dissuasive’ penalties or financial fines may apply in the case of non-compliance with the data protection rules.|$|R
50|$|A {{right to}} be {{forgotten}} was replaced by a more limited right to erasure in the version of the GDPR adopted by the European Parliament in March 2014. Article 17 provides that the <b>data</b> <b>subject</b> has the <b>right</b> to request erasure of personal data related to them on any {{one of a number of}} grounds including non-compliance with article 6.1 (lawfulness) that includes a case (f) where the legitimate interests of the controller is overridden by the interests or fundamental rights and freedoms of the <b>data</b> <b>subject</b> which require protection of personal data (see also Google Spain SL, Google Inc. v Agencia Española de Protección de Datos, Mario Costeja González).|$|R
40|$|May {{a company}} {{photograph}} {{the daily lives}} of people all over the world, store those photos, and publish them on the internet? This article assesses which obligations Google has to fulfil in order to respect the European data protection rules. The focus lies on three questions. First, which data processed for the Street View service are personal data? Second, does Google have a legitimate ground for processing personal data? Third, does Google comply with its transparency obligations and does it respect the <b>rights</b> of the <b>data</b> <b>subjects,</b> specifically their <b>right</b> to information...|$|R
5000|$|The European Parliament {{was once}} [...] "expected {{to adopt the}} {{proposals}} in first reading in the April 2013 Plenary session". The {{right to be forgotten}} was replaced by a more limited right to erasure in the version of the GDPR adopted by the European Parliament in March 2014. Article 17 provides that the <b>data</b> <b>subject</b> has the <b>right</b> to request erasure of personal data related to him on any {{one of a number of}} grounds including non-compliance with article 6.1 (lawfulness) that includes a case (f) where the legitimate interests of the controller is overridden by the interests or fundamental rights and freedoms of the <b>data</b> <b>subject</b> which require protection of personal data (see also Costeja).|$|R
5000|$|The <b>data</b> <b>subject</b> {{should have}} the right not to be subject to a decision, which may include a measure, {{evaluating}} personal aspects relating to him or her which is based solely on automated processing and which produces legal effects concerning him or her or similarly significantly affects him or her, such as automatic refusal of an online credit application or e-recruiting practices without any human intervention....In any case, such processing should be subject to suitable safeguards, which should include specific information to the <b>data</b> <b>subject</b> and the <b>right</b> to obtain human intervention, to express his or her point of view, to obtain an explanation of the decision reached after such assessment and to challenge the decision.|$|R
40|$|Parametric {{estimation}} of cause-specific hazard functions in a competing risks model is considered. An approximate likelihood procedure for estimating parameters of cause-specific hazard functions based on competing risks <b>data</b> <b>subject</b> to <b>right</b> censoring is proposed. In an assumed parametric model {{that may have}} been misspecified, an estimator of a parameter is said to be consistent if it converges in probability to the pseudo-true value of the parameter as the sample size becomes large. Under censorship, the ordinary maximum likelihood method does not necessarily give consistent estimators. The proposed approximate likelihood procedure is consistent even if the parametric model is misspecified. An asymptotic distribution of the approximate maximum likelihood estimator is obtained, and the efficiency of the estimator is discussed. Datasets from a simulation experiment, an electrical appliance test and a pneumatic tire test are used to illustrate the procedure. Aalen-Johansen estimator, cause-specific cumulative incidence function, Censored data, Kaplan-Meier estimator,...|$|R
40|$|As a {{generalization}} of the accelerated failure time models, we consider parametric models of lifetime Y, where the conditional mean E(Y|X;beta) can depend nonlinearly on the covariates X and some parameters beta. The error distribution can be heteroscedastic and dependent on X. With observed <b>data</b> <b>subject</b> to <b>right</b> censoring, we propose regression analysis for beta based on Kaplan-Meier {{estimates of the}} means over several regions of X. Consistency and asymptotic distributional properties of the estimators are established under general conditions. A resulting estimator of beta is shown to be the sum of two possibly dependent asymptotic normal quantities, based on which conservative confidence intervals and tests are derived. Simulation studies are conducted to investigate {{the performance of the}} proposed estimator and to compare it with Buckley-Jame's method. To illustrate the methodology, we study an example with kidney transplant data, where a nonlinear relationship called "mixtures-of-experts", proposed in the neural networks literature, is used to model the relationship between the survival time and the age of the patients. ...|$|R
40|$|This article {{discusses}} {{the provisions of}} the Irish Data Protection (Amendment) Act 2003 which implements Council Directive 95 / 46, including: (1) how data protection rights may be enforced in the workplace; (2) duties of employers in the processing of employee data, including sensitive data; (3) limits on the length of time data may be held; (4) security measures for the transmission of data over a computer network; (5) the prohibition of decision making relating to employees {{solely on the basis of}} automated analysis of their working practices; (6) employees' rights of access to their personal data; (7) amendments to the requirement that prospective employees who would work with children exercise their data protection rights of access to their police files to prove that they have not been guilty of an offence under the Sex Offenders Act 2001; and (8) the exemption of job references from the <b>data</b> <b>subject's</b> general <b>right</b> of access to his personal data...|$|R
5000|$|Under the Act, a <b>data</b> <b>subject</b> has the <b>right</b> to {{have his}} {{personal}} data corrected (section 33), to access his personal data (section 35); to prevent the {{processing of personal data}} that causes or is likely to cause unwarranted damage or distress to him (section 39); to prevent processing of personal data for purposes of direct marketing (section 40); to require a data controller not to take a decision that would significantly affect him solely on the processing by automatic means (section 41); to exempt manual data (Section 42), to be compensated for the data controller’s failure to comply with the provisions of the Act, upon proof of damages (Section 43); and to have inaccurate data rectified (Section 44) ...|$|R
40|$|Privacy {{has become}} an {{important}} component in systems that handle personal information. <b>Data</b> <b>subjects</b> have a <b>right</b> {{to control their own}} personal information. Hippocratic Databases represent a new era of database technology that has been proposed to fulfill personal information privacy protection requirements at the database level. However, the concept of “purpose” is introduced because the owner of the information has no control over his/her own personal information. The owner, also referred to as the <b>data</b> <b>subject,</b> not only needs to control to whom his/her information should be disclosed but should also be able to access his/her own personal information regardless of the purpose. For this reason, we concluded that there are two requirements when managing personal information: The owner should be able to control his/her own personal information and should be able to access it at any time and for any purpose. This Study introduces a new architecture that fulfills the above requirements, which we refer to as the owner-controlled architecture for Hippocratic Databases. First, we highlight the importance of controlling personal information in an information flow model, and then we explain the architecture that supports the proposed model...|$|R
40|$|The {{decisions}} {{of everyday life}} are to an increasing extent made by a new deciding force: the proprietary algorithm. The information society of today is a “world of automatic decision-making”, {{as more and more}} decisions are delegated to automatic systems which are able to process data and make decisions, often with little supervision from human decision-makers. Such automated decision-making presents an interesting dichotomy of interests: the companies that develop the algorithms and data processing mechanisms have an interest in keeping them protected and hidden from competitors, while individuals {{have a vested interest in}} getting insight into, and understanding of, how decisions based on their personal data are being made. This thesis sets out to analyze this conflict, looking to answer how data protection rights to a transparent processing of personal data can be reconciled with trade secret rights in automated decision-making. The topic is studied from an EU point of view with legal dogmatism as the methodological base, and the main research question is divided into three parts corresponding with the chapters of the thesis. Firstly, the data protection rights to a transparent processing of personal data are as regards automated decision-making are reviewed. The main rights relating to the transparent processing of personal data are found in the notification obligations, access rights and safeguards relating to automated decision-making of the upcoming General Data Protection Regulation, and the rights are also collectively referred to as the “right to explanation”. The analysis in the chapter shows multiple ambiguities and gaps, leaving the scope of the rights unclear and to be further clarified in case law and through guidance by data protection authorities. The multiple requirements and the ambiguous nature of the right to an explanation results in that it allows both <b>data</b> <b>subjects</b> and trade secret holders to read the right in a favorable way to their own interests, giving rise to a significant legal uncertainty. Secondly, the trade secret rights behind automated decision-making are analyzed. Trade secret law offers a wide protection for various types of data processing algorithms and methods a private company may use in the automated processing of personal data. As regards the protection of such information in the light of <b>data</b> <b>subject</b> transparency <b>rights,</b> the most crucial interest of trade secret holders is to ensure that sufficient steps are taken to keep the information secret. This interest is jeopardized by the obligations on companies to meet the requirements of <b>data</b> <b>subject</b> transparency <b>rights,</b> since a disclosure to a large amount of <b>data</b> <b>subjects</b> could render the information generally known, losing its trade secret status. While trade secret rights may function as one of companies’ main ways of limiting the applicability of the right to explanation, the General Data Protection Regulation also makes clear that trade secret rights cannot be the base for refusing <b>data</b> <b>subjects</b> all information. Thus, data controllers must walk a fine line between meeting their data protection obligations while simultaneously keeping their data processing methods secret. Lastly, a balancing exercise is conducted in order to analyze how the conflicting rights in automated decision-making can be balanced and reconciled. The analysis of the balancing norms of the Trade Secrets Directive and the General Data Protection Regulation shows a contradictory situation that affirms both a precedence of trade secrets on data protection rights, as well as a precedence of data protection rights on trade secrets. Ultimately, a slight preference for data protection rights is seen. However, national legislation and case law on the previous right of access under the Data Protection Directive shows another reality where trade secret rights have a strong restricting factor on any transparency <b>rights</b> of <b>data</b> <b>subjects.</b> It remains to be seen whether the reinforced rights of the General Data Protection Regulation might embolden national legislators and courts to implement a more transparency-friendly approach in which trade secrets are given less protection when compared to the right to an explanation. Until then, the possibilities of the right to an explanation to enhance transparency in automated decision-making must be viewed with certain disbelief, due to the ambiguity of the right itself and the strong restricting nature of trade secret rights. As a consequence, solutions for reconciliation must be found elsewhere, and the thesis concludes by presenting three alternative approaches to reconciling the rights in order to enable a functioning transparency in automated decision-making...|$|R
40|$|Quality-adjusted {{lifetime}} (QAL) {{has been}} playing {{an important role in}} clinical trials to evaluate treatments for chronic diseases. Much research has been focused on estimating the mean QAL when the <b>data</b> are <b>subject</b> to <b>right</b> censoring. The estimated mean QAL is often presented with a confidence interval, obtained by using either the available variance estimator or the bootstrapping method. However, no research has shown which method yields confidence intervals with better coverage probability and shorter length, especially when the sample size is not big. In this paper, we examine the confidence intervals obtained by some available methods for the mean QAL with censored data. Simulation studies are employed to compare the performance of these methods with various sample sizes and different censoring rates. Methods with the best performance will be identified. A data example from a breast cancer clinical trial study is used to illustrate the application of these methods. ...|$|R
40|$|In recent years, various {{national}} medical databases {{have been}} set up in the EU from disparate local databases and file systems. Medical records contain personal data and are as such protected by EU and member states' legislation. Medical data, in addition to being personal data, is also defined in the EU legislation as being especially sensitive and warrants special connection with these processes. Such issues relate to the merits of compiling a nationwide database, deciding on who has access to such a database, legitimate uses of medical data held, protection of medical <b>data,</b> and <b>subject</b> access <b>rights</b> amongst others. This paper examines some of these issues and argues that such databases are inevitable due to technological change; however there are major legal and information security caveats that have to be addressed. Many of these caveats have not yet been resolved satisfactorily, hence making medical databases that already exist problematic...|$|R
40|$|In {{clinical}} trials of chronic {{diseases such as}} AIDS, cancer or cardiovascular diseases, the concept of quality-adjusted lifetime (QAL) has received more and more attention. In this paper we consider {{the problem of how}} the covariates affect the mean QAL when the <b>data</b> are <b>subject</b> to <b>right</b> censoring. We allow a very general form for the mean model as a function of covariates. Using the idea of inverse probability weighting, we first construct a simple weighted estimating equation for the parameters in our mean model. We then find the form of the most efficient estimating equation, which yields the most efficient estimator for the regression parameters. Since the most efficient estimator depends on the distribution of the health history processes, and thus cannot be estimated non-parametrically, we consider different approaches for improving the efficiency of the simple weighted estimating equation using observed data. The applicability...|$|R
40|$|Digitalization {{processes}} have profoundly {{changed the}} news ecology. As a consequence, {{the very definition}} of news itself is under pressure. Whilst tradi‑ tional news production typically revolves around news values in determining what is newsworthy, datafication principles are increasingly being used to determine what is newsworthy and what news offerings should consist of (Hammond, 2015). This impacts the news ecology on different levels: decisions on the editorial floor about what content needs to be produced is increasingly based on what generates ‘engagement’ (c. g., internet traffic) (Lee, Lewis & Powers, 2014), while various news outlets experiment with algorithms to offer personalized news (Carlson, 2015). To facilitate the filtering of news and in‑ formation, news recommender systems have been developed. They are powerful and popular tools for audiences to cope with the information overload and assist in decision-making processes, based on the user’s news preferences. As such, these systems are clear examples of the algorithmic culture at work in the big data era. The increasing importance of algorithms and datafication brings about new opportunities, e. g. offering a customized news experience and facilitating innovative journalistic practices, but might also entail less positive consequences. As such, the hyper-personalized news selection may endanger the basic function of news, since it may result in a ‘filter bubble’(Pariser, 2011), a world created by the shift from ‘human gatekeepers’to ‘algorithmic gate‑ keepers’employed by Facebook and Google, which present the content they believe a user is most likely to click. Against this background, this paper aims to conceptually explore an innovative and societally relevant use of algorithmic power, i. e. ‘public service algorithms’that make recommendations that help in opening our horizons and offer something‘new and different’. First, the concept of‘news diversity’will be analyzed both from a communication science and legal perspective (e. g. source diversity versus exposure diversity; Burri, 2015, Helberger 2015). In a second step, based on the foregoing analysis {{on the one hand and}} a literature study on the functioning of algorithms on the other hand, the paper will identify and examine essential principles that a public service algorithm must embody (e. g. transparency, user control and <b>data</b> <b>subject</b> <b>rights,</b> accountability). REFERENCES Burri, M. (2015). Contemplating a 'Public Service Navigator': In Search of New (and Better) Functioning Public Service Media. International Journal of Communication, 9, 1341 – 1359. Carlson, M. (2015). The robotic reporter. Automated journalism and the redefinition of labor, compositional forms, and journalistic authority. Digital Journalism, 3 (3), 416 – 431. Hammond, P. (2015). From computer-assisted to data-driven: Journalism and big data. Journalism, doi: 10. 1177 / 1464884915620205 Helberger, N. (2015), “Merely Facilitating or Actively Stimulating Diverse Media Choices? Public Service Media at the Crossroad”, International Journal of Communica‑ tion, 9, 1324 – 1340. Lee, A. M., Lewis, S. C. & Powers, M. (2014). Audience clicks and news placement: A study of time-lagged influence in online journalism. Communication Research, 41 (4), 505 – 530. Pariser, E. (2011). The filter bubble: What the Internet is hiding from you. Penguin UK...|$|R
40|$|This paper {{considers}} the shape that a “right to be forgotten” is {{taking in the}} online world, {{in the aftermath of}} the Google Spain decision, in which the Court of Justice of the European Union found (against Google) that European <b>data</b> <b>subjects</b> had the <b>right</b> to request that search engines de-index webpages that feature in searches on their names. The judgment, and Google’s response, raises a series of questions that are addressed in this paper. In particular, the judgment affects the nature of the balance between free speech and privacy on the Internet. Google’s presentation of its search as a neutral reflection of the state of the Web (and for that reason, a valuable resource for Web users) was found wanting by the court, and indeed Google itself has often adjusted its PageRank algorithm to improve its output by excluding, for example, spam, link farms and child pornography. Such methods cannot be transparent, since they would then be gamed by the spammers, and so Google has to present as a corporate “black box. ” Yet it is a big step to devolve issues of privacy and freedom to an opaque process — even if it is accepted that a private sector actor can legitimately make decisions in this area. The final section of the paper considers whether individuals might manage their personal data with flexible architectures that could act as points of contact for those wishing to use the data. In such a technological ecosystem, many issues could be addressed within a system that respected the autonomy of the <b>data</b> <b>subject</b> in providing limited abilities to control self-presentation. However, this remains a thought experiment at this stage — such technologies, though technologically feasible, are not yet the subject of great demand or takeup from consumers, while the state of current regulation means that business models favour sidelining <b>data</b> <b>subjects</b> from decisions made about the use of their data...|$|R
40|$|Health and {{wellbeing}} apps are proliferating at an exponential rate. It {{has created}} a billion dollar market {{that would make the}} industry seek every opportunity to take a competitive advantage. At times, this may be {{at the expense of the}} privacy of the app users. While most such apps do not collect sensitive medical data, some may collect or generate sensitive data during processing thus creating a high degree of risk towards users privacy. However, the current EU laws seem to be inadequate in protecting the privacy expectations of the users of such apps especially in the light of technologies such as cloud computing, big data and profiling. Meanwhile the EU and the National regulators seem to be facing a dilemma of harmonizing economic and wider societal benefits of personal and sensitive data processing and the <b>data</b> <b>subject</b> s <b>right</b> to privacy. This thesis postulates that privacy risk assessment is one strategy to harmonize these interests and ensure privacy of the health and wellbeing app users. Thus, it embarked first on enumerating the contribution to privacy risks by the health and wellbeing apps before recognizing the current state of privacy risk assessments within the EU context. It then recognized means of harmonizing the EU laws, industry interests and the privacy expectation of the app users. Through its analysis, the thesis proposes a scalable and a transparent privacy risk assessment obligation on the app developers and data controllers as a solution. However, in order to implement such an obligation, the EU laws ought to provide appropriate methodologies to ease the legal uncertainties as recommended through this thesis. At the same time, the National laws ought to provide standards for privacy risk assessments based on reasonable expectations of the app users, principles of proportionality, reasonable terms and qualitative parameters of privacy rights, supplementing the EU laws...|$|R
2500|$|Special {{personal}} data may, however, be processed {{where it is}} necessary or the <b>data</b> <b>subject</b> has given consent to the processing (Section 37(2)). Processing of {{personal data}} is necessary where it is to exercise a right, or fulfil an obligation conferred or imposed by law on an employer (Section 37(3)). Special personal data relating to <b>data</b> <b>subjects</b> may also be processed where {{it is necessary for}} the protection of the vital interest of the <b>data</b> <b>subject,</b> where it is impossible for the <b>data</b> <b>subject</b> to give consent, or the data controller cannot reasonably be expected to obtain consent, or consent by the <b>data</b> <b>subject</b> has been unreasonably withheld. (Section 37(4)) ...|$|R
