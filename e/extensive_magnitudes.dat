8|21|Public
5000|$|... § 24. The first {{physical}} {{principle of}} pure understanding subsumes all {{spatial and temporal}} phenomenal appearances under the concept of quantity. All appearances are <b>extensive</b> <b>magnitudes.</b> It is {{the principle of the}} axioms of intuition.|$|E
50|$|Taken in its immediacy, a Number is an Extensive Magnitude, that is, a {{collection}} of a certain Amount of self-same Units. These Units, say ten or twenty of them, are the sublated moments of the <b>Extensive</b> <b>Magnitudes</b> ten or twenty. However, the Number ten or twenty, though made up of Many, is also a self-determining One, independent of other Numbers for its determination. Taken in this way, ten or twenty (a) differentiates itself from Extensive Magnitude and becomes an Intensive Magnitude, which is expressed as the tenth or twentieth Degree. Just as the One was completely indifferent to the other Ones of the Many yet depended on them for its existence, each Degree is indifferent to every other Degree, yet they are externally related {{to one another in}} ascending or descending flow through a scale of Degrees.|$|E
40|$|The {{philosophy}} of mathematics {{has been accused}} of paying insufficient attention to mathematical practice: one way to cope with the problem, the one we will follow in this paper on <b>extensive</b> <b>magnitudes,</b> is to combine the ‘history of ideas ’ and the ‘{{philosophy of}} models ’ in a logical and epistemologica...|$|E
5000|$|... § 26. The {{table of}} the Universal Principles of Natural Science is perfect and complete. Its {{principles}} are limited only to possible experience. The principle of the axioms of intuition states that appearances {{in space and time}} are thought of as quantitative, having <b>extensive</b> <b>magnitude.</b> The principle of the anticipations of perception states that an appearance's sensed reality has degree, or intensive magnitude. The principles of the analogies of experience state that perceptual appearances, not things in themselves, are thought of as experienced objects, in accordance with a priori rules of the understanding.|$|R
40|$|Based on an <b>extensive</b> <b>magnitude</b> {{estimation}} experiment, a new color appearance {{model for}} unrelated self-luminous stimuli, CAM 15 u, has been designed. With the spectral radiance {{of the stimulus}} as unique input, the model predicts the brightness, hue, colorfulness, saturation and amount of white. The main features of the model are {{the use of the}} CIE 2006 cone fundamentals, the inclusion of an absolute brightness scale and a very simple calculation procedure. The CAM 15 u model performs much better than existing models and has been validated by a validation experiment. The model is applicable to unrelated self-luminous stimuli with an angular extent of 10 ° and a photopic, but non-glare-inducing, luminance level. status: publishe...|$|R
50|$|Although thus {{differentiated}} {{from each}} other, <b>Extensive</b> and Intensive <b>magnitude</b> are essentially (b) the same. “They are only distinguished {{by the one}} having amount within itself and the other having amount outside itself.” It {{is at this point}} that the moment of the Something reasserts itself having remained implicit {{over the course of the}} development of Quantity. This Something, which reappears when the negation between <b>Extensive</b> and Intensive <b>Magnitude</b> is itself negated, is the re-emergence of Quality within the dialectic of Quantity.|$|R
40|$|The {{philosophy}} of mathematics {{has been accused}} of paying insufficient attention to mathematical practice: one way to cope with the problem, the one we will follow in this paper on <b>extensive</b> <b>magnitudes,</b> is to combine the `history of ideas' and the `{{philosophy of}} models' in a logical and epistemological perspective. The history of ideas allows the reconstruction of the theory of <b>extensive</b> <b>magnitudes</b> as a theory of ordered algebraic structures; the philosophy of models allows an investigation into the way epistemology might affect relevant mathematical notions. The article takes two historical examples {{as a starting point for}} the investigation of the role of numerical models in the construction of a system of non-Archimedean magnitudes. A brief exposition of the theories developed by Giuseppe Veronese and by Rodolfo Bettazzi at the end of the 19 th century will throw new light on the role played by magnitudes and numbers in the development of the concept of a non-Archimedean order. Different ways of introducing non-Archimedean models will be compared and the influence of epistemological models will be evaluated. Particular attention will be devoted to the comparison between the models that oriented Veronese's and Bettazzi's works and the mathematical theories they developed, but also to the analysis of the way epistemological beliefs affected the concepts of continuity and measurement...|$|E
40|$|Monte Carlo {{simulations}} {{of a model}} for gamma-Fe 2 O 3 (maghemite) single particle of spherical shape are presented aiming at the elucidation of the specific {{role played by the}} finite size and the surface on the anomalous magnetic behavior observed in small particle systems at low temperature. The influence of the finite-size effects on the equilibrium properties of <b>extensive</b> <b>magnitudes,</b> field coolings, and hysteresis loops is studied and compared to the results for periodic boundaries. It is shown that for the smallest sizes the thermal demagnetization of the surface completely dominates the magnetization while the behavior of the core {{is similar to that of}} the periodic boundary case, independently of D. The change in shape of the hysteresis loops with D demonstrates that the reversal mode is strongly influenced by the presence of broken links and disorder at the surfac...|$|E
40|$|Abstract. In {{order to}} produce high ball release speeds, fast bowlers in cricket require high run-up speeds, {{generate}} large ground reaction forces, and produce high joint torques. <b>Extensive</b> <b>magnitudes</b> of external loading in combination with kinematic factors such as counter-rotation experienced during fast bowling {{have been associated with}} a high incidence of lower lumbar injury. A full mechanical analysis of the technique is lacking such that attempts to modify techniques remain on a trial and error basis. The {{purpose of this study was}} to develop a forward solution model to predict the causal factors associated with counter-rotation of fast bowlers. It was shown that a reduction of the shoulder-hip torque differential lead to a minimisation of counter-rotation. This approach potentially gives the cricket coach a scientific means of modifying the technique of high-risk action bowlers to reduce their susceptibility to lower lumbar injury...|$|E
40|$|This paper {{tracks the}} {{systematic}} dialectical determination of mathematical concepts in Hegel’s Encyclopädie der philosophischen Wissenschaften (1830, 1817) and investigates the insights {{that can be}} gained from such a perspective on the mathematical. To begin with, the determination of Numbers and arithmetical operations from Being shows that the One and the successor function have a qualitative base and need not be presupposed. It is also shown that even for infinite Intensive Magnitudes (cardinals) there exists an <b>Extensive</b> <b>Magnitude</b> through which they gain meaning. This makes the ‘bad’ in Hegel’s ‘bad infinity’ a trifle problematic. Finally, if ‘Dasein’ is interpreted as the whole of perception in the present, Place {{can be viewed as}} the spatial Now, Motion as the passage from Place to Now and Matter as the actual (as opposed to observed) Presence of the natural realm...|$|R
40|$|This essay {{focuses on}} the brief but {{significant}} passage of Physics IV 13, 222 a 28 -b 7, where Aristotle provides two {{arguments in favor of}} the extensive infinitude of time. The first one, Vigo argues, presents its extensive infinitude as depending on infinitude of movement. By contrast, the second argument proceeds immanently out of the consideration of the properties the ‘now’ possesses as being a limit that accounts for both the possibility of limitation (divisibility) and the continuity of time. The paper also explores some systematic consequences of this last argument and attempts to figure out some puzzles it involves from the methodological point of view. Such difficulties underline some structural limits of Aristotle’s attempt to perform a re(con) ductive treatment of the properties of time regarded as a way of continuum dependent of other two more basic domains: movement and spatially <b>extensive</b> <b>magnitude...</b>|$|R
50|$|German {{forces in}} Finnish {{territory}} launched an offensive {{against the city}} in 1941 as part of Operation Silver Fox. Murmansk suffered <b>extensive</b> destruction, the <b>magnitude</b> of which was rivaled only by the destruction of Leningrad and Stalingrad. However, fierce Soviet resistance and harsh local weather conditions with the bad terrain prevented the Germans from capturing the city and cutting off the vital Karelian railway line and the ice-free harbor.|$|R
40|$|In a comment, {{hitherto}} unremarked upon, Alfred Binet, {{well known}} for constructing the first intelligence scale, claimed that his scale did not measure intelligence, but only enabled classification {{with respect to a}} hierarchy of intellectual qualities. Attempting to understand the reasoning behind this comment leads to an historical excursion, beginning with the ancient mathematician, Euclid and ending with the modern French philosopher, Henri Bergson. As Euclid explained (Heath, 1908), magnitudes constituting a given quantitative attribute are all of the same kind (i. e., homogeneous), but his criterion covered only <b>extensive</b> <b>magnitudes.</b> Duns Scotus (Cross, 1998) included intensive magnitudes by considering differences, which raised the possibility (later considered by Kant (Sutherland, 2004)) of ordered attributes with heterogeneous differences between degrees (heterogeneous orders). Of necessity, such attributes are non-measurable. Subsequently, this became a basis for the quantity objection to psychological measurement, as developed first by Tannery (1875) and then by Bergson (1889). It follows that for attributes investigated in science, there are three structural possibilities: (1) classificatory attributes (with heterogeneous differences between categories); (2) heterogeneous orders (with heterogeneous differences between degrees); and (3) quantitative attributes (with thoroughly homogeneous differences between magnitudes). Measurement is possible only with attributes of kind (3) and, as far as we know, psychological attributes are exclusively of kinds (1) or (2). However, contrary to the known facts, psychometricians, for their own special reasons insist that test scores provide measurements...|$|E
40|$|Calcolo Geometrico, G. Peano's first {{publication}} in mathematical logic, {{is a model}} of expository writing, with a significant impact on 20 th century mathematics. Kannenberg's lucid and crisp translation, Geometric Calculus, will appeal to historians of mathematics, researchers, graduate students, and general readers interested in the foundations of mathematics {{and the development of a}} formal logical language. In Chapter IX, with the innocent-sounding title "Transformations of a linear system," one finds the crown jewel of the book: Peano's axiom system for a vector space, the first-ever presentation of a set of such axioms. The very wording of the axioms (which Peano calls "definitions") has a remarkably modern ring, almost like a modern introduction to linear algebra. Peano also presents the basic calculus of set operation, introducing the notation for 'intersection,' 'union,' and 'element of,' many years before it was accepted. Despite its uniqueness, Calcolo Geometrico has been strangely neglected by historians of mathematics, and even by scholars of Peano. The book has never been reprinted in its entirety, and only two chapters have ever been translated into English. In part, this neglect has been due to Peano's organization of the work. That is, the section on mathematical logic bears almost no relation to the rest of the book, and the material there was superseded only a year after its publication by Peano's second book. Since all but this first section was generally thought to be expository rather than original work, it was regarded lightly, if noticed at all, and ultimately all but forgotten. Only in very recent years have the book's unique merits begun to be recognized. Among these merits are Peano’s presentation of the essential features of Grassmann’s notoriously obscure Ausdehnungslehre, a clarification and improvement upon Grassmann’s theory of <b>extensive</b> <b>magnitudes,</b> and a dissemination of other hard-to-understand material. Readers of this valuable translation will gain insight into the work of a distinguished mathematician and founder of mathematical logic...|$|E
40|$|We {{propose a}} simple, linear-combination {{automatic}} evaluation measure (AEM) to approximate post-editing (PE) effort. Effort is measured both as PE time {{and as the}} number of PE operations performed. The ultimate goal is to define an AEM {{that can be used to}} optimize machine translation (MT) systems to minimize PE effort, but without having to perform unfeasible repeated PE during optimization. As PE effort is expected to be an <b>extensive</b> <b>magnitude</b> (i. e., one growing linearly with the sentence length and which may be simply added to represent the effort for a set of sentences), we use a linear combination of extensive and pseudo-extensive features. One such pseudo-extensive feature, 1 –BLEU times the length of the reference, proves to be almost as good a predictor of PE effort as the best combination of extensive features. Surprisingly, effort predictors computed using independently obtained reference translations perform reasonably close to those using actual post-edited references. In the early stage of this research and given the inherent complexity of carrying out experiments with professional post-editors, we decided to carry out an automatic evaluation of the AEMs proposed rather than a manual evaluation to measure the effort needed to post-edit the output of an MT system tuned on these AEMs. The results obtained seem to support current tuning practice using BLEU, yet pointing at some limitations. Apart from this intrinsic evaluation, an extrinsic evaluation was also carried out in which the AEMs proposed were used to build synthetic training corpora for MT quality estimation, with results comparable to those obtained when training with measured PE efforts. Work supported by the Spanish government through project EFFORTUNE (TIN 2015 - 69632 -R) and through grant PRX 16 / 00043 for Mikel L. Forcada, and by the European Commission through QT 21 project (H 2020 No. 645452) ...|$|R
40|$|This essay {{deals with}} the concept of continuity, as it has been {{developed}} both in philosophy and in mathematics during the 19 th and 20 th Centuries. In particular, the problem of focus is the relation which the continuum has to number. The debate on whether the continuum can be given as a class of atomic individuals is the principle item of consideration in this work. The negation of this claim is argued for. [...] The two sides of this debate are presented in terms of the philosophical characterizations of <b>extensive</b> <b>magnitude</b> found in the writings of Bertrand Russell (representing the claim that such a reduction is possible) and of Immanuel Kant (representing the negation of this claim). In particular, the epistemological distinction between the two figures is connected with their relative positions on this debate, discussed mainly in connexion with the issue of synthetic a priori judgments. [...] The principal claim argued for in this paper is that the classical analysis of the geometric continuum, and hence Russell's logical reduction of space and time, tacitly presupposes an original undifferentiated continuum among its initial principles. This point is intended to lend support to the more general view of the continuum holding the undifferentiated whole to be utterly prior over its parts. In addition to Kant, one should attach to this view the names of Peirce and Brouwer. In particular, I shall attempt to establish an understanding of the 'spatial point' as an entity which can be individuated only {{as the result of a}} synthetic act. Finally, an examination of the relation of intuitionist choice sequences with the classical set of real numbers is presented, concluding with the conjecture that no law-like system can exhaust all possible positions on the line...|$|R
40|$|Thesis (M. A.) [...] Memorial University of Newfoundland, 2003. PhilosophyIncludes bibliographical {{references}} (leaves 116 - 118) This essay {{deals with}} the concept of continuity, as it has been developed both in philosophy and in mathematics during the 19 th and 20 th Centuries. In particular, the problem of focus is the relation which the continuum has to number. The debate on whether the continuum can be given as a class of atomic individuals is the principle item of consideration in this work. The negation of this claim is argued for. [...] The two sides of this debate are presented in terms of the philosophical characterizations of <b>extensive</b> <b>magnitude</b> found in the writings of Bertrand Russell (representing the claim that such a reduction is possible) and of Immanuel Kant (representing the negation of this claim). In particular, the epistemological distinction between the two figures is connected with their relative positions on this debate, discussed mainly in connexion with the issue of synthetic a priori judgments. [...] The principal claim argued for in this paper is that the classical analysis of the geometric continuum, and hence Russell's logical reduction of space and time, tacitly presupposes an original undifferentiated continuum among its initial principles. This point is intended to lend support to the more general view of the continuum holding the undifferentiated whole to be utterly prior over its parts. In addition to Kant, one should attach to this view the names of Peirce and Brouwer. In particular, I shall attempt to establish an understanding of the 'spatial point' as an entity which can be individuated only {{as the result of a}} synthetic act. Finally, an examination of the relation of intuitionist choice sequences with the classical set of real numbers is presented, concluding with the conjecture that no law-like system can exhaust all possible positions on the line...|$|R
40|$|We {{propose a}} {{heuristic}} search algorithm for finding optimal policies {{in a new}} class of sequential decision making problems. This class extends Markov decision processes by a limited type of hidden state, paying tribute to the fact that many robotic problems indeed possess hidden state. The proposed search algorithm exploits the problem formulation to devise a fast bound-searching algorithm, which in turn cuts down the complexity of finding optimal solutions to the decision making problem by orders of <b>magnitude.</b> <b>Extensive</b> comparisons with state-of-the-art MDP and POMDP algorithms illustrate the effectiveness of our approach. ...|$|R
40|$|Complex {{interactions}} of creep/fatigue/environment control dwell fatigue crack growth (DFCG) in superalloys. Crack tip stress relaxation during dwells significantly changes the crack driving force and influence DFCG. Linear Elastic Fracture Mechanics, Kmax, parameter unsuitable for correlating DFCG behavior due to <b>extensive</b> visco-plastic deformation. <b>Magnitude</b> of remaining crack tip axial stresses controls DFCG resistance {{due to the}} brittle-intergranular nature of the crack growth process. Proposed a new empirical parameter, Ksrf, which incorporates visco-plastic evolution {{of the magnitude of}} remaining crack tip stresses. Previous work performed at 704 C, extend the work to 760 C...|$|R
40|$|Submitted to NIPS 2001 We {{propose a}} {{heuristic}} search algorithm for finding optimal policies {{in a new}} class of sequential decision making problems. This class extends Markov decision processes by a limited type of hidden state, paying tribute to the fact that many robotic problems indeed possess hidden state. The proposed search algorithm exploits the problem formulation to devise a fast bound-searching algorithm, which in turn cuts down the complexity of finding optimal solutions to the decision making problem by orders of <b>magnitude.</b> <b>Extensive</b> comparisons with state-of-the-art MDP and POMDP algorithms illustrate the effectiveness of our approach. ...|$|R
40|$|Schrödinger {{suggested}} that thermodynamical functions cannot {{be based on}} the gratuitous allegation that quantum-mechanical levels (typically the orthogonal eigenstates of the Hamiltonian operator) are the only allowed states for a quantum system [E. Schrödinger, Statistical Thermodynamics (Courier Dover, Mineola, 1967) ]. Different authors have interpreted this statement by introducing density distributions on the space of quantum pure states with weights obtained as functions of the expectation value of the Hamiltonian of the system. In this work we focus on one of the best known of these distributions, and we prove that, when considered in composite quantum systems, it defines partition functions that do not factorize as products of partition functions of the noninteracting subsystems, even in the thermodynamical regime. This implies that {{it is not possible to}} define <b>extensive</b> thermodynamical <b>magnitudes</b> such as the free energy, the internal energy or the thermodynamic entropy by using these models. Therefore, we conclude that this distribution inspired by Schrödinger's idea can not be used to construct an appropriate quantum equilibrium thermodynamics. Comment: 32 pages, revtex 4. 1 preprint style, 5 figures. Published version with several changes with respect to v 2 in text and reference...|$|R
40|$|This paper {{investigates the}} impact of {{alternative}} monetary policy regimes {{on the creation of}} new varieties in open economies. Using a dynamic two-country model incorporating nominal rigidities, international trade and firm entries we compare an independent monetary policy regime to a monetary union regime. We find that a common monetary policy defined by a nominal interest rate rule reactive to inflation increases extensive margin of trade volatility. Simulations based on business cycle frequencies indicate that on average this increase reaches 3 %. Although monetary policy interdependence is found to be a key ingredient in generating this effect, we stress that those parameters affecting international trade structures are crucial in determining its <b>magnitude.</b> <b>Extensive</b> Margin, Variety Effect, Monetary Union, Monetary Policy...|$|R
40|$|Interacting quantum systems {{evolving}} from an uncorrelated composite {{initial state}} generically develop quantum correlations [...] entanglement. As a consequence, a local description of interacting quantum system is impossible as a rule. A unitarily evolving (isolated) quantum system generically develops <b>extensive</b> entanglement: the <b>magnitude</b> of the generated entanglement will increase without bounds with the effective Hilbert space {{dimension of the}} system. It is conceivable, that coupling of the interacting subsystems to local dephasing environments will restrict the generation of entanglement to such extent, that the evolving composite system may be considered as approximately disentangled. This conjecture is addressed {{in the context of}} some common models of a bipartite system with linear and nonlinear interactions and local coupling to dephasing environments. Analytical and numerical results obtained imply that the conjecture is generally false. Open dynamics of the quantum correlations is compared to the corresponding evolution of the classical correlations and a qualitative difference is found. Comment: 35 pages, 10 figures. Revised according to comments of the referees. Accepted for publication in Phys. Rev. ...|$|R
40|$|Arterial medial {{calcification}} {{is a major}} complication {{in patients}} with chronic kidney disease and is a strong predictor of cardiovascular and all-cause mortality. We sought to determine the role of dietary phosphorus and the severity of uremia on vascular calcification in calcification-prone DBA/ 2 mice. Severe and moderate uremia was induced by renal ablation of varying <b>magnitudes.</b> <b>Extensive</b> arterial-medial calcification developed only when the uremic mice were placed on a high-phosphate diet. Arterial calcification in the severely uremic mice fed a high-phosphate diet {{was significantly associated with}} hyperphosphatemia. Moderately uremic mice on this diet were not hyperphosphatemic but had a significant rise in their serum levels of fibroblast growth factor 23 (FGF- 23) and osteopontin that significantly correlated with arterial medial calcification. Although there was widespread arterial medial calcification, there was no histological evidence of atherosclerosis. At early stages of calcification, the osteochondrogenic markers Runx 2 and osteopontin were upregulated, but the smooth muscle cell marker SM 22 α decreased in medial cells, as did the number of smooth muscle cells in extensively calcified regions. These findings suggest that phosphate loading and the severity of uremia play critical roles in controlling arterial medial calcification in mice. Further, FGF- 23 and osteopontin may be markers and/or inducers of this process...|$|R
40|$|The single-chip crosspoint-queued (CQ) switch is {{a compact}} {{switching}} architecture {{that has all}} its buffers placed at the crosspoints of input and output lines. Scheduling is also performed inside the switching core, and does not rely on latency-limited communications with input or output line-cards. Compared with other legacy switching architectures, the CQ switch has the advantages of high throughput, minimal delay, low scheduling complexity, and no speedup requirement. However, the crosspoint buffers are small and segregated, thus how to efficiently use the buffers and avoid packet drops remains a major problem {{that needs to be}} addressed. In this paper, we consider load balancing, deflection routing, and buffer pooling for efficient buffer sharing in the CQ switch. We also design scheduling algorithms to maintain the correct packet order even while employing multi-path switching and resolve contentions caused by multiplexing. All these techniques require modest hardware modifications and memory speedup in the switching core, but can greatly boost the buffer utilizations by up to 10 times and reduce the packet drop rates by one to three orders of <b>magnitude.</b> <b>Extensive</b> simulations and analyses have been done to demonstrate the advantages of the proposed buffering and scheduling techniques in various aspects. By pushing the on-chip memory to the limit of current ASIC technology, we show that a cell drop rate of 10 e- 8, which is low enough for practical uses, can be achieved under real Internet traffic traces corresponding to a load of 0. 9...|$|R
40|$|Colour {{appearance}} models, i. e. {{models that}} attempt to predict the colour appearance of a stimulus by taking the physical properties of the stimulus and its surroundings into account, have been developed and investigated for more than 40 years. Most of these models were developed to handle related colours, i. e. colours perceived {{in relation to other}} colours. A typical example is the rsquo; colour of an object as seen in an illuminated scene. However, two models - CAM 97 u and CAMFu -nbsp;developed to predict the appearance of unrelated colours, i. e. colours perceived in isolation from any other colour (e. g. a traffic light seen at night). Unfortunately, {{due to the lack of}} psychophysical data, neither of these two models has been investigated extensively. Before beingnbsp;to extend these models to other types of stimuli and viewing conditions, they need to benbsp;using new visual data. The aim of this doctoral research is to investigate the colour appearance of unrelated self-luminous stimuli. An accurate prediction of the colour appearance of these stimuli through a colour appearance modelnbsp;be a valuable tool: it can assist in the development of requirements for light-emitting diode (LED) signs, in the standardization of the appearance of marine, aviation or traffic lights viewed during a dark night, in the continuous development of colour appearance models for other viewinghellip; In a first series of psychophysical experiments, the brightness of stimulinbsp;a constant luminance has been evaluated by a group of observers. The stimuli were shown in a darkened room, specially designed for this doctoralnbsp;project. In the centre ofnbsp;wall, a circular self-luminous area was present. The colour of this stimulus area wasnbsp;controllable bynbsp;the flux of the R(ed) G(reen) B(lue) W(white) LED behind it. The observers viewed the stimulus area from a distance that ensures a 10 ° field of view. The brightness evaluation of these stimuli was performed using a magnitude estimation method by scaling the brightness of each test stimulus compared to that of a reference stimulus to which anbsp;value of 50 was attributed. The predictive performance of the CAM 97 u and CAMFu colour appearance models and four other vision models, specially designed to predict brightness, was investigated. Due to, among others, a severe underestimation of the effect of colourfulness onnbsp;- also known as the Helmholtz-Kohlrausch effect -nbsp;of the models seemed to be able to adequately predict the brightness perceived by the observers. Adapting the CAM 97 u model by increasing the colourfulness contribution in the brightness attribute, resulted in a modified model, called CAM 97 um, which allows for a substantially better brightness prediction. The performance ofnbsp;newnbsp;was confirmed by the results of both a matching experiment and an <b>extensive</b> <b>magnitude</b> estimation experiment in which the test stimuli covered a widenbsp;of luminance and chromaticity values. In anbsp;series of psychophysical experiments, in addition to the brightness, the hue and “amount of white” perception of unrelatednbsp;stimuli was also investigatednbsp;a magnitude estimation method. The amount of white is a newly proposed attribute, and basically corresponds to a layperson’s conception of attributes such asnbsp;chroma or saturation. It was introducednbsp;on the results of a preliminary pilot study revealing that laypersons often have difficulty understanding, and hence judging, the colourfulness of a stimulus. Again, unrelated self-luminous 10 ° stimuli, with anbsp;range of luminance and chromaticity values, were evaluated by observers in the darkened room. Based on the obtained visual data, a new colour appearance model for unrelated self-luminous stimuli, CAM 15 u, was developed. The main features of the model are the use of the absolute spectral radiance of the stimulus as input, the use of the CIEnbsp;cone fundamentals and a simplified calculation procedure compared to existing models. The model predictsnbsp;brightness, hue, colourfulness, saturation and the amount of white. The CAM 15 u model is restricted to photopic, non-glary unrelated stimuli having a field of view ofdeg;. The model was validated using the results of an additional experiment. It was found that, despite its simplicity, CAM 15 unbsp;as well or better than other,nbsp;complicated, CAMs. In a final series ofnbsp;experiments, the brightness perception of different sized,nbsp;self-luminous stimuli was investigated in a magnitude estimation experiment. The stimuli were shown in a darkened room on a wide gamut LCD monitor. A significant, hue independent, effect of stimulus size on brightness was found, effectively modeled by a simple power function. Finally, the dependence of brightness on stimulus size was incorporated into the brightness prediction of the CAM 15 u model. nbsp;predictive performance of the modified brightness predictionnbsp;validated using the results obtained in an additional experiment in which observers evaluated the brightness of unrelated self-luminousnbsp;stimuli with variable size, chromaticity and luminance. Although further improvements and extensions are still possible, CAM 15 u has proven itsnbsp;in predicting the appearance of unrelated self-luminous stimuli. It can be a valuable tool for the improvement of existing standards and guidelines for traffic signs, LED billboards,…nrpages: 148 status: publishe...|$|R
40|$|We present Monte Carlo {{models of}} open stellar {{clusters}} {{with the purpose}} of mapping out the behavior of integrated colors with mass and age. Our cluster simulation package allows for stochastic variations in the stellar mass function to evaluate variations in integrated cluster properties. We find that UBVK colors from our simulations are consistent with simple stellar population (SSP) models, provided the cluster mass is large, Mcluster >= 10 ^ 6 M_Sun. Below this mass, our simulations show two significant effects. First, the mean value of the distribution of integrated colors moves away from the SSP predictions and is less red, in the first 10 ^ 7 to 10 ^ 8 years in UBV colors, and for all ages in (V - K). Second, the 1 σ dispersion of observed colors increases significantly with lower cluster mass. The former we attribute to the reduced number of red luminous stars in most of the lower mass clusters and the later we attribute to the increased stochastic effect of a few of these stars on lower mass clusters. This later point was always assumed to occur, but we now provide the first public code able to quantify this effect. We are completing a more <b>extensive</b> database of <b>magnitudes</b> and colors as a function of stellar cluster age and mass that will allow the determination of the correlation coefficients among different bands, and improve estimates of cluster age and mass from integrated photometry. Comment: 11 pages, 5 figures, submitted to Astrophysical Journal Letter...|$|R
40|$|Article first {{published}} online: 2 OCT 2013 [1] The Paleocene-Eocene Thermal Maximum (PETM) {{is marked by}} a prominent negative carbon isotope excursion (CIE) of 3 – 5 ‰ that has a characteristic rapid onset, stable body, and recovery to near pre-CIE isotopic composition. Although the CIE is the major criterion for global correlation of the Paleocene-Eocene boundary, spatial variations in the position and shape of the CIE have not been systematically evaluated. We measured carbon isotope ratios of bulk organic matter (δ 13 Corg) and pedogenic carbonate (δ 13 Ccarb) at six PETM sections across a 16 km transect in the SE Bighorn Basin, Wyoming. Bed tracing and high-resolution floral and faunal biostratigraphy allowed correlation of the sections independent of chemostratigraphy. The onset of the CIE in bulk organic matter at all six sections occurs within a single laterally <b>extensive</b> geosol. The <b>magnitude</b> of the CIE varies from 2. 1 to 3. 8 ‰. The absolute and relative stratigraphic thickness of the body of the CIE in bulk organic matter varies significantly across the field area and underrepresents the thickness of the PETM body by 30 %– 80 %. The variations cannot be explained by basinal position and instead suggest that δ 13 Corg values were influenced by local factors such as reworking of older carbon. The stratigraphic thickness and shape of the CIE have been used to correlate sections, estimate timing of biotic and climatic changes relative to the presumed carbon isotope composition of the atmosphere, and calculate rates of environmental and biotic change. Localized controls on δ 13 Corg values place these inferences in question by influencing the apparent shape and duration of the CIE. Allison A. Baczynski, Francesca A. McInerney, Scott L. Wing, Mary J. Kraus, Jonathan I. Bloch, Doug M. Boyer, Ross Secord, Paul E. Morse, Henry C. Frick...|$|R
40|$|In this thesis, I {{present results}} from a two-year study of strain-rate {{variations}} along a flow line on the western margin of the Greenland ice sheet. I used baseline network solutions to investigate variations in longitudinal strain rates over the 2006 and 2007 melt seasons. Analyses revealed high-magnitude, short-duration events of increased longitudinal strain early in the melt season coincident with a high melt year, suggesting a link between melt production {{and its effects on}} seasonal ice flow. Results from 2006 data show that longitudinal strain rates became variable shortly after the onset of melt (day 186) changing up to ~ 15 x 10 - 4 a- 1 within 24 hours. The onset of melting occurred earlier in 2007 (day 153) and was also followed closely by strain-rate deviation from background rates calculated prior to melting. The data revealed rapid (hours to days), high-magnitude (two to ten times greater than background rates) changes in longitudinal strain rates (hereafter referred to as ‘high-strain’ events) that occurred both on the small-scale (affecting 1 - 4 baselines) and on the large-scale (affecting 5 or more baselines). Large-scale high-strain events were infrequent, on the order of two events per season. Events were likely caused by drainage of supraglacial meltwater that penetrated to the bed of the glacier raising the basal water pressure. The increase in pressure reduced the basal resistive stress, and allowed rapid local acceleration. The basal stress reduction was transmitted to areas of higher stress which resulted in longitudinal compression of the ice down glacier and longitudinal extension up glacier. The evolution of high-strain events altered longitudinal strain rates more than 15 km along flow from the site of initiation. I estimated the origin and spatial extent of highstrain events by assessing the magnitude of the strain-rate variations in various baselines, and observing whether the altered strain regime was <b>extensive</b> or compressive. <b>Magnitude</b> and timing of changes in strain suggest that high-strain events originated in the ablation zone, the equilibrium zone, and inland of the equilibrium zone, and indicate that short-term altered stress conditions are not confined to the ablation zone. The background strain-rate for 2007 (~ - 7 x 10 - 4 a- 1 for a 37 km longitudinal baseline) was similar to the 2006 longitudinal background rate. When extrapolating the 2006 background rate over the melt season, the expected change in baseline length (~ 11 m) was similar to the observed change (~ 9 m). In contrast, when extrapolating the 2007 background rate over the melt season, the expected shortening was ~ 6 m, but the observed shortening was less than 1 m. This result suggests that seasonal high-strain events have the ability to alter longitudinal baseline length, allowing a greater ice flux to lower elevations where melting occurs for a larger portion of the year. However, the cumulative seasonal effects of both large-scale and small-scale strain events are modest, and indicate that seasonal changes in strain rates have a minor effect on the overall stability of the ice sheet. Nevertheless, it is possible that over much longer timescales these seasonal changes may become more important with increasing temperatures and available melt. Results from this study may also be useful in making broader inferences regarding the response of grounded portions of the ice sheet to seasonal changes in basal resistive stress...|$|R

