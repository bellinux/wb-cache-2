645|339|Public
25|$|Early in the {{development}} of high-level programming languages, the introduction of the block enabled the construction of programs in which a group of statements and declarations could be treated as if they were one statement. This, alongside the introduction of subroutines, enabled complex structures to be expressed by <b>hierarchical</b> <b>decomposition</b> into simpler procedural structures.|$|E
50|$|The work {{breakdown}} structure is a <b>hierarchical</b> <b>decomposition</b> of the {{work to be done}} in a project.|$|E
50|$|HOOD (Hierarchic Object-Oriented Design) is a {{detailed}} software design method. It {{is based on}} <b>hierarchical</b> <b>decomposition</b> of a software problem. It comprises textual and graphical representations of the design.|$|E
40|$|<b>Hierarchical</b> <b>decompositions</b> of graphs are {{interesting}} for algorithmic purposes. There are {{several types of}} <b>hierarchical</b> <b>decompositions.</b> Tree decompositions are the best known ones. On graphs of tree-width at most k, i. e., that have tree decompositions of width at most k, where k is fixed, every decision or optimization problem expressible in monadic secondorder logic has a linear algorithm. We prove that {{this is also the}} case for graphs of clique-width at most k, where this complexity measure is associated with <b>hierarchical</b> <b>decompositions</b> of another type, and where logical formulas are no longer allowed to use edge set quantifications. We develop applications to several classes of graphs that include cographs and are, like cographs, defined by forbidding subgraphs with "too many" induced paths with four vertices...|$|R
40|$|Database design aims {{to find a}} {{database}} schema that permits the efficient processing of common types of queries and updates on future database instances. Full first-order <b>hierarchical</b> <b>decompositions</b> constitute a large class of database constraints that can provide assistance to the database designer in identifying a suitable database schema. We establish finite axiomatisations of full first-order <b>hierarchical</b> <b>decompositions</b> that mimic best database design practice. That is, an inference engine derives all the independent collections of the universal schema during database normalization, and the designer determines during database denormalization which recombinations of these independent collections manifest the final database schema. We also show that well-known correspondences between multivalued dependencies, degenerated multivalued dependencies, and a fragment of Boolean propositional logic do not extend beyond binary full first-order <b>hierarchical</b> <b>decompositions...</b>|$|R
40|$|In [11] {{a system}} was {{described}} for finding good <b>hierarchical</b> <b>decompositions</b> of complex systems represented as collections of nodes and links, using a genetic algorithm, with an information theoretic fitness function (representing complexity) {{derived from a}} minimum description length principle. This paper describes the application of this approach {{to the problem of}} reverse engineering the high-level structure of software systems. Ref: 11. Lutz, R. (2001) Evolving Good <b>Hierarchical</b> <b>Decompositions</b> of Complex Systems. Journal of Systems Architecture, 47, pp. 613634...|$|R
50|$|Process design: {{synthesis}} of energy recovery networks, {{synthesis of}} distillation systems (azeotropic), synthesis of reactor networks, <b>hierarchical</b> <b>decomposition</b> flowsheets, superstructure optimization, design multiproduct batch plants. Design {{of the production}} reactors {{for the production of}} plutonium, design of nuclear submarines.|$|E
5000|$|It {{has been}} {{proposed}} that perception itself {{is a process of}} <b>hierarchical</b> <b>decomposition</b> , and that phenomena which are not essentially hierarchical in nature may not even be [...] "theoretically intelligible" [...] to the human mind ( [...] ,). In Simon's words, ...|$|E
50|$|The {{discrete}} {{construction of}} reversibly assembled cellular composites introduces a new {{degree of freedom}} that determines global functional properties from the local placement of heterogeneous components. Because the individual parts are literally finite elements, a <b>hierarchical</b> <b>decomposition</b> describes the part types and their combination in a structure.|$|E
40|$|Abstract. A novel {{approach}} for multiscale image based on integro-differential equations (IDEs) representations was proposed in [29]. These IDEs which stem naturally from multiscale (BV,L 2) <b>hierarchical</b> <b>decompositions,</b> yield inverse scale representations of {{images in the}} sense that the BV-dual norm of their residuals are inversely proportional the scaling parameters. Motivated by the fact that (BV,L 1) decomposition is more suitable for extracting local scale-space features than (BV,L 2), we introduce here the IDEs which arise from multiscale (BV,L 1) <b>hierarchical</b> <b>decompositions.</b> We study several variants of this (BV,L 1) -based IDE model, depending on modifications to the curvature term. Content...|$|R
40|$|<b>Hierarchical</b> <b>decompositions</b> of graphs are {{interesting}} for algorithmic purposes. Many NP complete problems have linear complexity on graphs with tree-decompositions of bounded width. We investigate alternate <b>hierarchical</b> <b>decompositions</b> {{that apply to}} wider classes of graphs and still enjoy good algorithmic properties. These decompositions are motivated and inspired by the study of vertex-replacement context-free graph grammars. The complexity measure of graphs associated with these decompositions is called clique width. In this paper we bound the clique width of a graph {{in terms of its}} tree width on the one hand, and of the clique width of its edg...|$|R
40|$|We {{introduce}} {{an architecture}} based on deep <b>hierarchical</b> <b>decompositions</b> to learn effective representations of large graphs. Our framework extends classic R-decompositions used in kernel methods, enabling nested "part-of-part" relations. Unlike recursive neural networks, which unroll a template on input graphs directly, we unroll a neural network template over the decomposition hierarchy, {{allowing us to}} deal with the high degree variability that typically characterize social network graphs. Deep <b>hierarchical</b> <b>decompositions</b> are also amenable to domain compression, a technique that reduces both space and time complexity by exploiting symmetries. We show empirically that our approach is competitive with current state-of-the-art graph classification methods, particularly when dealing with social network datasets...|$|R
50|$|A {{decomposition}} diagram shows {{a high-level}} function, process, organization, data subject area, or {{other type of}} object broken down into lower level, more detailed components. For example, decomposition diagrams may represent organizational structure or functional decomposition into processes. Decomposition diagrams provide a logical <b>hierarchical</b> <b>decomposition</b> of a system.|$|E
50|$|Early in the {{development}} of high-level programming languages, the introduction of the block enabled the construction of programs in which a group of statements and declarations could be treated as if they were one statement. This, alongside the introduction of subroutines, enabled complex structures to be expressed by <b>hierarchical</b> <b>decomposition</b> into simpler procedural structures.|$|E
50|$|In graph theory, the modular {{decomposition}} is a decomposition of a graph into subsets of vertices called modules. A module is a generalization of a connected {{component of a}} graph. Unlike connected components, however, one module can be a proper subset of another. Modules therefore lead to a recursive (<b>hierarchical)</b> <b>decomposition</b> of the graph, instead of just a partition.|$|E
40|$|Many {{application}} areas {{represent the}} architecture of complex systems by means of hierarchical graphs containing basic entities with directed links between them, and showing the decomposition of systems into a hierarchical nested “module” structure. An interesting question is then: How best should such a complex system be decomposed into a hierarchical tree of nested “modules”? This paper describes an interesting complexity measure (based on an information theoretic minimum description length principle) {{which can be used}} to compare two such <b>hierarchical</b> <b>decompositions.</b> This is then used as the fitness function for a genetic algorithm (GA) which successfully explores the space of possible <b>hierarchical</b> <b>decompositions</b> of a system. The paper also describes the novel crosssover and mutation operators that are necessary in order to do this, and gives some examples of the system in practice...|$|R
40|$|International audiencePHIDAL (parallel <b>hierarchical</b> {{interface}} <b>decomposition</b> algorithm) is {{a parallel}} incomplete factorization method which exploits a <b>hierarchical</b> interface <b>decomposition</b> of the adjacency graph of the coefficient matrix. The {{idea of the}} decomposition {{is similar to that}} of the well-known wirebasket techniques used in domain decomposition. However, the method is devised for general, irregularly structured, sparse linear systems. This paper describes a few algorithms for obtaining good quality <b>hierarchical</b> graph <b>decompositions</b> and discusses the parallel implementation of the factorization procedure. Numerical experiments are reported to illustrate the scalability of the algorithm and its effectiveness as a general purpose parallel linear system solver...|$|R
40|$|In [12] {{a system}} was {{described}} for finding good <b>hierarchical</b> <b>decompositions</b> of complex systems represented as collections of nodes and links, using a genetic algorithm, with an information theoretic fitness function (representing complexity) {{derived from a}} minimum description length principle. This paper describes the application of this approach {{to the problem of}} reverse engineering the high-level structure of software systems...|$|R
5000|$|The system {{delimitation}} is {{the base}} of an efficient modeling. Starting out from a conceptual formulation {{the area of the}} real system to be shown is selected and interfaces will be defined to an environment. In addition, the detail depth of the model is also determined, i.e. the depth of the <b>hierarchical</b> <b>decomposition</b> relations in the view [...] "business process model".|$|E
50|$|Product {{structure}} is a <b>hierarchical</b> <b>decomposition</b> of a product, typically {{known as the}} bill of materials (BOM).As business becomes more responsive to unique consumer tastes and derivative products grow to meet the unique configurations, BOM management can become unmanageable. For manufacturers, a bill of materials (BOM) is a critical product information record that lists the raw materials, assemblies, components, parts and the quantities of each needed to manufacture a product.|$|E
5000|$|A work {{breakdown}} structure (WBS), in project management and systems engineering, is a deliverable-oriented breakdown of a project into smaller components. A {{work breakdown}} structure is a key project deliverable that organizes the team's work into manageable sections. The Project Management Body of Knowledge (PMBOK 5) defines the {{work breakdown structure}} as a [...] "A <b>hierarchical</b> <b>decomposition</b> of the total scope {{of work to be}} carried out by the project team to accomplish the project objectives and create the required deliverables." ...|$|E
30|$|In SDA, {{the number}} of {{possible}} decomposition formulas is given as n!, where n denotes {{the number of}} variables. Dietzenbacher and Los (1998) recommended that studies should report the mean of all n! decompositions. In analyzing the changes in DVA generated by gross exports, we applied the principle of polar decompositions (for detail, see De Haan 2001) and <b>hierarchical</b> <b>decompositions</b> (for detail, see Koller and Stehrer 2010) {{to reduce the number}} of decompositions.|$|R
5000|$|The {{best way}} to {{approach}} requirements analysis is {{through a process of}} parallel — not <b>hierarchical</b> — <b>decomposition</b> of user requirements.|$|R
40|$|Large scale {{optimization}} {{problems are}} tractable {{only if they}} are somehow decomposed. <b>Hierarchical</b> <b>decompositions</b> are inappropriate for some types of problems and do not parallelize well. Sobieszczanski-Sobieski has proposed a nonhierarchical decomposition strategy for nonlinear constrained optimization that is naturally parallel. Despite some successes on engineering problems, the algorithm as originally proposed fails on simple two dimensional quadratic programs. The algorithm is carefully analyzed for quadratic programs, and a number of modifications are suggested to improve its robustness...|$|R
50|$|However, while perfect {{functional}} decomposition is usually impossible, the spirit {{lives on in}} {{a large number of}} statistical methods that are equipped to deal with noisy systems. When a natural or artificial system is intrinsically hierarchical, the joint distribution on system variables should provide evidence of this hierarchical structure. The task of an observer who seeks to understand the system is then to infer the hierarchical structure from observations of these variables. This is the notion behind the <b>hierarchical</b> <b>decomposition</b> of a joint distribution, the attempt to recover something of the intrinsic hierarchical structure which generated that joint distribution.|$|E
50|$|Multi-attribute global {{inference}} {{of quality}} (MAGIQ) is a multi-criteria decision analysis technique. MAGIQ {{is based on}} a <b>hierarchical</b> <b>decomposition</b> of comparison attributes and rating assignment using rank order centroids. The MAGIQ technique is used to assign a single, overall measure of quality to each member of a set of systems where each system has an arbitrary number of comparison attributes. The MAGIQ technique has features similar to the analytic hierarchy process and the simple multi-attribute rating technique exploiting ranks (SMARTER) technique. The MAGIQ technique was first published by James D. McCaffrey. The MAGIQ process begins with an evaluator determining which system attributes are to be used as the basis for system comparison. These attributes are ranked by importance to the particular problem domain, and the ranks are converted to ratings using rank order centroids. Each system under analysis is ranked against each comparison attribute and the ranks are transformed into rank order centroids. The final overall quality metric for each system is the weighted (by comparison attribute importance) sum of each attribute rating. The references provide specific examples of the process. There is little direct research on the theoretical soundness and effectiveness of the MAGIQ technique as a whole, however the use of <b>hierarchical</b> <b>decomposition</b> and the use of rank order centroids in multi-criteria decision analyses have been studied, with generally positive results. Anecdotal evidence suggests that the MAGIQ technique is both practical and useful.|$|E
50|$|The {{simulation}} {{technique used}} is a next event Monte Carlo Simulation, with full array of building blocks, animation, graphical interface, unlimited <b>hierarchical</b> <b>decomposition,</b> full connectivity and interactivity with other programs, library based, build in compiled C-like programming language, to mention {{some of the}} most important features. The flexible method enables modelling of multi purpose tasks like system availability, on demand failure probability, multiple product flow, loading/storage capacities and spare part optimization. In offshore development projects all these features are used during the concept selection process. There is an interface between the simulation tool and Excel for input data, which makes it easy to structure data, extract data, running sensitivities and customize results presentation.|$|E
40|$|Many {{techniques}} {{to reduce the}} cost at test time in large-scale problems involve a hierarchical organization of classifiers, but are either too expensive to learn or degrade the classification performance. Conversely, in this work we show that using ensembles of randomized <b>hierarchical</b> <b>decompositions</b> of the original problem can both improve the accuracy and reduce the computational complexity at test time. The proposed method is evaluated in the ImageNet Large Scale Visual Recognition Challenge’ 10, with promising results. Peer ReviewedPostprint (author’s final draft...|$|R
40|$|Graph {{structure}} is a flexible concept covering {{many different types}} of graph properties. <b>Hierarchical</b> <b>decompositions</b> yielding the notions of tree-width and clique-width, expressed by terms written with appropriate graph operations and associated with Monadic Second-order Logic are important tools for the construction of Fixed-Parameter Tractable algorithms and also for the extension of methods and results of Formal Language Theory to the description of sets of finite graphs. This informal overview presents the main definitions, results and open problems and tries to answer some frequently asked questions...|$|R
40|$|Supervisory {{control systems}} {{must deal with}} various types of {{intelligence}} distributed throughout the layers of control. Typical layers are real-time servo control, off-line planning and reasoning subsystems and finally, the human operator. Design methodologies must account {{for the fact that}} the majority of the intelligence will reside with the human operator. <b>Hierarchical</b> <b>decompositions</b> and feedback loops as conceptual building blocks that provide a common ground for man-machine interaction are discussed. Examples of types of parallelism and parallel implementation on several classes of computer architecture are also discussed...|$|R
5000|$|...-GOMSL / NGOMSL: GOMS Language or Natural GOMS Language, which {{focus on}} the <b>hierarchical</b> <b>decomposition</b> of goals, but with an {{analysis}} including methods - procedures people use to accomplish those goals. Many generic mental operations in the KLM are replaced with detailed descriptions of the cognitive activity involving the organization of people's procedural knowledge into methods. A detailed GOMSL analysis allows for the prediction of not only execution time, but also {{the time it takes}} for learning the procedures, and the amount of transfer that can be expected based on already known procedures (Gong and Kieras, 1994). These models are not only useful for informing redesigns of user-interfaces, but also quantitatively predict execution and learning time for multiple tasks.|$|E
40|$|We propose an {{algorithm}} for the <b>hierarchical</b> <b>decomposition</b> {{of a large}} automaton-based discrete-event system. We {{also provide}} an estimation of the numerical cost {{in terms of the}} size of the system. Our <b>hierarchical</b> <b>decomposition</b> is illustrated with simple examples from dynamical ST-control and supervisory control with uncontrollable events and unsafe states...|$|E
40|$|Abstract—Software {{systems are}} {{decomposed}} hierarchically, for example, into modules, packages and files. This <b>hierarchical</b> <b>decomposition</b> has a profound influence on evolvability, maintainability and work assignment. <b>Hierarchical</b> <b>decomposition</b> is thus clearly of central concern for empirical software engineering researchers; {{but it also}} poses a quandary. At what level do we study phenomena, such as quality, distribution, collaboration an...|$|E
40|$|Original article can {{be found}} at : [URL] Copyright Eszterhazy Karoly College [Full text of this article is not {{available}} in the UHRA]There are many ways to construct <b>hierarchical</b> <b>decompositions</b> of transformation semigroups. The holonomy algorithm is especially suitable for computational implementations and it is used in our software package. The structure of the holonomy decomposition is determined by the action of the semigroup on certain subsets of the state set. Here we focus on this structure, the skeleton, and investigate some of its properties that are crucial for understanding and for efficient calculations...|$|R
40|$|Harald Räcke [STOC 2008] {{described}} a new method to obtain <b>hierarchical</b> <b>decompositions</b> of networks {{in a way}} that minimizes the congestion. Räcke’s approach is based on an equivalence that he discovered between minimizing congestion and minimizing stretch (in a certain setting). Here we present Räcke’s equivalence in an abstract setting that is more general than the one described in Räcke’s work, and clarifies the power of Räcke’s result. In addition, we present a related (but different) equivalence that was developed by Yuval Emek [ESA 2009] and is only known to apply to planar graphs. ...|$|R
40|$|A graph {{complexity}} {{measure that}} we call clique-width is associated in a natural way with certain graph decompositions, more or less like tree-width is associated with tree-decomposition which are, actually, <b>hierarchical</b> <b>decompositions</b> of graphs. In general, a decomposition of a graph G {{can be viewed as}} a finite term, written with appropriate operations on graphs, that evaluates to G. Infinitely many operations are necessary to define all graphs. By limiting the operations in terms of some integer parameter k, one obtains complexity measures of graphs. Specifically, a graph G has complexity at most k iff it has a decomposition defined in terms of k operations. <b>Hierarchical</b> graph <b>decompositions</b> are interesting for algorithmic purposes. In fact, many NP-complete problems have linear algorithms on graphs of tree-width or of clique-width bounded by some fixed k, and the same will hold for graphs of clique-width at most k. The graph operations upon which clique-width and the related decomp [...] ...|$|R
