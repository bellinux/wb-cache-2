308|1397|Public
2500|$|Twenty five patents {{were issued}} by the U.S. Patent Office to Bode for his inventions. The patents covered areas such as data {{transmission}} networks, electronic filters, amplifiers, averaging mechanisms, [...] <b>data</b> <b>smoothing</b> networks and artillery computers.|$|E
2500|$|Bode, in addition, applied [...] his {{extensive}} skills with feedback amplifiers {{to design the}} target <b>data</b> <b>smoothing</b> and position [...] predictor networks of an improved model of Director T-10, called the Director T-15. The work on Director T-15 was undertaken under a new project at Bell Labs called Fundamental Director Studies {{in cooperation with the}} NDRC under the directorship of Walter McNair.|$|E
2500|$|In 1945, {{as the war}} {{was coming}} to an end, the NDRC was issuing a summary of {{technical}} reports as a last step prior to its eventual closing down. Inside the volume on fire control, a special essay titled <b>Data</b> <b>Smoothing</b> and Prediction in Fire-Control Systems, coauthored by Shannon, Ralph Beebe Blackman, and Hendrik Wade Bode, formally treated the problem of smoothing the data in fire-control by analogy with [...] "the problem of separating a signal from interfering noise in communications systems." [...] In other words, it modeled the problem in terms of data and signal processing and thus heralded the coming of the Information Age.|$|E
40|$|In this article, we {{consider}} the estimation of semiparametric panel <b>data</b> <b>smooth</b> coefficient models. We propose a class of local generalized method of moments (LGMM) estimators that are simple and easy to implement in practice. We show that the proposed LGMM estimators are consistent and asymptotically normal. Monte Carlo simulations suggest that our proposed estimator performs quite well in finite samples. An empirical application using a large panel of U. K. firms is also presented. Local Generalized Method of Moments, Monte Carlo simulation, Semiparametric panel <b>data</b> model, <b>Smooth</b> coefficient, C 13, C 14, C 33,...|$|R
40|$|Low pass filters, {{which are}} used to remove high {{frequency}} noise from time series <b>data,</b> <b>smooth</b> the signals they are applied to. In this paper we examine the action of low pass filters on discontinuous or non-differentiable signals from non-smooth dynamical systems, where this smoothing action {{can be thought of}} as a smoothing of the underlying system...|$|R
50|$|The plots below {{show the}} {{bootstrap}} distributions {{of the standard}} deviation, median absolute deviation (MAD) and Qn estimator of scale. The plots are based on 10,000 bootstrap samples for each estimator, with some Gaussian noise added to the resampled <b>data</b> (<b>smoothed</b> bootstrap). Panel (a) shows {{the distribution of the}} standard deviation, (b) of the MAD and (c) of Qn.|$|R
2500|$|In 1945, {{as the war}} was winding down, the NDRC was {{issuing a}} summary of {{technical}} reports as the prelude to its eventual closing down. Inside the volume on Fire Control a special essay titled <b>Data</b> <b>Smoothing</b> and Prediction in Fire-Control Systems, coauthored by Ralph Beebe Blackman, Hendrik Bode, and Claude Shannon, formally introduced the problem of fire control as a special case of transmission, manipulation and utilization of intelligence, in other words it modeled the problem in terms of data and signal processing and thus heralded {{the coming of the}} information age. Shannon, considered to be the father of information theory, was greatly influenced by this work. It is clear that the technological convergence ...|$|E
50|$|Walker also {{introduces}} the reader {{at some length}} to simple feedback and control systems, providing spreadsheets to demonstrate feedback, oscillation and <b>data</b> <b>smoothing</b> to illustrate his arguments. While the diet is a fairly straightforward calorie-counting approach, <b>data</b> <b>smoothing</b> (exponential moving averages in particular) is considered {{a key element of}} the monitoring system. Walker presents techniques for Excel-aided or paper-and-pencil <b>data</b> <b>smoothing</b> to allow the dieter to adjust the diet for themselves using the long-term trend and to not be discouraged by short-term fluctuations based on water retention or other factors.|$|E
50|$|SSA and Density Estimation.Since SSA {{can be used}} as {{a method}} of <b>data</b> <b>smoothing</b> it {{can be used as}} a method of {{non-parametric}} density estimation (Golyandina et al., 2012).|$|E
5000|$|... the {{solution}} is smooth on {{the closure of the}} domain if the boundary <b>data</b> is <b>smooth.</b>|$|R
30|$|Next, {{we shall}} derive lower semicontinuity of the {{existence}} time, provided the initial <b>data</b> is <b>smooth</b> enough.|$|R
40|$|We {{present the}} usage of a non-discrete degree of {{interest}} (DOI) function, obtained by brushing multi-valued 3 D simulation data in information visualization views, to define opacity, color, and geometrical transfer functions for 3 D rendering in a scientific visualization view via linking. To reflect the smooth nature of features in flow simulation <b>data,</b> <b>smooth</b> brushing was chosen. Different available views and interaction methods of a prototype system are discussed, and examples from 3 D flow simulation are shown...|$|R
50|$|Twenty five patents {{were issued}} by the U.S. Patent Office to Bode for his inventions. The patents covered areas such as data {{transmission}} networks, electronic filters, amplifiers, averaging mechanisms, <b>data</b> <b>smoothing</b> networks and artillery computers.|$|E
5000|$|... where α is the <b>data</b> <b>{{smoothing}}</b> factor, 0 < α < 1, β is {{the trend}} smoothing factor, 0 < β < 1, and γ is the seasonal change smoothing factor, 0 < γ < 1.|$|E
50|$|An alpha beta filter (also called alpha-beta filter, f-g filter or g-h filter) is a {{simplified}} form of observer for estimation, <b>data</b> <b>smoothing</b> and control applications. It {{is closely related}} to Kalman filters and to linear state observers used in control theory. Its principal advantage {{is that it does not}} require a detailed system model.|$|E
40|$|AbstractIn {{order to}} improve the {{navigation}} accuracy of an inertial navigation system (INS), composed of quartz gyroscopes, the existing real-time compensation methods for periodic errors in quartz gyroscope drift and the periodic error term relationship between sampled original <b>data</b> and <b>smoothed</b> <b>data</b> are reviewed. On {{the base of the}} results, a new compensation method called using former period characteristics to compensate latter smoothness data (UFCL for short) method is proposed considering the INS working characteristics. This new method uses the original <b>data</b> without <b>smoothing</b> to work out an error conversion formula at the INS initial alignment time and then compensate the <b>smoothed</b> <b>data</b> errors by way of the formula at the navigation time. Both theoretical analysis and experimental results demonstrate that this method is able to cut down on computational time and raise the accuracy which makes it a better real-time compensation approach for periodic error terms of quartz micro electronic mechanical system (MEMS) gyroscope's zero drift...|$|R
40|$|AbstractBased {{upon the}} idea of {{construction}} of <b>data</b> driven <b>smooth</b> tests for composite hypotheses presented in Inglotet al. (1997) and Kallenberg and Ledwina (1997), two versions of <b>data</b> driven <b>smooth</b> test for bivariate normality are proposed. Asymptotic null distributions are derived, and consistency of the newly introduced tests against every bivariate alternative with marginals having finite variances is proved. Included results of power simulations show {{that one of the}} proposed tests performs very well in comparison with other commonly used tests for bivariate normality...|$|R
30|$|The data subsets were {{normalized}} so {{that the}} data rage fell between − 1 and 1. Such scaling of <b>data</b> <b>smooths</b> the solution space and averages {{out some of the}} noise (ASCE 2000). Since results from these normalized models indicated that performance of the models did not change very much, the results here are represented without normalized data. The available records of monthly water quality parameters of Asi River at the steel bridge station suffer additionally from missing data. Some of appropriate strategies to treat the missing data are used (Honaker and King 2010).|$|R
50|$|Blackman {{graduated}} from the California Institute of Technology in 1926 and started work at Bell Laboratories the same year. His early research were {{in the fields of}} hearing, acoustics and mechanical filters. Later he focused on applied mathematics, specifically linear networks and feedback amplifiers.Starting in 1940, Blackman worked on <b>data</b> <b>smoothing</b> for anti-aircraft fire control systems.|$|E
50|$|Bode, in addition, applied his {{extensive}} skills with feedback amplifiers {{to design the}} target <b>data</b> <b>smoothing</b> and position predictor networks of an improved model of Director T-10, called the Director T-15. The work on Director T-15 was undertaken under a new project at Bell Labs called Fundamental Director Studies {{in cooperation with the}} NDRC under the directorship of Walter McNair.|$|E
50|$|In statistics, kernel density {{estimation}} (KDE) is a non-parametric way {{to estimate}} the probability density function of a random variable. Kernel density estimation is a fundamental <b>data</b> <b>smoothing</b> problem where inferences about the population are made, based on a finite data sample. In some fields such as signal processing and econometrics it is also termed the Parzen-Rosenblatt window method, after Emanuel Parzen and Murray Rosenblatt, who are usually credited with independently creating it in its current form.|$|E
40|$|Low pass filters, {{which are}} used to remove high {{frequency}} noise from time series <b>data,</b> <b>smooth</b> the signals they are applied to. In this paper we examine the action of low pass filters on discontinuous or non-differentiable signals from non-smooth dynamical systems. We show that {{the application of the}} filter is equivalent to a change of variables, which transforms the non-smooth system into a smooth one. We examine this smoothing action on a variety of examples and demonstrate how it is useful in the calculation of a non-smooth system’s Lyapunov spectrum...|$|R
40|$|<b>Smoothed</b> <b>data</b> maps {{permit the}} reader to {{identify}} general spatial trends by removing the background noise of random variability often present in raw <b>data.</b> To <b>smooth</b> mortality <b>data</b> from 798 small areas comprising the contiguous United States, we extended the head-banging algorithm to allow for differential weighting of the values to be smoothed. Actual and simulated data sets {{were used to determine}} how head-banging smoothed spike and edge features in the data, and to observe the degree to which weighting affected the results. As expected, spikes were generally removed while edges and clusters of high rates near the U. S. borders were maintained by the unweighted head-banging algorithm. Incorporating weights inversely proportional to standard errors had a substantial effect on <b>smoothed</b> <b>data,</b> for example determining whether observed spikes were retained or removed. The process used to obtain the <b>smoothed</b> <b>data,</b> including the choice of head-banging parameters, is discussed. Results are considered in the context of general spatial trends...|$|R
40|$|Computational {{efficient}} filters speed {{processing of}} two-dimensional experimental <b>data.</b> Two-dimensional <b>smoothing</b> filter used to attenuate highfrequency noise in two-dimensional numerical data arrays. Filter provides <b>smoothed</b> <b>data</b> values equal to values obtained by fitting surface with secondand third-order terms to 5 by 5 subset of data points centered on points and replacing data at each point by value of surface fitted at point. Especially suited for efficient analysis of two-dimensional experimental data on images...|$|R
5000|$|In 1945, {{as the war}} {{was coming}} to an end, the NDRC was issuing a summary of {{technical}} reports as a last step prior to its eventual closing down. Inside the volume on fire control, a special essay titled <b>Data</b> <b>Smoothing</b> and Prediction in Fire-Control Systems, coauthored by Shannon, Ralph Beebe Blackman, and Hendrik Wade Bode, formally treated the problem of smoothing the data in fire-control by analogy with [...] "the problem of separating a signal from interfering noise in communications systems." [...] In other words, it modeled the problem in terms of data and signal processing and thus heralded the coming of the Information Age.|$|E
50|$|In 1945, {{as the war}} was winding down, the NDRC was {{issuing a}} summary of {{technical}} reports as the prelude to its eventual closing down. Inside the volume on Fire Control a special essay titled <b>Data</b> <b>Smoothing</b> and Prediction in Fire-Control Systems, coauthored by Ralph Beebe Blackman, Hendrik Bode, and Claude Shannon, formally introduced the problem of fire control as a special case of transmission, manipulation and utilization of intelligence, in other words it modeled the problem in terms of data and signal processing and thus heralded {{the coming of the}} information age. Shannon, considered to be the father of information theory, was greatly influenced by this work. It is clear that the technological convergenceof the information age was preceded by the synergy between these scientific minds and their collaborators.|$|E
5000|$|It {{uses the}} Qt widget set for its {{graphical}} interface. It is {{integrated with the}} KDE desktop and has drag and drop support with KDE's applications. The handbook is written in KDE and conforms to the Khelpcenter standards. It is scriptable using Qt Script for Applications (QSA). 2D and 3D plots of data can be rendered in a [...] "worksheet", either by directly reading datafiles or from a spreadsheet, which LabPlot supports. It has interfaces to several libraries, including GSL for data analysis, the Qwt3d libraries for 3D plotting using OpenGL, FFTW for fast Fourier transforms and supports exporting to 80 image formats and raw PostScript. Other key features include support for the FITS format, for LaTeX and Rich Text labels, data masking, multiple plots in the same worksheet, pie charts, bar charts/histograms, interpolation, <b>data</b> <b>smoothing,</b> peak fitting, nonlinear curve fitting, regression, deconvolution, integral transforms, and others (see developers website listed below for details). The graphs are publication-quality. Interface translated in various languages.|$|E
5000|$|Koren (Koren, 1993) - third-order {{accurate}} for sufficiently <b>smooth</b> <b>data</b> ...|$|R
3000|$|... bounded {{with two}} {{concentric}} spheres that present solid thermoinsulated walls, {{assuming that the}} initial density and temperature are strictly positive and that the initial <b>data</b> are <b>smooth</b> enough spherically symmetric functions.|$|R
40|$|International audienceIn this paper, {{we propose}} a goodness-of-fit test of {{normality}} for the innovations of an ARMA(p, q) model with known mean or trend. The test {{is based on the}} <b>data</b> driven <b>smooth</b> test approach and is simple to perform. An extensive simulation study is conducted to study the behaviour of the test for moderate sample sizes. It is found that our approach is generally more powerful than existing tests while holding its level throughout most of the parameter space and, thus, can be recommended. This agrees with theoretical results showing the superiority of the <b>data</b> driven <b>smooth</b> test approach in related contexts...|$|R
3000|$|Figures  2 B and 3 {{show the}} {{influence}} of <b>data</b> <b>smoothing</b> on CMRglc and rate constants. <b>Data</b> <b>smoothing</b> did not affect CMRglc significantly, but significant differences occurred between estimates of all single rate constants with the original TAC and their estimates achieved with any combination of the smoothed data vectors (P[*]<[*] 0.05) with only one exception (comparison K [...]...|$|E
3000|$|... 1 {{with smooth}} IF versus {{original}} data, see Figure  3 A). In general, <b>data</b> <b>smoothing</b> leads to underestimation of K [...]...|$|E
40|$|Abstract. This paper {{presents}} a unified image {{processing and analysis}} framework for cortical thickness in characterizing a clinical population. The {{emphasis is placed on}} the development of <b>data</b> <b>smoothing</b> and analysis framework. The human brain cortex is a highly convoluted surface. Due to the convoluted non-Euclidean surface geometry, <b>data</b> <b>smoothing</b> and analysis on the cortex are inherently difficult. When measurements lie on a curved surface, it is natural to assign kernel smoothing weights based on the geodesic distance along the surface rather than the Euclidean distance. We present a new <b>data</b> <b>smoothing</b> framework that address this problem implicitly without actually computing the geodesic distance and present its statistical properties. Afterwards, the statistical inference is based on the random field theory based multiple comparison correction. As an illustration, we have applied the method in detecting the regions of abnormal cortical thickness in 16 high functioning autistic children. ...|$|E
40|$|Based {{upon the}} idea of {{construction}} of <b>data</b> driven <b>smooth</b> tests for composite hypotheses presented in Inglotet al. (1997) and Kallenberg and Ledwina (1997), two versions of <b>data</b> driven <b>smooth</b> test for bivariate normality are proposed. Asymptotic null distributions are derived, and consistency of the newly introduced tests against every bivariate alternative with marginals having finite variances is proved. Included results of power simulations show {{that one of the}} proposed tests performs very well in comparison with other commonly used tests for bivariate normality. Schwarz's BIC criterion tests of bivariate normality goodness-of-fit score test smooth test Neyman's test Monte Carlo simulations...|$|R
3000|$|... [...]. We will {{consider}} this existence problem elsewhere. In this work, we assume {{the existence of}} the solution and its continuous dependence on the initial <b>data</b> under <b>smooth</b> enough assumptions on the initial perturbation.|$|R
40|$|Abstract—Promising {{agreement}} {{over land}} and sea has been obtained between NEXRAD 3 -GHz radar observations of pre-cipitation rate and retrievals based on simultaneous passive observations at 50 – 191 GHz from the Advanced Microwave Sounding Unit (AMSU) on the NOAA- 15 meteorological satellite. A neural network with three hidden nodes and one linear output node operated on 15 km resolution data at 183 1 and 183 7 GHz, plus the cosine of scan angle, to produce estimates that match well the morphology of NEXRAD hurricane and frontal precipitation <b>data</b> <b>smoothed</b> to 15 -km resolution. A second neural network operated on the same three parameters used in the first network, but smoothed to 50 -km resolution, plus spatially-filtered cold perturbations detected in three AMSU tropospheric temper-ature-sounding channels (channels 4 – 6), which also have 50 -km resolution. Comparison with the same NEXRAD <b>data</b> <b>smoothed</b> to 50 -km resolution yielded root mean square (rms) discrepancies for two frontal systems and two passes over Hurricane Georges of 1. 1 mm/h, and 1. 4 dB for those precipitation events over 4 mm/h. Only 8. 9 % of the total AMSU-derived rainfall was in areas where AMSU saw more than 1 - mm/h and NEXRAD saw less than 1 -mm/h, and only 6. 2 % of the total NEXRAD-derived rainfall was in areas where NEXRAD saw more than 1 -mm/h and AMSU saw less than 1 -mm/h. Index Terms—Measurement, microwave propagation, mi-crowave radiometry, microwave spectroscopy, precipitation, remote sensing. I...|$|R
