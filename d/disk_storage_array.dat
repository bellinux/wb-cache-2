3|1751|Public
50|$|On a Spirit Datacine Phantom TransferEngine {{software}} running on an SGI computer or Bones Linux-based software is used to record the DPX files from the Spirit DataCine. These files are stored in the virtual telecine or on a SAN hard <b>disk</b> <b>storage</b> <b>array.</b>|$|E
5000|$|A Spirit DataCines outputing DPX files {{was used}} in the 2000 movie O Brother, Where Art Thou?. The DPX files were color {{corrected}} with a VDC-2000 and a Pandora Int. Pogle Color Corrector with MegaDEF. [...] A Kodak Lightning II film recorder was used to put the data output to back to film. To output the movie the Spirit Datacine’s Phantom Transfer Engine software running on an SGI computer is used to record the DPX files from the Spirit DataCine. These files are stored in the virtual telecine or on a SAN hard <b>disk</b> <b>storage</b> <b>array.</b> The Phantom Transfer Engine has been replaced with Bones software running on a Linux-based PC. First generation of DPX interface for data files was the optical fiber HIPPI cables (up to 6 frame/s at 2K), the next generation interface is GSN-Gigabit Ethernet fiber Optic (up to 30 frame/s at 2K). GSN is also called HIPPI-6400 and was later renamed GSN (for Gigabyte System Network). The SAN hard disks are interfaces to by dual FC-Fibre Channel, cables. The newest DPX output interface is infiniband.|$|E
40|$|Many {{applications}} {{today are}} highly data intensive and have stringent performance requirements. In {{order to meet}} the performance demands of these applications, we need to optimize both the processing and I/O subsystems. One promising approach to optimize performance is to use “Active Storage ” systems, where we use disk drive controllers and storage array controllers as offload processors for the data intensive parts of an application and exploit Data-Level Parallelism (DLP) across the ensemble of processing components. From the architecture viewpoint, the design space of such active storage systems is large. There are a number of choices for the microarchitecture of the processors at the <b>disk,</b> <b>storage</b> <b>array</b> controller, and host, the organization of the electro-mechanical parts of the disk drive, and the characteristics of the interconnection network between these components. Since these design choices can {{have a significant impact on}} performance, we need a simulator that can give us detailed insights into the behavior of these systems. This paper presents the Modeling Infrastructure for Dynamic Active Storage (MIDAS). MIDAS is an accurate execution-driven simulator that captures both the processing and I/O behavior of active storage systems. We describe the design of MIDAS, providing details about the various simulation models and how they interact. We then present three case studies that illustrate how MIDAS can be used to study architectural design tradeoffs in active storage systems. ...|$|E
50|$|Vodien {{also has}} a {{partnership}} with Dell, in which Vodien exclusively uses server hardware from Dell. This partnership covers the Dell Poweredge Servers, Dell PowerConnect Switches, and Dell PowerVault Highly Available Modular <b>Disk</b> <b>Storage</b> <b>Arrays.</b>|$|R
50|$|Universal Storage Platform (USP) was {{the brand}} name for an Hitachi Data Systems line of {{computer}} data <b>storage</b> <b>disk</b> <b>arrays</b> circa 2004 to 2010.|$|R
50|$|In 2011, EMC {{introduced}} the new VNX series of unified <b>storage</b> <b>disk</b> <b>arrays</b> intended to replace both Clariion and Celerra products. In early 2012, Clariion and Celerra were discontinued.|$|R
5000|$|Although iSCSI can {{communicate}} with arbitrary types of SCSI devices, system administrators almost always use it to allow server computers (such as database servers) to access <b>disk</b> volumes on <b>storage</b> <b>arrays.</b> iSCSI SANs often have one of two objectives: ...|$|R
50|$|Symbios Logic was a {{manufacturer}} of SCSI host adapter chipsets and <b>disk</b> <b>array</b> <b>storage</b> subsystems. It was originally established as a division of NCR Corporation in 1972, before NCR's takeover by AT&T Corporation in 1991.|$|R
50|$|In 2011, EMC {{introduced}} the new VNX series of unified <b>storage</b> <b>disk</b> <b>arrays</b> intended to replace both CLARiiON and Celerra products. Internally the VNX is labeled the CX5. In early 2012, both CLARiiON and Celerra were discontinued.|$|R
5000|$|The HP Storageworks XP is a {{computer}} data <b>storage</b> <b>disk</b> <b>array</b> sold by Hewlett Packard Enterprise using Hitachi Data Systems hardware and adding their own software to it. [...] It's based on the Hitachi Virtual Storage Platform and targeted towards enabling large scale consolidation, large database, Oracle, SAP, Exchange, and online transaction processing (OLTP) environments.|$|R
40|$|Fault {{tolerance}} {{requirements for}} near term <b>disk</b> <b>array</b> <b>storage</b> systems are analyzed. The excellent reliability provided by RAID Level 5 data organization {{is seen to}} be insufficient for these systems. We consider various alternatives [...] improved MTBF and MTTR times as well as smaller reliability groups and increased numbers of check disks per group [...] to obtain the necessary improved reliability. The paper begins by introducing two data organization schemes based on maximum distance separable error correcting codes. Several figures of merit are calculated using a standard Markov failure and repair model for these organizations. Based on these results, the multiple check disk approach to improved reliability is an excellent option. 1 Introduction <b>Disk</b> <b>array</b> <b>storage</b> systems, especially those with redundant array of independent disks (RAID) Level 5 data organization [13], provide excellent cost, run-time performance as well as reliability and will {{meet the needs of}} computing systems for th [...] ...|$|R
5000|$|In 1999 EMC {{purchased}} Data General for 1.2 {{billion dollars}} primarily {{to gain access}} to its CLARiiON line of <b>disk</b> <b>array</b> <b>storage</b> products and associated software. Under the terms of the [...] "pooling of interests merger," [...] EMC maintained the server line for two years, but discontinued it as soon as the terms of the deal allowed, at which point Aviion disappeared.|$|R
50|$|TACC's {{long-term}} {{mass storage}} solution is an Oracle® StorageTek Modular Library System, named Ranch. Ranch utilizes Oracle's Sun Storage Archive Manager Filesystem (SAM-FS) for migrating files to/from a tape archival {{system with a}} current offline storage capacity of 40 PB. Ranch's disk cache is built on Oracle's Sun ST6540 and DataDirect Networks 9550 disk arrays containing approximately 110 TB of usable spinning <b>disk</b> <b>storage.</b> These <b>disk</b> <b>arrays</b> are controlled by an Oracle Sun x4600 SAM-FS Metadata server which has 16 CPUs and 32 GB of RAM.|$|R
40|$|The Los Alamos High-Performance Data System (HPDS) {{is being}} {{developed}} to meet the very large data storage and data handling requirements of a high-performance computing environment. The HPDS will consist of fast, large-capacity storage devices that are directly connected to a high-speed network and managed by software distributed in workstations. The HPDS model, the HPDS implementation approach, and experiences with a prototype <b>disk</b> <b>array</b> <b>storage</b> system are presented...|$|R
50|$|There is {{a number}} of SRM {{implementations}} in use, with varying capabilities. The Disk Pool Manager (DPM) is used for fairly small SEs with disk-based storage only, while CASTOR is designed to manage large-scale MSS, with front-end disks and back-end tape storage. dCache is targeted at both MSS and large-scale <b>disk</b> <b>array</b> <b>storage</b> systems. Other SRM implementations are in development, and the SRM protocol specification itself is also evolving.|$|R
40|$|Live {{migration}} of virtual hard <b>disks</b> between <b>storage</b> <b>arrays</b> {{has long been}} possible. However, there is a dearth of online tools to perform automated virtual disk placement and IO load balancing across multiple <b>storage</b> <b>arrays.</b> This problem is quite challenging because the performance of IO workloads depends heavily on their own characteristics {{and that of the}} underlying storage device. Moreover, many device-specific details are hidden behind the interface exposed by <b>storage</b> <b>arrays.</b> In this paper, we introduce BASIL, a novel software system that automatically manages virtual disk placement and performs load balancing across devices without assuming any support from the <b>storage</b> <b>arrays.</b> BASIL uses IO latency as a primary metric for modeling. Our technique involves separate online modeling of workloads and storage devices. BASIL uses these models to recommend migrations between devices to balance load and improve overall performance. We present the design and implementation of BASIL in the context of VMware ESX, a hypervisor-based virtualization system, and demonstrate that the modeling works well {{for a wide range of}} workloads and devices. We evaluate the placements recommended by BASIL, and show that they lead to improvements of at least 25 % in both latency and throughput for 80 percent of the hundreds of microbenchmark configurations we ran. When tested with enterprise applications, BASIL performed favorably versus human experts, improving latency by 18 - 27 %. ...|$|R
40|$|Virtual memory tiling enables {{applications}} efficiently {{to handle}} much larger arrays of spatial data than is otherwise possible, without requiring expensive additional computing resources. It has particular application to {{geographical information systems}} (GIS) with the wide availability of large sets of digital spatial data from remote sensing and other sources. The size of these data sets often greatly exceeds the capabilities of most applications on standard computer platforms. In this paper a virtual memory tiling approach is developed and implemented in the C++ language. A tiled array class with a similar syntax and usage to standard arrays is constructed which is readily integrated with existing algorithms and applications. A framework is developed for classifying operations on spatial data in terms of small and large regions. These two categories are representative of {{a broad range of}} operations on spatial data in GIS. Results show that with current levels of processing power and <b>disk</b> <b>storage,</b> processing of <b>arrays</b> in the range of hundreds of megabytes and above is possible at present on desktop platforms...|$|R
40|$|The {{excellent}} reliability {{provided by}} RAID Level 5 data organization {{has been seen}} to be insufficient for future mass storage systems. We analyze the multi-dimensional disk array {{in search of the}} necessary improved reliability. The paper begins by introducing multi-dimensional disk array data organization schemes based on maximum distance separable error correcting codes and incorporating both strings and spares. Several figures of merit are calculated using a standard Markov failure and repair model for these organizations. Based on our results, the multi-dimensional disk array organization is an excellent approach to providing improved reliability. 1 Introduction <b>Disk</b> <b>array</b> <b>storage</b> systems, especially those with redundant arrays of independent disks (RAID) Level 5 data organization [6], provide excellent cost, run-time performance as well as reliability and will meet the needs of computing systems for the immediate future. Computing systems, especially those with massive storage requ [...] ...|$|R
40|$|In this paper, {{we propose}} a highly {{reliable}} RAID architecture called a Dual-Crosshatch Disk Array. It uses the proposed interleaved 2 d-parity scheme, a low overhead triple-erasure correcting parity organization. It is a hybrid approach of RAID- 4 and RAID 5 with one dedicated parity group and another parity group using block interleaved data and parity. The results obtained from simulations {{indicate that this}} architecture possesses extremely high reliability with low overheads, good degraded performance, and acceptable normal-mode performance. 1 Introduction To {{keep pace with the}} rapidly increasing processing power, disk array architectures have been proposed that can achieve high I/O capacity and performance. <b>Disk</b> <b>array</b> <b>storage</b> systems, such as Redundant Array of Inexpensive Disks (RAID) level- 5 data organization [1], provide fault tolerance against disk drive failures, and are cost-effective and have good run-time performance. However, a storage subsystem consists of more than just d [...] ...|$|R
40|$|Redundant disk arrays provide highly-available, {{high-performance}} <b>disk</b> <b>storage</b> to a {{wide variety}} of applications. Because these applications often have distinct cost, performance, capacity, and availability requirements, researchers continue to develop new array architectures. RAIDframe was developed to assist researchers in the implementation and evaluation of these new architectures. It was designed specifically to reduce the burden of implementation by restricting code changes to mapping, algorithms and other functions that are known to be specific to an array architecture. Algorithms are executed using a general mechanism which automates the recovery from device errors, such as a failed disk read. RAIDframe enables a single implementation to be evaluated in a self-contained simulator, or against real disks as either a user process or a functional device driver. 1995, 1996, Carnegie Mellon University. All rights reserved. This research is supported in part by the National Science Foundation through the Data Storage Systems Center, an NSF engineering research center, under grant number ECD- 8907068 and an AT&T fellowship. It is also supported in part by industry members of the Parallel DataConsortium, including: Hewlett-Packard, Data General Corporation, Digital Equipment Corporation, International Business Machines, Seagate Technology, Storage Technology, and Symbios Logic. Keywords: <b>disk</b> <b>array,</b> <b>storage,</b> architecture, simulation, directed acyclic graph, software. RAIDframe: A Rapid Prototyping Tool for RAID Systems v Version 1. 0 6 / 24 / 9...|$|R
5000|$|<b>Disk</b> <b>storage</b> {{controller}} (1956 enhancement {{for then}} new IBM 355 <b>Disk</b> <b>Storage</b> Unit) (5 extra operation codes) ...|$|R
5000|$|... 2314 Storage Control Unit Model A1 - This SCU was {{announced}} initially {{as a part}} of an A Series DASF and shortly thereafter unbundled. The unbundled DASD models were the one drive 2312 <b>Disk</b> <b>Storage,</b> the four drive 2313 <b>Disk</b> <b>Storage</b> and the two drive 2318 <b>Disk</b> <b>Storage.</b> One to nine drives could be attached as in the Model A1 DASF.|$|R
5000|$|... 2314 Storage Control Unit Model B1 - Part of the B series, this SCU {{attaches}} a three drive 2319-B1 <b>Disk</b> <b>Storage</b> and optionally {{one or two}} additional, three drive 2319-B2 <b>Disk</b> <b>Storages.</b>|$|R
50|$|IBM 2311 <b>Disk</b> <b>Storage.</b>|$|R
50|$|Tegile Systems is a {{manufacturer}} of flash <b>storage</b> <b>arrays</b> based in Newark, California. Both hybrid and all-flash <b>storage</b> <b>arrays</b> use technology called IntelliFlash.|$|R
5000|$|Data compression—reducing {{transmission}} bandwidth and <b>disk</b> <b>storage</b> ...|$|R
50|$|For <b>disk</b> <b>storage,</b> MenuetOS {{supports}} the FAT32 file system.|$|R
50|$|IBM 2302 <b>Disk</b> <b>Storage,</b> Models 3, four units maximum.|$|R
50|$|IBM 2311 <b>Disk</b> <b>Storage,</b> Model 1, eight units maximum.|$|R
5000|$|Util-linux ionice (see {{manual for}} <b>Disk</b> <b>storage</b> I/O priorities) ...|$|R
50|$|REV is a {{removable}} hard <b>disk</b> <b>storage</b> system from Iomega.|$|R
5000|$|<b>Disk</b> <b>storage</b> {{for both}} {{magnetic}} and optical recording of disks ...|$|R
5000|$|<b>Disk</b> <b>storage</b> {{products}} {{that can be}} attached via FICON include: ...|$|R
5000|$|Goddard and Lynott, , [...] "Direct Access Magnetic <b>Disk</b> <b>Storage</b> Device" ...|$|R
50|$|The first {{commercial}} digital <b>disk</b> <b>storage</b> device was the IBM 350 which shipped in 1956 {{as a part}} of the IBM 305 RAMAC computing system. The random-access, low-density <b>storage</b> of <b>disks</b> was developed to complement the already used sequential-access, high-density storage provided by tape drives using magnetic tape. Vigorous innovation in <b>disk</b> <b>storage</b> technology, coupled with less vigorous innovation in tape storage, has reduced the difference in acquisition cost per terabyte between <b>disk</b> <b>storage</b> and tape storage; however, the total cost of ownership of data on disk including power and management remains larger than that of tape.|$|R
5000|$|... {{approximately}} six petabytes of raw <b>disk</b> <b>storage</b> {{with high}} input/output bandwidth; ...|$|R
