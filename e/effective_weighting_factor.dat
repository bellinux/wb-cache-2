1|5518|Public
40|$|Collaborative {{filtering}} is an algorithm successfully {{and widely}} used in recommender system. However, it suffers from data sparsity, recommendation accuracy and system scalability problems. This paper proposes an improved user model for collaborative filtering to explore a solution to these problems. The ratings are firstly been normalized by decoupling normalization method, and then a nonlinear forgetting function is introduced to assign the ratings different time weights to mimic the users ’ interest drift. In similarity computation, an <b>effective</b> <b>weighting</b> <b>factor</b> {{is added to the}} Pearson correlation similarity computation to get more accurate neighbor users. The algorithm is tested on MovieLens dataset and the comparative experiment shows that the algorithm proposed in this paper can provide a better performance...|$|E
30|$|Variation {{index is}} {{directly}} {{associated with the}} weighting system of the model. New or <b>effective</b> <b>weights</b> for each input parameters were computed using the Eqs. (3) and (4) and reported in Table  5. The <b>effective</b> <b>weight</b> <b>factor</b> results clearly indicate that the parameter D dominates the vulnerability index with an average weight of 23.84  % against the theoretical weight of 21.74  %. The actual weight of parameter I (16.77  %) is smaller than the theoretical weight (21.74). The calculated weight of parameter T (7.07  %) is greater than theoretical weight (4.35  %). The highest <b>effective</b> <b>weight</b> of parameter D clearly indicates the presence of shallow groundwater table in the most {{part of the study}} area and the calculated <b>effective</b> <b>weight</b> of parameter T is more than theoretical weight {{due to the fact that}} the slope in most of the part of the study area is < 6  %.|$|R
40|$|Fault {{tolerance}} {{has been}} a new issue for the DCT networks. This paper introduces an effective concurrent error detection scheme for 1 D-DCT networks. The design then will be expanded for 2 D-DCT networks. By employing an <b>effective</b> checksum <b>weighting</b> <b>factors,</b> the overflow probability is minimized. All errors occur in the DCT network will be detected. The error locatio...|$|R
30|$|It is {{important}} to test {{the consistency of the}} DRASTIC factors in the vulnerability assessment by validating and evaluating the sensitivity analysis in any study; the results show that the <b>effective</b> <b>weight</b> for each <b>factor</b> is different from the theoretical weights assigned by the DRASTIC model. The lack or uncertainties in recharge, impact of vadose zone, and hydraulic conductivity are the highest factors that reduce the efficiency of the vulnerability index model.|$|R
30|$|It {{is clearly}} {{observed}} in the study that the calculated <b>effective</b> <b>weights</b> for each parameter are not equal to the theoretical weight assigned in DRASTIC method. This {{is due to the}} fact that <b>weight</b> <b>factors</b> are strongly related to the value of a single parameter in the context of value chosen for the other parameters. Therefore, the determination of <b>effective</b> <b>weights</b> is very useful to revise the <b>weight</b> <b>factors</b> assigned in DRASTIC method and may be applied more scientifically to address the local issues.|$|R
30|$|Sensitivity {{analysis}} {{results indicate that}} the new <b>effective</b> <b>weights</b> for each parameter are not equal to the theoretical weight assigned in DRASTIC method. Thus, the computation of <b>effective</b> <b>weights</b> is very useful to revise the <b>weight</b> <b>factors</b> assigned in DRASTIC method and may be applied more scientifically to address the local issues.|$|R
40|$|Every bank seeks {{methods to}} {{optimize}} its assets and liabilities, thus the main subject is managing assets-liabilities {{in the balance}} sheet and the main question is by which factor banks will be enabled to have an optimized combination of assets and liabilities in a common level of risk {{to get the most}} return. This case study is dedicated to Refah bank and is an applicable study. The data has collected from the headquarter by a questionnaire and finally <b>effective</b> <b>factors</b> <b>weight</b> on optimizing bank balance sheet determined by using Fuzzy analytical hierarchy process. Results showed that revenue has more effect on optimizing for % 39. 5 and also loan to deposit ratio for %. 74, regarding revenue as a symbol of efficiency in banks, {{it seems to be the}} most important factor and goal in banking industry. Furthermore banks need to have some liquidity to respond customers demand to cover one of the most important risks of banking. This factor importance determined to be % 18 in Refah Bank by using model and experts view...|$|R
40|$|AbstractThe {{main purpose}} of this study is to develop an optimal river {{dredging}} management model by applying MCDA (Multi-Criteria Decision Analysis) technique specifically in Korea where river dredging research are scarce. This model supports the decision making by providing <b>weight</b> <b>factors</b> covering dredging cost, and social and environmental impacts. This model requires various input data which are dredging points, dredging machines, dewatering machines, disposal sites. Furthermore, for the model development, the researcher applied imaginary dredging project area for model assessment and analysis. We used a total of 5 cases of <b>weight</b> <b>factors</b> combination. It can classify <b>weight</b> <b>factors</b> combination in 2 categories. The first category is <b>weight</b> <b>factors</b> combination for single-objective optimization and another category is <b>weight</b> <b>factors</b> combination for multi-objective optimization. The single-objective optimization is same that one <b>weight</b> <b>factor</b> is 1. 0 and other <b>weight</b> <b>factor</b> is 0. 0. The multi-objective optimization is used equality <b>weight</b> <b>factors</b> combination or <b>weight</b> <b>factors</b> from previous study. According to model simulation, it shows that cost of dredging is different approximately 18 % due to <b>weight</b> <b>factors.</b> Besides, the maximum cost is obtained when <b>weight</b> <b>factor</b> of social impact is 1. 0. In contrast, minimum cost of dredging projects is discovered when <b>weight</b> <b>factor</b> of cost is 1. 0. From the <b>weight</b> <b>factors</b> in previous study, it illustrates that there are about 2 % beyond the minimum cost of dredging projects, and approximately 18 % less than maximum cost. Next study will be used real river dredging areas or simulation results of bed changes. The results of this study can be efficiently applied to 4 major rivers in Korea where periodical river dredging is necessary...|$|R
30|$|CAD-VHO {{algorithm}} {{does not}} apply any <b>weighting</b> <b>factors</b> while the other algorithm applies <b>weighting</b> <b>factors</b> to compute dwell time.|$|R
40|$|Report was {{compiled}} using actual caseloads {{in each of}} the three alternative providers adjusted for a common <b>weighting</b> <b>factor.</b> This <b>weighting</b> <b>factor</b> is the same <b>weighting</b> <b>factor</b> utilized in establishing staffing requirements and is predicated on American Bar Association and National Legal Aid and Defender Association standards...|$|R
40|$|Abstract—Predictive {{control is}} a {{powerful}} and promising control algorithm in the control of power converter and electrical machine drive’s system. The system performance depends on the selection of <b>weighting</b> <b>factor</b> in the cost function. Therefore, this paper proposed a <b>weighting</b> <b>factor</b> optimization method to reduce the torque ripple of induction motor fed by an indirect matrix converter. Also, predictive torque and flux control with conventional <b>weighting</b> <b>factor</b> is being investigated in this paper and is compared with the proposed optimum <b>weighting</b> <b>factor</b> based predictive control algorithm. The introduced <b>weighting</b> <b>factor</b> optimization method in predictive control algorithm is validated through simulation and shows potential tracking of variables and control with their corresponding references and consequently minimizes the torque ripple compare to the conventional <b>weighting</b> <b>factor</b> based predictive control method...|$|R
3000|$|Since PTS-AIC modifies the {{transmission}} block by the <b>weighting</b> <b>factors,</b> the receiver {{must be aware}} which set of <b>weighting</b> <b>factors</b> is applied {{so that it can}} recover the original symbol block. This can be done by sending the index of the optimum <b>weighting</b> <b>factor</b> as side information that amounts to [...]...|$|R
30|$|The hybrid method {{combines}} the IAHP with IE method through the <b>weight</b> <b>factor,</b> {{which is a}} value from [0, 1]. In this paper, the <b>weight</b> <b>factor</b> is 0.5, but in practical application the choice of <b>weight</b> <b>factor</b> is not determined by an absolute optimal value. Considering the EUE in practice, the enumeration method is used to determine a relatively better value of the <b>weight</b> <b>factor,</b> or the best value of the factor will be optimized by the sensitivity analysis method according to the actual application.|$|R
30|$|Low {{cohesion}} {{exists at}} an early phase of influence spreading or when nodes’ activities are low, i.e. low node <b>weighting</b> <b>factors.</b> A result {{of this is that}} corresponding pairs of time and <b>weighting</b> <b>factor</b> values can be found such that they provide comparable results. In [46] this has been demonstrated in cases of low values of time with high values of <b>weighting</b> <b>factors,</b> and high values of time with low values of <b>weighting</b> <b>factors.</b> Almost identical results are obtained for T = 1.0,w_N = 1.0 and T = 4.5,w_N = 0.5.|$|R
40|$|Maintaining {{fairness}} using <b>weighting</b> <b>factors</b> is {{a common}} approach in resource allocation. However, computing <b>weighting</b> <b>factors</b> for multiservice wireless networks is not trivial because users' rate requirements are heterogeneous and their channel gains are variable. In this paper, we propose <b>weighting</b> <b>factor</b> computation and scheduling schemes for orthogonal frequency division multiple access (OFDMA) networks. The <b>weighting</b> <b>factor</b> computation scheme determines each user's share of rate for maintaining a utility notion of fairness. We then present a scheduling scheme which takes the users' <b>weighting</b> <b>factors</b> into consideration to allocate sub-carriers and power in OFDMA networks. The simulation results demonstrate that the proposed scheduling scheme outperforms an opportunistic scheme in terms of fairness performance in different scenarios, where the users are fixed or mobile. ...|$|R
30|$|To {{solve the}} multi-objective {{problem in our}} model, lexicographic method was applied which {{optimized}} second objective function while maintaining optimality of the first objective. Meanwhile, DM determined the <b>weighting</b> <b>factor</b> (B). We avoided using analytical methods to determine the <b>weighting</b> <b>factor.</b> However, multi-criteria decision-making techniques such as analytic hierarchy process (AHP) {{could be used to}} precisely determine the <b>weighting</b> <b>factor.</b>|$|R
40|$|Abstract – This paper {{proposes a}} <b>weighting</b> <b>factor</b> {{optimization}} method in predictive control algorithm for torque ripple reduction in an induction motor fed by an indirect matrix converter (IMC). In this paper, the torque ripple behavior is analyzed {{to validate the}} proposed <b>weighting</b> <b>factor</b> optimization method in the predictive control platform and shows {{the effectiveness of the}} system. Therefore, an optimization method is adopted here to calculate the optimum <b>weighting</b> <b>factor</b> corresponds to minimum torque ripple and is compared with the results of conventional <b>weighting</b> <b>factor</b> based predictive control algorithm. The predictive control algorithm selects the optimum switching state that minimizes a cost function based on optimized <b>weighting</b> <b>factor</b> to actuate the indirect matrix converter. The conventional and introduced <b>weighting</b> <b>factor</b> optimization method in predictive control algorithm are validated through simulations and experimental validation in DS 1104 R&D controller platform and show the potential control, tracking of variables with their respective references and consequently reduces the torque ripple...|$|R
3000|$|... = 1. To {{facilitate}} {{discussion of}} the impact of the <b>weight</b> <b>factor</b> on the overall transmission power and leased time, five sets of <b>weight</b> <b>factor</b> are supposed as following, {ω [...]...|$|R
3000|$|... || 2, {{the smaller}} the <b>weighting</b> <b>factor</b> of the squared l 2 norm. So within each nonzero column, the <b>weighting</b> <b>factor</b> of the {{equivalent}} l 2 norm is kind of adaptive.|$|R
30|$|In {{the state}} {{independent}} model, propagation occurs independently of nodes’ states. The probability to receive and forward influence {{is determined by}} the time dependent probability, node <b>weighting</b> <b>factor</b> and link <b>weighting</b> <b>factor.</b>|$|R
40|$|Monte Carlo (MC) {{simulations}} of many systems, in particular those with conflicting constraints, can be considerably speeded up by using multicanonical or related methods. Some {{of these approaches}} sample with a-priori unknown <b>weight</b> <b>factors.</b> After introducing the concept, I shall focus on two aspects: (i) Opinions about the optimal choice of <b>weight</b> <b>factors.</b> (ii) Methods to get <b>weight</b> <b>factor</b> estimates, with emphasize on a multicanonical recursion. 1...|$|R
50|$|Radiation <b>weighting</b> <b>factors</b> that go from {{physical}} energy to biological effect {{must not be}} confused with tissue <b>weighting</b> <b>factors.</b> The tissue <b>weighting</b> <b>factors</b> are used to convert an equivalent dose to a given tissue in the body, to an effective radiation dose, a number that provides an estimation of total danger to the whole organism, {{as a result of the}} radiation dose to part of the body.|$|R
30|$|The single-parameter {{sensitivity}} {{analysis showed that}} the LULC parameter {{tends to be the}} most effective parameter in the vulnerability assessment (mean effective wt% is 23.2) in agreement with the result from map removal {{sensitivity analysis}}. Its <b>effective</b> <b>weight</b> exceeds the theoretical weight (15.2  %). The impact of vadose zone also shows high <b>effective</b> <b>weight</b> (18.7  %) that exceeds the theoretical weights (15.2  %). The rest of the parameters, excluding topography have lower <b>effective</b> <b>weights</b> as compared to the theoretical weights. The significance of impact of vadose zone and LULC parameters highlights the importance of obtaining accurate, detailed, and representative information about these factors.|$|R
40|$|Motivation: The {{compound}} identification in gas chromatography-mass spectrometry (GC-MS) {{is achieved}} by matching the experimental mass spectrum to the mass spectra in a spectral library. It is known that the intensities with higher m/z value in the GC-MS mass spectrum are the most diagnostic. Therefore, to increase the relative significance of peak intensities of higher m/z value, the intensities and m/z values are usually transformed {{with a set of}} <b>weight</b> <b>factors.</b> A poor quality of <b>weight</b> <b>factors</b> can significantly decrease the accuracy of compound identification. With the significant enrichment of the mass spectral database and the broad application of GC-MS, it is important to re-visit the methods of discovering the optimal <b>weight</b> <b>factors</b> for high confident compound identification. Results: We developed a novel approach to finding the optimal <b>weight</b> <b>factors</b> only through a reference library for high accuracy compound identification. The developed approach first calculates the ratio of skewness to kurtosis of the mass spectral similarity scores among spectra (compounds) in a reference library and then considers a <b>weight</b> <b>factor</b> with the maximum ratio as the optimal <b>weight</b> <b>factor.</b> We examined our approach by comparing the accuracy of compound identification using the mass spectral library maintained by the National Institute of Standards and Technology (NIST). The results demonstrate that the optimal <b>weight</b> <b>factors</b> for fragment ion peak intensity and m/z value found by the developed approach outperform the current <b>weight</b> <b>factors</b> for compound identification...|$|R
40|$|This paper {{presents}} an extended version of Partial Parallel Interference Cancellation (PPIC) called Variance reduced Partial Parallel Interference Cancellation (VRPPIC) for multicarrier code {{division multiple access}} uplink systems. The combination of PPIC receiver and new bit estimator is called VRPPIC detector. This realization is derived for the main PPIC operation and soft decision (SD) from PPIC, which are linear combination of the bit estimator using appropriate <b>Weighting</b> <b>Factors</b> (WF). These <b>weighting</b> <b>factors</b> are derived from Partial Parallel Interference Cancellation (PPIC) <b>weighting</b> <b>factors.</b> It is used to reduce the conditional variance of the final stage signal estimation. An Optimal <b>Weighting</b> <b>Factor</b> (OWF) selection algorithm has been derived for VRPPIC detector scheme, to minimize a monotonically increasing condition variance function. For all the interference cancellation stages the derived OWFs are equal and can easily be obtained from a linear function of active users. Simulation has been done in VRPPIC detector and PPIC detector using optimal <b>weighting</b> <b>factors</b> and randomly selected <b>weighting</b> <b>factors.</b> The result shows that the VRPPIC with OWFs outperforms both VRPPIC and PPIC with randomly selected <b>weighting</b> <b>factors.</b> Also it is shown in this paper that if there are multiple antennas at the receiving end, the performance of the detector is further improved. This leads to generalization that if data comes from single stream (singl...|$|R
40|$|Abstract—This paper {{presents}} a novel algorithm for least squares (LS) estimation of both stationary and nonstationary signals which arise from Volterra models. The algorithm concerns the recursive implementations {{of the method}} of LS which usually have a <b>weighting</b> <b>factor</b> in the cost function. This <b>weighting</b> <b>factor</b> enables nonstationary signal models to be tracked. In particular, {{the behavior of the}} <b>weighting</b> <b>factor</b> is known to influence the performance of the LS estimation. However, there are certain constraints on the <b>weighting</b> <b>factor.</b> In this paper, we have refor-mulated the LS estimation with the commonly used exponential <b>weighting</b> <b>factor</b> as a constrained optimization problem. Specif-ically, we have addressed this constrained optimization using the Lagrange programming neural networks (LPNN’s) thereby enabling the <b>weighting</b> <b>factor</b> to be adapted. The utility of our adaptive weighted least squares (AWLS) algorithm is demon-strated in the context of Volterra signal modeling in stationary and nonstationary environments. By using the Kuhn–Tucker conditions, all the LS estimated parameters may be shown to be optimal. Index Terms—Least square estimation, neural networks, non-linear signal modeling, Volterra system. I...|$|R
50|$|The ICRP tissue <b>weighting</b> <b>factors</b> {{are chosen}} to {{represent}} the fraction of health risk, or biological effect, which is attributable to the specific tissue named. These <b>weighting</b> <b>factors</b> have been revised twice, {{as shown in the}} chart above.|$|R
30|$|The <b>weighting</b> <b>factors</b> (b 1 and b 2 in (1)) {{for each}} image {{representation}} are applied equally {{to all of}} the ‘channels’. Since some of the channels are more robust than others, like in the case of HSV for example, each channel should have its own <b>weighting</b> <b>factor.</b> Since this study allows us to cut down the number of useful representations, we propose to study the best behaving ones in more detail with separate <b>weighting</b> <b>factors</b> where needed.|$|R
40|$|Weighting in {{life cycle}} {{assessment}} (LCA) is a controversial subject due to its dependence upon value judgements. One {{of the issues that}} has to be dealt with is whose values that shall be reflected. In a democratic society, a fair candidate to put forward would be the government. In environmental economics taxes and fees set up by governments to protect natural resources and the environment are sometimes used as an approximation of the monetary value of these assets. In this master thesis a new set of <b>weighting</b> <b>factors</b> for LCA, based on Swedish environmental taxes and fees will be presented. <b>Weighting</b> <b>factors</b> are derived for the most commonly used impact categories. The <b>weighting</b> <b>factors</b> are also combined with different sets of characterisation factors, in order to obtain one-step <b>weighting</b> <b>factors</b> that may be applied directly on inventory data. An estimation of the uncertainty in the valuation is obtained when alternative <b>weighting</b> <b>factors</b> are combined with different sets of characterisation factors. Content...|$|R
40|$|Due to the {{character}} of the original source materials and the nature of batch digitization, quality control issues may be present in this document. Please report any quality issues you encounter to digital@library. tamu. edu, referencing the URI of the item. Includes bibliographical references. This study assesses gender effects on radiation dose <b>weighting</b> <b>factors</b> used in internal dosimetry. A <b>weighting</b> <b>factor</b> represents the relative contribution of a particular organ or tissue to the total detriment due to the effects from uniform whole body irradiation. <b>Weighting</b> <b>factors</b> are used in the calculation of effective dose formerly called the effective dose equivalent. The 1990 recommendations of International Commission on Radiological Protection (ICRP) Publication 60 introduce a set of <b>weighting</b> <b>factors</b> based on total detriment which includes the probability of attributable fatal cancer, the weighted probability of attributable non-fatal cancer, the weighted probability of severe hereditary effects, and the relative length of life lost. The ICRP method used to determine <b>weighting</b> <b>factors</b> involves averaging the male and female data for five national populations for each component that comprises the total detriment. This research developed a separate set of <b>weighting</b> <b>factors</b> for males and females using United States population data and methods based on ICRP Publication 60. A definite difference between the male and female <b>weighting</b> <b>factors</b> was discovered, but it is was hard to state a justifiable difference due to the many uncertainties involved in the process. Until more research is performed and many of the uncertainties are more firmly established, it is not appropriate to decide whether two sets of <b>weighting</b> <b>factors</b> for the sexes should be used in internal dosimetry assessments...|$|R
40|$|We propose path {{integral}} description for quantum {{mechanical systems}} on compact graphs consisting of N {{segments of the}} same length. Provided the bulk Hamiltonian is segment-independent, scale-invariant boundary conditions given by self-adjoint extension of a Hamiltonian operator {{turn out to be}} in one-to-one correspondence with N × N matrix-valued <b>weight</b> <b>factors</b> on the path integral side. We show that these <b>weight</b> <b>factors</b> are given by N-dimensional unitary representations of the infinite dihedral group. Comment: 13 pages, 14 figures; typos corrected, references added, discussion of <b>weight</b> <b>factors</b> improve...|$|R
40|$|Equity {{considerations}} may {{justify the}} use of <b>weight</b> <b>factors</b> when estimating the costs of climate change. This paper reviews different <b>weight</b> <b>factors</b> {{that have been used}} in the climate economics literature. Based on a simple model, it is shown that although the different <b>weight</b> <b>factors</b> imply substantially different cost-damage estimates, they actually yield the same optimal emission reductions. This paradox is {{explained by the fact that}} some of the approaches require that also the abatement costs are weighted – and this offsets the effect of the diverging cost-damage estimates. The model is then used to analyse the importance weighting may have on the overall cost-benefit analysis. At present, when most of the global emissions of (fossil) CO 2 originate from the industrialised countries, the global optimal emissions are considerably lower if costs are weighted. However, the more the emissions in developing countries grow, the less important becomes the introduction of <b>weight</b> <b>factors</b> in cost-benefit analysis of climate change for the global emission reductions, in the model developed here. On a regional level, the introduction of <b>weight</b> <b>factors</b> continues to play an important role, implying substantially lower emissions in the rich region and slightly higher (!) in the poor. Copyright Kluwer Academic Publishers 1999 climate change, cost-benefit analysis, developing countries, value of a statistical life, <b>weight</b> <b>factors,...</b>|$|R
40|$|Graduation date: 2010 Tissue <b>weighting</b> <b>factors,</b> w[subscript T], used {{to convert}} {{equivalent}} dose to effective dose, account {{for differences in}} radiosensitivity of various organs {{in the human body}} and allow users to compare individual organ detriment to whole body risk. They are explicitly calculated in ICRP 26, 60, and 103, although the methods of calculation, as well as the <b>weighting</b> <b>factors</b> themselves, vary greatly between reports. The ICRP 26 report bases its <b>weighting</b> <b>factors</b> solely on fatal cancer risk for eleven organs, whereas ICRP 60 and 103 use detriment (with unique definitions in each report) to calculate the tissue <b>weighting</b> <b>factors</b> for twenty‐two and twenty‐eight organs, respectively. Each new report introduces levels of uncertainty into the calculation of w[subscript T], which ultimately may reduce the significance of individual <b>weighting</b> <b>factors.</b> A review of the three calculational methods used in ICRP 26, 60, and 103 is covered, as well as an examination of the uncertainties introduced by each <b>weighting</b> <b>factor</b> parameter utilized in ICRP 103 and the total uncertainties of the various w[subscript T] values. Finally, the effective dose is calculated for several exposure scenarios using the ICRP 26 and 60 methodologies for comparison to ICRP 103 effective dose calculation and uncertainties thereof...|$|R
30|$|Finding {{the best}} <b>weighting</b> <b>factor</b> for fusion.|$|R
40|$|The <b>effective</b> <b>weight</b> of rats was {{manipulated}} by centrifugation. Two <b>effective</b> <b>weight</b> levels were obtained. In three escape avoidance conditions a lever press produced {{a change from}} a base level of 2. 1 g to a response level of 1. 1 g. In a punishment condition a response produced a change from a 1. 1 g level to a 2. 1 g level and in an extinction condition responses {{had no effect on}} the 2. 1 g <b>effective</b> <b>weight</b> level present. All changes took 30 sec and were maintained for an additional 10 sec before a return to base level was initiated. When responses occurred closer together than the 40 sec, they delayed the return to base level by 40 sec. This 40 sec interval is referred to as response-contingent-time. The response rate and amount of response-contingent-time served as the data. The results confirmed previous data that centrifugation is aversive. The results are interpreted as indicating that the aversiveness is attributable to the increase in <b>effective</b> <b>weight,</b> and that rats can discriminate the different angular velocity-radius of rotation combinations used...|$|R
40|$|The {{behavior}} of an iterative decoding algorithm for a code defined on a graph with cycles and a given decoding schedule {{is characterized by}} a cycle-free computation tree. The pseudocodewords of such a tree are the words that satisfy all tree constraints; pseudocodewords govern decoding performance. Wiberg [12] determined the <b>effective</b> <b>weight</b> of pseudocodewords for binary codewords on an AWGN channel. This paper extends Wiberg's formula for AWGN channels to nonbinary codes, develops similar results for BSC and BEC channels, and gives upper and lower bounds on the <b>effective</b> <b>weight.</b> The 16 -state tail-biting trellis of the Golay code [2] is used for examples. Although in this case no pseudocodeword is found with <b>effective</b> <b>weight</b> less than the minimum Hamming weight of the Golay code on an AWGN channel, it is shown by example that the minimum <b>effective</b> pseudocodeword <b>weight</b> can be less than the minimum codeword weight...|$|R
