0|23|Public
5000|$|I/O: <b>Joystick</b> and console <b>switch</b> IO {{handled by}} 6532 RIOT and TIA ...|$|R
50|$|The Joyport was a {{white plastic}} brick {{about the size of}} a {{paperback}} novel that connected to the standard (internal) Apple II gameport and broke it out into two Atari joystick ports and two Apple ports. A switch in the centre controlled whether to activate the paddles or the <b>joysticks.</b> Another <b>switch</b> could disable either the left- or right-side jacks (or neither).|$|R
40|$|D. Tech. Electrical Engineering. Proposes a vision-based {{solution}} for intention {{recognition of a}} person from the motions {{of the head and}} the hand This solution is intended to be applied in the context of wheelchair bound individuals whose intentions of interest are the wheelchairs direction and speed variation indicated by a rotation and a vertical motion respectively. Both head-based and hand-based solutions are proposed as an alternative to solutions using <b>joysticks</b> pneumatic <b>switches,</b> etc...|$|R
40|$|PURPOSE: Young {{children}} with severe motor, cognitive, and communication deficits are often dismissed as either too young or too physically involved {{to use the}} common power mobility options such as power wheelchair, ride-on-toys for lack of external support necessary to ensure safety and effective access to motion initiators (<b>joystick</b> or <b>switch).</b> Given that self-initiated locomotion is critical {{in the development of}} you children’s perceptual and social skills, the Play 2 ̆ 6 Mobility Device affords an opportunity for them to safely explore power mobility while providing sufficient external support to optimize a child’s safety, posture and access to motion initiators. PROCEDURES: The Play 2 ̆ 6 Mobility Device was designed and built following medical device design procedure, which takes customer’s requirements (sponsor’s in our case) as inputs, develops the design and then the device using engineering know-how and skills, and finally validates the performance of the device through test runs by the customer. Safety, power mobility, maneuverability and control components constitute this device. Small size and maneuverability of the device, allows its transportation and storage without any special requirement. OUTCOME: A child can drive and control the device through the <b>joystick</b> or <b>switch</b> as interface. Parent or attendant of the child can set the speed of the device and override child’s control as safe. Because of the use of FDA licensed, crush tested car seat with 5 -point harness system, the child’s posture remains secured. Use of a universal arm to hold the <b>joystick</b> or <b>switch</b> ensures any child’s convenient access to these motion initiators. IMPACT: Building the Play 2 ̆ 6 Mobility Device utilized the involved students’ electromechanical and biomedical design know-how. The process got them through the regulator’s manufacturing guidelines, licensing and patenting procedures. Engineering discipline enriched its footprints with the contribution of a performing device to the real field of pediatric physiotherapy...|$|R
40|$|Children with {{cerebral}} palsy access powered mobility {{through a variety of}} means such as <b>joystick,</b> <b>switching</b> and scanning. However, access tends to focus on physical aspects - such as where to place the joystick, what type of switch, and whether hand or head movements are most effective. When considering driving using a scanning system a range of pre-requisite skills is necessary relating to physical ability, cognitive function and visual perception to name a few. However, it is unclear as to precisely what those skills are and how each should be objectively measured. Provision appears to be based largely on clinical experience and success judged the same way. At present, the process in which the child with {{cerebral palsy}} develops the skills, and the steps to gain those skills to access powered mobility through scanning is uncertain. It is not easy to objectively predict if a person using a switch scanning system will be a successful and safe wheelchair driver. This presentation will present a summary of the current literature as to powered mobility access through scanning, as well as proposing a research project to objectively measure success and the steps to success...|$|R
50|$|The Analog <b>Joystick</b> has a <b>switch</b> {{to select}} either Analog or Digital mode. When in the Digital mode, both sticks {{function}} as the gamepad on a regular PS1 controller. Older PS1 games that {{do not support the}} PS1 DualShock sticks can work with the Analog Joystick.|$|R
50|$|The {{original}} TrackIR {{product was}} an affordable assistive technology device used for Windows cursor control. An early customer provided feedback to NaturalPoint {{that the product}} {{could be used in}} some flight simulators. This prompted NaturalPoint to re-brand the original device as SmartNav and launch a new TrackIR with improved gaming features to specifically target the flight simulation market. TrackIR was embraced by PC flight simulation enthusiasts who saw it as a better alternative to a <b>joystick</b> hat <b>switch</b> for view control. The technology used in SmartNav and TrackIR was later extended to another brand called OptiTrack, creating a more affordable professional motion capture solution.|$|R
40|$|This paper {{describes}} ThumbTEC, a novel {{general purpose}} input device for the thumb or finger that {{is useful in}} a wide variety of applications from music to text entry. The device is made up of three switches in a row and one miniature joystick on top of the middle switch. The combination of <b>joystick</b> direction and <b>switch(es)</b> controls what note or alphanumeric character is selected by the finger. Several applications are detailed...|$|R
50|$|The fidget cube has six sides {{that have}} {{different}} sensory stimuli mechanisms, that involve buttons discs, balls and cogs, <b>switches,</b> <b>joysticks,</b> and dents. Each face {{has been given}} a name based on their features. === Buttons === Usually the cube has five buttons arranged with one button at each corner and one {{in the middle of the}} face. Some of these buttons will make sounds when pressed to stimulate auditory sensors as well as touch sensors in the fingers.|$|R
40|$|Assistive {{devices are}} now {{available}} that allow persons with severe physical disabilities to complete tasks independently. When the user has severe physical limitations, it may be advantageous to have an integrated control system where a single control interface (e. g., <b>joystick,</b> head <b>switches,</b> voice recognition system, keypad) is used to operate two or more assistive devices (e. g., power wheelchairs, augmentative communication devices, computers, environmental control units, and other devices that are controlled electronically). The advantages of integrated control are that persons with limited motor control can access several devices with one access site without assistance, and the user {{does not need to}} learn a different operating mechanism for each device. The purpose of this review is to convey the depth and breadth of the research that has been conducted on integrated control systems, as well as to provide some insights into future directions. We reviewed research works pertaining to communication and environmental control, computer access, and wheelchair guidance systems. Information gathered in this study will help people become fully aware of the status of contemporary integrated control technology in order to increase {{the quality of life of}} people who use electronic assistive devices. © 2003 RESNA...|$|R
50|$|The Atari 2600 version plays {{somewhat}} differently due to {{the fact}} the energy shield is always up. The rules are almost identical. It uses either a joystick or a trackball, although the rules book supplied with the game only mentions use of a <b>joystick.</b> The difficulty <b>switches</b> set the sensitivity of the stick or ball, which can yield a control scheme very {{similar to that of the}} arcade. There are no voice overs, and the bonus counter is invisible. Additionally, the bonus counter can only hold so many points, after which points are added directly to the player's score.Players are not given the option to start with seven lives.A bug in this version causes the collision detection to be erratic near the entry to the bonus chambers.|$|R
50|$|The Germans {{developed}} an electrical two-axis joystick around 1944. The device {{was used as}} part of the Germans' Funkgerät FuG 203 Kehl radio control transmitter system used in certain German bomber aircraft, used to guide both the rocket-boosted anti-ship missile Henschel Hs 293, and the unpowered pioneering precision-guided munition Fritz-X, against maritime and other targets. Here, the joystick of the Kehl transmitter was used by an operator to steer the missile towards its target. This <b>joystick</b> had on-off <b>switches</b> rather than analogue sensors. Both the Hs 293 and Fritz-X used FuG 230 Straßburg radio receivers in them to send the Kehl's control signals to the ordnance's control surfaces. A comparable joystick unit was used for the contemporary American Azon steerable munition, strictly to laterally steer the munition in the yaw axis only.|$|R
50|$|Some {{people may}} not be able to use a {{conventional}} input device, such as the mouse or the keyboard, therefore, it is important for software functions to be accessible using both devices. Ideally, software will use a generic input API that permits the use even of highly specialized devices unheard of at the time of software's initial development. Keyboard shortcuts and mouse gestures are ways to achieve this access, as are more specialized solutions, including on-screen software keyboards and alternate input devices (<b>switches,</b> <b>joysticks</b> and trackballs). Users may enable a bounce key feature, allowing the keyboard to ignore repeated presses of the same key. Speech recognition technology is also a compelling and suitable alternative to conventional keyboard and mouse input as it simply requires a commonly available audio headset.|$|R
40|$|Communication is made {{possible}} for disabled individuals {{by means of an}} electronic system, developed at Stanford University's School of Medicine, which produces highly intelligible synthesized speech. Familiarly known as the "talking wheelchair" and formally as the Versatile Portable Speech Prosthesis (VPSP). Wheelchair mounted system consists of a word processor, a video screen, a voice synthesizer and a computer program which instructs the synthesizer how to produce intelligible sounds in response to user commands. Computer's memory contains 925 words plus a number of common phrases and questions. Memory can also store several thousand other words of the user's choice. Message units are selected by operating a simple <b>switch,</b> <b>joystick</b> or keyboard. Completed message appears on the video screen, then user activates speech synthesizer, which generates a voice with a somewhat mechanical tone. With the keyboard, an experienced user can construct messages as rapidly as 30 words per minute...|$|R
40|$|Controlling the {{movements}} of mobile robots, including driv-ing the robot through the world and panning the robot’s cam-eras, typically requires many physical <b>joysticks,</b> buttons, and <b>switches.</b> Operators will often employ a technique called “chording ” {{to cope with this}} situation. Much like a piano player, the operator will simultaneously actuate multiple joy-sticks and switches with his or her hands to create a combina-tion of complimentary movements. However, these controls are in fixed locations and unable to be reprogrammed eas-ily. Using a Microsoft Surface multi-touch table, we have designed an interface that allows chording and simultaneous multi-handed interaction anywhere that the user wishes to place his or her hands. Taking inspiration from the biome-chanics of the human hand, we have created a dynamically resizing, ergonomic, and multi-touch controller (the DREAM Controller). This paper presents the design and testing of this controller with an iRobot ATRV-JR robot...|$|R
40|$|PURPOSE: Children {{with severe}} motor, cognitive, and {{communication}} deficits have limited self-mobility skills. Our Power Wheelchair Trainer (Trainer) {{provides an opportunity}} for children to safely explore power mobility and their environment. SUBJECTS: Three children (ages 18, 7, and 7 years) with cerebral palsy at Gross Motor Function Classification System levels IV or V. METHODS AND MATERIALS: The Trainer allows a manual wheelchair to be temporarily converted to a power wheelchair permitting children to practice power mobility in their own chair. The control panel interfaces with traditional <b>joystick</b> or <b>switch</b> use. Examination procedures included the Caregiver Priorities 2 ̆ 6 Child Health Index of Life with Disabilities (CPCHILD), the Power Mobility Screen, the Pediatric Evaluation of Disability Inventory Computer Adaptive Test, and assessment of power access options using <b>switches</b> or a <b>joystick.</b> Individualized interventions focused on structured repetition of mobility tasks and opportunities for self-directed exploration. Frequency and duration ranged from 30 - 60 minutes, 2 - 3 times per week for up to 12 weeks. ANALYSES: Beginning sessions were characterized by accidental activation. As familiarity increased, independent purposeful activation increased and movement exploration emerged. RESULTS: Increases in purposeful stops and obstacle avoidance were observed. All cases demonstrated improvements on the Power Mobility Screen and 2 on the CPCHILD. All participants’ parents reported incidental benefits in contentment and engagement. CONCLUSIONS: With consistent, repetitive practice, participants demonstrated improvements in environmental exploration while simultaneously improving skills for power mobility. Future research includes the development of child-centered instructional methods and valid assessment instruments to optimize use of the Trainer...|$|R
40|$|In the Fall of 1993, NASA Ames {{deployed}} {{a modified}} Phantom S 2 Remotely-Operated underwater Vehicle (ROV) into an ice-covered sea environment near McMurdo Science Station, Antarctica. This deployment {{was part of}} the antarctic Space Analog Program, a joint program between NASA and the National Science Foundation to demonstrate technologies relevant for space exploration in realistic field setting in the Antarctic. The goal of the mission was to operationally test the use of telepresence and virtual reality technology in the operator interface to a remote vehicle, while performing a benthic ecology study. The vehicle was operated both locally, from above a dive hole in the ice through which it was launched, and remotely over a satellite communications link from a control room at NASA's Ames Research Center. Local control of the vehicle was accomplished using the standard Phantom control box containing <b>joysticks</b> and <b>switches,</b> with the operator viewing stereo video camera images on a stereo display monitor. Remote control of the vehicle over the satellite link was accomplished using the Virtual Environment Vehicle Interface (VEVI) control software developed at NASA Ames. The remote operator interface included either a stereo display monitor similar to that used locally or a stereo head-mounted head-tracked display. The compressed video signal from the vehicle was transmitted to NASA Ames over a 768 Kbps satellite channel. Another channel was used to provide a bi-directional Internet link to the vehicle control computer through which the command and telemetry signals traveled, along with a bi-directional telephone service. In addition to the live stereo video from the satellite link, the operator could view a computer-generated graphic representation of the underwater terrain, modeled from the vehicle's sensors. The virtual environment contained an animate graphic model of the vehicle which reflected the state of the actual vehicle, along with ancillary information such as the vehicle track, science markers, and locations of video snapshots. The actual vehicle was driven either from within the virtual environment or through a telepresence interface. All vehicle functions could be controlled remotely over the satellite link...|$|R
40|$|Computers and {{computer-based}} {{technology have}} {{become an integral part}} of the lives of many individuals with disabilities. One of the most common activities that can be computer assisted is the generation of text. People who cannot accurately control their extremities (due to disabilities such as cerebral palsy and spinal cord injury) use computers as writing tools. People whose physical disability restricts their spoken output may use a computer as a communication prosthesis. In both cases, the generation of text is a necessary activity that can be physically demanding. It should be made as easy for the user as possible. While the standard computer keyboard is an efficient interface for able-bodied people and some disabled people, it may present significant access problems for others. In these cases some alternative interface is necessary. Virtual Keyboards Alternative interfaces for generating text, which we call virtual keyboards, can be modeled as having two components: a physical interface which accesses elements from a language set. The physical interface consists of the sensors and/or devices that the user physically interacts with (e. g., <b>switch,</b> <b>joystick).</b> The language set is a structured collection of linguistic units (e. g., letters, words, phrases) that the user selects from. The virtual keyboard is functionally defined by th...|$|R
40|$|A {{low-cost}} {{electronic wheelchair}} {{was designed and}} developed which can perform the similar functions and features as a commercially available wheelchair. It also provides obstacle avoidance capability as added value. The electronic wheelchair was   realized by modification of a lightweight manual wheelchair. It uses two electric motors each of 320 W 24 V DC, 5 - 24 VDC 6 A H-bride drivers, and a 12 V 17 Ah rechargeable lead acid battery. It equipped with <b>switches,</b> <b>joystick,</b> infrared sensors and ultrasonic sensors. A Gizduino AtMega 328 microcontroller is used to read and interpret commands. User’s acceptance evaluation results shows that the developed low - cost wheelchair is able to receive and interpret commands provided by the joystick, detect if a person  is seated on it, navigate to avoid obstacles {{as well as to}} detect edge and stairs. Technical evaluation result shows that on a flat surface it could move at the speed of around 39. 9 m/minute without load and 32 m/minute with 80 kg load. At 10 degrees inclined surface, the maximum weight limit is 30 kg with the speed of 12 m/min ute. At 20 degrees inclined surface, the maximum weight limit is 1 0 kg with the speed of 3 m/min ute. Regarding cost, it is just a fraction of a cost compared to the commercially available model. Therefore, the developed wheelchair offers an option for potential users who cannot afford to buy the commercially available one. </span...|$|R
40|$|Poster project {{completed}} at Wichita State University, Department of Electrical Engineering and Computer Science. BioKansas Winner. Presented at the 12 th Annual Capitol Graduate Research Summit, Topeka, KS, February 12, 2015. We propose an effective model for ALS patients {{to enhance their}} mobility and to increase the possibilities of interaction such as talking and sending an email. The model additionally caters to their everyday requirements empowering a simple cooperation without controls such as <b>joysticks,</b> buttons and <b>switches.</b> Here, we design a talk back system and exoskeleton interfaced to the human brain using Brain Neural Computer Interface (BNCI). The system converts EEG signals generated by human brain to electronic commands that are processed by the computer. The logic unit then differentiates between a control and a communication signal. Accordingly, {{the output of the}} logic unit is directed either to the exoskeleton or the text to speech converter. The subjects are initially trained to ensure maximum effectiveness of the system. During the training phase, EEG signals are collected while they work on some common tasks, such as concentrating on various objects and moving the right hand. Once this training is accomplished and maximum effectiveness is ensured, the system is deployed to the patient. After deployment the control unit compares with predefined logic and is used to control legs and limbs fixtures automatically. The system uses non-invasive EEG electrodes placed on the scalp to detect and receive brain signals. In addition to EEG electrodes, force detecting sensors are placed on the body to maintain stability (just like a Segway). A virtual gaming software based system where subjects learns controlling in a gaming environment can also be developed...|$|R
40|$|Presented to the 11 th Annual Symposium on Graduate Research and Scholarly Projects (GRASP) {{held at the}} Heskett Center, Wichita State University, April 24, 2015. Research {{completed}} at Department of Electrical Engineering and Computer Science, College of EngineeringIn this project we execute an effective model for ALS patients to enhance their mobility and increase the possibilities of interaction such as talking, sending an email, and playing video games. The model additionally caters to their everyday requirements empowering a simple cooperation without controls such as <b>joysticks,</b> buttons and <b>switches.</b> In this model we design a talk back system and exoskeleton interfaced to the human brain using Brain Neural Computer Interface (BNCI). The system converts EEG signals generated by the human brain to electronic commands that are processed by the computer. The logic unit then differentiates between a control signal and a communication signal. Accordingly, {{the output of the}} logic unit is directed either to the exoskeleton or the text to speech converter. The subjects are initially trained to ensure maximum effectiveness of the system. In the training, signals are collected while they concentrate on various objects. Secondly, they are given a simple task to be accomplished, such as thinking about a word, moving right hand finger, etc. Once the effectiveness is ensured, the system is deployed to the patient. The control unit is responsible to control the whole system. After deployment the control unit compares with predefined logic and is used to control legs and limbs fixtures automatically. The system uses non-invasive EEG-electrodes placed on the scalp to detect and receive brain signals. In addition to EEG electrodes, force detecting sensors are placed on the body to maintain stability. For example if the volunteer leans forward, the sensor detects and moves forward (just like a segway). In addition to the system we also design a virtual gaming software where volunteer learns how to move and control the actuators by practicing them using a gaming environment. Graduate School, Academic Affairs, University Librarie...|$|R
40|$|Introduction: Electronic Assistive Technology (EAT) aims {{to empower}} {{individuals}} with disabilities by reducing the environmental and societal barriers they encounter. Power wheelchairs aid mobility, communication aids allow for more efficient conversation, environmental controls permit greater autonomy, and personal computers provide access to information, social networking and educational activities. In order to control EAT, a computer input device is necessary. Mice and keyboards are typically used but in certain cases, Special Access Technology (SAT) is required. SAT refers to adapted and alternative computer input devices that are utilised when mainstream software and hardware are not suitable. Examples are <b>switches,</b> <b>joysticks</b> and screen-scanning software. Unfortunately, high costs can hinder access to many types of assistive technology (AT), including SAT, and even when this is overcome, device abandonment can occur. Abandonment {{has been linked to}} inappropriate product design leading to devices that are difficult to use, fail during use, and have poor aesthetics resulting in the user feeling stigmatised. This research proposes and explores ways of improving the design process of AT. It focuses on user participation and ties in the theory of mass customisation as a tool for increasing market size and improving the adaptability of AT. Method: This paper describes the first of two phases which compose a framework for the participatory design of customisable AT. The practical development of a SAT hardware device scaffolds the research process. This initial phase involves professionals working with AT users who have disabilities and employs a Delphi study to establish consensus on important design issues relating to a specific type of AT, in this case SAT. An adapted morphological matrix is also described and shows how the results of the Delphi study can be practically applied during the concept generation phase of the product design process. Findings: A panel of 14 professionals participated in the Delphi study, with 64 % (n= 9) working in the Republic of Ireland, and 36 % (n= 5) in Northern Ireland. Occupational therapists, physiotherapists, speech & language therapists, AT trainers and engineers took part. The process resulted in consensus on; 1) prevalent parts of SAT that malfunction, 2) primary reasons for SAT malfunction, 3) characteristics of a client which are associated with SAT selection, 4) client needs regarding SAT use and training, 5) desirable traits of SAT, and 6) clinicians’ frustrations with SAT. Conclusion: The study reveals a wide range of problems related to SAT and also highlights the complexities of finding the right type of SAT for a particular individual. Each list of design issues resulting from the six questions provides different insights and drives the concept generation phase. The study demonstrates that a structured approach aids in the efficient generation and application of expert users’ knowledge during the design process of customisable AT...|$|R
40|$|A human's {{ability to}} perform {{physical}} tasks is limited, not only by his intelligence, but by his physical strength. If, in an appropriate environment, a machine's mechanical power is closely integrated with a human arm's mechanical power {{under the control of}} the human intellect, the resulting system will be superior to a loosely integrated combination of a human and a fully automated robot. Therefore, we must develop a fundamental solution to the problem of 'extending' human mechanical power. The work presented here defines 'extenders' as a class of robot manipulators worn by humans to increase human mechanical strength, while the wearer's intellect remains the central control system for manipulating the extender. The human, in physical contact with the extender, exchanges power and information signals with the extender. The aim is to determine the fundamental building blocks of an intelligent controller, a controller which allows interaction between humans and a broad class of computer-controlled machines via simultaneous exchange of both power and information signals. The prevalent trend in automation has been to physically separate the human from the machine so the human must always send information signals via an intermediary device (e. g., <b>joystick,</b> pushbutton, light <b>switch).</b> Extenders, however are perfect examples of self-powered machines that are built and controlled for the optimal exchange of power and information signals with humans. The human wearing the extender is in physical contact with the machine, so power transfer is unavoidable and information signals from the human help to control the machine. Commands are transferred to the extender via the contact forces and the EMG signals between the wearer and the extender. The extender augments human motor ability without accepting any explicit commands: it accepts the EMG signals and the contact force between the person's arm and the extender, and the extender 'translates' them into a desired position. In this unique configuration, mechanical power transfer between the human and the extender occurs because the human is pushing against the extender. The extender transfers to the human's hand, in feedback fashion, a scaled-down version of the actual external load which the extender is manipulating. This natural feedback force on the human's hand allows him to 'feel' a modified version of the external forces on the extender. The information signals from the human (e. g., EMG signals) to the computer reflect human cognitive ability, and the power transfer between the human and the machine (e. g., physical interaction) reflects human physical ability. Thus the information transfer to the machine augments cognitive ability, and the power transfer augments motor ability. These two actions are coupled through the human cognitive/motor dynamic behavior. The goal is to derive the control rules for a class of computer-controlled machines that augment human physical and cognitive abilities in certain manipulative tasks...|$|R

