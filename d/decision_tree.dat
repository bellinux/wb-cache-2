10000|4945|Public
5|$|Qualitative {{models are}} {{occasionally}} used. One example is qualitative scenario planning in which possible future events are played out. Another example is non-numerical <b>decision</b> <b>tree</b> analysis. Qualitative models often suffer {{from lack of}} precision.|$|E
5|$|In 2005, {{data mining}} tools like a <b>decision</b> <b>tree</b> and neural net were added with version 5 {{as well as}} Linux support, which was later {{withdrawn}} in JMP 9. Later in 2005, JMP 6 was introduced. JMP began integrating with SAS in version 7.0 in 2007 and has strengthened this integration ever since. Users can write SAS code in JMP, connect to SAS servers, and retrieve and use data from SAS. Support for bubble plots was added in version 7. JMP 7 also improved data visualization and diagnostics.|$|E
25|$|An {{alternating}} <b>decision</b> <b>tree</b> (ADTree) is {{a machine}} learning method for classification. It generalizes decision trees and has connections to boosting.|$|E
40|$|Abstract. For the {{well-known}} concept of <b>decision</b> <b>trees</b> {{as it is}} used for inductive inference we study the natural concept of equivalence: two <b>decision</b> <b>trees</b> are equivalent {{if and only if}} they represent the same hypothesis. We present a simple e cient algorithm to establish whether two <b>decision</b> <b>trees</b> are equivalent or not. The complexity of this algorithm is bounded by the product of the sizes of both <b>decision</b> <b>trees.</b> The hypothesis represented by adecision tree is essentially a boolean function, just like a proposition. Although every boolean function can be represented in this way, we show that disjunctions and conjunctions of <b>decision</b> <b>trees</b> can not e ciently be represented as <b>decision</b> <b>trees,</b> and simply shaped propositions may require exponential size for representation as <b>decision</b> <b>trees.</b> ...|$|R
5000|$|Linear <b>decision</b> <b>trees,</b> {{just like}} the simple <b>decision</b> <b>trees,</b> make a {{branching}} decision based {{on a set of}} values as input. As opposed to binary <b>decision</b> <b>trees,</b> linear <b>decision</b> <b>trees</b> have three output branches. A linear function [...] is being tested and branching decisions are made based on the sign of the function (negative, positive, or 0).|$|R
40|$|We {{study the}} {{relationships}} between the complexity of a task description and the minimal complexity of deterministic and nondeterministic <b>decision</b> <b>trees</b> solving this task. We investigate <b>decision</b> <b>trees</b> assuming a global approach i. e. arbitrary checks from a given check system can be used for constructing <b>decision</b> <b>trees.</b> Introduction <b>Decision</b> <b>trees</b> are widely used in different fields related to problem solving and knowledge representation. <b>Decision</b> <b>trees</b> over finite check systems are studied in such fields as test theory [3, 6, 7], theory of information systems and of rough sets [11, 15], theory of questionnaires [12], theory of decision tables [4], machine learning [14], and searching theory [1, 16]. <b>Decision</b> <b>trees</b> over infinite check systems have not been intensively investigated, with the exception of linear and algebraic <b>decision</b> <b>trees</b> [2, 5, 6] and some their generalizations [8]. Linear and algebraic <b>decision</b> <b>trees</b> are often used in computational geometry [13]. Furthermore in [...] ...|$|R
25|$|Decision {{complexity}} {{of a game}} {{is the number of}} leaf nodes in the smallest <b>decision</b> <b>tree</b> that establishes the value of the initial position.|$|E
25|$|Zhang, J. and Honavar, V. (2003). Learning <b>Decision</b> <b>Tree</b> Classifiers from Attribute Value Taxonomies and Partially Specified Data. In: Proceedings of the International Conference on Machine Learning (ICML-03).|$|E
25|$|Atramentov, A., Leiva, H., and Honavar, V. (2003). A Multi-Relational <b>Decision</b> <b>Tree</b> Learning Algorithm– Implementation and Experiments.. In: Proceedings of the Thirteenth International Conference on Inductive Logic Programming. Berlin: Springer-Verlag.|$|E
40|$|AbstractBy proving {{exponential}} {{lower and}} polynomial upper bounds for parity <b>decision</b> <b>trees</b> and collecting similar bounds for nondeterministic and co-nondeterministic <b>decision</b> <b>trees,</b> the complexity classes related to polynomial-size deterministic, nondeterministic, co-nondeterministic, parity, and alternating <b>decision</b> <b>trees</b> are completely separated. Considering alternating <b>decision</b> <b>trees,</b> it is {{shown that the}} number of alternations between, say, ⋎-nodes and ⋏-nodes strongly influences their computational power...|$|R
5000|$|Smaller <b>decision</b> <b>trees</b> - C5.0 gets {{similar results}} to C4.5 with {{considerably}} smaller <b>decision</b> <b>trees.</b>|$|R
40|$|International audienceIn this paper, a {{study is}} {{presented}} to explore ensembles of fuzzy <b>decision</b> <b>trees.</b> First of all, a quick recall {{of the state of}} the art related to ensembles of (fuzzy) <b>decision</b> <b>trees</b> in Machine Learning is presented. Afterwards, a new approach to construct a forest of fuzzy <b>decision</b> <b>trees</b> is proposed. Two experiments are described, one with forests of fuzzy <b>decision</b> <b>trees,</b> and the other with bagging of fuzzy <b>decision</b> <b>trees.</b> The results highlight the interest of using fuzzy set theory in this kind of approaches...|$|R
25|$|The game-tree {{complexity}} {{of a game}} {{is the number of}} leaf nodes in the smallest full-width <b>decision</b> <b>tree</b> that establishes the value of the initial position. A full-width tree includes all nodes at each depth.|$|E
25|$|In 2014, Padma et al. used {{combined}} wavelet statistical texture {{features to}} segment and classify AD benign and malignant tumor slices. Zhang et al. found kernel {{support vector machine}} <b>decision</b> <b>tree</b> had 80% classification accuracy, with an average computation time of 0.022s for each image classification.|$|E
25|$|Analogous {{definitions}} can be {{made for}} space requirements. Although time and space are the most well-known complexity resources, any complexity measure {{can be viewed as a}} computational resource. Complexity measures are very generally defined by the Blum complexity axioms. Other complexity measures used in complexity theory include communication complexity, circuit complexity, and <b>decision</b> <b>tree</b> complexity.|$|E
40|$|The {{enormous}} growth {{experienced by}} the credit industry has led researchers to develop sophisticated credit scoring models that help lenders decide whether to grant or reject credit to applicants. This paper proposes a credit scoring model based on boosted <b>decision</b> <b>trees,</b> a powerful learning technique that aggregates several <b>decision</b> <b>trees</b> to form a classifier given by a weighted majority vote of classifications predicted by individual <b>decision</b> <b>trees.</b> The performance of boosted <b>decision</b> <b>trees</b> is evaluated using two publicly available credit card application datasets. The prediction accuracy of boosted <b>decision</b> <b>trees</b> is benchmarked against two alternative data mining techniques: the multilayer perceptron and support vector machines. The results show that boosted <b>decision</b> <b>trees</b> are a competitive technique for implementing credit scoring models. ...|$|R
5000|$|In <b>decision</b> <b>trees,</b> {{the depth}} of the tree determines the variance. <b>Decision</b> <b>trees</b> are {{commonly}} pruned to control variance.|$|R
40|$|Decision {{lists and}} <b>decision</b> <b>trees</b> are two models of {{computation}} for boolean functions. Blum has shown (Information Processing Letters 42 (1992), 183 - 185) that rank-k <b>decision</b> <b>trees</b> are a subclass of decision lists. Here we identify precisely, by giving a syntactical characterization, the subclass of decision lists which correspond exactly {{to the class}} of bounded rank <b>decision</b> <b>trees.</b> Furthermore we give a more general algorithm to recover reduced <b>decision</b> <b>trees</b> from <b>decision</b> lists. Postprint (published version...|$|R
25|$|Class {{prediction}} analysis: This approach, called supervised classification, {{establishes the}} basis for developing a predictive model into which future unknown test objects can be input in order to predict the most likely class membership of the test objects. Supervised analysis for class prediction involves use of techniques such as linear regression, k-nearest neighbor, learning vector quantization, <b>decision</b> <b>tree</b> analysis, random forests, naive Bayes, logistic regression, kernel regression, artificial neural networks, support vector machines, mixture of experts, and supervised neural gas. In addition, various metaheuristic methods are employed, such as genetic algorithms, covariance matrix self-adaptation, particle swarm optimization, and ant colony optimization. Input data for class prediction are usually based on filtered lists of genes which are predictive of class, determined using classical hypothesis tests (next section), Gini diversity index, or information gain (entropy).|$|E
500|$|The Aanderaa–Karp–Rosenberg {{conjecture}} also {{states that}} the randomized <b>decision</b> <b>tree</b> complexity of non-trivial monotone functions is [...] The conjecture again remains unproven, but has been resolved for the property of containing a [...] clique for [...] This property {{is known to have}} randomized <b>decision</b> <b>tree</b> complexity [...] For quantum decision trees, the best known lower bound is , but no matching algorithm is known for the case of [...]|$|E
500|$|The (deterministic) <b>decision</b> <b>tree</b> {{complexity}} of determining a graph property {{is the number}} of questions of the form [...] "Is there an edge between vertex [...] and vertex ?" [...] that have to be answered in the worst case to determine whether a graph has a particular property. That is, it is the minimum height of a boolean <b>decision</b> <b>tree</b> for the problem. There are [...] possible questions to be asked. Therefore, any graph property can be determined with at most [...] questions. It is also possible to define random and quantum <b>decision</b> <b>tree</b> {{complexity of}} a property, the expected number of questions (for a worst case input) that a randomized or quantum algorithm needs to have answered in order to correctly determine whether the given graph has the property.|$|E
40|$|In {{order to}} improve {{accuracy}} of fuzzy <b>decision</b> <b>trees</b> classification we propose a procedure of parameters adaptation by means of neural network training. In the direct cycle, fuzzy <b>decision</b> <b>trees</b> {{are based on the}} algorithm of fuzzy ID 3; in the feedback cycle, parameters of fuzzy <b>decision</b> <b>trees</b> are adapted based on stochastic gradient algorithm by traverse to the root nodes back from the leaves. Using this strategy, the hierarchical structure of the fuzzy <b>decision</b> <b>trees</b> remains fixed...|$|R
40|$|The aim of {{this thesis}} is to present the {{characteristics}} of <b>decision</b> <b>trees</b> and their use in practice and to describe the solution of <b>decision</b> <b>trees</b> {{with the use of}} PrecisionTree software. The work is divided into theoretical and practical parts. In the theoretical part the basic information about <b>decision</b> <b>trees</b> and their software support is described. In the practical part the use of <b>decision</b> <b>trees</b> for managerial decision-making with help of software support is shown on example...|$|R
40|$|Abstract. This paper {{proposes a}} new {{methodology}} for designing Fuzzy Cognitive Maps using crisp <b>decision</b> <b>trees</b> {{that have been}} fuzzified. Fuzzy cognitive map is a knowledge-based technique that works as an artificial cognitive network inheriting the main aspects of cognitive maps and artificial neural networks. <b>Decision</b> <b>trees,</b> in the other hand, are well known intelligent techniques that extract rules from both symbolic and numeric data. Fuzzy theoretical techniques are used to fuzzify crisp <b>decision</b> <b>trees</b> in order to soften decision boundaries at decision nodes inherent {{in this type of}} trees. Comparisons between crisp <b>decision</b> <b>trees</b> and the fuzzified <b>decision</b> <b>trees</b> suggest that the later fuzzy tree is significantly more robust and produces a more balanced decision making. The approach proposed in this paper could incorporate any type of fuzzy <b>decision</b> <b>trees.</b> Through this methodology, new linguistic weights were determined in FCM model, thus producing augmented FCM tool. The framework is consisted of a new fuzzy algorithm to generate linguistic weights that describe the cause-effect relationships among the concepts of the FCM model, from induced fuzzy <b>decision</b> <b>trees.</b> ...|$|R
500|$|Because the {{property}} of containing a clique is monotone, it is covered by the Aanderaa–Karp–Rosenberg conjecture, which states that the deterministic <b>decision</b> <b>tree</b> complexity of determining any non-trivial monotone graph property is exactly [...] For arbitrary monotone graph properties, this conjecture remains unproven. However, for deterministic decision trees, and for any [...] in the range , [...] {{the property}} of containing a -clique was shown to have <b>decision</b> <b>tree</b> complexity exactly [...] by [...] Deterministic decision trees also require exponential size to detect cliques, or large polynomial size to detect cliques of bounded size.|$|E
500|$|In 2012, Mejdal {{became the}} Director of Decision Sciences for the Houston Astros, where he {{supported}} recruitment {{decisions based on}} physical tests and historical player performance. Hiring Mejdal to apply an analytics-based <b>decision</b> <b>tree</b> on their player choices {{was part of the}} effort to revitalize the team and address performance issues in prior seasons. He helped the team create the STOUT system, named after the combination of [...] "stat" [...] and [...] "scout," [...] for making player choices. The system was criticized for de-humanizing players, but after trading off some players and making new recruits, the Astro's farm system became ranked among the best in baseball. The Astros also used analytics to persuade players that were uncomfortable with non-traditional positions on the field to embrace shifts, which the team now uses very heavily.|$|E
2500|$|The {{two most}} common tools are <b>Decision</b> <b>Tree</b> Analysis (DTA) and Real options {{valuation}} (ROV); they may often be used interchangeably: ...|$|E
30|$|<b>Decision</b> <b>trees</b> {{often need}} pruning rules to keep trees from growing too large and {{overfitting}} the data. Some concepts, {{such as the}} XOR, are not modeled easily by <b>decision</b> <b>trees</b> and will often produce a large and complex <b>tree.</b> <b>Decision</b> <b>trees</b> use greedy algorithms and may not produce a globally optimum <b>tree</b> [44]. <b>Decision</b> <b>trees</b> are unstable learners. A small change in training data can sometimes have a large effect on the tree. However, when used with ensemble learners, this characteristic can be advantageous [32].|$|R
40|$|<b>Decision</b> <b>trees</b> {{induction}} is {{a well-known}} classification method. In {{the middle of the}} eighties Quinlain [1, 2] described the basics of <b>decision</b> <b>trees</b> induction method that is known as ID 3 method. ID 3 induction method provides a <b>decision</b> <b>trees</b> construction that consists of many nodes with attributes tests. Using of such constructed trees allows us to perform dat...|$|R
40|$|Meta <b>decision</b> <b>trees</b> (MDTs) are {{a method}} for {{combining}} multiple classifiers. We present an integration of the algorithm MLC 4. 5 for learning MDTs into the Weka data mining suite. We compare classifier ensembles combined with MDTs to bagged and boosted <b>decision</b> <b>trees,</b> and to classifier ensembles combined with other methods: voting and stacking with three different meta-level classifiers (ordinary <b>decision</b> <b>trees,</b> naive Bayes, and multi-response linear regression - MLR). Meta <b>decision</b> <b>trees.</b> Techniques for combining predictions obtained from multiple base-level classifiers can be clustered in three combining frameworks: voting (used in bagging and boosting), stacked generalization or stacking [7] and cascading. Meta <b>decision</b> <b>trees</b> (MDTs) [5] adopt the stacking framework of combining base-level classifiers. The difference between meta and ordinary <b>decision</b> <b>trees</b> (ODTs) is that MDT leaves specify which base-level classifier should be used, instead of predicting the class value directly. Th [...] ...|$|R
2500|$|... : Many shogi games (professional, online, AI) {{put into}} a <b>decision</b> <b>tree</b> {{structure}} with user-generated commentary and references and some opening classifications.|$|E
2500|$|According to Jürgen Schmidhuber, the {{appropriate}} mathematical theory of Occam's razor already exists, namely, Solomonoff's theory of optimal inductive inference and its extensions. See discussions in David L. Dowe's [...] "Foreword re C. S. Wallace" [...] for the subtle {{distinctions between the}} algorithmic probability work of Solomonoff and the MML work of Chris Wallace, and see Dowe's [...] "MML, hybrid Bayesian network graphical models, statistical consistency, invariance and uniqueness" [...] both for such discussions and for (in section 4) discussions of MML and Occam's razor. For a specific example of MML as Occam's razor in the problem of <b>decision</b> <b>tree</b> induction, see Dowe and Needham's [...] "Message Length as an Effective Ockham's Razor in <b>Decision</b> <b>Tree</b> Induction".|$|E
2500|$|Conceptual {{clustering}} is {{a machine}} learning paradigm for unsupervised classification developed mainly during the 1980s. [...] It is distinguished from ordinary data clustering by generating a concept description for each generated class. [...] Most conceptual clustering methods {{are capable of}} generating hierarchical category structures; [...] see Categorization {{for more information on}} hierarchy. [...] Conceptual clustering is closely related to formal concept analysis, <b>decision</b> <b>tree</b> learning, and mixture model learning.|$|E
5000|$|Let , where n is {{the number}} of vertices. Find all optimal <b>decision</b> <b>trees</b> on r vertices. This can be done in time O(n) (see <b>Decision</b> <b>trees</b> above).|$|R
5000|$|Minimum message length (<b>decision</b> <b>trees,</b> <b>decision</b> graphs, etc.) ...|$|R
30|$|Note that in {{different}} <b>decision</b> <b>trees,</b> the training set is divided differently because that initial k cluster central points are selected randomly at each node of <b>decision</b> <b>trees.</b>|$|R
