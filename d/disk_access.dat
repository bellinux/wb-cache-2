464|564|Public
5|$|The +3 saw the {{addition}} of two more 16KB ROMs. One was home to {{the second part of}} the reorganised 128 ROM and the other hosted the +3's disk operating system. This was a modified version of Amstrad's PCWDOS (the <b>disk</b> <b>access</b> code used in LocoScript), called +3DOS. These two new 16KB ROMs and the original two 16KB ROMs were now physically implemented together as two 32KB chips. To be able to run CP/M, which requires RAM at the bottom of the address space, the bank-switching was further improved, allowing the ROM to be paged out for another 16KB of RAM.|$|E
25|$|To {{speed the}} search further, the first 13 to 14 {{comparisons}} (which each required a <b>disk</b> <b>access)</b> must be sped up.|$|E
25|$|When Windows 95 started up, MS-DOS loaded, {{processed}} CONFIG.SYS, launched COMMAND.COM, ran AUTOEXEC.BAT {{and finally}} ran WIN.COM. The WIN.COM program used MS-DOS {{to load the}} virtual machine manager, read SYSTEM.INI, load the virtual device drivers, and then turn off any running copies of EMM386 and switch into protected mode. Once in protected mode, the virtual device drivers (VxDs) transferred all state information from MS-DOS to the 32-bit file system manager, and then shut off MS-DOS. These VxDs allow Windows 9x to interact with hardware resources directly, as providing low-level functionalities such as 32-bit <b>disk</b> <b>access</b> and memory management. All future file system operations would get routed to the 32-bit file system manager. In Windows ME, win.com was no longer executed during the startup process; instead it went directly to execute VMM32.VXD from IO.SYS.|$|E
50|$|<b>Disk</b> <b>accesses</b> {{would often}} be batched {{together}} for efficiency.|$|R
40|$|Range {{trees are}} used for solving the {{orthogonal}} range searching problem, a problem that has applications in e. g. databases and computer graphics. We study the problem of storing range trees in secondary memory. To this end, we partition range trees into parts that are stored in consecutive blocks in secondary memory. This paper gives a number of partition schemes that limit the part-sizes {{and the number of}} <b>disk</b> <b>accesses</b> necessary to perform updates and queries. We show e. g., that for each fixed positive integer k, there is a partition of a two-dimensional range tree into parts of size O(n 1 /k), such that each update requires at most k(2 k+ 1) <b>disk</b> <b>accesses,</b> and each query requires at most 8 k 2 + 2 k+ 2 t <b>disk</b> <b>accesses,</b> where t is the number of answers to the range query...|$|R
40|$|Conventional B-tree {{insertion}} algorithms typically require several <b>disk</b> <b>accesses</b> per insertion. This is inconveniently slow {{for full}} text databases. Group update can significantly speed up insertion. The {{idea is to}} collect and sort many keys in memory, and then merge them with the B-tree on disk. This paper presents a concurrent group update algorithm for B + -trees; in addition to allowing concurrent operation, the new algorithm provides 90 % CPU time savings compared to existing group update algorithms. Experimental {{results show that the}} algorithm uses on the order of 0. 003 <b>disk</b> <b>accesses</b> per key in full-text indexing. 1 Introduction In full text databases, any word in the stored documents {{can be used as a}} search key. A database of 100 megabytes of English text contains about 17 000 000 words and 210 000 unique word forms. The huge number of keys is problematic in conventional B-tree based indexing systems; assuming each insertion takes three <b>disk</b> <b>accesses</b> [2], the initial indexing [...] ...|$|R
500|$|The disk {{loading time}} was the most criticised aspect from reviewers. Houghton found the loading times [...] "hefty" [...] and {{recommended}} that the player use two disk drives instead of one, as constant disk swapping was required in the game. Ricketts similarly found the swapping of [...] "half a dozen" [...] floppy disks a [...] "nightmare" [...] on the Atari ST. Keating found the [...] "huge" [...] amount of disk swapping to be the game's largest drawback, {{as well as the}} long loading times that accompanied it. Davies stated that scrolling times were [...] "painfully slow" [...] due to the excessive disk accessing. White stated that the game had a <b>disk</b> <b>access</b> routine [...] "so bad" [...] that it was almost impossible to access icons during gameplay, as the cursor movement was always a second behind actual mouse movements.|$|E
500|$|Pool of Radiance {{received}} positive reviews. G.M. {{called the}} game's graphics [...] "good" [...] and praised its role-playing and combat aspects. They felt that [...] "roleplayers will find Pools {{is an essential}} purchase, but people who are solely computer games oriented may hesitate before buying it [...] it will be their loss". Tony Dillon from Commodore User giving it a score of 9 out of 10. The only complaint was a slightly slow <b>disk</b> <b>access,</b> but the reviewer {{was impressed with the}} game's features, awarding it a Commodore User superstar and proclaiming it [...] "the best RPG ever to grace the C64, or indeed any other computer". Issue #84 of the British magazine Computer + Video Games rated the game highly, saying that [...] "Pools is a game which no role player or adventurer should be without and people new to role playing should seriously consider buying as an introductory guide". Another UK publication, The Games Machine, gave the game an 89% rating. The reviewer noted that the third-person arcade style combat view is a great improvement for SSI, as they had traditionally incorporated simplistic graphics in their role-playing games. The reviewer was critical that Pool of Radiance was not original in its presentation and that the colors were a little drab, but concluded that the game is [...] "classic Dungeons & Dragons which SSI have recreated excellently". A review from Zzap was less positive, giving the game a score of 80%. The reviewer felt that the game required too much [...] "hacking, slicing and chopping" [...] without enough emphasis on puzzle solving. The game was awarded 49% for its puzzle factor.|$|E
5000|$|No <b>disk</b> <b>access</b> for read {{operations}} and limited <b>disk</b> <b>access</b> for write operations ...|$|E
50|$|To avoid a {{bottleneck}} in <b>disk</b> <b>accesses,</b> {{the data}} is kept in screen display format. This results in {{the average number of}} disk reads per report display to be less than two.|$|R
40|$|Given that {{commercial}} search engines cover billions of web pages, efficiently managing the corresponding volumes of disk-resident data needed to answer user queries quickly {{is a formidable}} data manipulation challenge. We present a general technique for e#ciently carrying out large sets of simple transformation or querying operations over external-memory data tables. It greatly reduces the number of performed <b>disk</b> <b>accesses</b> and seeks by maximizing the temporal locality of data access and organizing most of the necessary <b>disk</b> <b>accesses</b> into long sequential reads or writes of data that is reused many times while in memory. This techniqu...|$|R
40|$|A query optimizer {{requires}} cost {{models to}} calculate the costs of various access plans for a query. An effective method to estimate the number of <b>disk</b> (or page) <b>accesses</b> for spatio-temporal queries {{has not yet been}} proposed. The TPR-tree is an efficient index that supports spatio-temporal queries for moving objects. Existing cost models for the spatial index such as the R-tree do not accurately estimate the number of <b>disk</b> <b>accesses</b> for spatio-temporal queries using the TPR-tree, because they do not consider the future locations of moving objects, which change continuously as time passes. In this paper, we propose an efficient cost model for spatio-temporal queries to solve this problem. We present analytical formulas which accurately calculate the number of <b>disk</b> <b>accesses</b> for spatio-temporal queries. Extensive experimental results show that our proposed method accurately estimates the number of <b>disk</b> <b>accesses</b> over various queries to spatio-temporal data combining reallife spatial data and synthetic temporal data. To evaluate the effectiveness of our method, we compared our spatio-temporal cost model (STCM) with an existing spatial cost model (SCM). The application of the existing SCM has the average error ratio from 52 % to 93 %, whereas our STCM has the average error ratio from 11 % to 32 %...|$|R
50|$|Also, {{it should}} not be {{confused}} with 32-bit <b>disk</b> <b>access.</b> Although both technologies are similar, 32-bit <b>disk</b> <b>access</b> (also known as FastDisk) pre-dates Windows for Workgroups 3.11. 32-bit file access provided a 32-bit code path for Windows to directly access the disk bus by intercepting the MS-DOS Int 21H services while remaining in 386 protected mode, rather than handling the Int 21H services in real mode by MS-DOS. 32-bit <b>disk</b> <b>access</b> offers relatively less performance and is less likely to work on many computers than 32-bit file access. 32-bit file access does not need 32-bit <b>disk</b> <b>access.</b>|$|E
5000|$|... 32-bit <b>disk</b> <b>access</b> {{should not}} be {{confused}} with 32-bit file access. Although both technologies are similar, 32-bit <b>disk</b> <b>access</b> was introduced with Windows 3.1 and file access with Windows for Workgroups 3.11. 32-bit file access provided a 32-bit code path for Windows to directly access the disk bus by intercepting the MS-DOS Int 21H services while remaining in 386 protected mode and at CPU speeds, rather than handling the Int 21H services in real mode by MS-DOS. 32-bit <b>disk</b> <b>access</b> offers less performance and is less likely to work on many computers than 32-bit file access. 32-bit file access does not require 32-bit <b>disk</b> <b>access.</b>|$|E
5000|$|UNLOCK : Disables {{low-level}} <b>disk</b> <b>access.</b> (DOS 7.1, Windows 95, 98, Me only) ...|$|E
30|$|The {{experimental}} {{results show that}} trees built with the proposed policies outperform those built with the original ones {{with regard to the}} number of <b>disk</b> <b>accesses,</b> the amount of distance calculations, and the time required to run the queries.|$|R
40|$|One of {{the most}} {{important}} kind of queries in spatial databases to support location-based services (LBS) is the continuous nearest neighbors (CNN) query. Given a spatial data set of points of interest and a moving query point q, the CNN query partitions q into a set of adjacent disjoint intervals associated with their nearest points of interest. Existing solutions to this problem are known to be sub-optimal in terms of <b>disk</b> <b>accesses.</b> In this paper, we present an algorithm to compute the CNN query that is I/O optimal. With an experimental evaluation, we show that not only the number of <b>disk</b> <b>accesses</b> is reduced with the optimal algorithm, but also the CPU performance is improved, in some cases...|$|R
40|$|Abstract. We {{present a}} {{self-adjusting}} layout scheme for suffix trees in secondary storage that provides optimal number of <b>disk</b> <b>accesses</b> for {{a sequence of}} string or substring queries. This has been an open problem since Sleator and Tarjan presented their splaying technique to create self-adjusting binary search trees in 1985. In addition to resolving this open problem, our scheme provides two additional advantages: 1) The partitions are slowly readjusted, requiring fewer <b>disk</b> <b>accesses</b> than splaying methods, and 2) the initial state of the layout is balanced, making it useful even when the sequence of queries is not highly skewed. Our method is also applicable to PATRICIA trees, and potentially to other data structures. ...|$|R
5000|$|When not {{executing}} 8-bit code, the Zilog Z80 {{was used}} for floppy <b>disk</b> <b>access.</b> [...] The 8088 bus {{was used for}} control of all other subsystems, including graphics, hard <b>disk</b> <b>access,</b> and communications. [...] While {{it may have been}} theoretically possible to load Z80 binary code into the Rainbow to execute alongside 8088 code, this procedure has never been demonstrated.|$|E
50|$|Spreadsort {{also works}} {{efficiently}} on problems {{too large to}} fit in memory and thus requiring <b>disk</b> <b>access.</b>|$|E
5000|$|... 32-bit <b>Disk</b> <b>Access</b> (also {{known as}} FastDisk) {{refers to a}} special <b>disk</b> <b>access</b> and caching mode {{available}} in older, MS-DOS-based Microsoft Windows operating systems. It was a set of protected mode device drivers that worked together {{to take advantage of}} advanced disk I/O features in the system BIOS. It filtered interrupt 13h BIOS calls to the disk controller and directed them in the most efficient way for the system — either through the 32-bit interface with the hard disk controller or through the system BIOS. Using 32-bit <b>Disk</b> <b>Access</b> allowed for more pageable memory in Windows to page MS-DOS-based applications to disk to free enough RAM for applications when they needed to use it. Sometimes enabling this mode would break older applications of the day.|$|E
40|$|This paper {{presents}} a strong security scheme for network-attached storage (NAS) {{that is based}} on capability and uses a key distribution scheme to keep network-attached storage from performing key management. Our system uses strong cryptography to protect data from spoofing, tampering, eavesdropping and replay attacks, and it also guarantees that the data stored on the storage is copy-resistant. In spite of this level of security, our system does not impose much performance penalty. Our experimental results shows that, using a relatively inexpensive CPU in the storage device, there are little performance penalty for random <b>disk</b> <b>accesses</b> and about 9 - 25 % performance degradation for large sequential <b>disk</b> <b>accesses</b> (≥ 4 KB). 1...|$|R
40|$|Cataloged from PDF {{version of}} article. The {{inverted}} index partitioning problem is investigated for parallel text retrieval systems. The {{objective is to}} perform efficient query processing on an inverted index distributed across a PC cluster. Alternative strategies are considered and evaluated for inverted index partitioning, where index entries are distributed according to their document-ids or term-ids. The performance of both partitioning schemes depend on {{the total number of}} <b>disk</b> <b>accesses</b> and the total volume of communication in the system. In document-id partitioning, the total volume of communication is naturally minimum, whereas the total number of <b>disk</b> <b>accesses</b> may be larger compared to term-id partitioning. On the other hand, in term-id partitioning the total number of <b>disk</b> <b>accesses</b> is already equivalent to the lower bound achieved by the sequential algorithm, albeit the total communication volume may be quite large. The studies done so far perform these partitioning schemes in a round-robin fashion and compare the performance of them by simulation. In this work, a parallel text retrieval system is designed and implemented on a PC cluster. We adopted hypergraph-theoretical partitioning models and carried out performance comparison of round-robin and hypergraph-theoretical partitioning schemes on our parallel text retrieval system. We also designed and implemented a query interface and a user interface of our system. Çatal, AytülM. S...|$|R
5000|$|Bloom {{proposed}} the technique for applications where {{the amount of}} source data would require an impractically large amount of memory if [...] "conventional" [...] error-free hashing techniques were applied. He gave {{the example of a}} hyphenation algorithm for a dictionary of 500,000 words, out of which 90% follow simple hyphenation rules, but the remaining 10% require expensive <b>disk</b> <b>accesses</b> to retrieve specific hyphenation patterns. With sufficient core memory, an error-free hash could be used to eliminate all unnecessary disk accesses; on the other hand, with limited core memory, Bloom's technique uses a smaller hash area but still eliminates most unnecessary accesses. For example, a hash area only 15% of the size needed by an ideal error-free hash still eliminates 85% of the <b>disk</b> <b>accesses</b> - an 85-15 form of the Pareto principle.|$|R
5000|$|LOCK : Enables {{external}} {{programs to}} perform low-level <b>disk</b> <b>access</b> to a volume. (DOS 7.1, Windows 95, 98, Me only) ...|$|E
5000|$|Raw hard <b>disk</b> <b>access</b> - allows {{physical}} {{hard disk}} partitions on the host system {{to appear in}} the guest system ...|$|E
50|$|To {{speed the}} search further, the first 13 to 14 {{comparisons}} (which each required a <b>disk</b> <b>access)</b> must be sped up.|$|E
40|$|The suffix tree of {{a string}} is the {{fundamental}} data structure of string processing. Recent focus on massive data sets has sparked interest in overcoming the memory bottlenecks of known algorithms for building suffix trees. Our main contribution is a new algorithm for suffix tree construction in which we choreograph almost all <b>disk</b> <b>accesses</b> to be via the sort and scan primitives. This algorithm achieves optimal results {{in a variety of}} sequential and parallel computational models. Two of our results are: In the traditional external memory model, in which only the number of <b>disk</b> <b>accesses</b> is counted, we achieve an optimal algorithm, both for single and multiple disk cases. This is the first optimal algorithm known for either model. Traditional <b>disk</b> page <b>access</b> counting does not differentiate between random page accesses and block transfers involving several consecutive pages. This difference is routinely exploited by expert programmers to get fast algorithms on real machines. We adopt a simple accounting scheme and show that our algorithm achieves the same optimal tradeoff for block versus random page accesses as the one we establish for sorting...|$|R
40|$|Metric Access Methods (MAM) are {{employed}} {{to accelerate the}} processing of similarity queries, such as the range and the k-nearest neighbor queries. Current methods improve the query performance minimizing the number of <b>disk</b> <b>accesses,</b> keeping a constant height of the structures stored on disks (height-balanced trees). The Slim-tree and the M-tree are the most efficient dynamic MAM so far. However, the overlapping between their nodes has a very high influence on their performance. This paper presents a new dynamic MAM called the DBM-tree (Density-Based Metric tree), which can minimize the overlap between high-density nodes by relaxing the height-balancing of the structure. Thus, {{the height of the}} tree is larger in denser regions, in order to keep a tradeoff between breadth-searching and depth-searching. Moreover, an optimization algorithm called Shrink is also presented, which improves the performance of an already built DBM-tree by reorganizing the elements among their nodes. Experiments performed over both synthetic and real datasets showed that the DBM-tree is, in average, 50 % faster than traditional MAM and reduces the number of distance calculations by up to 72 % and <b>disk</b> <b>accesses</b> by up to 54 %. After performing the Shrink algorithm, the performance improves up to 30 % regarding the number of <b>disk</b> <b>accesses</b> for range and k-nearest neighbor queries. In addition, the DBM-tree scales up well, exhibiting sub-linear performance with growing number of elements in the database. 1...|$|R
50|$|Index {{structures}} like B-trees {{also reduce}} the number of <b>disk</b> <b>accesses,</b> and are more often used to index on-disk data in part because they can index many types of data and can be updated online. Still, interpolation search may be useful when one is forced to search certain sorted but unindexed on-disk datasets.|$|R
50|$|A {{new version}} called Virtual Volumes is being developed. This new version has a layered modular design. It {{supports}} multiple filesystems and multiple <b>disk</b> <b>access</b> methods.|$|E
50|$|If {{using the}} Plus3 in screen modes 0-3, the pseudo-variable TIME would be thrown off, as the {{interrupts}} were disabled during <b>disk</b> <b>access</b> in these modes.|$|E
50|$|When the {{graphical}} {{user interface}} is started, the virtual machine manager takes over the filesystem-related and disk-related functionality. MS-DOS itself is demoted to a compatibility layer for 16-bit device drivers. This contrasts with earlier versions of Windows which rely on MS-DOS to perform file and <b>disk</b> <b>access</b> (Windows for Workgroups 3.11 could also largely bypass MS-DOS when 32-bit file access and 32-bit <b>disk</b> <b>access</b> were enabled). Keeping MS-DOS in memory allows Windows 95 to use DOS device drivers when suitable Windows drivers are unavailable. Windows 95 is capable of using all 16-bit Windows 3.x drivers.|$|E
40|$|URL] paper {{addresses}} two problems. We {{investigate the}} problem of parallel external sorting {{in the context of}} a form of heterogeneous clusters then we investigate the impact of efficient <b>disk</b> remote <b>accesses</b> on the performance of external sorting. We explore three techniques to show how they can be deployed for clusters with proportional processor performances. We also validate the READ 2 library, an efficient implementation of remote SCSI <b>disk</b> <b>accesses.</b> We derive a new parallel sorting algorithm that is adapted to the READ 2 interface. The expected gain of using READ 2 is compared to the measured gain for one external sorting implementation...|$|R
30|$|In {{the last}} decades, several MAM were {{proposed}} aiming at {{reducing the number}} of distance calculations, the number of <b>disk</b> <b>accesses,</b> and the total time spent to compute distance-based queries. At first, these methods were tailored to provide efficient similarity queries processing. There were no concerns about the effectiveness for the data mining approaches based on similarity comparisons.|$|R
40|$|In {{this paper}} we analyze the {{performance}} of a traditional parametric search system and compare it to a system using an in memory auxiliary index. An analysis shows traditional database-based parametric systems incur a huge time hit due to <b>disk</b> <b>accesses.</b> We show that using an in-memory index in such scenarios results in huge time savings...|$|R
