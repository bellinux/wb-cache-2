56|82|Public
5000|$|Do {{not rely}} on a central {{authority}} for the relaying of messages (<b>decentralized</b> <b>computing).</b>|$|E
50|$|<b>Decentralized</b> <b>computing</b> is the {{allocation}} of resources, both hardware and software, to each individual workstation, or office location. In contrast, centralized computing exists when the majority of functions are carried out, or obtained from a remote centralized location. <b>Decentralized</b> <b>computing</b> is a trend in modern-day business environments. This {{is the opposite of}} centralized computing, which was prevalent during the early days of computers. A decentralized computer system has many benefits over a conventional centralized network. Desktop computers have advanced so rapidly, that their potential performance far exceeds the requirements of most business applications. This results in most desktop computers remaining idle (in relation to their full potential). A decentralized system can use the potential of these systems to maximize efficiency. However, it is debatable whether these networks increase overall effectiveness.|$|E
50|$|One of {{the most}} {{interesting}} debates over <b>decentralized</b> <b>computing</b> involved Napster, a music file sharing application, which granted users access to an enormous database of files. Record companies brought legal action against Napster, blaming the system for lost record sales. Napster was found in violation of copyright laws by distributing unlicensed software, and was shut down.|$|E
30|$|This section {{introduces}} {{a model for}} aggregation of states. A state represents a (aggregation) value of an application parameter at a specific point in time. The state of an application parameter changes during runtime. <b>Decentralized</b> aggregation <b>computes</b> aggregation functions that receive as input the states of different nodes for the same application parameter.|$|R
40|$|Abstract — Agent based {{computing}} {{offers the}} ability to <b>decentralize</b> <b>computing</b> solutions by incorporating autonomy and intelligence into cooperating, distributed applications. It provides an effective medium for expressing solutions to problems that involve interaction with real-world environments and allows modelling of the world state and its dynamics. This model can be then used to determine how candidate actions affect the world, and how to choose the best from a set of actions. Most agent paradigms overlook real-time requirements and computing resource constraints. In this paper, we discuss the application of agent based computing to RoboCup and examine methods to improve it. In particular, we discuss the incorporation of a meta-level reasoning mechanism that handles individual agent organization, plan generation, task allocation, integration and plan execution. We also propose an architecture where a meta-agent is further enhanced by combining it with system-level resource allocation and optimization. The approach adopted by us unifies agent based computing with adaptive resource management for dynamic real-time systems. The goal is to build and implement a distributed, intelligent, agent based system for dynamic real-time applications. I...|$|R
50|$|Peer-to-peer (P2P) {{computing}} {{and networking}} to enable <b>decentralized</b> cloud <b>computing</b> {{has been an}} area of research for sometime. Social cloud computing intersects peer-to-peer cloud computing with social computing to verify peer and peer owner reputation thus providing security and quality of service assurances to users. On demand computing environments may be constructed and altered statically or dynamically across peers on the Internet based on their available resources and verified reputation to provide such assurances.|$|R
50|$|The early help desks of the 1980s {{incorporated}} simple workflow models: {{problems were}} reported, dispatched, routed to a technician, resolved, and then closed. As <b>decentralized</b> <b>computing</b> matured, customized workflow solutions such as change management, configuration management, and problem management, enabled the IT department {{to focus on}} its primary objectives - resolving problems, and rolling out new applications faster, more reliably, and with greater ease.|$|E
40|$|International audienceIn this {{research}} {{we use a}} <b>decentralized</b> <b>computing</b> approach to allocate and schedule tasks on a massively distributed grid. Using emergent properties of multi-agent systems, the algorithm dynamically creates and dissociates clusters to serve the changing resource demands of a global task queue. The algorithm is compared to a standard first-in first-out (FIFO) scheduling algorithm. Experiments done on a simulator show that the distributed resource allocation protocol (dRAP) algorithm outperforms the FIFO scheduling algorithm on time to empty queue, average waiting time and CPU utilization. Such a <b>decentralized</b> <b>computing</b> approach holds promise for massively distributed processing scenarios like SETI@home and Google MapReduce...|$|E
40|$|The aim of P 2 P {{computing}} is {{to build}} virtual computing systems dedicated to large-scale computational problems. JXTA 1 proposes an underlying infrastructure on which JNGI 2, {{one of the first}} P 2 P <b>decentralized</b> <b>computing</b> frameworks is built. In order to test this framework, we have built a tool named P 2 PPerf, which allows us to study the behavior of JNGI and to optimize it according to our simulation results. 1...|$|E
40|$|This SpringerBrief {{discusses}} {{the characteristics of}} spatiotemporal movement data, including uncertainty and scale. It investigates three core aspects of Computational Movement Analysis: Conceptual modeling of movement and movement spaces, spatiotemporal analysis methods aiming at {{a better understanding of}} movement processes (with a focus on data mining for movement patterns), and using <b>decentralized</b> spatial <b>computing</b> methods in movement analysis. The author presents Computational Movement Analysis as an interdisciplinary umbrella for analyzing movement processes with methods from a range of f...|$|R
40|$|Abstract—One of the {{critical}} challenges for service oriented computing systems is the capability to guarantee scalable and reliable service provision. This paper presents Reliable GeoGrid, a <b>decentralized</b> service <b>computing</b> architecture based on geographical location aware overlay network for supporting reliable and scalable mobile information delivery services. The reliable GeoGrid approach offers two distinct features. First, we develop a distributed replication scheme, aiming at providing scalable and reliable processing of location service requests in <b>decentralized</b> pervasive <b>computing</b> environments. Our replica management operates on a network of heterogeneous nodes and utilizes a shortcut-based optimization to increase the resilience of the system against node failures and network failures. Second, we devise a dynamic load balancing technique that exploits the service processing capabilities of replicas to scale the system in anticipation of unexpected workload changes and node failures by taking into account of node heterogeneity, network proximity, and changing workload at each node. Our experimental evaluation shows that the reliable GeoGrid architecture is highly scalable under changing service workloads with moving hotspots and highly reliable {{in the presence of}} both individual node failures and massive node failures. I...|$|R
5000|$|The second era (personal computer) {{began in}} 1965 as {{microprocessors}} started {{to compete with}} mainframes and minicomputers and accelerated the process of <b>decentralizing</b> <b>computing</b> power from large data centers to smaller offices. In the late 1970s, minicomputer technology gave way to personal computers and relatively low-cost computers were becoming mass market commodities, allowing businesses to provide their employees access to computing power that ten years before would have cost {{tens of thousands of}} dollars. This proliferation of computers created a ready market for interconnecting networks and the popularization of the Internet. (NOTE that the first microprocessor — a four-bit device intended for a programmable calculator — was introduced in 1971 and microprocessor-based systems were not readily available for several years. The MITS Altair 8800 was the first commonly known microprocessor-based system, followed closely by the Apple I and II. It is arguable that the microprocessor-based system did not make significant inroads into minicomputer use until 1979, when VisiCalc prompted record sales of the Apple II on which it ran. The IBM PC introduced in 1981 was more broadly palatable to business, but its limitations gated its ability to challenge minicomputer systems until perhaps the late 1980s to early 1990s.) ...|$|R
40|$|International audienceThis paper {{considers}} decentralized convex optimization over {{a network}} in large scale contexts, where large simultaneously applies to {{number of training}} examples, dimensionality and number of networking nodes. We first propose a centralized optimization scheme that generalizes successful existing methods based on gradient averaging, improving their flexibility by making the number of averaged gradients an explicit parameter of the method. We then propose an asynchronous distributed algorithm that implements this original scheme for large <b>decentralized</b> <b>computing</b> networks...|$|E
40|$|Role-based {{access control}} (RBAC) is an {{emerging}} concept for security administration for large and <b>decentralized</b> <b>computing</b> environments. The Security Administration Manager (SAM) of Schumann Security Software, Inc. (Schumann) {{is a tool}} for enterprise-wide security management that implements many RBAC concepts. Schumann has accrued extensive experience while successfully implementing RBAC at large corporations. This paper summarizes this experience and compares it with the standards which are currently suggested for RBAC. In addition, it briefly discusses issues relating to the migration from conventional security administration to RBAC...|$|E
30|$|Fog {{computing}} is a <b>decentralized</b> <b>computing</b> architecture whereby data {{is processed}} and stored between {{the source of}} origin and a cloud infrastructure. This results in the minimisation of data transmission overheads, and subsequently, improves the performance of computing in Cloud platforms by reducing the requirement to process and store large volumes of superfluous data. The Fog computing paradigm is largely motivated by a continuous increase in Internet of Things (IoT) devices, where an ever increasing amount of data (with respect to volume, variety, and velocity [1]) is generated from an ever-expanding array of devices.|$|E
40|$|Abstract—Security {{in large}} {{distributed}} computing infrastructures, peer-to-peer, or clouds, remains {{an important issue}} and probably a strong obstacle {{for a lot of}} potential users of these types of computing infrastructures. In this paper, we propose an architecture for large scale distributed infrastructures guaranteeing confidentiality and integrity of both the computation and the host computer. Our approach is based on the use of virtualization and we introduce the notion of confidence link to safely execute programs. We implemented and tested this approach using the POP-C++ tool, which is a comprehensive object-oriented system to develop applications in large <b>decentralized</b> distributed <b>computing</b> infrastructures. Keywords-virtualization; security in large distributed system; grid middleware. I...|$|R
5000|$|Qarnot sells both Q.rads and on-demand cloud {{computing}} service. Connected to the Internet, the heaters form a <b>decentralized</b> network of <b>computing</b> nodes and the electricity consumed by every heater is refunded to the companies or individuals hosting them [...] Qarnot also provides Platform as a Service and Software as a Service {{to companies that}} require massive remote processing power, like banks and animation studios [...] [...]|$|R
40|$|We present chemlambda (or the {{chemical}} concrete machine), an artificial chemistry {{with the following}} properties: (a) is Turing complete, (b) has a model of <b>decentralized,</b> distributed <b>computing</b> associated to it, (c) works {{at the level of}} individual (artificial) molecules, subject of reversible, but otherwise deterministic interactions with a small number of enzymes, (d) encodes information in the geometrical structure of the molecules and not in their numbers, (e) all interactions are purely local in space and time. This {{is part of a larger}} project to create computing, artificial chemistry and artificial life in a distributed context, using topological and graphical languages. Comment: 8 pages, 21 colour figures, conference paper submitted to ALIFE 1...|$|R
40|$|International audienceOne of {{the current}} trends in {{computer}} science leads {{to the design of}} computing organizations based on the activity of a multitude of tiny cheap <b>decentralized</b> <b>computing</b> entities. Whether these chips are integrated into paintings or disseminated in open environments like dust, the fundamental problem lies in their cooperative operation so that global functions are obtained collectively. In this paper, we address the issue of the creation of visual ambiences based on the coordinated activity of computing entities. These entities are distributed randomly on a 2 D canvas and can only change their own color and perceive their immediate neighbors...|$|E
40|$|AbstractThe {{new method}} of the {{organization}} of the distributed adaptive computations is offered in the presented paper. The method allows using resources of computers united by a network for solving coherent tasks. The main feature of the given method is adaptive correcting of process of computation if parameters of computing nodes are varying or nodes fail. This ability is achieved because of system decentralization and multiagent approach: all nodes of computing system are equal and nodes have to unite to solve incoming tasks. Also this paper presents algorithms based on the developed method and program model of <b>decentralized</b> <b>computing</b> system...|$|E
40|$|Abstract. One of {{the current}} trends in {{computer}} science leads {{to the design of}} computing organizations based on the activity of a multitude of tiny cheap <b>decentralized</b> <b>computing</b> entities. Whether these chips are integrated into paintings or disseminated in open environments like dust, the fundamental problem lies in their cooperative operation so that global functions are obtained collectively. In this paper, we address the issue of the creation of visual ambiences based on the coordinated activity of computing entities. These entities are distributed randomly on a 2 D canvas and can only change their own color and perceive their immediate neighbors. 1...|$|E
40|$|This paper {{attempted}} to <b>decentralize</b> volunteer <b>computing</b> (VC) coordination {{with the goal}} of reducing the reliance on a central coordination server, which had been criticized for performance bottleneck and single point of failure. On analyzing the roles and functions that the VC components played for the centralized master/worker coordination model, this paper proposed a decen-tralized VC coordination framework based on distributed hash table (DHT) and peer-to-peer (P 2 P) overlay and then successfully mapped the centralized VC coordination into distributed VC coordination. The proposed framework has been implemented on the performance-proven DHT P 2 P overlay Chord. The initial verification has demonstrated the effectiveness of the framework when working in distributed environments...|$|R
40|$|As {{computing}} {{becomes more}} pervasive, smart computing devices are increasingly connected. Applications provide pervasive services based on contexts of their host devices {{as well as}} other connected devices. However, the use of contexts commonly follows a centralized approach, i. e., copying all required contexts of other devices to one device that performs a computing task. This causes space overhead and privacy threats to resource-limited, personal devices. In this paper, we propose a constraint-based approach to systematically <b>decentralizing</b> constraint-expressed <b>computing</b> tasks to connected devices. This approach is expressive to support pervasive computing tasks such as situation evaluation to complete in a device-collaborating way. We show that the approach is effective for reducing space cost and protecting device privacy in pervasive computin...|$|R
5000|$|Werbach's {{areas of}} {{interest}} are emerging internet technologies, telecommunications policy, electronic commerce, wireless communication, and regulation. He advises major information technology and communications companies on strategic business and policy implications of emerging technologies. At The Wharton School Werbach is currently working {{in the areas of}} evolving Internet architecture policy implications, regulation of Internet video, next-generation broadband access, and <b>decentralized</b> communications, <b>computing,</b> and media business implications. As a Network Neutrality advocate, Werbach supports the 2015 FCC's rule on network neutrality. Together with Phil Weiser, a professor and dean at the Colorado Law School, Werbach posted their public statements in the column on Huffington Post and Medium, getting much attention and mentions by well-known publications as Wired, The New York Times, Fortune and other significant publications.|$|R
40|$|This paper {{argues for}} the {{possibility}} of 2 ̆ 7 artificial life 2 ̆ 7 and computational evolution, first by discussing (via a highly simplified version) John von Neumann 2 ̆ 7 s self-reproducing automaton and then by presenting some recent work focusing on computational evolution, in which 2 ̆ 7 cellular automata 2 ̆ 7, a form of parallel and <b>decentralized</b> <b>computing</b> system, are evolved via 2 ̆ 7 genetic algorithms 2 ̆ 7. It is argued that such in silico experiments can help {{to make sense of the}} question of whether we can eventually build computers that are intelligent and alive...|$|E
40|$|Computing {{increasingly}} happens somewhere, {{with that}} geographic location {{important to the}} computational process itself. Many new and evolving spatial technologies, such as geosensor networks and smartphones, embody this trend. Conventional approaches to spatial computing are centralized, and do {{not account for the}} inherently decentralized nature of "computing somewhere": the limited, local knowledge of individual system components, and the interaction between those components at different locations. On the other hand, despite being an established topic in distributed systems, <b>decentralized</b> <b>computing</b> is not concerned with geographical constraints to the generation and movement of information. In this context, of (centralized) spatial computing and decentralized (non-spatial) computing, the key question becomes: "What makes decentralized spatial computing special?" In Part I of the book the author covers the foundational concepts, structures, and design techniques for <b>decentralized</b> <b>computing</b> with spatial and spatiotemporal information. In Part II he applies those concepts and techniques to the development of algorithms for decentralized spatial computing, stepping through a suite of increasingly sophisticated algorithms: from algorithms with minimal spatial information about their neighborhoods; to algorithms with access to more detailed spatial information, such as direction, distance, or coordinate location; to truly spatiotemporal algorithms that monitor environments that are dynamic, even using networks that are mobile or volatile. Finally, in Part III the author shows how decentralized spatial and spatiotemporal algorithms designed using the techniques explored in Part II can be simulated and tested...|$|E
40|$|This thesis {{examines}} the {{issues relating to}} non-discretionary access controls for <b>decentralized</b> <b>computing</b> systems. Decentralization changes {{the basic character of}} a computing system from a set of processes referencing a data base to a set of processes sending and receiving messages. Because messages must be acknowledged, operations that were read-only in a centralized system become read-write operations. As a result, the lattice model of non-discretionary access control, which mediates operations based on read versus read-write considerations, does not allow direct transfer of algorithms from centralized systems to decentralized systems. This thesis develops new mechanisms that comply with the lattice model and provide the necessary functions for effective decentralized computation. Secur...|$|E
40|$|In {{the thesis}} we propose two <b>decentralized</b> methods for <b>computing</b> Pareto-optimal {{solutions}} in multi-party negotiations. We call a method decentralized if its use {{does not require}} the decision makers to know each others' value functions and neither does it require anyone outsider to know all the values. In the methods the preference information is gathered during an interactive procedure between the decision makers and a mediator, who works as a neutral coordinator. In the firs...|$|R
40|$|Nowadays {{the smart}} phone market is clearly growing {{due to the}} new type of {{functionalities}} that mobile devices have and the role that they play in everyday life. Their utility and benefits rely on the applications that can be installed on the device (the so-called mobile apps). Cloud computing {{is a way to}} enhance the world of mobile application by providing disk space and freeing the user of the local storage needs, this way providing cheaper storage, wider acces-sibility and greater speed for business. In this paper we introduce various aspects of mobile computing and we stress the importance of obtaining cloud maturity by using machine learning for automating configurations of software applications deployed on cloud nodes using the open source application ERP 5 and SlapOS, an open source operating system for <b>Decentralized</b> Cloud <b>Computing...</b>|$|R
40|$|The recent {{advancement}} of mobile computing technology and smartphones {{have changed the}} way we live, communicate, interact, and understand the world. Smartphones have various salient features that make them promising system platforms for the development of context-aware applications, e. g., embedded sensors in smartphones make them more convenient to be used for making context-rich information available to applications. Although {{the state of the art}} development of smartphones has endued developers to build advanced context-aware applications, many challenges still remain. Those are mostly due to the limited resources available in the mobile devices including computational and communication resources. This paper surveys the recent advances in context-aware applications in mobile platforms, and proposes a <b>decentralized</b> context-aware <b>computing</b> model that makes use of the smartphone platform, a P 2 P communication model, and declarative rule-based programming...|$|R
40|$|Structural health {{monitoring}} (SHM) systems are implemented for structures (e. g., bridges, buildings) {{to monitor their}} operations and health status. Wireless sensor networks (WSNs) are becoming an enabling technology for SHM applications that are more prevalent and more easily deployable than traditional wired networks. However, SHMbrings new challenges to WSNs: engineering-driven optimal deployment, a large volume of data, sophisticated computing, and so forth. In this paper, we address two important challenges: sensor deployment and <b>decentralized</b> <b>computing.</b> We propose a solution, to deploy wireless sensors at strategic locations to achieve the best estimates of structural health (e. g., damage) by following the widely used wired sensor system deployment approach from civil/structural engineering. We found that faults (caused by communication errors, unstable connectivity, sensor faults, etc.) in such a deployed WSN greatly affect the performance of SHM. To make the WSN resilient to the faults, we present an approach, called FTSHM (fault-tolerance in SHM), to repair the WSN and guarantee a specified degree of fault tolerance. FTSHM searches the repairing points in clusters in a distributed manner, and places a set of backup sensors at those points {{in such a way}} that still satisfies the engineering requirements. FTSHM also includes an SHM algorithm suitable for <b>decentralized</b> <b>computing</b> in the energy-constrained WSN, with the objective of guaranteeing that the WSN for SHM remains connected in the event of a fault, thus prolonging the WSN lifetime under connectivity and data delivery constraints. We demonstrate the advantages of FTSHM through extensive simulations and real experimental settings on a physical structure. Department of Computin...|$|E
30|$|In {{the era of}} <b>decentralized</b> <b>computing,</b> a {{breakthrough}} in blockchain technology, which underlines Bitcoin (Nakamoto 2008), {{can be used to}} preserve users’ privacy and prevent information fraud. Blockchain is a public ledger that verifies every transaction, stores it based on group consensus, and records it indisputably (Soska and Christin 2015; Vandervort 2014). As it can provide transaction records permanently, incorruptibly, and irreversibly, it may help fundamentally prevent some types of information fraud (Khan 2015, Pwc 2015). In this paper, we analyze the effectiveness of blockchain technology in fraud detection. While there are various types of information fraud, in this study, we focus on one popular type: rating fraud. We consider a piece of information fraudulent {{as long as it is}} not consistent with real information.|$|E
40|$|Cellular Automation ” (CA) is a <b>decentralized</b> <b>computing</b> model {{providing}} an excellent platform for performing complex computation {{with the help}} of only local information. Researchers, scientists and practitioners from different fields have exploited the CA paradigm of local information, decentralized control and universal computation for modeling different applications. This article provides a survey of available literature of some of the methodologies employed by researchers to utilize cellular automata for modeling purposes. The survey introduces the different types of cellular automation being used for modeling and the analytical methods used to predict its global behaviour from its local configurations. It further gives a detailed sketch of the efforts undertaken to configure the local settings of CA from a given global situation; the problem, which has bee...|$|E
40|$|Summary. We {{consider}} {{the problem of}} <b>computing</b> <b>decentralized</b> control policies for stochastic systems with finite state and action spaces. Synthesis of optimal de-centralized policies for such problems {{is known to be}} NP-hard [1]. Here we focus on methods for efficiently <b>computing</b> meaningful suboptimal <b>decentralized</b> control policies. The algorithms we present here are based on approximation of optimal Q-functions. We show that the performance loss associated with choosing decentralized policies with respect to an approximate Q-function is related to the approximation error. ...|$|R
40|$|Tasks {{that require}} parallelism, redundancy, and {{adaptation}} to dynamic, possibly hazardous environments can potentially be performed very efficiently and robustly by a swarm robotic system. Such a system {{would consist of}} {{hundreds or thousands of}} anonymous, resource-constrained robots that operate autonomously, with little to no direct human supervision. The massive parallelism of a swarm would allow it to perform effectively in the event of robot failures, and the simplicity of individual robots facilitates a low unit cost. Key challenges in the development of swarm robotic systems include the accurate prediction of swarm behavior and the design of robot controllers that can be proven to produce a desired macroscopic outcome. The controllers should be scalable, meaning that they ensure system operation regardless of the swarm size. ^ This thesis presents a comprehensive approach to modeling a swarm robotic system, analyzing its performance, and synthesizing scalable control policies that cause the populations of different swarm elements to evolve in a specified way that obeys time and efficiency constraints. The control policies are <b>decentralized,</b> <b>computed</b> a priori, implementable on robots with limited sensing and communication capabilities, and have theoretical guarantees on performance. To facilitate this framework of abstraction and top-down controller synthesis, the swarm is designed to emulate a system of chemically reacting molecules. The majority of this work considers well-mixed systems when there are interaction-dependent task transitions, with some modeling and analysis extensions to spatially inhomogeneous systems. ^ The methodology is applied to the design of a swarm task allocation approach that does not rely on inter-robot communication, a reconfigurable manufacturing system, and a cooperative transport strategy for groups of robots. The third application incorporates observations from a novel experimental study of the mechanics of cooperative retrieval in Aphaenogaster cockerelli ants. The correctness of the abstractions and the correspondence of the evolution of the controlled system to the target behavior are validated with computer simulations. The investigated applications form the building blocks for a versatile swarm system with integrated capabilities that have performance guarantees. ...|$|R
40|$|Fault {{tolerance}} {{is one of}} {{the most}} desirable property in <b>decentralized</b> grid <b>computing</b> systems, where computational resources are geographically distributed. These resources collaborate in order to execute workflow applications as fast as possible. In workflow applications, tasks are dependent on each other, so it becomes extremely vital that scheduling techniques should also have some decentralized fault tolerant mechanism. In this paper, we have proposed a decentralized fault tolerant mechanism which utilize the checkpoint concept; for Heterogeneous Limited Duplication (HLD) algorithm. HLD is based on task duplication scheduling in heterogeneous environment. There are two fold benefits firstly; if node failure occurs then rest of grid nodes sustain the execution of application. Secondly, less makespan of application is obtained using checkpoint concept. Therefore, application scheduled over decentralized grid systems (which are known for their unreliable behavior) will yield results fast utilizing algorithm proposed in this paper...|$|R
