0|37|Public
40|$|International audienceTwo {{approaches}} to Spoken Language Understanding based on frames describing <b>chunked</b> <b>knowledge</b> are described. They {{are applied to}} the MEDIA corpus annotated in terms of concepts expressing chunks of spoken sentences. General rules of knowledge composition and inference appear to be adequate to effectively applying the application ontology for obtaining frame based representations of dialogue turns. The main difficulty {{appears to be the}} characterization of the syntactic knowledge expressing semantic links between <b>knowledge</b> <b>chunks.</b> This <b>knowledge</b> can be hand-crafted or automatically learned from examples. It is shown that the latter approach outperforms the former if applied to ASR error prone transcriptions...|$|R
40|$|A {{sequence}} of images, sounds, or words {{can be stored}} at several levels of detail, from specific items and their timing to abstract structure. We propose a taxonomy of five distinct cerebral mechanisms for sequence coding: transitions and timing <b>knowledge,</b> <b>chunking,</b> ordinal <b>knowledge,</b> algebraic patterns, and nested tree structures. In each case, we review the available experimental paradigms and list the behavioral and neural signatures of the systems involved. Tree structures require a specific recursive neural code, as yet unidentified by electrophysiology, possibly unique to humans, and which may explain the singularity of human language and cognition...|$|R
40|$|International audienceIn this paper, we describle a {{model that}} relies on the {{following}} assumption; ontology negotiation and creation is necessary to make knowledge sharing and KM successful through communication. We mostly focus on the modifying process, i. e. dialogue, and we show a dynamic modification of agents knowledge bases could occur through messages exchanges, messages being <b>knowledge</b> <b>chunks</b> to be mapped with agents KB. Dialogue takes account of both success and failure in mapping. We show that the same process helps repair its own anomalies. We describe an architecture for agents knowledge exchange through dialogue. Last we conclude {{about the benefits of}} introducing dialogue features in knowledge management...|$|R
40|$|Management {{literature}} {{recognizes that}} knowledge is replacing more traditional sources of competitive advantage, and perhaps the only sustainable competitive advantage {{is the ability to}} use and embed knowledge into an organization’s life. In this chapter, we look at the notion of embedding <b>knowledge</b> <b>chunks</b> in a variety of contexts from the viewpoint of ontological frames. We suggest a three level typology that was driving our efforts to develop a knowledge-rich application based on Semantic Web technologies. The core concepts refer to our Frame-Annotate-Navigate framework, and we discuss this extrapolation in terms of moving from designing knowledge portals and centralized repositories toward supporting open and modular knowledge spaces. ...|$|R
40|$|Abstract: In this paper, we describle a {{model that}} relies on the {{following}} assumption; ontology negotiation and creation is necessary to make knowledge sharing and KM successful through communication. We mostly focus on the modifying process, i. e. dialogue, and we show a dynamic modification of agents knowledge bases could occur through messages exchanges, messages being <b>knowledge</b> <b>chunks</b> to be mapped with agents KB. Dialogue takes account of both success and failure in mapping. We show that the same process helps repair its own anomalies. We describe an architecture for agents knowledge exchange through dialogue. Last we conclude {{about the benefits of}} introducing dialogue features in knowledge management. ...|$|R
40|$|This paper {{defines a}} clean {{multi-layer}} architecture {{for the design}} of knowledge-based synthesis systems within the logic programming paradigm. The intent is to provide an integrated logical framework for modeling the different kinds of knowledge involved during synthesis processes, a workbench of inference-based generic methods for the constructive solution of knowledge acquisition and synthesis tasks. As major advantages, evolutionary synthesis processes, the re-usability of <b>knowledge</b> <b>chunks,</b> and multiple views are facilitated and correctness criteria are provided. The approach depends upon the definition of hierarchically organized abstraction layers: domain, domain abstractions, and finally, distributed knowledge sources. Each layer is partitioned into data type, declarative, and operational specifications. For the interaction between heterogenous representation schemes and abstract representations the concept of logical viewpoints is introduced...|$|R
40|$|A {{routine is}} a habitually {{repeated}} performance of some actions. Agents use routines {{to guide their}} everyday activities and to enrich their abstract concepts about acts. This dissertation addresses {{the question of how}} an agent who is engaged in ordinary, routine activities changes its behavior over time, how the agent's internal representations about the world is affected by its interactions, and what is a good agent architecture for learning routine interactions with the world. In it, I develop a theory that proposes several key processes: (1) automaticity, (2) habituation and skill refinement, (3) abstraction-bychunking, and (4) discovery of new <b>knowledge</b> <b>chunks.</b> The process of automaticity caches the agent's knowledge about actions into a flat stimulus-response data structure that eliminates knowledge of action consequences. The stimulus-response data structure produces a response to environmental stimuli in constant time. The process of habituation and skill refinement uses environm [...] ...|$|R
40|$|INTRODUCTION Much of the detectives' and magistrates' task can be {{regarded}} as "knowledge processing. " They have, typically, to acquire <b>knowledge</b> <b>chunks,</b> find the contradictions inside and across the various depositions, link the consistent hypotheses in a causally connected lattice and judge the credibility of the information and the reliability of the witnesses. * Sometimes, the complexity of the case could justify the assistance of an appropriate Decision Support System (DSS); we call Inquiry Support System (ISS) this specialized DSS. A first task of an intelligent ISS could be that of generating stereotypical hypotheses about the case under consideration. However, the ultimate task of an ISS should be that of providing a most credible and coherent set of beliefs about the case. Part of these beliefs comes from investigations on the spot and verified facts, part from the depositions of the various witnesses, part from hypotheses introduced by the detective hi...|$|R
40|$|We {{present the}} {{architecture}} of an Inquiry Support System whose aim is to help a detective or a judge in: 1. generating hypotheses (automatically in some stereotypical cases), 2. eliciting a maximally consistent set of beliefs as the most believable piece of knowledge to reason with; this is done by: 2. 1. finding the incoherences inside and across the various depositions, 2. 2. generating the alternate maximally consistent sets of beliefs, 2. 3. estimating the credibilities of the various evidences, 2. 4. estimating the reliabilities of the various informants. The solution of the case {{is intended to be}} searched among the various possible plots compatible with the maximally consistent set of beliefs retained by the system as the most believable one. 1. INTRODUCTION 1. 1 Belief Revision Much of the detectives' and magistrates' task can be regarded as "knowledge processing. " They have, typically, to acquire <b>knowledge</b> <b>chunks,</b> find the contradictions inside and across the various depositions, l [...] ...|$|R
40|$|The aim of {{the paper}} is to {{establish}} an ontology-based case encoding tool with sufficient formalization and expansibility to assist users for organizing the case information and increasing the feasibility of the design knowledge in a case library. The tool is named Open Case Study (OCS). OCS is a formalized and expandable tool for authoring metadata of a case library and organizing them by their semantic ontology. By using the templates constructed by design experts, such as design teachers or experienced architects, OCS provides the user with explicit but adaptable guidelines for case analysis and encoding. OCS then performs the searching and mapping function provided by Open Ontology. Thus, when the user is encoding the information segments of cases, relevant <b>knowledge</b> <b>chunks</b> in the case library can then be immediately provided, such as relevant senses in similar cases, all atoms of a relevant sense, and known value ranges of a relevant property. This assists users to avoid data mistake and duplication in encoding design cases...|$|R
40|$|This paper {{introduces}} {{our current}} research progress {{in creating a}} new architecture and framework for Knowledge Management System (KMS) that borrows and builds upon ideas from the highly successful blog and wiki technologies. We call our framework KlogMS (Klog Management System). Klog (knowledge weblog) is the term commonly used for weblogs that contains content for knowledge sharing, usually professional opinions on a particular topic. Just like personal blogs, these klogs can be highly cross-linked with other related klogs, forming a virtual community of "expert " opinions, enabling the readers {{to be exposed to}} all points of view. Our research focuses on creating a new type of KMS framework that supports klogging within an organization for the purpose of internal knowledge sharing, borrowing ideas and features found in blog and wiki frameworks and from AI and psychology. The core technology behind KlogMS is an engine that performs what is equivalent to the concept of "knowledge chunking " on this repository. The paper explains the design objectives behind KlogMS, requirements for klogging as a KM tool, our architecture design, the <b>knowledge</b> <b>chunking</b> process, and the state of our prototype...|$|R
40|$|Transfer of {{research}} results in production systems requires, among others, that knowledge be explicit and understandable by stakeholders. Such transfer is demanding, {{as so many}} researchers have been studying alternative ways to classic approaches such as books and papers that favour knowledge acquisition on behalf of users. In this context, we propose the concept of Knowledge Experience Package (KEP) with a specific structure as an alternative. The KEP contains both the conceptual model(s) of the research results which make up the innovation, including all the necessary documentation ranging from papers or book chapters; and the experience collected in acquiring it in business processes, appropriately structured. The structure allows {{the identification of the}} <b>knowledge</b> <b>chunk(s)</b> that the developer, who is acquiring the knowledge, needs in order to simplify the acquisition process. The experience is needed to point out the scenarios that the user will most likely face and therefore refer to. Both structure and experience are important factors for the innovation transferability and efficacy. Furthermore, we have carried out an experiment which compared the e±cacy of this instrument with the classic ones, along with the comprehensibility of the information enclosed in a KEP rather than in a set of Papers. The experiment has pointed out that knowledge packages are more effective than traditional ones for knowledge transfe...|$|R
40|$|Several {{studies have}} found {{learning}} of biconditional grammars only under intentional rule-search conditions (e. g., Johnstone & Shanks, 2001). Memorization of strings merely led to the learning of chunks. We used a musical grammar, a diatonic inversion, that {{is a type of}} biconditional grammar. Participants either were required to memorize a set of grammatical tunes (incidental learning), or were asked to search for the underlying rule whilst being given feedback about their performance (intentional learning). The results showed that participants in the incidental-learning condition did not learn the inversion rule and merely acquired explicit <b>knowledge</b> about <b>chunks.</b> However, participants in the intentional-learning condition learnt both the inversion rule and chunks...|$|R
40|$|Often {{knowledge}} engineers encounter situations {{during the}} interviewing {{process in which}} experts have difficulty expressing the knowledge to be captured. In these situations, the experts cannot readily present their knowledge so that the knowledge engineers can encode it in the chosen formalism (for example, in production rules). During {{the development of an}} expert system for underwriting homeowner insurance policies, this situation was occasionally encountered. When the experts could not express their <b>knowledge</b> in <b>chunks</b> suitable for encoding directly in production rules, circuit minimization techniques were used to construct the set of production rules from exhaustive tables of acquired knowledge. The techniques also served to find errors in the acquired knowledge. Circuit minimization techniques, therefore, have been found to provide valuable assistance in the knowledge engineering process, both in the acquisition and verification of knowledge...|$|R
40|$|Dominant {{theories}} of implicit learning assume that implicit learning merely involves {{the learning of}} chunks of adjacent elements in a sequence. In the experiments presented here, participants implicitly learned a nonlocal rule, thus suggesting that implicit learning can go beyond the learning of chunks. Participants were exposed {{to a set of}} musical tunes that were all generated using a diatonic inversion. In the subsequent test phase, participants either classified test tunes as obeying a rule (direct test) or rated their liking for the tunes (indirect test). Both the direct and indirect tests were sensitive to <b>knowledge</b> of <b>chunks.</b> However, only the indirect test was sensitive to knowledge of the inversion rule. Furthermore, the indirect test was overall significantly more sensitive than the direct test, thus suggesting that knowledge of the inversion rule was below an objective threshold of awareness...|$|R
40|$|Implicit {{learning}} {{abilities of}} nine amnesic patients were explored {{by using an}} artificial grammar learning task in which the test strings were constructed {{in such a way}} that grammaticality judgments could not be based on a simple knowledge of bigrams and trigrams (chunks). Results show that amnesic patients and controls performed at the same level during the classification task, whereas amnesic patients performed worse than controls in an explicit generation task. Moreover, there was no correlation between the implicit and explicit measures. These results are compatible with the existence of two kinds of representation intervening in artificial grammar learning. The first one based on processes leading to fragment-specific <b>knowledge</b> (the <b>chunks,</b> which can be accessed explicitly), and the second based on the learning of simple associations and more complex conditional relations between elements. (C) 2003 Elsevier Science (USA). All rights reserved. Peer reviewe...|$|R
40|$|International audienceIn many applications, the {{management}} of geographic knowledge is very important especially not only for urban and environmental planning, but also for any application in territorial intelligence. However there are several practical problems hindering the efficiency, some of them being technical and other being more conceptual. The goal {{of this paper is}} to present a tentative conceptual framework for managing practical geographic knowledge taking account of accuracy, rotundity of earth, the mobility of objects, multiple-representation, multi-scale, existence of sliver polygons, differences in classifying real features (ontologies), the many-to-many relationship of place names (gazetteers) and the necessity of interoperability. In other words, this framework must be robust against scaling, generalization and small measurement errors. Therefore, geographic objects must be distinguished into several classes of objects with different properties, namely geodetic objects, administrative objects, manmade objects and natural objects. Regarding spatial relations, in addition to conventional topological and projective relations, other relations including tessellations and ribbon topology relations are presented in order to help model geographic objects by integrating more practical semantics. Any conceptual framework is based on principles which are overall guidelines and rules; moreover, principles allow at making predictions and drawing implications and are finally the basic building blocks of theoretical models. But before identifying the principles, one needs some preliminary considerations named prolegomena. In our case, principles will be essentially rules for transforming geographic knowledge whereas prolegomena will be assertions regarding more the foundations of geographic science. Based on those considerations, 12 principles are given, preceded by 12 prolegomena. For instance, some principles deal with the transformation of spatial relationships based on visual acuity and granularity of interest, with the influence of neighboring information and cross-boundary interoperability. New categories of geographic knowledge types are presented, spatial facts, cluster of areas, flows of persons, goods, etc., topological constraints and co-location rules. To represent <b>knowledge</b> <b>chunks,</b> three styles are presented, based respectively on descriptive logics, XML and visual languages. To conclude this paper, after having defined contexts of interpretation, an example of visual language to manage geographic knowledge is proposed...|$|R
40|$|We {{have been}} {{witnessing}} {{an explosion of}} user involvement in knowledge creation, publication and access both from within and between organisations. This {{is partly due to}} the widespread adoption of Web technology. But, it also introduces new challenges for knowledge engineers, who have to find suitable ways for sharing and integrating all this <b>knowledge</b> in meaningful <b>chunks.</b> In this paper we are exposing our experiences in using two technologies for capturing, representing and modelling semantic integration that are relatively unknown to the integration practitioners: Information Flow and Formal Concept Analysis...|$|R
40|$|Linguistic {{adaptation}} is {{a phenomenon}} where language representations change in response to linguistic input. Adaptation can occur on multiple linguistic levels such as phonology (tuning of phonotactic constraints), words (repetition priming), and syntax (structural priming). The persistent nature of these adaptations suggests {{that they may be}} a form of implicit learning and connectionist models have been developed which instantiate this hypothesis. Research on implicit learning, however, has also produced evidence that explicit <b>chunk</b> <b>knowledge</b> is involved in the performance of these tasks. In this review, we examine how these interacting implicit and explicit processes may change our understanding of language learning and processing...|$|R
40|$|Adults {{can expand}} their limited working memory {{capacity}} by using stored conceptual <b>knowledge</b> to <b>chunk</b> items into interrelated units. For example, adults {{are better at}} remembering the letter string PBSBBCCNN after parsing it into three smaller units: the television acronyms PBS, BBC, and CNN. Is this chunking a learned strategy acquired through instruction? We explored the origins of this ability by asking whether untrained infants can use conceptual knowledge to increase memory. In {{the absence of any}} grouping cues, 14 -month-old infants can track only three hidden objects at once, demonstrating the standard limit of working memory. In four experiments we show that infants can surpass this limit when given perceptual, conceptual, linguistic, or spatial cues to parse larger arrays into smaller units that are more efficiently stored in memory. This work offers evidence of memory expansion based on conceptual knowledge in untrained, preverbal subjects. Our findings demonstrate that without instruction, {{and in the absence of}} robust language, a fundamental memory computation is available throughout the lifespan, years before the development of explicit metamemorial strategies...|$|R
40|$|In {{a unified}} {{knowledge}} representation, data, information and knowledge are all represented {{in a single}} formalism. A unified knowledge representation based on "items" is described. Items contain two classes of constraints that apply equally to knowledge and to data. Items are compared to an if-then, or rule-based, <b>knowledge</b> representation. Simple <b>chunks</b> of <b>knowledge</b> {{that can only be}} represented by a number of rules are represented as single items. Rule-based formalisms are prone to the introduction of potential maintenance hazards caused by one rule being hidden within another. A single operation for items enables some of these hidden relationships to be removed. Items make i...|$|R
40|$|While {{traditionally}} {{information systems}} (IS) PhD students first completed a thesis then published, today they often publish before completing. While at first publishing seems yet another PhD student burden, {{it can be}} a useful learning experience, raise motivation, provide helpful feedback, and unite student and advisor. PhD publications also help with grant and job applications. However PhD research publishing is an extra demand, suggesting the need for a learning support tool. The PhD publishing checklist proposed: 1. <b>Chunks</b> <b>knowledge</b> into elements, for easier handling. 2. Grounds the elements, with examples and summary statements, for practicality, and 3. Structures the elements, in academic format, for easier location. Research is currently underway, to discover the value of this checklist...|$|R
40|$|While {{traditionally}} {{information systems}} (IS) students graduated then published, today they often publish before they graduate. While publishing seems yet another student burden, {{it can be}} a useful learning experience, raise motivation, provide helpful feedback, help grant and job applications, and give student and advisor a common focus. That research publishing is an extra demand suggests the need for a support tool. The research publishing checklist: 1. <b>Chunks</b> <b>knowledge</b> into elements for easier handling. 2. Grounds elements with practical examples and summary statements, and 3. Structures the elements in academic format for easy location. It can be used not only in student advising, but for new authors in any context, whether conference, journal or book chapter. The checklist is available a...|$|R
40|$|Rule bases are {{commonly}} acquired, by expert and/or knowledge engineer, {{in a form}} which is well suited for acquisition purposes. When the knowledge base is executed, however, a different structure may be required. Moreover, since human experts normally do not provide the <b>knowledge</b> in compact <b>chunks,</b> rule bases often suffer from redundancy. This may considerably harm efficiency. In this paper a procedure is examined to transform rules that are specified in the knowledge acquisition process into an efficient rule base by way of decision tables. This transformation algorithms allows the generation of a minimal rule representation of the knowledge, and verification and optimization of rule bases and other specification (e. g. legal texts, procedural descriptions, [...] .). The proposed procedures are fully supported by the PROLOGA tool. ...|$|R
40|$|Indiana University-Purdue University Indianapolis (IUPUI) The {{amount of}} {{information}} produced {{in the form of}} electronic free text in healthcare is increasing to levels incapable of being processed by humans for advancement of his/her professional practice. Information extraction (IE) is a sub-field of natural language processing with the goal of data reduction of unstructured free text. Pertinent to IE is an annotated corpus that frames how IE methods should create a logical expression necessary for processing meaning of text. Most annotation approaches seek to maximize meaning and <b>knowledge</b> by <b>chunking</b> sentences into phrases and mapping these phrases to a knowledge source to create a logical expression. However, these studies consistently have problems addressing semantics and none have addressed the issue of semantic similarity (or synonymy) to achieve data reduction. To achieve data reduction, a successful methodology for data reduction is dependent on a framework that can represent currently popular phrasal methods of IE but also fully represent the sentence. This study explores and reports on the benefits, problems, and requirements to using the predicate-argument statement (PAS) as the framework. A convenient sample from a prior study with ten synsets of 100 unique sentences from radiology reports deemed by domain experts to mean the same thing will be the text from which PAS structures are formed...|$|R
50|$|First, the {{knowledge}} of individual subject-matter experts engaged in knowledge engineering often is not fully integrated when dealing with complex problems, at least initially. Rather, this knowledge may exist in a somewhat more loosely organized state, a sort of <b>knowledge</b> soup with <b>chunks</b> of <b>knowledge</b> floating about in it. A common observation of knowledge engineers experienced in graphically designing knowledgebases is {{that the process of}} constructing a graphic representation of problem-solving knowledge in a formal logical framework seems to be synergistic, with new insights into the expert’s knowledge emerging as the process unfolds. (At the moment, this assertion is largely anecdotal. Contributors to this article need to find a suitable way to document this point, because it is actually a rather important finding not simply limited to NetWeaver, but knowledge engineering more broadly).|$|R
40|$|A single {{experiment}} is reported that investigated implicit learning using a conjunctive rule set applied to natural words. Participants memorized a training list consisting {{of words that}} were either rare-concrete and common-abstract or common-concrete and rare-abstract. At test, they were told of the rule set, but not told what it was. Instead, they were shown all four word types and asked to classify words as rule-consistent words or not. Participants classified the items above chance, but were unable to verbalize the rules, even when shown a list that included the categories {{that made up the}} conjunctive rule and asked to select them. Most participants identified familiarity as the reason for classifying the items as they did. An analysis of the materials demonstrated that conscious micro rules (i. e., <b>chunk</b> <b>knowledge)</b> could not have driven performance. We propose that such materials offer an alternative to artificial grammar for studies of implicit learning...|$|R
30|$|After a {{successful}} interaction, the agent {{that started the}} communication is updated. For simplicity, {{no change in the}} respondent’s state is produced, assuming that knowledge, being an intangible good, does not decrease when shared, and there is no cost of processing and transmission. Skill and interest are always non-negative quantities. For the skill parameter associated to each topic an agent owns, we assumes that it simply increases in chunks calculated as a fraction of the knowledge difference between two interacting agents. This implies that in subsequent interactions between two agents the less skilled one accumulates <b>knowledge</b> in <b>chunks</b> of diminishing size. In this simple model of knowledge transmission, skill is a monotonic positive function with diminishing marginal increments. For completeness, although not specifically relevant for this work, the model also considers the effect of trust as an enabling factor for sharing knowledge: the more the trust between the two agents, the better the diffusion of knowledge. Here the assumption is that trust between two interacting agents is built with successful communication. In this case, when agents interact for the first time, the <b>chunk</b> of <b>knowledge</b> transferred is reduced by a discounting factor representing the absence of trust. This discount factor progressively vanishes with subsequent communications. This is a simplified form of trust (and distrust) modeling, but motivations could be found in the literature about collective behavior [39, 40] and refers both to the prevalence of egocentrism in assimilating new information and to trust dynamics.|$|R
40|$|Soar is an {{architecture}} {{for a system}} that is intended to be capable of general intelligence. Chunking, a simple experience-based learning mechanism, is Soar's only learning mechanism. Chunking creates new items of information, called chunks, {{based on the results of}} problem-solving and stores them in the <b>knowledge</b> base. These <b>chunks</b> are accessed and used in appropriate later situations to avoid the problem-solving required to determine them. It is already well-established that chunking improves performance in Soar when viewed in terms of the subproblems required and the number of steps within a subproblem. However, despite the reduction in number of steps, sometimes there may be a severe degradation in the total run time. This problem arises due to expensive chunks, i. e., chunks that require a large amount of effort in accessing them from the knowledge base. They pose a major problem for Soar, since in their presence, no guarantees can be given about Soar's performance...|$|R
40|$|As a {{research}} topic computer game playing has contributed problems to AI that manifest exponential {{growth in the}} problem space. For the most part, in games such as chess and checkers these problems have been surmounted with enormous computing power on brute-force search methods using massive databases. It {{remains to be seen}} whether such techniques will extend to other games such as go and shogi. One suggestion is that these games and even chess might benefit from a knowledge-based treatment but such approaches have met with limited success. The problem, as ever from such approaches, is the characterisation of the knowledge to be used by the system. This paper deals with the Tal system, which employs case-based reasoning techniques for chess playing. In the paper, rather than focus on playing, we concentrate on the automatic generation of suitable case <b>knowledge</b> using a <b>chunking</b> technique on a corpus of grandmaster games. 1 Introduction Many games like chess and checker [...] ...|$|R
40|$|TCD-CS- 95 - 16 As a {{research}} topic computer game playing has contributed problems to AI that manifest exponential {{growth in the}} problem space. For the most part, in games such as chess and checkers these problems have been surmounted with enormous computing power on brute-force search methods using massive databases. It {{remains to be seen}} whether such techniques will extend to other games such as go and shogi. One suggestion is that these games and even chess might benefit from a knowledge-based treatment but such approaches have met with limited success. The problem, as ever from such approaches, is the characterisation of the knowledge to be used by the system. This paper deals with the Tal system, which employs case-based reasoning techniques for chess playing. In the paper, rather than focus on playing, we concentrate on the automatic generation of suitable case <b>knowledge</b> using a <b>chunking</b> technique on a corpus of grandmaster games...|$|R
40|$|Slow {{restoration}} due to chunk fragmentation is {{a serious}} problem facing inline chunk-based data deduplication systems: restore speeds for the most recent backup can drop orders of magnitude over the lifetime of a system. We study three techniques—increasing cache size, container capping, and using a forward assembly area— for alleviating this problem. Container capping is an ingest-time operation that reduces chunk fragmentation at the cost of forfeiting some deduplication, while using a forward assembly area is a new restore-time caching and prefetching technique that exploits the perfect <b>knowledge</b> of future <b>chunk</b> accesses available when restoring a backup {{to reduce the amount of}} RAM required for a given level of caching at restore time. We show that using a larger cache per stream—we see continuing benefits even up to 8 GB—can produce up to a 5 – 16 X improvement, that giving up as little as 8 % deduplication with capping can yield a 2 – 6 X improvement, and that using a forward assembly area is strictly superior to LRU, able to yield a 2 – 4 X improvement while holding the RAM budget constant. ...|$|R
40|$|Adriaan de Groot, the Dutch {{psychologist}} and chess Master, argued that “perception and memory {{are more important}} differentiators of chess expertise than the ability to look ahead in selecting a chess move” (Groot 1978). A component of expertise in chess {{has been attributed to}} the expert having knowledge of ‘chunks’ and this knowledge gives the expert the ability to focus quickly on “good moves with only moderate look-ahead search” (Gobet and Simon 1998). The effects of chunking in chess are widely reported in the literature, however papers reporting the nature of chunks are largely based on inference from psychological experimentation. This thesis reports original work resulting from extensive data mining {{of a large number of}} chessboard configurations to explore the nature of chunks within the game of chess and the associated moves played by expert chess players. The research was informed by work in the psychology of chess and explored with software engineering techniques, employing large datasets consisting of transcripts from expert players games. The thesis reports results from an analysis of chunks throughout the game of chess, explores the properties of meaningful chunks and reports effects of the application of <b>chunk</b> <b>knowledge</b> to move searching. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|The {{performance}} of large-scaled peer-to-peer (P 2 P) video-on-demand (VoD) streaming {{systems can be}} very challenging to analyze. In practical P 2 P VoD systems, each peer only interacts with {{a small number of}} other peers/neighbors. Further, its upload capacity may vary randomly, and both its downloading position and content availability change dynamically. In this paper, we rigorously study the achievable streaming capacity of large-scale P 2 P VoD systems with sparse connectivity among peers, and investigate simple and decentralized P 2 P control strategies that can provably achieve close-to-optimal streaming capacity. We first focus on a single streaming channel. We show that a close-to-optimal streaming rate can be asymptotically achieved for all peers with high probability as the number of peers N increases, by assigning each peer a random set of Θ(log N) neighbors and using a uniform rate-allocation algorithm. Further, the tracker does not need to obtain detailed <b>knowledge</b> of which <b>chunks</b> each peer caches, and hence incurs low overhead. We then study multiple streaming channels where peers watching one channel may help in another channel with insufficient upload bandwidth. We propose a simple random cache-placement strategy, and show that a close-to-optimal streaming capacity region for all channels can be attained with high probability, again with only Θ(logN) per-peer neighbors. These results provide important insights into the dynamics of large-scale P 2 P VoD systems, which will be useful for guiding the design of improved P 2 P control protocols. © 2013 IEEE. published_or_final_versio...|$|R
40|$|Ankara : Institute of Economics and Social Sciences of Bilkent University, 1997. Thesis (Master's) [...] Bilkent University, 1997. Includes bibliographical {{references}} leaves 83 - 85 The Turkish {{education system}} has frequently been characterized as {{based on a}} read and repeat model, imposing <b>knowledge</b> in <b>chunks</b> based on memorization, instead of enabling the individual to think creatively, solve problems and interpret information. The latter set of skills have been collectively labeled as critical thinking (CT). In its basic form, critical thinking in reading (CR) is held to promote the student's ability to think autonomously, {{by being able to}} make judgments and predictions, draw conclusions, make inferences, and detect biases during reading. CR is particularly important as these skills are needed to be developed for success in academic studies and post university professions. This study sought to identify the factors that can promote CR. The research questions asked in the study were as follows: 1. What are the factors that teachers judge "critical" in CR? 2. To what extent do students use CR skills in carrying out reading tasks? 3. Which instructional procedures do teachers employ to promote CR? 4. What instructional procedures do students think can provide them with effective use of CR skills? These research questions were investigated by administration of questionnaires to students and their teachers at Erciyes University and through interviews with the teachers of the subject students. The student questionnaire included a reading passage with assigned tasks requiring application of CR skills. These tasks were given to determine the actual performance of the students in terms of CR. The subjects (students and teachers) were asked to rate CR skills according to their perceived importance. In addition, instructors were asked to rate the frequency and kind of CT activities used in their reading classes. The findings of the study are controversial since the responses given by students and teachers were not consistent with one another. Teachers supported the idea that students need more practice, and hence, further enchamcement of CT skills in reading. In contrast, the majority of students indicated little need for the enhancement of CT skills in reading and very few stated that they lacked these skills. The results of the reading text assessment indicated that student performance was not consistent with their questionnaire responses and that, generally, they lacked the ability to perform CR tasks. The mismatch in the answers of students and teachers indicates further need for instructional focus on CR skills. Akyüz, BirolM. S...|$|R
40|$|Peer-to-Peer (P 2 P) {{streaming}} technologies {{can take}} advantage of the upload capacity of clients, and hence can scale to large content distribution networks with lower cost. It is highly desirable that most of the control decisions be decentralized, with each peer only interacting with a small number of other peers/neighbors. However, this requirement has made the analytical study of the performance of P 2 P systems particularly challenging. In this thesis, we rigorously study the achievable streaming capacity of large-scale P 2 P streaming systems, either live streaming or video-on-demand, with sparse connectivity among peers, and investigate simple and decentralized P 2 P control strategies that can provably achieve close-to-optimal streaming capacity. We show that even with a random peer-selection algorithm and uniform rate allocation, as long as each peer maintains Θ (log N) downstream neighbors, where N is the total number of peers in the system, the system can asymptotically achieve a streaming rate that is close to the optimal streaming rate of a complete network. Further, the tracker does not need to obtain detailed <b>knowledge</b> of which <b>chunks</b> each peer caches, and hence incurs low overhead. We then study multiple streaming channels where peers watching one channel may help in another channel with insufficient upload bandwidth. We propose a simple random cache-placement strategy, and show that a close-to-optimal streaming capacity region for all channels can be attained with high probability, again with only Θ (log N) per-peer neighbors. A distinct advantage of the cache placement strategy is its robustness to imprecise estimates of system parameters such as video popularity and upload bandwidth distribution. Simulation results are provided to verify our analysis. ...|$|R
