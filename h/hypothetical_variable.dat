6|40|Public
50|$|In statistics, a {{mediation}} {{model is}} one that seeks to identify and explain the mechanism or process that underlies an observed relationship between an independent variable and a dependent variable via {{the inclusion of a}} third <b>hypothetical</b> <b>variable,</b> known as a mediator variable (also a mediating variable, intermediary variable, or intervening variable). Rather than a direct causal relationship between the independent variable and the dependent variable, a mediation model proposes that the independent variable influences the (non-observable) mediator variable, which in turn influences the dependent variable. Thus, the mediator variable serves to clarify {{the nature of the relationship}} between the independent and dependent variables.|$|E
40|$|A novel {{approach}} {{to deal with}} the stereo correspondence problem induced by the implicit assumptions made by cost aggregation (CA) strategies is proposed. CA relies on the implicit assumption that disparity varies smoothly within neighboring points except at depth discontinuities and state-of-the-art CA strategies adapt their support to image content by classifying each pixel based on geometric and photometric constraints. Our proposal explicitly models this behavior from a different perspective, by gathering for each point, multiple assumptions that locally would be made by a <b>hypothetical</b> <b>variable</b> CA strategy. This framework enables to derive a function that locally captures the plausibility of the underlying geometric and photometric constraints independently enforced by supports of neighboring points. Experimental results confirm the effectiveness of our proposal. 1...|$|E
40|$|The {{object of}} this {{research}} is to compare and contrast delinquent and non-delinquent boys on certain aspects of parent and peer relations. It should be obvious at the outset that this study was not designed to discover all the causes of delinquency, rather it was assumed that nay one boy is a delinquent for a complex variety of reasons. Two of the most complex being the effects of family and peer relationships. The main aim is to show that although juvenile delinquency is largely associated with the lower socio-economic groups, a major difference between delinquents and non-delinquents will be shown in the different kinds of attachment that the boys show towards their parents. Throughout attachment to parents is emphasized as a crucial <b>hypothetical</b> <b>variable,</b> as it is thought that adequate parental attachment is a very important aid to an effective socialization process...|$|E
40|$|The {{theoretical}} framework on the cognitive information processing holds that mental {{processes such as}} formation of beliefs, attitudes, or perceptions are impossible to directly observe and measure. This condition is resolved by implementing questionnaires integrated with <b>hypothetical</b> <b>variables</b> which can measure individual responses as real entities. <b>Hypothetical</b> <b>variables</b> related to individual personality features {{in this study were}} ascription of responsibility, universal values, personal skills, and awareness of consequences. Universal values discern on inherent conflicts among people 9 ̆ 2 s motivational goals. Personal skills are concerned with the capacity to understand the intentions of others and oneself. Ascription of responsibility and awareness of consequences explain people 9 ̆ 2 s desire to take action. The four factors are statistically highly related and proved explaining the construct of sustainable behavior (SB) (Juárez-Nájera, 2009). SB as is defined in this work is an effective disposition to act. It was tested by applying a 67 -issue questionnaire among 106 participants, 69 Mexicans from the Universidad Autónoma Metropolitana, Azcapotzalco campus and 37 Germans from Leuphana Universitaet Lueneburg, Institut fur Umweltkommunication. SB was validated by means of an exploratory principal component analysis (PCA) which searches a factor structure underlying the <b>hypothetical</b> <b>variables.</b> To this end, this paper presents a pattern of the first components found by the PCA and the representative relations of the personality factors which explains sustainable behavior across participants in higher education institutions from two countries with vastly different cultures and economies. Ascription of responsibility appears as the main personal factor...|$|R
50|$|From the {{perspective}} of Pearl and his colleagues, a major shortcoming of RCM is that all assumptions and background knowledge pertaining to a given problem must first be translated into the language of counterfactuals (e.g., ignorability) before analysis can commence. In SEM, by comparison, Pearl (2000) and Heckman (2008)hold that background knowledge is expressed directly in the vocabulary of ordinary scientific discourse, invoking cause-effectrelationships among realizable, not <b>hypothetical</b> <b>variables.</b>|$|R
5000|$|Sometimes latent {{variables}} {{correspond to}} aspects of physical reality, which could in principle be measured, {{but may not}} be for practical reasons. In this situation, the term hidden variables is commonly used (reflecting {{the fact that the}} variables are [...] "really there", but hidden). Other times, latent variables correspond to abstract concepts, like categories, behavioral or mental states, or data structures. The terms <b>hypothetical</b> <b>variables</b> or <b>hypothetical</b> constructs may be used in these situations.|$|R
40|$|The Project Team {{accomplished}} two tasks {{during the}} third quarter: preparation and presentation of professional papers; and development of simulation models and sub models of the <b>hypothetical</b> <b>variable</b> wall mining installation. The project team also continued its search for the suitable animation software to be adapted to the underground mining systems. Meanwhile work is progressing {{along the lines of}} updating the original open loop flow diagram that deals with the automatic control of the thrusting, advance, and rotation of the auger train which both cuts (extracts) and transports the coal across the face. The team is integrating the control systems into a deterministic mathematical equation for optimizing the mining and material flow rate in the operating system. The long range plan is to integrate the current deterministic equations in a suitable animation program with a number of adjustable and controllable parameters. This will enable coal operators and engineers to visualize how the variations can affect the safety, cost and production levels of the system...|$|E
40|$|Abstract The article {{presents}} {{development of a}} closed-form corridor choice model under <b>hypothetical</b> <b>variable</b> message sign (VMS) based traffic information. A single VMS board is assumed to display traffic information at a junction of two alternative and competitive traffic corridors connecting two catchment areas in Kolkata city, India. The corridor choice models are developed by combining revealed preference (RP) and stated preference (SP) data sources. The development of a combined RP-SP model is a challenging task as different data sources have their respective error terms. In this work, the data sources are combined by exploiting their respective merits; while discarding their respective de-merits. Here a procedure of developing composite utility function is presented, which is constituted of estimates of attributes from SP data source and alternative specific constant term of alternatives calibrated from RP data source, while fixing all coefficients of attributes at SP estimates. The construction of corridor choice models is demonstrated for two types of VMS based traffic information, which differs {{in terms of the}} content displayed on VMS board and also for two types of trip maker- namely private car and taxi. Under the influence of VMS-based traffic information, trip makers are found to take corridor choice decision based on the rational trade-off between travel time information and direct travel cost of alternative traffic corridors. The alternative-specific-constant term of choice models indicates that in presence of VMS-based information, private car trip makers are likely to be less biased to choose comparatively longer but almost free-flow traffic corridor (thereby less travel time corridor); while taxi trip makers {{are likely to be more}} biased to choose longer but almost free-flow traffic corridor to arrive at their destination...|$|E
40|$|This study {{seeks to}} {{identify}} {{factors that contribute}} to why county extension agents choose to stay employed with the Texas A&M AgriLife Extension Service. Demographics that have been identified define the relationship levels that exist between organizational, work and non-work individual related factors with demographics of why county extension agents choose to remain employed with Extension. The data for the target population were collected from 560 Texas county Extension agents. A web-based questionnaire was used to collect data for this study. The questionnaire was adapted by the researcher from a previously used instrument conducted on county Extension agent turnover by the University of Kentucky Cooperative Extension Service. The questions were modified to reflect why agents stay with Extension as opposed to why agents leave Extension. The researcher used a Likert scale to measure attitudes, knowledge, perceptions, values, and behavior changes. Content validity of the questionnaire was established by a panel of Extension administrators. Data was analyzed using SPSS 2014 software package. Descriptive statistics were utilized to analyze the data including means, medians, standard deviations, percentages, and frequencies. Correlation matrix and reliability were calculated employing Cronbach?s alpha. Construct is the <b>hypothetical</b> <b>variable</b> that is being measured. All observed variables, except the demographic and open-ended items, were subjected to Shapiro-Wilk test for normality and were found to have a normal distribution. T-test (independent samples) were utilized to predict the dependent variables (organizational, individual work, and individual non-work factors) with the independent variables (demographic factors) with only two choices (ex: male and female) for reasons why county Extension agents choose to stay in Extension. Analysis of variance F-tests were utilized to predict the dependent variables with demographic factors with two or more choices (ex: ethnicity) for reasons why county Extension agents choose to stay in Extension. The ultimate goal and mission of Extension is carried out through employees. Retention of these employees and continuing to decrease employee turnover is paramount for Extension to attain its primary goal of education...|$|E
50|$|Construct {{validity}} {{refers to}} the extent to which the independent and dependent variables in a study represent the abstract <b>hypothetical</b> <b>variables</b> of interest. In other words, it has to do with whether the manipulated and/or measured variables in a study accurately reflect the variables the researcher hoped to manipulate. Construct validity also reflects the quality of one’s operational definitions. If a researcher has done a good job of converting the abstract to the observable, construct validity is high.|$|R
40|$|In this paper, the {{proportion}} of SC in each state, rural/urban composition and its decadal changes and the migration phe-nomena are studied using the technique developed by Tomba (2010). As suggested by Patterson and Greenwood (1960, 1969) prior migrants {{play an important role}} by providing shelter, help or guidance to the potential migrants. Taking the sex ratio and growth rate of some standard population, estimation of male migrants (opportunity looking) and settled mi-grants in the place of study can be made without using any dummy or <b>hypothetical</b> <b>variables.</b> The flow of migration and the net opportunity analysis can also be studied with the use of migration indices...|$|R
30|$|The {{purpose of}} the present study was to {{investigate}} the validity of IELTS LCT. In this context of IELTS LCT validation, the <b>hypothetical</b> <b>variables</b> were associated with a construct or test method (gap filling, multiple choice, diagram labelling, and short answer); here, the researchers first hypothesized a model and then examined whether the model is advocated by the present sample. The overall model fit in the phase 1 of the study provided evidence of construct validity for IELTS LCT, so the hypothesized SEM model enjoyed a good fit. What is hence clearly outlined in the analysis is that the individual items revealed to be valid indicators of their assumed factors or constructs, i.e., gap filling, diagram labelling, multiple choice, and short answer.|$|R
40|$|We {{propose a}} novel method for {{inferring}} whether X causes Y {{or vice versa}} from joint observations of X and Y. The basic idea is to model the observed data using probabilistic latent variable models, which incorporate the effects of unobserved noise. To this end, we consider the <b>hypothetical</b> effect <b>variable</b> to {{be a function of}} the <b>hypothetical</b> cause <b>variable</b> and an independent noise term (not necessarily additive). An important novel aspect of our work is that we do not restrict the model class, but instead put general non-parametric priors on this function and on the distribution of the cause. The causal direction can then be inferred by using standard Bayesian model selection. We evaluate our approach on synthetic data and real-world data and report encouraging results. ...|$|R
5000|$|The {{concept of}} a [...] "{{construct}}" [...] {{has a long history}} in science; it is used in many, if not most, areas of science. A construct is a <b>hypothetical</b> explanatory <b>variable</b> that is not directly observable. For example, the concepts of motivation in psychology and center of gravity in physics are constructs; they are not directly observable.|$|R
40|$|Modern cognitive-behavioural {{theories}} and therapies {{are based on}} the assumption that cognition is central to human behaviour. They assume that the assessment of cognitive variables through self-report is the appropriate way to understand human activity, and that a focus on altering these cognitive variables will form the basis of effective therapeutic techniques. This paper argues that cognitively oriented theories are empirically only moderately successful, and that as scientific theories they are untenable. The invocation of various types of unconscious cognition to explain human behaviour, without the definitional precision necessary for the assessment of these <b>hypothetical</b> <b>variables,</b> cannot be justified. I argue that contemporary theories which propose untestable models involving unobservable variables can be seen as efforts to shore up an unsatisfactory paradigm, and suggest that an emphasis on the development of new models which do not make the assumption of cognitive primacy may be the most fruitful for continued development in psychology...|$|R
30|$|When only an {{approximating}} discrete {{random variable}} is observable, estimation procedures employing the <b>hypothetical</b> continuous random <b>variables</b> are sometime biased and hence a discrete distribution is more appropriate for an observed data (Holland 1975).|$|R
40|$|If nonlocality {{is to be}} {{inferred}} from a violation of Bell's inequality, an important assumption is that the measurement settings are freely chosen by the observers, or alternatively, that they are random and uncorrelated with the <b>hypothetical</b> local <b>variables.</b> We demonstrate a connection between models that weaken this assumption, allowing partial correlation, and (i) models that allow classical communication between the distant parties, (ii) models that exploit the detection loophole. Even if Bob's choices are completely independent, all correlations from projective measurements on a singlet can be reproduced, with mutual information between Alice's choice and local variables {{less than or equal}} to one bit...|$|R
40|$|Abstract. Finger {{forces are}} known to change involun-tarily during {{multi-finger}} force-production tasks, even when a finger’s involvement in a task is not consciously changed (the enslaving effect). Furthermore, during maximal force-production (MVC) tests, the force pro-duced by a given finger in a multi-finger task is smaller than the force generated by this finger in its single-finger MVC test (the force-deficit effect). A set of <b>hypothetical</b> control <b>variables</b> – modes – is introduced. Modes can be estimated based on individual finger forces during single-finger MVC tests. We show that a simple formal model based on modes with only one free parameter accounts for finger forces during a variety of multi-finger MVC tests. The free parameter accounts for the force-deficit effect, and its value depends only on the numbe...|$|R
40|$|This paper {{describes}} the conjunctive counterpart of De Boeck and Rosenberg's hierarchical classes model. Both the original model and its conjunctive counterpart represent the set-theoretical {{structure of a}} two-way two-mode binary matrix. However, unlike the original model, the new model represents the row-column association as a conjunctive function {{of a set of}} <b>hypothetical</b> binary <b>variables.</b> The conjunctive nature of the new model further implies that it may represent some conjunctive higher order dependencies among rows and columns. The substantive significance of the conjunctive model is illustrated with empirical applications. Finally, it is shown how conjunctive and disjunctive hierarchical classes models relate to Galois lattices, and how hierarchical classes analysis can be useful to construct lattice models of empirical data. status: publishe...|$|R
40|$|Flow is a {{construct}} imported in marketing research from social sciences {{in order to}} examine consumer behavior in the online medium. The construct describes a state of deep involvement in a challenging activity, most frequently characterized by high levels of enjoyment, control and concentration. Researchers found that the degree to which online experience is challenging can be defined, measured, and related well to important marketing variables. As shown by our extensive literature review, flow measurements include antecedents, dimensions and consequences of flow. The present paper represents {{a detailed description of the}} construct`s operationalization in the context of online information search. In this respect, our main goal is to produce a basic instrument to evaluate the flow experience of online search, in order to capitalize on the premises of an interactive, complex informational medium â€“ the World Wide Web â€“ and on the consequence of an exploratory informational behavior of users. The instrument is conceived to offer a primal possibility to collect data. The composition, source and significance of the 11 scales used to measure the multiple factors of the flow experience during online search are detailed in this study with the aim to ensure the compliance with scientific rigors and to facilitate correct reports of data related to the reliability and validity of measurements. For further research, we propose factor analysis to test the resulted instrument and to ensure that the measures employed are psychometrically sound. Factor analysis refers to a wide range of statistic techniques used to represent a set of variables in concordance with a reduced number of <b>hypothetical</b> <b>variables</b> called factors. Factorial analysis is used to solve two types of problems: reducing the number of variables to increase data processing speed and identifying hidden patterns in the existent data relations. However, we expect our scales to perform different in the context of information search with second generation web tools and suggest further qualitative research concerning web determinants of flow...|$|R
40|$|Depression {{is one of}} {{the most}} {{insidious}} problems faced by older adults, and its incidence is increasing with the growth of an aging population. Koenig and Blazer reported that the prevalence of major depression was about 1 % among community-dwelling older adults and that less severe depressive disorder was present in over 25 %. 1 More-over, they reported that the rate of major depressive dis-order in older adult hospitalized patients with illness was more than 10 times greater than that of the unhospital-ized aging population. Depression is not only psychologi-cally traumatic but also quite costly 2 because it is related to psychosomatic symptoms resulting in a higher fre-quency of examination and prescription of drugs. Fur-thermore, depression also decreases the morale of older peo-ple and increases the risk of being housebound. Although it is very important to adequately diagnose and treat depression in its early stage, it often remains unrecognized or untreated. 3 One of the main reasons for this is that depressive symptoms often resemble those of the aging process itself, such as progressive cognitive deterioration or physical disabilities. 4 The Geriatric Depression Scale (GDS) is a self-admin-istered questionnaire with 30 items 5 and is recommended by the Royal College of Physicians and British Geriatrics Society as a valid screening method for depression in older adults. 6 A short form of the GDS (GDS- 15) was developed later 7 and was translated into Japanese. 8 The validity and reliability of the GDS- 15 have been confirmed in both community and hospital settings. 9 - 11 Several studies have subjected the GDS- 15 data to a factor analysis, which is a statistical technique to analyze interrelationships within a set of variables, resulting in the construction of a few <b>hypothetical</b> <b>variables.</b> To our knowledge, however, there has been only 1 study involving factor analysis of the Japanese version of the GDS- 15, reported by Schreiner et al in poststroke patients. 12 In addition, there have been few studies demonstrating the relationship between GDS- 15 factor loading and disabilities in the older population...|$|R
40|$|Biomechanics {{and motor}} control are {{discussed}} {{as parts of}} a more general science, physics of living systems. Major problems of biomechanics deal with exact definition of variables and their experimental measurement. In motor control, major problems are associated with formulating currently unknown laws of nature specific for movements by biological objects. Mechanics-based hypotheses in motor control, such as those originating from notions of a generalized motor program and internal models, are non-physical. The famous problem of motor redundancy is wrongly formulated; {{it has to be}} replaced by the principle of abundance, which does not pose computational problems for the central nervous system. Biomechanical methods {{play a central role in}} motor control studies. This is illustrated with studies with the reconstruction of <b>hypothetical</b> control <b>variables</b> and those exploring motor synergies within the framework of the uncontrolled manifold hypothesis. Biomechanics and motor control have to merge into physics of living systems, and the earlier this process starts the better...|$|R
40|$|If nonlocality {{is to be}} {{inferred}} from a violation of Bell's inequality, an important assumption is that the measurement settings are freely chosen by the observers, or alternatively, that they are random and uncorrelated with the <b>hypothetical</b> local <b>variables.</b> We study the case where this assumption is weakened, so that measurement settings and local variables are at least partially correlated. As we show, {{there is a connection}} between this type of model and models which reproduce nonlocal correlations by allowing classical communication between the distant parties, and a connection with models that exploit the detection loophole. We show that even if Bob's choices are completely independent, all correlations obtained from projective measurements on a singlet can be reproduced, with the correlation (measured by mutual information) between Alice's choice and local variables {{less than or equal to}} a single bit. Comment: 5 pages, 1 figure. v 2 Various improvements in presentation. Results unchange...|$|R
40|$|The primary {{objective}} of the thesis is to analyze, from an economic viewpoint, the determinants of cooperation among farmers in developing countries. In pursuing this objective, quite diverse but interrelated literature were reviewed: general economic theories of cooperation; property rights; origin {{and nature of the}} firm, and indigenous cooperation. From the review of the literature seven factors were deduced as determining agricultural cooperation, with six of them affecting cooperation in general. The validity of these in determining cooperation among farmers was then tested by means of case studies and a sample survey. First is {{the presence or absence of}} indivisibilities in productive inputs and/or processes. Second is whether timeliness in the execution of farm tasks is necessary. Third is the degree of elasticity of substitution among such inputs and/or processes. Fourth is the presence or absence of economies, whether size/scale economies or associational economies. Fifth is the extent of transaction costs, which consist of search, information, bargaining, decision-making and monitoring costs. Sixth is the extent of inequality in the distribution of income/assets among the potential members of a group. And seventh is the kind of atmosphere that surrounds the exchange relationship. Empirical research focused on Northern and Central Luzon in the Philippines. The most widespread form of indigenous cooperation in these two regions was the formation of labour teams to transplant and reap paddy. These were analyzed by means of a case study of two agricultural teams and a sample survey of twenty eight agricultural teams. The results indicated that six of the seven <b>hypothetical</b> <b>variables</b> were operative. Indivisibilities were not a relevant factor because they were absent from the farm tasks involved. Nevertheless, this cannot be interpreted to mean that this variable would be irrelevant in other tasks such as irrigation. Income/asset distribution was highly skewed but its potential for diminishing cooperation in the case of the two study teams was countered by the presence of other factors such as neighbourhood and kinship, which created a bond among members. However, it should showed up as a significant variable in the sample survey. As for the other variables, the results indicated that the existence of the labour teams and the degree of cooperation achieved were the result of (1) the necessity for the timely accomplishment of farm tasks; (2) a zero elasticity of substitution between labour and machinery; (3) the realization of both associational and scale economies; (4) low transaction costs; and (5) the existence of a non-calculative atmosphere...|$|R
40|$|This is a {{brief reply}} to S. Goldstein's article "Quantum theory without {{observers}} " in Physics Today. It is pointed out that Bohm's pilot wave theory is successful only because it keeps Schrodinger's (exact) wave mechanics unchanged, {{while the rest of}} it is observationally meaningless and solely based on classical prejudice. Key words: quantum theory, pilot wave theory, quantum trajectories, decoherence. In his recent article on "Quantum theory without observers " [1], Sheldon Goldstein raised a number of important questions about quantum theory. Even though I cannot quite understand what a universal physical theory without any concept of observers could mean, I agree with many of his critical remarks. Bell, who once objected "against measurement, " also pointed out that Bohm's quantum theory depends on the assumption that only its <b>hypothetical</b> classical <b>variables</b> can directly affect the consciousness of an observer | 2]. However, Goldstein's characterization of the consequences that have to be drawn from his criticism appears one-sided, since he neglects many essential aspects. In particular, he does not mention at all that Bohm's classical trajectories can neither be experimentally confirmed nor refuted if the theory is exactly valid. It is always possible to postulate otherwise unobservable (hence arbitrary) causes for stochastic events. Bohm's presumed ensemble of classical configurations is merely dynamically consistent because of their presumed unobservable dynam- 19...|$|R
40|$|The {{demonstration}} {{and use of}} nonlocality, as defined by Bell's theorem, rely strongly on dealing with non-detection events due to losses and detectors' inefficiencies. Otherwise, the so-called detection loophole could be exploited. The only way to avoid this is to have detection efficiencies that are above a certain threshold. We introduce the intermediate assumption of limited detection efficiency, that is, in each run of the experiment, the overall detection efficiency is lower bounded by $eta_{min} > 0 $. Hence, in an adversarial scenario, the adversaries have arbitrary large but not full control over the inefficiencies. We analyse the set of possible correlations that fulfill Limited Detection Locality (LDL) and show that they necessarily satisfy some linear Bell-like inequalities. We prove that quantum theory predicts the violation {{of one of these}} inequalities for all $eta_{min} > 0 $. Hence, nonlocality can be demonstrated with arbitrarily small limited detection efficiencies. We validate this assumption experimentally via a twin-photon implementation in which two users are provided with one photon each out of a partially entangled pair. We exploit on each side a passive switch followed by two measurement devices with fixed settings. Assuming the switches are not fully controlled by an adversary, nor by <b>hypothetical</b> local <b>variables,</b> we reveal the nonlocality of the established correlations despite a low overall detection efficiency...|$|R
40|$|The {{philosophy}} {{of science is}} the branch of philosophy that critically examines the foundations, assumptions, methods, products, and implications of the activ-ity called science. The present sketch reviews {{the historical development of}} the {{philosophy of}} science, representative individuals in the field, and topics of long-standing interest. The sketch is intended to prepare readers for subsequent discus-sions on behaviorism, cognitive psychology, and the meaning of mental terms. Key words: philosophy of science, logical positivism, operational definition, intervening <b>variable,</b> <b>hypothetical</b> construct, mediational neobehaviorism, radical behaviorism The philosophy of science is the branch of philosophy that critically examines the foundations, methods, products, and implications of the activity called science. Representative topics in the philosophy of science include (a) the origin and nature of scientific language (e. g., terms, concepts, statements, laws, theories, explanations, predictions), (b) the validity of scientific language (e. g., definitions, meanings, applications), (c) the nature of the scientific method...|$|R
40|$|A complex binary trait is a {{character}} that has a dichotomous expression but with a polygenic genetic background. Mapping quantitative trait loci (QTL) for such traits is difficult because of the discrete nature and the reduced variation in the phenotypic distribution. Bayesian statistics are {{proved to be a}} powerful tool for solving complicated genetic problems, such as multiple QTL with nonadditive effects, and have been successfully applied to QTL mapping for continuous traits. In this study, we show that Bayesian statistics are particularly useful for mapping QTL for complex binary traits. We model the binary trait under the classical threshold model of quantitative genetics. The Bayesian mapping statistics are developed {{on the basis of the}} idea of data augmentation. This treatment allows an easy way to generate the value of a <b>hypothetical</b> underlying <b>variable</b> (called the liability) and a threshold, which in turn allow the use of existing Bayesian statistics. The reversible jump Markov chain Monte Carlo algorithm is used to simulate the posterior samples of all unknowns, including the number of QTL, the locations and effects of identified QTL, genotypes of each individual at both the QTL and markers, and eventually the liability of each individual. The Bayesian mapping ends with an estimation of the joint posterior distribution of the number of QTL and the locations and effects of the identified QTL. Utilities of the method are demonstrated using a simulated outbred full-sib family. A computer program written in FORTRAN language is freely available on request...|$|R
40|$|The {{phenotypic}} {{observation of}} some reproduction traits (e. g., insemination success, interval from lambing to insemination) {{is the result}} of environmental and genetic factors acting on 2 individuals: the male and female involved in a mating couple. In animal genetics, the main approach (called additive model) proposed for studying such traits assumes that the phenotype is linked to a purely additive combination, either on the observed scale for continuous traits or on some underlying scale for discrete traits, of environmental and genetic effects affecting the 2 individuals. Statistical models proposed for studying human fecundability generally consider reproduction outcomes as the product of <b>hypothetical</b> unobservable <b>variables.</b> Taking inspiration from these works, we propose a model (product threshold model) for studying a binary reproduction trait that supposes that the observed phenotype is the product of 2 unobserved phenotypes, 1 for each individual. We developed a Gibbs sampling algorithm for fitting a Bayesian product threshold model including additive genetic effects and showed by simulation that it is feasible and that it provides good estimates of the parameters. We showed that fitting an additive threshold model to data that are simulated under a product threshold model provides biased estimates, especially for individuals with high breeding values. A main advantage of the product threshold model is that, in contrast to the additive model, it provides distinct estimates of fixed effects affecting each of the 2 unobserved phenotypes...|$|R
40|$|International audienceQuantum nonlocality {{stands as}} a {{resource}} for device independent quantum information processing (DIQIP). It finds repercussions in applications such as, among others, quantum key distribution and generation of randomness. In this work, we investigate two different approaches to attest nonlocality. First we follow the assumption of limited measurement dependence, i. e., that the measurement settings used in Bell inequality tests or DIQIP are partially influenced by the source of entangled particles and/or by an adversary. Then, we introduce the intermediate assumption of limi- ted detection efficiency, that is, in each run of the experiment, the overall detection efficiency is lower bounded by eta_min > 0. Hence, in an adversarial scenario, the adversaries have arbitrary large but not full control over the inefficiencies. We analyse the set of possible correlations that fulfil Measurement Dependence/Limited Detection Locality (MDL/LDL) and show that they necessarily satisfy some linear Bell-like inequalities. In both scenarios, quantum theory predicts the violation of such inequalities for l > 0 in the first case, and eta_min > 0 in the other. We validate these assumptions experimentally via a twin-photon implementation in which two users are provided each with one photon out of a partially entangled pair. On one hand, we show with the first inequality that the measurement independence assumption can be widely relaxed while still demonstrating quantum nonlocality. On the other hand with the second inequality, assuming the switches between the measurement bases are not fully controlled by an adversary, nor by <b>hypothetical</b> local <b>variables,</b> we reveal the nonlocality of the established correlations despite a low overall detection efficiency...|$|R
40|$|Generalized linear mixed models (GLMM) {{are used}} for {{inference}} and prediction {{in a wide range}} of different applications providing a powerful scientific tool for the researchers and analysts coming from different fields. In most of these fields more and more sources of data are becoming available introducing a variety of <b>hypothetical</b> explanatory <b>variables</b> for these models to be considered. Selection of an optimal combination of these variables is thus becoming crucial. In a Bayesian setting, the posterior distribution of the models can be viewed as a relevant measure for the model evidence, based on the observed data. The number of models to select from is exponential in the number of candidate variables, moreover the search space in this context is often extremely non-concave and has numerous local extrema or statistically speaking modes. Hence efficient search algorithms have to be adopted for evaluating the posterior distribution within a reasonable amount of time. In this paper a novel MCMC algorithm for search through the model space via efficient mode jumping for GLMMs is introduced. The algorithm is based on that marginal likelihoods can be efficiently calculated within each model. We recommend that either exact expressions or precise approximations of marginal likelihoods are applied. We further apply the suggested algorithm to some simulated data, the famous U. S. crime data, protein activity data and epigenetic data and compare its performance to some of the existing approaches like MCMC algorithms with local moves between models and the Bayesian adaptive sampling algorithm by Clyde et al. (2011). Comment: 30 pages, 4 figures, 8 table...|$|R
40|$|In this study, the {{well-being}} evaluation method, a technique for measuring individual utility, {{was used to}} study how people in the wildland urban interface of Colorado (USA) felt about their lives before and after two <b>hypothetical</b> wildfire scenarios. <b>Variables</b> such as age, family size, fire frequency, and property value were found to affect initial well-being levels. However, if a wildfire were to occur, many variables that initially affected well-being were no longer significant. It was found that after wildfire, the frequency of wildfire occurrence became the most important influence on well-being. These results have several implications for wildfire managers. First, {{the well-being}} of Colorado wildland urban interface residents would be enhanced by {{a reduction in the}} frequency of high-intensity wildfires. Secondly, an extremely high percentage of respondents were in favor of prescribed burning. Therefore, the reduction of high-intensity fires could not only be accomplished by conducting a rotation of prescribed fires, but that prescribed burning would be accepted by the public living in the wildland urban interface. well-being evaluation method; Colorado; happiness; wildland urban interface; wildfire intensity...|$|R
40|$|Background To {{investigate}} {{the role of}} geography (place of residence) as a moderator {{in the relationship between}} dental caries disease and treatment experience and dental fear in 16 -year-olds living in Malaysia. Methods A multi-stage-stratified sampling method was employed. Five hundred and three, 16 -year-olds from 6 government secondary schools participated in this study. The questionnaire examined participants’ demographic profile and assessed their dental fear using the Dental Fear Survey (DFS). The clinical examination consisted of the DMFT as the outcome measure of dental caries disease and treatment experience by a single examiner (ICC[*]=[*] 0. 98). Structural equation modelling inspected the relationship between dental fear and dental caries disease and treatment experience. Results The mean DMFT was 2. 76 (SD 3. 25). The DT, MT and FT components were 0. 64 (SD 1. 25), 0. 14 (SD 0. 56) and 1. 98 (SD 2. 43) respectively. Rural compared with urban adolescents had significantly greater mean numbers of decayed and missing teeth. The mean DFS score was 40. 8 (SD 12. 4). Rural compared with urban adolescents had significantly higher mean scores for physical symptoms of dental fear. The correlation between dental fear (DFS) and dental caries disease and treatment experience (DMFT) was 0. 29, p[*]<[*] 0. 0001. The structural equation model fitted the raw data well (χ 2 [*]=[*] 9. 20, df[*]=[*] 8, p[*]=[*] 0. 34). All components of DMFT were closely associated in equal strength to the unidimensional <b>hypothetical</b> latent <b>variable</b> of dental caries disease and treatment experience. The strength of the relationship between dental fear and dental caries disease and treatment experience varied in accordance with place of residence. Conclusion In conclusion a relationship between dental fear and dental caries disease and treatment experience was shown to exist in 16 -year-old adolescents living in Malaysia. This study showed that the rural–urban dichotomy acted as a moderator upon this relationship...|$|R
40|$|The Marmara Sea Gateway {{connects the}} hypersaline (< 36 [per thousand]) Aegean Sea and the low-salinity (∼ 17 - 22 [per thousand]) Black Sea through the Straits of Bosphorus and Dardanelles and the {{landlocked}} Marmara Sea. This gateway forms a natural laboratory {{in which to}} {{study the effects of}} climate change, sea-level fluctuations and water-mass exchange between small basins. [...] Surface sediment samples were collected from 137 stations across six transects in the Black Sea, the Marmara Sea and the Aegean Sea. At each station grab samples and CTD (conductivity, temperature, depth) measurements were also collected. [...] In the Marmara Sea, CTD data revealed the presence of a low salinity surface water mass representing Black Sea outflow and a high salinity deeper water mass, separated by a sharp mixing zone. Across the Southwestern Black Sea shelf the CTD data showed the presence of 3 water masses: (1) a low salinity surface water mass extended down to 10 m; (2) a low salinity, but colder water mass occupies water depth below 40 m; (3) and a higher salinity Mediterranean water occurs between these two water masses. In the Aegean Sea CTD data revealed a relatively low salinity Black Sea outflow water mass at the surface, and high salinity water mass at the bottom separated by a mixing zone. [...] The mollusc shells from grab samples were identified using a number of taxonomic keys and analyzed qualitatively and quantitatively in order to investigate the relationship between the community structure and the environment. The mollusc absence and presence data in quadrats was used to delineate seven different mollusc assemblages, with each assemblage representing a distinct set of environmental conditions. Principal component analysis was used to constrain mollusc faunal assemblages and their relationship to environmental <b>variables.</b> Seven <b>hypothetical</b> faunal assemblages and three <b>hypothetical</b> environmental <b>variables</b> were extracted, explaining 73. 9...|$|R
40|$|This work {{presents}} a new methodology to control seawater intrusion in coastal aquifers using artificial surface recharge. The proposed method {{is based on}} a combination of abstraction of saline water near shoreline and recharge of aquifer using surface ponds. The efficiency of this method is investigated in terms of quality and cost by comparing its results with another different scenario of groundwater management using continuous abstraction only in the <b>hypothetical</b> aquifer. The <b>variable</b> density flow and solute transport model, SUTRA, is used to simulate this aquifer system in both 2 D and 3 D under steady state conditions. A Simple Genetic Algorithm as optimization tool is integrated with simulation model to search for optimal solution of each scenarios of seawater intrusion control. The main objectives of these models are to minimize the total cost of management process and the total salt concentration in the aquifer. The total cost is defined as the sum of capital and operational costs of the abstraction and recharge wells. The results show that the proposed system performs significantly better than using abstraction alone as it gives the least cost and least salinity in the aquifer...|$|R
40|$|A central goal of {{generative}} linguistics is {{to determine}} what constitutes a possible grammar of a natural language. This thesis works toward that goal in positing a constraint on the possible semantic types of variables in natural language. Specifically, I argue here that the logical forms (LFs) of natural languages do not contain higher-type variables, i. e., variables of a type {{higher than that of}} an individual, type e (see Chierchia (1984) and Baker (2003) for similar proposals). I refer to this constraint as the No Higher-Type Variables constraint (NHTV). ^ Assuming that the domain of individuals, D, includes at least objects, kinds, events, event-kinds, degrees, situations, worlds, times, and locations, all of which have been independently argued to be necessary members of D, what NHTV predicts not to occur are object language expressions that vary over, e. g., generalized quantifiers, relations, or properties. While NHTV thus predicts a very restricted inventory of variable denoting expressions, I argue that it accounts for a surprisingly wide range of data in characterizing which variable-denoting expressions do and do not occur. ^ I motivate NHTV based primarily on data from English, and to a lesser extent, German and Polish. I focus empirically on two types of expressions that are commonly analyzed as involving variables: (i) (overt) pro-forms, and (ii) A 2 ̆ 7 -movement gaps. In particular, I look closely at pro-forms and gaps that have the syntactic distribution of items that are commonly taken to be of a higher-type, namely, APs, AdvPs, VPs, and NPs. While all of these expressions are commonly taken to denote properties of individuals, I argue that pro-forms and gaps that have the distribution of these categories should not be analyzed as property variables, but instead either (i) vary over individuals, or (ii) do not involve variables at all. ^ The bulk of the thesis is devoted to backing up NHTV by (i) showing that <b>hypothetical</b> higher-type <b>variables</b> are systematically missing, and (ii) looking closely at potential counterexamples on a case-by-case basis. ...|$|R
