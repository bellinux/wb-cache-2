0|19|Public
40|$|International audienceAn {{important}} observation {{which can}} be made from most of the studies on speech/gesture coordination is that the results are highly variable from one study to another. The nature of the communicative relationship between gesture and speech may be one factor explaining those discrepancies. The aim {{of this study is to}} further explore the possible modulation of speech / <b>gesture</b> <b>coordination</b> by the communicative relationship between gesture and speech. In order to do so, we designed a paradigm very close to that used in a previous experiment in which we varied the communicative relationship between the pointing gesture and speech within the designa- tion framework. In the aforementionned experiment, the pointing gesture and prosodic focus designated exactly the same object while in the present study, prosodic focus designated only one aspect of the information carried by both the entire spoken utterance and the visual target pointed at...|$|R
40|$|Two {{decades of}} {{attempts}} to model the emergence of language as a collective cognitive activity have demonstrated a number of principles {{that might have been}} part of the historical process that led to language. Several models have demonstrated the emergence of structure in a symbolic medium, but none has demonstrated the emergence of the capacity for symbolic representation. The current shift in cognitive science toward theoretical frameworks based on embodiment is already furnishing computational models with additional mechanisms relevant to the emergence of symbolic language. An analysis of embodied interaction among captive, but not human-enculturated, bonobo chimpan-zees reveals a number of additional features of embodiment that are relevant to the emergence of symbolic language, but that have not yet been explored in computational simulation models; for example, complementarity of action in addition to imitation, iconic in addition to indexical <b>gesture,</b> <b>coordination</b> among multiple sensory and perceptual modalities, and the orchestration of intra- and inter-individual motor coordination. The bonobos provide an evolutionarily plausible intermediate stage in the development of symbolic expression that can inform efforts to model the emergence of symbolic language...|$|R
40|$|International audienceIncreasing {{networking}} performances {{as well as}} {{the emergence}} of Mixed Reality (MR) technologies make possible providing advanced interfaces to improve remote collaboration. In this paper, we present our novel interaction paradigm called Vishnu that aims to ease collaborative remote guiding. We focus on collaborative remote maintenance as an illustrative use case. It relies on an expert immersed in Virtual Reality (VR) in the remote workspace of a local agent helped through an Augmented Reality (AR) interface. The main idea of the Vishnu paradigm is to provide the local agent with two additional virtual arms controlled by the remote expert who can use them as interactive guidance tools. Many challenges come with this: collocation, inverse kinematics (IK), the perception of the remote collaborator and <b>gestures</b> <b>coordination.</b> Vishnu aims to enhance the maintenance procedure thanks to a remote expert who can show to the local agent the exact gestures and actions to perform. Our pilot user study shows that it may decrease the cognitive load compared to a usual approach based on the mapping of 2 D and de-localized informations, and it could be used by agents in order to perform specific procedures without needing to have an available local expert...|$|R
40|$|International audienceCommunication is multimodal. In particular, {{speech is}} often {{accompanied}} by manual <b>gestures.</b> Moreover, their <b>coordination</b> has often been related to prosody. The {{aim of this study}} was to further explore the coordination between prosodic focus and different manual gestures (pointing,beat and control gestures) on ten speakers using motion capture. As compared to previous studies, results show that the <b>coordination</b> between <b>gestures</b> and speech is modulated by the relationship between themanual gesture and speech, especially for the pointing gesture. Moreover, this study shows that different strategies might be adopted so as to adapt to the changes in this relationshi...|$|R
40|$|Com. OraleInternational audienceThe aim of {{this study}} is to {{characterize}} the interaction between speech and manual gestures in the production of prosodic focus. Prosodic focus consists in putting forward a word or a group of words within an utterance or discourse. It can actually be considered as a form of pointing towards the part of the utterance bearing the important information. In that sense, it is naturally close to manual pointing. We would like to address several key questions: (1) Is coordination between speech and hand related to the gesture produced (communicative vs. non communicative) ? (2) Is coordination between speech and hand dependent on the type of communicative gesture (deictic vs. non deictic) ? (3) For the pointing <b>gesture,</b> is the <b>coordination</b> dependent on the correspondence between what speech focuses and what the hand shows...|$|R
40|$|Most {{people in}} the world speak more than one language, making {{bilingualism}} the norm rather than the exception. Furthermore, speakers generally also move their hands – they <b>gesture</b> – in <b>coordination</b> with speech and language in nontrivial ways. Bilingualism and multimodality should thus be on research agendas focused on the nature of linguistic systems and language use in context, yet they are often overlooked. Conversely, research and theorizing on bilingualism and multimodality is often based on Western-European, standardized languages, and little is known about other linguistic contexts. This paper makes the point that language documentation data has the potential to inform theoretical and empirical studies of linguistics, bilingualism and multimodality in entirely new ways, and, conversely, that documentation work would benefit from taking the bilingual and multimodal nature of its data into account. National Foreign Language Resource Cente...|$|R
40|$|International audienceIntroduction - Manual {{gestures}} {{are naturally}} produced in spoken communication. Most studies qualitatively showed that brachio-manual gestures and speech are tightly bound (e. g. gestures {{are linked to}} prosody). Some studies have tried to specify the coordination of speech and manual gestures using motion capture: Levelt et al., de Ruiter, Rochet-Capellan et al. all studied speech/pointing <b>gesture</b> <b>coordination</b> using either noun phrases or non-words. They found effects of gesture on speech, effects of speech stress location on gesture and relations between gesture hold and timings of syllables. Studies on gesture/speech coordination usually focused on only one type of gesture (namely deictics or iconics). The {{aim of this study}} is to analyze the coordination between speech and manual gestures -of different types- in the production of prosodic contrastive focus. Focus consists in emphasizing (in a sense, pointing at) a phrase within an utterance. It is thus related to manual pointing. This study addresses two main issues: The influence of focus position and the role of gesture type on gesture/speech coordination. Methodology - We compared two prosodic focus conditions: subject vs. object focus; and four gesture conditions: no gesture (speech only) vs. index-finger pointing (deictic communicative) vs. beat gesture (non-deictic communicative) vs. control gesture (button press; non-deictic non-communicative). A correction task was used to elicit the production of focus. The participants sat in a chair facing a screen showing visual targets. We recorded their vocal productions and the motion of their mouth and right hand (tracked using a motion capture device). Points of interest were annotated for gestures (beginning, apex, return, end). Articulatory lip targets (protrusion and aperture peaks) and acoustic cues (fundamental frequency and intensity peaks, syllables) were also annotated. Ten adults participated in the experiment. Results - Focus condition has a significant effect on the timing of gesture production. The timing of the acoustic cues of focus are not influenced by the production of a gesture nor by gesture type, except for intensity. The positions of the gestures' points of interest were compared to those of the articulatory targets and acoustic cues. The pointing gesture is more strictly coordinated with focus than the other gestures. In particular, the pointing gesture apex is tightly aligned with articulatory lip targets corresponding to vocalic gestures. Control and beat gestures appear to be either more difficult to elicit or less precise. Speech/gesture coordination is influenced by the type of gesture. Moreover the data show that speech onset is influenced by the production of a gesture but that the utterance's internal organization is not influenced. Gesture onset is always influenced by the focus condition. However, the gesture's internal organization is influenced by the focus condition only for pointing. Conclusion - This study allows for a more precise characterization of speech/gesture coordination in spoken communication (entire sentences and dialog context). It also shows that speech/gesture coordination is dependent on the functional relationship between gesture and speech. Promising analyses from a second study where speech targets are "semantically included" in gestural targets even more emphasize this functional relationship...|$|R
40|$|When people talk, {{they often}} move {{their hands and}} their arms. These {{movements}} seem to {{have some type of}} relationship with concurrent speech. Some of the results of a research project on gestures are presented here. These studies aimed at describing different gestures occurring during speech, and at understanding their roles in different contexts of social interaction. Research topics therefore are: categorization of gesture communication, reliability of tools for coding hand gestures, and functions of gesture in speech and in interaction. In particular, these studies had two aims in mind: a) the development of a reliable coding system and a relative multi-medial support for the categorization of hand gesture (descriptive-structural aim); b) individuation of gesture functions in conversation through statistical analysis of significant speech-gesture co-occurrences (descriptive-functional aim). The main theoretical premises relating to each aim are here synthetically explained. Key-words: gesture structure, <b>gesture</b> function, gesture-speech <b>coordination...</b>|$|R
40|$|AbstractAutism Spectrum Disorder (ASD) is {{characterized}} by difficulties in communication and social interaction. Abnormalities {{in the use of}} gestures or flow of conversation are frequently amongst the clinical observations that contribute to a diagnosis of the disorder but the mechanisms underlying these communication difficulties remain unclear. While studies in children show a reduced use of co-speech gestures overall (e. g., Sowden et al., 2013), de Marchena and Eigsti (2010) found that in adults some gestures are used with similar frequency in ASD and typically developing (TD) populations, but that gestures produced by ASD individuals are less strongly time-locked to speech than in TD individuals. This may indicate that abnormal temporal processes contribute to impaired social skills in ASD (Allman, 2011). The objectives of this study were three-folded: 1) characterise the temporal dynamics of speech and <b>gesture</b> <b>coordination</b> in ASD in naturalistic speech; 2) quantify the use of gestures in ASD; 3) test the hypothesis that atypical temporal coordination between speech and gestures results in a lesser quality in communication. The context of a previously published study of memory in ASD (Maras et al., 2013) provided the opportunity to examine video recordings of 16 ASD and 17 TD adults attempting to recall details of a standardised event they had participated in (a first aid scenario). The current analysis was designed to quantify the participants’ communicative behaviours in three ways: 1) Segmenting the videos to identify each gesture and document its precise timing aimed to establish whether gestures are used differently in ASD and TD participants; 2) Extracting the quantity of movement as well as the pitch and volume of speech over time allowed us to characterise whether gestures were time-locked to speech similarly in ASD and TD individuals; 3) Collecting subjective ratings on the quality of communication displayed in the audio-only or audio-visual recordings helped to establish to what extent the use of gestures improved the quality of communication in ASD and TD participants. Overall, our results indicated no main group difference in the use and coordination of speech and gesture: both groups produced the same quantity of movement over time [t(33) =- 0. 165, p>. 8], and gestures were produced within the same time window and with a similar distribution by ASD and TD individuals (η 2 p=. 042). Similarly, no group differences were found in the subjective ratings on the quality of communication: in both groups the use of gestures improved comprehension and engagement from the listener. Interestingly, in line with previous report, we found that ASD individuals spoke on average louder than TD individuals [t(33) =- 3. 520, p<. 005]. Notably, all measures showed a large inter-individual variability in both groups. The current data do not suggest that ASD individuals experience more difficulties than TD participants in time processes relevant to communicating personally experienced events. However, large inter-individual differences could contribute to communication difficulties in some participants. It will be important for future studies to examine the timing of communicative behaviours during reciprocal interactions, that place demands not only on coordinating speech with gesture but to coordinate one's own behaviour with that of others...|$|R
30|$|The {{essential}} {{aim of the}} pollicisation {{procedure is}} to restore an active pinch [16, 17, 18]. In our video-assisted scoring system, we selected seven activities representative of daily life activities to assess the results of pollicisation. These activities {{require the use of}} the thumb pinch. For example, the pulp pinch is used while ‘writing,’ the lateral pinch while ‘eating’ and the grip while ‘drinking.’ The realisation of complex <b>gestures</b> represents bimanual <b>coordination.</b> All these conditions are essential for a correct social integration [17, 18, 19]. These tasks were performed both on request and in real-life conditions using a hidden camera, to assess whether patients used their neo thumb in the same way in both conditions. We found that children tended to use their neo thumb correctly on request but tended to use inappropriately a lateral pinch on hidden camera condition. This explains why the results of pollicisation were better on request than on hidden camera. This also suggests that improvements in the surgical technique might improve the inappropriate use of the pinch [20, 21]. In this respect, pollicisation with tendon transfers using an abductor digiti minimi or a flexor digiti superficial of the fourth finger have been described to improve the neo thumb opposition [22, 23].|$|R
40|$|Synchronous Distributed Collaborative Work (SDCW) {{occurs when}} group members work {{together}} {{at the same time}} from different places together to achieve a common goal. Effective SDCW requires good communication, continuous coordination and shared information among group members. SDCW is possible because of groupware, a class of computer software systems that supports group work. Shared-workspace groupware systems are systems that provide a common workspace that aims to replicate aspects of a physical workspace that is shared among group members in a co-located environment. Shared-workspace groupware systems have failed to provide the same degree of coordination and awareness among distributed group members that exists in co-located groups owing to unintuitive interaction techniques that these systems have incorporated. Natural User Interfaces (NUIs) focus on reusing natural human abilities such as touch, speech, gestures and proximity awareness to allow intuitive human-computer interaction. These interaction techniques could provide solutions to the existing issues of groupware systems by breaking down the barrier between people and technology created by the interaction techniques currently utilised. The aim of this research was to investigate how NUI interaction techniques could be used to effectively support SDCW. An architecture for such a shared-workspace groupware system was proposed and a prototype, called GroupAware, was designed and developed based on this architecture. GroupAware allows multiple users from distributed locations to simultaneously view and annotate text documents, and create graphic designs in a shared workspace. Documents are represented as visual objects that can be manipulated through touch <b>gestures.</b> Group <b>coordination</b> and awareness is maintained through document updates via immediate workspace synchronization, user action tracking via user labels and user availability identification via basic proxemic interaction. Members can effectively communicate via audio and video conferencing. A user study was conducted to evaluate GroupAware and determine whether NUI interaction techniques effectively supported SDCW. Ten groups of three members each participated in the study. High levels of performance, user satisfaction and collaboration demonstrated that GroupAware was an effective groupware system that was easy to learn and use, and effectively supported group work in terms of communication, coordination and information sharing. Participants gave highly positive comments about the system that further supported the results. The successful implementation of GroupAware and the positive results obtained from the user evaluation provides evidence that NUI interaction techniques can effectively support SDCW...|$|R
2500|$|KITECH {{researched}} and developed EveR-1, an android interpersonal communications model capable of emulating human emotional expression via facial [...] "musculature" [...] {{and capable of}} rudimentary conversation, having a vocabulary of around 400 words. She is [...] tall and weighs , matching the average figure of a Korean woman in her twenties. EveR-1's name derives from the Biblical Eve, plus the letter r for robot. EveR-1's advanced computing processing power enables speech recognition and vocal synthesis, {{at the same time}} processing lip synchronization and visual recognition by 90-degree micro-CCD cameras with face recognition technology. An independent microchip inside her artificial brain handles <b>gesture</b> expression, body <b>coordination,</b> and emotion expression. Her whole body is made of highly advanced synthetic jelly silicon and with 60 artificial joints in her face, neck, and lower body; she is able to demonstrate realistic facial expressions and sing while simultaneously dancing. In South Korea, the Ministry of Information and Communication [...] has an ambitious plan to put a robot in every household by 2020. Several robot cities have been planned for the country: the first will be built in 2016 at a cost of 500 billion won (440 million USD), of which 50 billion is direct government investment. The new robot city will feature research and development centers for manufacturers and part suppliers, as well as exhibition halls and a stadium for robot competitions. The country's new Robotics Ethics Charter will establish ground rules and laws for human interaction with robots in the future, setting standards for robotics users and manufacturers, as well as guidelines on ethical standards to be programmed into robots to prevent human abuse of robots and vice versa.|$|R
40|$|International audienceSpeech is multisensory {{since it}} is {{perceived}} through several senses. Audition {{is the most important}} one as speech is mostly heard. The role of vision has long been acknowledged since many articulatory gestures can be seen on the talker's face. Sometimes speech can even be felt by touching the face. The best-known multisensory illusion is the McGurk effect, where incongruent visual articulation changes the auditory percept. The interest in the McGurk effect arises from a major general question in multisensory research: How is information from different senses combined? Despite decades of research, a conclusive explanation for the illusion remains elusive. This is a good demostration of the challenges in the study of multisensory integration. Speech is special in many ways. It is the main means of human communication, and a manifestation of a unique language system. It is a signal with which all humans have a lot of experience. We are exposed to it from birth, and learn it through development in face-to-face contact with others. It is a signal that we can both perceive and produce. The role of the motor system in speech perception has been debated for a long time. Despite very active current research, it is still unclear to which extent, and in which role, the motor system is involved in speech perception. Recent evidence shows that brain areas involved in speech production are activated during listening to speech and watching a talker's articulatory <b>gestures.</b> Speaking involves <b>coordination</b> of articulatory movements and monitoring their auditory and somatosensory consequences. How do auditory, visual, somatosensory, and motor brain areas interact during speech perception? How do these sensorimotor interactions contribute to speech perception? It is surprising that despite a vast amount of research, the secrets of speech perception have not yet been solved. The multisensory and sensorimotor approaches provide new opportunities in solving them. Contributions to the research topic are encouraged for a wide spectrum of research on speech perception in multisensory and sensorimotor contexts, including novel experimental findings ranging from psychophysics to brain imaging, theories and models, reviews and opinions...|$|R
40|$|This thesis {{concerns}} {{the nature of}} the gestures performed by five Swedish children. The children are followed from 18 to 30 months of age: an age range which is characterized by a rapid succession of developmental changes in children's abilities to communicate by means of both spoken language and gesture. There are few studies of gesture in children of these ages, making it essential to ask a number of basic questions: What sort of gestural actions do the children perform? How does the use of gesture change over time, from 18 to 30 months of age? How are the <b>gestures</b> performed in <b>coordination</b> with speech? The answers provided to these questions are both quantitative and qualitative in kind. Several transitions in the use of gesture are identified, relating to developmental changes in the organization of speech — highlighting the symbiotic relationship between gesture and speech in the communicative ecology. Considerable attention is paid to the even more basic question of what sort of actions qualify for the label "gesture". Instead of treating gestural qualities as a matter of a binary distinction between actions counting as gesture and those that do not, a multi-level approach is advocated. This approach allows for descriptions of gestures in terms of several different levels of complexity. Furthermore, a distinction is made between levels of communicative explicitness on the one hand, and levels of semiotic complexity on the other. This distinction allows for the recognition that some gestural actions are semiotically complex, without being explicitly communicative, and vice versa: that some gestural actions are explicitly communicative, without being semiotically complex. The latter is particularly consequential for this thesis, since a large number of communicative gestural actions reside in the borderland between practical action and expressive gesture. Hence, the gestures analyzed include not only the prototypical "empty-handed" gestures, but also gestures that involve handling of physical objects. Overall, the role of conventionality in children's gestures is underscored. The approach is (a) cognitive {{in the sense that it}} pays attention to the knowledge and bodily skills involved in the performance of the gestures, (b) social and interactive in the sense that it views gestures as visible and accountable parts of mutually organized social activities, and (c) semiotic in the sense that the analysis tries to explicate how signification is brought about, in contrast to treating the meanings of gestures as transparently given, the way participants themselves often do when engaged in social interaction...|$|R
40|$|The way {{in which}} infants attempt to {{instigate}} communication based on shared events has been the object of less intense investigation than the study of infants’ abilities, for example, for gaze following, possibly because of the specific challenges presented by production studies with pre-verbal participants. However, there are uniquely interesting aspects {{in the study of}} infant pointing. If gaze following may be considered the displacement of one’s attention onto the target selected by someone else for either communication or action purposes, pointing is not simply its production twin. When a baby follows the gaze of an adult, usually contingent with a head-turn, we assume that this indicates the baby’s acknowledgment that there must be something worthwhile to look at ‘over there’. From a behavioural point of view, all is required of the baby is to mirror the adult’s visual action, in the first instance. When a baby points, s/he takes the initiative to select a target and uses a gesture with symbolic status and cultural variants in order to instigate someone else’s displacement of attention onto the selected target. Not only the action is more complex than gaze following, but it may fail to achieve its goal if care is not taken to produce the crucial <b>gesture</b> in good <b>coordination</b> with the ongoing interaction with the addressee – namely, with the addressee’s own attention. In this chapter, I will analyze the kind of knowledge that infants develop about sharing attention with other social agents using evidence from my own experiments on declarative pointing by infants and toddlers. It is argued that attention, in comparison with other mental states, presents at least some, however minimal, behavioral correlates (hence higher imageability) : for example, body and head orientation, eyes open, temporal synchrony between these behaviours and the topical events, postural adjustments etc. Also, attention can be shared by acting it out: infant pointing may be considered as embodied imagery allowing the alignment of cognitive states between social partners. In this respect, communicative experiences arising from pointing would provide the infant with an insight into the mental domain, supported by the comparison and integration of one’s own and the addressee’s perceptual experiences. The special relationship that has for long been suggested between pointing and language may stem from the extension of communication beyond the here-&-now, for instance pointing to something that the addressee has not witnessed. This could instigate a new type of multi-modal shared communication acts (e. g., see McNeill et al., 2008; Quek, Rose & McNeill, 2005), thus facilitating the development of symbolic communication...|$|R
40|$|International audienceA {{large amount}} of work has {{established}} various kinds of articulatory strengthening of segments and of the modifications of the intra-/inter-segmental articulation dynamics depending {{on the level of}} prosodic boundaries. Studies on various languages have showed that a prosodic hierarchy accounts for variations of articulatory <b>gestures</b> and their <b>coordination</b> towards segmental strengthening and of reduction of coarticulation in initial, final or cross-boundary positions of constituents as function as up to 5 hierarchical levels (among others, [1; 2; 6]). In French, studies on this subject mainly focused on accent types [7] or initial position in prosodic constituents of different levels [3]. The present study on French [8] focuses on the effects of a 4 -level prosodic hierarchy on intra- and inter-segmental articulatory dynamics of linguopalatal gestures in initial, final and cross-boundary positions of prosodic units. Prosodic-dependant articulatory variations were observed in aC#Ca sequences embedded in a sentence, where CC stands for /kl/, /lk/, /kt/ or /tk/ - # for prosodic boundary -. Accent always fell on the first vowel /a/, which is in the nucleus of the final syllable of the prosodic constituent. 7 to 8 sentences were read without pause 12 or 15 times by 3 French speakers. Prosodic boundaries varied by manipulating the syntactic and thematic structures of utterances. The prosodic hierarchy was composed of 4 levels, from the lower to the higher: syllable/word level accentual level versus syllable/word level > terminal intonational level versus non terminal intonational level. The interpretation of the results concerns the architectures of the phonological representation based on a hierarchical prosodic constituency of speech structure. The discussion focuses mainly on the nature and, the number of, as well as the relationships between prosodic hierarchical levels, determined by specific intonational models of French prosody [4; 5]. References:[1] Byrd, D., Kaun, A., Narayanan, S. & E. Saltzman (2000). Phrasal signature in articulation. In M. B. Broe & J. B. Pierrehumbert (eds), Papers in Laboratory Phonology V, 70 - 87. Cambridge: CUP. [2] Cho, T. (2001). Effects of Prosody on Articulation in English. UCLA: PhD dissertation. [3] Fougeron, C. (2001). Articulatory properties of initial segments in several constituents in French. Journal of Phonetics, 29, 109 - 135. [4] Hirst, D. & A. Di Cristo (1984). French intonation: a parametric approach. Die Neueren Sprache, 5, 554 - 569. [5] Jun, A-S. & C. Fougeron (2000). A phonological model of French intonation. In Intonation: Models and Technology, A. Botinis (ed.). Dordrecht: Kluwer, 209 - 242. [6] Keating, P., Cho, T., Fougeron, C. & C. H. Hsu (2003). Domain-initial articulatory strengthening in four languages. In J. Local, R. Odgen & R. Temple (eds), Papers in Laboratory Phonology VI. Cambridge: CUP. [7] Loevenbruck, H. (2000). An investigation of articulatory correlates of the accentual phrase in French. Proceedings of the 14 th ICPhS, 667 - 670. San Francisco. [8] Meynadier, Y. (2003). Interaction entre prosodie et (co) articulation linguopalatale en français. Université de Provence: PhD dissertation...|$|R
40|$|In general, the {{dissertation}} {{deals with}} the problem of organizing work. In particular, the issue addressed through the research focused in the practice of directing organizational technical activities. Under this concern, I conducted a field study in a steelworks company located in Santiago, Chile. As part of this endeavour, I approached ethnographically the study of the practice of work in several teams. Each team was made up by engineers and workers, all engaged in carrying out a specialized technical activity. The study was developed during six months, under the framework of a continuous improvement program, in a regular daily daytime shift. Within this context, I focused on interactions (engineers - worker; worker - worker) happening on key workplaces (three control rooms). I also focused in the way engineers conceived plans, from their offices, regarding the work being carried out by their teams. ! In both circumstances, I also studied the way engineers and workers used technologies while performing their tasks. Analysis was based on my field notes complemented with two kinds of video records: interactions between engineers and workers while working, and engineers explaining the way they conceive the work of their team (approximately 30 hours of recordings). Based on activity theoretical and phenomenological approaches, I focused on the interactive construction of meaning through language (speech, prosody, and <b>coordination),</b> <b>gesture</b> (pointing, body position) and the use of space and tools. Analysis conducted suggests the practice of engineers as strongly related with the possibility of continuously establishing a collective activity that results intelligible from the perspective of workers participating from particular workplaces, inhabited by persons and technologies. In order to establish the collective activity, engineers develop a general u! nderstanding of the scene being played from those particular w! orkplace s. A distinctive character of the engineers' practice seems to lie, then, in the possibility of rendering intelligible both: the implications of the planned activity for the work of every participant, and the implications of the action being executed in particular workplace for the layout of the planned activity. In this sense, engineers seem to accomplish the direction of work through the development of a significant understanding of the action pursued at the workplace level and the planned activity level that links action in several workplaces. Within this context, the practice of engineers seems to be closely related to both: presenting the implications of the planned activity at the time and place where work is executed, and representing the implications of the executed work at the time and place where the activity is layout through planning. Finally, research advances in the categorization of several presentational and representational aspects of engineering practi! ce embedded into Information and Communication Technologies used as managerial and operational tools. En gran parte del mundo, las organizaciones han llegado a convertirse en un aspecto fundamental de la vida en sociedad. Intentando dar respuestas a las posibilidades y requerimientos de su entorno relevante, las organizaciones especializan su accionar en torno a múltiples actividades que representan una manera particular de llevar adelante el trabajo. Así, las actividades organizacionales consideran una peculiar forma de establecer tareas y de disponer de recursos humanos y tecnológicos. En este contexto, el problema de la dirección de las actividades organizacionales adquiere un papel de gran relevancia para el desarrollo de las organizaciones y, finalmente, para su viabilidad. La posibilidad de dirigir resulta determinante para asegurar la calidad del trabajo que las organizaciones despliegan en su entorno. En este sentido, la dirección de las actividades organizacionales contribuye a caracterizar la pertinencia que las tareas establecidas y los recursos empleados guardan para con los requerimientos de ese entorno. Con la finalidad de ganar una mejor comprensión acerca de los alcances de la dirección en las organizaciones modernas, esta investigación centró su interés en la práctica directiva de los ingenieros en una compañía siderúrgica chilena. Los ingenieros en esta compañía dirigían actividades donde concurrían equipos de trabajadores, tecnologías y lugares de trabajo especializados. La investigación puso el énfasis en una aproximación etnográfica a la práctica cotidiana de ingenieros y operarios en medio de un programa rutinario de mejoras a la calidad. El trabajo de terreno se extendió por seis meses en tiempo completo. La realización de registros, que permitieran el posterior análisis del trabajo de ingenieros y operarios en un medio constituido por instalaciones particulares, contempló la utilización del formato audio video. La investigación sugiere que la acción directiva de los ingenieros está fuertemente relacionada con la posibilidad de ir concibiendo una actividad organizacional que resulte inteligible para cada trabajador desde las particularidades de un lugar de trabajo, desde donde se acometen las tareas encargadas y donde concurren trabajadores y tecnologías. Esta manera de concebir la actividad contempla una forma precisa de entender el lugar de trabajo así como la propia participación en ese lugar. Al respecto, la acción de los ingenieros adquiere un carácter distintivo a la hora de presentar en el lugar de trabajo los alcances de la actividad donde cada trabajador participa. Igualmente, la acción de los ingenieros parece adquirir un carácter distintivo a la hora de representar en cada lugar de trabajo las explicaciones necesarias para entender los alcances de la participación de cada cual en la actividad colectiva...|$|R

