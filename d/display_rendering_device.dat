0|595|Public
40|$|The re-capturing {{of video}} content poses {{significant}} challenges to algorithms {{in the fields}} of video forensics, watermarking, and near-duplicate detection. Using a camera to record a video from a display introduces a variety of artifacts, such as geometric distortions, luminance transformations, and temporal aliasing. A deep understanding of the causes and effects of such phenomena is required for their simulation, and for making the affected algorithms more robust. In this paper, we provide a detailed model of the temporal effects in re-captured video. Such effects typically result in the re-captured frames being a blend of the original video's source frames, where the specific blend ratios are difficult to predict. Our proposed parametric model captures the temporal artifacts introduced by interactions between the video <b>renderer,</b> <b>display</b> <b>device,</b> and camera. The validity of our model is demonstrated through experiments with real re-captured videos containing specially marked frames...|$|R
40|$|DE 102005008333 A 1 UPAB: 20061208 NOVELTY - The device has {{a control}} monitor (2) to monitor an extent of {{utilization}} situation {{of a wave}} field synthesis system with a wave field synthesis <b>rendering</b> <b>device.</b> An audio object manipulation device (3) varies a start point/end point of an audio object within a time period or an actual position of a virtual source within a local span, by the <b>rendering</b> <b>device,</b> depending on {{the situation of the}} wave field synthesis system. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following: (1) a method of controlling a wave field synthesis <b>rendering</b> <b>device</b> arranged in a wave field synthesis system (2) a computer program with a program code for executing a method of controlling a wave field synthesis <b>rendering</b> <b>device</b> arranged in a wave field synthesis system. USE - Used in entertainment electronics such as multimedia systems, for controlling a wave field synthesis <b>rendering</b> <b>device</b> arranged in a wave field synthesis system, where the <b>rendering</b> <b>device</b> is designed to produce synthesis signals from audio object comprising an audio file for a number of loudspeakers within a loudspeaker array. ADVANTAGE - The audio object manipulation device varies a start point/end point of an audio object within a time period or an actual position of a virtual source within a local span, by the <b>rendering</b> <b>device,</b> depending on the extent of utilization situation of the wave field synthesis system, thereby avoiding capacity constraint on the transmission lines or in the <b>rendering</b> <b>device.</b> The control device hence reduces the quality break-down of the wave field synthesis system and enables the wave field synthesis system to achieve a high flexibility of application at the same time. The control device controls the wave field synthesis <b>rendering</b> <b>device</b> arranged in a wave field synthesis system in a flexible manner at low cost...|$|R
40|$|DE 102005008366 A 1 UPAB: 20061018 NOVELTY - A {{device for}} driving a wave-field {{synthesis}} <b>rendering</b> <b>device</b> (3) with audio objects from which are generated synthesis signals {{which can be}} relayed {{through a number of}} loudspeakers (4). A device (8) supplies a scene description which defines a time sequence of audio-objects in an audio scene, and a device (0) processes the audio objects in order to generate the data stream for supplying to the wave-field synthesis <b>rendering</b> <b>device</b> (3). DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following. (1) A method for driving a wave-field synthesis <b>rendering</b> <b>device</b> with audio objects and (2) A computer program with program code. USE - For operating a wave-field synthesis <b>rendering</b> <b>device</b> with data to be processed ADVANTAGE - Facilitates provision of a flexible concept for operating a wave-field synthesis <b>rendering</b> <b>device</b> which allows portability of a scene description onto another system...|$|R
50|$|Microsoft Office 2013 {{supports}} either Direct2D+DirectWrite or GDI+Uniscribe for <b>display</b> <b>rendering</b> and typography.|$|R
5000|$|Tiled web maps <b>display</b> <b>rendered</b> maps {{made up of}} {{raster image}} [...] "tiles".|$|R
50|$|AR <b>displays</b> can be <b>rendered</b> on <b>devices</b> {{resembling}} eyeglasses. Versions include eyewear {{that employs}} cameras to intercept {{the real world}} view and re-display its augmented view through the eye pieces and devices in which the AR imagery is projected through or reflected off the surfaces of the eyewear lens pieces.|$|R
5000|$|HTML {{forms were}} to be {{replaced}} by XForms, an XML-based user input specification allowing forms to be displayed appropriately for different <b>rendering</b> <b>devices.</b>|$|R
50|$|Universal Media Server is a DLNA-compliant UPnP media server. It {{originated}} as a fork of PS3 Media Server. It allows streaming {{of media}} files {{to a wide}} range of devices including video game consoles, smart TVs, smartphones, and Blu-ray players. It streams and transcodes multimedia files over a network connection to the <b>rendering</b> <b>device,</b> ensuring that a supported <b>rendering</b> <b>device</b> will receive the content in a format supported by the device. Transcoding is accomplished through packages from AviSynth, FFMpeg, MEncoder, and VLC.|$|R
40|$|A {{fundamental}} {{element of}} stereoscopic and/or autostereoscopic image production is the geometrical {{analysis of the}} shooting and viewing conditions {{in order to obtain}} a qualitative 3 D perception experience. This paper firstly compares the perceived depth with the shot scene depth from the viewing and shooting geometries for a couple of shooting and <b>rendering</b> <b>devices.</b> This yields a depth distortion model whose parameters are expressed from the geometrical characteristics of shooting and <b>rendering</b> <b>devices.</b> Secondly, these expressions are inverted in order to design convenient shooting layouts yielding chosen distortions on specific <b>rendering</b> <b>devices.</b> Thirdly, this design scheme provides three shooting technologies (3 D computer graphics software, photo rail, and camera box system) producing qualitative 3 D content for various kinds of scenes (real or virtual, still or animated), complying with any prechosen distortion when rendered on any specific multiscopic technology or device formerly specified...|$|R
5000|$|... 80×25 {{character}} <b>display,</b> <b>rendered</b> with a 9×16 pixel font, with {{an effective}} resolution of 720×400 in either 16 colors or monochrome, {{the latter being}} compatible with legacy MDA-based applications.|$|R
40|$|Brain-computer {{interface}} (BCI) systems {{based on}} the steady-state visual evoked potential (SSVEP) provide higher information throughput and require shorter training than BCI systems using other brain signals. To elicit an SSVEP, a repetitive visual stimulus (RVS) has to {{be presented to the}} user. The RVS can be rendered on a computer screen by alternating graphical patterns, or with external light sources able to emit modulated light. The properties of an RVS (e. g., frequency, color) depend on the <b>rendering</b> <b>device</b> and influence the SSVEP characteristics. This affects the BCI information throughput and the levels of user safety and comfort. Literature on SSVEP-based BCIs does not generally provide reasons for the selection of the used <b>rendering</b> <b>devices</b> or RVS properties. In this paper, we review the literature on SSVEP-based BCIs and comprehensively report on the different RVS choices in terms of <b>rendering</b> <b>devices,</b> properties, and their potential influence on BCI performance, user safety and comfort...|$|R
50|$|CAML (Collaborative Application Markup Language) is an XML based markup {{language}} used with Microsoft SharePoint technologies (Windows Sharepoint Services and Office SharePoint Server). Unlike plain XML, CAML contains specific groups of tags to both define and <b>display</b> (<b>render)</b> data.|$|R
40|$|International audienceHigh {{dynamic range}} (HDR) {{displays}} use local backlight modulation to produce both high brightness levels and large contrast ratios. Thus, the <b>display</b> <b>rendering</b> algorithm and its parameters may greatly affect HDR visual experience. In this paper, we analyze {{the impact of}} <b>display</b> <b>rendering</b> on perceived quality for a specific display (SIM 2 HDR 47) and for a popular application scenario, i. e., HDR image compression. To this end, we assess whether significant differences exist between subjective quality of compressed images, when these are displayed using either the built-in <b>rendering</b> of the <b>display,</b> or a <b>rendering</b> algorithm developed by ourselves. As a second contribution of this paper, we investigate whether the possibility to estimate the true pixel-wise luminance emitted by the display, offered by our rendering approach, can improve the performance of HDR objective quality metrics that require true pixel-wise luminance as input...|$|R
40|$|We report {{overall design}} {{considerations}} and preliminary results {{for a new}} haptic <b>rendering</b> <b>device</b> based on an audio loudspeaker. Our application models tissue properties during microsurgery. For example, the device could respond {{to the tip of}} a tool by simulating a particular tissue, displaying a desired compressibility and viscosity, giving way as the tissue is disrupted, or exhibiting independent motion, such as that caused by pulsations in blood pressure. Although limited to one degree of freedom and with a relatively small range of displacement compared to other available haptic <b>rendering</b> <b>devices,</b> our design exhibits high bandwidth, low friction, low hysteresis, and low mass. These features are consistent with modeling interactions with delicate tissues during microsurgery. In addition, our haptic <b>rendering</b> <b>device</b> is designed to be simple and inexpensive to manufacture, in part through an innovative method of measuring displacement by existing variations in the speaker’s inductance as the voice coil moves over the permanent magnet. Low latency and jitter are achieved by running the real-time simulation models on a dedicated microprocessor, while maintaining bidirectional communication with a standard laptop computer for user controls and data logging...|$|R
40|$|Note: {{appendices}} {{for this}} title available here. Advances in {{digital signal processing}} technology have created {{a wide variety of}} video <b>rendering</b> <b>devices</b> from mobile phones and portable digital assistants to desktop computers and high definition television. This has resulted in wide diversity of video content with spatial and temporal properties fitting into their intended <b>rendering</b> <b>devices.</b> However the sheer ubiquity of video content creation and distribution mechanisms has effectively blurred the classification line resulting in the need for interchangeable rendering of video content across devices of varying spatio-temporal properties. This results in a need for efficient and effective conversion techniques; mostly to increase the resolution (referred to as super resolution) in-order to enhance quality of perception, user satisfaction and overall the utility of the video content...|$|R
40|$|High-quality {{computer}} graphics let mobile-device users access more compelling content. Still, the devices' {{limitations and requirements}} differ substantially from those of a PC. This survey of mobile graphics research describes current solutions in terms of specialized hardware (including 3 D <b>displays),</b> <b>rendering</b> and transmission, visualization, and user interfaces. © 2008 IEEE...|$|R
50|$|WARP is a full-featured Direct3D 10.1 <b>renderer</b> <b>device</b> with {{performance}} {{on par with}} current low-end graphics cards, such as Intel GMA 3000, when running on multi-core CPUs. To achieve this level of rendering performance, WARP employs advanced techniques such as just-in-time compilation to x86 machine code and support for advanced vector extensions such as SSE2 and SSE4.1.|$|R
5000|$|Pluggable {{software}} device: Performs software <b>rendering.</b> This <b>device</b> {{was introduced}} with DirectX 9.0.|$|R
5000|$|... #Caption: The {{museum and}} its {{internal}} layout, with around seventy <b>display</b> areas <b>rendered</b> in Lego ...|$|R
2500|$|The devices {{on display}} in stores are {{equipped}} with special demo versions of their respective operating systems. iOS devices prevent passcodes from being enabled and Mac computers revert to original state after a reboot. Additionally, in case of theft, a security measure <b>renders</b> <b>devices</b> useless, by activating a [...] "kill switch" [...] disabling them once out-of-reach of the store's Wi-Fi network.|$|R
50|$|Developing {{a native}} mobile BI app poses challenges, {{especially}} concerning data <b>display</b> <b>rendering</b> and user interactivity. Mobile BI App development {{has traditionally been}} a time-consuming and expensive effort requiring businesses to justify the investment for the mobile workforce. They do not only require texting and alerts, they need information customized for their line of work which they can interact with and analyze to gain deeper information.|$|R
50|$|Most usages in the {{immersive}} media segment require compute-intensive scene analysis, {{which must}} often {{be performed in}} real time or near-real time. As with all visual cloud applications, workloads will be distributed between end devices and the cloud. For example, head-mounted <b>display</b> <b>rendering</b> might be done locally to the user to minimize latency, but live VR content distribution could be done predominantly from the cloud.|$|R
40|$|In optical see-through displays, {{light coming}} from {{background}} ob-jects mixes with the light originating from the display, causing {{what is known as}} the color blending problem. Color blending negatively affects the usability of such displays as it impacts the legibility and color encodings of digital content. Color correction aims at reduc-ing the impact of color blending by finding an alternative display color which, once mixed with the background, results in the color originally intended. In this paper we model color blending based on two distortions in-duced by the optical see-through <b>display.</b> The <b>render</b> distortion ex-plains how the <b>display</b> <b>renders</b> colors. The material distortion ex-plains how background colors are changed by the display material. We show the render distortion has a higher impact on color blend-ing and propose binned-profiles (BP) - descriptors of how a <b>display</b> <b>renders</b> colors- to address it. Results show that color blending pre-dictions using BP have a low error rate- within nine just noticeable differences (JND) in the worst case. We introduce a color correction algorithm based on predictions using BP and measure its correction capacity. Results show light display colors can be better corrected for all backgrounds. For high intensity backgrounds light colors in the neutral and CyanBlue regions perform better. Finally, we elab-orate on the applicability, design and hardware implications of our approach...|$|R
5000|$|January 18, 1839, the {{government}} organized, <b>displaying,</b> and <b>rendering</b> honors {{to the new}} flag in the Plaza de Ciudad Guerrero, Tamaulipas.|$|R
50|$|Multi-function <b>displays</b> can <b>render</b> a {{separate}} navigation display unnecessary. Another {{option is to}} use one large screen to show both the PFD and navigation display.|$|R
5000|$|First, the latency of KMixer {{is around}} 30 ms [...] and it cannot be reduced, because this {{component}} sits just right above the port class audio driver, so every audio stream, including those issued by DirectSound (except {{in cases of}} hardware mixing) and WinMM, come through the kernel mixer. If the audio hardware supports hardware mixing (also known as hardware buffering or DirectSound hardware acceleration), DirectSound buffers directly to the <b>rendering</b> <b>device.</b> Thus, if DirectSound streams use hardware mixing, KMixer is bypassed.|$|R
5000|$|In {{the common}} {{usage of the}} term, [...] "bricking" [...] {{suggests}} that the damage is so serious as to have <b>rendered</b> the <b>device</b> permanently unusable.|$|R
50|$|There {{are several}} major {{advantages}} to tiled maps. Each time the user pans, {{most of the}} tiles are still relevant, and can be kept displayed, while new tiles are fetched. This greatly improves the user experience, compared to fetching a single map image for the whole viewport. It also allows individual tiles to be pre-computed, a task easy to parallelise. Also, <b>displaying</b> <b>rendered</b> images served from a web server is much less computationally demanding than rendering images in the browser, a benefit over technologies such as WFS.|$|R
30|$|Retargeting {{algorithms}} {{are mainly}} designed for adapting an image to different <b>rendering</b> <b>devices,</b> such as mobile phones [1, 3, 24, 138]. The {{goal is to}} preserve the main content of the image while discarding unnecessary or redundant information {{in such a way}} the main content is more visible than if a simple resample were applied. Global energy optimization for the whole image may be used for image retargeting [127]. Face detection, text detection, and visual attention, all combined, may also be used for finding content in photos [24].|$|R
5000|$|Resolutions {{less than}} one {{micrometer}} (typically 0.1 μm) and elongations up to 900 mm can be achieved, which <b>renders</b> these <b>devices</b> suitable for the most complex testing.|$|R
50|$|Blu-ray {{equipment}} {{is required to}} implement the High-bandwidth Digital Content Protection (HDCP) system to encrypt the data sent by players to <b>rendering</b> <b>devices</b> through physical connections. This is aimed at preventing the copying of copyrighted content as it travels across cables. Through a protocol flag in the media stream called the Image Constraint Token (ICT), a Blu-ray Disc can enforce its reproduction in a lower resolution whenever a full HDCP-compliant link is not used. In order to ease the transition to high definition formats, the adoption of this protection method was postponed until 2011.|$|R
50|$|In recent years, SVG {{has become}} a {{significant}} format that is completely independent of {{the resolution of the}} <b>rendering</b> <b>device,</b> typically a printer or display monitor. SVG files are essentially printable text that describes both straight and curved paths, as well as other attributes. Wikipedia prefers SVG for images such as simple maps, line illustrations, coats of arms, and flags, which generally are not like photographs or other continuous-tone images. Rendering SVG requires conversion to raster format at a resolution appropriate for the current task. SVG is also a format for animated graphics.|$|R
40|$|Increasing size {{of surface}} models has {{lead to a}} great deal of {{research}} in progressive multi-resolution representation of these models. Network streaming visualization of large models is particularly interesting. Quite a few existing techniques use a point based rendering system. This approach accelerates rendering but suffers from a degradation in the image quality. To mitigate image degradation, we present a hybrid point and triangle primitive based streaming viewer for triangular meshes. The advantage of using such a mixture of rendering primitives is that, while points can accelerate object <b>display</b> <b>rendering,</b> rendering selective objects using triangles can compensate for the degradation in image quality caused due to the point based rendering. We have based our system on an existing point based renderer, QSplat, and extended it to incorporate triangle based rendering. The extension achieves a vast improvement in the quality of the rendered image as compared to QSplat. Like QSplat, the system is meant to be deployed in environments with moderate network bandwidths (e. g. LAN). ...|$|R
50|$|The Meural Canvas employs an {{in-plane}} switching <b>display</b> that <b>renders</b> over 16 million colors. It {{also has a}} low reflective matte finish and an ambient light sensor for adaptive lighting.|$|R
40|$|Abstract — Smart homes {{integrated}} with sensors, actuators, wireless networks and context-aware middleware will soon {{become part of}} out daily life. This paper describes a context-aware middleware providing an automatic home service based on a user’s preference inside a smart home. The context-aware middleware include an appliance controller, a context-aware agent and a scalable browser. The appliance controller takes charge of communication between appliances in the context-aware middleware. The context-aware middleware use OSGi(Open Service Gateway Initial) as framework of the home network. The scalable browser recognize the properties of all <b>rendering</b> <b>device,</b> and it figure out their screen size. We use UIML(User Interface Markup Language) as multiple <b>rendering</b> <b>device.</b> The context-aware agent utilizes 6 basic data values for learning and predicting the user’s preference for the home appliances: the pulse, the body temperature, the facial expression, the room temperature, the time, and the location. The six data sets construct the context model and are used by the context manager module. The user profile manager maintains history information for home appliances chosen by the user. The user-pattern learning and predicting module {{is based on a}} neural network, which predicts the proper home service for the user. The test results show that the pattern of an individual’s preference can be effectively evaluated and predicted by adopting the proposed context mode 1...|$|R
40|$|We {{approach}} {{the problem of}} creating haptic simulators that effectively impart skill without requiring high-fidelity devices by identifying perceptually salient events that signal transitions in the interaction. By augmenting these events, we seek to overcome deficiencies in the fidelity of the rendering hardware. We present an extension of event-based haptic rendering to noncollision events, and we describe a user-study of the training effectiveness of passive force-field haptic simulation vs. active eventaugmented simulation in a tool-manipulation task. The results indicate that active augmentation improves skill transfer without requiring {{an increase in the}} quality of the <b>rendering</b> <b>device.</b> ...|$|R
