3|31|Public
50|$|Step 2 of the {{instruction}} Cycle is called the <b>Decode</b> <b>Cycle.</b> The decoding process allows the CPU to determine what instruction is to be performed, so that the CPU can tell how many operands it needs to fetch in order to perform {{the instruction}}. The opcode fetched from the memory is decoded for the next steps {{and moved to the}} appropriate registers.The decoding is done by the CPU's Control Unit.|$|E
40|$|Compressed Instruction Set {{architecture}} {{refers to}} support for coexistence of instructions of different instruction widths. By re-encoding {{the most frequently}} used instructions in lesser number of widths code density can be increased at the expense of some extra work in the <b>decode</b> <b>cycle.</b> In this project we have provided support for Compressed Instruction Set in an existing re-targetable compiler-simulator framework, EXPRESSION. We tested the generic implementation on the motorola PowerPC architecture. We decided upon a Compressed Instruction Set for the same, and executed our compiler optimization to estimate the improvements in code size and impact on performance. We found encouraging improvements of upto 20 % in the code size for not a setback i...|$|E
40|$|Copyright 2007 Society of Photo-Optical Instrumentation Engineers. One {{print or}} {{electronic}} copy {{may be made}} for personal use only. Systematic electronic or print reproduction and distribution, duplication of any material in this paper for a fee or for commercial purposes, or modification {{of the content of}} the paper are prohibited. This paper can also be found at: [URL] extensive use of context-based adaptive binary arithmetic or variable length coding makes streams highly susceptible to channel errors, a common occurrence over networks such as those used by mobile devices. Even a single bit error will cause a decoder to discard all stream data up to the next fixed length resynchronisation point, the worst scenario is that an entire slice is lost. In cases where retransmission and forward error concealment are not possible, a decoder should conceal any erroneous data in order to minimise the impact on the viewer. Stream errors can often be spotted early in the <b>decode</b> <b>cycle</b> of a macroblock which if aborted can provide unused processor cycles, these can instead be used to conceal errors at minimal cost, even as part of a real time system. This paper demonstrates a technique that utilises Sobel convolution kernels to quickly analyse the neighbourhood surrounding erroneous macroblocks before performing a weighted multi-directional interpolation. This generates significantly improved statistical (PSNR) and visual (IEEE structural similarity) results when compared to the commonly used weighted pixel value averaging. Furthermore it is also computationally scalable, both during analysis and concealment, achieving maximum performance from the spare processing power available...|$|E
40|$|We {{propose a}} {{two-phase}} successive cancellation (TPSC) decoder architecture for polar codes that exploits the array-code property of polar codes by breaking the decoding of a length-TV polar code {{into a series}} of length-√ L <b>decoding</b> <b>cycles.</b> Each <b>decoding</b> <b>cycle</b> consists of two phases: a first phase for decoding along the columns and a second phase for decoding along the rows of the code array. The reduced decoder size makes it more affordable to implement the core decoder logic using distributed memory elements consisting of flip-flops (FFs), as opposed to slower random access memory (RAM), leading to a speed up in clock frequency. To minimize the circuit complexity, a single decoder unit is used in both phases with minor modifications. The re-use of the same decoder module makes it necessary to recall certain internal decoder state variables between <b>decoding</b> <b>cycles.</b> Instead of storing the decoder state variables in RAM, the decoder discards them and calculates them again when needed. Overall, the decoder has O(√ L) circuit complexity excluding RAM, and a latency of approximately 2. 57 V. A RAM of size O(N) is needed for storing the channel log-likelihood variables and the decoder decision variables. As an example of the proposed method, a length N = 214 bit polar code is implemented in an FPGA and the synthesis results are compared with a previously reported FPGA implementation. The results show that the proposed architecture has lower complexity, lower memory utilization with higher throughput, and a clock frequency that is less sensitive to code length. © 2013 IEEE...|$|R
40|$|Throughout {{the history}} of the polar icecaps, dust and {{aerosols}} have been transported through the atmosphere to the poles, to be preserved within the annually freezing ice of the growing ice shields. Therefore, the Antarctic ice sheet is a “time capsule" for environmental data, containing information of ancient periods of Earth’s history. To unravel this history and <b>decode</b> <b>cycles</b> in glaciations and global change is among the major goals of the Dome Fuji Ice Coring Project...|$|R
40|$|In this paper, a {{parallel}} Variable-Length Decoding (VLD) scheme is introduced. The scheme {{is capable of}} decoding all the codewords in an N -bit buffer whose accumulated codelength is at most N. The proposed method partially breaks the recursive dependency related to the MPEG- 2 VLD. All possible codewords in the buffer are detected in parallel and {{the sum of the}} codelengths is provided to the external shifter aligning the variable-length coded input stream for a new <b>decoding</b> <b>cycle.</b> Two length detection mechanisms are proposed: the first approach determines the length in {{a parallel}}/serial fashion and the second using a new device denoted as MultiplexedAdd. In order to prove feasibility and determine the limiting factors of our proposal, the parallel/serial codeword detector with 32 bit input has been described in behavioral non-optimized VHDL and mapped onto Altera's ACEX EP 1 K 100 FPGA. The implemented prototype exhibits a latency of 110 ns and uses 32 % of the logic cells of the device. When applied to MPEG- 2 standard benchmark scenes, on average 3. 5 symbols are <b>decoded</b> per <b>cycle...</b>|$|R
30|$|As {{mentioned}} before, {{we developed}} a new table mapping algorithm for CAVLD and designated instructions for multi-format bitstream decoding. The added instructions, Showbits, GetBits, and Skipbits instructions run in 2 cycles, and the other instructions, CLZ and I_ONERETURN, run in 1 cycle. To evaluate {{the performance of the}} new table mapping algorithm for the coeff_token, level, and run_before syntax elements, we used a developed CAVLD decoder and compared the <b>decoding</b> <b>cycles</b> of the CAVLD. For example, to evaluate algorithms for coeff_token decoding, we just replaced the proposed algorithm for coeff_token with other conventional algorithms for the syntax element. In that case, the designated instructions are also applied not only for the proposed table mapping algorithm but also for the conventional algorithms.|$|R
40|$|Abstract The {{hardware}} {{design of a}} bit-slice microprocessor-based realtime cyclic error-correcting communications decoder is presented. A microprocessor-based architecture is preferred because of its programmability, low cost and simplicity of design. To augment the throughput of the decoder for realtime decoding, the ALU word length is chosen to be {{equal to that of}} a code word and the decoding operation is accomplished in two steps, i. e. error detection and error correction. A buffer memory stores incoming blocks as more than one block may be received during a <b>decoding</b> <b>cycle.</b> The design is versatile: different decoding algorithms can be executed by changing the microprogram. Only simple changes in the design are necessary to decode words of longer block length. Keywords: microprocessors; digital communications; decodin...|$|R
40|$|International audienceStochastic {{decoding}} that {{is inspired}} by stochastic computation is an alternative technique for decoding of error-correcting codes. The extension of this approach to decode convolutional codes and turbo codes is discussed in this article. The switching activity sensitivity is circumvented and the latching problem is reduced by transforming the stochastic additions into stochastic multiplications in the exponential domain and using multiple streams with deterministic shufflers. The number of <b>decoding</b> <b>cycles</b> is thus considerably reduced with no performance degradation. Stochastic decoding, previously applied to the decoding of LDPC codes, can now be applied to decoding of turbo codes. In addition, the first hardware architecture for stochastic decoding of turbo codes is presented. The proposed architecture makes fully-parallel turbo decoding viable on FPGA devices. Results demonstrate the potential of stochastic decoding to implement fully-parallel turbo decoders...|$|R
40|$|This thesis {{presents}} a clockless stochastic low-density parity-check (LDPC) decoder implemented on a Field-Programmable Gate Array (FPGA). Stochastic computing reduces the wiring complexity necessary for decoding by replacing operations such as multiplication and division with simple logic gates. Clockless decoding increases the throughput of the decoder {{by eliminating the}} requirement for node signals to be synchronized after each <b>decoding</b> <b>cycle.</b> With this partial-update algorithm the decoder’s speed {{is limited by the}} average wire delay of the interleaver rather than the worst-case delay. This type of decoder has been simulated in the past but not implemented on silicon. The design is implemented on an ALTERA Stratix IV EP 4 SGX 230 FPGA and the frame error rate (FER) performance, throughput and power consumption are presented for (96, 48) and (204, 102) decoders...|$|R
40|$|Stochastic {{decoding}} that {{is inspired}} by stochastic computation is an alternative technique for decoding of error-correcting codes. The extension of this approach to decode convolutional codes and turbo codes is discussed in this article. The switching activity sensitivity is circumvented and the latching problem is reduced by transforming the stochastic additions into stochastic multiplications in the exponential domain and using multiple streams with deterministic shufflers. The number of <b>decoding</b> <b>cycles</b> is thus considerably reduced with no performance degradation. Stochastic decoding, previously applied to the decoding of LDPC codes, can now be applied to decoding of turbo codes. In addition, the first hardware architecture for stochastic decoding of turbo codes is presented. The proposed architecture makes fully-parallel turbo decoding viable on FPGA devices. Results demonstrate the potential of stochastic decoding to implement fully-parallel turbo decoders. 1...|$|R
40|$|The Institute for Telecommunication Sciences (ITS) has {{developed}} an objective video quality assessment system that emulates human perception. The system returns results that agree closely with quality judgements made by a large panel of viewers. Such a system is valuable because it provides broadcasters, video engineers and standards organizations with the capability for making meaningful video quality evaluations without convening viewer panels. The issue is timely because compressed digital video systems present new quality measurement questions that are largely unanswered. The perception-based system was developed and tested for {{a broad range of}} scenes and video technologies. The 36 test scenes contained widely varying amounts of spatial and temporal information. The 27 impairments included digital video compression systems operating at line rates from 56 kbits/sec to 45 Mbits/sec with controlled error rates, NTSC encode/ <b>decode</b> <b>cycles,</b> VHS and S-VHS record/play cycles, and VH [...] ...|$|R
40|$|An {{iterative}} turbo decoder based cross layer {{error recovery}} scheme for compressed video {{is presented in}} this paper. The soft information exchanged between two convolutional decoders are reinforced both by channel coded parity and video compression syntactical information. An algorithm to identify the video frame boundaries in corrupted compressed sequences is formulated. The paper continues to propose algorithms to deduce the correct values for selected fields in the compressed stream. Modifying the turbo extrinsic information using these corrections act as reinforcements in the turbo decoding iterative process. The optimal number of turbo iterations suitable for the proposed system model is derived using EXIT charts. Simulation results reveal that a transmission power saving of 2. 28 % can be achieved using the proposed methodology. Contrary to typical joint cross layer decoding schemes, the additional resource requirement is minimal since the proposed <b>decoding</b> <b>cycle</b> does not involve the decompression function...|$|R
40|$|Abstract A {{bit-slice}} microprocessor-based real-time decoder {{has been}} proposed in this paper. A microprocessor-based architecture is preferable because of its programmability, availability, low cost and simplicity of design. Two strategies are adapted to increase throughput of the decoder for real-time decoding. First, bit-slice microprocessors are used and ALU word length is chosen to be {{equal to that of}} a code word. Second, decoding operation is accomplished in two steps, namely (1) Error detection and (2) Error correction. It takes relatively much longer time to correct errors. Therefore, a buffer memory is used to store incoming blocks as more than one block may be received during a <b>decoding</b> <b>cycle.</b> The design is versatile since different decoding algorithms can be executed by changing the microprogram. Minor, apparent and simple changes have {{to be made in the}} design to decode codes of longer block length. Keywords: Bit-slice devices; Coding; Decoding; Microprogramming; Universal AHPL...|$|R
40|$|Abstract Background The {{evolutionary}} {{forces that}} determine {{the arrangement of}} synonymous codons within open reading frames and fine tune mRNA translation efficiency are not yet understood. In order to tackle this question we have carried out a large scale study of codon-triplet contexts in 11 fungal species to unravel associations or relationships between codons present at the ribosome A-, P- and E-sites during each <b>decoding</b> <b>cycle.</b> Results Our analysis unveiled high bias {{within the context of}} codon-triplets, in particular strong preference for triplets of identical codons. We have also identified a surprisingly large number of codon-triplet combinations that vanished from fungal ORFeomes. Candida albicans exacerbated these features, showed an unbalanced tRNA population for decoding its pool of codons and used near-cognate decoding for a large set of codons, suggesting that unique evolutionary forces shaped the evolution of its ORFeome. Conclusion We have developed bioinformatics tools for large-scale analysis of codon-triplet contexts. These algorithms identified codon-triplets context biases, allowed for large scale comparative codon-triplet analysis, and identified rules governing codon-triplet context. They could also detect alterations to the standard genetic code. </p...|$|R
40|$|In {{this paper}} we study in-order packet {{delivery}} delay of two recently proposed network coded transmission schemes with applications in wireless broadcast. Unlike previous works where asymptotic behaviour of decoding or delivery delay was presented, we provide a general {{analysis of the}} three conditions under which in-order packet delivery is possible at a receiver: by 1) catching up with the sender, 2) receiving while a leader, and 3) chance decoding. We use a Markov model to represent {{the difference between the}} knowledge space of the sender and a receiver. For the first condition, we calculate the expected distribution of <b>decoding</b> <b>cycle</b> lengths under the Markov model. For the second condition, we propose to use a simplifying independent Markov model among receivers to shed light on the factors that determine the probability of receiving while a leader. Finally, we compare the chance decoding probabilities of two transmission schemes and a baseline random transmission algorithm to show that surprisingly (and fortunately) the probability of chance decoding is significant in one of the transmission schemes. We verify our analysis by extensive simulations and discuss the usefulness of our study for understanding and design of better transmission algorithms...|$|R
40|$|It is {{speculated that}} the most {{probable}} channel noise realizations (instantons) that cause the iterative decoding of low-density parity-check codes to fail make the decoding not to converge. A simple example is given of an instanton {{that is not a}} pseudo-codeword and causes iterative <b>decoding</b> to <b>cycle.</b> A method of finding the instantons for large number of iterations is presented and tested on Tanner's [155, 64, 20] code and Gaussian channel. The inherently dynamic instanton with effective distance of 11. 475333 is found. Comment: 5 pages, 7 figure...|$|R
40|$|In this paper, an {{algorithm}} for {{the fast}} decoding of binary variable-length codes is analysed, both regaxding the decoding speed, {{relative to a}} traditional tree-search decoder, artd with regaxd to the memory spce. The algorithm {{is based on a}} lookup table, which will allow blocks of compressed data to be <b>decoded</b> in one <b>cycle...</b>|$|R
40|$|A {{telecommunication}} network is said survivable {{if it is}} still able to provide service after one of its components fails. Survivability is achieved by redirecting the data through other spans of the network where spare capacity was previously introduced. There are two important aspects {{to take into account}} while providing survivability to the network: fast recovery and low cost. pCycles are structures that were introduced to design the spare capacity of optical networks providing quick restoration. Representing the network as a 2 connected graph, a pCycle is a cycle composed of one preconfigured spare channel on each span (edge) it crosses. Each pCycle provides one protection channel (unit of demand) to each span it crosses and two protection channels to each span that is not in the cycle but its ending nodes are (straddling span). We deal here with the Spare Capacity Allocation (SCA) problem which requires protecting all working demands against any span failure with pCycles at minimum cost. We propose a greedy heuristic that builds a solution for this problem iteratively. At each step, a Genetic Algorithm builds a cycle trying to maximize the Actual Efficiency, which was defined in the literature for developing other heuristics for this problem. To achieve this, we propose to <b>decode</b> <b>cycles</b> from genes representing fundamental cycles of a basis of cycle space. We give two alternatives for fitness evaluation to treat disjoint cycles or closed walks with repeated nodes. Several computational experiments were performed and promising results were obtained...|$|R
40|$|The Very Simple CPU Simulator is an {{instructional}} aid for students studying computer architecture and CPU design, typically {{at the junior}} or senior level. It simulates a 4 -instruction CPU introduced in the textbook Computer Systems Organization and Architecture. Students first enter an assembly language program, which is assembled by the simulator. After correcting any syntax errors, the user simulates the fetch, <b>decode,</b> and execute <b>cycles</b> of each instruction. The simulato...|$|R
40|$|This {{dissertation}} {{focuses on}} multiuser communications through shallow, underwater acoustic channels. These channels {{are characterized by}} channel impulse responses with long delay spreads undergoing rapid fluctuations {{with respect to the}} digital signaling time. When multiple users (e. g. AUVs, gliders, or sensor nodes) need to transmit information to a common receiver, they must share the channel in some fashion. The designs presented in this dissertation utilize a sharing scheme known as Space Division Multiple Access (SDMA), where the inherent disparity in the impulse responses sampled at different spatial locations are leveraged by the system to provide users with interference-free uplinks to the common receiver. Compared to other channel sharing methods, SDMA benefits from high data throughput and a low reliance on feedback from the receiver, two desirable qualities in a bandwidth limited, rapidly evolving environment. The receivers discussed throughout this dissertation will employ successive decoding techniques to retrieve each user's information independently but will use knowledge from previous <b>decoding</b> <b>cycles</b> to model and remove multiple access interference along the way. With multiple iterations of estimation and interference cancellation, these receivers will progress towards the goal of providing each and all of the users with interference-free uplinks to the receiver. Three receivers will be discussed in this dissertation with each successive design more generally applicable than the previous: one will be applicable in time-invariant environments between geographically fixed users and a fixed, multiple-element receiver, the next will be applicable in a time-varying environment between fixed users and a fixed receiver array, and the final design will be applicable in situations with users in motion. All of the receivers discussed will require direct knowledge of the impulse response and will employ sparse channel estimation techniques to acquire this information and track any changes while decoding. The capabilities of all of the receivers will be analyzed with data collected during at-sea experiment...|$|R
40|$|In this paper, we {{investigate}} the error floors of non-binary low-density parity-check (LDPC) codes transmitted over the memoryless binary-input output-symmetric (MBIOS) channels. We provide a necessary and sufficient condition for successful <b>decoding</b> of zigzag <b>cycle</b> codes over the MBIOS channel by the belief propagation decoder. We consider an expurgated ensemble of non-binary LDPC codes {{by using the}} above necessary and sufficient condition, and hence exhibit lower error floors. Finally, we show lower bounds of the error floors for the expurgated LDPC code ensembles over the MBIOS channel. Comment: 15 pages, 9 figures, The material in this paper was presented in part at IEEE International Conference on Communications, submitted in IEICE transaction fundamental...|$|R
40|$|Abstract—Long polar codes {{can achieve}} the {{capacity}} of ar-bitrary binary-input discrete memoryless channels under a low complexity successive cancelation (SC) decoding algorithm. But for polar codes with short and moderate code length, the decoding performance of the SC decoding algorithm is inferior. The cyclic redundancy check (CRC) aided successive cancelation list (SCL) decoding algorithm has better error performance than the SC decoding algorithm for short or moderate polar codes. However, the CRC aided SCL (CA-SCL) decoding algorithm still suffer from long decoding latency. In this paper, a reduced latency list decoding (RLLD) algorithm for polar codes is proposed. For the proposed RLLD algorithm, all rate- 0 nodes and part of rate- 1 nodes are decoded instantly without traversing the corresponding subtree. A list maximum-likelihood decoding (LMLD) algorithm is proposed to decode the maximum likelihood (ML) nodes and the remaining rate- 1 nodes. Moreover, a simplified LMLD (SLMLD) algorithm is also proposed to reduce the computational complexity of the LMLD algorithm. Suppose a partial parallel list decoder architecture with list size L = 4 is used, for an (8192, 4096) polar code, the proposed RLLD algorithm can {{reduce the number of}} <b>decoding</b> clock <b>cycles</b> and <b>decoding</b> latency by 6. 97 and 6. 77 times, respectively. I...|$|R
40|$|Long polar codes {{can achieve}} the {{capacity}} of arbitrary binary-input discrete memoryless channels under a low complexity successive cancelation (SC) decoding algorithm. But for polar codes with short and moderate code length, the decoding performance of the SC decoding algorithm is inferior. The cyclic redundancy check (CRC) aided successive cancelation list (SCL) decoding algorithm has better error performance than the SC decoding algorithm for short or moderate polar codes. However, the CRC aided SCL (CA-SCL) decoding algorithm still suffer from long decoding latency. In this paper, a reduced latency list decoding (RLLD) algorithm for polar codes is proposed. For the proposed RLLD algorithm, all rate- 0 nodes and part of rate- 1 nodes are decoded instantly without traversing the corresponding subtree. A list maximum-likelihood decoding (LMLD) algorithm is proposed to decode the maximum likelihood (ML) nodes and the remaining rate- 1 nodes. Moreover, a simplified LMLD (SLMLD) algorithm is also proposed to reduce the computational complexity of the LMLD algorithm. Suppose a partial parallel list decoder architecture with list size $L= 4 $ is used, for an (8192, 4096) polar code, the proposed RLLD algorithm can {{reduce the number of}} <b>decoding</b> clock <b>cycles</b> and <b>decoding</b> latency by 6. 97 and 6. 77 times, respectively. Comment: 7 pages, accepted by 2014 IEEE International Workshop on Signal Processing Systems (SiPS...|$|R
40|$|The {{application}} and study of iterative message-passing decoders has exploded in recent years, {{due to their}} amazing efficiency and near-optimal performance. Much of the analysis of these decoders relies on a heuristic link between the local nature of these algorithms and certain graph structures, called graph covers, that are locally indistinguishable. The precise relationship between graph covers and computation trees, which Wiberg proved {{can be used to}} exactly model the behavior of iterative message-passing decoders, remains unclear. ^ The focus of this dissertation is to further explore the relationship between graph covers and computation trees, and their related pseudocodewords, so that the plethora of results on graph covers may be more readily applied to computation trees, and hence to the analysis of iterative message-passing decoding algorithms. We show that every graph cover pseudocodeword gives rise to a computation tree pseudocodeword and that, conversely, every computation tree pseudocodeword does indeed arise from a graph cover pseudocodeword. Although these results strengthen the relationship between these different types of pseudocodewords, it is clear that more study is needed, as we show that there is a single graph cover pseudocodeword that simultaneously gives rise to every computation tree pseudocodeword. We also present a completely graphical characterization of certain graph cover pseudocodewords which are known to cause errors in graph cover <b>decoding</b> of <b>cycle</b> codes. ...|$|R
40|$|In {{this paper}} we propose a method to {{watermark}} digital video content {{in such a way that}} detection in consumer electronics (CE) equipment is possible with very little hardware (a few thousand gates). The method proposes to modify the MPEG encoding procedure to choose the so-called Picture Type of video-frames not from a regular sequence but according to a message one would like to transmit. Removal of this embedded message, the PTY-Mark, from the resulting MPEG-stream without jeopardizing video quality is only possible after a complete MPEG <b>decoding</b> and re-encoding <b>cycle.</b> We investigate the modifications to current MPEG encoders which are necessary to accommodate these PTYMarks. Based on tests we comment on their feasibility. Detection of watermarks without secrets is very reminiscent of "public-key" cryptography. We discuss this relationship by contrasting PTY-marks with pixelwatermarking. Keywords: Watermarking, copy-protection, MPEG, DVD-video. 1 Introduction The last few years [...] ...|$|R
40|$|In {{this paper}} we propose an {{extension}} of the Recursive Auto-Associative Memory (RAAM) by Pollack. This extension, the Labeling RAAM (LRAAM), is able to encode labeled graphs with cycles by representing pointers explicitly. A theoretical analysis of the constraints imposed on the weights by the learning task under the hypothesis of perfect learning and linear output units is presented. Cycles and confluent pointers result to be particularly effective in imposing constraints on the weights. Some technical problems encountered in the RAAM, such as the termination problem in the learning and decoding processes, are solved more naturally in the LRAAM framework. The representations developed for the pointers seem to be robust to recurrent <b>decoding</b> along a <b>cycle.</b> Data encoded in a LRAAM can be accessed by pointer as well as by content. The direct access by content can be achieved by transforming the encoder network of the LRAAM in a Bidirectional Associative Memory (BAM). Different access pro [...] ...|$|R
40|$|The paper {{presents}} {{a case study}} on augmenting a TriMedia/CPU 64 processor with a Reconfigurable (FPGA-based) Functional Unit (RFU). We first propose {{an extension of the}} TriMedia/CPU 64 architecture, which consists of a RFU and its associated instructions. Then, we address the computation of the 8 8 IDCT on such extended TriMedia, and propose a scheme to implement an 8 -point IDCT operation on the RFU. Further, we address the decoding of Variable Length Codes (VLC) and describe the FPGA implementation of a Variable Length Decoder (VLD) computing facility. When mapped on an ACEX EP 1 K 100 FPGA from Altera, our 8 -point IDCT exhibits a latency of 16 and a recovery of 2 TriMedia cycles, and occupies 42 % of the FPGA's logic array blocks. The proposed VLD exhibits a latency of 7 TriMedia cycles when mapped on the same FPGA, and utilizes 6 of its embedded array blocks. By using the the 8 -point IDCT computing facility, an 8 8 IDCT including all overheads can be computed with the throughput of 1 / 32 IDCT/cycle. Also, with the proposed VLD computing facility, a single DCT coefficient can be <b>decoded</b> in 11 <b>cycles</b> including all overheads...|$|R
40|$|We {{show that}} a {{hardware}} implementation of a lossless image compression schemecan be used as means for lowering DDR memory bandwidth usage from a videostream. A prediction scheme based on LOCO-I is used to reduce correlative redundancybetween sequential pixels, before the data is encoded by Golomb coding. The data packages after source coding contain a continuous stream of prefix codes,in order to eliminate the header data imposed by more advanced packing schemes. This in turn results in a higher demand on the decoding side in terms of resourceusage, {{because of the need}} for high parallelism when a new prefix code is countedand <b>decoded</b> each clock <b>cycle.</b> The test images are reduced in size by 49 - 84 %, depending on their inherent complexity. Resource consumption for this design amounts to 10100 Logic Elements(synthesized for an Altera Cyclon III FPGA - EP 3 C 80 F 484 C 6), with a operatingfrequency of 152, 86 MHz for a throughput of 458 MB/s. These numbers can beimproved by reducing the algorithm complexities. LE usage is reduced to 4695,while accomplishing a image size reduction of 34 - 59 %. We present a way to increase decompression throughput by adding parallel decodermodules. Those changes will increase throughput to a multiple of 458 MB/s whileworsening the compression somewhat. LE cost increases depending on the level ofparallelism. </p...|$|R
40|$|In this paper, {{we propose}} an {{extension}} to the recursive auto-associative memory (RAAM) by Pollack. This extension, the labelling RAAM (LRAAM), can encode labelled graphs with cycles by representing pointers explicitly. Some technical problems {{encountered in the}} RAAM, such as the termination problem in the learning and decoding processes, are solved more naturally in the LRAAM framework. The representations developed for the pointers seem to be robust to recurrent <b>decoding</b> along a <b>cycle.</b> Theoretical and experimental {{results show that the}} performances of the proposed learning scheme depend on the way the graphs are represented in the training set. Critical features for the representation are cycles and confluent pointers. Data encoded in a LRAAM can be accessed by a pointer as well as by content. Direct access by content can be achieved by transforming the encoder network of the LRAAM into a particular bidirectional associative memory (BAM). Statistics performed on different instances of LRAAM show a strict connection between the associated BAM and a standard BAM. Different access procedures can be defined depending on the access key. The access procedures are not wholly reliable; however, they seem to have a good success rate. The generalization test for the RAAM is no longer complete for the LRAAM. Some suggestions on how to solve this problem are given. Some results on modular LRAAM, stability and application to neural dynamics control are summarized...|$|R
40|$|Abstract: Two {{innovative}} high-speed {{low power}} parallel 8 -bit counter architectures are proposed. Then, High speed 8 -bit frequency divider circuits using the proposed architectures are realized. The proposed parallel counter architectures consist of two sections – The Counting Path and the State Excitation Module. The counting path {{consists of three}} counting modules in which the first module (basic module) generates future states for the two remaining counting modules. The State Excitation Module decodes the count states of the basic module and carries this <b>decoding</b> over clock <b>cycles</b> through pipelined DFF to trigger the subsequent counting modules. The existing 8 -bit parallel counter architecture [1] consumed a total transistor count of 442 whereas the proposed parallel counters consumed only 274 transistors. The power dissipation of the existing parallel counter architecture and the proposed parallel counter architecture were 4. 21 mW (PINT) and 3. 60 mW (PINT) respectively at 250 MHz. The worst case delay observed for the 8 -bit counter using existing parallel counter architecture [1] and the proposed parallel counter architectures were 7. 481 ns, 6. 737 ns and 6. 677 ns respectively using Altera Quartus II. A reduction in area (transistor count) by 27. 45 % {{and a reduction in}} power dissipation by 16. 28 % are achieved for the frequency dividers using proposed counter architectures. Also a reduction in delay by 10. 75 % and 7. 62 % is achieved for the 8 -bit frequency divider circuits using proposed counter methods I & II respectively...|$|R
40|$|Abstract. The paper {{presents}} {{a case study}} on augmenting a TriMedia/CPU 64 processor with a Reconfigurable (FPGA-based) Functional Unit (RFU). We first propose {{an extension of the}} TriMedia/CPU 64 architecture, which consists of a RFU and its associated instructions. Then, we address the computation of the � ¢ � IDCT on such extended TriMedia, and propose a scheme to implement an 8 -point IDCT operation on the RFU. Further, we address the decoding of Variable Length Codes (VLC) and describe the FPGA implementation of a Variable Length Decoder (VLD) computing facility. When mapped on an ACEX EP 1 K 100 FPGA from Altera, our 8 -point IDCT exhibits a latency of 16 and a recovery of 2 TriMedia cycles, and occupies 42 % of the FPGA’s logic array blocks. The proposed VLD exhibits a latency of 7 TriMedia cycles when mapped on the same FPGA, and utilizes 6 of its embedded array blocks. By using the the 8 -point IDCT computing facility, an � ¢ � IDCT including all overheads can be computed with the throughput of 1 / 32 IDCT/cycle. Also, with the proposed VLD computing facility, a single DCT coefficient can be <b>decoded</b> in 11 <b>cycles</b> including all overheads. Simulation results indicate that by configuring each of the 8 -point IDCT and VLD computing facilities on a different FPGA context, and by activating the contexts as needed, the augmented TriMedia can perform MPEG macroblock parsing followed up by a pel reconstruction with an improvement of 20 - 25 % over the standard TriMedia. ...|$|R
40|$|Este trabalho propõe uma descodificação do universo musical de Frederic Chopin através da implementação dum sistema imagético – musical aplicado aos seus 24 Prelúdios op. 28. Esta investigação pretende criar e fundamentar uma interpretação nova e bela e, no nosso entender, mais artística e convincente. Pretendemos ainda descodificar o grafismo musical encontrado, relacionando os elementos ponto, linha, plano e cor, elementos que constituem, na visão do pintor Wassily Kandinsky, as bases de qualquer imagem, seja pictórica ou musical, com o universo discursivo e musical imaginado pelo compositor. Utilizando como {{principal}} ferramenta de implementação, a analogia e, em última análise, a transferência sensorial, descobriremos que, atrás do grafismo duma partitura existe um universo que abrange uma simbiose afirmada de conceitos imagéticos e conceitos musicais. Este foi denominado como sendo o Universo Divinatório, uma fonte incansável de recursos imagético – musicais, a qual, através de canais mais ou menos visíveis, direcciona mensagens de natureza invisível e inenarrável, as quais serão codificadas através da escrita gráfico – musical. Neste sentido, o intérprete identificará, na escrita gráfico – musical, elementos gráficos codificadores de momentos temporais e espaciais de tensão criativa e musical, e imagens imagético – musicais que se encontram em perpétua recriação e reformulação. É nosso propósito mostrá-los em concerto através da performance da obra. ABSTRACT: This {{work has}} as primary objective to <b>decode</b> Chopin´s musical <b>cycle</b> known as 24 Prelúdes op. 28. The {{basis of the}} construction is a methodology identified as being related to creation of a process of symbiosis manifested between concepts of imagistic and music. This process will be transposed using musical notation which uses as fundamental elements: the point, the line, the plane, and, on another level, colour. The author was {{able to create a}} relation between Frederic Chopin and Wassily Kandinsky by using as connection the graphism of the scores. Kandinsky created a series of books as “Point and Line to Plane” (1926), that tries to analyze the geometrical elements which and compose any kind of image (pictorial or musical), and others. He describes the cause and effect of their usage and elaborates a series of theories about the subjective effect produced. In order to reach the goal, the author used the 24 Preludes op. 28 of Frederic Chopin as the element of transition between the two worlds of interaction: the imagistic world and the musical world. The paradigm formed creates a new universe which the author denominated as being related to one Universe of Divination. It combines musical and imagistic fractions under the materialization of a symbiosis process. The main tools is used to define the materialization of this Universe is related to the implementation of the Kandinsky´s system of thinking, the graphism, which exists in a visible form in scores. It can translate in a correct way the utilization of motion with different directionality, and the creation of different tensions by combining those motions. We show our investigation through a performance, on concert,of the 24 Preludes op. 28. Mestrado em Músic...|$|R

