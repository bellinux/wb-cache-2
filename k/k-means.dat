10000|416|Public
5|$|Steinhaus {{was also}} the first person to {{conjecture}} the ham-sandwich theorem, {{and one of the first}} to propose the method of <b>k-means</b> clustering.|$|E
25|$|For {{the most}} part, {{computer}} vision algorithms used on color images are straightforward extensions to algorithms designed for grayscale images, for instance <b>k-means</b> or fuzzy clustering of pixel colors, or canny edge detection. At the simplest, each color component is separately {{passed through the}} same algorithm. It is important, therefore, that the features of interest can be distinguished in the color dimensions used. Because the R, G, and B components of an object’s color in a digital image are all correlated {{with the amount of}} light hitting the object, and therefore with each other, image descriptions in terms of those components make object discrimination difficult. Descriptions in terms of hue/lightness/chroma or hue/lightness/saturation are often more relevant.|$|E
2500|$|K-SVD is an {{algorithm}} that performs SVD at {{its core}} to update the atoms of the dictionary {{one by one and}} basically is a generalization of <b>K-means.</b> It enforces that each element of the input data [...] is encoded by a linear combination of not more than [...] elements in a way identical to the MOD approach: ...|$|E
3000|$|... [...]. The {{grouping}} {{procedure is}} based on the well-known <b>K-mean</b> technique. The difference here is that before the application of the <b>K-mean,</b> we first compute the algorithmic mean of [...]...|$|R
30|$|The <b>K-mean</b> {{algorithm}} {{has less}} space requirements because it only needs to store data points and centers. The required storage capacity is O (n[*]+[*]K), where n {{is the number}} of data points and the <b>K-mean</b> algorithm has less time requirements. Basically, it has a linear relationship with the number n of data points, that is, O (IKn), where I {{is the number of}} iterations of convergence, I is usually small, and can be bounded, that is, most of the changes usually occur in general. In the first few iterations, the <b>K-mean</b> algorithm is the most widely used algorithm because of its simplicity and high complexity. However, the <b>K-mean</b> algorithm also has many disadvantages.|$|R
40|$|Cluster {{analysis}} is a descriptive task {{that seek to}} identify homogenous group of object {{and it is also}} one of the main analytical method in data mining. <b>K-mean</b> is the most popular partitional clustering method. In this paper we discuss standard <b>k-mean</b> algorithm and analyze the shortcoming of kmean algorithm. In this paper three dissimilar modified <b>k-mean</b> algorithm are discussed which remove the limitation of <b>k-mean</b> algorithm and improve the speed and efficiency of <b>k-mean</b> algorithm. First algorithm remove the requirement of specifying the value of k in advance practically which is very difficult. This algorithm result in optimal number of cluster Second algorithm reduce computational complexity and remove dead unit problem. It select the most populated area as cluster center. Third algorithm use simple data structure that can be used to store information in each iteration and that information can be used in next iteration. It increase the speed of clustering and reduce time complexit...|$|R
2500|$|Once the {{parameter}} {{maintenance is}} made, foreground detection {{can be made}} and so on. An on-line <b>K-means</b> approximation is used to update the Gaussians. Numerous improvements of this original method developed by Stauffer and Grimson [...] have been proposed and a complete survey {{can be found in}} Bouwmans et al. A standard method of adaptive backgrounding is averaging the images over time, creating a background approximation which is similar to the current static scene except where motion occur.|$|E
2500|$|Libraries {{of these}} {{fragments}} are constructed from {{an analysis of}} the Protein Data Bank (PDB). First, a representative subset of the PDB is chosen which should cover a diverse array of structures, preferably at a good resolution. Then, for each structure, every set of n consecutive residues is taken as a sample fragment. The samples are then clustered into k groups, based upon how similar they are to each other in spatial configuration, using algorithms such as <b>k-means</b> clustering. The parameters n and k are chosen according to the application (see discussion on complexity below). The centroids of the clusters are then taken to represent the fragment. Further optimization can be performed to ensure that the centroid possesses ideal bond geometry, as it was derived by averaging other geometries.|$|E
2500|$|More commonly, {{expression}} profiling {{takes place}} before enough {{is known about}} how genes interact with experimental conditions for a testable hypothesis to exist. With no hypothesis, {{there is nothing to}} disprove, but expression profiling can help to identify a candidate hypothesis for future experiments. [...] Most early expression profiling experiments, and many current ones, have this form which is known as class discovery. [...] A popular approach to class discovery involves grouping similar genes or samples together using <b>k-means</b> or hierarchical clustering. Apart from selecting a clustering algorithm, user usually has to choose an appropriate proximity measure (distance or similarity) between data objects. The figure above represents the output of a two dimensional cluster, in which similar samples (rows, above) and similar gene probes (columns) were organized so that they would lie close together. [...] The simplest form of class discovery would be to list all the genes that changed by more than a certain amount between two experimental conditions.|$|E
30|$|The <b>K-mean</b> {{method was}} used to cluster face features. By {{analyzing}} the face feature sets with different light, expression, and scar, {{the results showed that}} <b>K-mean</b> can improve the recognition effect, and the feature change recognition rate due to expression works best, recognition rate increased to 91 %.|$|R
30|$|Therefore, it is {{important}} to extract the face features that can adapt to environmental changes and improve the robustness of the recognition results. Clustering technology [10 – 13] is to group similar features into a group, because of the similarity between the same class, so it {{can be used as a}} basis for classification, and the similarity which is not the same between the same class can allow differences in the type of features, which solves the problem that the partial change of the face features caused by the change of the environment affecting the classification accuracy. <b>K-mean</b> is a widely used method in cluster analysis. For example, in the research of Wagstaff [14], the <b>K-mean</b> method is used to research knowledge discovery, and the clustering research is carried out on 16 data sets to improve the accuracy of clustering. The research of Kang [15] using <b>K-mean</b> method to segment the image is to use the method of <b>K-mean</b> for clustering analysis. All of them are based on the clustering ability of <b>K-mean</b> for feature analysis.|$|R
30|$|Face {{recognition}} {{technology is}} an application technology of information security, {{which is a}} kind of multi-disciplinary technology, such as comprehensive mathematics, pattern recognition, and biological characteristics. With the development of technology applications, the requirements for accuracy and anti-counterfeiting of face recognition are also increasing. In this paper, the <b>K-mean</b> algorithm is used to analyze the face features. Firstly, the biometric features of the face are extracted, and then the <b>K-mean</b> method is used to cluster the face features. Lastly, the SVM method is used to classify. The results show that the <b>K-mean</b> method can achieve high recognition performance with fewer feature numbers.|$|R
2500|$|No agreed {{standard}} method defines which states are middle powers. Some researchers use Gross National Product (GNP) statistics to draw lists of middle powers around the world. Economically, middle powers are generally {{those that are}} not considered too [...] "big" [...] or too [...] "small", however that is defined. Economics is not always the defining factor. Under the original sense of the term, a middle power was one that had some degree of influence globally but did not dominate in any one area. [...] This usage is not universal, and some define middle power to include nations that can be regarded as regional powers. The international relations scholar Enrico Fels employs <b>k-means</b> clustering in order to identify six regional middle powers in Asia-Pacific based on a composite index that involves 54 indicators and covers the aggregated power base of 44 regional states. Based on a Realist ontology he conducts mixed-method research to analyse these six regional middle powers' balancing and bandwagoning strategies in order to outline whether a relational power shift between China and the United States has taken place {{since the end of the}} Cold War.|$|E
2500|$|Class {{discovery}} analysis: This analytic approach, {{sometimes called}} unsupervised classification or knowledge discovery, tries to identify whether microarrays (objects, patients, mice, etc.) or genes cluster together in groups. Identifying naturally existing groups of objects (microarrays or genes) which cluster together can enable {{the discovery of}} new groups that otherwise were not previously known to exist. During knowledge discovery analysis, various unsupervised classification techniques can be employed with DNA microarray data to identify novel clusters (classes) of arrays. This type of approach is not hypothesis-driven, but rather is based on iterative pattern recognition or statistical learning methods to find an [...] "optimal" [...] number of clusters in the data. Examples of unsupervised analyses methods include self-organizing maps, neural gas, <b>k-means</b> cluster analyses, hierarchical cluster analysis, Genomic Signal Processing based clustering and model-based cluster analysis. For some of these methods the user also has to define a distance measure between pairs of objects. Although the Pearson correlation coefficient is usually employed, several other measures have been proposed and evaluated in the literature. The input data used in class discovery analyses are commonly based on lists of genes having high informativeness (low noise) based on low values of the coefficient of variation or high values of Shannon entropy, etc. The determination of the most likely or optimal number of clusters obtained from an unsupervised analysis is called cluster validity. Some commonly used metrics for cluster validity are the silhouette index, Davies-Bouldin index, Dunn's index, or Hubert's [...] statistic.|$|E
5000|$|PSPP {{contains}} <b>k-means,</b> The QUICK CLUSTER command performs <b>k-means</b> clustering on the dataset.|$|E
40|$|Partitioning a given set {{of points}} into {{clusters}} is a well known problem in pattern recognition, data mining, and knowledge discovery. One of the well known methods for identifying clusters in Euclidean space is the <b>K-mean</b> algorithm. In using the <b>K-mean</b> clustering algorithm {{it is necessary to}} know the value of k (the number of clusters) in advance. We propose to develop algorithms for good estimation of k for points distributed in two dimensions. The techniques we pursue include a bucketing method, g-hop neighbors, and Voronoi diagrams. We also present experimental results for examining the performances of the bucketing method and <b>K-mean</b> algorithm...|$|R
40|$|The {{classification}} and de-aliasing methods {{with respect}} to multi-spectra and hyper-spectra have been widely studied in recent years. And both <b>K-mean</b> clustering algorithm and spectral similarity algorithm are familiar classification methods. The present paper improved the <b>K-mean</b> clustering algorithm by using spectral similarity match algorithm to perform a new spectral classification algorithm. Two spectra with the farthest distance first were chosen as reference spectra. The Euclidean distance method or spectral angle cosine method then were used to classify data cube {{on the basis of}} the two reference spectra, and delete the spectra which belongs to the two reference spectra. The rest data cube was used to perform new classification according to a third spectrum, which is the farthest distance or the biggest angle one corresponding to the two reference spectra. Multi-spectral data cube was applied in the experimental test. The results of <b>K-mean</b> clustering classification by ENVI, compared with simulation results of the improved <b>K-mean</b> algorithm and the spectral angle cosine method, demonstrated that the latter two classify two air bubbles explicitly and effectively, and the improved <b>K-mean</b> algorithm classifies backgrounds better, especially the Euclidean distance method can classify the backgrounds integrally...|$|R
40|$|Abstract — In Data Mining, Clustering is an {{important}} research topic and wide range of unsupervised classification application. Clustering is technique which divides a data into meaningful groups. <b>K-mean</b> {{is one of the}} popular clustering algorithms. <b>K-mean</b> clustering is widely used to minimize squared distance between features values of two points reside in the same cluster. Particle swarm optimization is an evolutionary computation technique which finds optimum solution in many applications. Using the PSO optimized clustering results in the components, {{in order to get a}} more precise clustering efficiency. In this paper, we present the comparison of <b>K-mean</b> clustering and the Particle swarm optimization...|$|R
5000|$|... <b>k-means</b> clustering, a {{generalization}} for multivariate data (Jenks natural breaks optimization {{seems to}} be one dimensional <b>k-means).</b>|$|E
5000|$|Hierarchical <b>K-Means</b> is {{the method}} that adapted the <b>K-Means</b> {{algorithms}} {{to work with}} textual data and create a tag hierarchy in a top-down manner ...|$|E
5000|$|... <b>k-means</b> clustering, and its {{associated}} expectation-maximization algorithm, is a special case of a Gaussian mixture model, specifically, the limit of taking all covariances as diagonal, equal, and small. It is often easy to generalize a <b>k-means</b> problem into a Gaussian mixture model. [...] Another generalization of the <b>k-means</b> algorithm is the K-SVD algorithm, which estimates data points as a sparse linear combination of [...] "codebook vectors". <b>K-means</b> corresponds to the special case of using a single codebook vector, with a weight of 1.|$|E
40|$|<b>K-mean</b> {{method is}} a {{clustering}} method in which grouping techniques are {{based only on}} distance measure among observed objects, without considering statistical aspects. Model-based clustering is a method that use statistical aspects, as its theoretical basis i. e. probability maximum criterion. This model has several variations {{with a variety of}} geometrical characteristics obtained by mean Gauss component. Data partition is conducted by utilizing EM (expectation-maximization) algorithm. Then by using Bayesian Information Criterion (BIC) the best model is obtained. This research aimed to comparing result of grouping methods between model-based clustering and <b>K-mean</b> clustering. The results showed that model-based clustering was more effective in separating overlap groups than <b>K-mean...</b>|$|R
30|$|This paper mainly uses Gabor {{transform}} smoothing method, <b>K-mean</b> clustering method, and SVM classification method.|$|R
40|$|Data mining in {{educational}} field {{is a major}} application of data mining, it use machine learning to learn from data by studying algorithms and their constructions. In Data mining, clustering is the task of grouping aset of objects {{in such a way}} that objects in the same group are more similar to each other than to those in other groups. There are too many algorithms for clustering technique but <b>k-mean</b> algorithm is easy to interpret and understand so, in this paper clustering <b>K-mean</b> algorithm is discussed and applied on students data sets to find the gender wise performance in theory and practical subjects of computer science course. <b>K-mean</b> algorithm is implemented on student’s dataset using WEKA tool...|$|R
5000|$|In {{the early}} days of color quantization, the <b>k-means</b> {{clustering}} algorithm was deemed unsuitable because of its high computational requirements and sensitivity to initialization. In 2011, M. Emre Celebi reinvestigated the performance of <b>k-means</b> as a color quantizer [...] He demonstrated that an efficient implementation of <b>k-means</b> outperforms a large number of color quantization methods.|$|E
50|$|<b>K-means</b> {{clustering}} is an algorithm for grouping genes or samples {{based on}} pattern into K groups. Grouping {{is done by}} minimizing {{the sum of the}} squares of distances between the data and the corresponding cluster centroid. Thus the purpose of <b>K-means</b> clustering is to classify data based on similar expression. (www.biostat.ucsf.edu). <b>K-means</b> clustering algorithm and some of its variants (including k-medoids) have been shown to produce good results for gene expression data (at least better than hierarchical clustering methods). Empirical comparisons of <b>k-means,</b> k-medoids, hierarchical methods and, different distance measures {{can be found in the}} literature.|$|E
5000|$|... #Caption: <b>k-means</b> {{clustering}} and EM clustering on {{an artificial}} dataset ("mouse"). The tendency of <b>k-means</b> to produce equal-sized clusters leads to bad results, while EM {{benefits from the}} Gaussian distribution present in the data set ...|$|E
40|$|Clustering can be {{considered}} the most important unsupervised learning problem. It deals with finding a structure {{in a collection of}} unlabeled data. We defined cluster is process of organizing objects into group whose member are similar in some way. In my paper we taken natural image and we apply unsupervised learning algorithm <b>k-mean</b> and Fuzzy c-mean that solve the well known clustering problem. Keyword:-segmentation, <b>k-mean,</b> fuzzy c-mean 1...|$|R
30|$|The {{result of}} Fig.  8 {{shows that the}} {{recognition}} rate is improved by <b>K-mean</b> clustering, but the degree of improvement is different. The <b>K-mean</b> clustering feature has the strongest anti-interference ability to the expression feature, and the recognition effect reaches 91 %. The anti-interference ability of {{the light and the}} scar are poor. The recognition rate increased from 81 to 88 % and from 76 to 85 % respectively.|$|R
40|$|In {{software}} engineering testing {{plays an important}} role in development and maintenance of software. Component based software development gained a lot of practical importance in the field of Software engineering by the academic researcher and industry for finding reusable efficient test cases. It is the predominant problem in {{software engineering}} that clustering reduces the search space of the component of test cases by grouping of similar entities together ensuring reduce time complexity and reduce the search time for retrieve test cases according to requirement. In this research paper we investigate how <b>k-mean</b> work on the set of requirement and usable test cases we also define how to resolve the <b>k-mean</b> clustering static number of cluster when new requirement or test cases will come. In this research paper we investigate how <b>k-mean</b> work on the set of requirement and usable test cases we also define how to resolve the <b>k-mean</b> clustering static number of cluster when new requirement or test cases will come. Here we purposed an approach for dynamic clustering for test cases and requirement...|$|R
50|$|An Efficient <b>k-Means</b> Clustering Algorithm: Analysis and Implementation - In {{this paper}} {{they provide a}} simpler and more {{efficient}} implementation of Lloyd's Algorithm, which is used in <b>k-means</b> clustering, The algorithm is called the filtering algorithm.|$|E
5000|$|... {{that under}} {{sparsity}} assumptions and when input data is pre-processed with the whitening transformation <b>k-means</b> produces {{the solution to}} the linear Independent component analysis task. This aids in explaining the successful application of <b>k-means</b> to feature learning.|$|E
50|$|The <b>k-means</b> {{problem is}} to find cluster centers that {{minimize}} the intra-class variance, i.e. the sum of squared distances from each data point being clustered to its cluster center (the center that is closest to it).Although finding an exact solution to the <b>k-means</b> problem for arbitrary input is NP-hard, the standard approach to finding an approximate solution (often called Lloyd's algorithm or the <b>k-means</b> algorithm) is used widely and frequently finds reasonable solutions quickly.|$|E
40|$|Abstract — Clustering {{is the one}} of the {{efficient}} datamining techniques for intrusion detection. In clustering algorithm <b>k-mean</b> clustering is widely used for intrusion detection. Because it gives efficient results incase of huge datasets. But sometime <b>k-mean</b> clustering fails to give best result because of class dominance problem and no class problem. So for removing these problems we are proposing two new algorithms for cluster to class assignment. According to our experimental results the proposed algorithm are having high precision and recall for low class instances. Keywords- Feature selection, <b>k-mean</b> clustering, fuzzy k mean clustering, and KDDcup 99 dataset Introduction (Heading 1) Intrusion is the sequence of the set of related activity which perform unauthorized access to the useful information an...|$|R
40|$|This paper {{describes}} {{three different}} fundamental mathematical programming approaches {{that are relevant}} to data mining. They are: Feature Selection, Clustering and Robust Representation. This paper comprises of two clustering algorithms such as <b>K-mean</b> algorithm and K-median algorithms. Clustering is illustrated by the unsupervised learning of patterns and clusters that may exist in a given databases and useful tool for Knowledge Discovery in Database (KDD). The results of k-median algorithm are used to collecting the blood cancer patient from a medical database. <b>K-mean</b> clustering is a data mining/machine learning algorithm used to cluster observations into groups of related observations without any prior knowledge of those relationships. The <b>k-mean</b> algorithm is one of the simplest clustering techniques and it is commonly used in medical imaging, biometrics and related fields...|$|R
3000|$|A feature {{extraction}} method based on gray-gradient maximum entropy method and an image segmentation method using <b>K-mean</b> method is defined [...]...|$|R
