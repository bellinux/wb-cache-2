232|10000|Public
5000|$|<b>Distributed</b> <b>data</b> <b>processing</b> {{can be done}} in Python {{using the}} Python Programmable Filter. This filter {{functions}} seamlessly with NumPy and SciPy.|$|E
5000|$|Open systems architecture, in telecommunication, is a {{standard}} that describes the layered hierarchical structure, configuration, or model of a communications or <b>distributed</b> <b>data</b> <b>processing</b> system that: ...|$|E
50|$|Hadoop {{implements}} a <b>distributed</b> <b>data</b> <b>processing</b> {{scheduling and}} execution environment and framework for MapReduce jobs. Hadoop includes a distributed file system called HDFS which {{is analogous to}} GFS in the Google MapReduce implementation. The Hadoop execution environment supports additional <b>distributed</b> <b>data</b> <b>processing</b> capabilities which are designed to run using the Hadoop MapReduce architecture. These include HBase, a distributed column-oriented database which provides random access read/write capabilities; Hive which is a data warehouse system built on top of Hadoop that provides SQL-like query capabilities for data summarization, ad hoc queries, and analysis of large datasets; and Pig - a high-level data-flow programming language and execution framework for data-intensive computing.|$|E
30|$|Since {{entropy coding}} {{performs}} efficiently on statistical non-uniform <b>distributed</b> <b>data</b> additional <b>processing</b> steps modify {{the distribution of}} the data prior to encoding. Those steps are decorrelation and quantization steps.|$|R
3000|$|... scalable, <b>distributed</b> and {{fault-tolerant}} <b>data</b> <b>processing</b> capabilities, including temporary or even permanent storage; [...]...|$|R
30|$|The SBDS also {{seeks to}} serve as an {{analytical}} platform where government users can distribute massive spatial datasets across multiple computer machines and carry out spatial analyses against those distributed datasets in a concurrent manner. To provide a high-performance computing environment, the SBDS encompasses a Hadoop system for <b>distributed</b> spatial <b>data</b> <b>processing,</b> which is an extension of the spatial Hadoop open source platform [21]. This Hadoop system currently has eight basic spatial operations (e.g., distance, convex hull, clip, spatial join) and six advanced ones (e.g., point density, location-allocation, hotspot detection) for <b>distributed</b> spatial <b>data</b> <b>processing.</b> The SBDS complies with the conditions of an open source platform, but the lack of open standards for big spatial data systems limits its openness in terms of standards.|$|R
5000|$|Although its successor's role in <b>distributed</b> <b>data</b> <b>processing</b> {{was said}} to be [...] "a turning point in the general {{direction}} of worldwide computer development," [...] the 3790 was described by Datamation in March 1979 as [...] "less than successful.".|$|E
50|$|CDH {{contains}} the main, core elements of Hadoop that provide reliable, scalable <b>distributed</b> <b>data</b> <b>processing</b> of large data sets (chiefly MapReduce and HDFS), {{as well as}} other enterprise-oriented components that provide security, high availability, and integration with hardware and other software.|$|E
50|$|VOD handles <b>{{distributed}}</b> <b>data</b> <b>processing</b> using a distributed two-phase commit protocol across multiply connected databases. In this process, VOD uses {{an internal}} resource manager that {{is handling the}} distributed transactions. Versant also supports the XA protocol allowing external transaction monitors to control the transactional context, so for example plug into a CORBA or J2EE application server.|$|E
5000|$|WebAGRIS is a {{multilingual}} Web-based {{system for}} <b>distributed</b> <b>data</b> input, <b>processing</b> and dissemination (through the Internet or on CD-Rom), of agricultural bibliographic information. It {{is based on}} common standards of data input and dissemination formats (XML, HTML, ISO2709), as well as subject categorization schema and AGROVOC.|$|R
40|$|TACO (Topologies and Collections) is a {{template}} library that introduces the flavour of <b>distributed</b> <b>data</b> parallel <b>processing</b> {{by means of}} reusable topology classes and C++ templates. This paper introduces TACO's basic abstractions and provides a performance analysis for basic collective operations on various cluster architectures with several different networks...|$|R
30|$|Yoon-Ki Kim is {{currently}} working toward the Ph. D. degree in Electronic and Computer Engineering at the Korea University. His research interests include high -performance computing, real-time <b>distributed,</b> and parallel <b>data</b> <b>processing</b> for IoT, Sensor <b>data</b> <b>processing.</b>|$|R
50|$|SyFa (Systems for Access) {{manufactured}} programmable <b>distributed</b> <b>data</b> <b>processing</b> systems {{using the}} LSI 2/60 {{and later the}} 2/120 as the core. These were used by many companies to perform jobs such as stock control, order processing, etc. Originally the systems were manufactured and assembled in the States and shipped to the UK for commissioning, but by the late Seventies a production facility was in place at a separate unit at Maple Cross near Rickmansworth in England.|$|E
5000|$|Modern {{scalable}} / {{high performance}} data persistence technologies rely on massively parallel <b>distributed</b> <b>data</b> <b>processing</b> across many commodity computers {{on a high}} bandwidth network. An example of one is Apache Hadoop. In such systems, the data is distributed across multiple computers and therefore any particular computer in the system must be represented in the key of the data, either directly, or indirectly. This enables the differentiation between two identical sets of data, each being processed on a different computer at the same time.|$|E
50|$|Other Datapoint inventions were ARCnet, {{invented in}} 1977, {{which was an}} early token-passing local area network (LAN) protocol, and the PL/B {{high-level}} programming language, which was originally called Databus (from Datapoint business language) and ran under the Datashare multi-user interpreter. Later developments included a Mapped Intelligent Disc System (MIDS) which networked 2200 series terminals to a single mass storage disc operating system and enhanced <b>Distributed</b> <b>Data</b> <b>Processing.</b> Proprietary operating systems included DOS and RMS, and Datapoint later moved its hardware {{to be based on}} Intel 386 CPUs.|$|E
50|$|RDM's {{database}} union feature {{provides a}} unified view of multiple identically structured databases. Since RDM allows highly <b>distributed</b> <b>data</b> storage and <b>processing,</b> this feature provides {{a mechanism for}} unifying the <b>distributed</b> <b>data,</b> giving it {{the appearance of a}} single, large database.|$|R
40|$|Abstract. With the {{continued}} advancements in location-based services involved infrastructures, {{large amount of}} time-based location data are quickly accumulated. Distributed processing techniques on such large trajectory data sets are urgently needed. We propose TRUSTER: a <b>distributed</b> trajectory <b>data</b> <b>processing</b> system on clusters. TRUSTER employs a distributed indexing method on large scale trajectory data sets, and it makes spatio-temporal queries execute efficiently on clusters. ...|$|R
50|$|Sector/Sphere is an {{open source}} {{software}} suite for high-performance <b>distributed</b> <b>data</b> storage and <b>processing.</b> It can be broadly compared to Google's GFS and MapReduce technology. Sector is a distributed file system targeting data storage over {{a large number of}} commodity computers. Sphere is the programming architecture framework that supports in-storage parallel <b>data</b> <b>processing</b> for <b>data</b> stored in Sector. Sector/Sphere operates in a wide area network (WAN) setting.|$|R
5000|$|In June 2010, Google {{rolled out}} a {{next-generation}} indexing and serving system called [...] "Caffeine" [...] which can continuously crawl and update the search index. Previously, Google updated its search index in batches using {{a series of}} MapReduce jobs. The index was separated into several layers, {{some of which were}} updated faster than the others, and the main layer wouldn't be updated for as long as two weeks. With Caffeine the entire index is updated incrementally on a continuous basis. Later Google revealed a <b>distributed</b> <b>data</b> <b>processing</b> system called [...] "Percolator" [...] which {{is said to be the}} basis of Caffeine indexing system.|$|E
5000|$|Meta{{database}} is {{a database}} model for (1) metadata management, (2) global query of independent databases, and (3) <b>distributed</b> <b>data</b> <b>processing.</b> The word metadatabase is {{an addition to}} the dictionary. Originally, metadata was only a common term referring simply to [...] "data about data", such as tags, keywords, and markup headers. However, in this technology, the concept of metadata is extended to also include such data and knowledge representation as information models (e.g., relations, entities-relationships, and objects), application logic (e.g., production rules), and analytic models (e.g., simulation, optimization, and mathematical algorithms). In the case of analytic models, it is {{also referred to as}} a Modelbase.|$|E
5000|$|It is a Linux OS {{multi-thread}} daemon {{application that}} implements business-logic functionality of tasks management {{that uses the}} DRCE Cluster Execution Environment to manage tasks as remote processes. It implements general management operations for distributed tasks scheduling, execution, state check, OS resources monitoring and so on. This application {{can be used for}} parallel tasks execution with state monitoring on hierarchical network cluster infrastructure with custom nodes connection schema. It is multipurpose application aimed to cover needs of projects with big data computations, <b>distributed</b> <b>data</b> <b>processing,</b> multi-host data processing with OS system resources balancing, limitations and so on. It supports several balancing modes including multicast, random, round-robin and system resource usage algorithms. Also, provides high level state check, statistics and diagnostic automation based on natural hierarchy and relations between nodes. Supports messages routing as a tasks and data balancing method or a tasks management.---- ...|$|E
40|$|Context {{processing}} {{refers to}} the operation of processing different types of context data and/or information using different kinds of operators. These operators are applied according to some conditions or constraints given in context queries. Existing context aware systems process context data in a centralized fashion to answer context queries and generate context information. However, this method can cause scalability issues and give poor system throughput. In this paper, we aim {{to address this issue}} by proposing a <b>distributed</b> context <b>data</b> <b>processing</b> mechanism in which the context <b>data</b> <b>processing</b> computations of different context queries will be distributed to different computing devices. Relying on the developed prototype, a performance evaluation was conducted with centralized context <b>data</b> <b>processing</b> method as benchmark...|$|R
40|$|<b>Distributing</b> <b>data</b> <b>processing</b> (DP) costs {{consists}} of estimating {{the value of}} the services provided by the DP department for each user department and transferring the costs to that department. An allocation system results in more realistic demands for services and better use of the services and provides a way of measuring and monitoring resources. There seems to be a consensus that the DP department should be a profit center and charge market prices for its services. To investigate the extent to which DP costing systems were being used, a questionnaire was mailed to 100 of the largest savings 2 ̆ 6 loan associations in the US. A small percentage of the respondents used charge-back systems, primarily because the system lacked the ability to capture the information. In designing a charge-back system, it is recommended that the user accept bids from the DP department and any outside vendors. If the variance in price is favorable to the DP 2 ̆ 7 s budgets, the users and the company will have spent the money wisely...|$|R
40|$|Abstract. Higher {{education}} {{resources are}} considered {{in this paper}} as a complex heterogeneous hierarchical system. The formal mathematical models for resources evaluation are suggested. The analysis of {{information and communication technologies}} applied in higher education establishments is conducted. The necessity of distributed hardware and software infrastructure for <b>data</b> storage and <b>processing</b> in the evaluation activities is shown and substantiated. The <b>distributed</b> <b>data</b> storage and <b>processing</b> architecture is presented...|$|R
5000|$|Hadoop is an {{open source}} {{software}} project sponsored by The Apache Software Foundation (http://www.apache.org) which implements the MapReduce architecture. The Hadoop execution environment supports additional <b>distributed</b> <b>data</b> <b>processing</b> capabilities which are designed to run using the Hadoop MapReduce architecture. These include Pig - a high-level data-flow programming language and execution framework for data-intensive computing. Pig was developed at Yahoo! to provide a specific data-centric language notation for data analysis applications and to improve programmer productivity and reduce development cycles when using the Hadoop MapReduce environment. Pig programs are automatically translated into sequences of MapReduce programs if needed in the execution environment. Pig provides capabilities in the language for loading, storing, filtering, grouping, de-duplication, ordering, sorting, aggregation, and joining operations on the data. [...] Figure 1 shows a sample Pig program and Figure 2 shows how this is translated {{into a series of}} MapReduce operations.|$|E
40|$|<b>Distributed</b> <b>data</b> <b>processing</b> is {{becoming}} a reality. Businesses {{want to do it}} for many reasons, and they often must do it in order to stay competitive. While much of the infrastructure for <b>distributed</b> <b>data</b> <b>processing</b> is already there (e. g., modern network technology), a number of issues make <b>distributed</b> <b>data</b> <b>processing</b> still a complex undertaking: (1) distributed systems can become very large, involving thousands of heterogeneous sites including PCs and mainframe server machines; (2) the state of a distributed system changes rapidly because the load of sites varies over time and new sites are added to the system; (3) legacy systems need to be integrated—such legacy systems usually have not been designed for <b>distributed</b> <b>data</b> <b>processing</b> and now need to interact with other (modern) systems in a distributed environment. This paper presents {{the state of the art}} of query processing for distributed database and information systems. The paper presents the “textbook ” architecture for distributed query processing and a series of techniques that are particularly useful for distributed database systems. These techniques include special join techniques, techniques to exploit intraquery parallelism, techniques to reduce communication costs, and techniques to exploit caching and replication of data. Furthermore, the paper discusses different kinds of distributed systems such as client-server, middleware (multitier), and heterogeneous database systems, and shows how query processing works in these systems...|$|E
40|$|Abstract—Recently, the {{computational}} {{requirements for}} largescale data-intensive analysis of scientific data have grown significantly. In High Energy Physics (HEP) for example, the Large Hadron Collider (LHC) produced 13 petabytes of data in 2010. This {{huge amount of}} data are processed on more than 140 computing centers distributed across 34 countries. The MapReduce paradigm {{has emerged as a}} highly successful programming model for large-scale data-intensive computing applications. However, current MapReduce implementations are developed to operate on single cluster environments and cannot be leveraged for large-scale <b>distributed</b> <b>data</b> <b>processing</b> across multiple clusters. On the other hand, workflow systems are used for <b>distributed</b> <b>data</b> <b>processing</b> across data centers. It has been reported that the workflow paradigm has some limitations for <b>distributed</b> <b>data</b> <b>processing,</b> such as reliability and efficiency. In this paper, we present the design and implementation of G-Hadoop, a MapReduce framework that aims to enable large-scale distributed computing across multiple clusters. G-Hadoop uses the Gfarm file system as an underlying file system and executes MapReduce tasks across distributed clusters. Experiments of the G-Hadoop framework on distributed clusters show encouraging results...|$|E
40|$|N������ � ��, ���� �hile {{distributed}} query processing {{has many}} advantages, {{the use of}} many independent, physically widespread computers almost universally leads to reliability issues. Several techniques {{have been developed to}} provide redundancy and the ability to recover from node failure during query processing. In this survey, we examine three techniques�u�stre�m ��c�u�, �ct�ve st�n���, and ��ss�ve st�n����that have been used in both <b>distributed</b> stream <b>data</b> <b>processing</b> and the <b>distributed</b> <b>processing</b> of static <b>data.</b> We also compare several recent systems that use these techniques, and explore which recovery techniques work well under various conditions...|$|R
40|$|Abstract. Most of {{existing}} <b>data</b> <b>processing</b> approaches of {{wireless sensor networks}} are real-time. However, historical data of wireless sensor networks are also significant for various applications. No previous study has specifically addressed <b>distributed</b> historical <b>data</b> query <b>processing.</b> In this paper, we propose an Index based Historical <b>Data</b> Query <b>Processing</b> scheme which stores historical data locally and processes queries energyefficiently by using a distributed index tree. The simulation study shows that our scheme achieves good performance on both query responding delay and network traffic...|$|R
40|$|International audienceBig {{data has}} {{revealed}} {{itself as a}} powerful tool for many sectors ranging from science to business. Distributed data-parallel computing is then common nowadays: using a large number of computing and storage resources makes possible <b>data</b> <b>processing</b> of a yet unknown scale. But to develop large-scale <b>distributed</b> big <b>data</b> <b>processing,</b> one have to tackle many challenges. One of the most complex is scheduling. As it is known to be an optimal online scheduling policy when it comes to minimize the average flowtime, Shortest Processing Time First (SPT) is a classic scheduling policy used in many systems. We then decided to integrate this policy into Hadoop, a framework for big <b>data</b> <b>processing,</b> and realize an implementation prototype. This paper describes this integration, as well as tests results obtained on our testbed...|$|R
40|$|Abstract: Recently, the {{computational}} {{requirements for}} large-scale data-intensive analysis of scientific data have grown significantly. In High Energy Physics (HEP) for example, the Large Hadron Collider (LHC) produced 13 petabytes of data in 2010. This {{huge amount of}} data are processed on more than 140 computing centers distributed across 34 countries. The MapReduce paradigm {{has emerged as a}} highly successful programming model for large-scale data-intensive computing applications. However, current MapReduce implementations are developed to operate on single cluster environments and cannot be leveraged for large-scale <b>distributed</b> <b>data</b> <b>processing</b> across multiple clusters. On the other hand, workflow systems are used for <b>distributed</b> <b>data</b> <b>processing</b> across data centers. It has been reported that the workflow paradigm has some limitations for <b>distributed</b> <b>data</b> <b>processing,</b> such as reliability and efficiency. In this paper, we present the design and implementation of G-Hadoop, a MapReduce framework that aims to enable large-scale distributed computing across multiple clusters. G-Hadoop uses the Gfarm file system as an underlying file system and executes MapReduce tasks across distributed clusters...|$|E
40|$|I * Approved {{for public}} release, {{distribution}} unlimited. 17. DISTRIBUTION STATEMENT (of the abstract entered In Block 20. It different from Report) IS. SUPPLEMENTARY NOTES 19. KEY WORDS (Continue on revere. side if neceeesry and identify by block number) <b>Distributed</b> <b>data</b> <b>processing</b> Local area network Microcomputer Management control [...] 20. ABSTRACT (Continue on reverse side If necesey and Identify by block number) [...] This paper discusses {{the possibility of}} implementing a Local Area Network (LAN) within the Marine Corps Infantry Battalion. The idea of a LAN is proposed as an automated alternative to the status quo. As a mechanism of <b>Distributed</b> <b>Data</b> <b>Processing</b> (DDP), the LAN is used to highlight one possible migration path along which the Bdttalion Consolidate...|$|E
40|$|<b>Distributed</b> <b>data</b> <b>processing</b> {{platforms}} such as MapReduce and Pregel have substantially simplified {{the design}} and de-ployment of certain classes of distributed graph analytics al-gorithms. However, these platforms do not represent a good match for distributed graph mining problems, as for exam-ple finding frequent subgraphs in a graph. Given an input graph, these problems require exploring {{a very large number}} of subgraphs and finding patterns that match some “interest-ingness ” criteria desired by the user. These algorithms are very important for areas such as social networks, semantic web, and bioinformatics. In this paper, we present Arabesque, the first <b>distributed</b> <b>data</b> <b>processing</b> platform for implementing graph mining algorithms. Arabesque automates the process of explorin...|$|E
40|$|In this paper, a {{distributed}} system storing and retrieving Broad- cast News data recorded from the Greek television is presented. These multimodal data are processed in a grid computational en- vironment interconnecting <b>distributed</b> <b>data</b> storage and <b>processing</b> subsystems. The innovative element {{of this system}} is the implemen- tation of the signal processing algorithms in this grid environment, offering additional flexibilit...|$|R
40|$|Abstract —In this paper, we {{describe}} {{the implementation of a}} <b>distributed</b> <b>data</b> retrieval and <b>processing</b> strategy enabled by using grid computing technologies and applied to distributed collaborative adaptive sensing environments. Underlying the grid computing and storage infrastructure there is a data dispersion algorithm to guarantee pervasive data management. Experimental results show that the proposed framework integrates successfully radar networks to grid infrastructures, while providing higher resources utilization than typical storage strategies. Index Terms — Distributed radar network, grid computing, <b>data</b> retrieval and <b>processing...</b>|$|R
40|$|This paper {{describes}} {{the use of}} Storm at Twitter. Storm is a real-time fault-tolerant and <b>distributed</b> stream <b>data</b> <b>processing</b> system. Storm is currently being used to run various critical computations in Twitter at scale, and in real-time. This paper {{describes the}} architecture of Storm and its methods for distributed scale-out and fault-tolerance. This paper also describes how queries (aka. topologies) are executed in Storm, and presents some operational stories based on running Storm at Twitter. We also present results from an empirical evaluation demonstrating the resilience of Storm in dealing with machine failures. Storm is under active development at Twitter and we also present some potential directions for future work. 1...|$|R
