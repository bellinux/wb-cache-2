75|175|Public
50|$|Dynamic analyzer: Adds code to {{a program}} to {{count the number of}} times each {{statement}} has been executed. It generates an <b>execution</b> <b>profile</b> for the statements to show the number of times they are executed in the program run.|$|E
50|$|Adaptive {{optimizing}} is {{a method}} in computer science that performs dynamic recompilation of parts of a program based on the current <b>execution</b> <b>profile.</b> With a simple implementation, an adaptive optimizer may simply make a trade-off between just-in-time compiling and interpreting instructions. At another level, adaptive optimizing may exploit local data conditions to optimize away branches and use inline expansion.|$|E
50|$|Adaptive {{optimization}} is {{a technique}} in computer science that performs dynamic recompilation of portions of a program based on the current <b>execution</b> <b>profile.</b> With a simple implementation, an adaptive optimizer may simply make a trade-off between just-in-time compilation and interpreting instructions. At another level, adaptive optimization may take advantage of local data conditions to optimize away branches and to use inline expansion to decrease the cost of procedure calls.|$|E
5000|$|Compiler modules - SML/NJ also {{includes}} a structure that provides control of the ML compiler, which contains substructures for <b>execution</b> <b>profiling,</b> control of compiler error-message printing and warnings, and customizable pretty printing.|$|R
40|$|Abstract: Regression test {{selection}} is to select and execute some test cases {{from a large}} test suite, such that the change part of software is verified. Cluster test selection approaches use historical <b>execution</b> <b>profiles</b> and clustering algorithms for test selection. Cluster test selection can {{reduce the cost of}} testing and preserve strong fault detection capability. In this paper, we design and implement an empirical study on cluster test selection approaches. The empirical study shows that different clustering algorithms will affect the results of test selection significantly, and different <b>execution</b> <b>profiles</b> will partially affect the results of test selection...|$|R
50|$|Scout is the {{successor}} {{of the code}} profiler introduced in Adobe Flash Builder. Scout was released in January 2013, and provided memory and code <b>execution</b> <b>profiling.</b> Stage3D support was added c. June 2013, along with an integrated Stage3D rendering preview and draw-call recording and replay toolset.|$|R
50|$|V8 compiles JavaScript {{directly}} to native machine code before executing it, instead of more traditional {{techniques such as}} interpreting bytecode or compiling the whole program to machine code and executing it from a filesystem. The compiled code is additionally optimized (and re-optimized) dynamically at runtime, based on heuristics of the code's <b>execution</b> <b>profile.</b> Optimization techniques used include inlining, elision of expensive runtime properties, and inline caching, among many others.|$|E
40|$|We propose an integratedfront-end/back-end flow for the {{automatic}} generation of a multi-bank memory architecture for embedded systems. The flow {{is based on}} an algorithm for {{the automatic}} partitioning of on-chip SRAM. Starting from the dynamic <b>execution</b> <b>profile</b> of an embedded application running on a given processor core, we synthesize a multi-bankedSRAMarchitecture optimally fitted to the <b>execution</b> <b>profile...</b>|$|E
40|$|We propose an {{integrated}} front-end/back-end flow for the automatic generation of a multi-bank memory architecture for embedded systems. The flow {{is based on}} an algorithm for the automatic partitioning of on-chip SRAM. Starting from the dynamic <b>execution</b> <b>profile</b> of an embedded application running on a given processor core, we synthesize a multi-banked SRAM architecture optimally fitted to the <b>execution</b> <b>profile.</b> The partitionin...|$|E
40|$|Many {{testing and}} {{analysis}} techniques {{have been developed}} for inhouse use. Although they are effective at discovering defects before a program is deployed, these techniques are often limited due to the complexity of real-world code and thus miss program faults. It will be the users of the program who eventually experience failures caused by the undetected faults. To take advantage of the large number of program runs carried by the users, recent work has proposed techniques to collect <b>execution</b> <b>profiles</b> from the users for developers to perform post-deployment failure analysis. However, in order to protect users 2 ̆ 7 privacy and to reduce run-time overhead, such profiles are usually not detailed enough for the developers to identify or fix the root causes of the failures. In this paper, we propose a novel approach to utilize user <b>execution</b> <b>profiles</b> for more effective in-house testing and analysis. Our key insight is that <b>execution</b> <b>profiles</b> for program failures can be used to simplify a program, while preserving its erroneous behavior. By simplifying a program and scaling down its complexity according to its profiles, in-house testing and analysis techniques can be performed more accurately and efficiently, and pragmatically program defects that occur more often and are (arguably) more relevant to users will be given preference during failure analysis. Specifically, we adapt statistical debugging on <b>execution</b> <b>profiles</b> to predict likely failure-related code and use a syntax-directed algorithm to trim failure-irrelevant code from a program, while preserving its erroneous behavior as much as possible. We conducted case studies on a testing engine, CUTE, and a software model checker, BLAST, to evaluate our technique. We used subject programs from the Aristotle Analysis System and the Software-artifact Infrastructure Repository (SIR). Our empirical results show that using simplified programs, CUTE and BLAST find more bugs with improved accuracy and performance: they were able to detect 20 and 21 (out of 139) more bugs respectively in about half of the time as they took on the original test programs...|$|R
40|$|Part 3 : Measurements, Testing, and Quality of SoftwareInternational audienceThe aim {{has been}} to {{minimize}} regression test suites while retaining fault detection capability of the test suite admissible. An appropriate minimized test suite should exercise different execution paths within a program. However, minimization of test suites may result in significant fault detection loss. To alleviate the loss, a new bi-criteria heuristic algorithm, using cluster analysis of test cases <b>execution</b> <b>profiles</b> is proposed in this paper. Cluster analysis of <b>execution</b> <b>profiles</b> categorizes test cases according to their similarity in terms of exercising a certain coverage criterion. Considering additional coverage criteria the proposed algorithm samples some test cases from each cluster. These additional criteria exercise execution paths, different from those covered by the main testing criteria. Experiments on the Siemens suite manifest {{the applicability of the}} proposed approach and present interesting insights into the use of cluster analysis to the bi-criteria test suite reduction...|$|R
40|$|Spectrum-based Fault Localization (SBFL) is a {{well-known}} debugging technique that locates fault in program code by utilizing <b>execution</b> <b>profiles</b> (spectra) of pass and fail test cases. Hence, the performance of SBFL depends on the test cases executed and the test results. In the most extreme scenarios, the debugging process {{may have to be}} conducted with only one fail test case, one pass test case or no pass test case. These scenarios might occur due to extremely high or extremely low failure rates or when software testers decide to stop running more test cases due to time and resource constraints. However, limited test case <b>execution</b> <b>profiles</b> may reduce the accuracy of SBFL metrics. In view of this, we evaluate the performance of SBFL metrics in these extreme scenarios to identify the best performing SBFL metric for each of these scenarios. From the experiment results, we have further discovered the convergence in performance for SBFL metrics under these extreme scenarios...|$|R
40|$|TECHNIQUES FOR THE <b>EXECUTION</b> <b>PROFILE</b> ANALYSIS AND OPTIMIZATION OF COMPUTATIONAL CHEMISTRY PROGRAMS, In {{this paper}} we review the basic {{techniques}} of performance analysis within the UNIX environment {{that are relevant}} in computational chemistry, with particular emphasis on the <b>execution</b> <b>profile</b> using the gprof tool. Two case studies (in ab initio and molecular dynamics calculations) are presented in order to illustrate how execution profiling {{can be used to}} effectively identify bottlenecks and to guide source code optimization. Using these profiling and optimization techniques it was possible to obtain significant speedups (of up to 30 %) in both cases...|$|E
40|$|We {{present a}} {{software}} architecture-based approach to compositional estimation of system’s reliability. Our approach is applicable to {{early stages of}} development when the implementation artifacts are not yet available, and exact <b>execution</b> <b>profile</b> is unknown. The uncertainty of the <b>execution</b> <b>profile</b> is modeled using stochastic processes with unknown parameters. The compositional approach calculates overall reliability of {{the system as a}} function of the reliability of its constituent components and their (complex) interactions. Sensitivity analysis to identify critical components and interactions will be provided. 1. Research problem and importance Reliability is defined as the probability that a system will perform its intended functionality under specifie...|$|E
40|$|This paper {{studies the}} idle time of {{processors}} during {{the execution of}} message-passing parallel programs. Detailed analysis reveals that, besides the well-known load imbalances, blocking overhead can be generated by {{any part of the}} <b>execution</b> <b>profile,</b> like communication overhead, message delays or partitioning. We investigate...|$|E
40|$|Spectrum-based Fault Localization (SBFL) {{has been}} widely studied as a {{debugging}} technique to reduce time and effort in locating faulty code in software. In SBFL, <b>execution</b> <b>profiles</b> (spectra) of pass and fail test cases are analyzed with SBFL metric to rank software code according to their likeliness to be faulty. However, there are significantly more pass test cases than fail test cases in typical test suites for faulty programs. The domination of pass test cases creates imbalance test suites that {{have a negative impact}} on the performance of SBFL. This is attributed to the fact that the <b>execution</b> <b>profiles</b> of fail test cases provide the essential information on the location of faulty code. In this paper, we propose to clone the spectra of fail test cases beyond balanced test suite to improve the performance of SBFL. Our empirical study shows that the proposed cloning method significantly improves the performance of commonly used SBFL metrics. Furthermore, we attempt to identify the amount of cloning required to achieve the optimal performance for the proposed cloning method...|$|R
40|$|An <b>execution</b> <b>profiling</b> {{attempts}} to provide feedback by {{reporting to the}} programmer information about inefficiencies within the program Instead of writing whole code highly optimized, the programmer can initially write simple maintainable code without much concern for efficiency. Profiling is an effective tool for finding hot spots in a program or sections of code that consumes most of the computing time and space. The paper presents already implemented execution profiler for process functional program. From the viewpoint of implementation, process functional language is between an impure eager functional language and a monadic lazy pure functional language. The key problem of <b>execution</b> <b>profiling</b> is to relate gathered information about an execution of the program back to the source code in well defined manner. The paper defines language constructs for monitoring resource utilization during the program execution. In our solution programmer can associate label with each expression in a program. All resources used during the evaluation of a labeled expression are mapped to the specified label. The paper is concerned with formal profiling model. Research results are presented on sample program illustrating different types {{of time and space}} profiles generated by already implemented profiler for process functional programs...|$|R
40|$|In {{this paper}} a dynamic {{approach}} {{to measure the}} coupling of software systems is proposed. The conventionally used static measures are only limited suitable for the evaluation and characterization of such systems. We extend the static methodology with a dynamic component and define new measures based on this approach. A model for system characterization based on varying <b>execution</b> <b>profiles</b> is given. The layout of an automated monitor system is presented, which can be directly integrated into a software development system. 1...|$|R
40|$|Execution {{profiles}} {{are important}} aid {{in analyzing the}} performance of computer programs on a given computer system. However, accurate and complete profiles are difficult to arrive atfor programs that follow the client-server model of computing, which is followed by programs in the popular X Window system. In such applications, considerable computation is invoked at the display-server and this computation {{is an important part}} of the overall <b>execution</b> <b>profile.</b> The profiler presented in this paper generates meaningful profiles for X Window applications by estimating the time spent in servicing the request messages in the display-server. The central idea is to analyze a protocol-level trace of the interaction between the application and the display-server and thereby construct an <b>execution</b> <b>profile</b> from the trace and a set of supplied metrics about the target display-server...|$|E
40|$|Memory-processor {{integration}} offers {{new opportunities}} for reducing the energy of a system. In the case of embedded systems, one solution consists of mapping the most frequently accessed addresses onto the on-chip SRAM to guarantee power and performance efficiency. This option is especially effective when memory access patterns can be profiled and studied at design time (as in typical real-time embedded systems). In this work, we propose an algorithm for the automatic partitioning of on-chip SRAM in multiple banks that can be independently accessed. Starting from the dynamic <b>execution</b> <b>profile</b> of an embedded application running on a given processor core, we synthesize a multi-banked SRAM architecture optimally fitted to the <b>execution</b> <b>profile.</b> The algorithm provides a globally optimum {{solution to the problem}} under realistic assumptions on the power cost metrics, and with constraints on the number of memory banks. Results, collected on a set of embedded applications for the ARM processor, have shown average energy savings around 42 %...|$|E
40|$|Memory-processor {{integration}} offers {{new opportunities}} for reducing the energy of a system. In the case of embedded systems, where memory access patterns can typically be profiled at design time, one solution consists of mapping the most frequently accessed addresses onto the on-chip SRAM to guarantee power and performance efficiency. In this work, we propose an algorithm for the automatic partitioning of on-chip SRAMs into multiple banks. Starting from the dynamic <b>execution</b> <b>profile</b> of an embedded application running on a given processor core, we synthesize a multi-banked SRAM architecture optimially fitted to the <b>execution</b> <b>profile.</b> The algortithm computes an optimal {{solution to the problem}} under realistic assumptions on the power cost metrics, and with constraints on the number of memory banks. The partitioning algorithm is integrated with the physical design phase into a complete flow that allows the back annotation of layout information to drive the partitioning process. Results, collected on a set of embedded applications for the ARM processor, have shown average energy savings around 34 %...|$|E
40|$|This paper {{examines}} common {{assumptions about}} userspecific profiling in profile-based optimization. We study <b>execution</b> <b>profiles</b> of interactive applications on Windows NT {{to understand how}} different users use the same program. The profiles were generated by the DIGITAL FX! 32 emulator/binary translator system, which automatically runs the x 86 version of Windows NT programs on NT/Alpha computers. We have found that people use the benchmark programs in different ways. These differences in program usage can have impact {{on the performance of}} profile-based FX! 32 program translation and optimization, up to 9 %. 1...|$|R
40|$|Regression tests {{ensure that}} {{software}} systems retain correctness in their existing functionalities while the systems evolve. During development, regression tests allow the programmer to quickly identify problems. If {{there are any}} new failed tests, since those failures surfaced after new changes to the code base, we need to identify where this failure originated in our code changes. In this paper, we combine regression test results with changes in both the source code and test <b>execution</b> <b>profiles</b> to narrow down and identify the changes that may have caused test failures...|$|R
40|$|Abstract — Malware authors {{attempt in}} an endless effort to find new methods to evade the malware {{detection}} engines. A popular method {{is the use of}} obfuscation technologies that change the syntax of malicious code while preserving the execution semantics. This leads to the evasion of signatures that are built based on the code syntax. In this paper, we propose a novel approach to develop an evasion-resistant malware signature. This signature is based on the malware’s <b>execution</b> <b>profiles</b> extracted from kernel data structure objects and neither uses malicious code syntax specific information code execution flow information. Thus, proposed signature is more resistant to obfuscation methods and resilient in detecting malicious code variants. To evaluate the effectiveness of the proposed approach, a prototype signature generation tool called SigGENE is developed. The effectiveness of signatures generated by SigGENE evaluated using an experimental root kit-simulation tool that employs techniques commonly found in rootkits. This simulation-tool is obfuscated using several different methods. In further experiments, real-world malware samples that have different variants with the same behavior used to verify the real-world applicability of the approach. The experiments show that the proposed approach is effective, not only in generating a signature that detects the malware and its variants and defeats different obfuscation methods, but also, in producing an <b>execution</b> <b>profiles</b> {{that can be used to}} characterize different malicious attacks...|$|R
40|$|Over 30 {{years of}} {{research}} have gone into software reliability engineering during testing. With today’s complex software systems however, reliability has to be built into the early phases of development, including architectural design. We present a software architecture-based approach to estimating component reliability. Our stochastic reliability model is applicable to early stages of development when the implementation artifacts are unavailable and the exact <b>execution</b> <b>profile</b> is unknown. 1...|$|E
40|$|UnrestrictedModeling and {{estimating}} {{software reliability}} during testing {{is useful in}} quantifying {{the quality of the}} software systems. However, such measurements applied late in the development process leave too little to be done to improve the quality and dependability of the software system in a cost-effective way. Reliability, an important dependability attribute, is defined as the probability that the system performs its intended functionality under specified design limits. We argue that reliability models must be built to predict the system reliability throughout the development process, and specifically when exact context and <b>execution</b> <b>profile</b> of the system is unknown, or when the implementation artifacts are unavailable. In the context of software architectures, various techniques for modeling software systems and specifying their functionality have been developed. These techniques enable extensive analysis of the specification, but typically lack quantification. Additionally, their relation to dependability attributes of the modeled software system is unknown.; In this dissertation, we present a software architecture-based approach to predicting reliability. The approach is applicable to early stages of development when the implementation artifacts are not yet available, and exact <b>execution</b> <b>profile</b> is unknown. The approach is two fold: first, the reliability of individual components is predicted via a stochastic reliability model built using software architectural artifacts. The uncertainty associated with the <b>execution</b> <b>profile</b> is modeled using Hidden Markov Models, which enable probabilistic modeling with unknown parameters. The overall system reliability is obtained compositionally {{as a function of the}} reliability of its constituent components, and their complex interactions. The interactions form a causal network that models how reliability at a specific time in a system's execution is affected by the reliability at previous time steps.; We evaluate our software architecture-based reliability modeling approach to demonstrate that reliability prediction of software systems architectures early during the development life-cycle is both possible and meaningful. The coverage of our architectural analyses, as well as our defect classification is evaluated empirically. The component-level and system-level reliability prediction methodology is evaluated using sensitivity, uncertainty, and complexity, and scalability analyses...|$|E
40|$|International audiencen {{this paper}} we review the basic {{techniques}} of performance analysis within the UNIX environment {{that are relevant}} in computational chemistry, with particular emphasis on the <b>execution</b> <b>profile</b> using the gprof tool. Two case studies (in ab initio and molecular dynamics calculations) are presented in order to illustrate how execution profiling {{can be used to}} effectively identify bottlenecks and to guide source code optimization. Using these profiling and optimization techniques it was possible to obtain significant speedups (of up to 30 %) in both cases...|$|E
40|$|The central {{aspect of}} {{implementing}} a {{functional programming language}} is the design of an evaluation model: an abstract view of how the underlying machine executes functional programs. To make informed decisions in designing an evaluation model, the implementation designer {{needs to have a}} good understanding of mechanisms constituting the available evaluation models and their relationship to functional programming and computer architecture. This thesis provides one way of arriving at that understanding through analyzing the execution behavior of a set of functional programs on implementations based on two contrasting evaluation models: an environment one and a reduction one. The essential difference between the environment and reduction evaluation models, is that the former centralizes the dynamic context 1 while the latter distributes it. The consequences of choosing either to centralize or distribute the dynamic context are studied using data on execution behavior of functional programs. In the experimental part of the thesis, environment and reduction evaluators are built and instrumented to generate <b>execution</b> <b>profiles,</b> which contain measures of computational resource usage. It is demonstrated how the <b>execution</b> <b>profiles</b> can be used to compute costs incurred by the two evaluators. ftn 1 In general, the dynamic context of a computation is that part of the overall context which changes during a computation and, therefore, reflects the progress of the computation...|$|R
40|$|The idea {{of failure}} {{clustering}} is to index all failures {{due to the}} same fault together. An effective failure clustering technique provides invaluable guidance to failure report analyses for both traditional applications and context-aware applications. Examples of these analyses include duplicate removal, failure prioritization, and patch suggestion. Underpinning an effective clustering technique is properly designed failure proximity. Failure proximity for conventional applications has been well studied. However, no such study has been conducted on context-aware applications. Existing failure proximity techniques commonly assume that the <b>execution</b> <b>profiles</b> of failing test runs due to different faults diverge. Unfortunately, this assumption does not necessarily hold for context-aware applications, which react to a continual data input stream capturing the contexts of their runtime environment. We observe {{that only a few}} segments of the input stream are failure-inducing. Applying failure proximity techniques to the whole input streams that lead to failing test runs do not often result in significantly different <b>execution</b> <b>profiles</b> when these test runs trigger different faults. In this thesis, we propose to perform failure clustering by applying failure proximity techniques only to those failure-inducing segments. Our experiments show that our approach greatly improved the accuracy of failure proximity for context-aware applications. To the best of our knowledge, our work is the first attempt of studying the effectiveness of failure proximity for context-aware applications...|$|R
40|$|AbstractMaximizing {{the data}} {{throughput}} {{is a very}} common implementation objective for several streaming applications. Such task is particularly challenging for implementations based on many-core and multi-core target platforms because, in general, it implies tackling several NP- complete combinatorial problems. Moreover, an efficient design space exploration requires an accurate evaluation {{on the basis of}} dataflow program <b>execution</b> <b>profiling.</b> The focus of the paper is on the methodology challenges for obtaining accurate profiling measures. Experimental results validate a many-core platform built by an array of Transport Triggered Architecture processors for exploring the partitioning search space based on the execution trace analysis...|$|R
40|$|Abstract — This work {{focuses on}} power {{optimization}} of realtime applications with conditional execution {{running on a}} dynamic voltage scaling (DVS) enabled multiprocessor system. A novel algorithm is proposed that performs simultaneous task mapping and ordering followed by task stretching of a conditional task graph (CTG). The algorithm minimizes the mathematical expectation of energy dissipation of nondeterministic applications with random branch selection by utilizing the task <b>execution</b> <b>profile.</b> Compared with existing scheduling algorithm, the experimental results show that our algorithm has 32 % energy reduction in average. I...|$|E
40|$|This paper {{presents}} {{an approach to}} minimize the average program execution time by optimizing the hardware/software implementation of error detection. We leverage the advantages of partial dynamic reconfiguration of FPGAs in order to speculatively place in hardware those error detection components that will provide the highest reduction of execution time. Our optimization algorithm uses frequency information from a counter-based <b>execution</b> <b>profile</b> of the program. Starting from a control flow graph representation, we build the interval structure and the control dependence graph, which we then use to guide our error detection optimization algorithm...|$|E
40|$|Abstract—The suffix array and Burrows-Wheeler Transform are {{critical}} index structures in next generation sequence analysis. The construction of such index structures for mammalian-sized genomes can take thousands of seconds (i. e. tens of minutes). Its construction {{is complicated by}} computational overheads that coming from irregular or complex memory-access patterns. This paper rigorously characterizes the <b>execution</b> <b>profile</b> of the SA-IS algorithm in order to guide its optimization. The resulting optimized SA-IS, which we refer to as sais-opt, outperforms previous implementations of SA-IS as well as “best in practice” algorithms, when applied to large DNA strings. Keywords—suffix array; Burrows-Wheeler Transform; irregular memory acces...|$|E
40|$|ABSTRACT <b>Execution</b> <b>profiles</b> {{have become}} {{increasingly}} important for guid-ing code optimization. However, little {{has been done to}} develop ways to check automatically that a profile does, in fact, reflect theactual execution behavior of a program. This paper describes a framework that uses program monitoring techniques in a way thatallows the automatic checking {{of a wide variety of}} profile data. We also describe our experiences with using an instance of this frame-work to check edge profiles. The profile verifier uncovered profiling anomalies that we had been unaware of and that would havebeen very difficult to identify using existing techniques. 1...|$|R
40|$|This {{technical}} report examines common assumptions about computer users in profile-based optimization. We study <b>execution</b> <b>profiles</b> of interactive applications on Windows NT {{to understand how}} different users use the same program. The profiles were generated by the DIGITAL FX! 32 emulator/binary translator system, which automatically runs the x 86 version of Windows NT programs on NT/Alpha computers. Our statistical analysis indicates that people use the benchmark programs in different ways. This {{technical report}} is a supplement to the paper "Evaluating the Importance of User-Specific Profiling," to appear in "Proceedings of the 2 nd USENIX Windows NT Symposium," USENIX Association, August 1998...|$|R
40|$|Speculative partial {{redundancy}} elimination (SPRE) uses <b>execution</b> <b>profiles</b> {{to improve}} the expected performance of programs. We show how the problem of placing expressions to achieve the optimal expected performance can be mapped to {{a particular kind of}} network flow problem and hence solved by well known techniques. Our solution is sufficiently efficient to be used in practice. Furthermore, the objective function may be chosen so that reduction in space requirements is the primary goal and execution time is secondary. One surprising result that an explosion in size may occur if speed is the sole goal, and consideration of space usage is therefore important...|$|R
