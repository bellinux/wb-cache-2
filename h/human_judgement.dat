293|380|Public
5|$|BLEU is {{designed}} to approximate <b>human</b> <b>judgement</b> at a corpus level, and performs badly if {{used to evaluate the}} quality of individual sentences.|$|E
5|$|BLEU has {{frequently}} been reported as correlating well with <b>human</b> <b>judgement,</b> and remains a benchmark {{for the assessment}} of any new evaluation metric. There are however a number of criticisms that have been voiced. It has been noted that although in principle capable of evaluating translations of any language, BLEU cannot in its present form deal with languages lacking word boundaries.|$|E
25|$|Before 2000 {{evaluation}} of REG systems {{has been of}} theoretical nature like the one done by Dale and Reiter. More recently, empirical studies have become popular which are mostly {{based on the assumption}} that the generated expressions should be similar to human-produced ones. Corpus-based evaluation began quite late in REG due to a lack of suitable data sets. Still corpus-based evaluation is the most dominant method at the moment though there is also evaluation by <b>human</b> <b>judgement.</b>|$|E
40|$|The {{papers in}} these {{proceedings}} were {{presented at the}} Coling 2008 workshop on <b>human</b> <b>judgements</b> in Computational Linguistics, held in Manchester on 23 August 2008. <b>Human</b> <b>judgements</b> {{play a key role}} in the development and the assessment of linguistic resources and methods in Computational Linguistics. They are commonly used in the creation of lexical resource...|$|R
40|$|Image {{description}} {{is a new}} natural lan-guage generation task, where {{the aim is to}} generate a human-like description of an im-age. The evaluation of computer-generated text is a notoriously difficult problem, how-ever, the quality of image descriptions has typically been measured using unigram BLEU and <b>human</b> <b>judgements.</b> The focus {{of this paper is to}} determine the correlation of automatic measures with human judge-ments for this task. We estimate the correla-tion of unigram and Smoothed BLEU, TER, ROUGE-SU 4, and Meteor against <b>human</b> <b>judgements</b> on two data sets. The main finding is that unigram BLEU has a weak correlation, and Meteor has the strongest correlation with <b>human</b> <b>judgements.</b> ...|$|R
40|$|We examine {{correlations}} between native speaker judgements on automatically generated German text against automatic evaluation metrics. We {{look at a}} number of metrics from the MT and Summarisation communities and find that for a relative ranking task, most automatic metrics perform equally well and have fairly strong correlations to the <b>human</b> <b>judgements.</b> In contrast, on a naturalness judgement task, the General Text Matcher (GTM) tool correlates best overall, although in general, correlation between the <b>human</b> <b>judgements</b> and the automatic metrics was quite weak. ...|$|R
500|$|A {{group of}} Ali's army, later known as Kharijites or Khawarij ("those who leave"), opposed against {{arbitration}} {{after the battle}} of Siffin, when he accepted arbitration with Mu'awiya. They opposed to <b>human</b> <b>judgement</b> {{in the matter of}} religion and used [...] "Judgment belongs to God alone," [...] as their slogan. In 658 they violated their oath of allegiance, revolted and openly threatened to kill any Muslim who would not join them. Ali defeated them at the Battle of Nahrawan. The killing of the Kharijites [...] was [...] "the most problematic event" [...] during Ali's caliphate, because they had been among his most vigorous allies in the war against Muawiah.|$|E
500|$|Since 1993, the {{proportion}} of lbws in each English season has risen steadily. According to cricket historian Douglas Miller, the percentage of lbw dismissals increased after broadcasters incorporated ball-tracking technology such as Hawk-Eye into their television coverage of matches. Miller writes: [...] "With {{the passage of time}} and the adoption of Hawkeye into other sports, together with presentations demonstrating its accuracy, cricket followers seem gradually to have accepted its predictions. Replay analyses have shown that a greater proportion of balls striking an outstretched leg go on to hit the wicket than had once been expected." [...] He also suggests that umpires have been influenced by such evidence; their greater understanding of which deliveries are likely to hit the stumps has made them more likely to rule out batsmen who are standing further away from the stumps. This trend is replicated in international cricket, where the increasing use of technology in reviewing decisions has altered the attitude of umpires. Spin bowlers in particular win far more appeals for lbw. However, the use of on-field technology has proved controversial; some critics regard it as more reliable than <b>human</b> <b>judgement,</b> while others believe that the umpire is better placed to make the decision.|$|E
2500|$|... "Visual" [...] {{tests are}} more {{intuitively}} appealing but subjective {{at the same}} time, as they rely on informal <b>human</b> <b>judgement</b> to accept or reject the null hypothesis.|$|E
40|$|We {{present a}} <b>human</b> <b>judgements</b> dataset and also an adapted metric for evalua-tion of Arabic machine translation. Our medium-scale dataset is {{first of its}} kind for Arabic with high {{annotation}} quality. We use the dataset to adapt the BLEU score for Arabic. Our score (AL-BLEU) provides partial credits for stem and mor-phological matchings of hypothesis and reference words. We evaluate BLEU, METEOR and AL-BLEU on our <b>human</b> <b>judgements</b> corpus and show that AL-BLEU has the highest correlation with hu-man judgements. We are releasing the The dataset and the software to the research community...|$|R
40|$|When {{evaluating}} {{a generation}} system, if a corpus of target outputs is available, a common and simple {{strategy is to}} compare the system output against the corpus contents. However, cross-validation metrics that test whether the system makes exactly the same choices as the corpus on each item have recently been shown not to correlate well with <b>human</b> <b>judgements</b> of quality. An alternative evaluation strategy is to compute intrinsic, task-specific properties of the generated output; this requires more domain-specific metrics, but can often produce a better assessment of the output. In this paper, a range of metrics using both of these techniques are used to evaluate three methods for selecting the facial displays of an embodied conversational agent, and the predictions of the metrics are compared with <b>human</b> <b>judgements</b> of the same generated output. The corpus-reproduction metrics show no relationship with the <b>human</b> <b>judgements,</b> while the intrinsic metrics that capture the number and variety of facial displays show a significant correlation with the preferences of the human users. ...|$|R
30|$|Since a ground-truth {{does not}} exist for the {{considered}} dataset, the evaluation of performances of the proposed methodology needs {{to be carried out}} against <b>human</b> <b>judgements</b> or against the results produced by other algorithms.|$|R
50|$|BLEU is {{designed}} to approximate <b>human</b> <b>judgement</b> at a corpus level, and performs badly if {{used to evaluate the}} quality of individual sentences.|$|E
50|$|METEOR (Metric for Evaluation of Translation with Explicit ORdering) is a metric for the {{evaluation}} of machine translation output. The metric is based on the harmonic mean of unigram precision and recall, with recall weighted higher than precision. It also has several features that are not found in other metrics, such as stemming and synonymy matching, along with the standard exact word matching. The metric was designed to fix some of the problems found in the more popular BLEU metric, and also produce good correlation with <b>human</b> <b>judgement</b> at the sentence or segment level. This differs from the BLEU metric in that BLEU seeks correlation at the corpus level.Results have been presented which give correlation of up to 0.964 with <b>human</b> <b>judgement</b> at the corpus level, compared to BLEU's achievement of 0.817 on the same data set. At the sentence level, the maximum correlation with <b>human</b> <b>judgement</b> achieved was 0.403.|$|E
5000|$|... "Visual" [...] {{tests are}} more {{intuitively}} appealing but subjective {{at the same}} time, as they rely on informal <b>human</b> <b>judgement</b> to accept or reject the null hypothesis.|$|E
40|$|ROUGE is {{a widely}} adopted, {{automatic}} evaluation measure for text summariza-tion. While {{it has been shown}} to corre-late well with <b>human</b> <b>judgements,</b> it is bi-ased towards surface lexical similarities. This makes it unsuitable for the evalua-tion of abstractive summarization, or sum-maries with substantial paraphrasing. We study the effectiveness of word embed-dings to overcome this disadvantage of ROUGE. Specifically, instead of measur-ing lexical overlaps, word embeddings are used to compute the semantic similarity of the words used in summaries instead. Our experimental results show that our pro-posal is able to achieve better correlations with <b>human</b> <b>judgements</b> when measured with the Spearman and Kendall rank co-efficients. ...|$|R
3000|$|... {{has been}} {{computed}} {{with respect to}} the mean of all <b>human</b> <b>judgements</b> for each valid brushstroke extracted. Finally, the last row of each table shows the performances of the proposed algorithm {{with respect to the}} mean of human judgments.|$|R
40|$|Measuring {{translation}} {{quality is}} a subjective process which relies on <b>human</b> <b>judgements.</b> This paper discusses different frameworks {{used in the}} process of translation evaluation with special focus on error classification schemes used both in the translation industry and in translation teaching institutions. Such error-based models, like BlackJack or SAE J 2450, allow <b>human</b> <b>judgements</b> to benefit from a consistent and systematic error classification. Apart from offering an overview of the existing error-based translation evaluation systems, this paper describes two recent projects which further the research in this field. This paper does not claim to provide an exhaustive list of existing error-based translation evaluation tools nor does it aim to impose or offer a particular model for translation analysis...|$|R
50|$|BLEU has {{frequently}} been reported as correlating well with <b>human</b> <b>judgement,</b> and remains a benchmark {{for the assessment}} of any new evaluation metric. There are however a number of criticisms that have been voiced. It has been noted that although in principle capable of evaluating translations of any language, BLEU cannot in its present form deal with languages lacking word boundaries.|$|E
5000|$|<b>Human</b> <b>judgement</b> {{often has}} wide {{variance}} {{on what is}} considered a [...] "good" [...] summary, which means that making the evaluation process automatic is particularly difficult. Manual evaluation can be used, but this is both time and labor-intensive as it requires humans to read not only the summaries but also the source documents. Other issues are those concerning coherence and coverage.|$|E
5000|$|Strictly speaking, {{any human}} error i.e. any {{acceptance}} of a false thought as truth can {{be the beginning of}} prelest: [...] "The source of self-delusion and demonic deception is the false thought" [...] (St. Ignatius (Brianchaninov)). But Holy Fathers wrote mainly about the errors in the <b>human</b> <b>judgement</b> about spiritual matters and especially about the errors in understanding of the personal spiritual state.|$|E
40|$|In {{this paper}} we {{investigate}} the automatic collection, generation {{and evaluation of}} sentential paraphrases. Valuable sources of paraphrases are news article headlines; they tend to describe the same event in various different ways, and can easily {{be obtained from the}} web. We describe a method for generating paraphrases by using a large aligned monolingual corpus of news headlines acquired automatically from Google News and a standard Phrase-Based Machine Translation (PBMT) framework. The output of this system is compared to a word substitution baseline. Human judges prefer the PBMT paraphrasing system over the word substitution system. We compare <b>human</b> <b>judgements</b> to automatic judgement measures and demonstrate that the BLEU metric correlates well with <b>human</b> <b>judgements</b> provided that the generated paraphrase is sufficiently different from the source sentence...|$|R
40|$|Abstract. This paper {{reports on}} the {{descriptive}} results of an experiment comparing automatically detected reflective and not-reflective texts against <b>human</b> <b>judgements.</b> Based on the theory of reflective writing assessment and their operationalisation five elements of reflection were defined. For each element of reflection a set of indicators was developed, which automatically annotate texts regarding reflection based on the parameterisation with authoritative texts. Using a large blog corpus 149 texts were retrieved, which were either annotated as reflective or notreflective. An online survey was then used to gather <b>human</b> <b>judgements</b> for these texts. These two data sets were {{used to compare the}} quality of the reflection detection algorithm with human judgments. The analysis indicates the expected difference between reflective and not-reflective texts...|$|R
40|$|Underlying all {{assessments}} are <b>human</b> <b>judgements</b> regarding {{the quality of}} students’ understandings. Despite their ubiquity, those judgements are conceptually elusive. The articles selected for inclusion in this issue explore the complexity of judgement practice raising critical questions that challenge existing views and accepted policy and practice...|$|R
5000|$|Visual Information Fidelity (VIF) {{is a full}} {{reference}} image quality assessment index based on natural scene statistics {{and the notion of}} image information extracted by the human visual system. [...] It was developed by Hamid R Sheikh and Alan Bovik at the Laboratory for Image and Video Engineering at the University of Texas at Austin in 2006 and shown to correlate very well with <b>human</b> <b>judgement</b> of quality.|$|E
50|$|Before 2000 {{evaluation}} of REG systems {{has been of}} theoretical nature like the one done by Dale and Reiter. More recently, empirical studies have become popular which are mostly {{based on the assumption}} that the generated expressions should be similar to human-produced ones. Corpus-based evaluation began quite late in REG due to a lack of suitable data sets. Still corpus-based evaluation is the most dominant method at the moment though there is also evaluation by <b>human</b> <b>judgement.</b>|$|E
50|$|High school {{timetables}} {{are quite}} different from university timetables. The main difference is that in high schools, students have to be occupied and supervised every hour of the school day, or nearly every hour. Also, high school teachers generally have much higher teaching loads than {{is the case in}} universities. As a result, it is generally considered that university timetables involve more <b>human</b> <b>judgement</b> whereas high school timetabling is a more computationally intensive task, see constraint satisfaction problem.|$|E
40|$|This papers {{reports the}} {{application}} of the QARLA evaluation framework to the DUC 2004 testbed (tasks 2 and 5). Our experiment addresses two issues: how well QARLA evaluation measures correlate with <b>human</b> <b>judgements,</b> and what additional insights can be provided by the QARLA framework to the DUC evaluation exercises...|$|R
40|$|In this paper, we {{consider}} the computational modelling of <b>human</b> plausibility <b>judgements</b> for verb-relation-argument triples, a task equivalent to the computation of selectional preferences. Such models have applications both in psycholinguistics and in computational linguistics. By extending a recent model, we obtain a completely corpus-driven model for this task which achieves significant correlations with <b>human</b> <b>judgements.</b> It rivals or exceeds deeper, resource-driven models while exhibiting higher coverage. Moreover, we show that our model can be combined with deeper models to obtain better predictions than from either model alone. ...|$|R
40|$|Supervised {{approaches}} to NLP tasks rely on high-quality data annotations, which typically result from expensive manual labelling procedures. For some tasks, however, the subjectivity of <b>human</b> <b>judgements</b> might reduce {{the usefulness of}} the annotation for real-world applications. In Machine Translation (MT) Quality Estimation (QE), for instance, using humanannotated data to train a binary classifier that discriminates between good (useful for a post-editor) and bad translations is not trivial. Focusing on this binary task, we show that subjective <b>human</b> <b>judgements</b> can be effectively replaced with an automatic annotation procedure. To this aim, we compare binary classifiers trained on different data: the human-annotated dataset from the 7 th Workshop on Statistical Machine Translation (WMT- 12), and an automatically labelled version of the same corpus. Our results show that human labels are less suitable for the task. ...|$|R
50|$|Abelson and Sussman also {{cooperate}} in codirecting the MIT Project on Mathematics and Computation, {{a project of}} the MIT Computer Science and Artificial Intelligence Laboratory (previously a joint project of the AI Lab and LCS, CSAIL's components). The goal of the project is to create better computational tools for scientists and engineers. But even with powerful numerical computers, exploring complex physical systems still requires substantial human effort and <b>human</b> <b>judgement</b> to prepare simulations and to interpret numerical results.|$|E
5000|$|The central {{thesis is}} a {{dichotomy}} between two modes of thought: [...] "System 1" [...] is fast, instinctive and emotional; [...] "System 2" [...] is slower, more deliberative, and more logical. The book delineates cognitive biases {{associated with each}} type of thinking, starting with Kahneman's own research on loss aversion. From framing choices to people's tendency to replace a difficult question with one which is easy to answer, the book highlights several decades of academic research to suggest that people place too much confidence in <b>human</b> <b>judgement.</b>|$|E
5000|$|Some {{researchers}} {{argue that}} this kind of big data analysis has severe limitations, and that the analytical results can only be regarded as indicative, and not as definitive. This was confirmed by Kellyanne Conway, Donald Trump’s campaign advisor and counselor, who emphasized the importance of <b>human</b> <b>judgement</b> and common sense in drawing conclusions from fuzzy data. Conway candidly admitted that much of her own research would [...] "never see the light of day", because it was client confidential. Another Trump adviser criticized Conway, claiming that she [...] "produces an analysis that buries every terrible number and highlights every positive number" ...|$|E
30|$|The affect heuristic: Explains how {{the current}} affective state may {{influence}} <b>human</b> <b>judgements.</b> For example, when {{in a positive}} mood, one may be more easily susceptible to deception and manipulation because of a tendency to making hasty and possibly incorrect judgements, whereas one may be less inclined to do so when in a negative mood.|$|R
40|$|This paper {{proposes a}} new {{evaluation}} approach for sign language machine translation (SLMT). It aims {{to show a}} better correlation between its automatically generated scores and <b>human</b> <b>judgements</b> of translation accuracy. To show the correlation, an Arabic Sign Language (ArSL) corpus {{has been used for}} the evaluation experiments and the results obtained by various methods...|$|R
40|$|We {{describe}} {{the use of}} a weakly supervised bootstrapping algorithm in discovering contrasting semantic categories from a source lexicon with little training data. Our method primarily exploits the patterns in sentential contexts where different categories of words may appear. Experimental results are presented showing that such automatically categorized terms tend to agree with <b>human</b> <b>judgements.</b> ...|$|R
