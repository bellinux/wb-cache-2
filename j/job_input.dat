13|67|Public
50|$|OS/360's <b>job</b> <b>input</b> {{and print}} queuing had limited {{operational}} flexibility and performance. This was addressed by two field-developed packages: Houston Automatic Spooling Priority (HASP), and Attached Support Processor (ASP).|$|E
40|$|As {{the number}} of nodes in {{high-performance}} computing environments keeps increasing, faults are becoming common place causing losses in intermediate results of HPC jobs. Furthermore, storage systems providing <b>job</b> <b>input</b> data {{have been shown to}} consistently rank as the primary source of system failures leading to data unavailability and job resubmissions. This chapter presents transparent techniques to improve the reliability, availability and performance of HPC I/O systems, for the <b>job</b> <b>input</b> data. In this area, the chapter contributes (1) a mechanism for offline <b>job</b> <b>input</b> data reconstruction to ensure availability of <b>job</b> <b>input</b> data and to improve center-wide performance at no cost to job owners; (2) an approach to automatic recover <b>job</b> <b>input</b> data at run-time during failures by recovering staged data from an original source; and (3) “just in time ” replication of <b>job</b> <b>input</b> data so as to maximize the use of supercomputer cycles. Experimental results demonstrate the value of these advanced fault tolerance techniques to increase fault resilience in HPC environments. ...|$|E
30|$|This theory {{suggests}} that employees ponder what they put into a <b>job</b> (<b>input)</b> against what they get from it (outcome) and then compare this ratio with the input-outcome ratio of other workers. If they find this ratio {{equal to that of}} the relevant others, a state of equity is said to exist (Robbins, 2005). It has been found that rewards increase employee satisfaction only when these rewards are valued and perceived as equitable by the employees (Perry et al., 2006).|$|E
5000|$|Individuals {{compare their}} <b>job</b> <b>inputs</b> and {{outcomes}} {{with those of}} others and then respond to eliminate any inequities.Referent Comparisons: ...|$|R
5000|$|...SOURCE* {{standard}} input (normally either a terminal or for batch <b>jobs,</b> the <b>input</b> queue); ...|$|R
30|$|Lexical {{analysis}} In this step, a {{lexical analyzer}} (lexer) tokenizes a <b>job</b> title <b>input</b> using defined regular expressions and matches them against dictionary files.|$|R
40|$|Storage {{systems in}} {{supercomputers}} {{are a major}} reason for service interruptions. RAID solutions alone cannot provide sufficient protection as 1) growing average disk recovery times make RAID groups increasingly vulnerable to disk failures during reconstruction, and 2) RAID does not help with higher-level faults such failed I/O nodes. This paper presents a complementary approach based on the observation that files in the supercomputer scratch space are typically accessed by batch jobs whose execution can be anticipated. Therefore, we propose to transparently, selectively, and temporarily replicate ”active” <b>job</b> <b>input</b> data by coordinating the parallel file system with the batch job scheduler. We have implemented the temporal replication scheme in the popular Lustre parallel file system and evaluated it with real-cluster experiments. Our results show that the scheme allows for fast online data reconstruction, with a reasonably low overall space and I/O bandwidth overhead...|$|E
40|$|Procurement and the {{optimized}} {{utilization of}} Petascale supercomputers and centers is a renewed national priority. Sustained performance {{and availability of}} such large centers is a key technical challenge significantly impacting their usability. Storage systems {{are known to be}} the primary fault source leading to data unavailability and job resubmissions. This results in reduced center performance, partially {{due to the lack of}} coordination between I/O activities and job scheduling. In this work, we propose the coordination of job scheduling with data staging/offloading and on-demand staged data reconstruction to address the availability of <b>job</b> <b>input</b> data and to improve centerwide performance. Fundamental to both mechanisms is the efficient management of transient data: in the way it is scheduled and recovered. Collectively, from a center’s standpoint, these techniques optimize resource usage and increase its data/service availability. From a user’s standpoint, they reduce the job turnaround time and optimize the allocated time usage...|$|E
40|$|As {{the number}} of nodes in {{high-performance}} computing environments keeps increasing, faults are becoming common place causing losses in intermediate results of HPC jobs. Furthermore, storage systems providing <b>job</b> <b>input</b> data {{have been shown to}} consistently rank as the primary source of system failures leading to data unavailability and job resubmissions. This dissertation presents a combination of multiple fault tolerance techniques that realize significant advances in fault resilience of HPC jobs. The efforts encompass two broad areas. First, at the job level, novel, scalable mechanisms are built in support of proactive FT and to significantly enhance reactive FT. The contributions of this dissertation in this area are (1) a transparent job pause mechanism, which allows a job to pause when a process fails and prevents it from having to re-enter the job queue; (2) a proactive fault-tolerant approach that combines process-level live migration with health monitoring to complement reactive with proactive FT and to reduce {{the number of}} checkpoints when a majority of th...|$|E
30|$|Finally, the ApplicationAnalyzer {{evaluates the}} {{possibility}} to split the parameter study into multiple jobs. The test case job compares 1000 files against the database. In this case, the ApplicationAnalyzer generates job descriptors with one <b>job</b> matching 1000 <b>input</b> files, two <b>jobs</b> matching 500 <b>input</b> files, 4 <b>jobs</b> matching 250 <b>input</b> files, and so on. In order to simplify the procedure, the test case does not further discuss the job descriptors created by the ApplicationAnalyzer and explains the adaptive job configuration process {{on the basis of}} the generated resource descriptors.|$|R
30|$|Unfortunately, {{the set of}} job {{activity}} questions used varies substantially {{across the}} different survey years. This almost certainly reduces {{the reliability of the}} IAB/BIBB data as a source for tracking the evolution of <b>job</b> task <b>inputs</b> in aggregate.|$|R
5000|$|At the Department of Labor, the Committee {{met with}} Secretary William Wirtz to demand jobs, living wages, <b>job</b> training, <b>input</b> on labor policy, {{and an end}} to discrimination. The Committee also called {{attention}} to the high unemployment rate among minorities, which they believed to be underreported by the Department.|$|R
40|$|Storage {{system failure}} {{is a serious}} concern as we {{approach}} Petascale computing. Even at today’s sub-Petascale levels, I/O failure {{is the leading cause}} of downtimes and job failures. We contribute a novel, on-the-fly recovery framework for <b>job</b> <b>input</b> data into supercomputer parallel file systems. The framework exploits key traits of the HPC I/O workload to reconstruct lost input data during job execution from remote, immutable copies. Each reconstructed data stripe is made immediately accessible in the client request order due to the delayed metadata update and finegranular locking while unrelated access to the same file remains unaffected. We have implemented the recovery component within the Lustre parallel file system, thus building a novel application-transparent online recovery solution. Our solution is integrated into Lustre’s two-level locking scheme using a two-phase blocking protocol. Combining parametric and simulation studies, our experiments demonstrate a significant improvement in HPC center serviceability and user job turnaround time. 1...|$|E
40|$|Supercomputers are {{stepping}} into the Peta-scale and Exascale era, wherein handling hundreds of concurrent system failures is an urgent challenge. In particular, storage system failures {{have been identified as}} a major source of service interruptions in supercomputers. RAID solutions alone cannot provide sufficient storage protection as (1) average disk recovery time is projected to grow, making RAID groups increasingly vulnerable to additional failures during data reconstruction, and (2) disk-level data protection cannot mask higherlevel faults, such as software/hardware failures of entire I/O nodes. This paper presents a complementary approach based on the observation that files in the supercomputer scratch space are typically accessed by batch jobs, whose execution can be anticipated. Therefore, we propose to transparently, selectively, and temporarily replicate ”active ” <b>job</b> <b>input</b> data, by coordinating the parallel file system with the batch job scheduler. We have implemented the temporal replication scheme in the popular Lustre parallel file system and evaluated it with both real-cluster experiments and trace-driven simulations. Our results show that temporal replication allows for fast online data reconstruction, with a reasonably low overall space and I/O bandwidth overhead...|$|E
40|$|To {{reduce the}} impact of network {{congestion}} on big data jobs, cluster management frameworks use various heuristics to schedule compute tasks and/or network flows. Most of these schedulers consider the <b>job</b> <b>input</b> data fixed and greed-ily schedule the tasks and flows that are ready to run. How-ever, a large fraction of production jobs are recurring with predictable characteristics, which allows us to plan ahead for them. Coordinating the placement of data and tasks of these jobs allows for significantly improving their network local-ity and freeing up bandwidth, {{which can be used}} by other jobs running on the cluster. With this intuition, we develop Corral, a scheduling framework that uses characteristics of future workloads to determine an offline schedule which (i) jointly places data and compute to achieve better data local-ity, and (ii) isolates jobs both spatially (by scheduling them {{in different parts of the}} cluster) and temporally, improving their performance. We implement Corral on Apache Yarn, and evaluate it on a 210 machine cluster using production workloads. Compared to Yarn’s capacity scheduler, Corral reduces the makespan of these workloads up to 33 % and the median completion time up to 56 %, with 20 - 90 % reduction in data transferred across racks. CCS Concepts •Networks→Data center networks; •Computer systems organization → Cloud computing...|$|E
40|$|For decades, the {{determination}} of causes for employee pay satisfaction has been han important area of study, surely due to the important role that this job attitude {{is thought to have}} in getting a sustained competitive advantage. Traditionally research on this issue has been paying attention on <b>job</b> related <b>inputs,</b> both extrinsic and intrinsic related-factors to the job itself. Together with <b>job</b> <b>inputs,</b> fairness issues have been showing an important role in explaining the employee pay satisfation phenomenon. However, literature is scarce on the study of the numerous factors linked with the fairness issue, such as the ethical leadershio phenomenon. Because ethical leadership entails necessarily the dimensional concept of fairness, the influential direct effect of this variable on employee pay satisfation is thought to exist. In addition, due to the dyadic moral-technical dimensions fulfillment necessity to get authentic seft-actualized employees this effect is thought to be interacted according to the level of motivational potential of jobs tasks performed. Since ethical leadership entails the enhancement of morals in employees, higher job enriching tasks may make stronger the effect for supervisor ethical leadership on employee pay satisfation as employees may feel the fulillment of the moral dimension more compelling to be authentically self-actualized. The direct and interated influential effect for ethical leadership on pay satisfaction will be discussed and practical implications and future research will be finally delineated. Peer Reviewe...|$|R
40|$|In {{this paper}} we {{construct}} {{an example of}} multiplicity to further illustrate the general equilibrium forces at work. We assume that good and bad <b>jobs</b> produce <b>inputs</b> that are perfect substitutes, only {{that there is a}} quality difference. This will accentuate the tendency toward multiplicity and enable to explicitly construct two equilibria...|$|R
5000|$|If STEPNAME is omitted, {{the entire}} <b>input</b> <b>job</b> {{whose name is}} {{specified}} on the EDIT statement is copied. If no job name is specified, the first job encountered is processed.|$|R
40|$|The self-tuning dynP {{scheduler}} {{for modern}} cluster resource management systems switches between different basic scheduling policies dynamically during run time. This allows to react on changing {{characteristics of the}} waiting jobs. In this paper we present an enhancement to the decision process of the self-tuning dynP scheduler. Adding slackness means, that the currently used policy is virtually improved by a given percentage. This prevents rapid and consecutive policy switches, which might be induced by users for cheating the scheduler. We use discrete event simulations to evaluate the performance. As <b>job</b> <b>input</b> for driving the simulations we use original traces from real supercomputer and cluster installations. To increase the workload to be processed by the scheduler, the average interarrival time is decreased with the shrinking factor. The evaluation of the slackness enhancement shows, that if slackness {{is applied to the}} simple and advanced deciders both are equal in their decisions and generate the same performance. This is due to the fact, that in the decision process cases with two equal performing basic policies do no longer occur. The results show, that it depends on the trace, if, and how much slackness is beneficial. In general, the performance of the self-tuning dynP scheduler is increased by applying small slackness values. The performance benefit of slackness is most evident for the CTC trace...|$|E
40|$|Abstract. The self-tuning dynP {{scheduler}} {{for modern}} cluster resource management systems switches between different basic scheduling policies dynamically during run time. This allows to react on changing {{characteristics of the}} waiting jobs. In this paper we present enhancements to the decision process of the self-tuning dynP scheduler and evaluate {{their impact on the}} performance: (i) While doing a self-tuning step a performance metric is needed for ranking the schedules generated by the different basic scheduling policies. This allows different objectives for the self-tuning process, e. g. more user centric by improving the response time, or more owner centric by improving the makespan. (ii) Furthermore, a self-tuning process can be called at different times of the scheduling process: only at times when the characteristics of waiting jobs change (half self-tuning), i. e. new jobs are submitted; or always when the schedule changes (full self-tuning), i. e. when jobs are submitted or running jobs terminate. We use discrete event simulations to evaluate the achieved performance. As <b>job</b> <b>input</b> for driving the simulations we use original traces from real supercomputer installations. The evaluation of the two enhancements to the decision process of the self-tuning dynP scheduler shows that a good performance is achieved, if the self-tuning metric {{is the same as the}} metric used measuring the overall performance at the end of the simulation. Additionally, calling the self-tuning process only when new jobs are submitted, is sufficient in most scenarios and the performance difference to full self-tuning is small. ...|$|E
40|$|International audienceHadoop {{has been}} {{recently}} used to process a diverse variety of applications, {{sharing the same}} execution infrastructure. A practical problem facing the Hadoop community is how to reduce job makespans by reducing job waiting times and ex- ecution times. Previous Hadoop schedulers have focused on improving job execution times, by improving data locality but not considering job waiting times. Even worse, enforcing data locality according to the <b>job</b> <b>input</b> sizes can be ineffi- cient: {{it can lead to}} long waiting times for small yet short jobs when sharing the cluster with jobs with smaller input sizes but higher execution complexity. This paper presents hSRTF, an adaption of the well-known Shortest Remaining Time First scheduler (i. e., SRTF) in shared Hadoop clus- ters. hSRTF embraces a simple model to estimate the re- maining time of a job and a preemption primitive (i. e., kill) to free the resources when needed. We have implemented hSRTF and performed extensive evaluations with Hadoop on the Grid’ 5000 testbed. The results show that hSRTF can significantly reduce the waiting times of small jobs and therefore improves their makespans, but at the cost of a rel- atively small increase in the makespans of large jobs. For instance, a time-based proportional share mode of hSRTF (i. e., hSRTF-Pr) speeds up small jobs by (on average) 45 % and 26 % while introducing a performance degradation for large jobs by (on average) 10 % and 0. 2 % compared to Fifo and Fair schedulers, respectively...|$|E
50|$|George 2 {{added the}} concept of spooling. <b>Jobs</b> and <b>input</b> data were read in from cards or paper tape to an input well on disk or tape. The jobs were then run, writing output to disk or tape spool files, which were then written to the output peripherals. The input/processing/output stages were run in parallel, {{increasing}} machine utilisation. On larger machines {{it was possible to}} run multiple jobs simultaneously.|$|R
5000|$|... {{specifies}} {{the name}} of the <b>input</b> <b>job</b> to which the EDIT statement applies. Each EDIT statement must apply to a separate job. If START is specified without TYPE and STEPNAME, the JOB statement and all job steps for the specified job are included in the output.|$|R
2500|$|Background jobs: Also {{called a}} PSJob, it allows a command {{sequence}} (script) or pipeline to be invoked asynchronously. [...] Jobs can be {{run on the}} local machine or on multiple remote machines. An interactive cmdlet in a PSJob blocks {{the execution of the}} <b>job</b> until user <b>input</b> is provided.|$|R
40|$|This report {{describes}} {{our work}} on the project part of the course Language Processing and Computational Linguistics. It presents a java written program that extracts six important pieces of information from French <b>job</b> advertisements. The <b>inputs</b> of the system are advertisements taken from the internet and converted as text files...|$|R
50|$|Jobs to be {{submitted}} are described using the Job Description Language (JDL), which specifies, for example, which executable {{to run and}} its parameters, files to be moved {{to and from the}} Worker Node on which the <b>job</b> is run, <b>input</b> Grid files needed, and any requirements on the CE and the Worker Node.|$|R
5000|$|Rare Carat {{launched}} in October 2016. His own challenging and frustrating experience shopping for an engagement ring in 2015 inspired Wharton Business School graduate Ajay Anand to found the company. The goal for Anand [...] "was {{to create a}} platform that made comparison shopping simple, saving future grooms-to-be from the tedious <b>job</b> of <b>inputting</b> the same search parameters across a number of sites, for days and days on end." [...] Anand says Rare Carat {{has the capacity to}} disrupt the market for the diamonds by bringing greater transparency to an industry that has traditionally relied on opacity.|$|R
50|$|CEO's {{are under}} {{pressure}} from their boards to maximize profits and are paid bonuses accordingly; one method of doing so is offshoring <b>jobs</b> or sourcing <b>inputs</b> in the lowest wage countries possible. Firms in low-cost labor countries actively lobby U.S. and European companies to offshore a variety of jobs or locate new jobs and facilities overseas.|$|R
40|$|This {{research}} was conducted {{to see if there}} is a relationship between employees' age and their willingness to accept new job duties. The hypothesis for this research states there is a relationship between one's age and one's willingness to accept a new <b>job</b> duty. After <b>inputting</b> the data and conducting a statistical Chi-Square Test, I did not support my hypothesis...|$|R
40|$|Abstract—Data replication, {{the main}} failure {{resilience}} strategy used for big data analytics jobs, can be unnecessarily inefficient. It can cause serious performance degradation {{when applied to}} intermediate job outputs in multi-job computations. For instance, for I/O-intensive big data jobs, data replication is especially expensive because very large datasets need to be replicated. Reducing the number of replicas is not a satisfactory solution as it only aggravates a fundamental limitation of data replication: its failure resilience guarantees are limited {{by the number of}} available replicas. When all replicas of some piece of intermediate job output are lost, cascading job recomputations may be required for recovery. In this paper we show how job recomputation can be made a first-order failure resilience strategy for big data analytics. The need for data replication can thus be significantly re-duced. We present RCMP, a system that performs efficient job recomputation. RCMP can persist task outputs across jobs and leverage them to minimize the work performed during job recomputations. More importantly, RCMP addresses two important challenges that appear during job recomputations. The first is efficiently utilizing the available compute node parallelism. The second is dealing with hot-spots. RCMP handles both by switching to a finer-grained task scheduling granularity for recomputations. Our experiments show that RCMP’s benefits hold across two different clusters, for <b>job</b> <b>inputs</b> as small as 40 GB or as large as 1. 2 TB. Compared to RCMP, data replication is 30 %- 100 % worse during failure-free periods. More importantly, by efficiently performing recomputations, RCMP is comparable or better even under single and double data loss events. I...|$|R
40|$|The central data {{reconstruction}} of the ATLAS experiment of LHC is a very challenging task, involving large-scale computing and {{a wide variety of}} data formats, applications and software versions. In 2009 - 2010, the ATLAS detector has recorded hundreds of millions of collision, single beam and cosmic events, during an unstable commisioning period that gradually evolved to a stable operation mode aiming at physics. In parallel, the ATLAS Collaboration processed comparable amounts of simulated data, and also produced a collection of sophisticated derived datasets. To handle all this complexity, we have developed a powerful data-driven auto-configuration mechanism and a unified configuration interface that provides a lot of flexibility: "Reco_trf". The auto-configuration mechanism consists of inspecting the metadata of each <b>job's</b> <b>input</b> file to automatically derive the configuration parameters relevant for the input format and the requested tasks. This also simplifies considerably the configuration of jobs from ordinary users, who can use the same script to run without modification on real or simulated data, on files belonging to different major production, using raw or derived input data of any format. Possible intermediate algorithms are automatically scheduled according to the content of the input file. Reco_trf is a so-called "job transformation" interface used for all centralized production tasks at CERN's Tier 0 and on the Grid, and is also largely used by normal users. Reco_trf adds a lot of flexibility in the Production systems by allowing the execution of arbitrary python commands without building new software releases, while still bookkeeping this information in the production databases...|$|R
30|$|WS-PGRADE (Kacsuk et al. 2012) is the {{flexible}} web user interface of the workflow system gUSE, which supports {{the management of}} DAG-based workflows. The control structure is defined by data dependencies and parameter sweep mechanisms allow for emulating loops over a defined range of parameters and data. Each task in a workflow is represented by a <b>job</b> with <b>input</b> and output datasets and each job can be configured for exploiting a resource independent of the configuration of dependent jobs. Thus, a job can be configured as SOAP web service and connected with jobs defined for applying local, cluster, grid and cloud resources or another SOAP web service. Thus, users can reuse the FlexScreen services in an intuitive way.|$|R
40|$|Grid jobs often {{consist of}} a large number of tasks. If the {{performance}} of a statically scheduled grid job is unsatisfactory, one must decide which code of which task should be improved. We propose a novel method to guide grid users as to which tasks of their grid job they should accelerate in order to reduce the makespan of the complete <b>job.</b> The <b>input</b> we need is the task schedule of the grid job, which can be derived from traces of a previous run of the job. We provide several algorithms depending on whether only one or several tasks can be improved, or whether task improvement is achieved by improvement of one processor...|$|R
40|$|Abstract Background Currently, {{there is}} no open-source, {{cross-platform}} and scalable framework for coalescent analysis in population genetics. There is no scalable GUI based user application either. Such a framework and application would not only drive the creation of more complex and realistic models but also make them truly accessible. Results As a first attempt, we built a framework and user application for the domain of exact calculations in coalescent analysis. The framework provides an API with the concepts of model, data, statistic, phylogeny, gene tree and recursion. Infinite-alleles and infinite-sites models are considered. It defines pluggable computations such as counting and listing all the ancestral configurations and genealogies and computing the exact probability of data. It can visualize a gene tree, trace and visualize the internals of the recursion algorithm for further improvement and attach dynamically a number of output processors. The user application defines jobs in a plug-in like manner {{so that they can}} be activated, deactivated, installed or uninstalled on demand. Multiple jobs can be run and their <b>inputs</b> edited. <b>Job</b> <b>inputs</b> are persisted across restarts and running jobs can be cancelled where applicable. Conclusions Coalescent theory plays an increasingly important role in analysing molecular population genetic data. Models involved are mathematically difficult and computationally challenging. An open-source, scalable framework that lets users immediately take advantage of the progress made by others will enable exploration of yet more difficult and realistic models. As models become more complex and mathematically less tractable, the need for an integrated computational approach is obvious. Object oriented designs, though has upfront costs, are practical now and can provide such an integrated approach. </p...|$|R
40|$|In this paper, we {{consider}} the problem of choosing a minimum cost set of resources for executing a specified set of <b>jobs.</b> Each <b>input</b> <b>job</b> is an interval, determined by its start-time and end-time. Each resource is also an interval determined by its start-time and end-time; moreover, every resource has a capacity and a cost associated with it. We consider two versions of this problem. In the partial covering version, we are also given as input a number k, specifying {{the number of jobs}} that must be performed. The goal is to choose $k$ jobs and find a minimum cost set of resources to perform the chosen k jobs (at any point of time the capacity of the chosen set of resources should be sufficient to execute the jobs active at that time). We present an O(log n) -factor approximation algorithm for this problem. We also consider the prize collecting version, wherein every job also has a penalty associated with it. The feasible solution consists of a subset of the jobs, and a set of resources, to perform the chosen subset of jobs. The goal is to find a feasible solution that minimizes the sum of the costs of the selected resources and the penalties of the jobs that are not selected. We present a constant factor approximation algorithm for this problem...|$|R
30|$|Physiological data such as weight, height, {{and body}} {{temperature}} for each worker, and planned work schedules including location and <b>job</b> description are <b>input</b> into the system. The TEP system provides the thermal environment data {{at the work}} place. Metabolism information for each job, based on a table proposed by ASHRAE (2009), is stored in the CBTP system database. The CBTP system computes worker core body temperatures for each minute, based on initial conditions, and displays the result to the user.|$|R
