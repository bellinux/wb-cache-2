2786|538|Public
5|$|Dynamic {{balance is}} {{provided}} through the three semicircular canals. These three canals are orthogonal (at right angles) to each other. At {{the end of}} each canal is a slight enlargement, known as the ampulla, which contains numerous cells with filaments in a central area called the cupula. The fluid in these canals rotates according to the momentum of the head. When a person changes acceleration, the inertia of the fluid changes. This affects the pressure on the cupula, and results in the opening of ion channels. This causes depolarisation, which is passed as a signal to the brain along the vestibulocochlear nerve. Dynamic balance also helps maintain <b>eye</b> <b>tracking</b> when moving, via the vestibulo–ocular reflex.|$|E
5|$|The {{specific}} {{access method}} {{will depend on}} the skills and abilities of the user. With direct selection a body part, pointer, adapted mouse, joystick, or <b>eye</b> <b>tracking</b> could be used, whereas switch access scanning is often used for indirect selection. Unlike direct selection (e.g., typing on a keyboard, touching a screen), users of Target Scanning can only make selections when the scanning indicator (or cursor) of the electronic device is on the desired choice. Those who are unable to point typically calibrate their eyes to use eye gaze as a way to point and blocking as a way to select desired words and phrases. The speed and pattern of scanning, as well as the way items are selected, are individualized to the physical, visual and cognitive capabilities of the user.|$|E
25|$|The {{excimer laser}} uses an <b>eye</b> <b>tracking</b> system {{that follows the}} patient's eye {{position}} up to 4,000 times per second, redirecting laser pulses for precise placement within the treatment zone. Typical pulses are around 1 millijoule (mJ) of pulse energy in 10 to 20 nanoseconds.|$|E
5000|$|Interest/Excitement (reaction to new situation/impulse to attend) - eyebrows down, <b>eyes</b> <b>tracking,</b> <b>eyes</b> looking, closer {{listening}} ...|$|R
40|$|Abstract—At present, it is a {{hot topic}} in applied {{psychology}} that interpreting people’s thinking from <b>eye</b> <b>tracks.</b> In this paper, research material is 4 * 4 sudoku. The Tobii eye tracker records subjects’ <b>eye</b> <b>tracks</b> when they are solving 4 * 4 sudouk. And then use feature values, the weighted value of the composite indicators, such as the duration, regression time of each AOI, to train SVM classifier. The experiments of two different classification tasks show that, the classification accuracy is very high, {{and the ability of}} generalization is strong. So SVM can be used for the classification of <b>eye</b> <b>tracks</b> and this method can decode different problem-solving strategies according to <b>eye</b> <b>tracks...</b>|$|R
5000|$|Blind <b>Eye</b> (<b>Track</b> 4) on {{the album}} First Strut Is The Deepest (2013) by Brother Strut ...|$|R
25|$|Accessible {{publishing}} {{uses the}} digitization of books {{to mark up}} books into XML and then produces multiple formats from this to sell to consumers, often targeting those with difficulty reading. Formats include a variety larger print sizes, specialized print formats for dyslexia, <b>eye</b> <b>tracking</b> problems and macular degeneration, as well as Braille, DAISY, audiobooks and e-books.|$|E
25|$|In 1986, the New York Academy of Sciences co-organized {{with the}} National Institute of Mental Health and the Office of Naval Research the first {{important}} conference on chaos in biology and medicine. There, Bernardo Huberman presented {{a mathematical model}} of the <b>eye</b> <b>tracking</b> disorder among schizophrenics. This led to a renewal of physiology in the 1980s {{through the application of}} chaos theory, for example, in the study of pathological cardiac cycles.|$|E
25|$|Recently, <b>eye</b> <b>tracking</b> {{has been}} used to study online {{language}} processing. Beginning with Rayner (1978) the importance and informativity of eye-movements during reading was established. Later, Tanenhaus et al. (1995) used the visual-world paradigm to study the cognitive processes related to spoken language. Assuming that eye movements are closely linked to the current focus of attention, language processing can be studied by monitoring eye movements while a subject is presented auditorily with linguistic input.|$|E
50|$|All 4 Bright <b>Eyes</b> <b>tracks</b> from Oh Holy Fools: The Music of Son, Ambulance & Bright Eyes {{plus two}} bonus tracks.|$|R
5000|$|The album {{includes}} the TV <b>Eyes</b> <b>track</b> [...] "She Gets Around" [...] which was originally released as the B {{side of the}} [...] "She's A Study" [...] 12-inch single in 2003.|$|R
40|$|Photography {{provides}} tangible and visceral mementos of im-portant experiences. Recent {{research in}} content-aware image processing to automatically improve photos {{relies heavily on}} automatically identifying salient areas in images. While au-tomatic saliency estimation has achieved estimable success, it will always face inherent challenges. <b>Tracking</b> the photogra-pher’s <b>eyes</b> allows a direct, passive means to estimate scene saliency. We show that saliency estimation is sometimes an ill-posed posed problem for automatic algorithms, made well-posed by the availability of recorded <b>eye</b> <b>tracks.</b> We instru-ment several content-aware image processing algorithms with <b>eye</b> <b>track</b> based saliency estimation, producing photos that ac-centuate {{the parts of the}} image originally viewed...|$|R
25|$|Through fMRI, EEG, {{steady state}} {{topography}} (SST) and magnetoencephalography (MEG) scans, marketers {{are able to}} study how consumers react, as a step towards understanding how they can influence consumption through marketing efforts. In conjunction with qualitative research methods, it can provide deep understanding into what, how and why people consume. Measuring <b>eye</b> <b>tracking</b> in e-service marketing {{is another example of}} quantitative biological methods measuring the way we consume. It analyses how consumers shop in an online environment by recording the number of mouse clicks and click maps based on eye movement.|$|E
25|$|The Samsung Galaxy S4 is an Android {{smartphone}} {{produced by}} Samsung Electronics and was first shown publicly on March 14, 2013 at Samsung Mobile Unpacked in New York City. It is {{the successor to}} the Galaxy S III which maintains a similar design, but with upgraded hardware and an increased focus on software features that take advantage of its hardware capabilities—such {{as the ability to}} detect when a finger is hovered over the screen, and expanded <b>eye</b> <b>tracking</b> functionality. A hardware variant of the S4 became the first smartphone to support the emerging LTE Advanced mobile network standard.|$|E
25|$|The {{department}} is closely {{affiliated with the}} , which fosters research on the neural underpinnings of psychological function. The CSBMB houses {{state of the art}} facilities for the study of brain function, including a research-dedicated, high-field fMRI scanner, an EEG laboratory, a TMS coil, an <b>eye</b> <b>tracking</b> laboratory, and high-performance computing facilities for data analysis and computational modeling. Seventeen faculty members from the department are affiliated with the CSBMB. Unique among research institutions that own and operate fMRI scanners, the CSBMB is the first facility to own a scanner that is run solely by neuroscientists that conduct basic research. Most scanners in the United States are located in clinical settings and are utilized primarily in applied research.|$|E
40|$|International audienceThe {{problem of}} <b>eye</b> gaze <b>tracking</b> has been {{researched}} and developed for a long time. The most difficult problem in the non-intrusive system of <b>eye</b> gaze <b>tracking</b> {{is the problem of}} head movements. Some of existing methods have to use two cameras and an active infrared (IR) illumination to solve this problem. Otherwise, with a single camera, the user has to hold the head uncomfortably still when performing a session of <b>eye</b> gaze <b>tracking.</b> If the head of the user moves away from original position, the accuracy of these eye gaze-tracking systems drops dramatically. In this paper, we propose a solution using Gaussian Processes for <b>eye</b> gaze <b>tracking</b> that allows free head movements with a single camera...|$|R
5000|$|Their {{cover of}} the Bright <b>Eyes</b> <b>track</b> [...] "Lover I Don't Have to Love" [...] was {{featured}} in episode 18 of season 3 of the FOX show The OC. Palomine's title track can be heard playing in the background during episode 4 of My So-Called Life.|$|R
5000|$|... #Subtitle level 2: Percentage <b>eye</b> {{openness}} <b>tracking</b> (PERCLOS) ...|$|R
25|$|There {{are several}} methods that {{successfully}} investigate sentence processing, {{some of which}} include <b>eye</b> <b>tracking,</b> self-paced listening and reading, or cross-modal priming. The most productive method however, is real-time grammaticality judgements. A grammaticality judgement is a test which involves showing participants sentences that are either grammatical or ungrammatical. The participant must {{decide whether or not}} they find the sentences to be grammatical as quickly as possible. Grammaticality is cross-linguistic, so this method has therefore be used on a wide variety languages Grammaticality judgements are largely based on an individuals linguistic intuition, and it has been pointed out that humans have the ability to understand as well as produce an infinitely large number of new sentences that that have never seen before. This allows us to accurately judge a sentence grammatical or ungrammatical, even if it a completely novel sentence.|$|E
25|$|Process indices {{examine how}} {{individuals}} process information in their environment, such as by analyzing communication patterns between team members or using <b>eye</b> <b>tracking</b> devices. Team communication (particularly verbal communication) supports the knowledge building and information processing {{that leads to}} SA construction (Endsley & Jones, 1997). Thus, since SA may be distributed via communication, computational linguistics and machine learning techniques can be combined with natural language analytical techniques (e.g., Latent Semantic Analysis) to create models that draw on the verbal expressions of the team to predict SA and task performance (Bolstad, Cuevas, Gonzalez, & Schneider, 2005; Bolstad, Foltz, Franzke, Cuevas, Rosenstein, & Costello, 2007). Although evidence exists to support the utility of communication analysis for predicting team SA (Foltz, Bolstad, Cuevas, Franzke, Rosenstein, & Costello, in press), time constraints and technological limitations (e.g., cost and availability of speech recording systems and speech-to-text translation software) may make this approach less practical and viable in time-pressured, fast-paced operations.|$|E
25|$|Later, Schwartz and Kroll used cognate and {{homograph}} as target words {{presented in}} low- and high-constraint sentences to Spanish-English bilinguals. They investigated how word presentation and the semantic constraint modulated language lexical access in bilinguals. Schwartz and Kroll used rapid serial visual presentation {{and the target}} word had to be named. No homograph effects were found, but less proficient bilinguals made more naming errors, especially in low-constraint sentences. They observed cognate facilitation (nonselective bilingual lexical access) in low-constraint sentences, but not in high-constraint ones. The {{results suggest that the}} semantic constraint of a sentence may restrict cross-lingual activation effects. Similar results on cognate effects were obtained by van Hell and de Groot in their study of Dutch-English bilinguals in an L2 lexical decision task and a translation task in forward (from L1 to L2) and in backward direction (from L2 to L1). Libben and Titone used <b>eye</b> <b>tracking</b> methodology and found that the cognate facilitation in semantically constraint sentences only happened at early stages of comprehension and rapidly resolved at later stages of comprehension.|$|E
5000|$|The third disc {{contains}} Hitchcock's 1990 album, <b>Eye.</b> <b>Tracks</b> 1-17 {{comprise the}} original CD {{version of the}} album, and tracks 18-21 are bonus tracks. The original CD release also contained a track called [...] "College of Ice", which is instead included on disc 5 of I Wanna Go Backwards.|$|R
3000|$|... c). According to Daly [13], the <b>eye</b> <b>tracks</b> {{all objects}} in the visual field with an {{efficiency}} of 82 %. We adopt the same efficiency value for our spatiotemporal volume. However, if the visual attention map is available, {{it is also possible}} to substitute this map as the tracking efficiency [51].|$|R
30|$|Another {{issue to}} {{consider}} is the <b>eye’s</b> <b>tracking</b> ability, known as smooth pursuit, which compensates {{for the loss of}} sensitivity due to motion by reducing the retinal speed of the object of interest to a certain degree. Daly [13] draws a heuristic for smooth pursuit according to the experimental measurements.|$|R
25|$|<b>Eye</b> <b>{{tracking}}</b> {{device is}} a tool created to help measure eye and head movements. The first devices for tracking eye movement took two main forms: those that relied on a mechanical connection between participant and recording instrument, and those in which light or some other form of electromagnetic energy was directed at the participant's eyes and its reflection measured and recorded. In 1883, Lamare {{was the first to}} use a mechanical connection, by placing a blunt needle on the participant's upper eyelid. The needle picked up the sound produced by each saccade and transmitted it as a faint clicking to the experimenter's ear through an amplifying membrane and a rubber tube. The rationale behind this device was that saccades are easier to perceive and register aurally than visually. In 1889, Edmund B. Delabarre invented a system of recording eye movement directly onto a rotating drum by means of a stylus with a direct mechanical connection to the cornea. Other devices involving physical contact with the surface of the eyes were developed and used {{from the end of the}} 19th century until the late 1920s; these included such items as rubber balloons and eye caps.|$|E
500|$|The {{method of}} access to a {{communication}} device depends on the type and severity of the disease. In the spinal form of ALS, the limbs are affected from {{the onset of the}} disease; in these cases a head mouse or <b>eye</b> <b>tracking</b> access may be used initially. [...] In the bulbar form, speech is affected before the limbs; here handwriting and typing on keyboard-style devices are frequently the first forms of AAC. AAC users may change access methods as the disease progresses. [...] Low-tech systems, such as eye gazing or partner assisted scanning, are used in situations when electronic devices are unavailable (for example, during bathing) and {{in the final stages of}} the disease.|$|E
500|$|The ear is {{the organ}} of hearing and, in mammals, balance. In mammals, the ear is usually {{described}} as having three parts—the outer ear, middle ear and the inner ear. The outer ear consists of the pinna and the ear canal. Since the outer ear is the only visible portion of the ear in most animals, the word [...] "ear" [...] often refers to the external part alone. The middle ear includes the tympanic cavity and the three ossicles. The inner ear sits in the bony labyrinth, and contains structures which are key to several senses: the semicircular canals, which enable balance and <b>eye</b> <b>tracking</b> when moving; the utricle and saccule, which enable balance when stationary; and the cochlea, which enables hearing. The ears of vertebrates are placed somewhat symmetrically {{on either side of}} the head, an arrangement that aids sound localisation.|$|E
5000|$|The Fans - Giving Me That Look in Your <b>Eye</b> (Additional <b>Track)</b> ...|$|R
40|$|A {{hierarchical}} framework suggesting how graph readers {{go beyond}} explicitly represented data to make inferences is presented. According to our hierarchical framework, graph readers use read-offs, integration and pattern extrapolation to make inferences. Verbal protocol data demonstrates highlevel {{differences in the}} way inferences are made and <b>eye</b> <b>track</b> data examines these processes at the perceptual level...|$|R
40|$|When the <b>eyes</b> <b>track</b> {{a moving}} object, {{the image of}} a {{stationary}} target shifts on the retina colinearly with the eye movement. A compensation process called position constancy prevents this image shift from causing perceived target motion commensurate with the image shift. The target either appears stationary or seems to move in the direction opposite to the eye movement, but much less than the image shift would warrant. Our work is concerned with the question of whether position constancy operates when the image shift and the eye movement are not colinear. That can occur when, during the eye movement, the target undergoes a motion of its own. Evidence is reported that position constancy fails to operate when the direction of the target motion forms an angle with the direction of the eye movement. In 1978, Wallach, Bacon, and Schulman raised the following question: Is the induced motion that is perceived when the <b>eyes</b> <b>track</b> the moving surround instead of looking a...|$|R
500|$|AAC {{systems are}} diverse: unaided {{communication}} uses no equipment and includes signing and body language, while aided approaches use external tools. Aided communication methods {{can range from}} paper and pencil to communication books or boards to devices that produce voice output (speech generating devices or SGD's)and/or written output. The symbols used in AAC include gestures, photographs, pictures, line drawings, letters and words, {{which can be used}} alone or in combination. Body parts, pointers, adapted mice, or <b>eye</b> <b>tracking</b> can be used to select target symbols directly, and switch access scanning is often used for indirect selection. Message generation is generally much slower than spoken communication, and as a result rate enhancement techniques may be used {{to reduce the number of}} selections required. These techniques include [...] "prediction", in which the user is offered guesses of the word/phrase being composed, and [...] "encoding", in which longer messages are retrieved using a prestored code.|$|E
500|$|In late 2010, Square Enix {{announced}} a franchise reboot titled Tomb Raider; the new Lara Croft {{would be a}} darker, grittier reimagining of the character. In examining the character, Crystal Dynamics concluded that Croft's largest failing was her [...] "Teflon coating", and that it needed a more human version that players would care about. The studio sought a new voice actress, trialling dozens of relatively unknown performers. The second reboot focuses {{on the origin of}} the character, and as a result, changes the previous back story. Staff opted to first work on the character's biography rather than cosmetic aspects. Crystal Dynamics sought to avoid the embellished physique of past renditions and pushed for realistic proportions. In redesigning the character's appearance, the designers began with simple concepts and added features that it felt made Lara Croft iconic: a ponytail, [...] "M-shaped" [...] lips, and the spatial relationship between her eyes, mouth, and nose. The company also changed the character's wardrobe, focusing on what it believed was more functional and practical. In designing the outfits, staff aimed to create a look that was [...] "relevant" [...] and [...] "youthful", but not too [...] "trendy" [...] or [...] "hip". To gauge the redesign, Crystal Dynamics conducted <b>eye</b> <b>tracking</b> studies on subjects who viewed the new version and previous ones.|$|E
2500|$|<b>Eye</b> <b>tracking</b> (or eye-movement recording): A {{more natural}} and more {{sensitive}} on-line technique, which records the participants’ eye movements and eye fixations while they read a text presented {{on a computer}} screen. It documents what the participants are looking at and also {{how long it takes}} for them. Experimental <b>eye</b> <b>tracking</b> data is obtained to investigate topics such as understanding of spoken language,cognitive processes related to spoken language, body language and lip reading, and etc.|$|E
40|$|Abstract—Existing <b>eye</b> gaze <b>tracking</b> systems {{typically}} {{require an}} explicit personal calibration process {{in order to}} estimate certain person-specific eye parameters. For natural human computer interaction, such a personal calibration is often cumbersome and unnatural. In this paper, we propose a new probabilistic <b>eye</b> gaze <b>tracking</b> system without explicit personal calibration. Unlike the traditional <b>eye</b> gaze <b>tracking</b> methods, which estimate the eye parameter deterministically, our approach estimates the probability distributions of the eye parameter and eye gaze. By using an incremental learning framework, the subject doesn’t need personal calibration before using the system. His/her eye parameter estimation and gaze estimation can be improved gradually when he/she is naturally interacting with the system. The experimental result shows that the proposed system can achieve less than three degrees accuracy for different people without calibration. Index Terms—Gaze estimation, gaze calibration, dynamic Bayesian network. ...|$|R
3000|$|... to get {{the object}} of {{interest}} in the fovea. When the <b>eye</b> is <b>tracking</b> a slow moving object (smooth pursuit) the head rotates with about [...]...|$|R
3000|$|Moreover, {{future studies}} using {{objective}} devices, such as <b>eyes</b> <b>tracking</b> device and facial action coding system, to measure correct identification of emotions from facial expressions and attention status are needed. In the future, {{we also want}} to observe how their viewing behaviors changed and whether these changes can further promote their social skills used in their daily social reciprocity behaviors over time. Finally, future research is warranted to determine how reinvent visual media to increase the recognition of emotion in adolescents and other age-groups with ASD.|$|R
