0|6611|Public
40|$|We {{investigate}} {{the performance of}} different <b>classification</b> <b>models</b> {{and their ability to}} recognize prostate cancer in an early stage. We build ensembles of <b>classification</b> <b>models</b> in order to increase the classification performance. We measure the performance of our models in an extensive cross-validation procedure and compare different <b>classification</b> <b>models.</b> The datasets come from clinical examinations and some of the <b>classification</b> <b>models</b> are already in use to support the urologists in their clinical work...|$|R
40|$|<b>Classification</b> <b>models</b> and in {{particular}} binary <b>classification</b> <b>models</b> are ubiquitous in many branches of science and business. Consider, for example, <b>classification</b> <b>models</b> in bioinformatics that classify catalytic protein structures as being in an active or inactive conformation. As an example {{from the field of}} medical informatics we might consider...|$|R
50|$|The <b>IC</b> <b>classification</b> {{has also}} been used in {{scientific}} research. In 2014, a global forensic database based on the IC codes was established. It contains the microsatellite (short tandem repeat) profiles of 7121 individuals from {{various parts of the}} world residing or applying to live in the UK and Ireland. The six population database is used in a forensic setting to ascertain distant relatedness or coancestry according to the fixation index (FST) measure of genetic distance.|$|R
40|$|The vast {{majority}} of the literature evaluates the performance of <b>classification</b> <b>models</b> using only the criterion of predictive accuracy. This paper reviews the case for considering also the comprehensibility (interpretability) of <b>classification</b> <b>models,</b> and discusses the interpretability of five types of <b>classification</b> <b>models,</b> namely decision trees, classification rules, decision tables, nearest neighbors and Bayesian network classifiers. We discuss both interpretability issues which are specific to each of those model types and more generic interpretability issues, namely the drawbacks of using model size as the only criterion to evaluate the comprehensibility of a model, and the use of monotonicity constraints to improve the comprehensibility and acceptance of <b>classification</b> <b>models</b> by users...|$|R
40|$|Recommended by XXX We {{investigate}} {{the performance of}} different <b>classification</b> <b>models</b> {{and their ability to}} recognize prostate cancer in an early state. We build ensembles of <b>classification</b> <b>models</b> in order to increase the classification performance. We measure the performance of our models in an extensive cross-validation procedure and compare different <b>classification</b> <b>models.</b> The data sets come from clinical examinations and some of the <b>classification</b> <b>models</b> are already in use to support the urologists in their clinical work. Copyright c ○ 0000 ???. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. ...|$|R
40|$|This paper compares a few <b>classification</b> <b>models</b> {{for mass}} <b>classification</b> and {{analyzes}} the feature significance for mass <b>classification</b> using various <b>models.</b> It involves a few algorithms for feature selection and also analyzes the individual feature significance. The comparison of <b>classification</b> <b>models</b> {{is based on}} the same datasets for mass diagnosis...|$|R
40|$|We {{investigate}} {{the performance of}} different <b>classification</b> <b>models</b> {{and their ability to}} recognize prostate cancer in an early stage. We build ensembles of <b>classification</b> <b>models</b> in order to increase the classification performance. We measure the performance of our models in an extensive cross-validation procedure and compare different <b>classification</b> <b>models.</b> The datasets come from clinical examinations and some of the <b>classification</b> <b>models</b> are already in use to support the urologists in their clinical work. Copyright © 2008 Joerg D. Wichard et al. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. 1...|$|R
40|$|Abstract. In {{order to}} reuse {{existing}} resources effectively {{and meet the}} specific demands of different users, {{it is necessary to}} study the <b>classification</b> <b>modeling</b> methods of parts for guiding product design and improving product quality. Firstly, the hierarchy theories of <b>classification</b> <b>modeling</b> are analyzed based on Design Structure Matrix(DSM). Then, the process of <b>classification</b> <b>modeling</b> is presented for parts of complex machinery product based on DSM modeling method. Meanwhile, the steps and notes to build classification structure were described in details. The results show that the theories and methods presented in this article could provid guidances for <b>classification</b> <b>modeling</b> of parts and realize efficient reuse and quick retrieval of product data for complex machinery manufactories. ...|$|R
40|$|This paper {{proposes a}} <b>classification</b> <b>model</b> {{to improve the}} {{accuracy}} of prediction for business performance. The proposed model uses a combination of forward selection method to select the optimum attributes and <b>classification</b> <b>models.</b> Business performance data set is {{used to evaluate the}} accuracy of the proposed model. From results of experiments show that the combination of forward selection and Naïve Bayes model can improve the prediction accuracy of business performance compared to the other <b>classification</b> <b>models,</b> namely Logistic Regression, k-NN, Naïve Bayes, C 4. 5 and Support Vector Machine models significantly. The proposed model also yields better result compared to the other attribute selection using backward elimination method. Keywords: Forward Selection, Naïve Bayes, Entrepreneur, Business performance, <b>Classification</b> <b>Model...</b>|$|R
40|$|The {{use for a}} {{readability}} <b>classification</b> <b>model</b> {{is mainly}} as an integrated part of an information retrieval system. By matching the user’s demands of readability to the documents with the corresponding readability, the <b>classification</b> <b>model</b> can further improve the results of, for example, a search engine. This thesis presents a new solution for classification into readability levels for Swedish. The results from the thesis {{are a number of}} <b>classification</b> <b>models.</b> The models were induced by training a Support Vector Machines classifier on features that are established by previous research as good measurements of readability. The features were extracted from a corpus annotated with three readability levels. Natural Language Processing tools for tagging and parsing were used to analyze the corpus and enable the extraction of the features from the corpus. Empirical testings of different feature combinations were performed to optimize the <b>classification</b> <b>model.</b> The <b>classification</b> <b>models</b> render a good and stable <b>classification.</b> The best <b>model</b> obtained a precision score of 90. 21 % and a recall score of 89. 56 % on the test-set, which is equal to a F-score of 89. 88...|$|R
30|$|<b>Classification</b> <b>model</b> {{considered}} as robust and accurate.|$|R
40|$|This paper {{presents}} a learning methodology {{based on a}} substructural <b>classification</b> <b>model</b> to solve decomposable classification problems. The proposed method consists of three important components: (1) a structural model that represents salient interactions between attributes for a given data, (2) a surrogate model which provides a functional approximation of the output {{as a function of}} attributes, and (3) a <b>classification</b> <b>model</b> which predicts the class for new inputs. The structural model is used to infer the functional form of the surrogate and its coefficients are estimated using linear regression methods. The <b>classification</b> <b>model</b> uses a maximally-accurate, least-complex surrogate to predict the output for given inputs. The structural model that yields an optimal <b>classification</b> <b>model</b> is searched using an iterative greedy search heuristic. Results show that the proposed method successfully detects key variable interactions in hierarchical problems, group them in linkages groups, and build maximally accurate <b>classification</b> <b>models.</b> The initial results on non-trivial hierarchical test problems indicate that the proposed method holds promise and have also shed light on several improvements to enhance the capabilities of the proposed method. ...|$|R
5000|$|Out {{of sample}} {{prediction}} in regression and <b>classification</b> <b>models.</b>|$|R
40|$|In {{this paper}} we derive {{asymptotic}} x 2 - tests for general linear hypotheses on variance components using repeated variance components models. In two examples, the two-way nested <b>classification</b> <b>model</b> and the two-way crossed <b>classification</b> <b>model</b> with interaction, we explicitly investigate {{the properties of}} the asymptotic tests in small sample sizes...|$|R
40|$|International audienceThis paper {{addresses}} the inference of probabilistic <b>classification</b> <b>models</b> using weakly supervised learning. In contrast to previous work, {{the use of}} proportion-based training data is investigated in combination to non-linear <b>classification</b> <b>models.</b> An application to fisheries acoustics and fish school classification is considered and experiments are reported for synthetic and real datasets...|$|R
30|$|The {{quality of}} the <b>classification</b> <b>model</b> that is {{developed}} by a classification algorithm in a reasonable (default) configuration or in an automatically optimized configuration provides an indication as to whether a reliable classification is possible at all, or not; for example. if such a <b>classification</b> <b>model</b> shows an area under the ROC curve of something close to 0.5, then it is rather unlikely to increase the {{quality of the}} <b>classification</b> <b>model</b> to a satisfying level just by adjusting and tuning the algorithms’ parameters. The more promising way is to adjust the input set of input variables.|$|R
40|$|Peer-to-peer (P 2 P) lending, as a novel {{economic}} lending model, {{has triggered}} new challenges on making effective investment decisions. In a P 2 P lending platform, one lender can invest N loans and a loan may {{be accepted by}} M investors, thus forming a bipartite graph. Basing on the bipartite graph model, we built an iteration computation model to evaluate the unknown loans. To validate the proposed model, we perform extensive experiments on real-world data from the largest American P 2 P lending marketplace-Prosper. By comparing our experimental results with those obtained by Bayes and Logistic Regression, we show that our computation model can help borrowers select good loans and help lenders make good investment decisions. Experimental results also show that the Logistic <b>classification</b> <b>model</b> is a good complement to our iterative computation model, which motivates us to integrate the two <b>classification</b> <b>models.</b> The experimental results of the hybrid <b>classification</b> <b>model</b> demonstrate that the logistic <b>classification</b> <b>model</b> and our iteration computation model are complementary to each other. We conclude that the hybrid model (i. e., the integration of iterative computation <b>model</b> and Logistic <b>classification</b> <b>model)</b> is more efficient and stable than the individual model alone...|$|R
5000|$|The ETIM <b>Classification</b> <b>model</b> {{is built}} using the {{following}} categories or entities: ...|$|R
40|$|Abstract. Predicting {{the quality}} of {{software}} modules prior to testing or system operations allows a focused software quality improvement endeavor. Decision trees are very attractive for classification problems, because of their comprehensibility and white box modeling features. However, optimizing the classification accuracy and the tree size is a difficult problem, and to our knowledge very few studies have addressed the issue. This paper presents an automated and simplified genetic programming (gp) based decision tree modeling technique for calibrating software quality <b>classification</b> <b>models.</b> The proposed technique is based on multiobjective optimization using strongly typed gp. Two fitness functions are used to optimize the classification accuracy and tree size of the <b>classification</b> <b>models</b> calibrated for a real-world high-assurance software system. The performances of the <b>classification</b> <b>models</b> are compared with those obtained by standard gp. It is shown that the gp-based decision tree technique yielded better <b>classification</b> <b>models...</b>|$|R
5000|$|ISI Event Model (ISI-002): A {{comprehensive}} security event <b>classification</b> <b>model</b> (taxonomy + representation) ...|$|R
40|$|We {{present a}} new {{comprehensive}} approach to create accurate and interpretable linear <b>classification</b> <b>models</b> through mixed-integer programming. Unlike existing approaches to linear classification, our approach can produce models that incorporate {{a wide range of}} interpretability-related qualities, and achieve a precise, understandable and pareto-optimal balance between accuracy and interpretability. We demonstrate the value of our approach by using it to train scoring systems, M-of-N rule tables, and “personalized ” <b>classification</b> <b>models</b> for applications in medicine, marketing, and crime. In addition, we propose novel methods to train interpretable <b>classification</b> <b>models</b> on large-scale datasets using off-the-shelf mixed-integer programming software. Our paper includes theoretical bounds to assess the predictive accuracy of our models. ...|$|R
40|$|This paper {{presents}} the Feature Decomposition Approach for improving supervised learning tasks. While in Feature Selection {{the aim is}} to identify a representative set of features from which to construct a <b>classification</b> <b>model,</b> in Feature Decomposition, the goal is to decompose the original set of features into several subsets. A <b>classification</b> <b>model</b> is built for each subset, and then all generated models are combined. This paper {{presents the}}oretical and practical aspects of the Feature Decomposition Approach. A greedy procedure, called DOT (Decomposed Oblivious Trees), is developed to decompose the input features set into subsets and to build a <b>classification</b> <b>model</b> for each subset separately...|$|R
40|$|H 5 N 1 avian {{influenza}} outbreak detection {{is a significant}} issue for early warning of epidemics. This paper proposes domain knowledge-based joint one class <b>classification</b> <b>model</b> for {{avian influenza}} outbreak. Instead of focusing on manipulations of the one class <b>classification</b> <b>models,</b> we delve into the one class avian influenza data-set, divide it into sub-classes by domain knowledge, train the sub-class classifiers and unify the result of each classifier. The proposed joint method solves the one class classification and feature selection problems together. The experiment results demonstrate that the proposed joint model definitely outperforms the normal one class <b>classification</b> <b>model</b> on the animal avian influenza data-set...|$|R
40|$|Feature {{selection}} {{method is}} becoming an essential procedure in data preprocessing step. The feature selection problem can affect the efficiency and accuracy of <b>classification</b> <b>models.</b> Therefore, it also relates to whether a <b>classification</b> <b>model</b> can have a reliable performance. In this study, we compared an original feature selection method and a proposed frequency-based feature selection method with four <b>classification</b> <b>models</b> and three filter-based ranking techniques using a cancer dataset. The proposed method was implemented in WEKA which is an open source software. The performance is evaluated by two evaluation methods: Recall and Receiver Operating Characteristic (ROC). Finally, we found the frequency-based feature selection method performed better than the original ranking method...|$|R
40|$|In {{the data}} stream {{classification}} process, {{in addition to}} the solution of massive and real-time data stream, the dynamic changes of the need to focus and study. From the angle of detecting concept drift, according to the dynamic characteristics of the data stream. This paper proposes a new classification method for data stream based on the combined use of concept drift detection and <b>classification</b> <b>model.</b> The data stream <b>classification</b> <b>model</b> can’t adapt to concept drift problem to solve. Before the <b>model</b> <b>classification,</b> the use of information entropy to judge the data block concept drift, the concept of history to have appeared, the use of a classifier pool mechanism to save it, to makes the <b>classification</b> <b>model</b> has stronger resistance to concept drift...|$|R
2500|$|... {{provides}} a <b>classification</b> <b>model</b> {{taking into consideration}} the uncertainty associated with measuring replicate samples.|$|R
30|$|While {{constructing}} the <b>classification</b> <b>model,</b> many variables may be included, {{but not all}} of these variables are actually important. Therefore, unimportant variables need to be eliminated in order to construct a simpler <b>classification</b> <b>model.</b> There is quite a number of ways to screen variables, of which the LASSO algorithm has shown excellent performance in reducing variables (Connor et al. 2015).|$|R
40|$|In many {{applications}} experts {{need to make}} decisions based on the analysis of multi-dimensional data. Various <b>classification</b> <b>models</b> can support the decision making process. To obtain an intuitive understanding of the <b>classification</b> <b>model,</b> interactive visualizations are essential. We argue that this is best done by a series of interactive 2 D scatterplots. In this paper, we define a set of characteristics of the multi-dimensional <b>classification</b> <b>model</b> that have to be visually represented in those scatterplots. Our proposed method presents those characteristics in a uniform manner for both linear and non-linear classification methods. We combine a visualization of a Voronoi based representation of multi-dimensional decision boundaries with visualization of the distances of the data elements to these boundaries. To allow the developer of the model to refine the threshold of the <b>classification</b> <b>model</b> and instantly observe the results, we use interactive decision point selection on a performance curve. Finally, we show how the combination of those techniques allows exploration of multi-dimensional decision boundaries in 2 D...|$|R
40|$|Churn is {{perceived}} as the behaviour of a customer to leave or to terminate a service. This behaviour causes the loss of profit to companies because acquiring new customer incurred high investment for advertisements and promotions compared to retaining existing ones. Thus, {{it is necessary to}} consider an efficient <b>classification</b> <b>model</b> to reduce the rate of churn. In the traditional approach of <b>classification</b> <b>modelling,</b> it do not produce straightforward result interpretation. Therefore, identifying the best <b>classification</b> <b>model</b> to reduce the rate of churn is indeed a challenging task. The main objective of this thesis is to propose a new <b>classification</b> <b>model</b> based on the Rough Set Theory to classify customer churn. This research utilized the Knowledge Discovery in Database (KDD) process involving data pre-processing, data discretization, attribute reduction, rule generation, classification process, as well as data analysis, using the Rough Set toolkit. The Rough Set theory elements consist of indiscernibility relation, lower and upper approximations, as well as reduction set. Those elements are applied to classify customer chum from uncertain and imprecise dataset. The results of the proposed model are compared with a few established existing approaches. The results of the study show that the proposed <b>classification</b> <b>model</b> outperformed the existing models and contributes to significant accuracy improvement. The model is tested using dataset form local telecommunication company which achieves 90. 32 %. In conclusion, the results proved that the <b>classification</b> <b>model</b> based on Rough Set Theory had been capable to classify customer chum compared to the existing model...|$|R
40|$|Classification is a Data Mining {{technique}} used {{for building a}} prototype of the data behaviour, using which an unseen data can be classified {{into one of the}} defined classes. Several researchers have proposed classification techniques but most of them did not emphasis much on the misclassified instances and storage space. In this paper, a <b>classification</b> <b>model</b> is proposed that takes into account the misclassified instances and storage space. The <b>classification</b> <b>model</b> is efficiently developed using a tree structure for reducing the storage complexity and uses single scan of the dataset. During the training phase, Class-based Closed Frequent ItemSets (CCFIS) were mined from the training dataset {{in the form of a}} tree structure. The <b>classification</b> <b>model</b> has been developed using the CCFIS and a similarity measure based on Longest Common Subsequence (LCS). Further, the Particle Swarm Optimization algorithm is applied on the generated CCFIS, which assigns weights to the itemsets and their associated classes. Most of the classifiers are correctly classifying the common instances but they misclassify the rare instances. In view of that, AdaBoost algorithm has been used to boost the weights of the misclassified instances in the previous round so as to include them in the training phase to classify the rare instances. This improves the accuracy of the <b>classification</b> <b>model.</b> During the testing phase, the <b>classification</b> <b>model</b> is used to classify the instances of the test dataset. Breast Cancer dataset from UCI repository is used for experiment. Experimental analysis shows that the accuracy of the proposed <b>classification</b> <b>model</b> outperforms the PSOAdaBoost-Sequence classifier by 7...|$|R
30|$|Using {{large volumes}} of data but with low variety for {{training}} the <b>classification</b> <b>models</b> [12].|$|R
3000|$|This paper {{presents}} a supervised <b>classification</b> <b>model,</b> where the indicators of correlation between dependent [...]...|$|R
30|$|Results Best {{performance}} of the <b>classification</b> <b>models</b> {{was carried out by}} decision trees, in all runs.|$|R
40|$|This study {{examine the}} credit card fraud problem and adopt some actual {{transactional}} data with an online questionnaire transaction data to identify and prevent fraud. The ultimate {{aim of this study}} is to compare the effectiveness of generating personalized <b>classification</b> <b>model</b> to represent the spending behavior of individuals in identifying fraud as compared to the general <b>classification</b> <b>model</b> constructed from the mass data collected from all individuals...|$|R
40|$|In {{this paper}} we examine some nonparametric {{evaluation}} methods {{to compare the}} prediction capability of supervised <b>classification</b> <b>models.</b> We show also the importance, in nonparametric models, to eliminate the noise variables with a simple selection procedure. It is shown that a simpler model usually gives lower prediction error and is more interpretable. We show some empirical results applying nonparametric <b>classification</b> <b>models</b> on real and artificial data sets...|$|R
40|$|We {{introduce}} a feature enrichment approach, by developing multi-gram cosine similarity <b>classification</b> <b>models.</b> Our approach combines cosine similarity features of different N-gram word models, and unsupervised sentiment features, into models with a richer feature set {{than any of}} the approaches alone can provide. We test the <b>classification</b> <b>models</b> using different machine learning algorithms on categories of hateful and violent web content, and show that our multi-gram models give across-the-board performance improvements, for all categories tested, compared to combinations of baseline unigram, N-gram, and sentiment <b>classification</b> <b>models.</b> Our multi-gram models perform significantly better on highly imbalanced sets than the comparison methods, while this enrichment approach leaves room for further improvements, by adding instead of exhausting optimization options...|$|R
