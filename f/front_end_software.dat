12|9484|Public
50|$|This {{omni-channel}} <b>front</b> <b>end</b> <b>software</b> {{is called}} Interact.|$|E
5000|$|In October 2009, {{an upgrade}} for consultants' <b>front</b> <b>end</b> <b>software</b> became problematic, and Facebook {{was used as}} a backup solution. For this decision, Scentsy {{received}} the [...] "Success Award" [...] from the Direct Selling Association.|$|E
50|$|In {{almost all}} cases it lessens {{the degree of}} data {{integrity}} that is maintained in the masked data set. It is not a realistic value and will then fail any application logic validation {{that may have been}} applied in the <b>front</b> <b>end</b> <b>software</b> that is in the system under test. It also highlights to anyone that wishes to reverse engineer any of the identity data that data masking has been applied to some degree on the data set.|$|E
30|$|The CORNET testbed is openly {{available}} for {{conducting research on}} advanced CR networks. It provides a collection of resources to researchers lacking the ability to perform advanced experiments because of limited exposure to software-defined radio and CR platforms. The resources are {{made available to the}} researchers through flexible RF <b>front</b> <b>ends,</b> open-source <b>software,</b> and FCC experimental licenses.|$|R
40|$|The <b>front</b> <b>end</b> of {{innovation}} {{is regarded as}} one of the most important steps in building new software products or services, and the most significant benefits in software development can be achieved through improvements in the <b>front</b> <b>end</b> activities. Problems in the <b>front</b> <b>end</b> phase have an impact on customer dissatisfaction with delivered software, and on the effectiveness of the entire software development process. When these processes are improved, the likelihood of delivering high quality software and business success increases. This thesis highlights the challenges and problems related to the early phases of software development, and provides new methods and tools for improving performance in the <b>front</b> <b>end</b> activities of <b>software</b> development. The theoretical framework of this study comprises two fields of research. The first section belongs to the field {{of innovation}} management, and especially to the management of the early phases of the innovation process, i. e. the <b>front</b> <b>end</b> of innovation. The second section of the framework is closely linked to the processes of software engineering, especially to the early phases of the software development process, i. e. the practice of requirements engineering. Thus, this study extends the theoretical knowledge and discloses the differences and similarities in these two fields of research. In addition, this study opens up a new strand for academic discussion by connecting these research directions. Several qualitative business research methodologies have been utilized in the individual publications to solve the research questions. The theoretical and managerial contribution of the study can be divided into three areas: 1) processes and concepts, 2) challenges and development needs, and 3) means and methods for the <b>front</b> <b>end</b> activities of <b>software</b> development. First, the study discloses the difference and similarities between the concepts of the <b>front</b> <b>end</b> of innovation and requirements engineering, and proposes a new framework for managing the <b>front</b> <b>end</b> of the <b>software</b> innovation process, bringing business and innovation perspectives into software development. Furthermore, the study discloses managerial perceptions of the similarities and differences in the concept of the <b>front</b> <b>end</b> of innovation between the software industry and the traditional industrial sector. Second, the study highlights the challenges and development needs in the <b>front</b> <b>end</b> phase of <b>software</b> development, especially challenges in communication, such as linguistic problems, ineffective communication channels, a communication gap between users/customers and software developers, and participation of multiple persons in software development. Third, the study proposes new group methods for improving the <b>front</b> <b>end</b> activities of <b>software</b> development, especially customer need assessment, and the elicitation of software requirements...|$|R
40|$|FRONTIER is a {{computer}} program that functions as a <b>front</b> <b>end</b> for any {{of a variety of}} other software of both the artificial intelligence (AI) and conventional data-processing types. As used here, <b>front</b> <b>end</b> signifies interface <b>software</b> needed for acquiring and preprocessing data and making the data available for analysis by the other software. FRONTIER is reusable in that it can be rapidly tailored to any such other software with minimum effort. Each component of FRONTIER is programmable and is executed in an embedded virtual machine. Each component can be reconfigured during execution. The virtual-machine implementation making FRONTIER independent of the type of computing hardware on which it is executed...|$|R
50|$|As {{the use of}} {{computers}} that supported color and graphics, such the Atari 800, Commodore 64, Texas Instruments TI-99/4A, the Apple II series and early IBM PC compatibles, increased, online services gradually developed framed or partially graphical information displays. Early services such as CompuServe added increasingly sophisticated graphics-based <b>front</b> <b>end</b> <b>software</b> to present their information, though they continued to offer text-based access for those who needed or preferred it. In 1985 Viewtron, which began as a Videotex service requiring a dedicated terminal, introduced software allowing home computer owners access. Beginning in the mid-1980s graphics based online services such as PlayNET, Prodigy, MSN, and Quantum Link (aka Q-Link) were developed. Quantum Link, which was based on Commodore-only Playnet software, later developed AppleLink Personal Edition, PC-Link (based on Tandy's DeskMate), and Promenade (for IBM), all of which (including Q-Link) were later combined as America OnLine.|$|E
40|$|To {{overcome}} the current diversity in AB [1] front end equipment software and {{pave the way}} towards LHC [2] for efficient development, diagnostic and maintenance in this area, the CERN Accelerator Controls group launched in April 2003 a project to develop the new CERN [3] accelerator standard infrastructure for <b>front</b> <b>end</b> <b>software.</b> This development {{is based on the}} infrastructure recently born to handle the SPS beam measurement systems and extends it to handle the PS and SPS multi-cycling schemes, the future requirements needed for LHC as well as providing a good backward compatibility with the existing infrastructures. The project, approach and first deliverables are presented...|$|E
40|$|This paper {{discusses}} the design, development, {{and evaluation of}} GRATEFUL MED, the National Library of Medicine's (NLM) <b>front</b> <b>end</b> <b>software</b> for microcomputers that was developed to assist physicians and other health professionals to search NLM's MEDLINE database. A search is constructed by filling out a form screen with information on the desired author, title, and/or subject(s); the search can be limited to English language, review articles, or a particular journal. No knowledge of Boolean connectors or the Library's Medical Subject Headings (MeSH) vocabulary is assumed. The search is constructed and the results reviewed on the user's microcomputer; that is, while not connected to the NLM mainframe. published or submitted for publicatio...|$|E
40|$|Many {{traditional}} {{control systems}} include a distributed collection of <b>front</b> <b>end</b> machines to control hardware. Back end tools {{are used to}} view, modify and record the signals generated by these <b>front</b> <b>end</b> machines. <b>Software</b> servers, which are a middleware layer between the <b>front</b> and back <b>ends,</b> can improve a control system in several ways. Servers can enable on-line processing of raw data, and consolidation of functionality. In many cases, data retrieved from the <b>front</b> <b>end</b> must be processed in order to convert the raw data into useful information. These calculations are often redundantly performed by different programs, frequently offline. Servers can monitor the raw data and rapidly perform calculations, producing new signals {{which can be treated}} like any other control system signal, and can be used by any back end application. Algorithms can be incorporated to actively modify signal values in the control system based upon changes of other signals, essentially producing feedback in a control system. Servers thus increase the flexibility of a control system. Lastly, servers running on inexpensive UNIX workstations can relay or cache frequently needed information, reducing the load on <b>front</b> <b>end</b> hardware by functioning as concentrators. Rather than many back end tools connecting directly to the <b>front</b> <b>end</b> machines, increasing the work load of these machines, they instead connect to the server. Servers like those discussed above have been used successfully at the Thomas Jefferson National Accelerator Facility to provide functionality such as beam steering, fault monitoring, storage of machine parameters, and on-line data processing. The authors discuss the potential uses of such servers, and share the results of work performed to date...|$|R
50|$|Typically journey {{planners}} use {{an efficient}} in-memory {{representation of the}} network and timetable to allow the rapid searching {{of a large number}} of paths. Database queries may also be used where the number of nodes needed to compute a journey is small, and to access ancillary information relating to the journey. A single engine may contain the entire transport network, and its schedules, or may allow the distributed computation of journeys using a distributed journey planning protocol such as JourneyWeb or Delfi Protocol. A journey planning engine may be accessed by different <b>front</b> <b>ends,</b> using a <b>software</b> protocol or application program interface specialised for journey queries, to provide a user interface on different types of device.|$|R
50|$|Another way to {{understand}} {{the difference between the two}} is {{to understand}} the knowledge required of a <b>front</b> <b>end</b> vs a back <b>end</b> <b>software</b> developer. The list below focuses on Web development as an example.|$|R
40|$|Cognitive radio {{networking}} is {{a promising}} approach {{to fulfill the}} future's need for intelligent, high-performance communication and improve the efficiency of overall spectrum utilization. Testbed evaluation of protocols and algorithms is a must {{for the development of}} cognitive radio networks. In this paper, we present an integrated testbed framework for cognitive radio networks. The framework includes necessary components for cognitive radio operations: flexible RF <b>front</b> <b>end,</b> <b>software</b> define signal processing, adaptive MAC layer and network layer as well as a cross layer management interface. Such a design eases the cross layer configuration and performance optimization of cognitive protocol stack, while retaining the advantage of modularity. We design a testbed for ad-hoc cognitive radio network...|$|E
40|$|Virtual {{networking}} is {{an important}} step in the evolution of data networks. The key idea of network virtualization is to build a diversified Internet to support a variety of network services and architectures through a shared substrate. Pattern Recognition is the process of establishing a close match between some new stimulus and previously stored stimulus patterns. This Paper describes the necessity of biometric systems for Network Data and Information Security, Role of virtual LAN’s in supporting the error free security system. An Attempt is made to use the finger print of an individual for accessing the secured network. On an experimental basis, a biometric attendance system of staff and students of college is done to test and validate the working of the designed algorithm using Cellular Neural Network. Designing of algorithm using Cellular Neural Network to carry out the <b>front</b> <b>end</b> <b>software,</b> Backend software to implement in the Virtual network and implemented on digital Signal Processor (DSP). Ideal finger implemented on Digital Signal Processor (DSP). Ideal finger print data was considered for analysis without any error...|$|E
40|$|This {{presentation}} will detail BI {{responsibility and}} organization for LHC distributed instrumentation, i. e. the beam position monitoring system (BPM), the beam loss monitoring system (BLM) and the beam synchronous timing system (BOB). It will also present BI commitments on the requirements {{listed in the}} 2 previous sessions. In particular, it will address the BPM and BLM functionality and performance expected for the different operation stages and the procedures and tests foreseen to reach these objectives. Finally, current major issues (i. e. uncovered requirements) and possible alternatives will be presented. BI RESPONSIBILITIES AB/BI will provide the monitors, the electronics, the <b>front</b> <b>end</b> <b>software</b> and corresponding expert applications necessary to develop, test, deploy, diagnose and maintain the different instruments produced by the group. AB/BI is not responsible for any software above the BDI front end servers necessary to operate the machine. Therefore, the BPM and BLM concentrators, the real-time feedback loops, the fixed displays, the middle tier black boxes, the operational applications, the post-mortem applications, the video and the analog signal transmission are outside {{the responsibility of the}} AB/BI group. Table 1 gives a list {{of the people in the}} AB/BI group responsible for the different components of the BPM system, the BLM system and the BST system. All components of the BI mandate are covered. The commissioning experts for the BPM and the BLM system from the AB/ABP and AB/OP group are also shown in the table. The BOB system will be commissioned by AB/BI experts...|$|E
40|$|This thesis {{deals with}} a {{theoretical}} analysis of the basic parameters of receivers, input circuit architecture and signal digitization. According to the specified assignment it is outlined block scheme of <b>front</b> <b>end</b> for <b>software</b> receiver with specified components and the total bilance is calculated. Individual parts of the system are designed and realized. This {{is a set of}} four input filters for bandwidths: short waves up to 30 MHz, 87, 5 - 108 MHz, 144 - 148 MHz and 174 - 230 MHz. The main point of design is a circuit containing a low-noise amplifiers, switches, and two amplifiers with adjustable amplification. Mainly are used integrated circuits from Analog Devices corporation. To control the various switches and adjustable amplifiers was designed a separate panel, which is connected to the main circuit via a cable. In the last phase was the whole system and its components subjected to measurements. Thanks to a number of mounted SMA connectors it is possible to measure different parts of the system and we are able to modify it partially...|$|R
40|$|A current {{trend in}} modern HCI is {{represented}} by Embodied Conversational Agents (ECAs), even designed to run on the Web. They are virtual 3 D human-like <b>front</b> <b>ends</b> coupled with <b>software</b> agents {{that are able to}} engage in a conversation with a user and execute complex tasks, such as, for example, searching for some specific information or ordering some items from the catalogue of an online shop. This paper presents SAMIR system, a framework to build intelligent agents for the Web. SAMIR consists of a 3 D face which is animated to exploit expressions which are perceived by the user; a custom version of the ALICE chatterbot to chat with the user; and finally an XCS classifier system {{to deal with the problem}} of keeping conversation and face expressions coherent with each other. Experimental results, taken from an online bookstorebased scenario, are presented at the end...|$|R
40|$|This paper {{describes}} {{the development and}} implementation of a Voting software that makes use of the Advance Encryption Standard (AES), to encrypt all votes cast in a biometric verifiable election system. The software allows votes that are cast to be sent and tallied safely. The software allows the voter to cast their ballot by clicking or selecting few buttons, making voting seamlessly. The software is composed of a two applications, the Server application and the Client application, the Client application basically encrypts the votes cast by the voter whiles the Sever application decrypts the encrypted votes, tally votes and manages Election related data and procedures. The biometric identification module enrolls qualified voters and verifiers them on Election Day reducing or eliminating the possibility of multiple registration and voting. The AES module prevents the possibility of manipulating a voter’s intent and thwarting malicious people from tampering of the elections results. The software was developed using C #. Net for the <b>front</b> <b>end</b> of the <b>software</b> and Microsoft SQL Server 2012 database at the back <b>end.</b> The <b>software</b> can be used for all public elections for choosing leaders and to inform decision making process...|$|R
40|$|A {{large number}} of magnets, both superconducting and normal conducting, are {{installed}} for {{the guidance of the}} two proton beams around the Large Hadron Collider (LHC), the world's largest particle accelerator currently under construction at the European Organization for Nuclear Research (CERN). Due to the unprecedented energies stored in the beams and the magnets, sophisticated systems are under development to protect the equipment in case of failure. However, scenarios have been identified where failures in the magnet powering will lead to very fast beam losses in less than 100 $mu$s, due to the low time constants of the electrical circuits and the consequent fast current decay. For these circuits, systems that are currently deployed will not be fast enough to generate and transmit a beam dump request before the magnetic field change affects the beam trajectory. A dedicated system for the detection of such fast failures is already operational at the Hadron-Electron Ring Accelerator (HERA) in Hamburg. This system, the Fast Magnet Current Change Monitor (FMCM), has been adapted to meet CERN requirements and needs to be integrated into the CERN accelerator environment. For remote monitoring and Post Mortem analysis every FMCM is connected to the CERN control system by means of an RS- 422 interface. This master's thesis is focused on the software development and analysis of the control interface for the described FMCM units. The communication between the FMCMs and the CERN control system has been designed and implemented in C++, following the guidelines given by the <b>Front</b> <b>End</b> <b>Software</b> Architecture (FESA) framework. An analysis of the RS- 422 interface with respect to Signal Integrity and Electromagnetic Compatibility verified the current setup of the RS- 422 serial interface for the given transmission parameters. Transient bursts are considered to be {{the most common type of}} disturbance in the LHC and the related surface buildings. Hence, error detection has been implemented to ensure reliable communication by causing retransmissions of the data until it has been correctly received. </p...|$|E
40|$|For the FAIR {{control system}} a new data {{acquisition}} concept {{is required to}} meet the demands of sophisticated beam diagnostics, real time performance and high level of standardization. For that purpose the <b>Front</b> <b>End</b> <b>Software</b> Architecture (FESA) developed at CERN was chosen [1, 2]. FESA provides the tools to design and operate DAQ systems (FESA classes) based on common platforms such as PCI, cPCI and VME. It handles common tasks like multiplexing and publishing of the data to Java based GUI applications. A complete FESA framework environment (V 2. 10) has now been established at GSI, which already allows developments for the existing accelerators to obtain the expertise with the development techniques and the lookand-feel of the final applications. In the last year several FESA classes were developed for general feasibility studies such as video imaging or access of PLC systems as well as fully operational tools like the beam position monitoring at the SIS 18. These developments confirmed the decision for FESA at FAIR. Fast Current Transformer Recently a BERGOZ fast current transformer with a bandwidth of 600 MHz was installed into the SIS 18. The data acquisition and system control is realized with FESA. Main part of the development is the integration of a SIS 3350 VME 12 -Bit ADC with 500 MSa/s and a high performance data throughput. To control the amplifier settings a SIS 3610 VME I/O is used. A beta version is available and will be tested in the first half of 2010. ABLASS for FAIR The beam loss measurement and scaling system (AB-LASS) [3] which mainly counts pulses from particle detectors installed in the SIS 18 and HEBT, is based on an outdated OS and programming language (Kylix) and can no longer be supported. A first new implementation based on FESA with a set of Java GUI applications using the hardware of the expert version ABLAX is already operational although not all features of ABLASS have been realized yet. A real improvement within that development is the FESA filter mechanism, which allows GUIs to request only partial data, thus reducing the bandwidth required in the transfer. In addition block transfer readout of the VME modules has been implemented, which increases the sampling speed from 4 kHz to 20 kHz for a complete readout of 6 modules with 32 channels each...|$|E
40|$|To use the {{scientific}} resource of marine data effectively the information system PANGAEA was developed. The system guarantees longtime storage {{of the data}} in consistent formats and provides easy access for {{the scientific}} community via World Wide Web or a system specific client software with high functionality. The system is able to store data together with raw data, evaluated data and all related meta-information necessary for their understanding. The system provides standardized import and export routines, easy access with uniform retrieval functions, and tools for the visualization of data. The system is designed as a network with client/server technology providing access and data exchange through the Internet. The great variety of parameters, methods, calibrations and interpretations used in paleoenvironmental reconstruction, {{as well as the}} modification of established methods, are major obstacles to the integrative use of data sets in a common system. The challenge of managing these heterogenic and dynamic data was met in PANGAEA through a highly flexible data model consisting of a relational data structure in combination with specialized server software generating an object-oriented view of the data. The simplified data structure reflects the standard processing steps for paleo-data. Lists including standardized meta-information are connected to the main data fields: different projects working in the field of paleoceanography carry out expeditions for sampling. During a cruise at a number of locations different samples may be taken or measurements made. At distinct points/intervals the medium to be investigated (e. g. sediment, water or ice) is subsampled or measured for different requirements. From each sample one or more analytical data points will be produced which {{can be found on the}} DATA level, with the related meta-information. The combination of the DATA, 'Parameter' and 'Method' fields is the essential part of the model, which allows the definition and storage of new, unique parameters by the user at any time. The middleware allows the user to retrieve complex data matrices, e. g. time slices. The main server runs SYBASE as the database management software. The client software for access to the server was written in 4 th Dimension (ACI); the WWW-client software is written in JAVA. 4 D provides tools for the design of a graphical user interface and allows optional compilation of the <b>front</b> <b>end</b> <b>software</b> code for the different operating systems found in personal computers (MacintoshOS, Windows). For the geographical presentation of data the tool PanMap was developed, which is either directly connected to the database front end or can be used as a stand-alone application. PanMap can be called directly by the user to draw sampling sites in a geographical context after selecting the required data set by the retrieval tool. Sites can be labelled with meta-data as well as analytical data. The PanPlot tool has a similar link to the database as PanMap and allows the user to plot data versus depth or time. The software is avialable on [URL]...|$|E
40|$|Big data {{analytics}} applications {{play a significant}} role in data centers, and hence it has become increasingly important to understand their behaviors in order to further improve the performance of data center computer systems, in which characterizing representative workloads is a key practical problem. In this paper, after investigating three most impor- tant application domains in terms of page views and daily visitors, we chose 11 repre- sentative {{data analytics}} workloads and characterized their micro-architectural behaviors by using hardware performance counters, so as to understand the impacts and implications of data analytics workloads on the systems equipped with modern superscalar out-of-order processors. Our study reveals that big data analytics applications themselves share many inherent characteristics, which place them in a different class from traditional workloads and scale-out services. To further understand the characteristics of big data analytics work- loads we performed a correlation analysis of CPI (cycles per instruction) with other micro- architecture level characteristics and an investigation of the big data software stack impacts on application behaviors. Our correlation analysis showed that even though big data ana- lytics workloads own notable pipeline <b>front</b> <b>end</b> stalls, the main factors affecting the CPI performance are long latency data accesses rather than the <b>front</b> <b>end</b> stalls. Our <b>software</b> stack investigation found that the typical big data software stack significantly contributes to the <b>front</b> <b>end</b> stalls and incurs bigger working set. Finally we gave several recommen- dations for architects, programmers and big data system designers with the knowledge acquired from this paper. Comment: arXiv admin note: substantial text overlap with arXiv: 1307. 801...|$|R
40|$|This paper {{reports the}} {{development}} of a computer-aided engineering (CAE) software for human machine interface (HMI) designers to predict and benchmark the usability of in-vehicle infotainment systems. At the <b>front</b> <b>end</b> of the <b>software</b> a graphic user interface (GUI) was developed that allows HMI designers to create digital mockups of designs and setup the tasks for simulation. At the back end a digital driver was created for simulating the driver cognition and performance based on the cognitive architecture of QN-MHP (Queuing Network-Model Human Processor). The software is able to simulate a driver performing in-vehicle secondary tasks (e. g., tuning radios) while steering a vehicle, make quantitative predictions of the driver’s task performance and workload. To validate the software outputs, an experiment was conducted on a driving simulator with two typical in-vehicle infotainment systems (a physical panel and a touch screen), and a radio tuning task was used as a test case. The results show that the software is able to generate task performance and workload estimates that are similar to the empirical data from human participants...|$|R
40|$|This project aims {{to develop}} a simple {{software}} defined radio transceiver for University of Khartoum educational ground station that provides users with basic capabilities of changing the filtering parameters, tuning frequency and modulation/demodulation scheme. The system is based on software in {{which most of the}} transceiver functions are implemented with as minimum hardware as possible. Software defined radio (SDR) has emerged from obscurity to be heralded in recent years as offering a potential solution to our historical and continued inability to achieve common global communication standards. Software defined radio offers a highly flexible system that can work with different communication standards. Moreover, it allows new features {{to be added to the}} system without requiring the underlying architecture to be changed. This project aims {{to develop a}} simple software defined radio transceiver for University of Khartoum educational ground station that provides users with basic capabilities of changing the filtering parameters, tuning frequency and modulation/demodulation scheme. The system is based on software in which most of the transceiver functions are implemented with as minimum hardware as possible. The system was developed and implemented successfully using a PC and an RF <b>front</b> <b>end.</b> The <b>software</b> program was written using C++ to cope with real time processing requirement. The software operation for baseband processing was tested. Transmission and reception of baseband signals were performed successfully. Although the results obtained from testing the system are satisfactory, a software improvement is still required in order to increase the signal quality. System integration was not completed due to time constraints...|$|R
40|$|The Relativistic Heavy Ion Collider {{control system}} {{has been used in}} the {{commissioning}} of the AGS to RHIC transfer line and in the first RHIC sextant test. Much of the controls infrastructure for networks and links has been installed throughout the collider. All of the controls hardware modules needed to be built for early RHIC operations have been designed and tested. Many of these VME modules are already being used in normal AGS operations. Over 150 VME based <b>front</b> <b>end</b> computers and device controllers will be installed by the Summer of 1998 in order to be ready for Fall of 1998. A few features are being added to the <b>front</b> <b>end</b> computer core <b>software.</b> The bulk of the Accelerator Device Objects (ADOs) which are instantiated in the FECs, have been written and tested in the early commissioning. A configuration database has been designed. Generic control and display of ADO parameters via a spreadsheet like program on the console level computers was provided early on in the control system development. User interface tools that were developed for the AGS control system have been used in RHIC applications. Some of the basic operations programs, like alarm display and save/restore, that are used in the AGS operations have been or will be expanded to support RHIC operations. A model for application programs which involves a console level manager servicing ADOs have been verified with a few RHIC applications. More applications need to be written for the Fall of 1998 commissioning effort. A sequencer for automatic control of the fill is being written with the expectation that it will be useful in early commissioning...|$|R
25|$|The {{meaning of}} a tree was {{somewhat}} different for different language <b>front</b> <b>ends,</b> and <b>front</b> <b>ends</b> could provide their own tree codes. This was simplified {{with the introduction of}} GENERIC and GIMPLE, two new forms of language-independent trees that were introduced with the advent of GCC 4.0. GENERIC is more complex, based on the GCC 3.x Java <b>front</b> <b>end's</b> intermediate representation. GIMPLE is a simplified GENERIC, in which various constructs are lowered to multiple GIMPLE instructions. The C, C++ and Java <b>front</b> <b>ends</b> produce GENERIC directly in the <b>front</b> <b>end.</b> Other <b>front</b> <b>ends</b> instead have different intermediate representations after parsing and convert these to GENERIC.|$|R
40|$|In {{the first}} phase of construction, sixteen {{insertion}} device beamline <b>front</b> <b>ends</b> and sixteen bending magnet beamline <b>front</b> <b>ends</b> will be built by 1995 for the Advanced Photon Source (APS). Designs for these <b>front</b> <b>ends</b> have been completed. In this paper, the particular designs and specifications as well as the optical and bremsstrahlung ray-tracing analysis for the APS <b>front</b> <b>ends</b> are presented...|$|R
5000|$|While {{technically}} a 'Sil80', {{the combination}} of the 180SX body with an S15 Silvia <b>front</b> <b>end</b> can be referred to as an 'S13.5' for clarity, the '.5' denoting the <b>front</b> <b>end</b> conversion, similarly an S14 <b>front</b> <b>end</b> would be '.4' (Or '4a' for an S14a front) and an S13 <b>front</b> <b>end</b> would be '.3'. This naming scheme can be referred to for all models S13-S15, e.g. S13.5, S14.5 ...|$|R
40|$|The <b>front</b> <b>end</b> user {{interfaces}} {{for a variety}} of programming environments are surveyed. Emphasis is on display oriented <b>front</b> <b>end</b> interfaces that allow the user to have multiple windows. <b>Front</b> <b>end</b> interlaces can be split into three categories based upon the type of machine the software runs on and how they interface with the programming environment's tools. After presenting these categories, current trends and issues relevant to <b>front</b> <b>end</b> interfaces are discussed. Finally, several <b>front</b> <b>end</b> interfaces are examined in detail and classified according to these categories. ~: This worle {{was supported in part by}} grants from the National Science Foun...|$|R
40|$|Submitted to SIGPLAN Notices for publication. Abstract It is difficult, if not impossible, for {{the code}} analyzers to employ <b>front</b> <b>end</b> from compilers, because these <b>front</b> <b>ends</b> extract {{different}} program information using different strategy. This paper describes our experiences in building C++ <b>front</b> <b>end</b> {{as a part}} of code analysis toolset. The <b>front</b> <b>end</b> customizes the lexical analyzer incorporating a special preprocessing technique to accurately associate program entities with physical source code location. To support analysis of different C++ languages, the <b>front</b> <b>end</b> employs YACC to generate the parser, and uses token lookahead technique to disambiguate C++ grammar for YACC...|$|R
50|$|Originally {{delivered}} with unpainted stainless steel <b>front</b> <b>ends,</b> the blue bodyside bands were {{extended to the}} <b>front</b> <b>ends</b> from 1988.|$|R
40|$|In {{this paper}} {{we seek to}} {{streamline}} various operations within the <b>front</b> <b>end</b> of a speech recognizer, both to reduce unnecessary computation and to simplify the conceptual framework. First, a novel view of the <b>front</b> <b>end</b> in terms of linear transformations is presented. Then we study the invariance property of recognition performance with respect to linear transformations (LT) at the <b>front</b> <b>end.</b> Analysis reveals that several LT steps can be consolidated into a single LT, which effectively eliminates the Discrete Cosine Transform (DCT) step, part of the traditional MFCC (Mel-Frequency Cepstral Coefficient) <b>front</b> <b>end.</b> Moreover, a highly simplified, data-driven front-end scheme is proposed as a direct generalization of this idea. The new setup has no Mel-scale filtering, {{another part of the}} MFCC <b>front</b> <b>end.</b> Experimental results show a 5 % relative improvement on the Broadcast News task. 1. LINEAR TRANSFORMATIONS IN THE TRADITIONAL <b>FRONT</b> <b>END</b> The <b>front</b> <b>end</b> is a relatively independent component [...] ...|$|R
5000|$|Front {{knuckles}} changed {{making the}} <b>front</b> <b>end</b> [...] "push". Extending the swing arm {{will affect the}} <b>front</b> <b>end.</b> = [...] "push" ...|$|R
50|$|Many other {{components}} are {{in various stages}} of development, including, but not limited to, the Rust compiler, a Java bytecode <b>front</b> <b>end,</b> a Common Intermediate Language (CIL) <b>front</b> <b>end,</b> the MacRuby implementation of Ruby 1.9, various <b>front</b> <b>ends</b> for Standard ML, and a new graph coloring register allocator.|$|R
40|$|National Natural Science Foundation of China (NSFC) [61205103, 61405211]FM-to-AM {{modulation}} is {{an important}} effect in the <b>front</b> <b>end</b> of high-power lasers that influences the temporal profile. Various methods have been implemented in standard-fiber and polarization-maintaining (PM) -fiber <b>front</b> <b>ends</b> to suppress the FM-to-AM modulation. To analyze the modulation in the <b>front</b> <b>end,</b> a theoretical model is established and detailed simulations carried out that show that the polarizing (PZ) fiber, whose fast axis has a large loss, can successfully suppress the modulation. Moreover, {{the stability of the}} FM-to-AM modulation can be improved, which is important for the <b>front</b> <b>end</b> to obtain a stable output. To verify the model, a PZ fiber <b>front</b> <b>end</b> is constructed experimentally. The FM-to-AM modulation, without any compensation, is less than 4 %, whereas that of the PM fiber <b>front</b> <b>end</b> with the same structure is nearly 20 %. The stability of the FM-to-AM modulation depth is analyzed experimentally and the peak-to-peak and standard deviation (SD) are 2 % and 0. 38 %, respectively, over 3 h. The experimental results agree with the simulation results and both prove that the PZ fiber <b>front</b> <b>end</b> can successfully suppress the FM-to-AM conversion. The PZ fiber <b>front</b> <b>end</b> is a promising alternative for improving the performance of the <b>front</b> <b>end</b> in high-power laser facilities. (C) 2016 Optical Society of Americ...|$|R
40|$|Should {{management}} control the <b>front</b> <b>end</b> of innovation in companies? And if so, how? This thesis examines {{the use of}} {{management control}} in the <b>front</b> <b>end</b> of innovation, how the different management control mechanisms are associated with <b>front</b> <b>end</b> performance, and how technology and market uncertainty influence this relationship. The <b>front</b> <b>end</b> of innovation is generally regarded as the most troublesome phase of the innovation process {{and at the same}} time as one of the greatest opportunities to improve the overall innovation capability of a company. The <b>front</b> <b>end</b> of innovation has been characterized as a highly uncertain and creative phase, thereby requiring considerable amounts of freedom and independence for those executing <b>front</b> <b>end</b> activities. However, a certain amount of control is necessary to secure the effective use of resources and the achievement of the company's strategic goals. The current findings on management control and its influence on performance in a new product development context in general are conflicting. For example, while many authors argue that behavioral control kills creativity, others emphasize the advantages of improved communication and coordination created by process formalization. Some authors stress the importance of setting specific and challenging strategic goals for development work, yet other articles indicate this inhibits creativity and learning. One challenge in interpreting the conflicting results of existing management control research in a new product development context is the fact that most studies treat the <b>front</b> <b>end</b> of innovation simultaneously with product development projects, thereby averaging the totally different characteristics of these two innovation phases. Studies investigating management control in the <b>front</b> <b>end</b> of innovation are still scarce. This theoretical gap is the focus of this thesis. This study develops a framework for management control in the <b>front</b> <b>end</b> of innovation and tests hypotheses on the relationship between different management control mechanisms and <b>front</b> <b>end</b> performance. Management control is covered through seven variables: input control, <b>front</b> <b>end</b> process formalization, outcome-based rewarding, strategic vision, informal communication, participative planning, and intrinsic task motivation. Product concept superiority and strategic renewal are used as <b>front</b> <b>end</b> performance indicators, reflecting both the short-term and long-term development needs of the organization. The influence of technology and market uncertainty as potential moderators on the control mechanism-performance relationship is investigated in relation to <b>front</b> <b>end</b> process formalization and outcome-based rewarding. Data from the <b>front</b> <b>end</b> phase of 133 new product development projects from different large and medium-sized Finnish companies have been collected and analyzed. A factor model was used to test the validity of the management control framework and a linear regression analysis used for hypothesis testing. The results show that management control mechanisms are associated with performance in different manners depending on the performance variable used. <b>Front</b> <b>end</b> process formalization, strategic vision, and intrinsic task motivation were positively associated with product concept superiority. No association was found between input control, outcome-based rewarding, informal communication, participative planning, and product concept superiority. The results show that input control and intrinsic task motivation were positively associated with strategic renewal in the <b>front</b> <b>end</b> of innovation. No association existed between <b>front</b> <b>end</b> process formalization, outcome-based rewarding, informal communication, participative planning, and strategic renewal. Three significant moderating relationships were found in the study: Market uncertainty positively moderates the positive association between <b>front</b> <b>end</b> process formalization and product concept superiority; Technology uncertainty negatively moderates the relationship between <b>front</b> <b>end</b> process formalization and strategic renewal, i. e. under high technology uncertainty, <b>front</b> <b>end</b> process formalization is negatively related to strategic renewal; Technology uncertainty also negatively moderates the relationship between outcome-based rewarding and strategic renewal, i. e. under high technology uncertainty, outcome-based rewarding is negatively related to strategic renewal. This study contributes to management control literature by making management control in the <b>front</b> <b>end</b> of innovation the focal point - an area which is still barely touched in management control theory. The findings contribute to the body of knowledge of <b>front</b> <b>end</b> management by showing that management should be actively involved in the <b>front</b> <b>end</b> of innovation and by providing evidence of the importance of this phase on a firm's dynamic capabilities. This thesis contributes to contingency theory also by demonstrating how both market uncertainty and technology uncertainty moderate the association between management control mechanisms and <b>front</b> <b>end</b> performance. The findings have practical implications for management as they show certain mechanisms lead to effective control in the <b>front</b> <b>end</b> of innovation...|$|R
