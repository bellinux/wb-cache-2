300|3035|Public
50|$|Image {{morphing}} is {{a technique}} to synthesize a fluid transformation from one <b>image</b> (<b>source</b> image) to another (destination <b>image).</b> <b>Source</b> image can be one or more than one images. There are two parts in the image morphing implementation. The first part is warping {{and the second part}} is cross-dissolving.|$|E
50|$|<b>Image</b> <b>Source</b> of London EC1 for Royalty free digital photography.|$|E
50|$|Moving <b>Image</b> <b>Source</b> is updated every Thursday with {{additions}} to the Articles and Calendar sections.|$|E
40|$|The {{acoustics}} {{of a room}} can be visualized by plotting <b>image</b> <b>sources</b> at {{the positions}} where they occur. Recently, the image-source method has been extended to include edge diffrac-tion, and a visualization technique for <b>image</b> <b>sources</b> including diffraction (edge <b>image</b> <b>sources)</b> has been presented. Visual-ization is performed in analogous fashion with visualization of mirror <b>image</b> <b>sources.</b> The receiver-relative direction and dis-tance correspond to the direction from which the sound reaches a receiver {{and the length of}} the propagation path, respectively. First-order edge diffraction is presented as curved line sources, and second-order edge diffraction as surface sources. As exam-ples, first-order diffraction in a stage-house, and second-order diffraction around a loudspeaker enclosure are visualized. 1...|$|R
5000|$|The {{ability to}} <b>image</b> <b>sources</b> arcminutes to degrees across at one {{arcsecond}} resolution ...|$|R
5000|$|... #Caption: <b>Image</b> <b>sourced</b> {{from the}} medGadget. This {{is an example}} of post {{operative}} care.|$|R
50|$|The <b>image</b> <b>source</b> {{does not}} have to be photographic. 3D renderings or {{drawings}} can be used.|$|E
50|$|Implement the {{extrapolated}} boundary condition by adding an <b>image</b> <b>source</b> of opposite sign {{above the surface}} at 'b.|$|E
50|$|The {{boundary}} {{effect of}} the scattering medium is removed by adding an <b>image</b> <b>source</b> to satisfy the boundary condition.|$|E
40|$|This paper {{presents}} the first {{results of an}} imaging technique that measures the geoacoustic structure of a seafloor in shallow water areas. The devices used were a broadband 100 Hz– 6 kHz acoustic source towed by a ship and a vertical array. Among all the acoustic paths existing in the water column, two are used: the direct one and the seabed-reflected one, the latter being composed of the reflections from the seafloor’s surface {{as well as that}} from each buried layer. Due to the good time resolution of the signal and to the short range configuration, the reflected signal can be modeled as a sum of contributions coming from <b>image</b> <b>sources</b> relative to the seabed layers. The seabed geometry and the sound speed profile can then be recovered with the detection and localization of these <b>image</b> <b>sources.</b> The map of the <b>image</b> <b>sources</b> is obtained by a function that combines back-propagation of signals and knowledge of the emitted pulse. The thickness and sound-speed of each layer is finally obtained by a position analysis of the <b>image</b> <b>sources.</b> The results obtained by this data-driven algorithm on both at-sea and synthetic data are satisfactory...|$|R
50|$|High-definition <b>image</b> <b>sources</b> include {{terrestrial}} broadcast, direct broadcast satellite, digital cable, {{high definition}} disc (BD), digital cameras, Internet downloads, and video game consoles.|$|R
5000|$|High-definition <b>image</b> <b>sources</b> include {{terrestrial}} broadcast, direct broadcast satellite, digital cable, IPTV (including GoogleTV, Roku {{boxes and}} AppleTV or built into [...] "Smart Televisions"), Blu-ray video disc (BD), and internet downloads.|$|R
50|$|Lens ShapeThe multi-processor {{accelerated}} BCC Lens Shape {{filter to}} generate stylized image highlights that take their shape from an alternate alpha <b>image</b> <b>source.</b>|$|E
50|$|The {{launch of}} Moving <b>Image</b> <b>Source</b> {{was marked by}} a special program at The Times Center in Manhattan at 6:30 p.m. on June 5, {{featuring}} a conversation between directors Werner Herzog (Encounters at the End of the World, opening June 11) and Jonathan Demme (The Silence of the Lambs).|$|E
5000|$|... #Caption: This cover {{comes from}} the 6th edition of the Australian Flying Saucer Bureau (AFSB) {{publication}} called the Australian Flying Saucer Magazine. Six were produced from May 1953 to Feb 1955. - No 6, February 1955, 8 pages, 21 cm x 28 cm - <b>image</b> <b>source</b> PRA Melbourne.|$|E
50|$|The {{endoscopic}} image can {{be combined}} with other <b>image</b> <b>sources</b> to provide the surgeon with additional information. For instance, the position of an anatomical structure or tumor might be shown in the endoscopic video.|$|R
30|$|Select <b>source</b> <b>image</b> {{and find}} the largest human region in each <b>source</b> <b>image.</b>|$|R
40|$|We {{investigate}} {{what conditions}} {{allow us to}} extract the relative distribution of freeze-out space and time points in an arbitrary reference frame using tomography and source imaging. The source function may be extracted from the two-particle correlation function measured in heavy-ion collisions using imaging techniques. This <b>imaged</b> <b>source</b> function {{is related to the}} relative distribution of freeze-out space and time points through a generalization of the Radon transform found in tomography. Using tomography, the <b>imaged</b> <b>source</b> function may be converted into the relative freeze-out distribution in the frame of interest. We describe how the tomography may be performed in practice. Comment: 11 pages, uses RevTeX. Submitted to Phys. Rev. Let...|$|R
5000|$|An <b>image</b> <b>source.</b> This is the {{electrical}} signal representing the visual image, {{and may be}} from a professional video camera {{in the case of}} live television, a video tape recorder for playback of recorded images, or telecine with a flying spot scanner for the transfer of motion pictures to video).|$|E
50|$|Manufacturers of {{home theater}} screens have {{attempted}} {{to resolve the issue}} of ambient light by introducing screen surfaces that direct more of the light back to the light source. The rationale behind this approach relies on having the <b>image</b> <b>source</b> placed near the audience, so that the audience will actually see the increased reflected light level on the screen.|$|E
50|$|Another type of 3D display {{which is}} a {{candidate}} member {{of the class of}} swept-volume 3D displays is the varifocal mirror architecture. One of the first references to this type of system is from 1966, in which a vibrating mirrored drumhead reflects a series of patterns from a high frame rate 2D <b>image</b> <b>source,</b> such as a vector display, to a corresponding set of depth surfaces.|$|E
50|$|In this series, Hipkins used {{ready-made}} <b>images,</b> <b>sourced</b> from kitschy offset prints made in Switzerland in 1978, {{which he}} bought in West Auckland. He reproduced the images as large rectangular wallpaper murals (2160 x 4800 mm each).|$|R
25|$|Google Earth and Virtual Earth 3D {{are both}} capable of {{displaying}} many more urban areas in high-resolution {{thanks to their}} private <b>image</b> <b>sources.</b> Both companies also hire chartered flights over major cities of the U.S. to take aerial images.|$|R
5000|$|Ruffo’s Flag Series from 2006 {{illustrates}} contemporary colonisation as predatory. The {{works are}} composed of layered geographic <b>image</b> <b>sources</b> upon which the artist draws national flags {{made up of the}} intricate skulls of predatory mammals. On these works, Santcatterina writes, ...|$|R
5000|$|In 2010, Film Comment, the Film Society of Lincoln Center's {{official}} journal, counted Not Coming to a Theater Near You {{as being}} among the top film criticism sites (alongside Moving <b>Image</b> <b>Source,</b> indieWIRE, Ain't It Cool News, and others), calling it [...] "an ambitious online resource for reevaluations of forgotten and fringe cinema." [...] David Hudson, writing for Mubi's The Auteurs Daily, included the site on a similar best-of list.|$|E
50|$|Although {{originally}} used {{to describe}} a process that started with film scanning and ended with film recording, digital intermediate is also {{used to describe}} color correction and color grading and even final mastering when a digital camera is used as the <b>image</b> <b>source</b> and/or when the final movie is not output to film. This is due to recent advances in digital cinematography and digital projection technologies that strive to match film origination and film projection.|$|E
50|$|Moving <b>Image</b> <b>Source</b> is {{a website}} of the Museum of the Moving Image (New York City) {{devoted to the}} history of film, television, and digital media. Made {{possible}} with support from the Hazen Polsky Foundation, it features original articles by leading critics, authors, and scholars; a calendar that highlights major retrospectives, festivals, and gallery exhibitions at venues around the world; and a regularly updated guide to online research resources. Film critic Dennis Lim currently serves as editor-in-chief.|$|E
50|$|The art booklet {{that accompanies}} the deluxe {{edition of the}} album {{includes}} a variety of found <b>images</b> <b>sourced</b> from ‘old books and magazines’. Many of the images are ‘portraits of people who’d emigrated’ through Barnes’ ‘home port’ of Liverpool in the early 1900s.|$|R
40|$|First, make a {{tool that}} has color {{transfer}} function using “matlab (GUI) ” or “MFC(C++) ”. Here {{is an example of}} a color transfer tool. It should have functions: open two <b>images</b> (<b>source</b> and target), viewers (source, target, and result) and execute button. Any type of images like “jpg, bmp, tif, png ” should be openned using “Open tab”. <b>Source</b> and target <b>images</b> sholud be shown in image viewers. “Execute ” will run actual color transfer algorithm and show results in another viewer. Result shows color transferred <b>images</b> from <b>source</b> and target <b>images.</b> It should have same color tone of a target image and same scene of a <b>source</b> <b>image.</b> Reference...|$|R
40|$|The {{editorial}} team reserves {{the right to}} refuse articles inappropriate for inclusion in this publication. Opinions expressed in this publication represent {{the views of the}} authors and do not necessarily reflect the opinions of Deakin University <b>Images</b> <b>sourced</b> from Google and academic private collections. ...|$|R
50|$|Liquid crystal light valves work by {{rotating}} light between two polarizing filters. Due to these internal polarizers, LCD shutter-glasses darken the display image of any LCD, plasma, or projector <b>image</b> <b>source,</b> {{which has the}} result that images appear dimmer and contrast is lower than for normal non-3D viewing. This {{is not necessarily a}} usage problem; for some types of displays which are already very bright with poor grayish black levels, LCD shutter glasses may actually improve the image quality.|$|E
50|$|A {{projection}} screen is an installation {{consisting of a}} surface and a support structure used for displaying a projected image for the view of an audience. Projection screens may be permanently installed, as in a movie theater; painted on the wall; or portable with tripod or floor rising models. as {{in a conference room}} or other non-dedicated viewing space. Another popular type of portable screens are inflatable screens for outdoor movie screening (open air cinema). Uniformly white or grey screens are used almost exclusively as to avoid any discoloration to the image, while the most desired brightness of the screen depends on a number of variables, such as the ambient light level and the luminous power of the <b>image</b> <b>source.</b> Flat or curved screens may be used depending on the optics used to project the image and the desired geometrical accuracy of the image production, flat screens being the more common of the two. Screens can be further designed for front or back projection, the more common being front projection systems, which have the <b>image</b> <b>source</b> situated {{on the same side of}} the screen as the audience.|$|E
50|$|Michael Atkinson (born 1962) is an American writer, {{poet and}} film critic. His debut novel is Hemingway Deadlights (St. Martin's Press/Minotaur Books, 2009), {{and he has}} written film and culture critiques for The Believer, Sight & Sound, The Guardian, Film Comment, The Village Voice, In These Times, True/Slant, Spin, Maxim, The Boston Phoenix, Details, The Progressive, Moving <b>Image</b> <b>Source,</b> IFC.com, TCM.com, Movieline, The Poetry Foundation, Chicago Reader, LA Weekly, The Stranger, The American Prospect, City Paper (Baltimore), Modern Painters, and other publications.|$|E
40|$|The {{inadequacy}} {{of the various}} theories which have been proposed for finding the reverberation time of rooms {{can be explained by}} an attempt to examine what might occur at a listening point when <b>image</b> <b>sources</b> of determined acoustic power are added to the actual source. The number and locations of the <b>image</b> <b>sources</b> are stipulated. The intensity of sound at the listening point can be calculated by means of approximations whose conditions for validity are given. This leads to the proposal of a new expression for the reverberation time, yielding results which fall between those obtained through use of the Eyring and Millington formulae; these results are made to depend on the shape of the room by means of a new definition of the mean free path...|$|R
25|$|Using {{the core}} engine and the map/satellite images hosted by Google, such tools can {{introduce}} custom location icons, location coordinates and metadata, and even custom map <b>image</b> <b>sources</b> into the Google Maps interface. The script-insertion tool Greasemonkey provides {{a large number}} of client-side scripts to customize Google Maps data.|$|R
30|$|Based on {{previous}} research [15, 16] {{and our own}} study (described in Section 3), most ADLs achieve significantly higher accuracy when using vision-based data than with motion sensor-based data. Thus, in many cases, if the three sources of information are fused directly, {{the accuracy of the}} output is often affected by the low specificity of the motion sensors. However, we still need to use motion sensors to identify ADLs that have significant motion signatures, such as “cleaning,” “walking outside,” and “lying down.” Therefore, considering the reliability of each information source, we consider user knowledge and <b>image</b> <b>sources</b> to be high-priority data and the motion sensor source to be low-priority data, i.e., we supplement the sensor information only when the fusion of user knowledge and <b>image</b> <b>sources</b> leads to a conflict.|$|R
