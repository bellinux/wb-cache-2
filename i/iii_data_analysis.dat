14|10000|Public
40|$|Objective: To {{develop a}} {{checklist}} for explicit and comprehensive reporting of qualitative studies (indepth interviews and focus groups). Results: Items most frequently {{included in the}} checklists related to sampling method, setting for data collection, method of data collection, respondent validation of findings, method of recording data, description of the derivation of themes and inclusion of supporting quotations. We grouped all items into three domains: (i) research team and reflexivity, (ii) study design and (<b>iii)</b> <b>data</b> <b>analysis</b> and reporting. Conclusions: The criteria included in COREQ, a 32 -item checklist, can help researchers to report {{important aspects of the}} research team, study methods, context of the study, findings, analysis and interpretations...|$|E
40|$|We {{present a}} new method for near-real-time {{monitoring}} of surface displacements due to landslide phenomena, namely ADVanced dIsplaCement monitoring system for Early warning (ADVICE). The procedure includes: (i) data acquisition and transfer protocols; (ii) data collection, filtering, and validation; (<b>iii)</b> <b>data</b> <b>analysis</b> and restitution through {{a set of}} dedicated software; (iv) recognition of displacement/velocity threshold, early warning messages via SMS and/or emails; (v) automatic publication of the results on a dedicated webpage. We show how the system evolved and the results obtained by applying ADVICE over three years into a real early warning scenario relevant to a large earthflow located in southern Italy. ADVICE has speed-up and facilitated the understanding of the landslide phenomenon, the communication of the monitoring results to the partners, and consequently the decision-making process in a critical scenario. Our work might have potential applications not only for landslide monitoring but also in other contexts, as monitoring of other geohazards and of complex infrastructures, as open-pit mines, buildings, dams, etc...|$|E
40|$|International audienceMetagenomics data {{analyses}} from independent studies {{can only be}} compared if the analysis workflows are described in a harmonized way. In this overview, we have mapped the landscape of data standards available for the description of essential steps in metagenomics: (i) material sampling, (ii) material sequencing, (<b>iii)</b> <b>data</b> <b>analysis,</b> and (iv) data archiving and publishing. Taking examples from marine research, we summarize essential variables used to describe material sampling processes and sequencing procedures in a metagenomics experiment. These aspects of metagenomics dataset generation have been to some extent addressed by the scientific community, but greater awareness and adoption is still needed. We emphasize the lack of standards relating to reporting how metagenomics datasets are analysed and how the metagenomics data analysis outputs should be archived and published. We propose best practice {{as a foundation for}} a community standard to enable reproducibility and better sharing of metagenomics datasets, leading ultimately to greater metagenomics data reuse and repurposing...|$|E
40|$|Why map the {{cerebellar}} surface? Because: (i) {{organization of}} motor, sensory and cognitive {{functions in the}} human cerebellum is poorly understood [1], (ii) the highly-folded structure conceals most of the cortical surface, and (<b>iii)</b> functional <b>data</b> <b>analysis</b> may improve with incorporation of surface organization [2]. We constructed a pipeline for extracting, labelling, and conformally mapping cerebellar surfaces from individuals' coregistered anatomical and functional volumes. Conformal mappings preserve angular information; other approaches attempt to minimize areal and metric distortions [3]...|$|R
40|$|Biological <b>data</b> <b>analysis</b> is {{typically}} implemented using a deep pipeline that combines {{a wide array}} of tools and databases. These pipelines must scale to very large datasets, and consequently require parallel and distributed computing. It is therefore important to choose a hardware platform and underlying data management and processing systems well suited for processing large datasets. There are many infrastructure systems for such data-intensive computing. However, in our experience, most biological <b>data</b> <b>analysis</b> pipelines do not leverage these systems. We give an overview of data-intensive computing infrastructure systems, and describe how we have leveraged these for: (i) scalable fault-tolerant computing for large-scale biological data; (ii) incremental updates to reduce the resource usage required to update large-scale compendium; and (<b>iii)</b> interactive <b>data</b> <b>analysis</b> and exploration. We provide lessons learned and describe problems we have encountered during development and deployment. We also provide a literature survey on the use of data-intensive computing systems for biological data processing. Our results show how unmodified biological <b>data</b> <b>analysis</b> tools can benefit from infrastructure systems for data-intensive computing...|$|R
40|$|PTRwid is a {{fast and}} user {{friendly}} tool {{that has been}} developed to process data from proton-transfer-reaction time-of-flight mass spectrometers (PTR-TOF-MS) that use HTOF (high-resolution time-of-flight) mass spectrometers from Tofwerk AG (Switzerland). PTRwid is designed for a comprehensive evaluation of whole laboratory or field-based studies. All processing runs autonomously, and entire laboratory or field campaigns can, in principle, be processed with a few mouse clicks. Unique features of PTRwid include (i) an autonomous and accurate mass scale calibration, (ii) the computation of a "unified mass list" that – {{in addition to a}} uniform data structure – provides a robust method to determine the precision of attributed peak masses, and (<b>iii)</b> fast <b>data</b> <b>analysis</b> due to well considered choices in data processing...|$|R
40|$|Environmental {{pollution}} in the urban areas of Hong Kong has become a serious public issue but most urban inhabitants have no means of judging their own living environment in terms of dangerous threshold and overall livability. Currently there exist many low-cost sensors such as ultra-violet, temperature and air quality sensors that provide reasonably accurate data quality. In this paper, the development and evaluation of Integrated Environmental Monitoring System (IEMS) are illustrated. This system consists of three components: (i) position determination and sensor data collection for real-time geospatial-based environmental monitoring; (ii) on-site data communication and visualization {{with the aid of}} an Android-based application; and (<b>iii)</b> <b>data</b> <b>analysis</b> on a web server. This system has shown to be working well during field tests in a bus journey and a construction site. It provides an effective service platform for collecting environmental data in near real-time, and raises the public awareness of environmental quality in micro-environments. Department of Land Surveying and Geo-Informatic...|$|E
40|$|The {{synthesis}} of gold nanoparticles (Au NPs) 15, 26, and 34 nm in diameter, {{followed by the}} investigation of their size-dependent optical and catalytic properties, is described herein as an undergraduate level experiment. The proposed experiment covers concepts on the synthesis, stabilization, and characterization of Au NPs, their size-dependent optical and catalytic properties at the nanoscale, chemical kinetics, {{and the role of}} a catalyst. The experiment should be performed by groups of two or three students in three lab sessions of 3 h each and organized as follows: i) {{synthesis of}} Au NPs of different sizes and investigation of their optical properties; ii) evaluation of their catalytic activity; and <b>iii)</b> <b>data</b> <b>analysis</b> and discussion. We believe that this activity enables students to integrate these multidisciplinary concepts in a single experiment as well as to become introduced/familiarized with an active research field and current literature in the areas of nanoparticle synthesis and catalysis...|$|E
40|$|Important {{factors in}} a top down systems biology study. In top down systems biology, {{the answer to}} a certain {{biological}} question is sought in the systems wide response of a biological system to the chosen experimental conditions. The response of the biological system is measured with -omics tools, such as, transcriptomics or metabolomics, and advanced data analysis tools are applied to extract biologically relevant information from the measurements. The success of a top down systems biology approach is highly dependent on the information richness of the data obtained from the -omics measurements. Therefore, it is essential for a successful top down systems biology study to balance the three key factors: (i) biological question, (ii) experimental design, and (<b>iii)</b> <b>data</b> <b>analysis.</b> In Chapter 1, we discuss these three key factors, their interdependence, and their significance for successful top down systems biology. In Chapters 2 to 6, different aspects of the relation between a biological question and a data analysis strategy, such as, data pretreatment and selection of the most suited data analysis method, are further explored...|$|E
40|$|The {{objective}} of the course is to introduce a series of statistical methods that will enable urban planning and policy students to: (i) formulate empirical problems {{for the purpose of}} making planning/policy decisions (ii) determine an appropriate method for estimating parameters (<b>iii)</b> conduct exploratory <b>data</b> <b>analysis</b> (iv) develop a model that is appropriate to the problem and the data and finally (v) to diagnose problems with the model and to validate the model. The broad framework of regression and related methods will be introduced using policy and planning-related examples as case studies. The instruction will emphasize upon the exploratory nature of empirical planning and policy analysis. To this end, the course will focus heavily on exploratory <b>data</b> <b>analysis</b> and model diagnostics. Students will work on a number of planning and policy-related examples that will highlight the methods. For this purpose, a number of datasets will be distributed in class, using which, students will be able to develop statistical models using, in many cases, real-life examples. Specific topics will include (i) linear and nonlinear regression (ii) qualitative and limited dependent variable models (<b>iii)</b> categorical <b>data</b> <b>analysis</b> and (iv) spatial analysis. The estimation techniques of (i) least squares and (ii) maximum likelihood will be covered. The relationships among different classes of models will be shown. We will begin the course with a review of random variables, probability distributions and matrices. The course will involve heavy usage of SAS, the Statistical Analysis System and Stata. Most techniques that we will cover will be implemented in the computer via appropriate examples. SAS is available for purchase at a heavily discounted price by students; go t...|$|R
40|$|Accepted {{manuscript}} version. The final publication {{is available}} at Springer via [URL]. Biological <b>data</b> <b>analysis</b> is typically implemented using a deep pipeline that combines {{a wide array of}} tools and databases. These pipelines must scale to very large datasets, and consequently require parallel and distributed computing. It is therefore important to choose a hardware platform and underlying data management and processing systems well suited for processing large datasets. There are many infrastructure systems for such data-intensive computing. However, in our experience, most biological <b>data</b> <b>analysis</b> pipelines do not leverage these systems. We give an overview of data-intensive computing infrastructure systems, and describe how we have leveraged these for: (i) scalable fault-tolerant computing for large-scale biological data; (ii) incremental updates to reduce the resource usage required to update large-scale compendium; and (<b>iii)</b> interactive <b>data</b> <b>analysis</b> and exploration. We provide lessons learned and describe problems we have encountered during development and deployment. We also provide a literature survey on the use of data-intensive computing systems for biological data processing. Our results show how unmodified biological <b>data</b> <b>analysis</b> tools can benefit from infrastructure systems for data-intensive computing...|$|R
40|$|AbstractThe {{influence}} of foreign ownership on bank performance has attracted special attention due to international consolidation and cross-border activities in banking system. Foreign ownership of {{banks in the}} developing and transitional countries has increased considerably in recent years. Thus, whether foreign ownership contributes to bank performance in developing countries has emerged as an important question. This study aims to compare the performance of foreign and Turkish banks by using Multi Criteria Decision Making methodologies namely; Topsis, Electre <b>III</b> and <b>Data</b> Envelopment <b>Analysis.</b> Findings declare that foreign banks show better performance than domestic ones...|$|R
40|$|The {{importance}} of body image dissatisfaction and identity development among women {{is becoming a}} significant concern when considering {{the risk of developing}} eating disorders. Analysis of the two variables suggests that {{further research is needed to}} determine new and effective treatments for body image dissatisfaction and eating disorders. The present study examines the relationship between identity development and body image dissatisfaction in college females. Two scales, the Multidimensional Body-Self Relations Questionnaire—Appearance Scales and the Tennessee Self Concept Scale were administered to 15 participants. Scores for the two instruments were correlated to determine whether a relationship exists between the two constructs with the purpose of possibly developing a new avenue for treatment of body image dissatisfaction. <b>iii</b> <b>Data</b> <b>analysis</b> using the Pearson’s r correlation coefficient suggests a positive correlation exists between some aspects of self-concept and level of body image dissatisfaction. Specific relationships found were: satisfaction with body areas and satisfaction with identity in terms of self-concept, satisfaction with body areas and how one is accepted by others, and identity in terms of self-concept and satisfaction wit...|$|E
40|$|This {{research}} paper presents an implementation {{of one of}} the Lean Manufacturing (LM) tools, known as Standardized Work (SW), at an automotive assembly line in Malaysia. The main functions of SW are to design, develop, document and visualize a set of manufacturing process with detail and proper study on it. SW is conducted to raise production efficiency and quality. Quality here referred to quality of the products to be produced and quality of the jobs to be performed. With the main objective to demonstrate a systematic and organized guideline on how data collection and analysis should be conducted with detail explanation on tools and methods used for efficient implementation of SW, a case study was conducted at an automotive components assembly line, at XYZ Manufacturing Sendirian Berhad. There are five major phases involved in the implementation of this research work; (i) observation, to understand the existing study area conditions, (ii) data collection through observation, referring to company’s production system and work study method, (<b>iii)</b> <b>data</b> <b>analysis</b> using SW tools, (iv) development of standardized process, and (v) results evaluation by using lean metrics to analyze the performance of the standardized assembly process. results show that, with systematic and proper implementation, SW offer a lots of benefits such as help to increase efficiency and quality and process stability in terms of productivity, quality and operator’s performance. Moreover, it also helps to achieve LM philosophies which are wastes elimination and continuous improvement during the implementation...|$|E
40|$|The {{purpose of}} the study was to {{determine}} the effects of shoe outsole design on maximum vertical acceleration and select kinematic parameters during level and uphill walking across phases of stance. Twelve participants, 7 males (75. 1 ± 9. 3 kg, 173. 6 ± 3. 6 cm, 22. 9 ± 3. 5 yrs) and 5 females (56. 5 ± 5. 1 kg, 158. 3 ± 4. 5 cm, 25. 4 ± 11. 1 yrs) granted written consent and preferred walking speed was determined. An accelerometer (480 Hz) was attached to the distal leg to measure maximum leg acceleration (Aleg) and an electrogoniometer (480 Hz) was placed on the back to measure sagittal lumbar motion (LumbarROM). Sagittal video capture (60 Hz) included thigh range of motion (ThighROM), knee range of motion (KneeROM) and ankle range of motion (AnkleROM). Participants walked on a treadmill in each of four randomized conditions: 1) Rounded outsole shoe (ROS) at 0 % incline, 2) Traditional outsole shoe (TOS) at 0 % incline, 3) ROS at 5 % incline and 4) TOS at 5 % incline. Participants walked at 10 % greater than preferred pace. Data were obtained from five consecutive gait cycles. For each gait cycle, stance was normalized to 100 %. Stance phase was further divided into Phase I, II, and <b>III.</b> <b>Data</b> <b>analysis</b> included a 2 (shoe) x 2 (incline) repeated measure ANOVA (α= 0. 05) for ALEG, LumbarROM, ThighROM, KneeROM, and AnkleROM at each phase of stance (Phase I, II, and III). Post hoc paired t-Tests were performed for significant interaction. Results show seven of the fifteen dependent variables for shoe differed significantly (...|$|E
5000|$|Monitor <b>III</b> <b>Data</b> Gatherer {{collects}} {{data for}} short-term and immediate <b>data</b> <b>analysis.</b> The <b>data</b> is collected in intervals ranging from 10 seconds to 10 minutes. The data is written to RMF Monitor <b>III</b> VSAM <b>data</b> sets and internally recorded in a wraparound buffer. The data gatherer also collects some special data for long-term <b>data</b> <b>analysis</b> and records them in RMF SMF records too.|$|R
40|$|In this paper, {{we report}} on {{experience}} in building and deploying an operational Internet broadcast system based on Overlay Multicast. In over a year, the system has been providing a cost-e#ective alternative for Internet broadcast, used by over 3600 users spread across multiple continents in home, academic and commercial environments. Technical conferences and special interest groups are the early adopters. Our experience confirms that Overlay Multicast can be easily deployed and can provide reasonably good application performance. The experience has led us to identify first-order issues that are guiding our future e#orts and are of importance to any Overlay Multicast protocol or system. Our key contributions are (i) enabling a real Overlay Multicast application and strengthening the case for overlays as a viable architecture for enabling group communication applications on the Internet, (ii) the details in engineering and operating a fully functional streaming system, addressing {{a wide range of}} real-world issues that are not typically considered in protocol design studies, and (<b>iii)</b> the <b>data,</b> <b>analysis</b> methodology, and experience that we are able to report given our unique standpoint...|$|R
40|$|The {{advent of}} Social Networking Systems (SNS) has {{introduced}} new possibilities for planners to refine and extend conventional engagement and data-gathering techniques by leveraging user-contributed, spatially-referenced content freely available online. This study examines {{the use of}} SNS content as community input, complementing input gathered through traditional participatory processes such as workshops, public comment hearings, and charrettes. Four case studies of recent community planning projects in the United States are analyzed, comparing the data gathered from traditional participatory processes with available SNS content related to each project study area, to determine {{to what extent the}} inclusion of SNS data would improve the overall data- gathering efforts of these projects. Three significant findings emerge from this analysis: (i) that SNS <b>data</b> <b>analysis</b> can positively complement data gathered from traditional participatory processes, (ii) that although SNS <b>data</b> <b>analysis</b> can provide useful data to planners, it is not a direct replacement for conventional engagement techniques, and (<b>iii)</b> that SNS <b>data</b> <b>analysis</b> is most effective for projects in neighborhoods with a well- defined identity. The study also examines the characteristics of effective SNS <b>data</b> <b>analysis</b> integration and discusses broader implications for planning practitioners and additional research needed...|$|R
40|$|Background and topic: Representing an {{epistemological}} shift within qualitative methodology (Boydell, 2012) healthcare research has increasingly employed visual methods {{as a means}} to further understand the patient’s experience of health and healthcare (Broadbent, 2009; Phillips et al, 2015). An advantage of using drawing, rather than any other form visual method, is its potential to offer a way of communicating other than speech. We discuss the use of drawing, in an exploratory, qualitative study, to enrich the narrative account during data collection using semi-structured interviews with a purposive sample of palliative healthcare professionals (n= 16) from one hospice in West Yorkshire, England (February-May 2016). This study aimed to utilise drawing as a tool to explore the process of drawing to help facilitate the exploration, communication and our understanding of how healthcare staff emotionally resource their roles within a Hospice setting. As such, we were not concerned with an end product, such as a representational image, that would lend itself to measurement and quantification, but the process of facilitation. Aims: To outline and debate the use of drawing, as a visual imagery method, within the research process and (ii) provide a critical reflection of the use of drawing in the research process. Methodological discussion: Discussion will focus on the (i) the practicalities of undertaking drawing during data collection using semi-structured interviews i. e. participant preparation, informed consent and dynamics (ii) participants perspectives in undertaking drawing during data collection using semi-structured interviews i. e. concern with the production of a ‘good’ picture (iii) the practicalities of undertaking data analysis. The presentation will then debate what this visual imagery method adds to: (i) data collection (ii) the the narrative account and (<b>iii)</b> <b>data</b> <b>analysis.</b> Conclusion: This paper will outline the use of drawing as a data collection tool alongside semi-structured interviews to enrich the narrative account...|$|E
40|$|Genomes are {{tremendous}} co-evolutionary holistic {{systems for}} molecular storage, processing and fabrication of information. Their system-biological complexity remains, however, still largely mysterious, despite immense sequencing achievements and huge {{advances in the}} understanding of the general sequential, three-dimensional and regulatory organization. Here, we present the GLOBE 3 D Genome Platform a completely novel grid based virtual “paper” tool and in fact the first system-biological genome browser integrating the holistic complexity of genomes in a single easy comprehensible platform: Based on a detailed study of biophysical and IT requirements, every architectural level from sequence to morphology of one or several genomes can be approached in a real and in a symbolic representation simultaneously and navigated by continuous scale-free zooming within a unique three-dimensional OpenGL and grid driven environment. In principle an unlimited number of multi-dimensional data sets can be visualized, customized in terms of arrangement, shape, colour, and texture etc. as well as accessed and annotated individually or in groups using internal or external data bases/facilities. Any information can be searched and correlated by importing or calculating simple relations in real-time using grid resources. A general correlation and application platform for more complex correlative analysis and a front-end for system-biological simulations both using again the huge capabilities of grid infrastructures is currently under development. Hence, the GLOBE 3 D Genome Platform {{is an example of a}} grid based approach towards a virtual desktop for genomic work combining the three fundamental distributed resources: i) visual data representation, ii) data access and management, and <b>iii)</b> <b>data</b> <b>analysis</b> and creation. Thus, the GLOBE 3 D Genome Platform is the novel system-biology oriented information system urgently needed to access, present, annotate, and to simulate the holistic genome complexity in a unique gateway towards a real understanding, educative presentation and curative manipulation planning of this tremendous evolutionary information grail – genomes...|$|E
40|$|Objectives A {{discussion}} {{of the meaning of}} reflexivity applying a model post hoc in a conducted and published grounded theory research project. Background Qualitative methods are well integrated in health care research. Its widespread use goes together with a need to attend questions of rigor and reflexivity as a qualitative researcher. This {{is due to the fact}} that is hardly possible for qualitative researchers to be fully objective of social phenomena because of their personal, contextual, political and social stance in the field of interest. Literature suggests that reflexivity needs to be considered throughout across the different stages of a research project. A number of approaches to reflexivity are available to researchers. Methods A model is applied to support reflexivity in a qualitative project in health care. This model has been proposed by other qualitative researchers and is based on arguments from Bourdieu and Finlay. It addresses the subjective relation to the object at three levels: (i) the overall social space, (ii) the field of specialists and the (iii) scholastic universe and overlays these with three stages of research: (i) pre-research, (ii) data collection and (<b>iii)</b> <b>data</b> <b>analysis.</b> Results The intersections of the subjective relation to the object at three levels with the three stages of research support the researcher to consider and challenge reflexivity. A matrix will be produced and displayed discussing the researcher’s methodological post hoc decisions on an already conducted and published research project. Based on this reflective exercise of quality appraisal, a storyline will be offered that discusses what junior researchers should know/do when engaging in a qualitative study. Non-optimal choices on the assumptions and methodological decisions that are part of early career in science shall be highlighted. Discussion Being reflexive requires the researcher to be open about decisions made during the research process. Additionally it can help prevent one’s own work being vulnerable to critiques from others. The model that is proposed in this project is a useful tool to enhance reflexivity in qualitative research. Implications for young researchers Young researchers are encouraged to explore the concept of reflexivity in their research practices and to apply models of reflexivity to create transparency and robustness of their research. The presented strategy may help young researchers in their reflexive endeavours. status: accepte...|$|E
40|$|Abstract: 2 ̆ 2 In this paper, {{we report}} on {{experience}} in building and deploying an operational Internet broadcast system based on Overlay Multicast. In over a year, the system has been providing a cost-effective alternative for Internet broadcast, used by over 3600 users spread across multiple continents in home, academic and commercial environments. Technical conferences and special interest groups are the early adopters. Our experience confirms that Overlay Multicast can be easily deployed and can provide reasonably good application performance. The experience has led us to identify first-order issues that are guiding our future efforts and are of importance to any Overlay Multicast protocol or system. Our key contributions are (i) enabling a real Overlay Multicast application and strengthening the case for overlays as a viable architecture for enabling group communication applications on the Internet, (ii) the details in engineering and operating a fully functional streaming system, addressing {{a wide range of}} real-world issues that are not typically considered in protocol design studies, and (<b>iii)</b> the <b>data,</b> <b>analysis</b> methodology, and experience that we are able to report given our unique standpoint. 2 ̆...|$|R
40|$|In the {{literature}} {{there are several}} tools that aim to assess organizational performance, such as Balanced Scorecard (BSC), National Quality Award (PNQ), Multicriteria Methodology for Decision Aiding (MCDA), <b>Data</b> envelopment <b>analysis</b> (DEA), Analytic Hierarchy Process (AHP), among others. However, it is perceived in practice doubts as to choose the most appropriate for each organizational context. In this sense, {{it is argued that}} to measure the performance of the organization it is necessary that this measure is based on an instrument that allows transparency in the evaluation form, and insert those interested in the evaluation process. The credibility of an evaluation therefore depends on the ability to produce consistent information for all subjects involved in the intervention. The search for tools to improve the way to evaluate organizational performance it is increasingly necessary. The objective {{of this study is to}} identify the main tools of organizational performance assessment published in national journals in the area of Administration, Accounting and Tourism, classified by CAPES Qualis A 1 to B 5. The study is characterized as descriptive and qualitative documentation. We analyzed 489 articles, which resulted in the following results:(i) identified the main theme of the authors and regular assessment of organizational performance, (ii) from the year 2000 publications on the subject grew substantially, (<b>iii)</b> <b>Data</b> Envelopment <b>Analysis</b> (DEA), Balanced Scorecard (BSC) and Multicriteria for decision aiding methodology(MCDA-C) are the most commonly used tools for evaluating performance, (iv) most of the tools found in {{the literature}} are multiple criteria, (v) described the process used by the tools to assess organizational performance...|$|R
40|$|This article {{surveys the}} Consumer Financial Protection Bureau 2 ̆ 7 s (2 ̆ 2 CFPB 2 ̆ 2 or the 2 ̆ 2 Bureau 2 ̆ 2) {{activities}} over its first five years, {{with an emphasis}} on publicly available empirical data. Part II provides an overview of the CFPB’s creation and operations, including discussion of its structure and powers. Part <b>III</b> provides <b>data</b> and <b>analysis</b> on the CFPB’s activities across its three primary functional areas—rulemaking, supervision, and enforcement— for the five-year period from July 21, 2011, to July 21, 2016. Part IV summarizes major developments in the CFPB’s sixth year and the agency’s stated policy priorities through 2017. Finally, Part V concludes with a discussion of potential CFPB reforms in 2017 and the possible impact of a court opinion holding the Bureau Director’s tenure is not protected...|$|R
40|$|Water quality {{describes}} the physical, {{chemical and biological}} characteristic of water that relates to human and nature life. Water quality parameters consist of Total Suspended Solids (TSS), turbidity, Secchi Disk Depth (SDD), temperature and colour. Example of factors affecting the quality of water bodies (i. e. lakes and reservoirs) are urban runoff, solid waste and soil erosion. Several methods i. e. conventional method, satellite remote sensing method, and digital camera method {{can be used to}} determine water quality parameters. Conventional method involves collecting water sample and laboratory analysis. This method is time consuming and not efficient to represent water quality of large area. Satellite remote sensing techniques using various sensors i. e. Landsat, Moderate Resolution Imaging Spectroradiometer (MODIS), IKONOS and Tiungsat- 1 can be used to assess several water quality parameters (i. e. TSS, turbidity, chlorophyll and SDD) on large area coverage. Although this method can be used to monitor water quality of large area, image acquisition and data processing are costly and time consuming. A number of studies have utilized digital cameras to determine the water quality parameters; the results are still not conclusive. The aim {{of this study is to}} access the potential use of low-cost digital cameras in water quality assessment. Based on this aim, the objectives are i) to determine the relationship between digital number (DN) and TSS concentrations using different digital cameras; ii) to determine turbidity concentrations based on the calculated TSS values and iii) to compare TSS and turbidity concentration based on different models, different digital cameras and different dates of study area. The study areas for this research are two lakes located at Section 7 and Section 14 (Taman Tasik Shah Alam). This study only concentrates on two water quality parameters (i. e. TSS and turbidity). Two digital cameras (i. e. Olympus FE- 100 and I-iC 2070) each costing less than RM 1000 were used to capture image of the water surface. The methodology adopted for this research involve i) collecting water samples, ii) capturing digital images of water surface, <b>iii)</b> <b>data</b> <b>analysis</b> (including regression analysis and generating water quality maps of the study area). Water samples collected in these two lakes were processed in the laboratory to determine the TSS and turbidity concentrations. Digital images captured on three different dates i. e. 1 st and 15 th September 2006 and 23 rd November 2006 were used to determine the regression models (or relationship between DN and water quality concentration) ...|$|E
40|$|This is an {{exploratory}} thesis but nevertheless methodological in nature {{which aims to}} examine {{the physical and emotional}} needs of the mentally ill and to compare those needs with the nursing care given. It also examines nursing workload in four wards within a large psychiatric hospital. The study was organisationally motivated with nurse managers expressing an interest in the outcome. A small working party comprising of charge nurses and nurse managers was convened to advise the Researcher on aspects of psychiatric nursing and hospital administration. Consultants, the Psychologist and staff representatives were kept informed of the study's progress. A review of pertinent literature v/as undertaken and this helped to form criteria from which the physical and emotional needs categories were derived and the research design constructed. The research methodology necessitated three different research designs and there- fore three separate studies v/ere undertaken. The first study v/as concerned with the development of a patient state assessment form which included nursing care requirements of a physical, technical and emotional nature. This form was. used by nurses to assess the patients' needs. The second study examined patient activity and the amount of observed nurse/patient interaction. The third examined nursing workload. The method used for studies 2 and 3 was systematic observation at 5 minute intervals in four wards selected for study. The sample v/as the total patient and nurse population in each of the wards studied. The systematic observation was carried out by the Researcher plus four other observers. The observations were continued from 7 am - 7 pm daily for 14 days on each ward giving a total of 194, 229 observations for Study II and 26, 745 for Study <b>III.</b> <b>Data</b> <b>analysis</b> v/as carried out using a computer. It was necessary to have programmes written especially for the data analysis; use was also made of the Statistical Package- for the Social Services (SPSS). The nurses using the patient state assessment form reported greater awareness of patients' needs and placed greater emphasis on emotional need. The patient state forms when analysed indicated different care requirements for different categories of patients. The psychogeriatric patients required totally different care from patients admitted to the acute admission ward. It was evident (and to be expected) that both physical and emotional dependency increases with advancing age. The observed nurse/patient interaction suggested that interaction is centred more around basic nursing tasks than around nurse/patient discourse. It was also observed there v/as little doctor/patient contact. The results indicate it is the least 'skilled' who have most contact with patients. The care given is predominantly task orientated, offering long-term supportive care with minimum medical intervention. Similarly, nursing officer involvement with staff and patients at ward level was minimal, neither was there any formal teaching observed on the wards. There were no directed group discussions nor multi-disciplinary group meetings during the period of study. Data from all wards suggests 'task' rather than individual patient centred care, with a great dependence on the use of psychotropic drugs. Likewise, the nursing workload was basic care. Staff meal breaks were observed to amount for 1 / 10 th of the total working time and nurses left to man-the-wards during mealtimes appeared under pressure. It was also observed that as the patient/ nurse ratio increases the nurse exerts control by whatever means she can often reverting to a 'herding' type of care...|$|E
40|$|The {{purpose is}} to examine investigative  work in cases of sexual abuse in Sweden. A {{systematic}} search for common problem areas in the investigative process was made in 15 cases with extensive documentation. In at least {{two thirds of the}} cases four problem themes were found to be important: I. Investigation strategy and generation of data, II. Documentation of <b>data,</b> <b>III.</b> <b>Analysis</b> work, IV. Ethics. The main conclusion is that there are serious defects in the investigative methods in all cases reviewec. The investigative methods are biased and incompatible with the objectivity principle stated in Swedish laws. There is a need for critical and scientific thinking in investigative work. Mina erfarenheter från en avsevärt större mängd senare fall till dags dato motsäger inte resultaten i denna granskning. Konferensföreläsning, 15 sidor...|$|R
40|$|Culex quinquefasciatus Say. is {{the main}} vectors in the {{transmission}} of filariasis disease in Indonesia. The use of plant-based insecticide is an alternative to reduce {{the negative impact of}} chemical insecticides. One of them is by using tobacco leaf extract. The {{purpose of this study was}} to determine the deadly power of tobacco leaf extract (Nicotiana tobacum L.) on mortality of larvae Culex quinquefasciatus Say. The type of this research is an experimental research with Posttest Only Control Group Design. The parameters observed are mortality of LC 50 and LC 90 larvae Culex quinquifasciatus Say. by using probit regression analysis. The population were all larvae Culex quinquefasciatus Say. instar <b>III.</b> The <b>data</b> <b>analysis</b> technique is by using One-Way Anova testing followed by the Games-Howell test. The results of this study indicate that LC 50 and LC 90 concentrations of tobacco leaf extracts are 0. 058 % and 0. 095 %. Based on One-Way Anova test is known that p value = 0. 001 (p < 0. 05). Games-Howell test results shows that the concentrations of 0. 031 %, 0. 066 %, and 0. 095 % have significant differences in the average mortality of larvae due to having a different letter notation. From this research results, it can be concluded that the tobacco leaves extract (Nicotiana tobacum L.) has a deadly power to larvae Culex quinquefasciatus Say. as larvicides. However, the need for further research regarding the killing power of tobacco leaf extract in the ratio of the amount of time the extract...|$|R
40|$|Background: Chemoradiation {{is one of}} the {{treatments}} of cervical carcinoma. Cisplatin is the most often used chemotherapy drugs. It has a nephrotoxic effect. Radiation on pelvic area can cause narrowing ureter that yields to impairment of renal function. Objective: To find out the effect of chemoradiation in cervical carcinoma patients with renal function, as shown from the differences of urea and creatinine levels before and after chemoradiation. Methods: This was quasi-experimental study with time series design. Data used were taken from medical records of patients with cervical carcinoma in RSUP dr. Kariadi Semarang, consisted of laboratory results of urea and creatinine levels before chemoradiation, after chemoradiation I, II, and <b>III.</b> <b>Data</b> were <b>analysis</b> using Friedman and Wilcoxon Signed Ranks test. Result: The mean urea levels of 28 patients in this study prior to chemoradiation, after I, II, III, and IV measurements were 24. 18 mg/dL, 25. 6 mg/dL, 23. 36 mg/dL, and 21. 25 mg/dL, respectively. Meanwhile, the mean creatinine levels were 0. 82 mg/dL, 0. 89 mg/dL, 0. 87 mg/dL, and 0. 82 mg/dL. Friedman test found significant differnces in levels of urea (p= 0. 013) and no significant differences in creatinine levels (p= 0. 353). In the Wilcoxon test of urea and creatinine levels obtained no significant differences between before and after chemoradiation I (p= 0. 386 and p= 0. 124), chemoradiation II (p= 0. 715 and p= 0. 156), and chemoradiation III (p= 0. 053 and p= 0. 509). Conclusion: There were no significant difference between urea and creatinine levels before and after I, II, and III chemoradiation. Thus, chemoradiation with cisplatin had no effect (safe) on renal function. Keywords: cervical carcinoma, chemoradiation, cisplatin, ureum, creatinin...|$|R
40|$|Non-invasive {{functional}} neuroimaging techniques enable {{researchers to}} study the neurobiological substrates of psychological processes. The increasingly large body of neuroimaging research has two fundamental purposes. The first is to identify the brain regions that underlie a particular psychological process while the second seeks to identify differential responses of these regions to various stimuli or task challenges. The latter focus yields insights into both how the brain accommodates varying task demands and how differences between individuals or between clinical and healthy comparison groups might be explained by differences in neurobiological functioning. To achieve these goals, {{it is essential that}} one be able to isolate the psychological process of interest and how best to do so, with particular regard to experimental design, is the focus of this chapter. Part II will describe issues particular to psychological experimental design, that is, experimental control over the cognitive or emotional process of interest. Part <b>III</b> focuses on <b>data</b> <b>analysis</b> with emphasis on optimizing and isolating the neuroimaging signal in the activated brain regions. Part III also addresses a number of practical matters that confront all researchers when designing their experiments...|$|R
40|$|In {{addition}} to the traditional benefits {{that are associated with}} the installation of structural health monitoring systems, reductions in construction, operational, and maintenance costs and improved performance and quality can be achieved by effectively using the acquired data. However, considered in isolation, the raw data is of little use and value. It needs to be processed and put into a geometric context within the infrastructure asset, which facilitates the interpretation and <b>analysis</b> of the <b>data.</b> This supports informed decision making that leads to effective actions. This study outlines a new approach that enables to model structural performance monitoring systems in a BIM environment and hence permits visualising sensor data directly on BIM models. The paper addresses aspects related to (i) interoperability and standard data models, (ii) management and visualisation of monitoring <b>data,</b> and (<b>iii)</b> <b>data</b> interpretation and <b>analysis.</b> A prestressed concrete bridge, with a comprehensive built-in structural performance monitoring system, {{has been used as a}} case study. The case study demonstrates that by including and visualising monitoring data directly on BIM models, the acquired data gains geometrical context within the built asset, which facilitates better interpretation, analysis, and all the data-sharing benefits associated with the BIM approach. Engineering and Physical Sciences Research Council, Innovate UK (CSIC Innovation and Knowledge Centre (Grant ID: EP/L 010917 / 1) ...|$|R
40|$|Cancer is a {{major concern}} among chronic {{diseases}} today. Spatial epidemiology plays a relevant role in this matter and we present here a review of this subject, including a discussion of the literature in terms of the level of geographic data aggregation, risk factors and methods used to analyse the spatial distribution of patterns and spatial clusters. For this purpose, we performed a websearch in the Pubmed and Web of Science databases including studies published between 1979 and 2015. We found 180 papers from 63 journals and noted that spatial epidemiology of cancer has been addressed with more emphasis during the last decade with research based on data mostly extracted from cancer registries and official mortality statistics. In general, the research questions present in the reviewed papers can be classified into three different sets: i) analysis of spatial distribution of cancer and/or its temporal evolution; ii) risk factors; <b>iii)</b> development of <b>data</b> <b>analysis</b> methods and/or evaluation of results obtained from application of existing methods. This review is expected to help promote research in this area through the identification of relevant knowledge gaps. Cancer’s spatial epidemiology represents an important concern, mainly for public health policies design aimed to minimise the impact of chronic disease in specific populations...|$|R
40|$|This study aims to {{calculate}} the rate of hospitalization in Blitar Aminah General Hospital based on Activity Based Costing System (ABC System). And that {{the focus of the}} research is overhead inpatient services in the VIP room, the ICU, class I, class II, class II, and class <b>III.</b> Stages of <b>data</b> <b>analysis</b> in this study is to classify the activity, determine the cost pool, determine cost drivers and determine rates each inpatient room. The results using the ABC System produces the VIP class fare per person at Rp. 222. 780, 11, ICU Rp. 118. 654, 12 Rp. 220. 534 class I, 18 class II, Rp. 109. 466, 66 and class III Rp. 43. 659, 68. When compared with the traditional method, the method of the ABC System provides lower results in the VIP room, ICU classes and class I and II. so expect more patients are interested in a more competitive rate. The hospital management should consider the calculation of rates of hospitalization services by the ABC method System. This is because the overhead of loading the ABC method is based on the cost driver system is precise and accurate, so that the resulting tariff in accordance with the consumption of each activity. ABC application of adequate human resource systems and high commitment...|$|R
40|$|Responsibility {{accounting}} represent {{accounting system}} acknowledging responsibility centers at a company. Responsibility accounting arise effect of existence of given authority and how justifying {{it in the}} form of report written the. The report {{in the form of}} responsibility reporting able to be used as by base analyse measurement of manager labour capacity to each;every responsibility center. Expense of in responsibility accounting system attributed to manager owning authority to consume resource. Because used resource have to be expressed in set of money and that represent expense, hence responsibility accounting system represent controller method of is expense of conducive of management to do management of expense. Target of which wish to be reached in this research is to know how evaluation applying of responsibility accounting as a means of performance of manajeme center the expense of part of production through case study at PT Standart megah Steelindotama Induestries Pasuruan. <b>iii</b> Pursuant to <b>data</b> <b>analysis</b> and solution, can be concluded that PT Standartmegah Steelindotama Induestries have evaluate applying of mirror responsibility accounting at compilation of budget which have entangled all shares, classification of account code, and compilation of clear organization chart. While for the classification of expense, PT Standartmegah Steelindotama Induestries has not applied the accountability accounting syste...|$|R
40|$|In {{order to}} {{understand}} gene regulation, accurate and comprehensive knowledge of transcriptional regulatory elements is essential. Here, we report our efforts in building a mammalian Transcriptional Regulatory Element Database (TRED) with associated <b>data</b> <b>analysis</b> functions. It collects cis- and trans-regulatory elements and is dedicated to easy <b>data</b> access and <b>analysis</b> for both single-gene-based and genomescale studies. Distinguishing features of TRED include: (i) relatively complete genome-wide promoter annotation for human, mouse and rat; (ii) availability of gene transcriptional regulation information including transcription factor binding sites and experimental evidence; (<b>iii)</b> <b>data</b> accuracy is ensured by hand curation; (iv) efficient user interface for easy and flexible data retrieval; and (v) implementation of on-the-fly sequence analysis tools. TRED can provide good training datasets for further genome-wide cisregulatory element prediction and annotation, assist detailed functional studies and facilitate the decipher of gene regulatory network...|$|R
40|$|Necessary Condition Analysis (NCA) {{is a new}} <b>data</b> <b>analysis</b> {{approach}} and technique to identify necessary (but not sufficient) conditions in datasets. It can complement traditional regression based <b>data</b> <b>analysis</b> as well as methods like QCA. This guide helps a novice user without knowledge of R or NCA to install the free R and NCA software on the user’s computer and to perform an NCA analysis within 15 minutes. The main instructions are: I. 	Install R II. 	Install NCA <b>III.</b> 	Load <b>data</b> IV. 	Run NCA. Details of the method {{can be found in}} Dul, J. (2016) Necessary Condition Analysis (NCA). Logic and Methodology of 'Necessary but not Sufficient' causality. Organizational Research Methods 19 (1), 10 - 52...|$|R
