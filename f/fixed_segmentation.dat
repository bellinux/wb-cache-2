13|28|Public
40|$|We {{present an}} {{extension}} to IBM Model 1 for training word-to-word lexicon probabilities. This model {{takes into account}} a given <b>fixed</b> <b>segmentation</b> of the source and target sentences in the estimation of the statistical dictionary. Our experimentation on the Europarl corpus shows that a statistical consistent improvement in the translation quality {{can be achieved by}} including our proposed model as a new information source in a log-linear combination of models...|$|E
40|$|Recognition of cursive handwritings such as Persian {{script is}} a hard task {{as there is no}} <b>fixed</b> <b>segmentation</b> and {{simultaneous}} segmentation and recognition is required. This paper presents a novel comparison method for such tasks which is based on a Multiple States Machine to perform robust elastic comparison of small segments with high speed through generation and maintenance of a set of concurrent possible hypotheses. The approach is implemented on Persian (Farsi) language using a typical feature set and a specific tailored genetic algorithm and the recognition and computation time is compared with dynamic programming comparison approach...|$|E
40|$|Abstract: This paper {{addresses}} {{the problem of}} binary phoneme classification via a neural net segment-based approach. Phoneme groups are categorized based on articulatory information. For an efficient segmental acoustic properties capture, the phoneme associated with a speech segment is represented using MFCC’s features extracted from different portions of that segment {{as well as its}} duration. These portions are obtained with fixed or variable size analysis. The classification is done with a Multi-Layer Perceptron trained using the Mackay’s Bayesian approach. Experimental results obtained from the Otago speech corpus favourites the use of <b>fixed</b> <b>segmentation</b> strategies over adaptive ones for resolving consonants/vowels, Fricatives/non fricatives, nasals/non nasals and stops/non stops binary classification problems...|$|E
40|$|Abstract. In {{this paper}} we compare the {{effectiveness}} of rhythm based signal segmentation technique with the traditional <b>fixed</b> length <b>segmentation</b> for music contents representation. We consider vocal regions, instrumental regions and chords which represent the harmony as different class of music contents to be represented. The effectiveness of segmentation for music content representation is measured based on intra class feature stability, inter class high feature deviation and class modeling accuracy. Experimental results reveal music content representation is improved with rhythm based signal <b>segmentation</b> than with <b>fixed</b> length <b>segmentation.</b> With rhythm based segmentation, vocal and instrumental modeling accuracy and chord modeling accuracy are improved by 12 % and 8 % respectively...|$|R
40|$|We {{participated in}} the MLWS 2017 on Tibetan word {{segmentation}} task, our system is trained in a unrestricted way, by introducing a baseline system and 76 w tibetan segmented sentences of ours. In the system character sequence is processed by the baseline system into word sequence, then a subword unit (BPE algorithm) split rare words into subwords with its corresponding features, after that a neural network classifier is adopted to token each subword into "B,M,E,S" label, in decoding step a simple rule is used to recover a final word sequence. The candidate system for submition is selected by evaluating the F-score in dev set pre-extracted from the 76 w sentences. Experiment shows that this method can <b>fix</b> <b>segmentation</b> errors of baseline system and result in a significant performance gain...|$|R
40|$|In recent highway safety studies, {{there has}} been {{increasing}} attention to the accuracy and timeliness of crash data, {{but there have been}} limited studies on data aggregation. This thesis tests the effects of the underlying preprocessing of crash data for identifying high crash locations. The first portion of this thesis shows a sensitivity analysis of assigning crashes to intersections by spatial proximity and crash data attributes. In addition the sensitivity of using <b>fixed</b> length <b>segmentation</b> was tested along with the influence of using crash rate for identifying high crash locations.;The results indicate minimal effects of using different spatial proximities in identifying high crash locations. The use of spatial proximity and crash data attributes in assigning crashes to intersections may have implications on benefit/cost analyses. Predetermined <b>fixed</b> length <b>segmentation</b> and the use of crash rate may both have impacts on identifying high crash locations...|$|R
40|$|The {{amount of}} {{multimedia}} content is increasing day by day, {{and there is}} a need to have automatic retrieval systems with high accuracy. In addition, there is a demand for event detectors that go beyond the simple finding of objects but rather detect more abstract concepts, such as “woodworking ” or a “board trick. ” This article presents a novelty approach for event classification that enables searching by audio concepts from the analysis of the audio track. This approach deals with the acoustic concepts recognition (ACR) creating a trained segmentation instead a <b>fixed</b> <b>segmentation</b> as segmental-GMM approach with broad concepts. Proposed approach has been evaluated on NIST 2011 TRECVID MED development set, which consists of usergenerated videos from the Internet, and has shown a EER o...|$|E
40|$|Abstract. We {{present a}} new {{approach}} to integrated motion estimation and segmentation by combining methods from discrete and continuous optimization. The velocity of each of a set of regions is modeled as a Gaussian-distributed random variable and motion models and segmentation are obtained by alternated maximization of a Bayesian a-posteriori probability. We show that for <b>fixed</b> <b>segmentation</b> the model parameters are given by a closed-form solution. Given the velocities, the segmentation is in turn determined using graph cuts which allows a globally optimal solution in the case of two regions. Consequently, there is no contour evolution based on differential increments as for example in level set methods. Experimental results on synthetic and real data show that good segmentations are obtained at speeds close to real-time. ...|$|E
40|$|This paper {{compares the}} {{correlation}} dimension (D 2) and Higuchi fractal dimension (HFD) approaches in estimating BIS index based on of electroencephalogram (EEG). The single-channel EEG data {{was captured in}} both ICU and operating room and different anesthetic drugs, including propofol and isoflurane were used. For better analysis, application of adaptive segmentation on EEG signal for estimating BIS index is evaluated and compared to <b>fixed</b> <b>segmentation.</b> Prediction probability (PK) {{is used as a}} measure of correlation between the predictors and BIS index to evaluate the proposed methods. The results show the ability of these algorithms (specifically HFD algorithm) in predicting BIS index. Also, evolving fixed and adaptive windowing methods for segmentation of EEG reveals no meaningful difference in estimating BIS index...|$|E
40|$|Sung {{language}} recognition {{relies on}} both effective feature extraction and acoustic modeling. In this paper, we study rhythm based music segmentation with the frame size being {{the duration of}} the smallest note in the music, as opposed to <b>fixed</b> length <b>segmentation</b> in spoken language recognition. It is found that acoustic features extracted from the rhythm based segmentation scheme outperform those from <b>fixed</b> length <b>segmentation.</b> We also study the effectiveness of a musically motivated acoustic feature. Octave scale cepstral coefficients (OSCCs) by comparing with the other acoustic features: Log frequency cepstral coefficients, Linear prediction coefficients (LPC) and LPC-derived cepstral coefficients. Finally, we examine the modeling capabilities of Gaussian mixture models and support vector machines in sung language recognition experiments. Experiments conducted on a corpus of 400 popular songs sung in English, Chinese, German, and Indonesian, showed that the OSCC feature outperforms other features. A sung language recognition accuracy of 64. 9 % was achieved when Gaussian mixture models were trained on shifted-delta-OSCC acoustic features, extracted via rhythm based music segmentation...|$|R
40|$|Abstract—Single-channel {{enhancement}} algorithms {{are widely}} used to overcome the degradation of noisy speech signals. Speech enhancement gain functions are typically computed from two quantities, namely, {{an estimate of the}} noise power spectrum and of the noisy speech power spectrum. The variance of these power spectral estimates degrades the quality of the enhanced signal and smoothing techniques are, therefore, often used to decrease the variance. In this paper, we present a method to determine the noisy speech power spectrum based on an adaptive time segmentation. More specifically, the proposed algorithm determines for each noisy frame which of the surrounding frames should contribute to the corresponding noisy power spectral estimate. Further, we demonstrate the potential of our adaptive segmentation in both maximum likelihood and decision direction-based speech enhancement methods by making a better estimate of the a priori signal-to-noise ratio (SNR). Objective and subjective experi-ments show that an adaptive time segmentation leads to significant performance improvements in comparison to the conventionally used <b>fixed</b> <b>segmentations,</b> particularly in transitional regions, where we observe local SNR improvements in the order of 5 dB. Index Terms—Adaptive time segmentation, a priori signal-to-noise ratio (SNR), decision directed approach, hypothesis test, speech enhancement. I...|$|R
40|$|To detect {{lung cancer}} {{at an earlier}} stage, a {{promising}} method is to apply perfusion magnetic resonance imaging (pMRI) modified to assess tumor angiogenesis. One key issue is to effectively characterize angiogenic patterns of pulmonary nodules. Based on our previous study addressing this issue, in this work, we develop STAT, a Spatio-Temporal Analysis Tool that implements not only our previously proposed pulmonary nodule modeling framework but also a user friendly interface and many extended functions. Our {{goal is to make}} STAT an easyto-use tool that can be applied to more general cases. STAT employs the following overall strategy for modeling pulmonary nodules: (1) nodule identification using a correlation maximization method, (2) nodule segmentation using edge detection, morphological operations and model-based strategy, and (3) nodule registration using landmark approach and thin-plate spline interpolation. In nodule identification, STAT provides new schemes for selecting the template and refining results in difficult cases. In nodule segmentation, STAT provides additional flexibilities for creating the weighting mask, selecting morphological structure elements and individually <b>fixing</b> <b>segmentation</b> result. In nodule registration, our previous study uses principal component analysis for landmark extraction, which may not work in general. To overcome this limitation, STAT provides an enhanced approach that minimizes the bending energy of the thin plate spline interpolation or mean square distance between eac...|$|R
40|$|This report {{presents}} the results of the study, "Pilot Project for <b>Fixed</b> <b>Segmentation</b> of the Pavement Network. " The goal of this pilot project was to study a small sample of the California Department of Transportation (Caltrans) network to determine the feasibility of expanding the pilot approach to the entire pavement network. The project's work included evaluating the effectiveness of ground penetrating radar (GPR) and limited coring for measuring pavement layer thicknesses and types, application of an algorithm for determining "fixed" segmentation of the pilot network, population of a database for the pilot network, then assessing costs of these activities. <b>Fixed</b> <b>segmentation</b> for use in the Pavement Management System (PMS) is required to develop the capability to do pavement performance modeling, which is essential for the following pavement management tasks: * Predicting future performance of segments of the network, and * Identifying the most cost-effective maintenance and rehabilitation (M&R) strategies based on life-cycle costs. Pavement layer-type and thickness data are also needed to develop effective pavement performance models and to conduct effective condition surveys of composite pavements (asphalt overlays of PCC pavement). The data are also useful for project-level engineering. Background information summarizing the experiences of several other states in using GPR for pavement work is also presented. The pilot network consisted of a total of eight roadways: three interstate highways (I- 5, I- 505, and I- 80), four state routes (SR- 16, SR- 45, SR- 99, and SR- 113), and one U. S. highway (US- 50). The roadways chosen are mostly in District 3, except for the I- 80 section and part of the I- 505 section, which are both in District 4. GPR data was collected on 681 lane-miles of the pilot network and analyzed for 305 lane-miles. Traffic data was obtained from Caltrans. Climate regions were determined from a recent map developed by Caltrans and the University of California Pavement Research Center (UCPRC). The UCPRC collected coring data for some of the locations on the pilot network. Some of the cores were provided to Infrasense, Inc., for GPR calibration and some were retained by the UCPRC for checking the accuracy of the layer thicknesses and types that Infrasense determined from the GPR data. The UCPRC also collected available as-built information and maintenance records, and the most recent Pavement Condition Survey (PCS) data from Caltrans. The UCPRC then used the data collected to develop <b>fixed</b> <b>segmentation</b> for the pilot network, resulting in 236 segments for the 305 lane-miles analyzed, with an average segment length of 1. 27 miles. Comparison of the cores retained by the UCPRC with the layer types and thicknesses identified by the GPR showed that the GPR data was reliable, especially for the top two layers of the pavement. Extrapolation of the costs on the pilot network for data collection and analysis and segmentation results in an estimate of approximately $ 7. 0 million of contracted field work consisting of GPR use and coring (including collection and analysis), plus 12. 3 person-years of additional analysis work to complete the segmentation for the entire Caltrans 49, 000 lane-mile network. If Caltrans moves ahead with collection of pavement structure data and <b>fixed</b> <b>segmentation,</b> it will be important to document as-built information in the structural database as future maintenance, rehabilitation, and reconstruction work occurs, {{in order to keep the}} database accurate. Work beyond this pilot study is underway to determine: * Whether PMS performance data can be used with the fixed segments to develop reasonable performance histories for the segment, and * Whether the performance models developed by the UCPRC from Washington State Department of Transportation (WSDOT) PMS data can be verified with Caltrans PMS performance histories using the fixed network segments and other necessary data developed in this pilot project. UCPRC-RR- 2005 - 11, Civil Engineering...|$|E
40|$|This paper proposes an {{approach}} for quantifying Depth of Anesthesia (DOA) based on correlation dimension (D 2) of electroencephalogram (EEG). The single-channel EEG data {{was captured in}} both ICU and operating room while different anesthetic drugs, including propofol and isoflurane, were used. Correlation dimension was computed using various optimized parameters {{in order to achieve}} the maximum sensitivity to anesthetic drug effects and to enable real time computation. For better analysis, application of adaptive segmentation on EEG signal for estimating DOA was evaluated and compared to <b>fixed</b> <b>segmentation,</b> too. Prediction probability (PK) was used as a measure of correlation between the predictors and BIS index to evaluate the proposed methods. Appropriate correlation between DOA and correlation dimension is achieved while choosing (D 2) parameters adaptively in comparison to fixed parameters due to the nonstationary nature of EEG signal...|$|E
40|$|Segmentation-based {{approach}} has shown significant success in stereo matching. By assuming pixels within one image seg-ment {{belong to the}} same 3 D surface, robust depth estimation can be achieved by taking the whole segment into considera-tion. However, segmentation has been mostly used for stereo matching at integer disparities rather than subpixel dispari-ties. One major reason is that small segments may be in-sufficient for estimating surfaces like slanted planes, while large segments may contain segmentation errors impacting the accuracy of depth estimation. In this work, we propose a segmentation-based scheme for subpixel stereo matching. Instead of using a <b>fixed</b> <b>segmentation,</b> segments are evolved to find a better support for reliable surface estimation. Given an initial estimation of segmentation and depth, the proposed algorithm jointly optimizes the segmentation and depth by evolving the segmentation at the pixel level and updating the plane parameters at the segment level. Justified with exper-iments performed on the Middlebury benchmark, we show that the proposed method achieves significant improvements for subpixel stereo matching...|$|E
30|$|Audio {{segmentation}} is {{the task}} of splitting an audio stream into segments of homogeneous content. Given a predefined set of audio classes, the process of segmentation involves joint boundary detection and classification, resulting in identification of segment regions as well as classification of those regions. Assuming that an audio signal has been divided into a sequence of audio segments using <b>fixed</b> window <b>segmentation,</b> our works focus on categorizing these audio segments into a set of predefined audio classes. Although {{there may be some}} differences between the traditional definition of audio classification and that in our work, the essential issues are the same.|$|R
30|$|Eight phantoms {{containing}} inserts {{of different}} sizes and shapes placed in air, water, and radioactive background were scanned using a Siemens SymbiaT SPECT/CT camera. Planar and tomographic scans with 177 Lu sources were used to measure CNF. Images were reconstructed with our SPEQToR software using resolution recovery, attenuation, and two scatter correction methods (analytical photon distribution interpolated (APDI) and triple energy window (TEW)). Segmentation was performed using a fixed threshold method for both air and cold water scans. For hot water experiments three segmentation methods were compared as folows: a 40 % <b>fixed</b> threshold, <b>segmentation</b> based on CT images, and our iterative adaptive dual thresholding (IADT). Quantification error, defined as the percent difference between experimental and true activities, was evaluated.|$|R
40|$|Abstract. Liver {{segmentation}} is {{an important}} prerequisite for planning of surgical interventions like liver tumor resections. For clinical applicability, the segmentation approach {{must be able to}} cope with the high variation in shape and gray-value appearance of the liver. In this paper we present a novel segmentation scheme based on a true 3 D segmentation refinement concept utilizing a hybrid desktop/virtual reality user interface. The method consists of two main stages. First, an initial segmentation is generated using graph cuts. Second, an interactive segmentation refinement step allows a user to <b>fix</b> arbitrary <b>segmentation</b> errors. We demonstrate the robustness of our method on ten contrast enhanced liver CT scans. Our segmentation approach copes successfully with the high variation found in patient data sets and allows to produce segmentations in a time-efficient manner. ...|$|R
40|$|Augmenting {{spectral}} data with spatial information for image classification has recently gained significant attention, as classification accuracy {{can often be}} improved by extracting spatial information from neighboring pixels. In this paper, we propose a new framework in which active learning (AL) and hierarchical segmentation (HSeg) are combined for spectral-spatial classification of hyperspectral images. The spatial information is extracted from a best segmentation obtained by pruning the HSeg tree using a new supervised strategy. The best segmentation is updated at each iteration of the AL process, thus taking advantage of informative labeled samples provided by the user. The proposed strategy incorporates spatial information in two ways: 1) concatenating the extracted spatial features and the original spectral features into a stacked vector and 2) extending the training set using a self-learning-based semi-supervised learning (SSL) approach. Finally, the two strategies are combined within an AL framework. The proposed framework is validated with two benchmark hyper{{spectral data}}sets. Higher classification accuracies are obtained by the proposed framework with respect to five other state-of-the-art spectral-spatial classification approaches. Moreover, {{the effectiveness of the}} proposed pruning strategy is also demonstrated relative to the approaches based on a <b>fixed</b> <b>segmentation...</b>|$|E
40|$|Figure 1 : Example edits {{performed}} with our system. Top: Spatially-varying reflectance represented with the Lafortune BRDF (left) and a non-parametric curve-based BRDF model (right). Bottom: Space- and time-varying reflectance data represented with the Cook-Torrance BRDF. By painting multiple editing constraints {{directly onto the}} model, which are automatically propagated to the entire dataset {{in a way that}} follows its spatial and temporal material patterns, a user performed these complex and realistic edits in a few minutes. We investigate a new approach to editing spatially- and temporallyvarying measured materials that adopts a stroke-based workflow. In our system, a user specifies a small number of editing constraints with a 3 -D painting interface which are smoothly propagated to the entire dataset through an optimization that enforces similar edits are applied to areas with similar appearance. The sparse nature of this appearance-driven optimization permits the use of efficient solvers, allowing the designer to interactively refine the constraints. We have found this approach supports specifying a wide range of complex edits that would not be easy with existing techniques which present the user with a <b>fixed</b> <b>segmentation</b> of the data. Furthermore, it is independent of the underlying reflectance model and we show edits to both analytic and non-parametric representations in examples from several material databases...|$|E
40|$|The goal of {{this pilot}} project was to study {{a small sample of}} the California Department of Transportation (Caltrans) network to {{determine}} the feasibility of expanding the pilot approach to the entire pavement network. The project’s work included evaluating the effectiveness of ground penetrating radar (GPR) and limited coring for measuring pavement layer thicknesses and types, application of an algorithm for determining “fixed ” segmentation of the pilot network, population of a database for the pilot network, then assessing costs of these activities. <b>Fixed</b> <b>segmentation</b> for use in the Pavement Management System (PMS) is required to develop the capability to do pavement performance modeling, which is essential for the following pavement management tasks: • Predicting future performance of segments of the network, and • Identifying the most cost-effective maintenance and rehabilitation (M&R) strategies based on life-cycle costs. Pavement layer-type and thickness data are also needed to develop effective pavement performance models and to conduct effective condition surveys of composite pavements (asphalt overlays of PCC pavement). The data are also useful for project-level engineering. Background information summarizing the experiences of several other states in using GPR for pavement work is also presented. The pilot network consisted of a total of eight roadways: three interstate highway...|$|E
40|$|The {{segmentation}} of the Arabic {{words for}} recognition still remains {{a problem to}} rise for the variability of the writing styles. The adopted technique rests on the hybridization of several segmentation methods based on the various reference levels. The segmentation process led to the decomposition of the writing or words in elementary entities of morphology relatively simple. The choice of primitives of various types is essential {{in order to ensure}} an understanding description. The work presented in this paper reveals the approach of the technique used and proposes an algorithm which has been tested with a set of Arabic words by different writers ranging from poor to acceptable quality. A particular attention was <b>fixed</b> to <b>segmentation</b> techniques that gave suitable results. The initial experimental results are very encouraging and promising. 1...|$|R
40|$|The {{performances}} of the Spann-Wilson algorithm on simulated synthetic aperture radar (SAR) images {{with varying degrees of}} speckle (one to four looks and varying amounts of white noise) is described. One hundred forty-eight test images are considered, of which the algorithm segmented most without any adjustment to the algorithm's parameters. The effect of speckle on fractal boundaries is studied. The effect of varying multiplicative and additive noise distributions for a <b>fixed</b> set of <b>segmentation</b> parameters is examined. The modified Spann-Wilson algorithm on four-look imagery is evaluated...|$|R
40|$|Image {{segmentation}} {{techniques are}} predominately based on parameter-laden optimization. The objective function typically involves weights for balancing competing image fidelity and segmentation regularization cost terms. Setting these weights suitably {{has been a}} painstaking, empirical process. Even if such ideal weights are found for a novel image, most current approaches fix the weight across the whole image domain, ignoring the spatially-varying properties of object shape and image appearance. We propose a novel technique that autonomously balances these terms in a spatially-adaptive manner through the incorporation of image reliability in a graph-based segmentation framework. We validate on synthetic data achieving a reduction in mean error of 47 % (p-value << 0. 05) {{when compared to the}} best <b>fixed</b> parameter <b>segmentation.</b> We also present results on medical images (including segmentations of the corpus callosum and brain tissue in MRI data) and on natural images. Comment: 12 pages, 7 figure...|$|R
40|$|There {{have been}} {{significant}} improvements in the accuracy of scene understanding due to a shift from recognizing objects ``in isolation'' to context based recognition systems. Such systems improve recognition rates by augmenting appearance based models of individual objects with contextual information based on pairwise relationships between objects. These pairwise relations incorporate common sense world knowledge such as co-occurrences and spatial arrangements of objects, temporal consistency, scene layout, etc. However, these relations, even though consistent in the 3 D world, change due to viewpoint of the scene. In this thesis, we investigate incorporating contextual information from three different perspectives for scene and video understanding (a) ``what'' contextual relations are useful and ``how'' they should be incorporated into Markov network during inference, (b) jointly solving the segmentation and recognition problem using a multiple segmentation framework based on contextual information in conjunction with appearance matching, and (c) proposing a discriminative spatio-temporal patch based representation for videos which incorporates contextual information for video understanding. Our work departs from traditional view of incorporating context into scene understanding where a fixed model for context is learned. We argue that context is scene dependent and propose a data-driven approach to predict the importance of relationships and construct a Markov network for image analysis based on statistical models of global and local image features. Since all contextual information is not equally important, we also address the related problem of predicting the feature weights associated with each edge of a Markov network for evaluation of context. We then {{address the problem of}} <b>fixed</b> <b>segmentation</b> while modeling context by using a multiple segmentation framework and formulating the problem as ``a jigsaw puzzle''. We formulate the labeling problem as segment selection from a pool of segments (jigsaws), assigning each selected segment a class label. Previous multiple segmentation approaches used local appearance matching to select segments in a greedy manner. In contrast, our approach is based on a cost function that combines contextual information with appearance matching. A relaxed form of the cost function is minimized using an efficient quadratic programming solver. Lastly, we propose a new representation for videos based on mid-level discriminative spatio-temporal patches. These patches might correspond to a primitive human action, a semantic object, or perhaps a random but informative spatiotemporal patch in the video. What define these spatiotemporal patches are their discriminative and representative properties. We automatically mine these patches from hundreds of training videos and experimentally demonstrate that these patches establish correspondence across videos. We propose a cost function that incorporates co-occurrence statistics and temporal context along with appearance matching to select subset of these patches for label transfer. Furthermore, these patches {{can be used as a}} discriminative vocabulary for action classification...|$|E
40|$|This paper first {{discusses}} how {{the signal}} segmentation and tonal characteristics of music notes effect in music chord detection. Two approaches, pitch class profile approach and psycho-acoustical approach, which differently represent these tonal characteristics, are examined for chord detection. The {{analysis of the}} tonal characteristics reveals that not only the fundamental frequency of music note but also its harmonics and sub-harmonies in different octaves contribute for detecting related music chord. A hierarchical approach, which transforms the music chord tonal characteristics in each octave onto probabilistic space, is then proposed for modeling the music chord. Our experimental results show that detection of chord type, Major, Minor, Diminish, and Augmented, and individual chords, 12 chords per chord type, are improved with the proposed hierarchical chord modeling approach. Experimental results also reveal that the tempo proportional signal segmentation is more effective extracting tonal characteristics than using <b>fixed</b> length <b>segmentation.</b> 1...|$|R
40|$|Abstract. Image {{segmentation}} {{techniques are}} predominately based on parameter-laden optimization. The objective function typically involves weights for balancing competing image fidelity and segmentation regularization cost terms. Setting these weights suitably {{has been a}} painstaking, empirical process. Even if such ideal weights are found for a novel image, most current approaches fix the weight across the whole image domain, ignoring the spatially-varying properties of object shape and image appearance. We propose a novel technique that autonomously balances these terms in a spatially-adaptive manner through the incorporation of image reliability in a graph-based segmentation framework. We validate on synthetic data achieving a reduction in mean error of 47 % (p-value << 0. 05) {{when compared to the}} best <b>fixed</b> parameter <b>segmentation.</b> We also present results on medical images (including segmentations of the corpus callosum and brain tissue in MRI data) and on natural images. Key words: Adaptive regularization, adaptive weights, image segmentation, energy minimization, energy functional, optimization, spectral flatness, noise detection...|$|R
40|$|Object {{recognition}} under uncontrolled illumination conditions {{remains one}} of hardest problems in machine vision. Under known lighting parameters, {{it is a simple}} task to calculate a transformation that maps sensed values to the expected colors in objects (and minimize the problems of reflections and/or texture). However, RoboCup aims to develop vision systems for natural lighting conditions in which the conditions are not only unknown but also dynamic. This makes <b>fixed</b> color-based image <b>segmentation</b> infeasible. We present a method for color determination under varying illumination conditions that succeeds in tracking the objects of interest in the RoboCup legged league. Griffith Sciences, School of Information and Communication TechnologyNo Full Tex...|$|R
40|$|This review {{provides}} {{an introduction to}} the use of parametric modelling techniques for time series analysis, and in particular the application of autoregressive modelling to the analysis of physiological signals such as the human electroencephalogram. The concept of signal stationarity is considered and, in the light of this, both adaptive models, and non-adaptive models employing <b>fixed</b> or adaptive <b>segmentation,</b> are discussed. For non-adaptive autoregressive models, the Yule-Walker equations are derived and the popular Levinson-Durbin and Burg algorithms are introduced. The interpretation of an autoregressive model as a recursive digital filter and its use in spectral estimation are considered, and the important issues of model stability and model complexity are discussed...|$|R
40|$|BACKGROUND AND PURPOSE: This study {{evaluated}} {{the use of}} total lesion glycolysis (TLG) determined by different automatic segmentation algorithms, for early response monitoring in {{non-small cell lung cancer}} (NSCLC) patients during concomitant chemoradiotherapy. MATERIALS AND METHODS: Twenty-seven patients with locally advanced NSCLC treated with concomitant chemoradiotherapy underwent 18 F-fluorodeoxyglucose (FDG) PET/CT imaging before and in the second week of treatment. Segmentation of the primary tumours and lymph nodes was performed using <b>fixed</b> threshold <b>segmentation</b> at (i) 40 % SUVmax (T 40), (ii) 50 % SUVmax (T 50), (iii) relative-threshold-level (RTL), (iv) signal-to-background ratio (SBR), and (v) fuzzy locally adaptive Bayesian (FLAB) segmentation. Association of primary tumour TLG (TLGT), lymph node TLG (TLGLN), summed TLG (TLGS=TLGT+TLGLN), and relative TLG decrease (DeltaTLG) with overall-survival (OS) and progression-free survival (PFS) was determined using univariate Cox regression models. RESULTS: Pretreatment TLGT was predictive for PFS and OS, irrespective of the segmentation method used. Inclusion of TLGLN improved disease and early response assessment, with pretreatment TLGS more strongly associated with PFS and OS than TLGT for all segmentation algorithms. This was also the case for DeltaTLGS, which was significantly associated with PFS and OS, with the exception of RTL and T 40. CONCLUSIONS: DeltaTLGS was significantly associated with PFS and OS, except for RTL and T 40. Inclusion of TLGLN improves early treatment response monitoring during concomitant chemoradiotherapy with FDG-PET...|$|R
40|$|Abstract: In the {{exploration}} of Chinese named entity recognition for a specific domain, the authors found that the errors caused during word segmentation and part-ofspeech (POS) tagging have obstructed {{the improvement of the}} recognition performance. In order to further enhance recognition recall and precision, the authors propose an error correction approach for Chinese named entity recognition. In the error correction component, transformation-based machine learning is adopted because it is suitable to <b>fix</b> Chinese word <b>segmentation</b> and POS tagging errors and produce effective correcting rules automatically. The Chinese named entity recognition component utilizes Finite-State Cascades which are automatically constructed by POS rules with semantic constraints. A prototype system, CNERS (Chinese Named Entity Recognition System), has been implemented. The experimental result shows that the recognition performance of most named entities have significantly been improved. On the other hand, the system is also fast and reliable...|$|R
40|$|The {{segmentation}} {{of optical}} flow fields {{is an important}} stage in real-time object tracking. The multi-point optical flow approach is a means to achieve this, but the approach is limited by uncertainty. This paper presents a segmentation scheme which incorporates multiresolution classification and the multi-point method to overcome the uncertainty. The classifier involves a spatio-featural focusing and a <b>fixing</b> process. The <b>segmentation</b> scheme produces a multiresolution segmentation map, where the central region of a physical object is represented using coarse resolutions so as to achieve a better estimation of the optical flow, and the border region is represented using fine resolutions so as to achieve a better estimation of the object boundaries. Keywords: optical flow field, object tracking, multiresolution, image segmentation, quad-tree, Gaussian pyramid 1 Introduction The tracking of moving objects through a sequence of images comprises two stages: the detection of moving targets [...] ...|$|R
40|$|This {{tutorial}} {{provides an}} introduction to the use of parametric modelling techniques for time series analysis, and in particular the application of autoregressive modelling to the analysis of physiological signals such as the human electroencephalogram. The concept of signal stationarity is considered and, in the light of this, both adaptive models, and non [...] adaptive models employing <b>fixed</b> or adaptive <b>segmentation,</b> are discussed. For non [...] adaptive autoregressive models, the Yule [...] Walker equations are derived and the popular Levinson [...] Durbin and Burg algorithms are introduced. The interpretation of an autoregressive model as a recursive digital filter and its use in spectral estimation are considered, and the important issues of model stability and model complexity are discussed. Keywords: autoregressive modelling, biomedical signal processing, human sleep EEG 1 INTRODUCTION Parametric modelling is a technique for time series analysis in which a mathematical model is fitted to a sample [...] ...|$|R
40|$|The {{legislative}} {{basis of}} market segmentation in Ukraine which is {{laid in the}} Accounting regulations (Standards) 29 "Segmental financial statements" defines methodological means of collecting information about incomes, costs, financial results, assets and liabilities of the reporting segments and their disclosure in the financial statement. The segment is recognized if it fits {{with the majority of}} business and geographical criteria. According to their risk availability and business structure, geographical segments are divided into two types: geographical industrial and geographical marketing segments. Theoretically segmentation is expected to study marketing before the beginning of its certain activity. The legislative basis of segmentation in Ukraine defines that enterprise segmentation takes place {{on the basis of its}} activity results in a certain accounting period according to standard methodology. While studying practical use of marketing segmentation of an enterprise, the author <b>fixed</b> paradox: <b>segmentation</b> of enterprise activity is not an instrument of its long term development in this or that direction, but at the same time, it may be used for the evaluation of its long term development in a certain direction. Economic zoning is a kind of geographical segmentation and development method of territorial form of economy organization in Ukraine. Economic zoning as a scientific research method and territorial business organization is the scientific alteration to segmentation and divides a country into separate districts according to its territorial division, each one with its specific character and integrity...|$|R
40|$|Stock data in {{the form}} of {{multiple}} time series are difficult to process, analyze and mine. However, when they can be transformed into meaningful symbols like technical patterns, it becomes easier. Most recent work on time series queries concentrates only on how to identify a given pattern from a time series. Researchers do not consider the problem of identifying a suitable set of time points for segmenting the time series in accordance with a given set of pattern templates (e. g., a set of technical patterns for stock analysis). On the other hand, using <b>fixed</b> length <b>segmentation</b> is a primitive approach to this problem; hence, a dynamic approach (with high controllability) is preferred so that the time series can be segmented flexibly and effectively according to the needs of users and applications. In {{view of the fact that}} such a segmentation problem is an optimization problem and evolutionary computation is an appropriate tool to solve it, we propose an evolutionary time series segmentation algorithm. This approach allows a sizeable set of stock patterns to be generated for mining or query. In addition, defining the similarity between time series (or time series segments) is of fundamental importance in fitness computation. By identifying perceptually important points directly from the time domain, time series segments and templates of different lengths can be compared and intuitive pattern matching can be carried out in an effective and efficient manner. Encouraging experimental results are reported from tests that segment the time series of selected Hong Kong stocks. Department of ComputingRefereed conference pape...|$|R
40|$|Both psychophysical and neurophysiological {{findings}} suggest that segmentation of a texture boundary based on orientation contrast is facilitated by contextual influences of elements collinear with, or parallel to, the boundary. Stimuli consisted of 8 × 8 test-matrices of Gabor elements all with the same orientation except those in one external row or column. After a short presentation interval, test matrices were masked by a matrix with no orientation cues. We measured the temporal threshold, defined as the test-matrix duration for discriminating (75 % correct) the orientation of texture boundary. Experiment 1 showed that orientation discrimination of the row or column flanked by uniform texture was faster in configurations with 0 ° or 90 ° than with 45 ° or 135 ° orientation differences, although orientation contrast was <b>fixed,</b> confirming that <b>segmentation</b> is faster when local elements are collinear with the texture border. Unexpectedly (experiment 2), in the 0 ° or 90 ° configuration, there is a two-fold increase in thresholds when uniform texture in the matrix has the same orientation as the border. These results suggest that implicit processing of uniform texture {{on the basis of}} long-range lateral interactions occurs and interferes with the explicit process of texture boundary...|$|R
