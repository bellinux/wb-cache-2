38|41|Public
40|$|In this work, {{we look at}} a novel {{approach}} for the realization of a fully parallel decoder based on the Viterbi algorithm and <b>hypercube</b> <b>architecture</b> using a rapid prototyping method on FPGAs. Our proposed modular/ <b>hypercube</b> <b>architecture</b> allows optimization of the surface by connecting modules together {{in such a way}} that a minimum of interconnections between modules is needed. Further optimization is possible using temporal multiplexing. 1...|$|E
40|$|Machines with {{distributed}} memory have the mapping problem—assigning processes to processors. In this paper we define the mapping problem as an optimization problem {{and discuss the}} question, how far is an optimum solution from an average or random solution. The term robustness is introduced and explained in detail with two examples, the SUPRENUM and the <b>hypercube</b> <b>architecture.</b> For the SUPRENUM architecture we show that a simple mapping strategy (optimal clustering of the processes) gives almost as good results as the optimal mapping. Optimal mapping is {{more important for the}} <b>hypercube</b> <b>architecture.</b> For nonhomogeneous networks adaptive routing seems promising...|$|E
40|$|The {{applicability}} of a parallel architecture {{to the solution}} of large electromagnetic scattering problems is demonstrated. Two techniques, finite difference and the method of moments, are used to {{provide insight into the}} comparative speedups which can be attained for several algorithms. The flexibility of the <b>hypercube</b> <b>architecture</b> for different analysis algorithms is illustrated...|$|E
50|$|In Usenet news group history, Steve Stevenson, a T-series owner at Clemson University {{started the}} moderated comp.hypercube news group. As <b>hypercube</b> <b>architectures</b> started to fade, {{he had the}} group renamed to comp.parallel which lasts to this day.|$|R
40|$|Three {{candidate}} {{fault tolerant}} <b>hypercube</b> <b>architectures</b> are modeled, their reliability analyses are compared, {{and the resulting}} implications of these methods of incorporating fault tolerance into hypercube multiprocessors are discussed. In the course of performing the reliability analyses, the use of HARP and fault trees in modeling sequence dependent system behaviors is demonstrated...|$|R
40|$|The {{synthesis}} of software algorithms and their hardware implementation {{require the use}} of efficient CAD tools providing detailed simulations and performance analysis. This paper presents a new behavioural simulator whose functional model is characterized by strict and non-strict functions, and by the concept of static and dynamic instances defined at different hierarchical levels of the structural description. Then, an extensive application of the simulator for the study of <b>hypercube</b> <b>architectures</b> is described...|$|R
40|$|Abstract { E cient data-parallel spatial join {{algorithms}} for pmr quadtrees and R-trees, common {{spatial data}} structures, are presented. The domain consists of planar line segment data (i. e., Bureau of the Census TIGER/Line les). Parallel algorithms for map intersection and a spatial range query are described. The algorithms are implemented using the SAM (Scan-And-Monotonic-mapping) model of parallel computation on the <b>hypercube</b> <b>architecture</b> of the Connection Machine...|$|E
40|$|Data-parallel {{algorithms}} for R-trees, {{a common}} spatial data structure are presented, {{in the domain}} of planar line segment data (e. g., Bureau of the Census TIGER/Line files). Parallel algorithms for both building the data-parallel R-tree, as well as determining the closed polygons formed by the line segments, are described and implemented using the SAM (Scan-And-Monotonic-mapping) model of parallel computation on the <b>hypercube</b> <b>architecture</b> of the Connection Machine...|$|E
40|$|A clear {{illustration}} of how parallel computers can be successfully appliedto large-scale scientific computations. This book demonstrates how avariety of applications in physics, biology, mathematics and other scienceswere implemented on real parallel computers to produce new scientificresults. It investigates issues of fine-grained parallelism relevant forfuture supercomputers with {{particular emphasis on}} <b>hypercube</b> <b>architecture.</b> The authors describe how they used an experimental approach to configuredifferent massively parallel machines, design and implement basic systemsoftware, and develo...|$|E
40|$|In {{this paper}} we present an {{efficient}} dense matrix multi-plication algorithm for distributed memory computers with a hypercube topology. The proposed algorithm performs better than all previously proposed algorithms {{for a wide}} range of matrix sizes and number of processors, especially for large matrices. We analyze the performance of the algorithms for two types of <b>hypercube</b> <b>architectures,</b> one in which each node can use (to send and receive.) at most one communication link at a time and the other & which eaeh node can use all communication links simultaneously...|$|R
40|$|Both Gray {{code and}} binary code are {{frequently}} used in mapping arrays into <b>hypercube</b> <b>architectures.</b> While {{the former is}} preferred when communication between adjacent array elements is needed, the latter is preferred for FFT-type communication. When different phases of computations have different types of communication patterns, the need arises to remap the data. We give a nearly optimal algorithm for permuting data from a Gray code mapping to a binary code mapping on a hypercube with communication restricted to one input and one output channel per node at a time. Our algorithm improves over the best previously known algorithm [6] by nearly a factor of two and is optimal to within a factor of n=(n Γ 1) with respect to data transfer time on an n-cube. The expected speedup is confirmed by measurements on an Intel iPSC/ 2 hypercube...|$|R
40|$|In {{this paper}} we {{investigate}} {{the impact of}} the data partitioning on the performance of parallel iterative solvers, for very large PDE problems, on distributed memory MIMD (DM-MIMD) machines. Our goal is to minimize the local synchronization time required for the parallel processing of such computations. We succeed by : (i) partitioning any two (or three) dimensional simplyconnected domains into nearly equal sized, compact subdomains, and have minimum number of adjacent subdomains and (ii) developing communication mechanism that reflect characteristics like interconnection network and routing scheme of the parallel hardware used. Heuristics are presented for mesh and <b>hypercube</b> <b>architectures</b> : (i) partitioning based on geometry characteristics (boundary shape and configuration), whose partitioning graph is 2 -chromatic and its degree is bounded by six, (ii) scheduling the local messages for fixed routing scheme, e-cube, on circuit switched interconnection networks and (iii) an efficient a [...] ...|$|R
40|$|In {{this paper}} a non-recursive Strassen’s matrix {{multiplication}} algorithm is presented. This new algorithm is suitable {{to run on}} parallel environments. Two computational schemes have been worked out exploiting different parallel approaches on <b>hypercube</b> <b>architecture.</b> A comparative analysis is reported. The experiments {{have been carried out}} on an nCUBE- 2 supercomputer, housed at CNUCE in Pisa, supporting the Express parallel operating system. © 1995, Taylor & Francis Group, LLC. All rights reserved...|$|E
40|$|The {{feasibility}} of implementing a fault-tolerant <b>hypercube</b> <b>architecture</b> for space applications is discussed. Node-level architectures and designs are considered and a first-order reliability model is presented. It is shown how error recovery {{can be implemented}} using program rollback or roll-forward techniques. Shared memory augmentations to the message-passing structure {{can be used to}} get around the inefficiencies of multicomputers to provide efficient use of hardware to achieve the needed reliabilities while maintaining performance...|$|E
40|$|This work first reviews an already-developed, {{existing}} deterministic parallel algorithm [1] {{to compute}} the complete histogram of an image in optimal number of steps (log n) on a <b>hypercube</b> <b>architecture</b> and utilizing memory space {{on the order of}} O(x^(1 / 2) log x) where x is the number of gray levels in the image, at each processing element. The paper then introduces our improvement to this algorithm's memory requirements by introducing the concept of randomization into the algorithm...|$|E
40|$|Theoretical {{analysis}} of ordered power-of-two Fast Fourier transforms (P 02 FFT) {{demonstrates that the}} most eficient algorithms for <b>hypercube</b> <b>architectures</b> may not always use nearest-neighbor communication. The ordered P 02 FFT which uses nearest-neighbor (distance 1) communication, requires d inter-processor communications for r/ 2 ~ d and 2 d – lr/ 2] interpro-cessor communications for r/ 2 < d, where 2 “ = N. In this paper we show that an ordered P 02 FFT can be obtained with just d inierprocessor communications for any r if the restriction that all communications are distance one is removed. This new algorithm uses [r/ 2 ~ distance one and d – ~r/ 2] distance two inter-processor communications. Packets of size N/ 2 d~ 1 are transmitted in both of the ordered P 02 FFT algorithms. The time complexity of both the distance one and dis-tance two algorithms is discussed. 1...|$|R
40|$|The {{direct method}} for solving N-body {{problems}} maps perfectly onto <b>hypercube</b> <b>architectures.</b> Unlike other <b>hypercube</b> implementations, we have implemented a direct N-body solver on the Connection Machine CM- 2 which makes optimum {{use of the}} full bandwidth of the hypercube. When N>> P, where P {{is the number of}} floating-point processors, the communication time of the algorithm is negligible, and the execution time is that of the arithmetic time giving a P-fold speed-up for real problems. To obtain this performance, we use “rotated and translated Gray codes ” which result in time-wise edge disjoint Hamiltonian paths on the hypercube; We further propose that this communication pattern has unexplored potential for other types of algorithms. Timings are presented for a collection of interacting point vortices in two dimensions. The computation of the velocities of 14, 099 vortices in 32 -bit precision takes 2 seconds on a 16 K CM- 2. ...|$|R
40|$|Computational {{complexity}} of 5 / 9 -point multi-level methods on <b>hypercube</b> <b>architectures</b> is analysed. In particular attention {{is given to}} minimization of data distances. 1 Introduction Recently, Axelsson and Eijkhout [1] proposed a multi-level method based on 9 point stencils. In this method, in contrast to ordinary 5 -point multi-level methods, {{the transition from the}} grid with meshwidth h to the one with width 2 h is made via an intermediate, skew, grid with meshwidth p 2 h. The method is based on the observation that imposing a red/black structure on a domain with 9 -point stencils in the nodes, and eliminating the black points after altering their stencils to an axiparallel 5 -point stencil, will give a rotated 9 -point stencils on the red grid. Repeating this argument, rotated by 45 degrees on every other grid, will then define a multilevel preconditioner with interesting properties. First of all, as the black points are mutually uncoupled, only systems with diagonal blocks need to be sol [...] ...|$|R
40|$|We {{discuss some}} issues {{that arise in}} the {{implementation}} of numerical algo-rithms for computational fluid dynamics (CFD) on multiprocessor systems such as hyper-cubes. We identify several important kernel numerical algorithms from CFD that map well onto the <b>hypercube</b> <b>architecture.</b> We emphasize the importance of considering the optimal mapping for a collection of kernel algorithms used in an application program rather than just on individual optimal mappings. Several examples illustrating the trade-offs between rearranging data to fit a particular kernel algorithm and using suboptimal mappings will be discussed...|$|E
40|$|Abstract—A {{quadtree}} is {{a hierarchical}} data structure {{used in many}} computer graphics, image processing, and computational geometry applications. In these applications, we need to obtain information about the quadtree nodes near a given quadtree node. Therefore, in a parallel environment, nodes that are geometrically close should be stored in processors that are close in the interconnection network. We present, in a <b>hypercube</b> <b>architecture,</b> the generation process of the tree based on boundary discretization and we propose a procedure to balance the load using a top-down approach. Keywords-Quadtrees; parallel computing. I...|$|E
40|$|AbstractWe propose several {{implementations}} of Gaussian elimination {{for solving}} banded linear systems on multiprocessors. Three simple architectures are considered: a multiprocessor ring, a grid array, and a hypercube. Our complexity analysis fully accounts for communication delays by using simple models where both latency and actual transfer times are incorporated. When {{the number of}} processors is small relative to the bandwidth of the system, a row-interleaved implementation of Gaussian elimination algorithm is attractive. Otherwise, a two-dimensional grid is essential for achieving higher speedup. The <b>hypercube</b> <b>architecture</b> gives the smallest communication latency times...|$|E
40|$|The {{gravitational}} N-body algorithm of Barnes and Hut [1] {{has been}} successfully implemented on a hypercube concurrent processor. The novel approach of their sequential algorithm has demonstrated itself to be well suited to <b>hypercube</b> <b>architectures.</b> The sequential code achieves O (NlogN) speed by recursively dividing space into subcells, thereby creating a hierarchical grouping of particles. Computing interactions between these groups dramatically reduces the amount of communication between processors, {{as well as the}} number of force calculations. Parallelism is achieved through an irregular spatial grid decomposition. Since the decomposition topology is not simple, a general loosely synchronous communication routine has been developed. Operations are simplified if the conventional grey code decomposition is modified so that the bits are taken alternately from each Cartesian dimension. A speedup of 180 has been achieved for a 500, 000 particle two-dimensional calculation on 256 processors. A speedup of 65 has been obtained for a 64, 000 particle three-dimensional calculation on 256 processors...|$|R
40|$|Abstract In this paper, {{we present}} a new {{technique}} for mapping the backpropagation algorithm on <b>hypercubes</b> and related <b>architectures.</b> A key component of this technique is a network partitioning scheme which is called checkerboarding. Checkerboarding allows us to replace the all-to-all broadcast operation performed by the commonly used vertical network partitioning scheme, with operations that are much faster on the <b>hypercubes</b> and related <b>architectures.</b> Checkerboarding can be combined with the pattern partitioning technique to form a hybrid scheme which performs better than either one of these schemes. Theoretical analysis and experimental results on nCUBE 2 TM y and CM 5 TM z show that our scheme performs {{better than the other}} schemes, both for uniform and non-uniform networks...|$|R
40|$|Many {{parallel}} computers consist of processors connected {{in the form}} of a d-dimensional mesh or hypercube. Two- and three-dimensional meshes have been shown to be efficient in manipulating images and dense matrices, whereas hypercubes have been shown to be well suited to divide-and-conquer algorithms requiring global communication. However, even a single faulty processor or communication link can seriously affect the performance of these machines. This paper presents several techniques for tolerating faults in d-dimensional mesh and <b>hypercube</b> <b>architectures.</b> Our approach consists of adding spare processors and communication links so that the resulting architecture will contain a fault-free mesh or hypercube in the presence of faults. We optimize the cost of the fault-tolerant architecture by adding exactly k spare processors (while tolerating up to k processor and/or link faults) and minimizing the maximum number of links per processor. For example, when the desired architecture is a d-dimensional mesh and k = 1, we present a fault-tolerant architecture that has the same maximum degree as the desired architecture (namely, 2 d) and has only one spare processor. We also present efficient layouts for fault-tolerant two- and three-dimensional meshes, and show how multiplexers and buses can be used to reduce the degree of fault-tolerant architectures. Finally, we give constructions for fault-tolerant tori, eight-connected meshes, and hexagonal meshes...|$|R
40|$|In {{this paper}} we {{describe}} algorithms for solving nonlinear least-squares problems on a message-passing multiprocessor. We demonstrate new parallel algorithms, including an efficient parallel algorithm {{for determining the}} Levenberg-Marquardt parameter and a new row-oriented QR factorization algorithm. Experimental results obtained on an Intel iPSC hypercube are presented and compared with sequential MINPACK code executed on a single processor. These experimental results show that essentially full efficiency is obtained for problems where the row size is sufficiently larger {{than the number of}} processors. These algorithms have the advantage of involving only simple data movements and consequently are not constrained to the <b>hypercube</b> <b>architecture...</b>|$|E
40|$|This paper {{highlights}} the analytical modeling of task scheduling for a subclass of message passing architecture which is categorized as Master-Slave Super-Hypercube (MSSHP) topology. To establish this analytical modeling, its foundation based on Hypercube (HP) and Super-Hypercube architectures is first laid {{down and it}} is then expanded to include the MSSHP architecture. In order to highlight {{the advantages of the}} proposed modeling scheme, graphical presentations are introduced and compared. This comparison underlines the relationship between the time complexity for the HP and the new version for the MSSHP architecture. The outcome of this study reveals that the completion time for matrix product on the MSSHP is more favorable than <b>Hypercube</b> <b>architecture...</b>|$|E
40|$|We {{describe}} {{a set of}} software utilities designed to facilitate the writing of parallel codes and porting sequential ones. Emphasis is placed on portability so that code can be developed simultaneously on a sequential and a parallel machine, and so that the completed code can be run and maintained {{on a wide variety}} of machine architectures. We describe the CrOS communication package for parallel machines, the CUBIX system to allow a code to run in parallel or sequentially, the PLOTIX parallel graphics foundation, and the parallel debugger NDB. While the system described has been implemented on qualitatively different machines the particular version described here is most efficient for the <b>hypercube</b> <b>architecture,</b> and was developed on NCUBE hypercubes under both the AXIS or XENIX operating systems...|$|E
40|$|Since the {{introduction}} by Shepp and Vardi [Shepp, L. A. & Vardi, Y. (1982) IEEE Trans. Med. Imaging 1, 113 - 121] of the expectation-maximization algorithm for {{the generation of}} maximum-likelihood images in emission tomography, a number of investigators have applied the maximum-likelihood method to imaging problems. Though this approach is promising, it is now well known that the unconstrained maximum-likelihood approach has two major drawbacks: (i) the algorithm is computationally demanding, resulting in reconstruction times that are not acceptable for routine clinical application, and (ii) the unconstrained maximum-likelihood estimator has a fundamental noise artifact that worsens as the iterative algorithm climbs the likelihood hill. In this paper the computation issue is addressed by proposing an implementation on the class of massively parallel single-instruction, multiple-data architectures. By restructuring the superposition integrals required for the expectation-maximization algorithm as the solutions of partial differential equations, the local data passage required for efficient computation on this class of machines is satisfied. For dealing with the "noise artifact" a Markov random field prior determined by Good's rotationally invariant roughness penalty is incorporated. These methods are demonstrated on the single-instruction multiple-data class of parallel processors, with the computation times compared with those on conventional and <b>hypercube</b> <b>architectures...</b>|$|R
40|$|This paper {{describes}} different {{schemes for}} tolerating faults in augmented <b>hypercube</b> multiprocessors. The <b>architectures</b> considered have a spare {{assigned to each}} subset of nodes (cluster). The approaches make use of hardware redundancy {{in the form of}} spare nodes and/or links and usually requires modifications in the communication as well as computation algorithms...|$|R
40|$|Abstract- The <b>hypercube</b> {{parallel}} <b>architecture</b> {{is one of}} {{the most}} popular interconnection networks due to many of its attractive properties and its suitability for general purpose parallel processing. An attractive version of the hypercube is the crossed cube. It preserves the important properties of the hypercube and most importantly reduces the diameter by a factor of two. In this paper, we show the ability of the crossed cube as a versatile architecture to simulate other interconnection networks efficiently. We present new schemes to embed complete binary trees, complete quad trees, and cycles into crossed cubes. Index Terms- binary trees, cycles, dilation, embedding, expansion, hypercubes, crossed cubes...|$|R
40|$|Processor {{allocation}} and {{the task}} scheduling technique in parallel processing systems {{play a significant role}} in improving the performance of a message-passing architecture. Adapting the right algorithms and further improvements in areas such as time complexity, execution time, speed up and synchronization mechanisms undoubtedly facilitates implementation of advanced applications on a parallel processing system. These applications include but are not limited to DNA computing, artificial immune systems and optical computing to name a few. This paper highlights the communication cost related to a Super-Hypercube topology for being a subclass of traditional <b>Hypercube</b> <b>architecture.</b> Furthermore, a particular reference is made to the mathematical modeling of Hypercube and Super-Hypercube architectures. Finally, graphical presentations are carried out based on mathematical calculations to address the advantage of Super-Hypercube topology...|$|E
40|$|The {{problem of}} {{embedding}} a single n-processor guest network G into an n-processor host network H {{is an important}} problem in parallel processing. Results on this problem not only demonstrate computational equivalence (or non-equivalence) between networks of different topology, but efficient embeddings lead to efficient simulations of algorithms originally designed for G on host H. Embeddings and their implications to parallel processings have recently been studied extensively [2, 4, 6, 8, 10, 11, 14]. The <b>hypercube</b> <b>architecture</b> {{has proven to be}} a versatile and suitable architecture for designing and implementing parallel algorithms and it allows for efficient embeddings of many networks (e. g., trees, meshes, butterflies, pyramids) [3, 5, 6, 13, 14]. It is not surprising that many co [...] ...|$|E
40|$|Diffusion limited {{aggregation}} (DLA) {{has proved}} {{very successful in}} modelling systems which display fractal characteristics, like viscous fingering. However, by nature, such simulations are very processor intensive, requiring large amounts of processor time even for relatively small models. We have performed simulations of viscous fingering on the NCUBE parallel computer which has <b>hypercube</b> <b>architecture.</b> We find that, {{as long as the}} number of processors used is much less than both the total number of walkers released and the overall dimensions of the model, the fractal dimensions obtained using serial and parallel algorithms give similar results whilst achieving a considerable speed-up in the parallel implementation. An average fractal dimension of 1. 71 was obtained along with a speed-up of 106 (in the best case) and 83 % efficiency using 128 processor...|$|E
40|$|In this paper, {{we present}} a new {{technique}} for mapping the backpropagation algorithm on <b>hypercubes</b> and related <b>architectures.</b> A key component of this technique is a network partitioning scheme which is called checkerboarding. Checkerboarding allows us to replace the all-to-all broadcast operation performed by the commonly used vertical network partitioning scheme, with operations that are much faster on the <b>hypercubes</b> and related <b>architectures.</b> Checkerboarding can be combined with the pattern partitioning technique to form a hybrid scheme which performs better than either one of these schemes. Theoretical analysis and experimental results on nCUBE 2 TM y and CM 5 TM z show that our scheme performs {{better than the other}} schemes, both for uniform and non-uniform networks. 1 Introduction The Backpropagation algorithm (BP) [1] {{is one of the most}} popular neural network learning algorithms. It has been used in a large number of applications [2, 3, 4, 5]. This algorithm is computat [...] ...|$|R
40|$|ABSTRACT – We {{quantify}} why, as designers, {{we should}} prefer clique-based hypercubes (K-cubes) over traditional hypercubes based on cycles (C-cubes). Reaping fresh analytic results, {{we find that}} K-cubes minimize the wirecount and, simultaneously, the latency of <b>hypercube</b> <b>architectures</b> that tolerate failure of any f nodes. Refining the graph model of Hayes (1976), we pose the feasibility of configuration as a problem in multivariate optimization: What (f + 1) -connected n-vertex graphs with fewest edges ⎡n (f+ 1) / 2 ⎤ minimize the maximum a) radius or b) diameter of subgraphs (i. e., quorums) induced by deleting up to f vertices? (1) We solve (1) for f that is superlogarithmic but sublinear in n, {{and in the process}} prove: I) the fault tolerance of K-cubes is proportionally greater than that of C-cubes; II) quorums formed from K-cubes have a diameter that is asymptotically convergent to the Moore Bound on radius; III) under any conditions of scaling, by contrast, C-cubes diverge from the Moore Bound. Thus, K-cubes are optimal, while C-cubes are suboptimal. Our exposition furthermore: IV) counterexamples, corrects, and generalizes a mistaken claim by Armstrong and Gray (1981) concerning binary cubes; V) proves that K-cubes and certain of their quorums are the only graphs which can be labeled such that the edge distance between any two vertices equals the Hamming distance between their labels; and VI) extends our results to K-cube-connected cycles and edges. We illustrate and motivate our work with applications to the synthesis of multicomputer architectures for deep space missions. 1...|$|R
40|$|AbstractIn this paper, {{we propose}} and analyze a new {{interconnection}} network, the k-ary <b>hypercube.</b> This new <b>architecture</b> captures {{the advantages of}} the mesh network and those of the binary hypercube. We show that the hamiltoniacity of this network and its capability of efficiently simulating other topologies. It has a smaller degree than that of its equivalent binary hypercube (the one with at least as many nodes) and has a smaller diameter than its equivalent mesh of processors...|$|R
