708|2998|Public
5000|$|From the computer's perspective, an <b>interaction</b> <b>technique</b> involves: ...|$|E
50|$|In general, {{the less}} {{compatible}} {{the device is}} with the domain object, the more complex the <b>interaction</b> <b>technique.</b> For example, using a mouse to specify a 2D point involves a trivial <b>interaction</b> <b>technique,</b> whereas using a mouse to rotate a 3D object requires more creativity to design the technique and more lines of code to implement it.|$|E
5000|$|... #Caption: Fold n' Drop, a crossing-based <b>interaction</b> <b>technique</b> for {{dragging}} {{and dropping}} files between overlapping windows.|$|E
40|$|Abstract—Even though {{interaction}} {{is an important}} part of information visualization (Infovis), it has garnered a relatively low level of attention from the Infovis community. A few frameworks and taxonomies of Infovis <b>interaction</b> <b>techniques</b> exist, but they typically focus on low-level operations and do not address the variety of benefits interaction provides. After conducting an extensive review of Infovis systems and their interactive capabilities, we propose seven general categories of <b>interaction</b> <b>techniques</b> widely used in Infovis: 1) Select, 2) Explore, 3) Reconfigure, 4) Encode, 5) Abstract/Elaborate, 6) Filter, and 7) Connect. These categories are organized around a user’s intent while interacting with a system rather than the low-level <b>interaction</b> <b>techniques</b> provided by a system. The categories can act as a framework to help discuss and evaluate <b>interaction</b> <b>techniques</b> and hopefully lay an initial foundation toward a deeper understanding and a science of interaction. Index Terms—Information visualization, <b>interaction,</b> <b>interaction</b> <b>techniques,</b> taxonomy, visual analytics...|$|R
40|$|There are two typical {{approaches}} {{to the design of}} three-dimensional (3 D) <b>interaction</b> <b>techniques</b> for immersive virtual environments (VEs). Many 3 D <b>interaction</b> <b>techniques</b> are designed for generic user tasks such as navigation, selection, manipulation, and system control, without consideration of the domain-of-use. Despite a long history in HCI theory and practice of using domain knowledge for system-level design, it has not often been used for the design of <b>interaction</b> <b>techniques.</b> Other 3 D <b>interaction</b> <b>techniques</b> are designed for specific applications. While these techniques may be quite usable and useful in that single application, their reuse is limited. In this paper, we propose a middle ground: domain-specific design (DSD) of 3 D <b>interaction</b> <b>techniques.</b> The goal of DSD is to improve upon current practice so that 3 D <b>interaction</b> <b>techniques</b> are designed to address real-world tasks, while still allowing for reuse of the techniques within a particular domain. A three-level design framework provides a theoretical basis for DSD, and we show how this framework can be used to illustrate multiple paths for the design of domain-specific <b>interaction</b> <b>techniques.</b> We also provide a comprehensive, practical case study of an actual VE system, Virtual-SAP (structure analysis program), to illustrate how the approach is applied. Experimental results demonstrate that the use of the DSD approach increases the usefulness of this application, without sacrificing usability. Keywords: 3 D interaction, domain-specific interaction, design theory, virtual environment...|$|R
50|$|<b>Interaction</b> <b>techniques</b> are {{the glue}} between {{physical}} I/O devices and interaction tasks or domain objects. Different types of <b>interaction</b> <b>techniques</b> {{can be used}} to map a specific device to a specific domain object. For example, different gesture alphabets exist for pen-based text input.|$|R
50|$|Loren Brichter is an American {{software}} developer who {{is best known}} for creating Tweetie and the Pull-to-Refresh <b>interaction</b> <b>technique.</b>|$|E
50|$|This <b>interaction</b> <b>technique</b> has close {{associations}} with related techniques in {{graphical user interfaces}} that use pointing devices such as a computer mouse (by drag and drop, for example).|$|E
5000|$|... 2007 - 2010: Founded his own company, atebits in 2007, and {{released}} a small drawing app for Mac - Scribbles. Brichter then released his second app Tweetie in 2008, where the Pull-to-Refresh <b>interaction</b> <b>technique</b> was borne. In the same year, Brichter also co-founded a company Borange.|$|E
50|$|A {{large part}} of {{research}} in human-computer interaction involves exploring easier-to-learn or more efficient <b>interaction</b> <b>techniques</b> for common computing tasks. This includes inventing new (post-WIMP) <b>interaction</b> <b>techniques,</b> possibly relying on methods from user interface design, and assessing their efficiency with respect to existing techniques using methods from experimental psychology. Examples of scientific venues in these topics are the UIST and the CHI conferences. Other research focuses on the specification of <b>interaction</b> <b>techniques,</b> sometimes using formalisms such as Petri nets {{for the purposes of}} formal verification.|$|R
5000|$|... #Subtitle level 2: Comparison {{with other}} <b>interaction</b> <b>techniques</b> ...|$|R
40|$|Researchers have {{developed}} interaction concepts based on mobile projectors. Yet pursuing {{work in this}} area— particularly in building projector-based <b>interactions</b> <b>techniques</b> within an application—is cumbersome and timeconsuming. To mitigate this problem, we contribute ProjectorKit, a flexible open-source toolkit that eases rapid prototyping mobile projector <b>interaction</b> <b>techniques.</b> Author Keywords Mobile projectors, toolkit, rapid prototyping...|$|R
5000|$|From the user's perspective, an <b>interaction</b> <b>technique</b> {{is a way}} {{to perform}} a single {{computing}} task and can be informally expressed with user instructions or usage scenarios. For example, [...] "to delete a file, right-click on the file you want to delete, then click on the delete item".|$|E
50|$|From 2009 to 2012, Harrison was the Editor-in-Chief of ACM's Crossroads magazine, {{which he}} relaunched as XRDS, the {{flagship}} magazine for the over 30,000 student {{members of the}} ACM. Harrison also spun-off several technologies out of CMU to cofound the technology startup Qeexo in 2012, which provides machine-learning based <b>interaction</b> <b>technique</b> software for {{tens of millions of}} handsets worldwide as of 2016.|$|E
50|$|From {{the user}} {{interface}} designer's perspective, an <b>interaction</b> <b>technique</b> is a well-defined {{solution to a}} specific user interface design problem. Interaction techniques as conceptual ideas can be refined, extended, modified and combined. For example, contextual menus are {{a solution to the}} problem of rapidly selecting commands. Pie menus are a radial variant of contextual menus. Marking menus combine pie menus with gesture recognition.|$|E
40|$|We {{present a}} set of <b>interaction</b> <b>techniques</b> for {{electronic}} musical performance using a tabletop tangible interface. Our system, the Audiopad, tracks the positions of objects on a tabletop surface and translates their motions into commands for a musical synthesizer. We developed and refi ned these <b>interaction</b> <b>techniques</b> through an iterative design process, in which new <b>interaction</b> <b>techniques</b> were periodically evaluated through performances and gallery installations. Based on our experience refi ning the design of this system, we conclude that tabletop interfaces intended for collaborative use should use <b>interaction</b> <b>techniques</b> designed to be legible to onlookers. We also conclude that these interfaces should allow users to spatially reconfi gure the objects in the interface {{in ways that are}} personally meaningful. Categories and Subject Descriptors H. 5. 2 [User Interfaces]: interaction styles, input devices and strategies J. 5 : [Arts and Humanities]: performing art...|$|R
40|$|<b>Interaction</b> <b>Techniques</b> in Virtual Environments Doug A. Bowman, Chadwick A. Wingrave, Joshua M. Campbell, and Vinh Q. Ly Department of Computer Science (0106) Virginia Tech Blacksburg, VA 24061 USA {bowman, cwingrav, jocampbe, vly}@vt. edu Abstract Usable {{three-dimensional}} (3 D) <b>interaction</b> <b>techniques</b> {{are difficult}} to design, implement, and evaluate. One {{reason for this is}} a poor understanding of {{the advantages and disadvantages of}} the wide range of 3 D input devices, and of the mapping between input devices and <b>interaction</b> <b>techniques.</b> We present an analysis of Pinch Gloves^TM and their use as input devices for virtual environments (VEs). We have developed a number of novel and usable <b>interaction</b> <b>techniques</b> for VEs using the gloves, including a menu system, a technique for text input, and a two-handed navigation technique. User studies have indicated the usability and utility of these techniques. ...|$|R
40|$|We present 3 dml, a markup {{language}} for 3 D <b>interaction</b> <b>techniques</b> and virtual environment applications that involve non-traditional devices. 3 dml has two main purposes: readability and rapid development. Designers can read 3 dml-based representations of 3 D <b>interaction</b> <b>techniques,</b> compare them, and understand them. 3 dml {{can also be}} used as a front end for any VR toolkit, so designers without programming skills can create VR applications as 3 dml documents that plug together <b>interaction</b> <b>techniques,</b> VR objects, and devices. This paper focuses on the language features and presentation scheme designed in our websit...|$|R
50|$|Modern mobile augmented-reality systems use {{one or more}} of the {{following}} tracking technologies:digital cameras and/or other optical sensors, accelerometers, GPS, gyroscopes, solid state compasses, RFID and wireless sensors. These technologies offer varying levels of accuracy and precision. Most important is the position and orientation of the user's head. Tracking the user's hand(s) or a handheld input device can provide a 6DOF <b>interaction</b> <b>technique.</b>|$|E
50|$|What {{you see is}} {{what you}} meant (WYSIWYM) is a text editing <b>interaction</b> <b>technique</b> that emerged from two {{projects}} at University of Brighton. It allows users to create abstract knowledge representations such as those required by the Semantic Web using a natural language interface. Natural language understanding (NLU) technology is not employed. Instead, natural language generation (NLG) is used in a highly interactive manner.|$|E
5000|$|An <b>interaction</b> <b>technique,</b> user {{interface}} technique or input technique {{is a combination}} of hardware and software elements that provides a way for computer users to accomplish a single task. For example, one can go back to the previously visited page on a Web browser by either clicking a button, pressing a key, performing a mouse gesture or uttering a speech command. It is a widely used term in human-computer interaction. In particular, the term [...] "new interaction technique" [...] is frequently used to introduce a novel {{user interface}} design idea.|$|E
40|$|Mixed-reality {{games have}} the {{potential}} to let users play in the world surrounding them. However, to exploit this new approaches to game content creation, content presentation <b>techniques</b> and <b>interaction</b> <b>techniques</b> are required. In this paper we explore the potential of computer-vision on mobile devices with a camera as an interaction modality. Based on a theoretical review of the available design space potential <b>interaction</b> <b>techniques</b> are discussed. Some of these were implemented in an experimental game to enable practical evaluation. We provide an overview of the game and present intial experiences with the vision-based <b>interaction</b> <b>techniques</b> employed...|$|R
40|$|This paper {{presents}} {{a description of}} the <b>interaction</b> <b>techniques</b> used in the Chapel Hill Immersive Modeling Program (CHIMP). CHIMP is intended for the preliminary stages of architectural design. It is an immersive system; users work directly within a virtual world. The main goal has been to develop <b>interaction</b> <b>techniques</b> that exploit the benefits of working immersed while compensating for its limitations. <b>Interaction</b> <b>techniques</b> described and discussed in this paper include: • Action at a distance • Look-at menus • Remote controls (hand-held widgets) • Constrained object manipulation using twohands • Two-handed control panel interaction • Worlds in miniature • Interactive number...|$|R
40|$|Usable {{three-dimensional}} (3 D) <b>interaction</b> <b>techniques</b> {{are difficult}} to design, implement, and evaluate. One {{reason for this is}} a poor understanding of {{the advantages and disadvantages of}} the wide range of 3 D input devices, and of the mapping between input devices and <b>interaction</b> <b>techniques.</b> We present an analysis of Pinch Gloves™ and their use as input devices for virtual environments (VEs). We have developed a number of novel and usable <b>interaction</b> <b>techniques</b> for VEs using the gloves, including a menu system, a technique for text input, and a two-handed navigation technique. User studies have indicated the usability and utility of these techniques...|$|R
50|$|A scrollbar is an <b>interaction</b> <b>technique</b> or widget {{in which}} {{continuous}} text, pictures, {{or any other}} content can be scrolled in a predetermined direction (up, down, left, or right) on a computer display, window, or viewport so {{that all of the}} content can be viewed, even if {{only a fraction of the}} content can be seen on a device's screen at one time. It offers a solution to the problem of navigation to a known or unknown location within a two-dimensional information space. It was also known as a handle in the very first GUIs. They are present in a wide range of electronic devices including computers, graphing calculators, mobile phones, and portable media players. The user interacts with the scrollbar elements using some method of direct action, the scrollbar translates that action into scrolling commands, and the user receives feedback through a visual updating of both the scrollbar elements and the scrolled content.|$|E
50|$|Brichter {{experimented with}} two primary {{iterations}} of pull-to-refresh before releasing the final version. In the first iteration, users triggered the refresh when they scrolled across an invisible threshold on the screen. However, in this iteration {{there was no}} visual feedback that signaled to users that a refresh was occurring. Brichter believed it necessary to provide users with visual feedback, so the second and final iteration of pull to refresh added visual feedback when refreshing so users could better understand the gesture. This final iteration also included text alerting users that if {{the top of the}} page is pulled beyond a threshold and subsequently released, a refresh would occur. Brichter included this description text because he felt that since the gesture presented a new <b>interaction</b> <b>technique</b> that most users likely have not seen before, the purpose of the gesture had to be explicitly stated for users to understand its functionality. These two iterations of pull to refresh were created in a single afternoon with no user testing. Brichter states that he manually tested the iterations and the invisible threshold of the gesture until it felt “right” - that the threshold cannot be too small causing people to accidentally trigger the gesture, but it also cannot be too big causing it to be difficult for users to activate.|$|E
40|$|In this paper, {{we present}} a novel <b>interaction</b> <b>technique</b> – {{combining}} mobile projection and visible, fiducial marker based information display. We vision it to be suitable for small groups e. g. for narrative playful experiences and guided on places, where physical tags would be disturbing. This <b>interaction</b> <b>technique,</b> where one person (guide) is projecting a marker and other users can read it with their mobile devices, enables in situ information delivery while the guide can control {{the dynamics of the}} situation. We present an example use case of using the <b>interaction</b> <b>technique</b> on a guided tour, and a preliminary results from the user evaluatio...|$|E
40|$|This article {{summarizes}} the process I have developed to describe, evaluate and facilitate {{the creation of}} novel <b>interaction</b> <b>techniques.</b> First, it presents the CIS model for describing <b>interaction</b> <b>techniques</b> and predicting their effectiveness in real contexts of use. CIS shows {{that there is no}} absolute best technique but that performance depends on the context of use. The article then shows how to improve a technique by optimizing subcomponents of its CIS structure. Finally it describes SwingStates, a toolkit designed to help develop novel <b>interaction</b> <b>techniques</b> by exploring different CIS structures. ACM Classification: D. 2. 2 [Design tools and Techniques]...|$|R
40|$|We {{describe}} {{a demonstration of}} four novel <b>interaction</b> <b>techniques</b> for a cubic head-coupled 3 D display. The interactions illustrated include: viewing a static scene, navigating through a large landscape, playing with colliding objects inside a box, and stylus-based manipulation of objects. Users experience new <b>interaction</b> <b>techniques</b> for 3 D scene manipulation in a cubic display...|$|R
40|$|Designing {{non-traditional}} {{user interfaces}} is a challenging task for designers. NiMMiT, {{a high level}} description for 3 D multimodal interaction in virtual environments, provides a means to design, prototype or communicate about <b>interaction</b> <b>techniques.</b> The focus is on {{making it possible for}} designers to create new <b>interaction</b> <b>techniques</b> while lowering implementation efforts. status: publishe...|$|R
40|$|Cursive: A novel <b>interaction</b> <b>technique</b> for {{controlling}} expressive avatar gesture We are developing an <b>interaction</b> <b>technique</b> for rich nonverbal communication through an avatar. By writing a single letter on a pen tablet device, a user can express their ideas or intentions, nonverbally, using their avatar body. Our system solves the difficult problem {{of controlling the}} movements of a highly articulated, 3 D avatar model using a common input device {{within the context of}} an office environment. We believe that writing is a richly expressive and natural means {{for controlling}} expressive avatar gesture. KEYWORDS:Avatars, computer-mediated communication, virtual environments, gesture, novel <b>interaction</b> <b>technique,</b> pen gesture, nonverbal...|$|E
40|$|We propose an <b>interaction</b> <b>technique</b> for editing splines that {{is aimed}} at {{professional}} graphic designers. These users do not {{take full advantage of}} existing spline editing software because their mental representations of drawings do not match the underlying conceptual model of the software. Although editing splines by specifying control points and tangents may be appropriate for engineers, graphic designers think more in terms of strokes, shapes, and gestures appropriate for editing drawings. Our <b>interaction</b> <b>technique</b> matches the latter model: curves can be edited by means of marks, similar to the way strokes are naturally overloaded when drawing on paper. We describe this <b>interaction</b> <b>technique</b> and the algorithms used for its implementation. KEYWORDS: Mark-based interaction, Gestures, Spline editing, Interaction models, Graphic design, CAD...|$|E
40|$|This paper {{introduces}} a novel kinesthetic <b>interaction</b> <b>technique</b> for interactive floors. The interaction techniques utilize vision-based limb tracking on an interactive floor – a 12 m² glass surface with bottom projection. The kinesthetic <b>interaction</b> <b>technique</b> {{has been developed}} for an interactive floor implemented in a school square. The paper discusses the kinesthetic <b>interaction</b> <b>technique</b> and its potentials {{in the domain of}} learning applications: Kinesthetic interaction supports body-kinesthetic learning as argued in the learning literature. Kinesthetic interaction is fun and motivating thus encourages children to explore and learn. Kinesthetic interaction on large display surfaces supports collaborative, co-located play and learning through communication and negotiation among the participants. Finally, the paper discusses prospects and challenges in development of kinesthetic interaction for interactive floors...|$|E
40|$|I present mouse-based, symmetric, bimanual <b>interaction</b> <b>techniques</b> as a {{solution}} to both the lack of spatial input and the lack of natural <b>interaction</b> <b>techniques</b> for direct manipulation in desktop interfaces. I outline the techniques I have implemented and tested thus far and the techniques and interfaces yet to be developed as part of my doctoral thesis...|$|R
40|$|This paper {{introduces}} new <b>interaction</b> <b>techniques</b> for Smart-Skin, {{a sensor}} architecture for freehand manipulation. This sensor recognizes multiple hand positions and shapes and calculates {{the distance between}} the hand and the surface by using capacitive sensing and a mesh-shaped antenna. Our <b>interaction</b> <b>techniques</b> enable the users to use not only their hands, but also their fingers concurrently...|$|R
40|$|As {{immersive}} {{virtual environment}} (VE) applications become more complex, {{it is clear}} that we need a firm understanding of the principles of VE interaction. In particular, designers need guidance in choosing three-dimensional <b>interaction</b> <b>techniques.</b> In this paper, we present a systematic approach, testbed evaluation, for the assessment of <b>interaction</b> <b>techniques</b> for VEs. Testbed evaluation uses formal frameworks and formal experiments with multiple independent and dependent variables in order to obtain a wide range of performance data for VE <b>interaction</b> <b>techniques.</b> We present two testbed experiments, covering techniques for the common VE tasks of travel and object selection/manipulation. The results of these experiments allow us to form general guidelines for VE interaction, and to provide an empirical basis for choosing <b>interaction</b> <b>techniques</b> in VE applications. This has been shown to produce measurable usability gains in a real-world VE application. 1. INTRODUCTION Applications of imm [...] ...|$|R
