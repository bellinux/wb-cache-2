21|671|Public
2500|$|Waverly {{was caught}} {{somewhat}} {{unprepared for the}} river's height, causing officials to scramble; the river crested locally at [...] on June 10, with a second crest of 13.73ft experienced on June 15. More than 2500 people were evacuated. [...] Much of the town was inundated with many homes and businesses experiencing flood damage. [...] In the flood's aftermath 151 homes were reported to be flood - impacted, with 69 of the properties eventually being bought out by the city. [...] Since the floods of 2008, Waverly has been made an official <b>forecast</b> <b>point</b> to better help with predicting river crest levels and impact information - information that was not available during the floods of 2008. [...] In 2011, Waverly completed {{the construction of an}} 4.3 million dollar inflatable dam. [...] The dam is designed to protect more than 400 homes and businesses {{in the event of a}} 100-year flood and will provide relief in 500-year flood events such as 2008.|$|E
50|$|Its primary <b>forecast</b> <b>point</b> {{covers the}} Los Angeles County Coast, the Los Angeles County Valleys {{including}} the Santa Clarita Valley, Los Angeles County Mountains excluding the Santa Monica Range, and the Antelope Valley.|$|E
50|$|A typical warning will {{state the}} <b>forecast</b> <b>point</b> {{covered by the}} warning, the current flood stage, the {{established}} flood stage, and the expected crest, which is issued by the River Forecast Center. The crest will usually occur at least six hours {{after the start of}} the event and the flooding can be caused by heavy rain {{in the vicinity of the}} river, melting snow, or ice jams.|$|E
40|$|The National Weather Service 2 ̆ 7 s Advanced Hydrologic Prediction Service (AHPS) {{currently}} provides river flow/stage and forecast {{information at}} more than 3, 600 <b>forecast</b> <b>points</b> across the United States. Along with this information, AHPS describes potential flood impacts that may occur within the upstream and downstream influence of the <b>forecasts</b> <b>points.</b> However, similar information is not available for low flow events. In order to incorporate low flow information into the current AHPS river forecast database, the National Weather Service has undertaken pilot studies to obtain relevant low flow impact information near 83 <b>forecast</b> <b>points</b> in the Upper Mississippi, Upper Missouri, and North Platte river basins...|$|R
3000|$|As can be {{inferred}} from above, that model-based forecast of recoverable resources N [...] R [...] f [...] may not be equal to actual recoverable resources N [...] R, which is mainly due to the inequality of forecast and actual cumulative production before <b>forecasting</b> <b>point</b> (t = m); whereas after the <b>forecasting</b> <b>point,</b> Goal Seek of b ensures the estimated residual recoverable resource to be an actual one, thus guaranteeing {{the accuracy of the}} forecasting base.|$|R
40|$|The aim of {{this paper}} is to compare the {{forecasting}} performance of SETAR and GARCH models against a linear benchmark using historical data for the returns of the Japanese yen/US dollar exchange rate. The relative performance of the models is evaluated on <b>point</b> <b>forecasts</b> and on interval <b>forecasts.</b> <b>Point</b> <b>forecasts</b> evaluation over the whole forecast period indicates that the performance of the models, when distinguishable, tends to favour the linear models. However, we show that if the evaluation of <b>point</b> <b>forecasts</b> is conducted over distinct subsamples or specific regimes there is more evidence of forecasting gains, especially from the SETAR models. Moreover, when we evaluate the validity of interval forecasts, the results produce clear evidence of the superiority of the non-linear models, and tend to favour especially the GARCH models. nonlinearity, asymmetry, <b>forecasting</b> accuracy, <b>point</b> <b>forecasts,</b> interval forecasts, exchange rates...|$|R
50|$|PECOTA {{accounts}} for {{these sorts of}} factors by creating not a single <b>forecast</b> <b>point,</b> as other systems do, but rather a range of possible outcomes that the player could expect to achieve {{at different levels of}} probability. Instead of telling you that it's going to rain, we tell you that there's an 80% chance of rain, because 80% of the time that these atmospheric conditions have emerged on Tuesday, it has rained on Wednesday.|$|E
5000|$|Waverly {{was caught}} {{somewhat}} {{unprepared for the}} river's height, causing officials to scramble; the river crested locally at 19.33 ft on June 10, with a second crest of 13.73 ft experienced on June 15. More than 2500 people were evacuated. Much of the town was inundated with many homes and businesses experiencing flood damage. In the flood's aftermath 151 homes were reported to be flood - impacted, with 69 of the properties eventually being bought out by the city. [...] Since the floods of 2008, Waverly has been made an official <b>forecast</b> <b>point</b> to better help with predicting river crest levels and impact information - information that was not available during the floods of 2008. [...] In 2011, Waverly completed {{the construction of an}} 4.3 million dollar inflatable dam. The dam is designed to protect more than 400 homes and businesses {{in the event of a}} 100-year flood and will provide relief in 500-year flood events such as 2008.|$|E
3000|$|Step 1 : Set the {{electrical}} element parameters, including branch, generator, load, etc. Meanwhile, set P̅_T,k, D_r,l, p̅_LOLP,l and ρ_i. Initialize {{the minimum value}} of local reserve requirement D_r^ 0 = [D_r, 1 ^ 0,D_r, 2 ^ 0, [...]...,D_r,N_a - 1 ^ 0,D_r,N_a^ 0] [...]. In addition, the forecasted wind output distribution is provided, including <b>forecast</b> <b>point</b> value and the statistical deviation.|$|E
5000|$|... where At is {{the actual}} value at time t and Ft is the {{forecast}} value at time t. Variable N represents number of <b>forecasting</b> <b>points.</b> The function [...] is sign function and [...] is the indicator function.|$|R
50|$|The {{difference}} between At and Ft is {{divided by the}} Actual value At again. The absolute value in this calculation is summed for every <b>forecasted</b> <b>point</b> in time and divided {{by the number of}} fitted points n. Multiplying by 100 makes it a percentage error.|$|R
40|$|For the {{implementation}} of lumped conceptual models in flood <b>forecasting,</b> <b>point</b> precipitation time series records need to be aggregated into regionally averaged time series. Therefore a question arises: how many rainfall gauges inside a specific river basin are needed to provide sufficient precipitation records for the rainfall-runoff models...|$|R
40|$|This paper {{provides}} {{geographic information}} system (GIS) methods and empirical models to <b>forecast</b> <b>point</b> demand for home-delivered goods. A point forecast consists of stops on a street network, including demand at each stop. The purpose of the forecast is to support a network optimization model, based on the traveling salesman problem, to locate one or more new facilities in a region. We illustrate our approach with {{a case study of}} home-delivered meals (meals on wheels) in Allegheny County, Pennsylvania...|$|E
40|$|A large bibliographic survey {{provided}} data on Trypanosoma cruzi serology {{covering the}} period l 948 -l 984. Epidemiological-demographic methods provided {{an estimate of}} 11 % for the prevalenceof positive serology in Brazil, by 1984. Significant temporal trends were observed {{for most of the}} Brazilian geographical regions as well as for Brazil, as a whole. The parabolic curve that fit best for the entire country, indicates that by 1991, the incidence of new positive serology would be close to zero. This conclusion needs further fine-adjustment, since the <b>forecast</b> <b>point</b> is somewhat distant from the measured period...|$|E
40|$|An {{objective}} {{synoptic climatology}} of photochemical smog episodes in Sydney, Australia, are presented. This climatology was produced using multivariate statistical techniques including principle component analysis and cluster analysis {{in order to}} assign days into meteorologically homogeneous synoptic categories. The meteorological inputs to these statistical inputs include both surface and upper air observations for warm months - October to March, over a ten year period, 1992 - 2001. Days in which Sydney's air quality may {{have been affected by}} bushfires were removed from analyses. The results from the research are expected to be useful from the <b>forecast</b> <b>point</b> of view for future sources in Sydney and surrounding areas. link_to_subscribed_fulltex...|$|E
40|$|Turning {{points in}} {{commodity}} returns {{are important for}} decisions of policy makers, commodity producers and consumers reliant on medium term outcomes. However, <b>forecasting</b> of turning <b>points</b> has been a neglected feature of forecasting, especially in commodity markets. I <b>forecast</b> turning <b>points</b> in metals price returns using Bayesian Decision Theory. The method produces a probabilistic statement about our belief of a turning point occurring in the next period which, combined with a decision rule based on a loss function generates optimal turning <b>point</b> <b>forecasts.</b> This method produces positive results in <b>forecasting</b> turning <b>points</b> in metals returns, with the simple linear models investigated producing more accurate turning <b>point</b> <b>forecasts</b> than naive models across {{a number of different}} evaluation methods for the general case and for the specific example of a producing firm. ...|$|R
50|$|A River Flood Advisory {{is issued}} by the National Weather Service of the United States when minor {{flooding}} at formal <b>forecast</b> <b>points</b> with river gaging sites and established flood stages is possible. Flooding may have many causes, such as heavy rain {{in the vicinity of}} the river, melting snow or ice jams.|$|R
500|$|Because {{the output}} of {{forecast}} models based on atmospheric dynamics requires corrections near ground level, model output statistics (MOS) were developed in the 1970s and 1980s for individual <b>forecast</b> <b>points</b> (locations). The MOS apply statistical techniques to post-process {{the output of}} dynamical models with the most recent surface observations and the <b>forecast</b> <b>point's</b> climatology. [...] This technique can correct for model resolution as well as model biases. [...] Even with the increasing power of supercomputers, the forecast skill of numerical weather models only extends to about two weeks into the future, since the density and quality of observationstogether with the chaotic nature of the partial differential equations {{used to calculate the}} forecastintroduce errors which double every five days. The use of model ensemble forecasts since the 1990s helps to define the forecast uncertainty and extend weather forecasting farther into the future than otherwise possible.|$|R
40|$|This paper {{examines}} whether {{indicators of}} consumer and business confidence can predict movements in GDP over {{the business cycle}} for four European economies. The empirical methodology used to investigate {{the properties of the}} data comprises cross-correlation statistics, implementing an approach developed by den Haan ["Journal of Monetary Economics" (2000), Vol. 46, pp. 3 - 30]. The predictive power of confidence indicators is also examined, investigating whether they can predict discrete events, namely economic downturns, and whether they can quantitatively <b>forecast</b> <b>point</b> estimates of economic activity. The results indicate that both consumer and business confidence indicators are procyclical and generally {{play a significant role in}} predicting downturns. Copyright Blackwell Publishing Ltd and the Department of Economics, University of Oxford 2007. ...|$|E
40|$|This paper elaborates a {{forecasting}} {{model that}} uses a known starting point over a forecast horizon and historical values from past time periods to develop projections. Essentially, the model consists of estimating the trajectory of forecasts from a starting period to a projected end point in the future. In other work, Selzer (1997, 1998) suggested similar nonlinear models that use past observations to develop projections with the last <b>forecast</b> <b>point</b> set by methods different from the algorithm used to estimate other forecasted points. These studies showed that the proposed models worked as good or better than models that used the historical observations only to derive a curve giving forecasted values {{as a function of}} time...|$|E
40|$|The {{present study}} {{developed}} an {{artificial neural network}} (ANN) model to overcome the difficulties in training the ANN models with continuous data consisting of rainy and non-rainy days. Among the six models analyzed the ANN model which used generalized feedforward type network and a hyperbolic tangent function and a combination of meteorological parameters (relative humidity, air pressure, wet bulb temperature and cloudiness), and the rainfall {{at the point of}} forecasting and rainfall at the surrounding stations, as an input for training of the model was found most satisfactory in forecasting rainfall in Bangkok, Thailand. The developed ANN model was applied to derive rainfall forecast from 1 to 6 h ahead at 75 rain gauge stations in the study area as <b>forecast</b> <b>point</b> from the data of 3 consecutive years (1997 &ndash; 1999). Results were highly satisfactory for rainfall forecast 1 to 3 h ahead. Sensitivity analysis indicated that the most important input parameter beside rainfall itself is the wet bulb temperature in forecasting rainfall. Based on these results, it is recommended that the developed ANN model can be used for real-time rainfall forecasting and flood management in Bangkok, Thailand...|$|E
40|$|The aim of {{this paper}} is to compare the {{forecasting}} performance of SETAR and GARCH models against a linear benchmark using historical data for two bilateral dollar exchange rates, namely the Japanese Yen and the British Pound. The analysis is carried out with series sampled at weekly and daily frequencies. The relative performance of the models is evaluated on <b>point</b> <b>forecasts</b> and on interval <b>forecasts.</b> <b>Point</b> <b>forecasts</b> evaluation tends to favour on average the linear models, though the analysis produces some evidence of forecasting gains from nonlinear models in sub-samples characterised by stronger non-linearities. When we evaluate the validity of interval forecasts, the results clearly favour the GARCH model and show that the AR and SETAR forecasts are not correctly conditionally calibrated...|$|R
40|$|A {{comparison}} of the <b>point</b> <b>forecasts</b> and the central tendencies of probability distributions of inflation and output growth of the SPF indicates that the <b>point</b> <b>forecasts</b> are sometimes optimistic relative to the probability distributions. We consider and evaluate {{a number of possible}} explanations for this finding, including the degree of uncertainty concerning the future, computational costs, delayed updating, and asymmetric loss. We also consider the relative accuracy of the two sets of <b>forecasts.</b> Rationality; <b>point</b> forecasts; probability distributions...|$|R
40|$|BACKGROUND Any {{improvement}} in the forecast accuracy of life expectancy would be beneficial forpolicy decision regarding the allocation of current and future resources. In this paper, Irevisit some methods for forecasting age-specific life expectancies. OBJECTIVE This paper proposes a model averaging approach to produce accurate <b>point</b> <b>forecasts</b> ofage-specific life expectancies. METHODS Illustrated by data from fourteen developed countries, we compare <b>point</b> and interval <b>forecasts</b> among ten principal component methods, two random walk methods, and two univariate time-series methods. RESULTS Based on averaged one-step-ahead and ten-step-ahead forecast errors, random walk withdrift and Lee-Miller methods {{are the two most}} accurate methods for producing <b>point</b> <b>forecasts.</b> By combining their <b>forecasts,</b> <b>point</b> <b>forecast</b> accuracy is improved. As measured by averaged coverage probability deviance, the Hyndman-Ullah methods generally providemore accurate interval forecasts than the Lee-Carter methods. However, the Hyndman-Ullah methods produce wider half-widths of prediction interval than the Lee-Carter methods. CONCLUSIONS Model averaging approach should be considered to produce more accurate <b>point</b> <b>forecasts.</b> COMMENTS This study is a sequel to another Demographic Research paper by Shang, Booth and Hyndman (2011), in which the authors compared the principal component methods for forecasting age-specific mortality rates and life expectancy at birth...|$|R
40|$|CI-FLOW {{is a new}} {{technology}} being utilized to identify flood hazards. CI-FLOW stands for the Coastal and Inland Flooding Observation and Warning project. CI-FLOW was implemented ten {{years ago by the}} directors of Sea Grant and NOAA, along with other North Carolina partners and state agencies. The National Weather Service has one <b>forecast</b> <b>point</b> in Louisburg, NC, on the Tar River. The addition of additional USGS gauges is important for effective identification of flood hazards. The accurate and timely identification of flood hazards is important given the growing and seasonally fluctuating population of Dare County. Approximately one-half of the housing in Dare County is seasonal, and emergency mangers must account for this. For instance, it is estimated that in 2030, if a category three hurricane hit during a time of peak tourism, emergency managers would need to clear 30, 000 vehicles in 31 hours. Given the importance flood identification, an integrated approach is needed for accurate and timely identification. The utilization of storm surge models and observations of weather and river levels, is key in flood identification. Three-dimensional images linked with real-time data can help assess the dangers of a flood and aid in emergency management decisions. CI-FLOW is being used to time crests and discharges along with a coupled model which links water and storm surge discharge. In conclusion, the CI-FLOW project is a multi-agency evaluation of new technologies to better identify floods in the Tar-Pamlico and Neuse river basins...|$|E
40|$|Concentrations of {{tropospheric ozone}} often exceed Australian air quality goals in Sydney during summer. However, {{features}} in the occurrence of ozone in Sydney are yet to be fully explained. Meteorological conditions associated with ozone episodes in Sydney are caused by complex interactions between synoptic and meso-scale processes. This paper discusses the meteorological influences behind ozone pollution episodes in Sydney. A synoptic climatology of ozone episodes in Sydney was generated using multivariate statistical techniques, including principal component analysis (PCA) and a two-stage cluster analysis, to classify days into meteorologically homogeneous synoptic categories. Surface and upper air meteorological data for warm months (Oct–Mar) over a 10 -year period were used as input into the statistical analyses. Eleven synoptic categories were identified in Sydney during the warm season and ozone concentrations associated {{with each of the}} synoptic categories were investigated. One synoptic category was found to be associated almost exclusively with high pollution concentrations. High ozone concentrations were found to be associated with a high-pressure system located in the middle to eastern Tasman Sea producing light northwesterly gradient winds, an afternoon sea breeze, high afternoon temperatures, a shallow mixing height at the coast and warming aloft during the day. Over 90 % of all days exceeding current air quality goals for ozone in Sydney fell within the synoptic category associated with the highest ozone concentrations. It is envisaged that results from this research will be useful to Australian regulatory bodies from both a <b>forecast</b> <b>point</b> of view and for the siting of future ozone precursor sources in Sydney and surrounding regions. 15 page(s...|$|E
40|$|ABSTRACT The Truckee-Carson RiverWare {{system is}} being {{developed}} as a joint effort between the Bureau of Reclamation (BOR) and the Truckee River Operating Agreement Implementation Coordinator’s Office. The framework of the current system consists of three interlinked RiverWare models that perform water accounting, hydrologic forecasting, and river-reservoir operations scheduling. The operations model requires daily streamflow (naturalized reservoir inflows and streamflow forecasts at both gaged and ungaged locations) and reservoir evaporation forecasts at numerous forecast points throughout the basin. The forecast model is used to provide these hydrologic forecasts to the operations model. The Truckee-Carson RiverWare system is currently used for general short-term planning purposes, but not for official water accounting or scheduling reservoir operations. In the future the RiverWare system will be crucial for water accounting and operations scheduling under implementation of the new Truckee River Operating Agreement. The current configuration of the forecast model was designed around the use of volume forecasts at three key forecast points in the basin. The forecast model distributes all of the input volumes by time into daily flow hydrographs, and also distributes the Truckee River at Farad volumes spatially to upstream forecast points. A “similar-years ” approach is used to generate the daily flow hydrographs. The model determines similar years from historic runoff volumes and peak flow dates. The forecast model can also be configured to allow daily forecasts at any <b>forecast</b> <b>point</b> {{from a variety of}} other sources. Other sources of daily forecasts are the U. S. Geological Survey’s (USGS) Modular Modeling System model for the Truckee Basin, National Weather Service models for several forecast points in the basin, as well as any future models that may be developed...|$|E
5000|$|A river {{flood warning}} is {{issued by the}} National Weather Service of the United States when <b>forecast</b> <b>points</b> with formal river gaging sites and {{established}} flood stages along rivers, where flooding has already been forecast, is imminent or occurring. The National Weather Service defines river flooding as [...] "the inundation of normally dry areas {{as a result of}} increased water levels in an established water course." ...|$|R
5000|$|Husmeier, D. (1999), Neural Networks for Conditional Probability Estimation: <b>Forecasting</b> Beyond <b>Point</b> Predictions, Berlin: Springer Verlag, [...]|$|R
40|$|Forecasts for peak streamflows from annual {{snow melt}} {{are widely used}} by {{emergency}} management, reservoir operators, and river recreationists in the Colorado and Great Basins. The Colorado Basin River Forecast Center (CBRFC) has produced these forecasts {{for many years in}} the spring {{months leading up to the}} snowmelt. However, only recently has a systematic effort to verify these forecasts been undertaken. Historical forecasts were verified along with reforecasts from the River Forecast System to produce a suite of verification statistics for current <b>forecast</b> <b>points.</b> Preliminary results, although site specific, suggest that both forecasts and reforecast have better skill that climatology and that skill improves with lead time. The resulting analysis will be used to drive changes to the peak flow forecast program at the CBRFC. For example, observed forecast skill may be used together with stakeholder requirements to determine forecast issuance dates and forecast frequency. Results may also be used to validate new and existing forecast procedures, evaluate the validity of current and future <b>forecast</b> <b>points,</b> and provide future tools to stakeholders to evaluate forecast performance...|$|R
40|$|Survey data on {{expectations}} and economic forecasts {{play an important}} role in providing better insights into how economic agents make their own forecasts, what factors do affect the accuracy of these forecasts and why agents disagree in making them. Uncertainty is also important for better understanding many areas of economic behavior. Several approaches to measure uncertainty and disagreement have been proposed but a lack of direct observations and information on uncertainty and disagreement lead to ambiguous definitions of these two concepts. Using data from the European Survey of Professional Forecasters (SPF), which provide <b>forecast</b> <b>point</b> estimates and probability density forecasts, we consider several measures of uncertainty and disagreement at both aggregate and individual level. We overcome the problem associated with distributional assumptions of probability density forecasts by using an approach that does not assume any functional form for the individual probability densities but just approximating the histogram by a piecewise linear function. We extend earlier works to the European context for the three macroeconomic variables: GDP, inflation and unemployment. Moreover, we analyze how these measures perform with respect to different forecasting horizons. Looking at point estimates and disregarding the individual probability information provides misestimates of disagreement and uncertainty. Comparing the three macroeconomic variables of interest, uncertainty and disagreement are higher for GDP and inflation than unemployment, at short and long horizons. Besides this, it is difficult to find a common behavior between uncertainty and disagreement among the variables: results do not support evidence that, if uncertainty or disagreement are relatively high for one of the variable than it is the same for the others...|$|E
40|$|Ensemble {{forecasts}} {{have been}} used to increase the accuracy of Quantitative Precipitation Forecasts (QPF). Because of the challenging nature of developing a QPF, forecasters express uncertainty through Probability of Precipitation (POP) forecasts. POP forecasts can be developed using the percentage of agreement among ensemble members for a given <b>forecast</b> <b>point.</b> POP forecasts can also be developed through a more intricate statistical interpretation of model output called post-processing. There are numerous post-processing approaches to create POP forecasts, which include ensemble member agreement, calibration, binning precipitation amounts, and neighborhood approaches. The main purpose of this research is to expand upon previous works regarding the relationship between QPF and POP forecasts through the neighborhood approach. By redefining the traditional ensemble {{through the use of a}} neighborhood ensemble of points, previous works had found that a single deterministic model can achieve similar or better skill than traditional approaches. Ensemble forecasts provided by the Center for Analysis and Prediction of Storms from the 2007, 2008, and 2010 NOAA Hazardous Weather Testbed Spring Experiments were used with additional variations of the neighborhood approach in order to determine if even better skill could be obtained. Four neighborhood variation tests were conducted using Brier scores and Brier skill scores for both 20 km and 4 km horizontal grid spacing, and results were compared skill scores of traditional approaches. Results had shown that some neighborhood variations could provide better skill than previously obtained, as well as outperforming traditional methods. Using a single model for POP generation through the neighborhood variations shows operational forecasting potential by providing more accurate forecasts than traditional methods, as well as requiring fewer computational resources that could be focused on improving a single deterministic model...|$|E
40|$|Forecasts of {{the focal}} {{mechanisms}} of future earthquakes {{are important for}} seismic hazard estimates and Coulomb stress and other models of earthquake occurrence. Here we report on a high-resolution global forecast of earthquake rate density {{as a function of}} location, magnitude, and focal mechanism. In previous publications we reported forecasts of 0. 5 degree spatial resolution, covering the latitude range magnitude, and focal mechanism. In previous publications we reported forecasts of 0. 5 degree spatial resolution, covering the latitude range from - 75 to + 75 degrees, based on the Global Central Moment Tensor earthquake catalog. In the new forecasts we've improved the spatial resolution to 0. 1 degree and the latitude range from pole to pole. Our focal mechanism estimates require distance-weighted combinations of observed focal mechanisms within 1000 km of each grid point. Simultaneously we calculate an average rotation angle between the forecasted mechanism and all the surrounding mechanisms, using the method of Kagan & Jackson proposed in 1994. This average angle reveals the level of tectonic complexity of a region and indicates the accuracy of the prediction. The procedure becomes problematical where longitude lines are not approximately parallel, and where earthquakes are so sparse that an adequate sample spans very large distances. North or south of 75 degrees, the azimuths of points 1000 km away may vary by about 35 degrees. We solved this problem by calculating focal mechanisms on a plane tangent to the earth's surface at each <b>forecast</b> <b>point,</b> correcting for the rotation of the longitude lines at the locations of earthquakes included in the averaging. The corrections are negligible between - 30 and + 30 degrees latitude, but outside that band uncorrected rotations can be significantly off. Comment: 17 figure...|$|E
30|$|The {{implications}} {{of the labor market}} demand and supply <b>forecasts</b> all <b>point</b> in the direction of an increasing wage-income inequality under the muddling through scenario.|$|R
40|$|We {{describe}} {{two methods}} for forecasting a grouped time series, which provides <b>point</b> <b>forecasts</b> that are aggregated appropriately across {{different levels of}} the hierarchy. Using the regional infant mortality counts in Australia, we investigate the one-step-ahead to ten-step-ahead <b>point</b> <b>forecast</b> accuracy, and examine statistical significance of the <b>point</b> <b>forecast</b> accuracy between methods. Furthermore, we introduce a novel bootstrap methodology for constructing point-wise prediction interval in a grouped time series, investigate the interval forecast accuracy, and examine the statistical significance of the interval forecast accurac...|$|R
40|$|We {{introduce}} a survey-based measure of uncertainty about future inflation, asking consumers for density forecasts across inflation outcomes. Consumers are {{willing and able}} to express uncertainty, showing high response rates and response patterns that are reliably related to qualitative measures of uncertainty. Heterogeneity in expressed uncertainty is associated with demographic characteristics and financial literacy, and measures of central tendency derived from density forecasts are strongly correlated with <b>point</b> <b>forecasts.</b> Furthermore, expressed uncertainty is positively related to <b>point</b> <b>forecast</b> levels and to larger revisions in <b>point</b> <b>forecasts</b> over time...|$|R
