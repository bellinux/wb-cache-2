34|487|Public
50|$|The {{second step}} of the {{encoding}} process is the application of an <b>encoding</b> <b>object</b> to a hidden ECN type. The value to be encoded {{will be one of}} the possible values of an ASN.1 type defined in the ASN.1 specification, and the encoding process will select the hidden ECN type of that ASN.1 type and will apply the appropriate <b>encoding</b> <b>object</b> to it.|$|E
5000|$|... by {{utilizing}} a field containing {{the index of}} the alternative, and which {{was added to the}} ECN type by another <b>encoding</b> <b>object</b> applied at an earlier stage; ...|$|E
5000|$|... by {{utilizing}} a (typically boolean) field whose value indicates {{presence or absence}} of the optional component, and which was inserted in the ECN type by another <b>encoding</b> <b>object</b> applied at an earlier stage; ...|$|E
5000|$|... some <b>encoding</b> <b>objects</b> specify how to {{represent}} {{the number of items}} of a list (...) these <b>encoding</b> <b>objects</b> can only be applied to ECN types that are [...] types; ...|$|R
5000|$|... some <b>encoding</b> <b>objects</b> specify how to {{represent}} {{the presence or absence}} of a component of a [...] type that is declared these <b>encoding</b> <b>objects</b> can only be applied to ECN types that are optional components of a [...] type; ...|$|R
5000|$|... some <b>{{encoding}}</b> <b>objects</b> {{specify the}} bit-level encoding of the ECN type; ...|$|R
50|$|Lastly we {{introduce}} {{the concept of}} <b>encoding</b> <b>object.</b> This {{is a very important}} element of the ECN language, and refers to each individual encoding rule that is part of an ECN specification and is applied to an ECN type or ECN constructor keyword, either built-in or user-defined, occurring in the specification.|$|E
5000|$|In ECN {{there are}} several kinds of {{encoding}} objects. Some encoding objects completely determine the actual bit-level encoding of simple ECN types and are the easiest to understand. Others apply to ECN constructor keywords rather than to ECN types, and determine some structural aspects of the encoding of the complex ECN type (or part of it) constructed by an ECN constructor keyword (but do not specify its entire encoding). Others work by replacing an ECN type (or a part of it) with another ECN type, which must then be encoded by applying a different <b>encoding</b> <b>object</b> to it.|$|E
5000|$|The {{first step}} of the {{encoding}} process is the automatic generation of hidden ECN types from all ASN.1 types present in the ASN.1 specification. The hidden ECN types corresponding to complex user-defined ASN.1 types can be modified by a mechanism called coloring, which consists in replacing {{the names of the}} types of some of their components with synonyms. It is also possible to replace the ECN built-in constructor keywords (e.g., , [...] ) occurring in a hidden ECN type with synonyms. In ECN there are a few built-in synonyms for both constructor keywords and built-in types (e.g., [...] is a synonym of , [...] is a synonym of [...] ), but a user of the language can define both user-defined types and user-defined constructor keywords as synonyms of others. The purpose of the coloring step is to prepare a hidden ECN type for the next step, which is the encoding of its components, in case it is necessary to encode in a different way different occurrences of the same ECN type or different occurrences of the same ECN constructor keyword present in the hidden ECN type. For example, a complex hidden ECN type might contain two lists (...) , but one list is to be encoded by inserting a count field before the first item of the list, and the other is to be encoded by inserting a terminating pattern after the last item of the list. This can be done, for example, by replacing the first [...] keyword in the hidden ECN type with, say, , by replacing the second [...] keyword with, say, , and by declaring these two names as user-defined synonyms of the ECN constructor keyword [...] Once these two different constructor keywords have been included in the hidden ECN type, each of the two lists can be encoded with a different <b>encoding</b> <b>object.</b>|$|E
5000|$|The most {{important}} kinds of <b>encoding</b> <b>objects</b> in ECN are listed below: ...|$|R
5000|$|Here {{are some}} typical {{ways in which}} these <b>encoding</b> <b>objects</b> can {{represent}} {{the length of a}} list: ...|$|R
5000|$|Here {{are some}} typical {{ways in which}} these <b>encoding</b> <b>objects</b> can {{indicate}} which of the alternatives of a [...] is present: ...|$|R
40|$|Abstract. The current {{research}} sought {{to construct a}} computational model of human navigation for virtual three dimensional environments. The model was implemented within the ACT-R cognitive architecture [1]. The navigation model incorporates visual search, <b>encoding</b> <b>object</b> features and spatial relationships, motion, obstacle avoidance, and incidental visual memory. Keywords: walking navigation, ACT-R, digital human model, incidental visual memory, visual search...|$|E
40|$|Echolocating bats can {{identify}} three-dimensional objects exclusively through {{the analysis of}} acoustic echoes of their ultrasonic emissions. However, objects of the same structure can differ in size, and the auditory system must achieve a size-invariant, normalized object representation for reliable object recognition. This study describes both the behavioral classification and the cortical neural representation of echoes of complex virtual objects that vary in object size. In a phantom-target playback experiment, it is shown that the bat Phyllostomus discolor spontaneously classifies most scaled versions of objects according to trained standards. This psychophysical performance {{is reflected in the}} electrophysiological responses of a population of cortical units that showed an object-size invariant response (14 / 109 units, 13 %). These units respond preferentially to echoes from objects in which echo duration (<b>encoding</b> <b>object</b> depth) and echo amplitude (<b>encoding</b> <b>object</b> surface area) co-varies in a meaningful manner. These results indicate that {{at the level of the}} bat’s auditory cortex, an object-oriented rather than a stimulus-parameter–oriented representation of echoes is achieved...|$|E
40|$|We study {{unsupervised}} learning in a probabilistic generative model for occlusion. The problem of occlusion is addressed {{from the perspective}} of multiple-causes models such as NMF [1], sparse coding [2], or ICA. The model features- binary hidden variables <b>encoding</b> <b>object</b> presence- a vectorial hidden variable for object proximities- a non-linear combination of objects based on their proximities- two sets of parameters for each object: masks and feature...|$|E
5000|$|... some <b>encoding</b> <b>objects</b> specify {{that the}} ECN type {{is to be}} encoded by {{applying}} a standard set of encoding rules (e.g., PER) to it.|$|R
5000|$|... some <b>encoding</b> <b>objects</b> specify {{that the}} ECN type must be {{replaced}} by a user-defined ECN type that contains the former ECN type as one of its components; ...|$|R
50|$|There may {{be further}} steps {{consisting}} in the recursive application of <b>encoding</b> <b>objects</b> that work by replacing an ECN type (or part of it) with another ECN type.|$|R
40|$|The goal of {{this paper}} is to {{generate}} high-quality 3 D object proposals in the con-text of autonomous driving. Our method exploits stereo imagery to place propos-als in the form of 3 D bounding boxes. We formulate the problem as minimizing an energy function <b>encoding</b> <b>object</b> size priors, ground plane as well as several depth informed features that reason about free space, point cloud densities and distance to the ground. Our experiments show significant performance gains over existing RGB and RGB-D object proposal methods on the challenging KITTI benchmark. Combined with convolutional neural net (CNN) scoring, our approach outper-forms all existing results on all three KITTI object classes. ...|$|E
40|$|This paper {{investigates the}} lexical {{semantics}} of placement verbs in Mandarin. The majority of Mandarin placement verbs are directional verb compounds (e. g., na 2 -xia 4 -lai 2 ‘take-descend-come’). They {{are composed of}} two or three verbs in a fixed order, each encoding certain semantic components of placement events. The first verb usually conveys object manipulation and the second and the third verbs indicate the Path of motion, including Deixis. The first verb, typically <b>encoding</b> <b>object</b> manipulation, can be semantically general or specific: two general verbs, fang 4 ‘put’ and na 2 ‘take’, have large but constrained extensional categories, and a number of specific verbs are used based on the Manner of manipulation of the Figure object, the relationship between and the physical properties of Figure and Ground, intentionality of the Agent, and the type of instrument...|$|E
40|$|SummaryPrimary {{sensory cortex}} discriminates {{incoming}} sensory information and generates multiple processing streams toward other cortical areas. However, the underlying cellular mechanisms remain unknown. Here, by making whole-cell recordings in primary somatosensory barrel cortex (S 1) of behaving mice, {{we show that}} S 1 neurons projecting to primary motor cortex (M 1) and those projecting to secondary somatosensory cortex (S 2) have distinct intrinsic membrane properties and exhibit markedly different membrane potential dynamics during behavior. Passive tactile stimulation evoked faster and larger postsynaptic potentials (PSPs) in M 1 -projecting neurons, rapidly driving phasic action potential firing, well-suited for stimulus detection. Repetitive active touch evoked strongly depressing PSPs and only transient firing in M 1 -projecting neurons. In contrast, PSP summation allowed S 2 -projecting neurons to robustly signal sensory information accumulated during repetitive touch, useful for <b>encoding</b> <b>object</b> features. Thus, target-specific transformation of sensory-evoked synaptic potentials by S 1 projection neurons generates functionally distinct output signals for sensorimotor coordination and sensory perception...|$|E
5000|$|... some <b>encoding</b> <b>objects</b> specify how to {{indicate}} {{which of the}} alternatives of a [...] type is present, and can only be applied to ECN types that are [...] types; ...|$|R
5000|$|... some <b>encoding</b> <b>objects</b> specify {{that the}} ECN type must be {{replaced}} by a user-defined ECN type, and specify how to map each value of the former type to a value of the latter type; ...|$|R
30|$|Object {{reconstruction}} procedure {{described in}} this subsection does not require prior knowledge of object depths, or a manual search for them, in order to recover the <b>encoded</b> <b>objects</b> from their holograms. It automates this task.|$|R
40|$|Abstract Evidence {{suggests}} that infants and adults attribute different importance to certain object properties when performing object-directed actions. Namely, infants tend {{to rely on}} information about an object's location, whereas adults {{are more likely to}} base their actions on its features. In this study, we tested whether the strategic choices of infants (aged 13 months) and adults would be modified by the context of the demonstration. Participants watched as an experimenter hid a ball under one of two different coloured containers, using either a communicative or a non- communicative manner. Then, the locations of the two containers were changed {{out of sight of the}} participant. During the test, participants were encouraged to look for the ball under one of the containers. We found that adults were more likely to follow a feature-based strategy than infants. However, there was no effect of the context of the demonstration, suggesting that communication may play different roles in <b>encoding</b> <b>object</b> properties and directing overt behaviour...|$|E
40|$|Generating {{captions}} for {{images is}} a task that has recently received considerable attention. In this work we focus on caption generation for abstract scenes, or object layouts where the only information provided {{is a set of}} objects and their locations. We propose OBJ 2 TEXT, a sequence-to-sequence model that encodes a set of objects and their locations as an input sequence using an LSTM network, and decodes this representation using an LSTM language model. We show that our model, despite <b>encoding</b> <b>object</b> layouts as a sequence, can represent spatial relationships between objects, and generate descriptions that are globally coherent and semantically relevant. We test our approach in a task of object-layout captioning by using only object annotations as inputs. We additionally show that our model, combined with a state-of-the-art object detector, improves an image captioning model from 0. 863 to 0. 950 (CIDEr score) in the test benchmark of the standard MS-COCO Captioning task. Comment: Accepted at EMNLP 201...|$|E
40|$|Prior {{research}} conceptualised action understanding {{primarily as}} a kinematic matching of observed actions to own motor representations but has ignored the role of object information. The current study utilized fMRI to identify (a) regions uniquely involved in encoding the goal of others' actions, and (b) to test whether these goal understanding processes draw more strongly on regions involved in <b>encoding</b> <b>object</b> semantics or movement kinematics. Participants watched sequences of instrumental actions while attending to either the actions' goal (goal task), the movements performed (movement task) or the objects used (object task). The results confirmed, first, a unique role of the inferior frontal gyrus, middle temporal gyrus and medial frontal gyrus in action goal understanding. Second, they show {{for the first time}} that activation in the goal task overlaps directly with object- but not movement-related activation. Moreover, subsequent parametric analyses revealed that movement-related regions become activated only when goals are unclear, or observers have little action experience. In contrast to motor theories of action understanding, these data suggest that objects-rather than movement kinematics-carry the key information about others' actions. Kinematic information is additionally recruited when goals are ambiguous or unfamiliar...|$|E
40|$|AbstractWe {{define a}} {{powerful}} type inference mechanism with application to object-oriented programming. The types inferred are recursively constrained types, types {{that come with}} a system of constraints. These types {{may be viewed as}} generalizations of recursive types and F-bounded polymorphic types, the forms of type that are necessary to properly <b>encode</b> <b>object</b> typings. The base language we study, I-Soop, incorporates state and records, the two features critical to <b>encode</b> <b>objects</b> in a non-object-oriented language. Soundness and completeness of the type inference algorithm are established by operational means. Our method for establishing these properties is somewhat novel. We illustrate how the algorithm may be fruitfully applied to infer types of object-oriented programs...|$|R
50|$|These <b>encoding</b> <b>objects</b> apply {{mostly to}} simple ECN types, and have several {{parameters}} specifying the bit-level encoding of a value, {{the size of}} the encoding, any preceding or trailing padding, any alignment to an octet or word boundary, any bit reversals, etc.|$|R
40|$|Neuroimaging studies {{attempting}} {{to isolate the}} neural substrate of visual short-term memory in humans have concentrated {{on the behavior of}} neurons populating the posterior part of the parietal cortex as a possible source of visual short-term memory capacity limits. Using a standard change-detection task, fMRI studies have shown that maintenance of bilaterally <b>encoded</b> <b>objects</b> elicited bilateral increases of hemodynamic activation in the intra-parietal and intra-occipital sulci (IPS–IOS) proportional to the number of objects retained in visual short-term memory. We used a spatially cued variant of the change-detection task to record hemodynamic responses to unilaterally <b>encoded</b> <b>objects</b> using functional near-infrared spectroscopy (fNIRS). Electrophysiological studies that employed this task have shown that maintenance of unilaterally <b>encoded</b> <b>objects</b> elicited posterior unilateral (contralateral) increase in event-related negativity proportional to the number of objects retained in visual short-term memory. We therefore examined whether contralateral increases in oxy-hemoglobin concentration correlated with the number of retained objects. Contrary to the idea that bilateral increases in BOLD responses and unilateral increases in event-related negativity may be different reflections of the same underlying neural/functional processing, memory-related increases in oxy-hemoglobin concentration were found bilaterally even when objects had to be encoded unilaterally. The present findings suggest that EEG and fMRI/fNIRS techniques reveal distinct neural signatures of the mechanisms supporting visual short-term memory...|$|R
40|$|Abstract It is a {{remarkable}} fact that images are related to objects constituting them. In this paper, we propose to rep-resent images by using objects appearing in them. We intro-duce the novel concept of object bank (OB), a high-level image representation <b>encoding</b> <b>object</b> appearance and spa-tial location information in images. OB represents an image based on its response to {{a large number of}} pre-trained object detectors, or ‘object filters’, blind to the testing dataset and visual recognition task. Our OB representation demonstrates promising potential in high level image recognition tasks. It significantly outperforms traditional low level image rep-resentations in image classification on various benchmark image datasets by using simple, off-the-shelf classification algorithms such as linear SVM and logistic regression. In this paper, we analyze OB in detail, explaining our design choice of OB for achieving its best potential on different types of datasets. We demonstrate that object bank is a high level representation, from which we can easily discover semantic information of unknown images. We provide guidelines for effectively applying OB to high level image recognition tasks where it could be easily compressed for efficient computation in practice and is very robust to various classifiers. L. -J. Li (B...|$|E
40|$|The {{present study}} {{examined}} the memory performance and cortical connectivity of children with ASD, and investigated whether the memory deficits exhibited by these children {{were associated with the}} cortical connectivity. Twenty-one children with ASD and 21 children with normal development (NC), aged 5 - 14 years, participated in the study. Each child was administered a neuropsychological battery that included the Test of Non-verbal Intelligence (TONI-III), Digit Span test (DS), Rey-Osterrieth Complex Figure Test (Rey-O), and Hong Kong List Learning Test (HKLLT); and an EEG recording session when performing the visual <b>encoding</b> <b>Object</b> Recognition (OR) task. Six neuropsychological measures from the test battery and six EEG coherence measures in the theta band were compared between the children with ASD and normal children. Results indicated that children with ASD performed at comparable levels with normal children in the DS and Rey-O, but were significantly poorer in HKLLT and OR. They also exhibited significantly elevated long-range coherences in the fronto-posterior connections involving the left hemisphere (left anterior-left posterior; left anterior-right posterior). Pearson correlation showed significant negative associations between the anterior-posterior EEG coherences and memory performance. Institute of Textiles and Clothin...|$|E
40|$|Multilinear/tensor {{extensions}} of manifold learning based algorithms {{have been widely}} used in computer vision and pattern recognition. This paper first provides a systematic analysis of the multilinear extensions for the most popular methods by using alignment techniques, thereby obtaining a general tensor alignment framework. From this framework, it is easy to show that the manifold learning based tensor learning methods are intrinsically different from the alignment techniques. Based on the alignment framework, a robust tensor learning method called sparse tensor alignment (STA) is then proposed for unsupervised tensor feature extraction. Different from the existing tensor learning methods, L- 1 - and L- 2 -norms are introduced to enhance the robustness in the alignment step of the STA. The advantage of the proposed technique is that the difficulty in selecting the size of the local neighborhood can be avoided in the manifold learning based tensor feature extraction algorithms. Although STA is an unsupervised learning method, the sparsity encodes the discriminative information in the alignment step and provides the robustness of STA. Extensive experiments on the well-known image databases as well as action and hand gesture databases by <b>encoding</b> <b>object</b> images as tensors demonstrate that the proposed STA algorithm gives the most competitive performance when compared with the tensor-based unsupervised learning methods. Institute of Textiles and Clothin...|$|E
40|$|We {{present an}} {{alternative}} scheme {{to perform a}} multiple encrypting technique based {{on the use of}} a Joint Transform Correlator architecture. The basic approach relies on using an extra random phase mask placed before the correlator input plane, where we select different disjoint regions to <b>encode</b> each input <b>object.</b> In this way we avoid the cross talking when reconstructing the <b>encoded</b> <b>objects.</b> We experimentally validated the procedure using a photorefractive crystal as a storing medium. Facultad de Ciencias Exacta...|$|R
40|$|We propose two {{approaches}} to sense tangible objects on capacitive touch screens, {{which are used}} in off-the-shelf multi-touch devices such as Apple iPad, iPhone, and 3 M’s multi-touch displays. We seek for the approaches {{that do not require}} modifications to the panels: spatial tag and frequency tag. Spatial tag is similar to fiducial tag used by tangible tabletop surface interaction, and uses multi-point, geometric patterns to <b>encode</b> <b>object</b> IDs. Frequency tag simulates high-frequency touches in the time domain to <b>encode</b> <b>object</b> IDs, using modulation circuits embedded inside tangible objects to simulate high-speed touches in varying frequency. We will show several demo applications. The first combines simultaneous tangible + touch input system. This explores how tangible inputs (e. g., pen, easer, etc.) and some simple gestures work together on capacitive touch panels. ACM Classification: H 5. 2 [Information interfaces an...|$|R
40|$|A new {{algorithm}} {{to generate}} all Dyck words is presented, {{which is used}} in ranking and unranking Dyck words. We {{emphasize the importance of}} using Dyck words in <b>encoding</b> <b>objects</b> related to Catalan numbers. As a consequence of formulas used in the ranking algorithm we can obtain a recursive formula for the nth Catalan number...|$|R
