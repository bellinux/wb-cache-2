3|10000|Public
50|$|Plessey also {{pioneered the}} {{gathering}} and consolidation of accounting information {{from around the}} world using in-house software. Each of their 140 management reporting entities used HP125s with DIVAT (<b>data</b> <b>input,</b> <b>validation</b> and transmission) software. Nearly 450 validation rules ensured accuracy within and between various reports.The data were then transmitted to Ilford where a HP3000 used Fortran software for consolidation and reporting - also on HP125s.|$|E
40|$|AbstractTallinn (Estonia) {{water network}} has been {{recently}} updated (calibrated pipe roughness values, demand/leakage calibration) with commercially available tools. In current study two different approaches (scenarios) {{are used for}} model recalibration, using: (a) commercially available tools (genetic algorithm) and (b) custom research tools (Levenberg-Marquardt). Calibration studies are carried out with real network data (medium sized pressure zone: 2000 pipe model). The whole calibration process is also described from engineering perspective, including the knowledge {{that is needed to}} conduct the calibration itself (<b>data</b> <b>input,</b> <b>validation,</b> calibration parameter ranges, etc.). It was found that the calibration results are comparable to some extent...|$|E
40|$|Developing an {{application}} with some tables must concern the validation of input (specially in Table Child). In order {{to maximize the}} accuracy and <b>data</b> <b>input</b> <b>validation.</b> Its called lookup (took data from other dataset). There {{are two ways to}} look up data from Table Parent: 1) Using Objects (DBLookupComboBox and DBookupListBox), or 2) Arranging the properties of data types fields (shown by using DBGrid). In this article is using Borland Delphi software (Inprise product). The method is offered using 5 (five) practise steps: 1) Relational Database Scheme, 2) Form Design, 3) Object DatabasesRelationships Scheme, 4) Properties and Field Type Arrangement, and 5) Procedures. The result of this paper are: 1) The relationship that using lookup objects are valid, and 2) Delphi Lookup Objects can be used for 1 - 1, 1 -N, and M-N relationship. Comment: 16 page...|$|E
40|$|Future space {{transportation}} programme reasonable alternatives are considered with {{regard for the}} total risk forecast. The analysis run tool utilizing the methodology of arriving at a decision under uncertainty conditions and a given goals hierarchy, is bound to provide for <b>input</b> <b>data</b> <b>validation,</b> procedures verification and interactive analysis and selection mode. The statement of the problem actuality is demonstrated by the example of forming future Space Transportation Systems (STS) development programme options...|$|R
40|$|Geothermal {{operators}} use complex reservoir engineering {{models to}} design their well fields and production/injection strategies and {{to predict the}} performance of their reservoirs. Collection of in-situ <b>data</b> for <b>input</b> and <b>validation</b> of these models in wells is expensive, and geophysical measurements from the surface or remotely at some distance from boreholes can be cost effective. The Hydrothermal Research Program of DOE is developing techniques to track injected fluid and to monitor the effects of production and injection geothermal fields using geophysical means...|$|R
40|$|We {{present the}} HU-MOD- 2 {{model for the}} {{assessment}} of management impact on organic matter levels in arable soils. The model aims at optimal applicability as a management support tool in framing practice and therefore requires only easily available <b>input</b> <b>data.</b> In <b>validation,</b> the tool proved to be capable of giving a rough estimate on soil organic matter changes in arable soils. Taking into account the low demand for <b>input</b> <b>data,</b> the modeling error seems tolerable for a practice applicable decision support tool...|$|R
40|$|This paper aims {{to provide}} an {{introduction}} to {{the current state of the}} art of EUROMOD, the European Union tax-benefit microsimulation model. It explains the original motivations for building a multi-country EU-wide model and summarises its current organisation. It provides an overview of EUROMOD components, covering its policy scope, the <b>input</b> <b>data,</b> the <b>validation</b> process and some technical aspects such as the tax-benefit programming language and the user interface. The paper also reviews some recent applications of EUROMOD and, finally, considers future developments...|$|R
40|$|Abstract. In {{this paper}} we {{elaborate}} on the usage of multi-agent-based simula-tion (MABS) for quantitative impact assessment of transport policy and infras-tructure measures. We provide a general discussion {{on how to use}} MABS for freight transport analysis, focusing on issues related to <b>input</b> <b>data</b> management, <b>validation</b> and verification, calibration, output data analysis, and generalization of results. The discussion is built around an agent-based transport chain simulation tool called TAPAS (Transportation And Production Agent-based Simulator) and a simulation study concerning a transport chain around the Southern Baltic Sea...|$|R
40|$|The {{estimation}} of Net Primary Production {{is of great}} impoartance when trying {{to better understand the}} global carbon cycle, especially in the context of international environmental treaty monitoring [...] In this presentation several of the NPP estimation methodologies that are currently in use will be described, as will the plans for selection of the method, or combination of methods, that could be adopted for an operational product. Other issues concerning the development of an operational system, such as spatial and temporal interpolation of <b>input</b> <b>data,</b> process <b>validation,</b> and implementation, will also be addresse...|$|R
40|$|Information Systems Geographic {{information}} systems {{are used to}} collect, analyze, and present information describing the physical and logical properties of the geographic world. Geographically referenced data is the spatial data that pertain to a location on the earth’s surface. There are four major functional units in a typical geographic {{information systems}} (GIS) : • <b>Data</b> <b>Input</b> Unit. Measurements in GIS are taken by sensors such as cameras and global positioning systems. A manual process is then used for <b>inputting</b> <b>data</b> that cannot easily be processed automatically. The measurements are discretized, for example, by imposing a regular, multidimensional discrete grid over the surface to be measured, allowing points of interest to lie only {{at the intersection of}} the grid lines. In addition to the error imposed by this discretization process, measurement errors also reduce the accuracy of attribute values. The <b>data</b> <b>input</b> therefore needs <b>validation.</b> Various integrity constraints including topological constraints also need to be checked. An example of a topological constraint is “Minneapolis should be inside Minnesota. ” • Data Model. A conceptual data model is a type o...|$|R
40|$|The thesis {{presents}} a comparison {{study of the}} two most established simulation approaches in Operational Research, Discrete-Event Simulation (DES) and System Dynamics (SD). The aim of the research implemented is to provide an empirical view of the differences and similarities between DES and SD, in terms of model building and model use. More specifically, the main objectives of this work are: 1. To determine how different the modelling process followed by DES and SD modellers is. 2. To establish the differences and similarities in the modelling approach taken by DES and SD modellers in each stage of simulation modelling. 3. To assess how different DES and SD models of an equivalent problem are from the users’ point of view. In line with the 3 research objectives, two separate studies are implemented: a model building study based on the first and second research objectives and a model use study, dealing with the third research objective. In the former study, Verbal Protocol Analysis is used, where expert DES and SD modellers are asked to ‘think aloud’ while developing simulation models. In the model use study a questionnaire survey with managers (executive MBA students) is implemented, where participants are requested to provide opinions about two equivalent DES and SD models. The model building study suggests that DES and SD modelling are different regarding the model building process and the stages followed. Considering the approach taken to modelling, some similarities are found in DES and SD modellers’ approach to problem structuring, <b>data</b> <b>inputs,</b> <b>validation</b> & verification. Meanwhile, the modellers’ approach to conceptual modelling, model coding, <b>data</b> <b>inputs</b> and model results is considered different. The model use study does not identify many significant differences in the users’ opinions regarding the specific DES and SD models used, implying that from the user’s point of view the type of simulation approach used makes little difference if any. The work described in this thesis is the first of its kind. It provides an understanding of the DES and SD simulation approaches in terms of the differences and similarities involved. The key contribution {{of this study is that}} it provides empirical evidence on the differences and similarities between DES and SD from the model building and model use point of view. Albeit the study does not provide a comprehensive comparison of the two simulation approaches, the findings of the study, provide new insights about the comparison of the two simulation approaches and contribute to the limited existing comparison literature. EThOS - Electronic Theses Online ServiceWarwick Business School (WBS) University of Warwick (UoW) Lannara, ChristinaGBUnited Kingdo...|$|R
40|$|A novel {{characterization}} {{tool for}} identification of full cohesive laws is introduced. Cohesive zones, with associated traction separation laws, are {{most commonly used}} either to describe or to simulate delamination phenomena. Our tool {{is based on the}} Global Digital Image Correlation, in which the kinematic space is defined. After summarizing the principle of the method, we focus on its validation and then on its behavior with noisy <b>input</b> <b>data.</b> This <b>validation</b> is numerically made (i) by building a set of images using an advanced simulation tool, and (ii) by identifying, from theses images, interface behavior characteristics. First successful results already show the effectivity of the proposed method and its robustness...|$|R
40|$|This paper {{presents}} an empirical {{study on the}} comparison of model building in System Dynamics (SD) and Discrete-event Simulation (DES). We study the model building process of 10 expert modellers (5 SD and 5 DES modellers), who talk aloud while building prison simulation models. The transcripts were coded based on 7 modelling topics: problem structuring, conceptual modelling, <b>data</b> <b>inputs,</b> model coding, <b>validation</b> & verification, results & experimentation and implementation. Our results suggest that all modellers switch between modelling topics, however DES modellers follow a more linear progression than SD modellers. Model coding is a central topic for DES modellers, while conceptual modelling followed by model coding interest SD modellers the most. Interestingly, the combined verbalisations on conceptual modelling and model coding account for almost the same percentage of SD and DES protocols. The quantitative analysis of expert modellers’ behaviour {{presented in this paper}} contributes towards the comparison of SD and DES...|$|R
40|$|This paper {{provides}} an empirical {{study on the}} comparison of model building in Discrete-Event Simulation (DES) and System Dynamics (SD). Verbal Protocol Analysis (VPA) is used to study the model building process of ten expert modellers (5 SD and 5 DES). Participants are asked to build a simulation model based on a prison population case study and to think aloud while modelling. The generated verbal protocols are divided into 7 modelling topics: problem structuring, conceptual modelling, <b>data</b> <b>inputs,</b> model coding, <b>validation</b> & verification, results & experimentation and implementation and then analyzed. Our results suggest that all modellers switch between modelling topics, however DES modellers follow a more linear progression compared to SD modellers. DES modellers focus significantly more on model coding and verification & validation, whereas SD modellers on conceptual modelling. This quantitative analysis of the processes followed by expert modellers contributes towards the comparison of DES and SD modelling...|$|R
50|$|For instance, {{database}} normalization is {{the process}} of organizing the fields and tables of a relational database to minimize redundancy and dependency. In the field of software security, a common vulnerability is unchecked malicious input. The mitigation for this problem is proper <b>input</b> <b>validation.</b> Before <b>input</b> <b>validation</b> may be performed, the input must be normalized, i.e., eliminating encoding (for instance HTML encoding) and reducing the <b>input</b> <b>data</b> to a single common character set.|$|R
40|$|<b>Input</b> <b>validation</b> {{has long}} been {{recognized}} as an essen-tial part of a well–designed system, yet existing liter-ature gives {{very little in the}} way of formal axioms for <b>input</b> <b>validation</b> or guidance on how to put in practice what few recommendations exist. We present basic formal axioms for <b>input</b> <b>validation</b> and apply them to sql, where we demonstrate enhanced resistance to injection attacks...|$|R
40|$|We {{present a}} suite of programs, named CING for Common Interface for NMR Structure Generation that {{provides}} for a residue-based, integrated validation of the structural NMR ensemble {{in conjunction with the}} experimental restraints and other <b>input</b> <b>data.</b> External <b>validation</b> programs and new internal validation routines compare the NMR-derived models with empirical data, measured chemical shifts, distance- and dihedral restraints and the results are visualized in a dynamic Web 2. 0 report. A red-orange-green score is used for residues and restraints to direct the user to those critiques that warrant further investigation. Overall green scores below ~ 20  % accompanied by red scores over ~ 50  % are strongly indicative of poorly modelled structures. The publically accessible, secure iCing webserver ([URL]) allows individual users to upload the NMR data and run a CING validation analysis. 12237...|$|R
50|$|Two central {{categories}} of mitigation {{to the problems}} caused by weird machine functionality include <b>input</b> <b>validation</b> within the software and protecting against problems arising from the platform on which the program runs, such as memory errors. <b>Input</b> <b>validation</b> aims to limit the scope and forms of unexpected inputs e.g. through whitelists of allowed inputs, so that the software program itself would not end up in an unexpected state by interpreting the data internally. Equally importantly, secure programming practices such as protecting against buffer overflows make it less likely that <b>input</b> <b>data</b> becomes interpreted in unintended ways by lower layers, such as the hardware on which the program is executed.|$|R
40|$|In {{this paper}} we {{introduce}} the topic <b>input</b> <b>validation,</b> analyze its {{great importance to}} Web applications and suggest a new comprehensive approach to <b>input</b> <b>validation.</b> The approach has been developed {{as a result of}} an evaluation of current <b>input</b> <b>validation</b> approaches that showed that no sufficient solution to common <b>input</b> <b>validation</b> requirements is available at present. The paper describes important requirements for <b>input</b> <b>validation</b> frameworks, especially in the Web context, and introduces main concepts of this approach. The approach is based on the declarative, rule based definition of validation logic and the automatic translation of validation rules into server side and client side code. It supports conditional, composite and complex inter-field validation scenarios. It considers topics such as value normalization, inter-field dependencies and validation actions and integrates these aspects into a consistent validator based system. Our evaluation shows the benefits of this new approach and highlights its advantages compared to other popular and promising approaches such as PowerForms or Topes...|$|R
40|$|Abstract—Client-side {{computation}} in web applications {{is becoming}} increasingly common due to the popularity of powerful client-side programming languages such as JavaScript. Clientside computation is commonly used to improve an application’s responsiveness by validating user inputs before they are sent to the server. In this paper, we present an analysis technique for checking if a client-side <b>input</b> <b>validation</b> function conforms to a given policy. In our approach, <b>input</b> <b>validation</b> policies are expressed using two regular expressions, one specifying the maximum policy (the upper bound for the set of inputs that should be allowed) and the other specifying the minimum policy (the lower bound for the set of inputs that should be allowed). Using our analysis we can identify two types of errors 1) the <b>input</b> <b>validation</b> function accepts an input that is not permitted by the maximum policy, or 2) the <b>input</b> <b>validation</b> function rejects an input that is permitted by the minimum policy. We implemented our analysis using dynamic slicing to automatically extract the <b>input</b> <b>validation</b> functions from web applications and using automata-based string analysis to analyze the extracted functions. Our experiments demonstrate that our approach is effective in finding errors in <b>input</b> <b>validation</b> functions that we collected from real-world applications and from tutorials and books for teaching JavaScript. I...|$|R
5000|$|<b>Input</b> <b>validation,</b> such as {{whitelisting}} {{only known}} good values ...|$|R
40|$|This {{software}} {{provides an}} automated capability {{to measure and}} qualify the frequency stability performance of the Deep Space Network (DSN) ground system, using daily spacecraft tracking data. The results help to verify if the DSN performance is meeting its specification, therefore ensuring commitments to flight missions; in particular, the radio science investigations. The rich set of data also helps the DSN Operations and Maintenance team to identify the trends and patterns, allowing them to identify the antennas of lower performance and implement corrective action in a timely manner. Unlike the traditional approach where the performance can only be obtained from special calibration sessions that are both time-consuming and require manual setup, the new method taps into the daily spacecraft tracking data. This new approach significantly increases the amount of data available for analysis, roughly by two orders of magnitude, {{making it possible to}} conduct trend analysis with good confidence. The software is built with automation in mind for end-to-end processing. From the inputs gathering to computation analysis and later data visualization of the results, all steps are done automatically, making the data production at near zero cost. This allows the limited engineering resource to focus on high-level assessment and to follow up with the exceptions/deviations. To make it possible to process the continual stream of daily incoming data without much effort, and to understand the results quickly, the processing needs to be automated and the data summarized at a high level. Special attention needs to be given to <b>data</b> gathering, <b>input</b> <b>validation,</b> handling anomalous conditions, computation, and presenting the results in a visual form that makes it easy to spot items of exception/ deviation so that further analysis can be directed and corrective actions followed...|$|R
40|$|Abstract This review {{summarizes}} {{and discusses}} meth-odological approaches {{for studies on}} the impact of plant roots on the surrounding rhizosphere and for elucidation of the related mechanisms, covering a range from simple model experiments up to the field scale. A section on rhizosphere sampling describes tools and culture sys-tems employed for analysis of root growth, root morphology, vitality testing and for monitoring of root activity with respect to nutrient uptake, water, ion and carbon flows in the rhizosphere. The second section on rhizosphere probing covers techniques to detect physi-cochemical changes in the rhizosphere as a consequence of root activity. This comprises compartment systems to obtain rhizosphere samples, visualisation techniques, reporter gene approaches and remote sensing technolo-gies for monitoring the conditions in the rhizosphere. Approaches for the experimental manipulation of the rhizosphere by use of molecular and genetic methods as tools to study rhizosphere processes are discussed in a third section. Finally it is concluded that in spite of a wide array of methodological approaches developed in the recent past for studying processes and interactions in the rhizosphere mainly under simplified conditions in model experiments, there is still an obvious lack of methods to test the relevance of these findings under real field conditions or even on the scale of ecosystems. This also limits reliable <b>data</b> <b>input</b> and <b>validation</b> in current rhizosphere modelling approaches. Possible interactions between different environmental factors or plant-microbial interactions (e. g. mycorrhizae) are frequently not considered in model experiments. Moreover, most of the available knowledge arises from investigations with a very limited number of plant species, mainly crops and studies considering also intraspecific geno-typic differences or the variability within wild plant species are just emerging...|$|R
40|$|<b>Input</b> <b>validation</b> in web {{applications}} {{represents an}} important part of their functionality. With proper validation we ensure that provided <b>input</b> <b>data</b> is in accordance with technical constraints, defined by the developer and with business-related constraints. In web development frameworks, validation logic is coupled with program code. If one validation rule is changed, application needs to be recompiled and redeployed. In this thesis we developed a system for <b>input</b> <b>validation</b> based on business rules management system. Validation rules are stored in central repository, separated from implementation of web applications. Thus, we have achieved a simple and transparent way of declaring validation logic in the form of declarative business rules as well as simplifed applications maintenance in case of changes in validation logic...|$|R
2500|$|Security best {{practices}} (<b>Input</b> <b>Validation,</b> SQL Injection, Cross-Site Scripting, etc. [...] ) ...|$|R
40|$|Abstract — <b>Input</b> <b>validation</b> {{refers to}} {{checking}} user inputs {{to a program}} {{to ensure that they}} conform to expectations of the program. <b>Input</b> <b>validation</b> is used to check the format of numbers and strings, check the length of strings, and to ensure that strings do not contain invalid characters. <b>Input</b> <b>validation</b> testing (IVT) is particularly important for software that has a heavy reliance on user inputs, including Web applications. A common technique in Web applications is to perform <b>input</b> <b>validation</b> on the client by using HTML attributes and scripting languages such as JavaScript. An insidious problem with performing <b>input</b> <b>validation</b> on the client is that end users have the ability to bypass this validation. Bypass testing is a unique and novel way to create test cases that is available only because of the unusual mix of client-server, HTML GUI, and JavaScript technologies that are used in Web applications. This workshop paper presents the issues and concerns that allow bypass testing, the preliminary concepts behind the technique, and some early results on applying it. How effective and useful bypass testing can be in testing Web applications will be determined through ongoing research and automation...|$|R
40|$|Since web {{applications}} are easily accessible, and often store {{a large amount}} of sensitive user information, they are a common target for attackers. In particular, attacks that focus on <b>input</b> <b>validation</b> vulnerabilities are extremely effective and dangerous. To address this problem, we developed ViewPoints—a technique that can identify erroneous or insufficient validation and sanitization of the user inputs by automatically discovering inconsistencies between clientand server-side <b>input</b> <b>validation</b> functions. Developers typically perform redundant <b>input</b> <b>validation</b> in both the front-end (client) and the back-end (server) components of a web application. Clientside validation is used to improve the responsiveness of the application, as it allows for responding without communicating with the server, whereas server-side validation is necessary for security reasons, as malicious users can easily circumvent client-side checks. ViewPoints (1) automatically extracts client- and server-side <b>input</b> <b>validation</b> functions, (2) models them as deterministic finite automata (DFAs), and (3) compares client- and server-side DFAs to identify and report the inconsistencies between the two sets of checks. Our initial evaluation of the technique is promising: when applied to a set of real-world web applications, ViewPoints was able to automatically identify a large number of inconsistencies in their <b>input</b> <b>validation</b> functions...|$|R
2500|$|Ensure <b>input</b> <b>validation</b> {{to avoid}} cross-site {{scripting}} flaws or SQL injections flaws ...|$|R
5000|$|<b>Input</b> <b>validation</b> [...] - [...] {{verify that}} each sensor is {{providing}} valid data ...|$|R
40|$|Summary: Web {{applications}} {{have been}} the main intrusion target, and input errors from the web users lead to serious security vulnerabilities. Many web applications contain such errors, making them vulnerable to remotely exploitable <b>input</b> <b>validation</b> attacks such as SQL Injection, Command Injection, Meta-Characters, Formatting String, Path Traversal and Cross Site scripting. In this paper, we present ontology to represent patterns of <b>input</b> <b>validation</b> attacks on web applications. More specifically, our ontology is based on individual subclasses, properties and inverse functional properties, domain and range of <b>input</b> <b>validation</b> attack patterns. The ontology is implemented and interpreted with the web application development language OWL (Ontology Web Language) ...|$|R
5000|$|<b>Input</b> <b>validation,</b> e.g. (in SQL): [...] is {{an example}} of a SQL {{injection}} vulnerability ...|$|R
40|$|Abstract—Atlas-based {{approaches}} {{have demonstrated the}} ability to automatically identify detailed brain structures from 3 -D magnetic resonance (MR) brain images. Unfortunately, the accuracy {{of this type of}} method often degrades when processing data acquired on a different scanner platform or pulse sequence than the data used for the atlas training. In this paper, we improve the performance of an atlas-based whole brain segmentation method by introducing an intensity renormalization procedure that automatically adjusts the prior atlas intensity model to new <b>input</b> <b>data.</b> <b>Validation</b> using manually labeled test datasets has shown that the new procedure improves the segmentation accuracy (as measured by the Dice coefficient) by 10 % or more for several structures including hippocampus, amygdala, caudate, and pallidum. The results verify that this new procedure reduces the sensitivity of the whole brain segmentation method to changes in scanner platforms and improves its accuracy and robustness, which can thus facilitate multicenter or multisite neuroanatomical imaging studies. Index Terms—Brain atlas, brain imaging, computational neuroanatomy, magnetic resonance imaging (MRI) segmentation. I...|$|R
5000|$|Improper <b>input</b> <b>validation</b> [...] - [...] {{a type of}} {{software}} security vulnerability particularly relevant for user-given strings ...|$|R
40|$|Abstract—University {{management}} {{modernization and}} informatization {{plays an important}} role in improving the students quality, score management is one of the important links. In order to improve the accuracy of <b>data</b> <b>input</b> of the B/S structure program, prevent illegal input into the system, this paper study on validation framework based on Struts 2. Firstly, the research of Struts 2 framework <b>validation</b> process and <b>input</b> <b>validation</b> methods; then, carried on the system database design, including the conceptual structure design and logical structure design; Finally, the system used required, stringlength, int, double, date, email. 6 validators are described, and the design of the verification file example. Contents of this paper to facilitate system development, practical application can be used in combination with a regular expression, in order to make the verification method more scientific and effective...|$|R
40|$|Attackers goal: craft <b>input</b> <b>data</b> to {{gain some}} control over certain operationsConsequences of {{improper}} <b>input</b> <b>validation</b> Impersonate (sessions ID stored in cookies) Compromise confidential data – Access to information stored on databases behind web applications Denial of service attacks Data destruction Attackers goal: craft <b>input</b> <b>data</b> {{to gain some}} control over certain operationsScenarios Web applications with sensitive sinks (security critical operations) Security Policy Data received from a client is considerer untrustworthy (or tainted) Untrustworthy data can be made trustworthy (or untainted) by a sanitization process Untrustworthy data (or tainted) can't reach sensitive sinksDifferent kind of attacks “ 42 or 1 = 1 ” “ alert('hello') ” Other taint analysis PH...|$|R
40|$|The paper {{reports a}} field {{investigation}} on a {{reach of the}} lower Zambezi River about 230 – 240 km downstream of the Cahora Bassa dam in the Republic of Mozambique. In {{the framework of a}} wider research program, bathymetric measures of the riverbed were performed on a 10 km stretch of the river using an echo sounder, a GPS receiver, and an integrated navigation-acquisition system. Field observations and measures revealed a general agreement with macro-features of river morphology reported in early literature, dealing with the morphological response of the river to the construction of large dams, {{in the second half of}} the 20 th century. Results hereby reported are some of the few examples of direct field measures in the lower Zambezi reported in literature, and could be used by researchers and practitioners either as a knowledge base for further surveys or as <b>input</b> <b>data</b> for <b>validation</b> and calibration of mathematical models and remote sensing observations...|$|R
