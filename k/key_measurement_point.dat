3|6074|Public
40|$|Conversion, {{the process}} by which natural uranium ore (yellowcake) is puriﬁed and {{converted}} through a series of chemical processes into uranium hexaﬂuoride gas (UF 6), has historically been excluded from the nuclear safeguards requirements of the 235 U-based nuclear fuel cycle. With each step in the conversion process from yellowcake to feedstock for UF 6, intermediary uranium oxide and uranium ﬂuoride compounds become progressively attractive products for diversion toward activities noncompliant with international treaties. The diversion of this product material could potentially provide feedstock for a clandestine or undeclared enrichment for weapons development for state or non-state entities. With the realization of this potential, the International Atomic Energy Agency (IAEA) has only recently reinterpreted its policies to emphasize safeguarding this feedstock in response to such diversion pathways. This project employs a combination of simulation models and experimental measurements to develop and validate concepts of nondestructive assay monitoring systems in a natural uranium conversion plant (NUCP). In particular, uranyl nitrate (UN) solution exiting solvent extraction was identiﬁed as a <b>key</b> <b>measurement</b> <b>point</b> (KMP), where gamma-ray spectroscopy was selected as the process-monitoring tool. The Uranyl Nitrate Calibration Loop Equipment (UNCLE) facility at Oak Ridge National Laboratory was employed to simulate the full-scale operating conditions of a puriﬁed uranium-bearing aqueous stream exiting the solvent extraction process in an NUCP. This work investigates gamma-ray signatures UN circulating in the UNCLE facility and evaluates various gamma-ray detector (HPGe, LaBr 3 and NaI) sensitivities to UN. Ph. D...|$|E
40|$|Pacific Northwest National Laboratory (PNNL) {{intends to}} {{automate}} the UF 6 cylinder nondestructive assay (NDA) verification currently {{performed by the}} International Atomic Energy Agency (IAEA) at enrichment plants. PNNL is proposing the installation of a portal monitor at a <b>key</b> <b>measurement</b> <b>point</b> to positively identify each cylinder, measure its mass and enrichment, store the data along with operator inputs in a secure database, and maintain continuity of knowledge on measured cylinders until inspector arrival. This report summarizes {{the status of the}} research and development of an enrichment assay methodology supporting the cylinder verification concept. The enrichment assay approach exploits a hybrid of two passively-detected ionizing-radiation signatures: the traditional enrichment meter signature (186 -keV photon peak area) and a non-traditional signature, manifested in the high-energy (3 to 8 MeV) gamma-ray continuum, generated by neutron emission from UF 6. PNNL has designed, fabricated, and field-tested several prototype assay sensor packages in an effort to demonstrate proof-of-principle for the hybrid assay approach, quantify the expected assay precision for various categories of cylinder contents, and assess the potential for unsupervised deployment of the technology in a portal-monitor form factor. We refer to recent sensor-package prototypes as the Hybrid Enrichment Verification Array (HEVA). The report provides an overview of the assay signatures and summarizes the results of several HEVA field measurement campaigns on populations of Type 30 B UF 6 cylinders containing low-enriched uranium (LEU), natural uranium (NU), and depleted uranium (DU). Approaches to performance optimization of the assay technique via radiation transport modeling are briefly described, as are spectroscopic and data-analysis algorithms...|$|E
40|$|This study {{investigates the}} effect of the {{non-uniform}} nuclide composition in spent fuel on material accountancy in the pyroprocess. High-fidelity depletion simulations are performed using the Monte Carlo code SERPENT in order to determine nuclide composition as a function of axial and radial position within fuel rods and assemblies, and burnup. For improved accuracy, the simulations use short burnups step (25 days or less), Xe-equilibrium treatment (to avoid oscillations over burnup steps), axial moderator temperature distribution, and 30 axial meshes. Analytical solutions of the simplified depletion equations are built to understand the axial non-uniformity of nuclide composition in spent fuel. The cosine shape of axial neutron flux distribution dominates the axial non-uniformity of the nuclide composition. Combined cross sections and time also generate axial non-uniformity, as the exponential term in the analytical solution consists of the neutron flux, cross section and time. The axial concentration distribution for a nuclide having the small cross section gets steeper than that for another nuclide having the great cross section because the axial flux is weighted by the cross section in the exponential term in the analytical solution. Similarly, the non-uniformity becomes flatter as increasing burnup, because the time term in the exponential increases. Based on the developed numerical recipes and decoupling of the results between the axial distributions and the predetermined representative radial distributions by matching the axial height, the axial and radial composition distributions for representative spent nuclear fuel assemblies, the Type- 0, - 1, and - 2 assemblies after 1, 2, and 3 depletion cycles, is obtained. These data are appropriately modified to depict processing for materials in the head-end process of pyroprocess that is chopping, voloxidation and granulation. The expectation and standard deviation of the Pu-to- 244 Cm-ratio by the single granule sampling calculated by the central limit theorem and the Geary-Hinkley transformation. Then, the uncertainty propagation through the key-pyroprocess is conducted to analyze the Material Unaccounted For (MUF), which is a random variable defined as a receipt minus a shipment of a process, in the system. Finally, the Type-I error for the Pu MUF is evaluated for spent fuel assemblies with respect the size of granules and powders which are the key parameters to evaluate the uncertainty in determining ratio by sampling. The main observations of this calculation are the following: (1) the probabilities of Type-I error for the Pu MUF are occasionally greater than 5 % when the size of powder particles and granules are large and small; (2) the possibility of Type-I error greater than 5 % increases by increasing the depletion period, because the increasing production of Pu and 244 Cm with increasing burnup propagates the uncertainty through the system; (3) the major contributor to the uncertainty of ratio is the non-uniformity of 244 Cm; (4) Pu generation in the Type- 2 spent fuel assemblies is greater that in the others due to higher gadolinia bearing fuel (GBF) loading in the assembly, therefore, the area indicating the probability of Type-I error {{greater than or equal to}} 5 % is the widest among the figures for the Type- 0 and - 1 spent fuel assemblies. The random variable, LOPu, is defined for evaluating the non-detection probability at each <b>Key</b> <b>Measurement</b> <b>Point</b> (KMP) as the original Pu mass minus the Pu mass after a missing scenario. A number of assemblies for the LOPu to be 8 kg is considered in this calculation. The probability of detection for the 8 kg LOPu is evaluated with respect the size of granule and powder using the event tree analysis and the hypothesis testing method. We can observe there are possible cases showing the probability of detection for the 8 kg LOPu less than 95 %. In order to enhance the detection rate, a new Material Balance Area (MBA) model is defined for the key-pyroprocess. The probabilities of detection for all spent fuel types based on the new MBA model are greater than 99 %. Furthermore, it is observed that the probability of detection significantly increases by increasing granule sample sizes to evaluate the Pu-to- 244 Cm-ratio before the key-pyroprocess. Based on these observations, even though the Pu material accountability in pyroprocess is affected by the non-uniformity of nuclide composition when the Pu-to- 244 Cm-ratio method is being applied, that is surmounted by decreasing the uncertainty of measured ratio by increasing sample sizes and modifying the MBAs and KMPs...|$|E
40|$|The {{relative}} positions {{between the}} four slide blocks {{vary with the}} movement of the table due to the geometric errors of the guide rail. Consequently, the additional load on the slide blocks is increased. A new method of error measurement and identification by using a self-designed stress test plate was presented. BP neural network model was used to establish the mapping between the stress of <b>key</b> <b>measurement</b> <b>points</b> on the test plate and the displacements of slide blocks. By measuring the stress, the relative displacements of slide blocks were obtained, from which the geometric errors of the guide rails were converted. Firstly, the finite element model was built to find the <b>key</b> <b>measurement</b> <b>points</b> of the test plate. Then the BP neural network was trained by using the samples extracted from the finite element model. The stress at the <b>key</b> <b>measurement</b> <b>points</b> were taken as the input and the relative displacements of the slide blocks were taken as the output. Finally, the geometric errors of the two guide rails were obtained according to the measured stress. The results show that the maximum difference between the measured geometric errors and the output of BP neural network was 5 μm. Therefore, the correctness and feasibility of the method were verified...|$|R
40|$|Abstract. In the {{technology}} of thermal error compensation in positioning platform with large trip and high precision, selecting the temperature <b>measurement</b> <b>points</b> rationally is particular important for successfully establishing the model of compensation. The method uses simulation to track platform heat distribution and thermal deformation under various thermal conditions. Temperature variables are grouped by different surfaces of the platform. Then a degree of grey incidence from grey system theory is introduced to identify the <b>key</b> temperature <b>measurement</b> <b>points</b> of each surface. Through the experiment data of thermal stress coupling analysis on the platform, the degree of correlation between all temperature <b>measurement</b> <b>points</b> and thermal displacement can be solved. The <b>key</b> temperature <b>measurement</b> <b>points</b> are confirmed by the largest value {{of the degree of}} correlation of each surface...|$|R
40|$|Abstract: The {{paper is}} proposing a study and {{practical}} experiments of testing methods in digital television, transmitted in Digital Video Broadcasting standard. The main {{objective of the}} research {{presented in this paper}} was to establish if the testing tools designed and validated for DVB-S, are useable to evaluate DVB-S 2. This is important in educational environments and small enterprises, to find new applications for the existing equipment. Testing methods follows three directions: transmitted services (data link layer), RF (Radio Frequency) interface and reception (physical layer). We pursued the following objectives: DVB-S 2 model study (from Simulink Matlab), to establish the <b>key</b> <b>measurement</b> <b>points</b> in this evaluation, analysis of DVB-S/S 2 streams, RF parameters measurements in terms of C/N (Carrier to Noise Ratio) and spectrum...|$|R
40|$|Because {{of their}} impacts on {{long-term}} storage of {{high-level radioactive waste}} and their value as nuclear fuels, measurement and accounting of the minor actinides produced in nuclear power reactors are becoming significant issues. This paper briefly reviews the commercial nuclear fuel cycle with emphasis on reprocessing plants and <b>key</b> <b>measurement</b> <b>points</b> therein. Neutron signatures and characteristics are compared and contrasted for special nuclear materials (SNMs) and minor actinides (MAs). The paper focuses on application of neutron-based nondestructive analysis (NDA) methods that can be extended for verification of MAs. We describe current IAEA methods for NDA of SNMs and extension of these methods to satisfy accounting requirements for MAs in reprocessing plant dissolver solutions, separated products, and high-level waste. Recommendations for further systems studies and development of measurement methods are also included...|$|R
40|$|Because {{of their}} value as nuclear fuels {{and their impact}} on {{long-term}} storage of high-level radioactive waste, measurement and accounting for minor actinides (MAs) produced in nuclear power reactors are becoming significant issues. This report attempts to put the issues in perspective by reviewing the commercial nuclear fuel cycle with emphasis on reprocessing plants and <b>key</b> <b>measurement</b> <b>points</b> therein. Radiation signatures and characteristics are compared and contrasted for special nuclear materials (SNMs) and MAs. Also, inventories and relative amounts of SNMs and MAs are generally described for irradiated nuclear fuel and reprocessing plants. The bulk of the report describes appropriate measurement technologies, capabilities, and development needs to satisfy material accounting requirements for MAs, with emphasis on adaptation of current technologies. Recommendations for future systems studies and development of measurement methods are also included. 38 refs., 3 figs., 12 tabs...|$|R
40|$|This paper {{presents}} a methodology to diagnose sources of dimensional variation for compliant parts from the measurement data of final assemblies. The method is developed for a single-station assembly process. The proposed diagnosis tool {{is based on}} applying a predictive variation propagation model to determine the part-to-part and tooling interaction in the assembly system. The variation propagation model allows identifying the impact of different faulty component patterns in the final assembly product. Using the predictive assembly fault patterns and the designated component analysis, the contribution of each fault in the total system variation may be identified. The methodology incorporates an optimal sensor placement algorithm to determine the <b>key</b> <b>measurement</b> <b>points</b> in the assembly. Two case studies were conducted to illustrate that the methodology is capable of identifying part variation patterns from assembly measurement data, even under significant levels of noise. Although the methodology is presented for a single assembly station, it can be extended to a multiple-station assembly scenario using a multistation variation propagation model...|$|R
40|$|Nuclear fuel cycles {{safeguards}} {{should be}} considered in the dynamic context of a world deployment of various reactor types and varying availability of fuel-cycle services. There will be a close interaction between thermal-reactor cycles and the future deployment of fast breeders. The quantities of plutonium and the reprocessing, conversion, fabrication, and storage methods of the fuel for the fast breeders will {{have a significant impact on}} safeguards techniques. The approach to the fast breeder fuel cycle safeguards follows the general safeguards system approach proposed by the IAEA. Objective of IAEA safeguards is the detection of diversion of nuclear material and deterrence of such diversion. To achieve independent verification of material balance accountancy requires the capability to monitor inventory status and verify material flows and quantities of all nuclear materials subject to safeguards. Containment and surveillance measures are applied to monitor <b>key</b> <b>measurement</b> <b>points,</b> maintain integrity of material balance, and complement material accountancy. The safeguards study attempts to develop a generic reference IAEA Safeguards System and explores various system options using containment/surveillance and material accountancy instrumentation and integrated systems designs...|$|R
40|$|International Atomic Energy Agency (IAEA) {{inspectors}} currently perform {{periodic inspections}} at uranium enrichment plants to verify UF 6 cylinder enrichment declarations. Measurements are typically performed with handheld high-resolution sensors on {{a sampling of}} cylinders taken to {{be representative of the}} facility's entire product-cylinder inventory. Pacific Northwest National Laboratory (PNNL) is developing a concept to automate the verification of enrichment plant cylinders to enable 100 percent product-cylinder verification and potentially, mass-balance calculations on the facility as a whole (by also measuring feed and tails cylinders). The Integrated Cylinder Verification System (ICVS) could be located at <b>key</b> <b>measurement</b> <b>points</b> to positively identify each cylinder, measure its mass and enrichment, store the collected data in a secure database, and maintain continuity of knowledge on measured cylinders until IAEA inspector arrival. The three main objectives of this FY 09 project are summarized here and described in more detail in the report: (1) Develop a preliminary design for a prototype NDA system, (2) Refine PNNL's MCNP models of the NDA system, and (3) Procure and test key pulse-processing components. Progress against these tasks to date, and next steps, are discussed...|$|R
40|$|Research Overview: This {{summarizes}} {{the current and}} future work done in streamlining the processes and methods involved with study design and statistical analyses {{in order to ensure}} quality of statistical methods and reproducibility of research. Objectives/Goals: Key factors causing irreproducibility of research include those related to inappropriate study design methodologies and statistical analysis. In modern statistical practice irreproducibility could arise due to statistical (false discoveries, p-hacking, overuse/misuse of p-values, low power, poor experimental design) and computational (data, code & software management) issues. These require understanding the processes and workflows practiced by an organization, and the development and use of metrics to quantify reproducibility. Methods/Study Population: Within the Foundation of Discovery - Population Health Research, Center for Clinical and Translational Science, University of Utah, we are undertaking a project to streamline the study design and statistical analysis workflows and processes. As a first step we met with key stakeholders to understand the current practices by eliciting example statistical projects, and then developed process information models for different types of statistical needs using Lucidchart. We then reviewed these with the Foundation’s leadership and the Standards Committee to come up with ideal workflows and model, and defined <b>key</b> <b>measurement</b> <b>points</b> (such as those around study design, analysis plan, final report, requirements for quality checks, and double coding) for assessing reproducibility. As next steps we are using our finding to embed analytical and infrastructural approaches within the statisticians’ workflows. This will include data and code dissemination platforms such as Box, Bitbucket and GitHub, documentation platforms such as Confluence, and workflow tracking platforms such as Jira. These tools will simplify and automate the capture of communications as a statistician work through a project. Data-intensive process will use process-workflow management platforms such as Activiti, Pegasus and Taverna. Results/Anticipated Results: These strategies for sharing and publishing study protocols, data, code and results across the spectrum, active collaboration with the research team, automation of key steps, along with decision support. Discussion/Significance of Impact: This analysis of the statistical methods and process and computational methods to automate them ensure quality of statistical methods and reproducibility of research...|$|R
40|$|Key factors causing irreproducibility of {{research}} include {{those related to}} inappropriate study design methodologies and statistical analysis 1. In modern statistical practice irreproducibility could arise due to statistical (false discoveries, p-hacking, overuse/misuse of p-values, low power, poor experimental design) and computational (data, code & software management) issues 2. These require understanding the processes and workflows practiced by an organization, and the {{development and use of}} metrics to quantify reproducibility. Within the Foundation of Discovery - Population Health Research, Center for Clinical and Translational Science, University of Utah, we are undertaking a project to streamline the study design and statistical analysis workflows and processes. As a first step we met with key stakeholders to understand the current practices by eliciting example statistical projects, and then developed process information models for different types of statistical needs using Lucidchart. We then reviewed these with the Foundation’s leadership and Standards Committee to come up with ideal workflows and model, and defined <b>key</b> <b>measurement</b> <b>points</b> (such as those around study design, analysis plan, final report, requirements for quality checks, and double coding) for assessing reproducibility. As next steps we will use our finding to embed analytical and infrastructural approaches within the statisticians’ workflows. This will include data and code dissemination platforms such as Box, Bitbucket and GitHub, documentation platforms such as Confluence, and workflow tracking platforms such as Jira. These tools will simplify and automate the capture of communications as a statistician work through a project. Data-intensive process will use process-workflow management platforms such as Activiti 3, Pegasus 4 and Taverna 6. These strategies for sharing and publishing study protocols, data, code and results across the spectrum 7, active collaboration with the research team, automation of key steps, along with decision support will ensure quality of statistical methods and reproducibility {{of research}}. References 1. Reproducibility and reliability of biomedical research’, organised by the Academy of Medical Sciences, BBSRC, MRC and Wellcome Trust in April 2015. 2. National Academies of Sciences, Engineering, and Medicine. Statistical Challenges in Assessing and Fostering the Reproducibility of Scientific Results: Summary of a Workshop. Washington, DC: The National Academies Press, 2016. doi: 10. 17226 / 21915. 3. Acivit, [URL] 4. Pegasus, [URL] 5. Apache Taverna. [URL] 6. Peng, R. 2011. Reproducible research in computational science. Science 334 (6060) : 1226 - 122...|$|R
40|$|CREAM is {{an acronym}} for coal recovery, evaluation, {{analysis}} and management and is a business improvement project at BMA’s Goonyella Riverside mine that is focussed on maximising coal recovered. In August 2003, Goonyella Riverside was set a challenge to quantify coal loss and dilution {{and their relationship to}} cost, revenue and ultimately profit. While the mine was confident in its site forecasts of recovery (and loss), {{it was clear that the}} traditional measurement and analysis systems were inadequate to firstly, identify key loss areas and mechanisms and secondly, successfully quantify the benefit of various coal recovery initiatives. The path that BMA Goonyella Riverside has travelled to maximise coal recovered can be divided into four key steps: 1. Mapping the process – This step involved the development of a coal flow process map for Goonyella Riverside that stepped through each component of the process from the coal reserve to the customer and identified <b>key</b> <b>measurement</b> <b>points</b> for the purposes of reconciliation. 2. Understanding coal loss and identifying opportunities – This step involved conducting two detailed Reserve to Customer projects at the mine site to track a block of coal from the reserve to the customer and to gain a greater understanding of the relationship of dilution, coal loss mechanisms, revenue and costs associated with recovering additional coal. 3. Coal data systems development – Two important coal data systems have been developed and implemented at Goonyella Riverside in the past twelve months, namely Snowden’s Coal Reconcilor and SeamFix. These systems enable coal losses and dilution to be quantified and reconciled back to the initial coal reserve. 4. Implementation of loss reduction initiatives – This step involved the formation of a business improvement project referred to as ‘CREAM’ in August 2004, for the purposes of improving pit coal recovery and maximising business value. To date a series of coal recovery trials have been run in a number of pits at Goonyella Riverside that have yielded significant improvements in business value. The key to the success of this project has been the discipline to follow a defined process map, management commitment through resourcing and shared key performance indicators, a successful acceptance strategy resulting in holistic ownership and the implementation of sustainable reconciliation systems. Coal loss and dilution are now quantifiable and their relationship to cost, revenue and ultimately profit is well understood. This paper discusses the process that Goonyella Riverside has adopted to maximise coal recovered...|$|R
40|$|Analysis of {{the results}} of a data {{reconciliation}} program is made easier by extracting more information from the Jacobian matrix of the constraint equations. Standard deviation for all state variables (measured or not measured) is related to the standard deviation of measurements. Distinction between variables that are actually corrected by the validation process, and those that are merely derived from a single measurement is straightforward. Based on this information, decisions can be taken : deletion of unnecessary measurements, addition of new <b>measurement</b> <b>points</b> and their optimal selection, or identification of <b>key</b> <b>measurements</b> for which any enhancement of accuracy would result in significant improvement in the quality of the process validation. Peer reviewe...|$|R
40|$|Pacific Northwest National Laboratory (PNNL) is {{developing}} {{the concept of}} an automated UF 6 cylinder verification station that would be located at <b>key</b> <b>measurement</b> <b>points</b> to positively identify each cylinder, measure its mass and enrichment, store the collected data in a secure database, and maintain continuity of knowledge on measured cylinders until the arrival of International Atomic Energy Agency (IAEA) inspectors. At the center of this unattended system is a hybrid enrichment assay technique that combines the traditional enrichment-meter method (based on the 186 keV peak from 235 U) with non-traditional neutron-induced high-energy gamma-ray signatures (spawned primarily by 234 U alpha emissions and 19 F(alpha, neutron) reactions). Previous work by PNNL provided proof-of-principle for the non-traditional signatures to support accurate, full-volume interrogation of the cylinder enrichment, thereby reducing the systematic uncertainties in enrichment assay due to UF 6 heterogeneity and providing greater sensitivity to material substitution scenarios. The work described here builds on that preliminary evaluation of the non-traditional signatures, but focuses on a prototype field system utilizing NaI(Tl) and LaBr 3 (Ce) spectrometers, and enrichment analysis algorithms that integrate the traditional and non-traditional signatures. Results for the assay of Type- 30 B cylinders ranging from 0. 2 to 4. 95 wt% 235 U, at an AREVA fuel fabrication plant in Richland, WA, are described for the following enrichment analysis methods: 1) traditional enrichment meter signature (186 keV peak) as calculated using a square-wave convolute (SWC) algorithm; 2) non-traditional high-energy gamma-ray signature that provides neutron detection without neutron detectors and 3) hybrid algorithm that merges the traditional and non-traditional signatures. Uncertainties for each method, relative to the declared enrichment for each cylinder, are calculated and compared to the uncertainties from an attended HPGe verification station at AREVA, and the IAEA’s uncertainty target values for feed, tail and product cylinders. A summary of the major findings from the field measurements and subsequent analysis follows: • Traditional enrichment-meter assay using specially collimated NaI spectrometers and a Square-Wave-Convolute algorithm can achieve uncertainties comparable to HPGe and LaBr for product, natural and depleted cylinders. • Non-traditional signatures measured using NaI spectrometers enable interrogation of the entire cylinder volume and accurate measurement of absolute 235 U mass in product, natural and depleted cylinders. • A hybrid enrichment assay method can achieve lower uncertainties than either the traditional or non-traditional methods acting independently because there is a low degree of correlation in the systematic errors of the two individual methods (wall thickness variation and 234 U/ 235 U variation, respectively). This work has indicated that the hybrid NDA method has the potential to serve as the foundation for an unattended cylinder verification station. When compared to today’s handheld cylinder-verification approach, such a station would have the following advantages: 1) improved enrichment assay accuracy for product, tail and feed cylinders; 2) full-volume assay of absolute 235 U mass; 3) assay of minor isotopes (234 U and 232 U) important to verification of feedstock origin; single instrumentation design for both Type 30 B and Type 48 cylinders; and 4) substantial reduction in the inspector manpower associated with cylinder verification...|$|R
3000|$|... is {{the number}} of <b>measurement</b> <b>points),</b> each used <b>measurement</b> <b>point</b> {{carrying}} an individual angular parameter to be estimated simultaneously, together with the five ellipse parameters.|$|R
30|$|Notably our {{suggestions}} in {{the chapter}} <b>Key</b> <b>measurements</b> indicate those traits.|$|R
40|$|By {{setting a}} {{refractor}} {{at an angle}} to the optical axis of a closed circuit display (CCD) camera lens, the image of a <b>measurement</b> <b>point</b> recorded on the image plane is displaced by an amount that corresponds to the distance between the camera and the <b>measurement</b> <b>point.</b> When the refractor is rotated at high speed while the camera is operating, the image of the <b>measurement</b> <b>point</b> appears as an annular streak. Since the radius of the annular streak is inversely proportional to the distance between the camera and <b>measurement</b> <b>point,</b> the three-dimensional (3 -D) position of the <b>measurement</b> <b>point</b> can be obtained by extracting data from the streak. In this paper, this system is applied to measure a moving surface. To do this, multiple laser spots are projected onto the surface. The position of each laser spot reflected from the surface is measured using a circular dynamic stereo system. Moreover, this system is applied to flow measurement as a potential application and the results are described. ...|$|R
40|$|The {{problem for}} <b>measurement</b> <b>point</b> {{selection}} in damage detection procedures is addressed. The concept of average mutual information is applied {{in order to}} find the optimal distance between <b>measurement</b> <b>points.</b> The idea is to select the <b>measurement</b> <b>points</b> {{in such a way that}} the taken measurements are independent, i. e. the measurements do not 'learn' from each other. The average mutual information can be utilized as a kind of an autocorrelation function for the purpose. It gives the average amount of information that two points 'learn' from each other. Thus the minimum of the average mutual information will provide the distance between <b>measurement</b> <b>points</b> with independent <b>measurements.</b> The idea to use the first minimum of the average mutual information is taken from nonlinear dynamics. The proposed approach is demonstrated on a test case. The results show that it is possible to decrease significantly the number of <b>measurement</b> <b>points,</b> without decreasing the precision of the solution...|$|R
30|$|The {{proportion}} of identical answers at two <b>measurement</b> <b>points.</b>|$|R
40|$|AbstractÐThis {{longitudinal}} study evaluated {{the effects of}} two types of coping strategies, approach and avoidance, on anxiety, depression, and well-being in patients with coronary heart disease. Measurements were made at three timepoints: 1 month, 3 months, and 12 months after the cardiac event. Both cross-sectional and longitudinal relations were explored. At all three <b>measurement</b> <b>points</b> signi®cant negative cross-sectional relations were found between approach and well-being, and signi®cant positive cross-sec-tional relations were found between approach, on the one hand, and anxiety and depression, on the other. At the ®rst <b>measurement</b> <b>point,</b> avoidance showed a positive association with well-being, and a negative association with anxiety. Longitudinal analyses, however, revealed a negative relationship be-tween approach at the ®rst <b>measurement</b> <b>points</b> and anxiety and depression at later <b>measurement</b> <b>points.</b> Likewise, there was a positive association between approach at the ®rst two <b>measurement</b> <b>points</b> and well-being at later <b>measurement</b> <b>points.</b> The {{results of this study}} demonstrate the importance of facing and working through the trauma of the coronary event. Although unfavorable in the short term, working through the trauma can attenuate long-term emotional distress. These results suggest that assessment of the psychological consequences of coronary heart disease and development of interventions should not be based only on cross-sectional data, but should take into account longitudinal relations between coping and psychosocial outcome measures. 1999 Elsevier Science Inc...|$|R
40|$|Abstract- Analysis of {{the results}} of a data {{reconciliation}} program is made easier by extracting more information from the Jacobian matrix of the constraint equations. Standard deviation for all state variables (measured or not measured) is related to the standard deviation of measurements. Distinction between variables that are actually corrected by the validation process, and those that are merely derived from a single measurement is straightforward. Based on this information, decisions can be taken: deletion of unnecessary measurements, addition of new <b>measurement</b> <b>points</b> and their optimal selection, or identification of <b>key</b> <b>measurements</b> for which any enhancement of accuracy would result in significant improvement in the quality of the process validation. SCOPE Plant data reconciliation has been developed for a long time (Kalitventzeff (1978), Gosset (1980) and has been transposed with success from academic to industrial applications. It {{has proven to be a}} valuable tool in processing raw measurements collected in operating plant, and is now being linked to real time data logging systems (Kalitventzeff (1991). However results of validation programs can be difficult to interpret. For instance, when a measured variable is no...|$|R
40|$|Coordinate {{metrology}} generates important software issues. After measurement {{data are}} {{collected in the}} form of position vectors, the data analysis software must derive the necessary information from the set of points. Uncertainty is an important issue in this data analysis. When extreme fit approaches are employed for form error evaluation, the uncertainty is closely related to the sampling of <b>measurement</b> <b>points.</b> Those <b>measurement</b> <b>points</b> are a subset of the true surface and, consequently, the extreme fit result differs from the true value. In this paper, we investigate the functional relationship between such an extreme fit uncertainty and the sampling of <b>measurement</b> <b>points.</b> The important questions to be answered are: what parameters affect the functional relationship, and how can this scheme be applied to sampling of <b>measurement</b> <b>points.</b> An analytic and experimental approach are presented for a flatness example...|$|R
3000|$|Digits 1 – 8 report {{problems}} {{that may have}} occurred at {{one or more of}} the <b>measurement</b> <b>points</b> used for the computation of a current density value. Values 0 -N mark the number of points that were affected by that problem: N = 1 … 4 for normal FAC processing (4 <b>measurement</b> <b>points</b> involved), [...]...|$|R
40|$|Abstract—With the pervasion {{of mobile}} devices, crowdsourc-ing based {{received}} signal strength (RSS) fingerprint collection method has drawn {{much attention to}} facilitate the indoor localiza-tion since it is effective and requires no pre-deployment. However, in large open indoor environment like museums and exhibition centres, RSS <b>measurement</b> <b>points</b> cannot be collocated densely, which degrades localization accuracy. This paper focuses on <b>measurement</b> <b>point</b> collocation in different cases and their effects on localization accuracy. We first study two simple preliminary cases under assumption that users are uniformly distributed: when <b>measurement</b> <b>points</b> are collocated regularly, we propose a collocation pattern which is most beneficial to localization accuracy; when <b>measurement</b> <b>points</b> are collocated randomly, we prove that localization accuracy is limited by a tight bound. Under the general case that users are distributed asymmetrically, we show the best allocation scheme of <b>measurement</b> points: <b>measurement</b> <b>point</b> density is proportional to (c) 2 = 3 {{in every part of}} the region, where is user density and c is a constant determined by the collocation pattern. We also give some guidelines on collocation choice and perform extensive simulations to validate our assumptions and results. I...|$|R
30|$|N = 1 … 2 for single-satellite {{processing}} (2 <b>measurement</b> <b>points</b> involved).|$|R
40|$|AbstractThis paper {{explores the}} mass media {{perception}} of the European Union (EU) in Kazakhstan by utilizing the content analysis of the major mass media outlets. The authors examine news reports and periodical articles from four major national Kazakh newspapers using three <b>measurement</b> <b>points.</b> The first <b>measurement</b> <b>point</b> covers the early 1990 s when Kazakhstan declared independence and began to establish its foreign relations. The second <b>measurement</b> <b>point</b> covers the periods before and after introduction of the EU Strategy for Central Asia (2006 – 2008). The third <b>measurement</b> <b>point</b> covers the years (2011 – 2013) associated with implementation with the EU Strategy and assessing its results. Our main findings suggest that Kazakhstan's mass media positively perceives {{the role of the}} EU in the region. Moreover, they tend to portray the EU mainly as an economic powerhouse. Our findings support some suggestions by similar studies of the EU's external perception...|$|R
25|$|Accurate <b>measurement</b> <b>points</b> with {{absolute}} confirmation to provide {{to the second}} travel times.|$|R
30|$|First, a {{test has}} been carried out {{considering}} a possible realistic measurement configuration for the network. In particular, three <b>measurement</b> <b>points</b> have been supposed to be available in nodes 150 (primary station), 18 and 67. Each <b>measurement</b> <b>point</b> is composed of a voltage synchrophasor measurement on the node and of current synchrophasor measurements on all the branches converging to that node.|$|R
40|$|Abstract-The {{conventional}} {{approaches to}} distribution network fault location has certain drawbacks {{in terms of}} complexity, especially the fault occurred in the branch lines. In this paper, a novel fault location method based on D-type traveling wave was proposed. This method choose the <b>measurement</b> <b>point</b> which the initial traveling waves first reached as the reference <b>measurement</b> <b>point,</b> and the distance between multiple failure points received a fault traveling wave signal to the reference <b>measurement</b> <b>point</b> was calculated. Then, the maximum distance between multiple failure points to the reference <b>measurement</b> <b>point</b> was selected as location of the final failure point. The simulation result with ATP simulation software and MATLAB software show that the proposed method can achieve fault location fast and accurately within a very short period. Compared with the existing algorithms, this method was consistent whether the fault occurred in the main line or branch lines, which improved the accuracy and reliability of the traveling wave fault location in distribution network...|$|R
40|$|The aim {{of paper}} is to present results of the {{research}} work in development conducted to define the scalability conditions for the measuring points network at production cells in the resistant to the electromagnetic disturbances system based on the patented selective RFID gate technology. This is a conceptual paper that includes research results of the selective RFID gate and its place in a production system, a communication model between two measuring points in a complex production system that was the input to do further analysis required to define its scalability conditions. The proposed approach provides the following results: order and speed of data reading from <b>measurement</b> <b>points,</b> data transfer between <b>measurement</b> <b>points,</b> access to database, communication between the database and <b>measurement</b> <b>points,</b> data archiving, procedures of changing the <b>measurement</b> <b>point</b> configuration in the system, including error handling and network re-configuration. It is believed that both practitioners and researchers will benefit from it...|$|R
40|$|To {{validate}} {{the design process}} and {{ensure the safety of}} an important structure with sliding supports, a finite element-based force identification technique and a strain <b>measurement</b> <b>point</b> selection procedure have been presented in part I of this paper. The feasibility and accuracy of the proposed force identification technique and <b>measurement</b> <b>point</b> selection procedure are now verified in part II of this paper through numerical investigations on a steel plate support structure. The relationship between the orthogonal property of the selected <b>measurement</b> <b>points</b> and the condition number of the identification equation and the effectiveness of the sequential orthogonal searching algorithm are examined through Monte Carlo simulation. Two cases on force identification of the steel plate structure are then presented: one case concerns concentrated sliding force acting either at node or inside grid, and the other case pertains uniformly distributed sliding load covering a fixed area. For both cases, the strain <b>measurement</b> <b>points</b> of fairly good orthogonal property are firstly determined using the proposed <b>measurement</b> <b>point</b> selection procedure. The positions and magnitudes of initial and final forces are then identified using the proposed force identification technique with or without measurement noise included. The identification results obtained from both cases show that the proposed force identification method can effectively locate and identify the concentrated sliding force and the uniformly distributed sliding load covering a fixed area with no information required on the initial position and magnitude of the sliding force. It is also illustrated that the increase of the number of <b>measurement</b> <b>points</b> can improve the accuracy of identification results. Department of Civil and Environmental Engineerin...|$|R
5000|$|... 2001: James R. Holton for {{outstanding}} {{advances in the}} dynamics of the stratosphere through theoretical advances, perceptive use of models, and contributions to <b>key</b> <b>measurement</b> programs.|$|R
30|$|The TOA is reconstructed in each <b>measurement</b> <b>point</b> of any mobile {{trajectory}} of the BS.|$|R
5000|$|D is {{the value}} of the {{dispersion}} function at the <b>measurement</b> <b>point</b> in the particle accelerator ...|$|R
