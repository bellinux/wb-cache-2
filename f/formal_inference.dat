93|128|Public
25|$|Logic is {{generally}} considered formal when it analyzes and represents the form of any valid argument type. The form of an argument is displayed by representing its sentences in the formal grammar and symbolism of a logical language to make its content usable in <b>formal</b> <b>inference.</b> Simply put, formalising simply means translating English sentences into the language of logic.|$|E
2500|$|In 1933, Swiss {{astrophysicist}} Fritz Zwicky, {{who studied}} galactic clusters {{while working at}} the California Institute of Technology, made a similar inference. Zwicky applied the virial theorem to the Coma Cluster and obtained evidence of unseen mass that he called dunkle Materie 'dark matter'. Zwicky estimated its mass based on the motions of galaxies near its edge and compared that to an estimate based on its brightness and number of galaxies. He estimated that the cluster had about 400 times more mass than was visually observable. The gravity effect of the visible galaxies was far too small for such fast orbits, thus mass must be hidden from view. Based on these conclusions, Zwicky inferred that some unseen matter provided the mass and associated gravitation attraction to hold the cluster together. This was the first <b>formal</b> <b>inference</b> {{about the existence of}} dark matter. Zwicky's estimates were off by more than an order of magnitude, mainly due to an obsolete value of the Hubble constant; ...|$|E
50|$|In {{mathematical}} logic, a proof calculus {{corresponds to}} a family of formal systems that use a common style of <b>formal</b> <b>inference</b> for its inference rules. The specific inference rules {{of a member of}} such a family characterize the theory of a logic.|$|E
50|$|Like <b>formal</b> {{statistical}} <b>inference,</b> {{the purpose}} of informal inferential reasoning is to draw conclusions about a wider universe (population/process) from data (sample). However, {{it is to be}} contrasted with <b>formal</b> statistical <b>inference</b> that <b>formal</b> statistical procedure or methods are not necessarily used.|$|R
50|$|Any {{statistical}} inference requires some assumptions. A statistical {{model is a}} set of assumptions concerning the generation of the observed data and similar data. Descriptions of statistical models usually emphasize the role of population quantities of interest, about which we wish to draw inference. Descriptive statistics are typically used as a preliminary step before more <b>formal</b> <b>inferences</b> are drawn.|$|R
50|$|Recently, {{informal}} inferential reasoning {{has been}} the focus of research and discussion among researchers and educators in statistics education as it is seen as having a potential to help build fundamental concepts that underlie <b>formal</b> statistical <b>inference.</b> Many advocate that underlying concepts and skills of inference should be introduced early in the course or curriculum as they can help make the <b>formal</b> statistical <b>inference</b> more accessible (see published reaction of Garfield & Zieffler to).|$|R
50|$|Logic is {{generally}} considered formal when it analyzes and represents the form of any valid argument type. The form of an argument is displayed by representing its sentences in the formal grammar and symbolism of a logical language to make its content usable in <b>formal</b> <b>inference.</b> Simply put, formalising simply means translating English sentences into the language of logic.|$|E
50|$|There {{are three}} types of inferences: formal, {{informal}} and natural. <b>Formal</b> <b>inference</b> is logic in the deductive sense. For Newman, logic is indeed extremely useful especially in science and in society. However, its real-world applicability is very limited in that its usefulness is circumscribed by its initial assumptions. For Newman, to make logic work, human thought has to be trimmed to very specific and narrow meanings such that logical statements then lose real world applicability.|$|E
50|$|Aristotle's logical work is {{collected}} in the six texts that are collectively known as the Organon. Two of these texts in particular, namely the Prior Analytics and De Interpretatione, contain the heart of Aristotle's treatment of judgements and <b>formal</b> <b>inference,</b> and it is principally this part of Aristotle's works that is about term logic. Modern work on Aristotle's logic builds on the tradition started in 1951 with the establishment by Jan Lukasiewicz of a revolutionary paradigm. The Jan Lukasiewicz approach was reinvigorated in the early 1970s by John Corcoran and Timothy Smiley - which informs modern translations of Prior Analytics by Robin Smith in 1989 and Gisela Striker in 2009.|$|E
40|$|Functions play an {{important}} role throughoutbiology. Although molecular functions are cov-ered in the Gene Ontology, there is currently nopublicly available ontology of anatomical func-tions. Ontological considerations on the natureof functional abnormalities and their represen-tation in current phenotype ontologies showthat we can automatically extract a skeletonfor such an ontology of anatomical functionsby using a combination of process, phenotypeand anatomy ontologies. We provide an onto-logical analysis of the nature of functions andfunctional abnormalities. From this analysis,we derive an approach to the automatic ex-traction of anatomical functions from existingontologies using a combination of natural lan-guage processing, graph-based analysis of theontologies and <b>formal</b> <b>inferences.</b> Alternatively,we introduce a new relation to relate materialobjects to processes that realize the functionof the object to avoid a needless duplication ofprocesses already present in the Gene Ontol-ogy in a new ontology of anatomical functions. We discuss several limitations of the currentontologies that still need to be addressed to en-sure a consistent and complete representationof anatomical functions and functional abnor-malities...|$|R
40|$|We {{propose the}} use of <b>formal</b> ontological <b>inferencing,</b> rather than cladistics, to {{reconstruct}} phylogeny trees and to analyze the evolutionary relationships between species. For this experiment, we focused on the phylogeny of fungi. Lexical chaining technique {{has been used for}} incremental population of evolving ontological elements. Also category theory has been employed to provide an underlying formalism for capturing and analyzing the evolutionary behavior of the system...|$|R
40|$|If Rips is right, {{there are}} <b>formal</b> rules of <b>inference</b> in the mind; or else if Rips is wrong, there are <b>formal</b> rules of <b>inference</b> in the mind. Human {{reasoning}} is a mystery. Is {{it at the}} core of the mind, or an accidental and peripheral property? Does it depend on a unitary system, or on a set of disparate modules that somehow get along together to enable us to make valid inferences? And how is deductive ability acquired? Is it constructed from mental operations, as Piagetians propose; is it induced from examples, as connectionists claim; or is it innate, as philosophers and “evolutionary psychologists ” sometimes argue? Is deduction a matter of mobilizing <b>formal</b> rules of <b>inference</b> like those of a logical calculus, or of rules with a specific content like those of a computer “expert system”, or of remembered cases of valid reasoning like those exploited in other AI programs? Or could it depend on a grasp of meaning and of the fundamental semantic principle that a conclusion is valid if there are no cases in which the premises are true but it is false? Psychologists have been struggling with deduction for a century; cognitive scientists have recently honed in on it, and they hav...|$|R
50|$|Usually a given proof {{calculus}} encompasses {{more than}} a single particular formal system, since many proof calculi are under-determining and can be used for radically different logics. For example, a paradigmatic case is the sequent calculus, which can be used to express the consequence relations of both intuitionistic logic and relevance logic. Thus, loosely speaking, a proof calculus is a template or design pattern, characterized by a certain style of <b>formal</b> <b>inference,</b> that may be specialized to produce specific formal systems, namely by specifying the actual inference rules for such a system. There is no consensus among logicians on how best to define the term.|$|E
5000|$|The {{received}} {{point of}} view in analytic philosophy and formal logic, is that the calculus ratiocinator anticipates mathematical logic - an [...] "algebra of logic". The analytic {{point of view}} understands that the calculus ratiocinator is a <b>formal</b> <b>inference</b> engine or computer program, which can be designed so as to grant primacy to calculations. That logic began with Frege's 1879 Begriffsschrift and C.S. Peirce's writings on logic in the 1880s. Frege intended his [...] "concept script" [...] to be a calculus ratiocinator as well as a lingua characteristica. That part of formal logic relevant to the calculus comes under the heading of proof theory. From this perspective the calculus ratiocinator is only a part (or a subset) of the universal characteristic, and a complete universal characteristic includes a [...] "logical calculus".|$|E
5000|$|Ambrose {{began her}} {{career at the}} University of Michigan when she {{returned}} to the United States in 1935. She then took a position in Smith College in 1937, where she remained {{for the rest of her}} career. She was awarded the Austin and Sophia Smith chair in Philosophy in 1964 and became Professor Emeritus in 1972. From 1953-68 she was editor of the Journal of Symbolic Logic. She worked chiefly in logic and mathematical philosophy, writing a primer on the subject with her husband which became a widely used textbook and was known as [...] "Ambrose and Lazerowitz". She collaborated with her husband a number of works: Fundamentals of Symbolic Logic (1948), Logic: The Theory of <b>Formal</b> <b>Inference</b> (1961), Philosophical Theories (1976) and Essays in the Unknown Wittgenstein (1984). Even after her retirement she continued to teach and guest lecture at Smith and other universities around the country until her death, at the age of 94, on January 25, 2001.|$|E
40|$|There {{has been}} an {{enduring}} tension in modern cognitive psychology between the computational models available and the experimental data obtained. Standard computational models have assumed the symbolic paradigm: that it is constitutive of cognitive processes that they are mediated by the manipulation of symbolic structures. Such schemes easily handle <b>formal</b> <b>inferences,</b> and memory for arbitrary symbolic material. However, context-sensitive defeasible inference and content-addressable memory retrieval have remained problematic. By contrast, in the empirical data on human memory and inference, the opposite profile is observed. Everyday mundane reasoning is both context dependent and defeasible, and yet is performed easily and naturally, whereas subjects are typically unable to perform the simplest formal reasoning task (Wason and Johnson-Laud 1972; Evans 1982). In memory, content-addressable access in knowledge-rich domains seems natural and unproblematic for human subjects, whereas people can retain only very small quantities of arbitrary material. Despite this tension between experiment and theory, Fodor and Pylyshyn (1988) have recently reaffirmed what they term the “classical symbolic paradigm”. That is, they argue that symbolic cognitive processes are autonomous from their implementation. Thus they question the relevance of connectionist theorizing for psychology, and suggest that connectionism {{should be viewed as}} a theory of implementation for autonomous classical architectures...|$|R
5000|$|A {{white paper}} by the American Society of Human Genetics Ancestry and Ancestry Testing Task Force, Royal et al. (2010) {{observed}} of the Genghis Khan Haplogroup hypothesis:Although such a connection {{is by no}} means impossible, we currently have no way of assessing how much confidence to place in such a connection. We emphasize, however, that whenever <b>formal</b> <b>inferences</b> about population history have been attempted with uniparental systems, the statistical power is generally low. Claims of connections, therefore, between specific uniparental lineages and historical figures or historical migrations of peoples are merely speculative.Research published in 2016 suggested that Genghis possibly belonged to the haplogroup R-M343 (R1b). The controversial result was based on analysis of five bodies, dating from about 1130 - 1250, that were found in graves in Tavan Tolgoi, Mongolia. The remains of all 5 bodies belong to the Mongoloid physical type and are believed to be possibly related to members of the Mongol [...] "Golden Family", at around the time of Genghis Khan, although it is uncertain whether the Y-DNA haplogroup marker belongs to the Borijigin clan or the products of clan marriages between the female lineage of Genghis Khan’s Borjigin clan and males of other clans/tribes from Mongolia or Central Asia.|$|R
40|$|Stochastic DEA {{can deal}} {{effectively}} with noise in the non-parametric measurement of efficiency but unfortunately <b>formal</b> statistical <b>inference</b> on efficiency measures in not possible. In this paper, we provide a Bayesian approach to the problem organized around simulation techniques that allow for finite-sample inferences on efficiency scores. The new methods are applied to efficiency analysis of the Greek banking system for the period 1993 - 1999. The {{results show that the}} majority of the Greek banks operate close to best market practices. Efficiency measurement Stochastic DEA Bayesian methods Statistical inference...|$|R
5000|$|Lazerowitz co-wrote {{a number}} of books with Ambrose {{including}} a primer on logic which became a widely used textbook in the 1950s and was known as [...] "Ambrose and Lazerowitz". He collaborated with his wife on {{a number of}} works: Fundamentals of Symbolic Logic (1948), Logic: The Theory of <b>Formal</b> <b>Inference</b> (1961), Philosophical Theories (1976) and Essays in the Unknown Wittgenstein (1984). His chief contribution to philosophy was his development of the study of metaphilosophy. Morris claims to have created the term ‘metaphilosophy’ around 1940 and used it in print in 1942. He defined 'metaphilosophy' as an investigation of the nature of philosophical theories and their supporting arguments, with the aim of explaining their centuries-long irresolvability. (See Metaphilosophy 1990: 91 and The Language of Philosophy, chap. 1.) His most important books are The Structure of Metaphysics (1955) and Studies in Metaphilosophy (1964). He also wrote Philosophy and Illusion (1968), The Language of Philosophy: Freud and Wittgenstein (1977), and Cassandra in Philosophy (1983).|$|E
50|$|In 1933, Swiss {{astrophysicist}} Fritz Zwicky, {{who studied}} galactic clusters {{while working at}} the California Institute of Technology, made a similar inference. Zwicky applied the virial theorem to the Coma Cluster and obtained evidence of unseen mass that he called dunkle Materie 'dark matter'. Zwicky estimated its mass based on the motions of galaxies near its edge and compared that to an estimate based on its brightness and number of galaxies. He estimated that the cluster had about 400 times more mass than was visually observable. The gravity effect of the visible galaxies was far too small for such fast orbits, thus mass must be hidden from view. Based on these conclusions, Zwicky inferred that some unseen matter provided the mass and associated gravitation attraction to hold the cluster together. This was the first <b>formal</b> <b>inference</b> {{about the existence of}} dark matter. Zwicky's estimates were off by more than an order of magnitude, mainly due to an obsolete value of the Hubble constant; the same calculation today shows a smaller fraction, using greater values for luminous mass. However, Zwicky did correctly infer that the bulk of the matter was dark.|$|E
30|$|<b>Formal</b> <b>inference</b> rules can be found. The anti-fuzzification {{reasoning}} {{results can}} be used to obtain the credibility of the subject.|$|E
50|$|Scientific debate {{continues}} {{about whether}} human reasoning {{is based on}} mental models, versus <b>formal</b> rules of <b>inference</b> (e.g., O'Brien, 2009), domain-specific rules of inference (e.g., Cheng & Holyoak, 2008; Cosmides, 2005), or probabilities (e.g., Oaksford and Chater, 2007). Many empirical comparisons of the different theories {{have been carried out}} (e.g., Oberauer, 2006).|$|R
40|$|We {{show how}} a moment-based {{estimation}} procedure {{can be used}} to compute point estimates and standard errors for the two components of the widely used Olley–Pakes decomposition of aggregate (weighted average) productivity. When applied to business level microdata, the procedure allows for autocovariance and heteroscedasticity robust inference and hypothesis testing about, for example, the coevolution of the productivity components in different groups of firms. We provide an application to Finnish firm level data and find that <b>formal</b> statistical <b>inference</b> casts doubt on the conclusions that one might draw {{on the basis of a}} visual inspection of the components of the decomposition...|$|R
40|$|We {{propose a}} {{non-standard}} subsampling procedure to make <b>formal</b> statistical <b>inference</b> {{about the business}} cycle, {{one of the most}} important unobserved feature characterising fluctuations of economic growth. We show that some characteristics of business cycle can be modelled in a non-parametric way by discrete spectrum of the Almost Periodically Correlated (APC) time series. On the basis of estimated characteristics of this spectrum business cycle is extracted by filtering. As an illustration we characterise the man properties of business cycles in industrial production index for Polish economy. business cycle, industrial production index, almost periodically correlated time series, subsampling procedure...|$|R
40|$|It is {{well known}} that it is {{dangerous}} to infer causation from correlation. However, the mantra that correlation does not imply causation can lead to some researchers believing that <b>formal</b> <b>inference</b> is never possible from a correlational study. This paper presents a theoretical framework, a conceptual framework and a methodology for establishing <b>formal</b> <b>inference</b> from the analysis of prerequisites in an educational context. This is important in education because some prior knowledge is often required for success in any topic or course. The method is illustrated with a case study that investigates the effectiveness of a level four certificate as preparation for further study. The case study identified the unique contribution to subsequent performance made by individual courses in the certificate. It also identified the specific courses in subsequent study which were most affected by the certificate courses. We conclude that the approach can indeed enable <b>formal</b> <b>inference</b> from a correlational study...|$|E
30|$|It is {{important}} to emphasize the fundamental difference between PCA and sufficient dimension reduction. PCA reduces the number of predictors without considering the response variables, and choosing the number of principal components is not done through any <b>formal</b> <b>inference</b> paradigm. However, the idea of sufficient dimension reduction is to attain a sufficient subspace, which includes {{all of the information}} we need. There are asymptotic results for determining the number of dimensions in sufficient dimension reduction. These asymptotic results are derived and/or discussed in the references we cite for the dimension reduction techniques that we discuss below. Thus, using PCA is somewhat limited because it does not consider the response variable(s), nor does it have a <b>formal</b> <b>inference</b> mechanism for choosing the “best” number of principal components.|$|E
40|$|The ITP tool is an {{experimental}} inductive theorem prover for proving properties of Maude equational specifications, i. e., specifications in membership equational logic {{with an initial}} algebra semantics. The ITP tool has been written entirely in Maude and is in fact an executable specification of the <b>formal</b> <b>inference</b> system that it implements. ...|$|E
40|$|International audienceWe {{propose a}} novel {{geometric}} framework for analyzing spontaneous facial expressions, {{with the specific}} goal of comparing, matching, and averaging the shapes of landmarks trajectories. Here we represent facial expressions by {{the motion of the}} landmarks across the time. The trajectories are represented by curves. We use elastic shape analysis of these curves to develop a Riemannian framework for analyzing shapes of these trajectories. In terms of empirical evaluation, our results on two databases: UvA-NEMO and Cohn-Kanade CK+ are very promising. From a theoretical perspective, this framework allows <b>formal</b> statistical <b>inferences,</b> such as generation of facial expressions...|$|R
40|$|This paper {{develops}} two {{new methods}} for conducting <b>formal</b> statistical <b>inference</b> in nonlinear dynamic economic models. The two methods require very little analytical tractability, relying instead on numerical simulation of the model's dynamic behaviour. Although {{one of the}} estimators is asymptotically more efficient than the other, a Monte Carlo study shows that, for a specific application, the less efficient estimator has smaller mean squared error in samples of the size typically encountered in macroeconomics. The estimator with superior small sample performance is used to estimate the parameters of a real business cycle model using observed US time-series data. 1...|$|R
40|$|Abstract Background Several {{biomedical}} ontologies {{cover the}} domain of biological functions, including molecular and cellular functions. However, there is currently no publicly available ontology of anatomical functions. Consequently, no explicit relation between anatomical structures and their functions is expressed in the anatomy ontologies that are available for various species. Such an explicit relation between anatomical structures and their functions would be useful both for defining the classes of the anatomy and the phenotype ontologies accurately. Results We provide an ontological analysis of functions and functional abnormalities. From this analysis, we derive an approach to the automatic extraction of anatomical functions from existing ontologies which uses a combination of natural language processing, graph-based analysis of the ontologies and <b>formal</b> <b>inferences.</b> Additionally, we introduce a new relation to link material objects to processes that realize the function of these objects. This relation is introduced to avoid a needless duplication of processes already covered by the Gene Ontology in a new ontology of anatomical functions. Conclusions Ontological considerations {{on the nature of}} functional abnormalities and their representation in current phenotype ontologies show that we can extract a skeleton for an ontology of anatomical functions by using a combination of process, phenotype and anatomy ontologies automatically. We identify several limitations of the current ontologies that still need to be addressed to ensure a consistent and complete representation of anatomical functions and their abnormalities. Availability The source code and results of our analysis are available at [URL]. </p...|$|R
40|$|Description A {{package for}} {{analysing}} spatial data, mainly Spatial Point Patterns, including multitype/marked points and spatial covariates, in any two-dimensional spatial region. Also supports three-dimensional point patterns, and space-time point patterns {{in any number}} of dimensions. Contains over 1000 functions for plotting spatial data, exploratory data analysis,model-fitting, simulation, spatial sampling, model diagnostics,and <b>formal</b> <b>inference.</b> Data types include point patterns, line segment patterns, spatial windows, pixel images and tessellations. Exploratory methods includ...|$|E
40|$|The {{analysis}} and decomposition of time series is considered under autoregressive models {{with a new}} class of prior distributions for parameters defining latent components. The approach induces {{a new class of}} smoothness priors on autoregressive coefficients, provides for <b>formal</b> <b>inference</b> on model order, including very high order models, and permits for the incorporation of uncertainty about model order into summary inferences. The class of prior models also allows for subsets of un [...] ...|$|E
40|$|Abstract: Detecting {{functional}} errors on generic hardware components {{is often}} a complex task. This task becomes more complex in a componentwise approach when analyzing components without their embedded context that is the entire system description. In this paper, we propose a methodology that successfully detects just from the component’s description a pure functional error that neither extensive tests nor formal methods could find. Key–Words: static analysis, hardware functional error, logical formulae, <b>formal</b> <b>inference...</b>|$|E
40|$|A {{program of}} {{research}} in the field of artificial intelligence is presented. The research areas discussed include automatic theorem proving, representations of real-world environments, problem-solving methods, the design of a programming system for problem-solving research, techniques for general scene analysis based upon television data, and the problems of assembling an integrated robot system. Major accomplishments include {{the development of a new}} problem-solving system that uses both <b>formal</b> logical <b>inference</b> and informal heuristic methods, the development of a method of automatic learning by generalization, and the design of the overall structure of a new complete robot system. Eight appendices to the report contain extensive technical details of the work described...|$|R
40|$|Preferential {{attachment}} is {{a widely}} adopted paradigm for understanding the dynamics of social networks. <b>Formal</b> statistical <b>inference,</b> for instance GLM techniques, and model verification methods will require knowing test statistics are asymptotically normal even though node or count based network data is nothing like classical data from independently replicated experiments. We therefore study asymptotic normality of degree counts for a sequence of growing simple undirected preferential attachment graphs. The methods of proof rely on identifying martingales and then exploiting the martingale central limit theorems. S. Resnick and G. Samorodnitsky were supported by Army MURI grant W 911 NF- 12 - 1 - 0385 to Cornell Universit...|$|R
40|$|This is a semiannual {{progress}} report about {{a program of}} research in the field of Artificial Intelligence. The research areas discussed in-clude automatic theorem proving, representations of real-world environ-ments, problem-solving methods, the design of a programming system for problem-solving research, techniques for general scene analysis based upon television data, and the problems of assembling an integrated robot system. Major accomplishments include {{the development of a new}} problem-solving system that uses both <b>formal</b> logical <b>inference</b> and informal heuristic methods, the development of a method of automatic learning by generalization, and the design of the overall structure of a new complete robot system. Eight appendices to the report contain extensive technica...|$|R
