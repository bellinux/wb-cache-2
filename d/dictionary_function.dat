8|157|Public
50|$|The <b>dictionary</b> <b>function</b> {{allows the}} user to input Chinese {{characters}} or pinyin (including the phonetic marking) to search. The user can also highlight words in a text and search {{for them in the}} dictionary. Wenlin has its own built-in dictionaries, but user-created dictionaries can be uploaded into the software.|$|E
5000|$|Wenlin Software for Learning Chinese (...) is a {{software}} application designed by Tom Bishop, {{based on his}} experience {{of the needs of}} learners of the Chinese language, predominantly Mandarin. It contains a <b>dictionary</b> <b>function</b> as well as a text reader/writer function for reading and creating Chinese text files. Flashcards of new characters can also be compiled to aid in learning. By pointing the cursor at a Chinese character the software looks up an English word, and vice versa, working like a dictionary. If a word is selected out of context this [...] "could be extremely misleading for Chinese language students"http://www.csulb.edu/~txie/learn_online/wenlin.htm (note however that this review is rather dated and may not apply to the current version). The Wenlin software can recognize files in Unicode, GB 2312, Big5, and HZ format.|$|E
40|$|Abstract. Many {{recent efforts}} {{have shown the}} {{effectiveness}} of dictionary learning methods in solving several computer vision problems. However, when designing dictionaries, training and testing domains may be different, due to different view points and illumination conditions. In this paper, we present a function learning framework for the task of transforming a dictionary learned from one visual domain to the other, while maintaining a domain-invariant sparse representation of a signal. Domain dictionaries are modeled by a linear or non-linear parametric function. The <b>dictionary</b> <b>function</b> parameters and domain-invariant sparse codes are then jointly learned by solving an optimization problem. Experiments on real datasets demonstrate the effectiveness of our approach for applications such as face recognition, pose alignment and pose estimation. ...|$|E
40|$|The {{language}} of written business communication is characterised by the {{extensive use of}} phraseology, {{not only in terms}} of collocations and idiomatic expressions, but also of standard phrases in prototypical business genres. In any case, the phraseological information should be included in business dictionaries (in the following referred to as BDs) in accordance with the planned <b>dictionary</b> <b>functions.</b> Hence, the selection and presentation of the phraseological information should be decided by the lexicographer on the basis of the user needs alone and not on the recommendations of the phraseological literature about lexicographical practice. In this paper, I will firstly explain why lexicography and phraseology, although closely associated in a large number of studies, are quite different disciplines, and how their shared interest for dictionary practice in general is based on radically different views. I will then discuss the <b>dictionary</b> <b>functions</b> of BDs and focus on a number of concepts featuring extensive phraseological solutions to show and argue that dealing with phraseology in BDs should always keep focus on <b>dictionary</b> <b>functions...</b>|$|R
40|$|This paper {{presents}} a learning scheme for <b>dictionaries</b> of two-dimensional <b>functions</b> for matching pursuit applied in low-bitrate video coding. The motivation {{is to improve}} the coding performance of matching pursuit compression by adapting the structure of the <b>dictionary</b> <b>functions</b> to specific types of sequences. The proposed scheme is based on separable decomposition and vector quantization. The experiments with test video sequences prove that AVC/H. 264 video encoder with the proposed variant of matching pursuit coding of interframe residual exhibits improved compression performance. 1...|$|R
50|$|ANSI/NISO Z39.87 is a {{standard}} which defines a set of metadata elements for raster digital images. The purpose is {{to help in the}} development, exchange and interpretation of digital images. The <b>dictionary</b> <b>functions</b> of this standard assist in the interoperability between systems, services, and software. It is also an aid in the long-term management of and continuing access to digital image collections.|$|R
40|$|This volume aims {{to promote}} a {{discussion}} on the definition of collocation that will be useful for lexicographic purposes. Each of the papers the volume contains addresses in detail one or more aspects of three main issues. The first issue concerns, on the one hand, the boundaries between collocations and other word combinations, {{and the way in}} which lexicographers convey classifications to dictionary users. The second issue is the possibility, or even necessity, of adapting the definition of collocation to the objectives of different types of dictionaries, taking into account their specific micro- and macro-structural properties and their users’ needs. The third issue concerns the methods for collocation extraction. In order to tailor the definition of collocation to the actual <b>dictionary</b> <b>function</b> it is necessary to develop hybrid methods relying on corpus-based approaches and combining data processing with criteria such as native speakers' evaluation and contrastive analysis...|$|E
40|$|This study {{investigated}} how {{middle school students}} ’ comprehension was impacted by reading social studies texts online with a pop-up <b>dictionary</b> <b>function</b> for every word in the text. A quantitative counterbalance design {{was used to determine}} how 129 middle school students’ reading comprehension test scores for the pop-up dictionary reading differed from test scores for reading hard-copy texts or an online text without the dictionary. The pop-up dictionary reading was shown to be a statistically effective method for improving student test scores. The results suggest pop-up dictionaries may provide a helpful intervention for increasing middle-level learners ’ reading comprehension. (Keywords: reading comprehension, electronic dictionaries, content area reading.) Although America’s schools face strong pressures to improve standardized test scores (Lewis, 2005), expectations for social studies education were excluded from No Child Left Behind legislation. Thus, as school districts endeavor to im-prove scores in reading and mathematics, they often reduce or eliminate time previously allocated to social studies in elementary schools (Haskvitz, 2006...|$|E
40|$|Abstract Re-identification {{refers to}} the problem of {{recognizing}} a person at a dif-ferent location after one has been captured by a camera at a previous location. We discuss re-identification of faces using the domain adaptation approach which tack-les the problem where data in the target domain (different location) are drawn from a different distribution as the source domain (previous location), due to different view points, illumination conditions, resolutions, etc. In particular, we discuss the adap-tation of dictionary-based methods for re-identification of faces. We first present a domain adaptive dictionary learning (DADL) framework for the task of transform-ing a dictionary learned from one visual domain to the other, while maintaining a domain-invariant sparse representation of a signal. Domain dictionaries are modeled by a linear or non-linear parametric function. The <b>dictionary</b> <b>function</b> parameters and domain-invariant sparse codes are then jointly learned by solving an optimiza-tion problem. We then discuss an unsupervised domain adaptive dictionary learning (UDADL) method where labeled data are only available in the source domain. We propose to interpolate subspaces through dictionary learning to link the source and target domains. These subspaces are able to capture the intrinsic domain shift and form a shared feature representation for cross domain identification...|$|E
40|$|In {{the present}} paper we {{consider}} application of overcomplete dictionaries to solution of general ill-posed linear inverse problems. Construction of an adaptive optimal solution for such problems usually relies either on a singular value decomposition or representation of the solution via an orthonormal basis. The shortcoming of both approaches {{lies in the fact}} that, in many situations, neither the eigenbasis of the linear operator nor a standard orthonormal basis constitutes an appropriate collection of functions for sparse representation of the unknown function. In the context of regression problems, there have been an enormous amount of effort to recover an unknown function using an overcomplete dictionary. One of the most popular methods, Lasso, is based on minimizing the empirical likelihood and requires stringent assumptions on the dictionary, the, so called, compatibility conditions. While these conditions may be satisfied for the original <b>dictionary</b> <b>functions,</b> they usually do not hold for their images due to contraction imposed by the linear operator. In what follows, we bypass this difficulty by a novel approach which is based on inverting each of the <b>dictionary</b> <b>functions</b> and matching the resulting expansion to the true function, thus, avoiding unrealistic assumptions on the dictionary and using Lasso in a predictive setting. We examine both the white noise and the observational model formulations and also discuss how exact inverse images of the <b>dictionary</b> <b>functions</b> can be replaced by their approximate counterparts. Furthermore, we show how the suggested methodology can be extended to the problem of estimation of a mixing density in a continuous mixture. For all the situations listed above, we provide the oracle inequalities for the risk in a finite sample setting. Simulation studies confirm good computational properties of the Lasso-based technique...|$|R
3000|$|... [...]). In the TF decomposition-based {{works that}} will be {{presented}} at later part of this paper, a Gabor <b>dictionary</b> (Gaussian <b>functions,</b> i.e., [...]...|$|R
40|$|Appendices to {{the systems}} {{definition}} {{study for the}} space station Data System are compiled. Supplemental information on external interface specification, simulation and modeling, and function design characteristics is presented along with data flow diagrams, a data <b>dictionary,</b> and <b>function</b> allocation matrices...|$|R
40|$|This is an {{empirical}} {{investigation into the}} use of dictionaries by students of English and Science at Kuwait University with a particular focus on bilingual dictionaries of Arabic and English. In the introductory chapter we discuss the increasingly important role of vocabulary in EFL methodology and the relevant emphasis on improving existing dictionaries and teaching students how to make effective use of them. In chapter two we focus on bilingual dictionaries and review their status in EFL methodology. Then structural features {{of this type of}} dictionary are discussed with special reference to the problems of translation equivalents, sense discriminations, and intended <b>dictionary</b> <b>function.</b> Chapter three is a critical examination of two bilingual dictionaries in Kuwait. AL-MAWRID (English-Arabic) and DICTIONARY OF MODERN WRITTEN ARABIC (Arabic-English) are examined in terms of their users and uses, introductory matter, translation equivalents, sense discriminations, illustrative examples, collocations and idioms, grammatical information, and pronunciation. In the fourth chapter we review previous studies of dictionary users and uses and focus on their findings which bear relevance to our investigation. Chapter five is a description of the research method we follow in our investigation i. e. a questionnaire and two translation tests. In chapter six we present and analyse the findings on specific aspects of dictionary use addressed in the questionnaire. Chapter seven is an analysis of translation errors in relation to the type(s) of dictionary used in the Ll-L 2 and L 2 -Ll translation tests. The final chapter summarises the research findings and presents same suggestions with regard to the improvement of existing bilingual dictionaries of English and Arabic and the training of dictionary users. ...|$|E
40|$|The {{combination}} of wavelet theory and neural networks has {{lead to the}} development of wavelet networks. Wavelet networks are feed-forward neural networks using wavelets as activation function. Wavelet networks have been used in classification and identification problems with some success. The strength of wavelet networks lies in their capabilities of catching essential features in "frequency-rich" signals. In wavelet networks, both the position and the dilation of the wavelets are optimized besides the weights. Wavenet is another term to describe wavelet networks. Originally, wavenets did refer to neural networks using dyadic wavelets. In wavenets, the position and dilation of the wavelets are fixed and the weights are optimized by the network. We propose to adopt this terminology. The theory of wavenets has been generalized by the author to biorthogonal wavelets. This extension to biorthogonal wavelets has {{lead to the development}} of fuzzy wavenets. A serious difficulty with most neurofuzzy methods is that they do often furnish rules without a transparent interpretation. A solution to this problem is furnished by multiresolution techniques. The most appropriate membership functions are chosen from a dictionary of membership functions forming a multiresolution. The dictionary contains a number of membership functions that have the property to be symmetric, everywhere positive and with a single maxima. This family includes among others splines and some radial functions. The main advantage of using a dictionary of membership functions is that each term, such as "small", "large" is well defined beforehand and is not modified during learning. The multiresolution properties of the membership functions in the <b>dictionary</b> <b>function</b> permit to fuse or split membership functions [...] ...|$|E
40|$|Despite the {{popularity}} of e-readers and the enthusiasm of some for their use in secondary education, their utility in elementary education {{has not yet been}} systematically explored. Some advantages and disadvantages to teaching elementary literacy with e-readers are identified here. A convenience sample of ten teachers from a variety of different types of elementary schools and classrooms who were e-reader novices read a chapter of a grade-appropriate book on a Kindle and evaluated its use for their students. The teachers gave their opinions and ideas on how the devices could be implemented. Three specific technological affordances of an e-reader that carry pedagogical implications are discussed: the text-to-speech and <b>dictionary</b> <b>functions,</b> and the idea of unlimited access to books...|$|R
40|$|This paper {{focuses on}} the problem of data {{representation}} for feature selection and extraction of 1 D electronic nose signals. While PCA signal representation is a problem dependent method, we propose a novel approach based on frame theory where an over-complete <b>dictionary</b> of <b>functions</b> is considered in order to find the near-optimal representation of any 1 D signal considered. Feature selection is accomplished with an iterative methods called matching pursuit which select from the <b>dictionary</b> the <b>functions</b> that reduce the reconstruction error. In this case we can use the representation functions found for feature extraction or for signal compression purposes. Classification results of the selected features is performed with neural approach showing the high discriminatory power of the extracted feature...|$|R
40|$|We {{consider}} a general supervised learning problem with strongly convex and Lipschitz loss {{and study the}} problem of model selection aggregation. In particular, given a finite <b>dictionary</b> <b>functions</b> (learners) together with the prior, we generalize the results obtained by Dai, Rigollet and Zhang [Ann. Statist. 40 (2012) 1878 - 1905] for Gaussian regression with squared loss and fixed design to this learning setup. Specifically, we prove that the $Q$-aggregation procedure outputs an estimator that satisfies optimal oracle inequalities both in expectation and with high probability. Our proof techniques somewhat depart from traditional proofs by making most of the standard arguments on the Laplace transform of the empirical process to be controlled. Comment: Published in at [URL] the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
40|$|In {{this paper}} we use {{selected}} topics from formal language theory {{to show that}} programming language Python, with its syntax and semantics of primitive and structured statements (strings, tuples, lists and <b>dictionaries)</b> and <b>functions,</b> is suitable {{for use as a}} pseudo language and description of structures and algorithms in the formal language theory...|$|R
40|$|Within the South African {{lexicographical}} context, {{there have}} been several pleas for Afrikaans learner's and school dictionaries that incorporate innovative strategies and provide for specific identified target users and their particular problems. Nuwe Woordeboek sonder Grense (NWSG) is an Afrikaans learner's dictionary that is aimed at a specific group of users, can be used for text reception and text production and forms part of an established textbook series. Through new strategies and adaptations, this dictionary tries to form a bridge between different typological categories and to be a functional instrument for use in the classroom. In accordance with the theoretical formulation of <b>dictionary</b> <b>functions,</b> NWSG tries to further communication through initial support in and eventual assimilation of the foreign language. In this article, theoretical insights regarding learner's dictionaries as well as practical examples from NSWG are combined to illustrate the possibilities of a more user-friendly product. Articl...|$|R
40|$|Abstract: The South African wine {{industry}} identified {{the need for}} a special-field on-line dic-tionary on viticulture and oenology in Afrikaans, English and isiXhosa. The dictionary provides information on wine terminology as well as linguistic information on the use of such terminology. The {{purpose of this article is}} to give a description of the project. The process of compiling the dic-tionary is described, from the co-operation between the {{wine industry}} and lexicographers to the intended target users and the choice of languages of the <b>dictionary.</b> <b>Functions</b> of the <b>dictionary</b> are discussed, with reference to specific user situations, namely text production, text reception and translation. A system of labels has been designed for the dictionary and its benefit for the user is explained. In assisting the user to make an informed choice of a term, the notion of proscriptive-ness has been followed in the presentation of information in the wine dictionary...|$|R
40|$|Abstract: Within the South African {{lexicographical}} context, {{there have}} been several pleas for Afrikaans learner's and school dictionaries that incorporate innovative strategies and provide for specific identified target users and their particular problems. Nuwe Woordeboek sonder Grense (NWSG) is an Afrikaans learner's dictionary that is aimed at a specific group of users, can be used for text reception and text production and forms part of an established textbook series. Through new strategies and adaptations, this dictionary tries to form a bridge between different typological categories and to be a functional instrument for use in the classroom. In accordance with the theo-retical formulation of <b>dictionary</b> <b>functions,</b> NWSG tries to further communication through initial support in and eventual assimilation of the foreign language. In this article, theoretical insights regarding learner's dictionaries as well as practical examples from NSWG are combined to illus-trate the possibilities of a more user-friendly product...|$|R
3000|$|... {{determines the}} {{characteristics}} of the TF dictionary. The <b>dictionary</b> of TF <b>functions</b> can either suitably be modified or selected based on the application in hand. The scale factor [...]...|$|R
40|$|We {{present an}} {{optimization}} method and algorithm designed for three objectives: physical data independence, semantic optimization, and generalized tableau minimization. The method relies on generalized forms of chase and "backchase" with constraints (dependencies). By using <b>dictionaries</b> (finite <b>functions)</b> in physical schemas we can capture with constraints useful access {{structures such as}} indexes, materialized views, source capabilities, access support relations, gmaps, etc...|$|R
40|$|In {{modeling}} multivariate time series, it {{is important}} to allow time-varying smoothness in the mean and covariance process. In particular, there may be certain time intervals exhibiting rapid changes and others in which changes are slow. If such locally adaptive smoothness is not accounted for, one can obtain misleading inferences and predictions, with over-smoothing across erratic time intervals and under-smoothing across times exhibiting slow variation. This can lead to miscalibration of predictive intervals, which can be substantially too narrow or wide depending on the time. We propose a continuous multivariate stochastic process for time series having locally varying smoothness in both the mean and covariance matrix. This process is constructed utilizing latent <b>dictionary</b> <b>functions</b> in time, which are given nested Gaussian process priors and linearly related to the observed data through a sparse mapping. Using a differential equation representation, we bypass usual computational bottlenecks in obtaining MCMC and online algorithms for approximate Bayesian inference. The performance is assessed in simulations and illustrated in a financial application...|$|R
40|$|We {{consider}} {{the problem of}} model selection type aggregation {{in the context of}} density estimation. We first show that empirical risk minimization is sub-optimal for this problem and it shares this property with the exponential weights aggregate, empirical risk minimization over the convex hull of the <b>dictionary</b> <b>functions,</b> and all selectors. Using a penalty inspired by recent works on the $Q$-aggregation procedure, we derive a sharp oracle inequality in deviation under a simple boundedness assumption and we show that the rate is optimal in a minimax sense. Unlike the procedures based on exponential weights, this estimator is fully adaptive under the uniform prior. In particular, its construction does not rely on the sup-norm of the unknown density. By providing lower bounds with exponential tails, we show that the deviation term appearing in the sharp oracle inequalities cannot be improved. Comment: Published at [URL] in the Bernoulli ([URL] by the International Statistical Institute/Bernoulli Society ([URL]...|$|R
40|$|Summary. Although {{there is}} a rich {{literature}} on methods for allowing the variance in a univariate regression model to vary with predictors, time and other factors, relatively little {{has been done in}} the multivariate case. Our focus is on developing a class of nonparametric covariance regression models, which allow an unknown p × p covariance matrix to change flexibly with predictors. The proposed modeling framework induces a prior on a collection of covariance matrices indexed by predictors through priors for predictor-dependent loadings matrices in a factor model. In particular, the predictor-dependent loadings are characterized as a sparse combination of a collection of unknown <b>dictionary</b> <b>functions</b> (e. g, Gaussian process random functions). The induced covariance is then a regularized quadratic <b>function</b> of these <b>dictionary</b> elements. Our proposed framework leads to a highly-flexible, but computationally tractable formulation with simple conjugate posterior updates that can readily handle missing data. Theoretical properties are discussed and the methods are illustrated through simulations studies and an application to the Google Flu Trends data...|$|R
40|$|Non-uniform B-spline {{dictionaries}} on {{a compact}} interval are discussed. For each given partition, <b>dictionaries</b> of B-spline <b>functions</b> for the corresponding spline space are constructed. It is asserted that, {{by dividing the}} given partition into subpartitions and joining together the bases for the concomitant subspaces, slightly redundant <b>dictionaries</b> of B-splines <b>functions</b> are obtained. Such dictionaries are proved to span the spline space associated to the given partition. The proposed construction is shown to be potentially useful {{for the purpose of}} sparse signal representation. With that goal in mind, spline spaces specially adapted to produce a sparse representation of a given signal are considered...|$|R
40|$|Abstract-We {{introduce}} an algorithm, called matching pursuit, that decomposes any signal into {{a linear}} expansion of waveforms that are {{selected from a}} redundant <b>dictionary</b> of <b>functions.</b> These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a <b>dictionary</b> of Gabor <b>functions</b> a matching pursuit defines an adaptive time-frequency transform. We derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions. A matching pursuit isolates the signal structures that are coherent {{with respect to a}} given dictionary. An application to pattern extraction from noisy signals is described. We compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser. I...|$|R
40|$|Although {{there is}} a rich {{literature}} on methods for allowing the variance in a univariate regression model to vary with predictors, time and other factors, relatively little {{has been done in}} the multivariate case. Our focus is on developing a class of nonparametric covariance regression models, which allow an unknown p x p covariance matrix to change flexibly with predictors. The proposed modeling framework induces a prior on a collection of covariance matrices indexed by predictors through priors for predictor-dependent loadings matrices in a factor model. In particular, the predictor-dependent loadings are characterized as a sparse combination of a collection of unknown <b>dictionary</b> <b>functions</b> (e. g, Gaussian process random functions). The induced covariance is then a regularized quadratic <b>function</b> of these <b>dictionary</b> elements. Our proposed framework leads to a highly-flexible, but computationally tractable formulation with simple conjugate posterior updates that can readily handle missing data. Theoretical properties are discussed and the methods are illustrated through simulations studies and an application to the Google Flu Trends data. Comment: 33 pages, 8 figure...|$|R
40|$|Kernel {{methods are}} well known {{standard}} tools for solving function approximation and pattern classification problems. In this paper, we consider online learning in a reproducing kernel Hilbert space. We develop a simple and computationally efficient algorithm for sparse solutions. The approach is based on sequential projection learning and the coherence criterion, which is a fundamental parameter to characterize <b>dictionaries</b> of <b>functions</b> in sparse approximation problems. Experimental results show the effectiveness of our approach. 1...|$|R
40|$|We {{introduce}} an algorithm, called matching pursuit, that decomposes any signal into {{a linear}} expansion of waveforms that are {{selected from a}} redundant <b>dictionary</b> of <b>functions.</b> These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a <b>dictionary</b> of Gabor <b>functions,</b> a matching pursuit defines an adaptive time-frequency transform. We derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions [1]. A matching pursuit isolates the signal structures that are coherent {{with respect to a}} given dictionary. An application to pattern extraction from noisy signals is described. We compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser [4]. 1 This work was supported by the AFOSR grant F 49620 [...] ...|$|R
40|$|We {{consider}} a general supervised learning problem with strongly convex and Lipschitz loss {{and study the}} problem of model selection aggregation. In particular, given a finite <b>dictionary</b> <b>functions</b> (learners) together with the prior, we generalize the results obtained by Dai, Rigollet and Zhang [Ann. Statist. 40 (2012) 1878 – 1905] for Gaussian regression with squared loss and fixed design to this learning setup. Specifically, we prove that the Q-aggregation procedure outputs an estimator that satisfies optimal oracle inequalities both in expectation and with high probability. Our proof techniques somewhat de-part from traditional proofs by making most of the standard arguments on the Laplace transform of the empirical process to be controlled. 1. Introduction and main results. Let X be a probability space and let (X,Y) ∈ X × R be a random couple. Broadly speaking, the goal of statistical learning is to predict Y given X. To achieve this goal, we observe a dataset D = {(X 1, Y 1), [...] ., (Xn,Yn) } that consists of n independent copies of (X,Y) and use these observations to construct a function (learner) f:X →R such that f (X...|$|R
50|$|Multitran is an {{editable}} Russian multilingual online dictionary {{launched on}} 1 April 2001. The English-Russian-English dictionary contains over four million entries, while the total database has about eight million entries. The <b>dictionary</b> has a <b>function</b> for reporting translation errors for registered users.|$|R
40|$|This paper explores a novel {{approach}} for ventricular and atrial activities estimation in electrocardiogram (ECG) signals, based on sparse source separation. Sparse decompositions of ECG over signal-adapted multi-component dictionaries can lead to natural separation of its components. In this work, <b>dictionaries</b> of <b>functions</b> adapted to ventricular and atrial activities are respectively defined. Then, the weighted orthogonal matching pursuit algorithm is used to unmix the two components of ECG signals. Despite {{the simplicity of the}} approach, results are very promising, showing the capacity of the algorithm to generate realistic estimations of atrial and ventricular activities. 1...|$|R
40|$|In {{order to}} {{overcome}} the long training time caused by searching optimal basic functions based on greedy strategy from a redundant basis <b>function</b> <b>dictionary</b> for the intuitionistic fuzzy kernel matching pursuit (IFKMP), the particle swarm optimization algorithm with powerful ability of global search and quick convergence rate is applied to speed up searching optimal basic function data in <b>function</b> <b>dictionary.</b> The approach of intuitionistic fuzzy kernel matching pursuit based on particle swarm optimization algorithm, namely, PS-IFKMP, is proposed. This algorithm {{is applied to the}} aerospace target recognition, which requires real-time ability. Simulation results show that, compared with the conventional approaches, the proposed algorithm can decrease training time and improve calculation efficiency obviously with almost unchanged classification accuracy, while the model has better sparsity and generalization. It is also demonstrated that this approach is suitable for the application requiring both accuracy and efficiency...|$|R
40|$|The {{main goal}} in {{developing}} and deploying Geographic Information Services (GIS) at NOAA’s National Climatic Data Center (NCDC) is to provide users with simple access to data archives while integrating new and informative climate products. Users who are presented with data discovery options which flow into detailed product selection maps can search using standard “region finder ” tools or gazetteer (geographical <b>dictionary</b> search) <b>functions.</b> Each tabbed selection offers steps to help users progress through the systems. A series of additional base map layers or data types {{have been added to}} provide companion information. New map services include: Severe Weathe...|$|R
