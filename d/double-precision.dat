518|0|Public
25|$|Supports <b>double-precision</b> {{floating}} point.|$|E
25|$|VFPv5-D16-M: Implemented on Cortex-M7 when {{single and}} <b>double-precision</b> floating-point core option exists.|$|E
25|$|The {{instruction}} {{set for the}} floating point coprocessor also had several instructions added to it. An IEEE 754-compliant floating-point square root instruction was added. It supported both single- and <b>double-precision</b> operands. A set of instructions that converted single- and <b>double-precision</b> floating-point numbers to 32-bit words were added. These complemented the existing conversion instructions by allowing the IEEE rounding mode to be specified by the instruction instead of the Floating Point Control and Status Register.|$|E
25|$|In May 2008, IBM {{introduced}} the high-performance <b>double-precision</b> floating-point {{version of the}} Cell processor, the PowerXCell 8i, at the 65nm feature size.|$|E
25|$|Compared to its {{personal}} computer contemporaries, {{the relatively high}} overall floating point performance of a Cell processor seemingly dwarfs the abilities of the SIMD unit in CPUs like the Pentium 4 and the Athlon 64. However, comparing only floating point abilities of a system is a one-dimensional and application-specific metric. Unlike a Cell processor, such desktop CPUs are more suited to the general purpose software usually run on {{personal computer}}s. In addition to executing multiple instructions per clock, processors from Intel and AMD feature branch predictors. The Cell is designed to compensate for this with compiler assistance, in which prepare-to-branch instructions are created. For <b>double-precision</b> floating point operations, as sometimes used in personal computers and often used in scientific computing, Cell performance drops by an order of magnitude, but still reaches 20.8GFLOPS (1.8GFLOPS per SPE, 6.4GFLOPS per PPE). The PowerXCell 8i variant, which was specifically designed for <b>double-precision,</b> reaches 102.4GFLOPS in <b>double-precision</b> calculations.|$|E
25|$|Similarly, Python defines math.sin(x) {{within the}} {{built-in}} math module. Complex sine functions {{are also available}} within the cmath module, e.g. cmath.sin(z). CPython's math functions call the C math library, and use a <b>double-precision</b> floating-point format.|$|E
25|$|On May 13, 2008, IBM {{announced}} the BladeCenter QS22. The QS22 introduces the PowerXCell 8i processor with {{five times the}} <b>double-precision</b> floating point performance of the QS21, and the capacity for up to 32GB of DDR2 memory on-blade.|$|E
25|$|As JavaScript has unusual {{limitations}} – such as no explicit integer type, only <b>double-precision</b> binary {{floating point}} – languages that compile to JavaScript {{and do not}} take care to use the integer-converting shift and bitwise logical operators may have slightly different behavior than in other environments.|$|E
25|$|Number: {{a signed}} decimal number that may contain a {{fractional}} part and may use exponential E notation, but cannot include non-numbers such as NaN. The format makes {{no distinction between}} integer and floating-point. JavaScript uses a <b>double-precision</b> floating-point format for all its numeric values, but other languages implementing JSON may encode numbers differently.|$|E
25|$|MIPS IV added {{several new}} FP {{arithmetic}} instructions for both single- and <b>double-precision</b> FPNs: fused-multiply add or subtract, reciprocal, and reciprocal square-root. The FP fused-multiply add or subtract instructions perform either {{one or two}} roundings (it is implementation-defined), to exceed or meet IEEE 754 accuracy requirements (respectively). The FP reciprocal and reciprocal square-root instructions do not comply with IEEE 754 accuracy requirements, and produce results that differ from the required accuracy by one or two units of last place (it is implementation defined). These instructions serve applications where instruction latency {{is more important than}} accuracy.|$|E
25|$|DV (divide): Divide the {{contents}} of register A by the data at the referenced memory address. Store the quotient in register A and the absolute value of the remainder in register Q. Unlike modern machines, fixed-point numbers were treated as fractions (notional decimal point just to right of the sign bit), so you could produce garbage if the divisor was not larger than the dividend; there was no protection against that situation. In the Block II AGC, a <b>double-precision</b> dividend started in A and L (the Block II LP), and the correctly signed remainder was delivered in L. That considerably simplified the subroutine for double precision division.|$|E
25|$|<b>Double-precision</b> floating-point {{implementations}} of the gamma {{function and}} its logarithm {{are now available}} in most scientific computing software and special functions libraries, for example TK Solver, Matlab, GNU Octave, and the GNU Scientific Library. The gamma function was also added to the C standard library (math.h). Arbitrary-precision implementations are available in most computer algebra systems, such as Mathematica and Maple. PARI/GP, MPFR and MPFUN contain free arbitrary-precision implementations. A little-known feature of the calculator app included with the Android operating system {{is that it will}} accept fractional values as input to the factorial function and return the equivalent gamma function value. The same is true for Windows Calculator (in scientific mode).|$|E
25|$|The FireStream line is {{a series}} of add-on {{expansion}} cards released from 2006 to 2010, based on standard Radeon GPUs but designed to serve as a general-purpose co-processor, rather than rendering and outputting 3D graphics. Like the FireGL/FirePro line, they were given more memory and memory bandwidth, but the FireStream cards do not necessarily have video output ports. All support 32-bit single-precision floating point, and all but the first release support 64-bit <b>double-precision.</b> The line was partnered with new APIs to provide higher performance than existing OpenGL and Direct3D shader APIs could provide, beginning with Close to Metal, followed by OpenCL and the Stream Computing SDK, and eventually integrated into the APP SDK.|$|E
25|$|MIPS III {{removed the}} Coprocessor 3 (CP3) support instructions, and reused its opcodes {{for the new}} doubleword instructions. The {{remaining}} coprocessors gained instructions to move doublewords between coprocessor registers and the GPRs. The floating general registers (FGRs) were extended to 64 bits and the requirement for instructions to use even-numbered register only was removed. This is incompatible with earlier versions of the architecture; {{a bit in the}} floating-point control/status register is used to operate the MIPS III floating-point unit (FPU) in a MIPS I- and II-compatible mode. The floating-point control registers were not extended for compatibility. The only new floating-point instructions added were those to copy doublewords between the CPU and FPU convert single- and <b>double-precision</b> floating-point numbers into doubleword integers and vice versa.|$|E
2500|$|The {{implementations}} {{of floating}} point on Nvidia GPUs are mostly IEEE compliant; however, {{this is not}} true across all vendors. [...] This has implications for correctness which are considered important to some scientific applications. While 64-bit floating point values (double precision float) are commonly available on CPUs, these are not universally supported on GPUs. Some GPU architectures sacrifice IEEE compliance, while others lack <b>double-precision.</b> [...] Efforts have occurred to emulate <b>double-precision</b> floating point values on GPUs; however, the speed tradeoff negates any benefit to offloading the computing onto the GPU in the first place.|$|E
2500|$|Similar, a [...] "D" [...] {{was used}} by Sharp pocket {{computers}} PC-1280, PC-1470U, PC-1475, PC-1480U, PC-1490U, PC-1490UII, PC-E500, PC-E500S, PC-E550, PC-E650 and PC-U6000 to indicate 20-digit <b>double-precision</b> numbers in BASIC between 1987 and 1995.|$|E
2500|$|... (NB. This website {{contains}} {{open source}} floating-point IP cores {{for the implementation}} of floating-point operators in FPGA or ASIC devices. The project double_fpu contains verilog source code of a <b>double-precision</b> floating-point unit. The project fpuvhdl contains vhdl source code of a single-precision floating-point unit.) ...|$|E
2500|$|In {{contrast}} to the single and <b>double-precision</b> formats, this format does not utilize an implicit/hidden bit. [...] Rather, bit 63 contains the integer part of the significand and bits 62-0 hold the fractional part. [...] Bit 63 will be 1 on all normalized numbers. There were several advantages to this design when the 8087 was being developed: ...|$|E
2500|$|The IBM 7094, also {{introduced}} in 1962, supports single-precision and <b>double-precision</b> representations, {{but with no}} relation to the UNIVAC's representations. Indeed, in 1964, IBM introduced proprietary hexadecimal floating-point representations in its System/360 mainframes; these same representations are still available for use in modern z/Architecture systems. [...] However, in 1998, IBM included IEEE-compatible binary floating-point arithmetic to its mainframes; in 2005, IBM also added IEEE-compatible decimal floating-point arithmetic.|$|E
2500|$|This is very inefficient; by {{doubling}} the precision ahead of time, all additions must be <b>double-precision</b> {{and at least}} twice as many partial products are needed than for the more efficient algorithms actually implemented in computers. Some multiplication algorithms are designed for two's complement, notably Booth's multiplication algorithm. Methods for multiplying sign-magnitude numbers don't work with two's-complement numbers without adaptation. There isn't usually a problem when the multiplicand (the one being repeatedly added to form the product) is negative; the issue is setting the initial bits of the product correctly when the multiplier is negative. Two methods for adapting algorithms to handle two's-complement numbers are common: ...|$|E
2500|$|VFP (Vector Floating Point) {{technology}} is an FPU (Floating-Point Unit) coprocessor extension to the ARM architecture (implemented differently in ARMv8 - coprocessors not defined there). It provides low-cost single-precision and <b>double-precision</b> floating-point computation fully compliant with the ANSI/IEEE Std 754-1985 Standard for Binary Floating-Point Arithmetic. VFP provides floating-point computation {{suitable for a}} wide spectrum of applications such as PDAs, smartphones, voice compression and decompression, three-dimensional graphics and digital audio, printers, set-top boxes, and automotive applications. The VFP architecture was intended to support execution of short [...] "vector mode" [...] instructions but these operated on each vector element sequentially and thus did not offer the performance of true single instruction, multiple data (SIMD) vector parallelism. This vector mode was therefore removed shortly after its introduction, to be replaced with the much more powerful NEON Advanced SIMD unit.|$|E
2500|$|In 2008, IBM {{announced}} a revised {{variant of the}} Cell called the PowerXCell 8i, which is available in QS22 Blade Servers from IBM. The PowerXCell is manufactured on a 65nm process, and adds support for up to 32GB of slotted DDR2 memory, as well as dramatically improving <b>double-precision</b> floating-point performance on the SPEs {{from a peak of}} about 12.8GFLOPS to 102.4GFLOPS total for eight SPEs, which, coincidentally, is the same peak performance as the NEC SX-9 vector processor released around the same time. The IBM Roadrunner supercomputer, the world's fastest during 2008-2009, consists of 12,240 PowerXCell 8i processors, along with 6,562 AMD Opteron processors. The PowerXCell 8i powered super computers also dominated all of the top 6 [...] "greenest" [...] systems in the Green500 list, with highest MFLOPS/Watt ratio supercomputers in the world. Beside the QS22 and supercomputers, the PowerXCell processor is also available as an accelerator on a PCI Express card and is used as the core processor in the QPACE project.|$|E
2500|$|Although the {{ubiquitous}} x86 processors of today use little-endian storage {{for all types}} of data (integer, floating point, BCD), {{there are a number of}} hardware architectures where floating-point numbers are represented in big-endian form while integers are represented in little-endian form. There are ARM processors that have half little-endian, half big-endian floating-point representation for <b>double-precision</b> numbers: both 32-bit words are stored in little-endian like integer registers, but the most significant one first. Because there have been many floating-point formats with no [...] "network" [...] standard representation for them, the XDR standard uses big-endian IEEE 754 as its representation. It may therefore appear strange that the widespread IEEE 754 floating-point standard does not specify endianness. Theoretically, this means that even standard IEEE floating-point data written by one machine might not be readable by another. However, on modern standard computers (i.e., implementing IEEE 754), one may in practice safely assume that the endianness is the same for floating-point numbers as for integers, making the conversion straightforward regardless of data type. (Small embedded systems using special floating-point formats may be another matter however.) ...|$|E
5000|$|<b>Double-precision</b> is {{the same}} except that the {{mantissa}} (fraction) field is wider and the <b>double-precision</b> number is stored in a double word (8 bytes): ...|$|E
5000|$|... bytes {{if using}} <b>double-precision,</b> {{and half of}} that if using {{single-precision}} (however, numerically exact open quantum system dynamics {{has been shown to}} require <b>double-precision</b> in Fig. 2 of [...] ).|$|E
50|$|The IBM PPE Vector/SIMD manual {{does not}} define {{operations}} for <b>double-precision</b> floating point, though IBM has published material implying certain <b>double-precision</b> performance numbers {{associated with the}} Cell PPE VMX technology.|$|E
50|$|The {{theoretical}} <b>double-precision</b> {{processing power}} of a Fermi GPU is 1/2 of the single precision performance on GF100/110. However, in practice this <b>double-precision</b> power is only available on professional Quadro and Tesla cards, while consumer GeForce cards are capped to 1/8.|$|E
50|$|<b>Double-precision</b> floating-point format usually {{refers to}} binary64, as {{specified}} by the IEEE 754 standard, not to the 64-bit decimal format decimal64.In older computers, different floating-point formats of 8 bytes were used, e.g., GW-BASIC's <b>double-precision</b> data type was the 64-bit MBF floating-point format.|$|E
50|$|This gives from 15 to 17 {{significant}} decimal digits precision. If a decimal string with at most 15 significant digits {{is converted}} to IEEE 754 <b>double-precision</b> representation, and then converted back to a decimal string with {{the same number of}} digits, the final result should match the original string. If an IEEE 754 <b>double-precision</b> number {{is converted to}} a decimal string with at least 17 significant digits, and then converted back to <b>double-precision</b> representation, the final result must match the original number.|$|E
5000|$|<b>Double-precision</b> numbers occupy 64 bits. In double precision: ...|$|E
5000|$|... 1,000 <b>double-precision</b> MFLOPS {{based on}} the Whetstone {{benchmark}} ...|$|E
5000|$|Random Number Generators in both single- and <b>double-precision</b> ...|$|E
5000|$|... #Subtitle level 3: Execution {{speed with}} <b>double-precision</b> {{arithmetic}} ...|$|E
50|$|<b>Double-precision</b> floating-point numbers, {{introduced}} on the 7094, have {{a magnitude}} sign, an eight-bit excess-128 exponent, and a 54-bit magnitude. The <b>double-precision</b> number {{is stored in}} memory in an even-odd pair of consecutive words; the sign and exponent in the second word are ignored when the number is used as an operand.|$|E
5000|$|... 20 digits (mantissa) + 2 digits (exponent) in <b>double-precision</b> mode.|$|E
5000|$|... #Subtitle level 2: IEEE 754 <b>double-precision</b> binary floating-point format: binary64 ...|$|E
