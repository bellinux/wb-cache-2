1054|331|Public
5000|$|... #Subtitle level 3: <b>Ensemble</b> <b>learning</b> {{algorithms}} (supervised meta-algorithms for combining multiple learning algorithms together) ...|$|E
5000|$|Python : Scikit-learn,a {{package for}} Machine Learning in python offers {{packages}} for <b>ensemble</b> <b>learning</b> including packages for bagging and averaging methods.|$|E
5000|$|Dietterich, T. G. (2002). <b>Ensemble</b> <b>Learning.</b> In The Handbook of Brain Theory and Neural Networks, Second edition, (M.A. Arbib, Ed.), Cambridge, MA: The MIT Press, 2002. 405-408.|$|E
40|$|Abstract –A {{reliable}} and precise classification of tumours {{is essential for}} successful treatment of cancer. Recent researches have confirmed the utility of <b>ensemble</b> machine <b>learning</b> algorithms for gene expression data analysis. In this paper, a new <b>ensemble</b> machine <b>learning</b> algorithm is proposed for classification and prediction on gene expression data. The algorithm is tested and compared with three popular adopted ensembles, i. e. bagging, boosting and arcing. The {{results show that the}} proposed algorithm greatly outperforms existing methods, achieving high accuracy over 12 gene expression datasets. Index Terms – <b>ensemble</b> machine <b>learning,</b> pattern recognition, microarray I...|$|R
5000|$|The group's {{most notable}} {{performance}} came on January 28, 2008 when {{the campaign of}} then U.S. Presidential candidate Barack Obama asked OASN to open for a rally at AU where Obama was endorsed by Senator Ted Kennedy. The <b>ensemble</b> <b>learned</b> Obama's campaign theme song, Stevie Wonder's [...] "Signed, Sealed, Delivered I'm Yours" [...] (1970), but was forced offstage after a teleprompter was kicked during their performance.|$|R
40|$|Abstract. An <b>ensemble</b> of <b>learning</b> {{machines}} {{has been}} theoretically and empirically shown to generalise better than single learners. Diversity and accuracy are two key properties that ensemble members should possess {{in order for}} this generalisation principle to hold. Viewing these properties as objectives, we take the position of rendering multi-objective evolutionary algorithms as effective solution concepts {{to the problem of}} <b>ensemble</b> construction and <b>learning.</b> ...|$|R
50|$|Uplift {{modeling}} {{has been}} recently extended and incorporated into diverse machine learning algorithms, like Inductive Logic Programming, Bayesian Network, Statistical relational learning, Support Vector Machines, Survival Analysis and <b>Ensemble</b> <b>learning.</b>|$|E
5000|$|L.L.Minku, A.P.White, X.Yao, The Impact of Diversity on On-line <b>Ensemble</b> <b>Learning</b> in the Presence of Concept Drift, IEEE Transactions on Knowledge and Data Engineering, vol.22, no.5, pp. 730-742, 2010. Download from L.Minku webpage.|$|E
5000|$|Feature Bagging for Outlier Detection [...] runs LOF on {{multiple}} projections and combines {{the results for}} improved detection qualities in high dimensions. This is the first <b>ensemble</b> <b>learning</b> approach to outlier detection, for other variants see ref.|$|E
40|$|A {{reliable}} and precise classification of tumours {{is essential for}} successful treatment of cancer. Recent researches have confirmed the utility of <b>ensemble</b> machine <b>learning</b> algorithms for gene expression data analysis. In this paper, a new <b>ensemble</b> machine <b>learning</b> algorithm is proposed for classification and prediction on gene expression data. The algorithm is tested and compared with three popular adopted ensembles, i. e. bagging, boosting and arcing. The {{results show that the}} proposed algorithm greatly outperforms existing methods, achieving high accuracy over 12 gene expression datasets...|$|R
40|$|Abstract—Based on {{negative}} correlation learning and evolutionary learning, this brief paper presents evolutionary ensembles with {{negative correlation}} learning (EENCL) {{to address the}} issues of automatic determination {{of the number of}} individual neural networks (NNs) in an ensemble and the exploitation of the interaction between individual NN design and combination. The idea of EENCL is to encourage different individual NNs in the <b>ensemble</b> to <b>learn</b> different parts or aspects of the training data so that the <b>ensemble</b> can <b>learn</b> better the entire training data. The cooperation and specialization among different individual NNs are considered during the individual NN design. This provides an opportunity for different NNs to interact with each other and to specialize. Experiments on two real-world problems demonstrate that EENCL can produce NN ensembles with good generalization ability. Index Terms—Evolutionary <b>ensembles,</b> negative correlation <b>learning,</b> neural networks. I...|$|R
30|$|The main {{outcome of}} our {{approach}} is an <b>ensemble</b> machine <b>learning</b> algorithm that predicts energy consumption as a non-linear time series regression problem {{on a daily}} scale for each electrical line id.|$|R
50|$|In {{addition}} {{to the area of}} sensor network, other fields such as time-triggered architecture, safety of cyber-physical systems, data fusion, robot convergence, high-performance computing, software/hardware reliability, <b>ensemble</b> <b>learning</b> in artificial intelligence systems could also benefit from Brooks-Iyengar algorithm.|$|E
5000|$|Sine, Line, Plane, Circle and Boolean Data Sets, L.L.Minku, A.P.White, X.Yao, The Impact of Diversity on On-line <b>Ensemble</b> <b>Learning</b> in the Presence of Concept Drift, IEEE Transactions on Knowledge and Data Engineering, vol.22, no.5, pp. 730-742, 2010. Access from L.Minku webpage.|$|E
50|$|In machine {{learning}} the random subspace method, also called attribute bagging or feature bagging, is an <b>ensemble</b> <b>learning</b> method {{that attempts to}} reduce the correlation between estimators in an ensemble by training them on random samples of features instead of the entire feature set.|$|E
50|$|In {{statistics}} and machine <b>learning,</b> <b>ensemble</b> methods use multiple learning algorithms to obtain better predictive performance {{than could be}} obtained {{from any of the}} constituent learning algorithms alone.Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine <b>learning</b> <b>ensemble</b> consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.|$|R
40|$|The present paper {{describes}} the features {{and implementation of}} SmartVox, an application designed to help vocal <b>ensembles</b> <b>learn</b> and perform polyphonic music. Technically, SmartVox is a distributed web application that delivers audiovisual scores through the performer's mobile devices. From a singer's point of view, this setup allows for the synergy between visual and acoustic stimuli, which facilitates the interpretive and performative processes, particularly in polyphonic passages. It also enables spatial separation of the performers (cori spezzati), and speeds up the learning process of unfamiliar musical materials (e. g. microtonal tuning, texts in a foreign language). The ubiquity of smartphones makes such a distributed system affordable and allows the use of SmartVox in multiple contexts, from professional ensembles to pedagogical and recreational practices...|$|R
40|$|Abstract We {{present a}} package for R {{language}} containing {{a set of}} tools for regression using <b>ensembles</b> of <b>learning</b> machines and for time series forecasting. The pack-age contains implementations of Bagging and Adaboost for regression, and al-gorithms for computing mutual information, autocorrelation and false nearest neighbors...|$|R
50|$|Zhou {{is known}} for {{significant}} contributions to <b>ensemble</b> <b>learning,</b> multi-label learning, and learning with partial supervision (semi-supervised learning, multi-instance learning, etc.). He has authored two books and published more than 150 scientific articles in premium journals/conferences. According to Google Scholar, his h-index is 74. He also holds 18 patents.|$|E
50|$|Cascading is a {{particular}} case of <b>ensemble</b> <b>learning</b> based on the concatenation of several Classifiers, using all information collected from the output from a given classifier as additional information for the next classifier in the cascade. Unlike voting or stacking ensembles, which are multiexpert systems, cascading is a multistage one.|$|E
5000|$|LPBoost is an <b>ensemble</b> <b>learning</b> {{method and}} thus does not dictate {{the choice of}} base learners, the space of {{hypotheses}} [...] Demiriz et al. showed that under mild assumptions, any base learner can be used. If the base learners are particularly simple, they are {{often referred to as}} decision stumps.|$|E
5000|$|... "Little Stars", {{a sort of}} {{advanced}} level of Bambini. Children who have shown exceptional attention and achievement after at least a semester in Bambini are moved up to the Stelline ensemble. This <b>ensemble</b> generally <b>learns</b> {{one or two more}} songs than Bambini as well as some additional lessons.|$|R
5000|$|... #Caption: Kazan Cathedral, St. Petersburg, {{where it}} all started. Here A.V. Alexandrov, who would one day create the <b>ensemble,</b> began to <b>learn</b> his trade.|$|R
40|$|Independent Components Analysis (ICA) maximizes the {{statistical}} {{independence of the}} representational components of a training image ensemble, but it cannot distinguish between the different factors, or modes, inherent to image formation, including scene structure, illumination, and imaging. We introduce a nonlinear, multifactor model that generalizes ICA. Our Multilinear ICA (MICA) model of image <b>ensembles</b> <b>learns</b> {{the statistical}}ly independent components of multiple factors. Whereas ICA employs linear (matrix) algebra, MICA exploits multilinear (tensor) algebra. We furthermore introduce a multilinear projection algorithm which projects an unlabeled test image into the N constituent mode spaces to simultaneously infer its mode labels. In the context of facial image ensembles, where the mode labels are person, viewpoint, illumination, expression, etc., we demonstrate that the statistical regularities learned by MICA capture information that, in conjunction with our multilinear projection algorithm, improves automatic face recognition. ...|$|R
50|$|Random forests or random {{decision}} {{forests are}} an <b>ensemble</b> <b>learning</b> method for classification, regression and other tasks, that operate by constructing {{a multitude of}} decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.|$|E
50|$|Recursive {{partitioning}} {{methods have}} been developed since the 1980s. Well known methods of recursive partitioning include Ross Quinlan's ID3 algorithm and its successors, C4.5 and C5.0 and Classification and Regression Trees. <b>Ensemble</b> <b>learning</b> methods such as Random Forests help to overcome a common criticism of these methods - their vulnerability to overfitting of the data - by employing different algorithms and combining their output in some way.|$|E
50|$|In <b>ensemble</b> <b>learning</b> one {{tries to}} combine the models {{produced}} by several learners into an ensemble that performs better than the original learners. One way of combining learners is bootstrap aggregating or bagging, which shows each learner a randomly sampled subset of the training points so that the learners will produce different models that can be sensibly averaged. In bagging, one samples training points with replacement from the full training set.|$|E
50|$|Offers {{opportunities}} {{to perform in}} trios, quartets, or quintets or other combinations, studying the wealth of music written for chamber <b>ensembles.</b> Players <b>learn</b> independence as soloists within the supportive environment of a small group, {{under the guidance of}} professional artist musicians who serve as coaches. Typically about one-third of EYSO students also participate in the CMI.|$|R
40|$|A new {{approach}} to designing neural network ensembles has been proposed recently [1]. Experimental studies on some regression tasks {{have shown that the}} {{new approach}} performs significantly better than previous ones [1]. However, the regression tasks considered did not include any noise. This paper presents a new algorithm for designing neural network ensembles for classification problems with noise. This new algorithm is different from that used for regression tasks although the idea is similar. The idea behind this new algorithm is to encourage different individual networks in an <b>ensemble</b> to <b>learn</b> different parts or aspects of the training data so that the whole <b>ensemble</b> can <b>learn</b> the whole training data better. Negatively correlated networks are trained with a novel correlation penalty term in the error function to encourage such specialisation. In our algorithm, individual networks are trained simultaneously rather than sequentially. This provides an opportunity for different networks [...] ...|$|R
40|$|Motor skill {{learning}} is usually characterized by shortening of response time {{and performance of}} faster, more stereotypical movements. However, {{little is known about}} the changes in neural activity that underlie these behavioral changes. Here we used chronically implanted electrode arrays to record neuronal activity in the rat primary motor cortex (MI) as animals learned to execute movements in two directions. Strong modulation of MI single-neuron activity was observed while movement duration of the animal decreased. Despite many learning-induced changes, the precision with which single neurons fire did not improve with learning. Hence, prediction of movement direction from single neurons was bounded. In contrast, prediction of movement direction using neuronal ensembles improved significantly with learning, suggesting that, with practice, neuronal <b>ensembles</b> <b>learn</b> to overcome the uncertainty introduced by single-neuron stochastic activity. Key words: extracellular; motor cortex; movement; motion; motor activity; chronic recording; skill learning; neural variabilit...|$|R
5000|$|Yoav Freund (יואב פרוינד) is an Israeli {{professor}} of computer {{science at the}} University of California San Diego who mainly works on machine learning, probability theory and related fields and applications. He {{is best known for}} his work on the AdaBoost algorithm, an <b>ensemble</b> <b>learning</b> algorithm which is used to combine many [...] "weak" [...] learning machines to create a more robust one. He received the Gödel prize in 2003 for his work on AdaBoost with Robert Schapire.|$|E
50|$|Supervised {{methods are}} based on the {{assumption}} that the context can provide enough evidence on its own to disambiguate words (hence, common sense and reasoning are deemed unnecessary). Probably every machine learning algorithm going has been applied to WSD, including associated techniques such as feature selection, parameter optimization, and <b>ensemble</b> <b>learning.</b> Support Vector Machines and memory-based learning have been shown to be the most successful approaches, to date, probably because they can cope with the high-dimensionality of the feature space. However, these supervised methods are subject to a new knowledge acquisition bottleneck since they rely on substantial amounts of manually sense-tagged corpora for training, which are laborious and expensive to create.|$|E
50|$|Critics {{regard the}} {{procedure}} as a paradigmatic example of data dredging, intense computation often being an inadequate substitute for subject area expertise. Additionally, {{the results of}} stepwise regression are often used incorrectly without adjusting them for the occurrence of model selection. Especially the practice of fitting the final selected model as if no model selection had taken place and reporting of estimates and confidence intervals as if least-squares theory were valid for them, {{has been described as}} a scandal. Widespread incorrect usage and the availability of alternatives such as <b>ensemble</b> <b>learning,</b> leaving all variables in the model, or using expert judgement to identify relevant variables have led to calls to totally avoid stepwise model selection.|$|E
30|$|The genetic {{framework}} is stable, performs {{better than a}} random guessing approach, and {{is as good as}} an exhaustive framework. Our results confirm previous ones in the field, simple regression techniques with transformations could perform as well as nonlinear techniques, and <b>ensembles</b> of <b>learning</b> machines techniques such as SMO, M 5 P or M 5 R could optimize effort predictions.|$|R
40|$|<b>Ensemble</b> <b>learned</b> {{classification}} {{systems are}} often more accurate than single classifiers. This project investigates the improvements {{on the performance of}} neural networks by using ensembles of networks as a committee machine for a four-class classification problem. The task of the classifiers is to find the orientation a face is looking. Simple Committees of 4 -output neural networks, ensemble classifiers using error-correcting output codes (ECOC) and larger committee machines of ECOC ensembles that vote on each output bit are investigated, as well as classifiers created by Adaboost. ECC, a boosting algorithm that combines the advantages of ECOC and Adaboost. The effects of changing the number of input features, the number of hidden neurons and the number of training epochs are studied. It is found that by using committees of several error-correcting output coded ensemble classifiers with not too many input features (110 or 56) and enough hidden neurons (10) the generalization accuracy can be improved significantly up t...|$|R
40|$|This paper {{proposes a}} co-evolutionary {{learning}} system, i. e., CELS, to design neural network (NN) ensembles. CELS addresses {{the issue of}} automatic determination {{of the number of}} individual NNs in an ensemble and the exploitation of the interaction between individual NN design and combination. The idea of CELS is to encourage different individual NNs in the <b>ensemble</b> to <b>learn</b> different parts or aspects of the training data so that the <b>ensemble</b> can <b>learn</b> the whole training data better. The cooperation and specialisation among different individual NNs are considered during the individual NN design. This provides an opportunity for different NNs to interact with each other and to specialise. Experiments on two real-world problems demonstrate that CELS can produce NN ensembles with good generalisation ability. 1 Introduction Many real-world problems are too large and too complex for a single monolithic system to solve alone. There are many examples from both natural and artificial systems [...] ...|$|R
