0|273|Public
40|$|In {{an effort}} {{to provide a more}} {{efficient}} representation of the <b>acoustical</b> <b>speech</b> signal in the pre-classification stage of a speech recognition system, we consider the application of the Best-Basis Algorithm of Coifman and Wickerhauser. This combines the advantages of using a smooth, compactly-supported wavelet basis with an adaptive time-scale analysis dependent on the problem at hand...|$|R
40|$|When {{combined}} with <b>acoustical</b> <b>speech</b> information, visual speech information (lip movement) significantly improves Automatic Speech Recognition (ASR) in acoustically noisy environments. Previous {{research has demonstrated}} that visual modality is a viable tool for identifying speech. However, the visual information has yet to become utilized in mainstream ASR systems due to the difficulty in accurately tracking lips in real-world conditions. This paper presents our current progress in addressing this issue. We derive several algorithms based on a modified HSI color space to successfully locate the face, eyes, and lips. These algorithms are then tested over imagery collected in visually challenging environments...|$|R
40|$|When {{combined}} with <b>acoustical</b> <b>speech</b> information, visual speech information (lip movement) significantly improves Automatic Speech Recognition (ASR) in acoustically noisy environments. Previous {{research has demonstrated}} that visual modality is a viable tool for identifying speech. However, the visual information has yet to become utilized in mainstream ASR systems due to the difficulty in accurately tracking lips in real-world conditions. This paper presents our current progress in tracking face and lips in visually challenging environments. Findings suggest the mean shift algorithm performs poorly for small regions, in this case the lips, but it achieves near 80 % accuracy for facial tracking...|$|R
40|$|The {{method of}} asynchronously {{sampling}} speech {{is based upon}} the derivatives of the <b>acoustical</b> <b>speech</b> signal. The following results are apparent from experiments to date: (1) It is possible to represent speech by a string of pulses of uniform amplitude, where the only information contained in the string is the spacing of the pulses in time; (2) the string of pulses may be produced in a simple analog manner; (3) the first derivative of the original speech waveform is the most important for the encoding process; (4) the resulting pulse train can be utilized to control an acoustical signal production system to regenerate the intelligence of the original speech...|$|R
40|$|Speech pitch {{detection}} {{remains a}} fundamental problem due to importance in numerous aspects of speech processing. Current pitch detectors focus on determining the Glottal Closure Instant (GCI). Accurate GCI measures {{can be obtained}} from the Differentiated Electroglottograph (DEGG) signal. Unfortunately, DEGG signals are not available in most practical applications. A novel method of pitch detection is proposed here based on the nonlinear estimation of DEGG signals from the acoustic speech waveform. This method requires the DEGG signals only during optimization. In operation, the proposed pitch detector marks glottal closures based strictly on the <b>acoustical</b> <b>speech</b> waveform. In addition to the algorithm development, performance comparison results are presented...|$|R
40|$|A noise {{estimation}} {{algorithm is}} proposed for highly nonstationary noise environments. The noise estimate is updated by averaging the noisy <b>speech</b> <b>power</b> spectrum using a time and frequency dependent smoothing factor, which is adjusted based on signal presence probability in subbands. Signal presence is determined by computing {{the ratio of the}} noisy <b>speech</b> <b>power</b> spectrum to its local minimum, which is computed by averaging past values of the noisy <b>speech</b> <b>power</b> spectra with a look-ahead factor. The local minimum estimation algorithm adapts very quickly to highly non-stationary noise environments. This was confirmed with formal listening tests that indicated that our noise estimation algorithm when integrated in speech enhancement was preferred over other noise estimation algorithms. 1...|$|R
40|$|Recently, it {{has been}} shown that MMSE-based noise power estima-tion [1] results in an {{improved}} noise tracking performance with re-spect to minimum statistics-based approaches. The MMSE-based approach employs two estimates of the <b>speech</b> <b>power</b> to estimate the unbiased noise power. In this work, we improve the MMSE-based noise power estimator by employing a more advanced estimator of the <b>speech</b> <b>power</b> based on temporal cepstrum smoothing (TCS). TCS can exploit knowledge about the speech spectral structure. As a result, only one <b>speech</b> <b>power</b> estimate is needed for MMSE-based noise power estimation. Moreover, the presented estimator results in an improved noise tracking performance, especially in babble noise, where SNR improvements of 1 dB over the original MMSE-based approach can be observed. Index Terms — Noise <b>power</b> estimation, <b>speech</b> enhancement. 1...|$|R
5000|$|The Cato Institute is {{concerned}} that most proposed responses to Citizens United will give [...] "Congress unchecked new power over spending on political <b>speech,</b> <b>power</b> that will be certainly abused." ...|$|R
50|$|The Player is set of APIs (e.g. position2d, bumper, ir, <b>speech,</b> <b>power)</b> {{that can}} be {{implemented}} by a robot chassis (Roomba, Khephera etc.), possibly over serial line or network, or by Stage (2D simulator) or Gazebo (3D simulator).|$|R
50|$|Gorfs {{most notable}} feature is its robotic {{synthesised}} <b>speech,</b> <b>powered</b> by the Votrax speech chip. One {{of the first}} games to allow the player to buy additional lives before starting the game, Gorf allows the player to insert extra coins to buy up to seven starting lives.|$|R
5000|$|The [...] "Political Recoreda" [...] in 1987 was {{conducted}} in Iloilo City to drum up support for the Draft Constitution {{and at the same}} time the members distributed copies of the constitution and primers. <b>Speech</b> <b>power</b> of SABAKA members were heard when they spoke in rallies and symposia.|$|R
40|$|Enhancement {{algorithms}} {{are widely}} used to overcome the degra-dation of noisy speech signals. Most enhancement algorithms re-quire {{an estimate of the}} noise and noisy <b>speech</b> <b>power</b> spectra in order to compute the gain function used for the noise suppression. The variance of these power spectral estimates degrades the qual-ity of the enhanced signal and smoothing techniques are therefore often used to decrease the variance. In this paper we present a method to determine the noisy <b>speech</b> <b>power</b> spectrum based on an adaptive time segmentation. More specifically, the proposed al-gorithm determines for each noisy frame which of the surrounding frames should contribute to the corresponding noisy power spec-tral estimate. Objective and subjective experiments show that an adaptive time segmentation leads to significant performance im-provements, particularly in transitional speech regions. 1...|$|R
40|$|Abstract — A new {{bio-inspired}} speech analysis {{system that}} extracts <b>acoustical</b> <b>speech</b> events is proposed {{and used in}} the design of a variable frame rate (VFR) speech recognizer. The same speech recognizer (Hidden Markov Model-HMM-and Mel Frequency Cepstrum Coefficients-MFCC-) has been used with the proposed VFR analysis and conventional fixed frame rate (FFR) approach. In comparison with other VFR recognizers, the hierarchical features in the proposed system have the potential to serve as classification parameters of a complete bio-inspired speech recognition system. Also, no voice activity detection is required and there are no hard decisions to be taken by the system. Events are used to label and identify the moments at which the <b>acoustical</b> properties of <b>speech</b> are stable or changing. These events are markers on which an analysis window can be positioned to perform the recognition. Inspired by our knowledge of the auditory and visual systems, hierarchical complex features like transients and energy orientation are used. Training has been done on clean speech and recognition on noisy (from 20 dB to- 10 dB Signal to Noise Ratios –SNR) or reverberated speech by using the TI 46 -word database corrupted with 4 noises taken from the Aurora 2 data. In comparison with a FFR recognizer, our VFR system yields more than 50 % increase in recognition rates for a speaker independent isolated word recognition task when SNRs are between 0 and 20 dB. I...|$|R
40|$|Abstract: This paper {{deals with}} the problem of {{checking}} the consistency of the speech corpus during recording in terms of the level of <b>speech</b> <b>power</b> of individual recordings. The question was whether or not setting of the limits of RMS value is useful for checking the volume consistency of recordings destinated for unit selection speech synthesis...|$|R
50|$|On {{the day of}} his 18th birthday, Billy {{attended}} a <b>Speech</b> <b>Power</b> Toastmasters course, and registered his first finance business called the Australian Credit Network. He pursued a career in finance for approximately 15 years, working as a broker, financial planner and corporate banker. In 2011, he was awarded Corporate Solutions Executive of the Year by The Commonwealth Bank.|$|R
40|$|Most {{approaches}} {{to the problem of}} source separation use the assumption of statistical independence. To capture statistical independence higher order statistics are required. In this chapter we will demonstrate how higher order criteria, such as maximum kurtosis, arise naturally from the property of non-stationarity. We will also show that source sepa-ration of non-stationary signals can be based entirely on second order statistics of the signals. Natural signals, be it images or time sequences, are for the most part non-stationary. For natural signals therefore we argue that non-stationarity is the fundamental property, from which speci c second or higher order separation criteria can be derived. We contrast the linear bases obtained using second order non-stationarity and ICA for the cases of natural images and <b>speech</b> <b>powers.</b> Based on these results we argue that <b>speech</b> <b>powers</b> can in fact be understood as a linear superposition of non-stationary spectro-temporal independen...|$|R
30|$|The MS [13] {{approach}} {{has been shown to}} be a reliable estimator of the noise PSD for moderately time-varying noise conditions. This approach relies on the assumption that the minimum of the noisy <b>speech</b> <b>power,</b> P_x̃(k,ℓ), over a short temporal sliding window is not affected by the speech. The noise PSD σ _ṽ^ 2 (k,ℓ) is then estimated by tracking the minimum of P_x̃(k,ℓ) over this sliding window, whose usual length corresponds to 1.5 s according to [13].|$|R
40|$|Abstract—Single-channel {{enhancement}} algorithms {{are widely}} used to overcome the degradation of noisy speech signals. Speech enhancement gain functions are typically computed from two quantities, namely, {{an estimate of the}} noise power spectrum and of the noisy <b>speech</b> <b>power</b> spectrum. The variance of these power spectral estimates degrades the quality of the enhanced signal and smoothing techniques are, therefore, often used to decrease the variance. In this paper, we present a method to determine the noisy <b>speech</b> <b>power</b> spectrum based on an adaptive time segmentation. More specifically, the proposed algorithm determines for each noisy frame which of the surrounding frames should contribute to the corresponding noisy power spectral estimate. Further, we demonstrate the potential of our adaptive segmentation in both maximum likelihood and decision direction-based speech enhancement methods by making a better estimate of the a priori signal-to-noise ratio (SNR). Objective and subjective experi-ments show that an adaptive time segmentation leads to significant performance improvements in comparison to the conventionally used fixed segmentations, particularly in transitional regions, where we observe local SNR improvements in the order of 5 dB. Index Terms—Adaptive time segmentation, a priori signal-to-noise ratio (SNR), decision directed approach, hypothesis test, speech enhancement. I...|$|R
30|$|The {{distances}} from different microphone positions {{to the mouth}} are 20 – 27 cm (Pos. 1), 28 cm (Pos. 2 / 3), and 58 cm (Pos. 4). All microphones are calibrated {{to have the same}} <b>speech</b> <b>power</b> at standstill. This comparison shows that at higher frequencies, the behavior of all microphones is almost similar, whereas at low and medium frequencies, the belt microphone outperforms conventional hands-free microphones. An improvement of up to 6 – 10 dB in SNR can be achieved.|$|R
50|$|It is {{a version}} sold by Orbit Research, {{designed}} {{for people with}} disabilities. It includes <b>speech</b> features. <b>Power</b> source comes from 9V battery instead of solar panel.|$|R
40|$|A {{new method}} is {{proposed}} for solving the glottal inverse filtering (GIF) problem. The goal of GIF is to separate an <b>acoustical</b> <b>speech</b> signal into two parts: the glottal airflow excitation and the vocal tract filter. To recover such information {{one has to}} deal with a blind deconvolution problem. This ill-posed inverse problem is solved under a deterministic setting, considering unknowns {{on both sides of the}} underlying operator equation. A stable reconstruction is obtained using a double regularization strategy, alternating between fixing either the glottal source signal or the vocal tract filter. This enables not only splitting the nonlinear and nonconvex problem into two linear and convex problems, but also allows the use of the best parameters and constraints to recover each variable at a time. This new technique, called alternating minimization glottal inverse filtering (AM-GIF), is compared with two other approaches: Markov chain Monte Carlo glottal inverse filtering (MCMC-GIF), and iterative adaptive inverse filtering (IAIF), using synthetic speech signals. The recent MCMC-GIF has good reconstruction quality but high computational cost. The state-of-the-art IAIF method is computationally fast but its accuracy deteriorates, particularly for speech signals of high fundamental frequency (F 0). The results show the competitive performance of the new method: With high F 0, the reconstruction quality is better than that of IAIF and close to MCMC-GIF while reducing the computational complexity by two orders of magnitude...|$|R
30|$|This paper {{seeks to}} improve CSP {{analysis}} in noisy environments {{with a special}} weighting algorithm. We assume the target sound source is a human speaker and the noise is broadband noise such as a fan, wind, or road noise in an automobile. Denda et al. proposed weighted CSP analysis using average speech spectrums as weights [7]. The assumption is that a subband with more <b>speech</b> <b>power</b> conveys more reliable information for localization. However, it {{did not use the}} harmonic structures of human speech. Because the harmonic bins must contain more <b>speech</b> <b>power</b> than the other bins, they should give us more reliable information in noisy environments. The use of harmonic structures for localization has been investigated in prior art [8, 9], but not for CSP analysis. This work estimated the pitches (F 0) of the target sound and extracted localization cues from the harmonic structures based on those pitches. However, the pitch estimation and the associated voiced-unvoiced classification may be insufficiently accurate in noisy environments. Also, {{it should be noted that}} not all harmonic bins have distinct harmonic structures. Some bins may not be in the speech formants and be dominated by noise. Therefore, we want a special weighting algorithm that puts larger weights on the bins where the harmonic structures are distinct, without requiring explicit pitch detection and voiced-unvoiced classification.|$|R
40|$|In {{this paper}} we propose a new {{framework}} for utilizing frequency information from the short-term <b>power</b> spectrum of <b>speech.</b> Feature extraction is based on the cepstral coefficients derived from the histograms of subband spectral centroids (SSC). Two new feature extraction algorithms are proposed, one based on frequency information alone, and the other which efficiently combines the frequency and amplitude information from the <b>speech</b> <b>power</b> spectrum. Experimental study on an automatic speech recognition task has shown that the proposed methods outperform the conventional speech front-ends in presence of additive white noise, while they perform comparably in the noise-free conditions. 1...|$|R
40|$|In this paper, {{a speech}} signal {{recovery}} algorithm is presented for a personalized voice command automatic recognition system in vehicle and restaurant environments. This novel algorithm {{is able to}} separate a mixed speech source from multiple speakers, detect presence/absence of speakers by tracking the higher magnitude portion of <b>speech</b> <b>power</b> spectrum and adaptively suppress noises. An automatic speech recognition (ASR) process {{to deal with the}} multi-speaker task is designed and implemented. Evaluation tests have been carried out by using the speech da-tabase NOIZEUS and the experimental results show that the proposed algorithm achieves impres-sive performance improvements...|$|R
40|$|A {{practical}} {{speech enhancement}} system {{consists of two}} major components, the estimation of noise power spectrum, and the estimation of speech. In single channel speech enhancement systems, most algorithms require an estimation of average noise spectrum since a secondary channel is not available. This requires a reliable speech/silence detector. Thus the speech/silence detection can be a determining factor {{for the performance of}} the whole speech enhancement system. The speech/silence detection finds out the frames of the noisy speech that contain only noise. If the speech/silence detection is not accurate then speech echoes and residual noise tend to be present in the enhanced speech. The performance of noise estimation algorithm is usually a tradeoff between speech distortion and noise reduction. In existing methods, noise is estimated only during speech pauses and these pauses are identified using Voice Activity Detector (VAD). This paper describes novel noise estimation method to estimate noise in non-stationary environments. This approach uses an algorithm that classifies noisy speech signal into pure speech, quasi speech and non-speech frames based on adaptive thresholds without using of VAD. Speech presence is determined by computing the ratio of the noisy <b>speech</b> <b>power</b> spectrum to its local minimum, which is computed by averaging past values of the noisy <b>speech</b> <b>power</b> spectra with a look-ahead factor. To evaluate proposed method performance, segmental SNR as evaluation criteria and compared with weighted average noise estimation method. The simulation results of the proposed algorithm shows better performance than conventional methods...|$|R
40|$|In this paper, {{we propose}} a perceptually-motivated method for modifying the <b>speech</b> <b>power</b> {{spectrum}} {{to obtain a}} set of linear prediction coding (LPC) parameters that possess good noiserobustness properties in network speech recognition. Speech recognition experiments were performed to compare the accuracy obtained from MFCC features extracted from AMR-coded speech that use these modified LPC parameters, {{as well as from}} LPCCs extracted from AMR bitstream parameters. The results show that when using the proposed LP analysis method, the recognition performance was on average 1. 2 % - 6. 1 % better than when using the conventional LP method, depending on the recognition task. Griffith Sciences, Griffith School of EngineeringFull Tex...|$|R
40|$|This {{paper is}} to {{investigate}} the effectiveness of Prontest software to improve English pronunciation and proficiency for Japanese EFL learners. Several parameters such as <b>speech</b> duration, <b>speech</b> <b>power,</b> F 0 (pitch), the ratio of vowel and consonant length and power were introduced {{to find out how}} much students made progress in English pronunciation and overall English proficiency. The study concluded that the average score of CASEC computer test improved from 532 (SD 109. 2) in April to 583 (SD 83. 1) in July after having used this software for six lessons. The differences of parameters between pre and post-recorded readings indicated that this software helped students to improve English pronunciation. 1...|$|R
40|$|There {{has been}} an {{increase}} in use of noninvasive positive-pressure ventilators (NPPV) to provide breathing assistance to people who are limited in their ability to breathe on their own as a result of neuromuscular impairment. To date, essentially nothing is known about how NPPV inspirations are used to <b>power</b> <b>speech,</b> beginning with whether or not individuals actually use their NPPV device for the purposes of speech. This project aimed to quantify inspirations that <b>power</b> <b>speech</b> in users of NPPV, and the amount of speech that followed NPPV <b>powered</b> <b>speech.</b> While participants claimed that NPPV helped them speak, NPPV was found to power only 37...|$|R
40|$|The article {{explores the}} concept of {{censorship}} viewed as an integral attribute of any society. The authors describe censorship as a “social blindfold” intended to eliminate the implications triggered by the information warfare. Analyzing the modern regime of restrictions and constraints, the authors explore such relevant concepts as freedom of <b>speech,</b> <b>power,</b> mass media, stereotypes and manipulative technologies shaping an illusionary reality for the people. Censorship {{is described as a}} factor of information warfare which aims to filter the information through manipulation of individual and mass consciousness. Summing up the results of the study, the authors define the status and goals of censorship in modern society...|$|R
40|$|A {{reliable}} speech presence probability (SPP) estimator {{is important}} to many frequency domain speech enhancement algorithms. It is known that a good estimate of SPP {{can be obtained by}} having a smooth a-posteriori signal to noise ratio (SNR) function, which can be achieved by reducing the noise variance when estimating the <b>speech</b> <b>power</b> spectrum. Recently, the wavelet denoising with multitaper spectrum (MTS) estimation technique was suggested for such purpose. However, traditional approaches directly make use of the wavelet shrinkage denoiser which has not been fully optimized for denoising the MTS of noisy speech signals. In this paper, we firstly propose a two-stage wavelet denoising algorithm for estimating the <b>speech</b> <b>power</b> spectrum. First, we apply the wavelet transform to the periodogram of a noisy speech signal. Using the resulting wavelet coefficients, an oracle is developed to indicate the approximate locations of the noise floor in the periodogram. Second, we make use of the oracle developed in stage 1 to selectively remove the wavelet coefficients of the noise floor in the log MTS of the noisy speech. The wavelet coefficients that remained are then used to reconstruct a denoised MTS and in turn generate a smooth a-posteriori SNR function. To adapt to the enhanced a-posteriori SNR function, we further propose a new method to estimate the generalized likelihood ratio (GLR), which is an essential parameter for SPP estimation. Simulation results show that the new SPP estimator outperforms the traditional approaches and enables an improvement in both the quality and intelligibility of the enhanced speeches. Department of Electronic and Information Engineerin...|$|R
5000|$|... 1961. The Future of Catholic <b>Power</b> <b>Speech</b> to DAR, Am. United Sep. C & S ...|$|R
40|$|We propose and {{describe}} several methods for using <b>speech</b> <b>power</b> as {{an estimate of}} intentional loudness, and a mapping from this loudness estimate to a continuous control. This is performed {{in the context of}} a novel voice-based human-computer interface designed to enable individuals with motor impairments to use vocal tract parameters for both discrete and continuous control tasks. The interface uses vocal gestures to control continuous movement and discrete sounds for other events. We conduct a user preference survey to gauge user reaction to the various methods in a mouse cursor control context. We find that loudness is an effective mechanism to control mouse cursor movement speed when mapping vocalic gestures to spatial position. 1...|$|R
40|$|Abstract. In this paper, a noise {{estimator}} with rapid adaptation in a variable-level noisy environment is presented. To make noise estimation adapt quickly to highly non-stationary noise environments, a robust voice activity detector (VAD) is utilized {{in this paper}} and {{it depends on the}} variation of the spectral energy not on the amount of that. The noise power spectrum in subbands are estimated by averaging past spectral power values using a time and frequency dependent smoothing parameter, which is chosen as a sigmoid function changing with speech-present probability in subbands. The speech-present probability is determined by computing the ratio of the noisy <b>speech</b> <b>power</b> spectrum to its local minimum. Noise measurement, speech enhancement, spectral analysis, signal process. ...|$|R
30|$|Breathiness in {{laughing}} speech sounds {{different from}} the other items. One difference is that in laughing <b>speech,</b> the <b>power</b> of the voiced components also changes rhythmically, besides the breathy (aspirated) components, sounding like an alternation of the vowel sounds and the aspirated /h/.|$|R
40|$|This article {{reveals how}} {{the usage of}} {{particles}} influences the process of defining <b>speech</b> <b>power</b> in the sentences. Particles being illocutive indicators make intentions, conditions and emotions visible and noticiable. The outcome of this research gave the possibility {{to make sure that}} the means of clearing out the <b>speech</b> <b>power</b> have the important role in illocutive act: influences the perception of text content. The views of linguists on the problem of the role of particles in the sentence are presented. The meanings of such concepts as illocution, illocutive act, illocutive power are clarified. The differences between statements and other kinds of the texts of the diplomatic correspondence, the subject of the statement and the reasons that provoked its writing are defined. The particle “only” can be found six times in the text, it is used to highlight actions, signs. The particle “even” is used to highlight or enhance a particular word or phrase. In the text under consideration, it is marked eight times. The most often used particle in the text is “merely”. It is used with the restrictive excretory shade in the meaning: just, only at that time, only in such a situation, the only way. We can summarize that the contents of the certain information was allocated, reinforced with the help of illocutive indicators – particles. They provoke the reader to feel certain emotions that the author of the text aimed to cause. We observe how the direction of attention of mass consciousness on a specific problem occurs. In the future we plan to consider the use of particle “would” as a means of clarifying the strength of the statements in illocutive act...|$|R
40|$|An EM-type of {{recursive}} estimation {{algorithm is}} formulated in the DFT domain for joint estimation of time-varying parameters of distortion channel and additive noise from online degraded speech. Speech features are estimated from the posterior estimates of short-time <b>speech</b> <b>power</b> spectra in an on-the-fly fashion. Experiments {{were performed on}} speaker-independent continuous speech recognition using features of perceptually based linear prediction cepstral coefficients, log energy, and temporal regression coefficients. Speech data {{were taken from the}} TIMIT database and were degraded by simulated time-varying channel and noise. Experimental results showed significant improvement in recognition word accuracy due to the proposed recursive estimation as compared with the results from direct recognition using a baseline system and from performing speech feature estimation using a batch EM algorithm. 1...|$|R
