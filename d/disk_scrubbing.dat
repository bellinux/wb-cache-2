10|0|Public
40|$|A {{number of}} {{techniques}} {{have been proposed}} {{to reduce the risk}} of data loss in hard-drives, from redundant disks (e. g., RAID systems) to error coding within individual drives. <b>Disk</b> <b>scrubbing</b> is a background process that reads disks during idle periods to detect irremediable read errors in infrequently accessed sectors. Timely detection of such latent sector errors (LSEs) is important to reduce data loss. In this paper, we take a clean-slate look at <b>disk</b> <b>scrubbing.</b> We present the first formal definition in the literature of a scrubbing algorithm, and translate recent empirical results on LSE distributions into new scrubbing principles. We introduce a new simulation model for LSE incidence in disks that allows us to optimize our proposed scrubbing techniques and demonstrate the significant benefits of intelligent scrubbing to drive reliability. We show how optimal scrubbing strategies depend on disk characteristics (e. g., the BER rate), as well as disk workloads. ...|$|E
40|$|Two schemes {{proposed}} {{to cope with}} unrecoverable or latent media errors and enhance the reliability of RAID systems are examined. The first scheme is the established, widely used <b>disk</b> <b>scrubbing</b> scheme, which operates by periodically accessing disk drives to detect media-related unrecoverable errors. These errors are subsequently corrected by rebuilding the sectors affected. The second scheme is the recently proposed intradisk redundancy scheme which uses a further level of redundancy inside each disk, {{in addition to the}} RAID redundancy across multiple disks. Analytic results are obtained assuming Poisson arrivals of random I/O requests. Our results demonstrate that the reliability improvement due to <b>disk</b> <b>scrubbing</b> depends on the scrubbing frequency and the workload of the system, and may not reach the reliability level achieved by a simple IPC-based intra-disk redundancy scheme, which is insensitive to the workload. In fact, the IPC-based intra-disk redundancy scheme achieves essentially the same reliability as that of a system operating without unrecoverable sector errors. For heavy workloads, the reliability achieved by the scrubbing scheme can be orders of magnitude less than that of the intra-disk redundancy scheme...|$|E
40|$|Highly {{available}} storage uses replication {{and other}} redundant storage {{to recover from}} a component failure. If parity data calculated from an erasure correcting code is not updated or becomes otherwise corrupted, recovery from a failure does not recover the correct data but mostly garbled data. This paper presents an algebraic signature scheme that can detect parity discrepancies for parity calculated with XORing, generalized Reed-Solomon codes, or convolutional array codes. Maintaining and checking the signature of client and parity data allows us to ensure coherence in the storage system and thus to accurately rebuild data on lost devices. Our scheme is combined with <b>disk</b> <b>scrubbing,</b> necessary to detect masked disk failures...|$|E
40|$|The {{increasing}} use {{of computers}} for saving valuable data imposes stringent reliability constraints on storage systems. Reliability improvement via use of redundancy is a common practice. As the disk capacity improves, advanced techniques such as <b>disk</b> <b>scrubbing</b> are being employed to proactively fix latent sector errors. These techniques utilize the disk idle time for reliability improvement. However, the idle time {{is a key to}} dynamic energy management that detects such idle periods and turns-off the disks to save energy. In this paper, we are concerned with the distribution of the disk idle periods between reliability and energy management tasks. For this purpose, we define a new metric, energy-reliability product (ERP), to capture the effect of one technique on the other. Ou...|$|E
40|$|Latent Sector Error (LSE) is a {{well-known}} problem in HDD-based storage systems. LSEs, which occur silently, may result in data loss during RAID recovery from disk failure. LSEs in HDDs are caused by various reasons such as write errors or media imperfections [1], which result in bit/symbol errors that cannot be corrected with ECC. <b>Disk</b> <b>scrubbing</b> is used to detect LSEs by scrubbing the disk in the background. As pointed out in [2], the scrubbing strategy optimization requires a good model of LSE development, i. e., when and where LSE would likely happen. Inspired by previous work [1] [2], we are investigating the problem of modeling and detecting LSEs of NAND flash-based SSDs. Due to increased density, NAND flash memory {{is becoming more and}} more prone to bit errors [3], whic...|$|E
40|$|User I/O {{intensity}} {{can significantly}} impact {{the performance of}} on-line RAID reconstruction due to contention for the shared disk bandwidth. Based on this observation, this paper proposes a novel scheme, called WorkOut (I/O Workload Outsourcing), to significantly boost RAID reconstruction performance. WorkOut effectively outsources all write requests and popular read requests originally targeted at the degraded RAID set to a surrogate RAID set during reconstruction. Our lightweight prototype implementation of WorkOut and extensive tracedriven and benchmark-driven experiments demonstrate that, compared with existing reconstruction approaches, WorkOut significantly speeds up both the total reconstruction time and the average user response time. Importantly, WorkOut is orthogonal to and can be easily incorporated into any existing reconstruction algorithms. Furthermore, it can be extended to improving the performance of other background support RAID tasks, such as re-synchronization and <b>disk</b> <b>scrubbing.</b> ...|$|E
40|$|Abstract—Disk {{scrubbing}} periodically {{scans the}} contents of a disk array to detect the presence of irrecoverable read errors and reconstitute {{the contents of}} the lost blocks using the builtin redundancy of the disk array. We address the issue of scheduling scrubbing runs in disk arrays that can tolerate two disk failures without incurring a data loss, and propose to start an urgent scrubbing run of the whole array whenever a disk failure is detected. Used alone or in combination with periodic scrubbing runs, these expedited runs can improve the mean time to data loss of disk arrays over a wide range of disk repair times. As a result, our technique eliminates the need for frequent scrubbing runs and the need to maintain spare disks and personnel on site to replace failed disks within a twentyfour hour interval. Keywords-irrecoverable read errors; RAID arrays; <b>disk</b> <b>scrubbing.</b> I...|$|E
40|$|Large {{archival storage}} systems {{experience}} {{long periods of}} idleness broken up by rare data accesses. In such systems, disks may remain powered off {{for long periods of}} time. These systems can lose data for a variety of reasons, including failures at both the device level and the block level. To deal with these failures, we must detect them early {{enough to be able to}} use the redundancy built into the storage system. We propose a process called “disk scrubbing” in a system in which drives are periodically accessed to detect drive failure. By scrubbing all of the data stored on all of the disks, we can detect block failures and compensate for them by rebuilding the affected blocks. Our research shows how the scheduling of <b>disk</b> <b>scrubbing</b> affects overall system reliability, and that “opportunistic ” scrubbing, in which the system scrubs disks only when they are powered on for other reasons, performs very well without the need to power on disks solely to check them. 1...|$|E
40|$|Because of {{high demand}} that {{applications}} and new technologies have today for data storage capacity, more disk drives are needed, resulting in increased probability to inaccessible sectors, referred as Latent Sector Errors (LSE). Aiming to reduce data loss by LSE, two main techniques are extensively studied lately: <b>Disk</b> <b>Scrubbing,</b> which performs reading operations during idle periods on systems {{to search for}} errors and Intra Disk Redundancy {{which is based on}} redundancy codes. This paper reviews and discusses the problems of LSE and the main causes that lead to LSE, its properties and their correlation on nearline and enterprise disks. Focusing on reducing LSE with regards to security, processing overhead and disk space, we analyze and compare the latest techniques: Disc Scrubbing and Intra Disk Redundancy aiming to highlight the issues and challenges according to different statistical approaches. Furthermore, based on previous evaluation results, we discuss and introduce the benefits on using both schemes simultaneously: combining different IDR coding schemes with Accelerated Scrubbing and Staggered Scrubbing in particular regions of disc drives that store crucial data during idle periods. Finally, we discuss and evaluate from an extended statistical analysis the best ways on how reduce data loss with a minimum impact on system performance...|$|E
40|$|One of {{characteristics}} of the rising cloud storage technology is low-cost and high reliability. A distinct benefit of disk scanning or scrubbing operation is identifying the potential failure sectors as early as possible, thus providing high reliability. Obviously, the higher the scrubbing frequency is, the higher the system reliability is. However, {{it may take a}} few hours for a scanning process to check the whole disk. In other words, the scrubbing process may result in a downtime or a lower system performance. Furthermore, the scrubbing process consumes energy. In order to reduce the impact of <b>disk</b> <b>scrubbing</b> on disk performance and energy consumption, system designers choose to scan the disk in a low frequency, which results in a lower reliability. Thus it is essential to design a good scrubbing scheme in a large scale storage system over long time horizons. In this paper, we present a novel scrubbing scheme to solve the challenge. In this scheme, an optimum scrubbing cycle is decided by keeping a balance between data loss cost, scrubbing cost, and disk failure rate. Our research shows how the data price and the scrubbing cost affect scrubbing frequency, and the scrubbing scheme is applicable for storage with inexpensive data. Our experiment shows that our scheme outperforms routine method 73. 3 % in cost and 40 % in reliability...|$|E

