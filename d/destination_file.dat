16|22|Public
25|$|When moving or copying files, Windows Explorer {{displays}} the full source and destination path, size {{and number of}} items and the transfer speed in megabytes per second (MB/s). If a conflict or error is encountered, it does not terminate the copy, move or delete operation. Rather, the file is skipped {{and the rest of}} the files processed. At the end of the operation, the errors are presented to the user with resolution options (if available). If two files have the same name, an option is available to rename the file; in previous versions of Windows, the only options were to either replace the <b>destination</b> <b>file</b> or cancel the process. The user can also choose to apply the same action to further conflicts, if any. Also, for the first time in Windows, processing of dates when moving trees is somewhat consistent. Until now, every folder and subfolder obtained all three dates new to the time of operation. This was from the own nature of the operation at the file system level, create destination folders, move files, delete origin folders. Moved folders in Vista preserve creation and modification dates. Access date gets the time of the operation.|$|E
50|$|The default {{behavior}} of the mv and cp commands is to clobber their <b>destination</b> <b>file</b> if it already exists. This behavior may be overridden by invoking or aliasing the commands with the -i switch, causing the commands to prompt the user before overwriting the <b>destination</b> <b>file.</b>|$|E
5000|$|COPY : Copies one file {{to another}} (if the <b>destination</b> <b>file</b> already exists, MS-DOS asks whether to replace it). (See also XCOPY, an {{external}} command that could also copy directory trees).|$|E
5000|$|The {{use of this}} shell-like {{variable}} expansion when naming <b>files,</b> covering multiple <b>destination</b> <b>files</b> with {{a single}} statement ...|$|R
30|$|Here, “-o /dst” asks pf to prefix “/dst” to paths {{received}} in Dir messages and “-w” asks pf {{to create the}} corresponding files instead of printing them. File data received after directory entries for regular files is just sent to the <b>destination</b> <b>files.</b> The implementation uses a put request that accepts an input channel for file data. Therefore, data is streamed to the <b>destination</b> <b>files.</b> But this requires a file server that accepts such request (the Clive ZX file system does) and {{it is out of}} scope for this paper.|$|R
50|$|Since {{the early}} days MaxiSun has been {{modified}} {{to make it more}} capable of being configured by the client. The SQR extraction tool has been replaced with a dedicated program called Origin and the MaxiSun data conversion engine gained the benefit of a user interface that allowed the source and <b>destination</b> <b>files</b> to be mapped at field and line level.|$|R
50|$|The rename {{function}} {{from the}} C library in Windows does not implement the POSIX atomic behaviour; instead it fails if the <b>destination</b> <b>file</b> already exists. However, other {{calls in the}} Windows API do implement the atomic behaviour.|$|E
50|$|Some audio {{conversion}} functions can {{be performed}} by software or by specialized hardware. For example, an audio transcoder converts from one compressed audio format to another (e.g., MP3 to AAC) by means of two audio codecs: One for decoding (uncompressing) the source and one for encoding (compressing) the <b>destination</b> <b>file</b> or stream.|$|E
5000|$|DC-5 beta began {{development}} in early 2000. As {{the result of}} a substantial number of improvements, testing did not commence until June 2002 before being released in September 2002. Previous versions of DC-Art software used a source vs <b>destination</b> <b>file</b> setup termed [...] "Classic edit mode" [...] whereby each time a filter was applied to a source file it would generate a new, altered <b>destination</b> <b>file</b> tiled within the same window. Whilst this was commendable for archival purposes, for the home user or intermediate expert it resulted in too many semi-complete restoration files. DC-5 addressed this issue by introducing [...] "Fast Edit" [...] mode and a new [...] "Fast Edit" [...] history whereby each change to the original file could be reversed. In this mode, both the source and destination files became the one file on screen, whilst information regarding changes made were saved in the background to enable them to be selectively un-done.|$|E
5000|$|Robocopy syntax is {{markedly}} different from standard copy commands, as it accepts only folder names as its source and <b>destination</b> arguments. <b>File</b> names and wild-card characters (such as [...] "*.*") are not valid source or <b>destination</b> arguments. <b>Files</b> may be selected or excluded using the optional filespec filtering argument. Filespecs can refer {{only to the}} filenames relative to the folders already selected for copying. Fully qualified path names are not supported.|$|R
5000|$|Overwrite the <b>file</b> in the <b>destination</b> {{with the}} <b>file</b> {{from the source}} ...|$|R
50|$|Source and <b>destination</b> were <b>file</b> {{specification}} strings. These {{consisted of}} a device name, typically 2 characters for device type such as DK (disk), LP (line printer), MT (magnetic tape), etc. and a unit number from 0 to 7, a colon (:), filename and extension.|$|R
50|$|The Log Service enables {{applications}} to express and write log records through log streams {{that lead to}} particular output destinations, such as a named file. Once at the output destination, a log record is subject to output formatting rules, which are configurable and public. The logging application {{does not need to}} be aware of any of these aspects (e.g. the <b>destination</b> <b>file</b> location, file rotation or formatting, etc.) as the Log Service handles them based on the current settings for the targeted log stream. Since the output format is public, third party tools can read these log files.|$|E
5000|$|With splice (...) , one {{can move}} data from one file {{descriptor}} to another without incurring any copies from user space into kernel space, {{which is usually}} required to enforce system security and also to keep a simple interface for processes {{to read and write}} to files. splice (...) works by using the pipe buffer. A pipe buffer is an in-kernel memory buffer that is opaque to the user space process. A user process can splice the contents of a source file into this pipe buffer, then splice the pipe buffer into the <b>destination</b> <b>file,</b> all without moving any data through userspace.|$|E
50|$|C:\>move /?Moves {{files and}} renames files and directories.To move {{one or more}} files:MOVE | /-Y drive:pathfilename1,... destinationTo rename a directory:MOVE | /-Y drive:pathdirname1 dirname2 drive:pathfilename1 Specifies the {{location}} and name of the file or files you want to move. destination Specifies the new location of the file. Destination can consist of a drive letter and colon, a directory name, or a combination. If you are moving only one file, you can also include a filename {{if you want to}} rename the file when you move it. drive:pathdirname1 Specifies the directory you want to rename. dirname2 Specifies the new name of the directory. /Y Suppresses prompting to confirm you want to overwrite an existing <b>destination</b> <b>file.</b> /-Y Causes prompting to confirm you want to overwrite an existing destination file.The switch /Y may be present in the COPYCMD environment variable. This may be overridden with /-Y on the command line. Default is to prompt on overwrites unless MOVE command is being executed from within a batch script.|$|E
5000|$|Service by mail is {{permitted}} by most U.S. jurisdictions for service on defendants located in other U.S. states or foreign countries. Service by mail {{is not available}} if the country of <b>destination</b> has <b>filed</b> objections to service by mail pursuant to the multinational Hague Service Convention. In California, [...] "Any person providing the California Department of Motor Vehicles with a mailing address shall … consent toreceive service of process …".|$|R
25|$|SQL Server Integration Services (SSIS) {{provides}} ETL capabilities for SQL Server {{for data}} import, data integration and data warehousing needs. Integration Services includes GUI tools to build workflows such as extracting data from various sources, querying data, transforming data—including aggregation, de-duplication, de-/normalization and merging of data—and then exporting the transformed data into <b>destination</b> databases or <b>files.</b>|$|R
40|$|It {{has been}} a year of growth for IP rights in Australia and demand grew across all four IP rights last year, in line with global trends. In 2015, patent {{applications}} grew by 10 per cent, trade marks by 14 per cent (the best growth in a decade), design applications were the highest for any single year on record and applications for plant breeder 2 ̆ 7 s rights hit a five-year peak. 	Non-residents make up 92 per cent of all patent filings. The top three non-resident filers were the US, Japan and Germany 	The top three <b>destinations</b> for Australians <b>filing</b> patents abroad were the US, the European Patent Office and New Zealand 	Applications from Australian residents made up 64 per cent of trade mark applications 	China remains the top <b>destination</b> for Australians <b>filing</b> trade mark applications abroad. This {{is in line with}} global trends 	International applicants filed 53 per cent of the IP right applications received by IP Australia in 2015 	 2015 was the highest number of design filings on record, a total of 7 024 applications were received 	PBR applications increased by five per cent; this growth was driven solely by a 14 per cent increase in applications by Australian residents...|$|R
50|$|Other {{components}} {{that are involved}} in the snapshot creation process are writers. The aim of Shadow Copy is to create consistent reliable snapshots. But sometimes, this cannot simply be achieved by completing all pending file change operations. Sometimes, it is necessary to complete a series of inter-related changes to several related files. For example, when a database application transfers a piece of data from one file to another, it needs to delete it from the source file and create it in the <b>destination</b> <b>file.</b> Hence, a snapshot must not be between the first deletion and the subsequent creation, or else it is worthless; it must either be before the deletion or after the creation. Enforcing this semantic consistency is the duty of writers. Each writer is application-specific and has 60 seconds to establish a backup-safe state before providers start snapshot creation. If the Volume Shadow Copy service does not receive acknowledgement of success from the corresponding writers with this time-frame, it fails the operation.|$|E
50|$|When moving or copying files, Windows Explorer {{displays}} the full source and destination path, size {{and number of}} items and the transfer speed in megabytes per second (MB/s). If a conflict or error is encountered, it does not terminate the copy, move or delete operation. Rather, the file is skipped {{and the rest of}} the files processed. At the end of the operation, the errors are presented to the user with resolution options (if available). If two files have the same name, an option is available to rename the file; in previous versions of Windows, the only options were to either replace the <b>destination</b> <b>file</b> or cancel the process. The user can also choose to apply the same action to further conflicts, if any. Also, for the first time in Windows, processing of dates when moving trees is somewhat consistent. Until now, every folder and subfolder obtained all three dates new to the time of operation. This was from the own nature of the operation at the file system level, create destination folders, move files, delete origin folders. Moved folders in Vista preserve creation and modification dates. Access date gets the time of the operation.|$|E
40|$|Long-term {{users of}} {{engineering}} product data are {{hampered by the}} ephemeral nature of CAD file formats and the applications that work with them. STEP, the Standard for the Exchange of Product Model Data (ISO 10303), promises to help with meeting this challenge, but is not without problems of its own. We present a complementary solution based {{on the use of}} lightweight file formats to preserve specific aspects of the product data, in conjunction with a registry of relevant representation information as defined by the Open Archival Information System Reference Model (ISO 14721). This registry is used to identify suitable <b>destination</b> <b>file</b> formats for different purposes, and provides a resource to aid in the recovery of information from these formats in the future. ...|$|E
40|$|We {{study the}} drivers of {{international}} patent applications in ICT technologies by non-residents. We construct bilateral measures of foreign patent applications for all countries active {{as both a}} source of patents and a <b>destination</b> of applications <b>filed</b> between 1990 and 2007 to any patent office in the world. Despite the global character of the ICT industry, applicants from different regions follow different patenting strategies, only Japanese and US applicants are exceptionally active in seeking for patent protection {{in the majority of}} world markets. Applying a gravity model to explain the determinants of seeking patent protection in foreign markets, we find that economic and inventive capacity of a country attracts foreign patent applications. JRC. J. 3 -Information Societ...|$|R
40|$|We study {{drivers of}} {{international}} patent applications by non-residents {{and assess the}} importance of the PCT membership in their motivations. We construct bilateral measures of foreign patent applications for all countries active as both a source of patents and a <b>destination</b> of applications <b>filed</b> between 1970 and 2009. The data used originates from the EPO Patstat database. Applying a gravity model to explain the determinants of seeking patent protection in foreign markets, we find that there is a negative relationship between PCT membership and a country's attractiveness for foreign applicants. It is mainly the size of the market and a country's inventive capacity that attract foreign applicants to seek for patent protection in foreign countries. JRC. J. 3 -Information Societ...|$|R
40|$|Abstract. We {{describe}} Freenet, {{an adaptive}} peer-to-peer network application that permits the publication, replication, and {{retrieval of data}} while protecting the anonymity of both authors and readers. Freenet operates as a network of identical nodes that collectively pool their storage space to store data files and cooperate to route requests to the most likely physical location of data. No broadcast search or centralized location index is employed. Files are referred to in a location-independent manner, and are dynamically replicated in locations near requestors and deleted from locations {{where there is no}} interest. It is infeasible to discover the true origin or <b>destination</b> of a <b>file</b> passing through the network, and difficult for a node operator to determine or be held responsible for the actual physical contents of her own node. ...|$|R
40|$|Huge {{amount of}} data are stored in the network {{security}} environment and quantities of data in these fields tend to increase year on year. For this reason efficient File matching algorithms should be used which minimizes computer storage which results in minimum response time on searching process. There are many problems involved in file content matching technique. This technique works for finding the file content similarity. In this study, we propose the new algorithm called Adaptive File Comparison Technique, which converts the source and <b>destination</b> <b>file</b> contents into tokens and finally it makes a matrix format data called sparse matrix. The comparison technique compares the sparse matrices of source and destination files to find the content similarity and dissimilarity which leads to an efficient content matching technique rather than comparing the entire file content...|$|E
40|$|Abstract—In {{this paper}} {{we look at}} the {{performance}} characteristics of three tools used to move large data sets over dedicated long distance networking infrastructure. Although performance studies of wide area networks have been a frequent topic of interest, performance analyses have tended to focus on network latency characteristics and peak throughput usingnetwork traffic generators. In this study we instead perform an end-to-end long distance networking analysis that includes reading large data sets from a source file system and committing the data to a remote <b>destination</b> <b>file</b> system. An evaluation of end-to-end data movement is also an evaluation of the system configurations employed and the tools used to move the data. For this paper, we have built several storage platforms and connected them with a high performance long distance network configuration. We use these systems to analyze the capabilities of three data movement tools: BBcp, GridFTP, and XDD. Our studies demonstrate that existingdatamovementtoolsdonotprovideefficientperformance levels or exercise the storage devices in their highest performance modes. I...|$|E
40|$|In {{this project}} {{we look at}} the {{performance}} characteristics of three tools used to move large data sets over dedicated long distance networking infrastructure. Although performance studies of wide area networks have been a frequent topic of interest, performance analyses have tended to focus on network latency characteristics and peak throughput using network traffic generators. In this study we instead perform an end-to-end long distance networking analysis that includes reading large data sets from a source file system and committing large data sets to a <b>destination</b> <b>file</b> system. An evaluation of end-to-end data movement is also an evaluation of the system configurations employed and the tools used to move the data. For this paper, we have built several storage platforms and connected them with a high performance long distance network configuration. We use these systems to analyze the capabilities of three data movement tools: BBcp, GridFTP, and XDD. Our studies demonstrate that existing data movement tools do not provide efficient performance levels or exercise the storage devices in their highest performance modes. We describe the device information required to achieve high levels of I/O performance and discuss how this data is applicable in use cases beyond data movement performance...|$|E
40|$|This report {{provides}} a collation {{of data and}} information about the intellectual property (IP) system in Australia, where Australia sits in the global IP system, and how it measures up against other countries. It {{is the first in}} a regular series of publications about the IP system.  The report highlights key developments in IP rights: China is now the top <b>destination</b> for Australians <b>filing</b> trade mark applications abroad.  Eastern states (QLD, NSW, VIC and TAS) have seen double digit growth in patent filings for 2012.  Australians file more patents in the USA than in Australia. 90 % of patent applications in Australia are from foreign applicants. Advanced economies are shifting from tangible assets to intangibles like research and development, skills and branding. Australia has not yet made this shift with primary industries still supporting our place in the global economy.  ...|$|R
40|$|We {{describe}} Freenet, a {{peer-to-peer network}} application that permits the publication, replication, and {{retrieval of data}} while protecting the anonymity of both authors and readers. Freenet operates as a network of identical nodes that collectively pool their storage space to store data files, and cooperate to route requests to the most likely physical location of data. No broadcast search or centralized location index is employed. Files are referred to in a location-independent manner, and are dynamically replicated in locations near requestors and deleted from locations {{where there is no}} interest. It is infeasible to discover the true origin or <b>destination</b> of a <b>file</b> passing through the network, and difficult for a node operator to determine or be held responsible for the actual physical contents of her own node. 1 Introduction Computer networks are rapidly growing in importance as a medium for the storage and exchange of information. However, current systems afford little pr [...] ...|$|R
40|$|Internet access {{management}} {{refers to the}} capability of controlling the accesses to selected Web sites according to a pre-defined policy. Access requests to web sites/files other than what is allowed are denied. In this project we have designed and implemented SBFilter, a fast URL filter engine used for controlling access to internet resources. Instead of using a global access control list, SBFilter supports a fine-grained access control based on the source host addresses and/or subnets and the <b>destination</b> Web sites' <b>file</b> system sub-directories. Therefore, SBFilter is particularly useful for controlling Internet accesses {{in the context of}} an Internet Service Provider's (ISP) main proxy server, where individual subscribers may choose different access control policies tailored to their own needs. The filter engine uses an intelligent two-level caching scheme to speed up URL request filtering. The access control is transparent to client machines and adds little to the proxy server's overall performance overhead. Further, SBFilter's performance can scale with an increasing number of users as it is designed to run on a cluster of workstations...|$|R
5000|$|Development of DeBabelizer Pro has now {{stopped as}} of 2008, {{but the program}} is {{currently}} available for free download without support. It will not work on versions of Windows after Windows 10, or on versions of OS X after Mac OS X Lion. The company has since replaced DeBabelizer with MediaRich HotFolder, speeding up batch processing applications 20-fold for modern Windows, Mac or Linux and enabling folder mounting on servers. MediaRich HotFolder does provide Linux, Windows 10, Windows Server 2012 and Windows Server 2016 support, and is now available in both AWS and Microsoft Azure instances. The company added MediaRich Server connected to Microsoft's SharePoint environment to enable [...] "DeBabelizer" [...] for web content delivery and batch processing of content from within Microsoft's SharePoint environments. MediaRich ECM for SharePoint has been release for SharePoint 2003, 2007, 2010, 2013, 2016 and now is available for SharePoint OnLine. The tool has become a way to both transform content, but also visualize, review and approve content from anywhere, rather than just batch processing to final <b>file</b> <b>destinations.</b>|$|R
3000|$|... mWe have {{conducted}} {{a number of}} sensitivity analyses {{for the results of}} the parameters of the countries of <b>destination</b> (see Additional <b>file</b> 1 : Table S 5). The first was the deletion one by one each of the countries of destination from model 3, the deletion of four destination countries with few cases or the deletion of three small countries. The parameters happen to remain stable and not substantially affected by the deletion of {{one or more of the}} countries of destination. The second sensitivity analysis used the difference on the Hofstede (1980) collectivism-individualism dimension between origin and destination country as an indicator of the cultural distance between migrants’ origin and destination. The addition of this difference show that migrant children, who origin from countries with a higher collectivism score relative to their destination country, have lower educational outcomes, but the addition of this cultural distance does not alter the effects of the characteristics of the educational systems. The third sensitivity analysis was the deletion of the four largest combinations of origin and destination countries. The parameters happen to remain stable and not substantially affected by the deletion of one of these combinations of origin and destination.|$|R
40|$|ODS {{layout of}} PDF output can arrange {{multiple}} tables and graphs {{on the same}} page, and even overlay tables onto graphs and vice versa. This can replace GREPLAY coding of multiple graphs with reduction of the most tedious aspects. It also provides easier and more flexible coding of certain elements of graph annotation. Matching style elements such as colors, lines, and symbols of graphs with text helps integrate the appearance for easier user interpretation. DESTINATIONS Layout works essentially only to the PDF at this time. The RTF destination by its nature does not handle tables within tables, which {{is the basis of}} LAYOUT. The HTML <b>destination</b> will write <b>files</b> when LAYOUT is active, but does not work for absolute layout. The PRINTER destination works with LAYOUT, but this paper will focus exclusively on PDF and assumes throughout that an ODS PDF statement has already started writing to that destination. This paper also assumes that the creation of the test data used in these examples is trivial and does not show the code. The data is not the point. VERSION AND PLATFORM All of the output in this paper was produced using SAS ® Version 9. 2 stage 2 on the Windows platform. BASIC LAYOUT For a simple demonstration of a graph followed by a few tables, start LAYOUT and define the REGION for the plot. Th...|$|R
40|$|Today {{people have}} {{multiple}} personal computers, personal digital assistants and smart phones. Today's advanced handheld devices have powerful processors, with a process frequency {{of up to}} 1 GHz, huge storage capacities, flash storage capacities up to 32 GB, a large (multi) touch screen, and a user-friendly user interface. Additionally, the device may have various input and output devices, thus leading to people utilizing different devices for different use cases. In order to provide the latest information to the users via any of their devices, data synchronization becomes a requirement for users. There are many data synchronization solutions for synchronizing database records and files. In the current database synchronization solutions, {{there is no clear}} source and target. For example, consider the case where a PDA synchronizes with a PC; the record could have been edited (changed) on both the PDA and PC. In this case it is not clear which should be synchronized with what should be the source for the value. In contrast, a files synchronization system has a clearly specified synchronizatiom sourse and destination structure. In this case the client Synchronizes their files with that of the server. In a version control system the client synchronize files with a repository acting as a version control server. There are many synchronizing protocols and each has been designed for different purpose. Protocols for synchronizing database records often provide continuous synchronization, leading to a lot of data being exchanging during the synchronization process, as a result the synchronization process takes a longer period of time, but maintains the semantics of the database updates (either a complete transaction completes or it must be rolled back). On the other hand, protocols for synchronizing files may require a short synchronization time, as the whole file transferred and replaces the previous version of the <b>file</b> at the <b>destination.</b> Note <b>file</b> synchronization may also transfer only the differences between the files, with a local transformation of an existing file copy of the by applying these differences as updates to the files. Sending only the updates to a file enables large files with a small number of changes to be quickly updated. However, file based updated does not efficiently support record level updates of a database. In this thesis we designed a new synchronization protocol for synchronizing two SQLite databases. This synchronization protocol borrows from (and hence offers the advantages of) a version control system in order to rapidly perform SQLite database synchronization. Moreover, this solution brings SQLite database additional functions, for example supporting multiple-user, transaction logs, and data roll-back...|$|R
40|$|ATM, SDH or {{satellite}} {{have been}} used in the last century as the contribution network of Broadcasters. However the attractive price of IP networks is changing the infrastructure of these networks in the last decade. Nowadays, IP networks are widely used, but their characteristics do not offer the level of performance required to carry high quality video under certain circumstances. Data transmission is always subject to errors on line. In the case of streaming, correction is attempted at destination, while on transfer of files, retransmissions of information are conducted and a reliable copy of the file is obtained. In the latter case, reception time is penalized because of the low priority this type of traffic on the networks usually has. While in streaming, image quality is adapted to line speed, and line errors result in a decrease of quality at <b>destination,</b> in the <b>file</b> copy the difference between coding speed vs line speed and errors in transmission are reflected in an increase of transmission time. The way news or audiovisual programs are transferred from a remote office to the production centre depends on the time window and the type of line available; in many cases, it must be done in real time (streaming), with the resulting image degradation. The main purpose of this work is the workflow optimization and the image quality maximization, for that reason a transmission model for multimedia files adapted to JPEG 2000, is described based on the combination of advantages of file transmission and those of streaming transmission, putting aside the disadvantages that these models have. The method is based on two patents and consists of the safe transfer of the headers and data considered to be vital for reproduction. Aside, the rest of the data is sent by streaming, being able to carry out recuperation operations and error concealment. Using this model, image quality is maximized according to the time window. In this paper, we will first give a briefest overview of the broadcasters requirements and the solutions with IP networks. We will then focus on a different solution for video file transfer. We will take the example of a broadcast center with mobile units (unidirectional video link) and regional headends (bidirectional link), and we will also present a video file transfer file method that satisfies the broadcaster requirements. ...|$|R
40|$|The {{principal}} question {{investigated in}} the thesis is: were changes in school attendance behaviour primarily {{the result of}} socially differentiated family strategies. The behaviour analyzed is the decision of London parents to send or not {{to send their children}} to school between 1826 and l 871.;Parents in this study were divided into three cultural groups: Protestants, Roman Catholics and Blacks. They were also classified into occupational groups which were employed as surrogate measures for social class: upper, middle, and lower. Enrollments and attendance rates provided the proxy measure of the behavioural decision. The time frame was differentiated into four periods of study: 1826 to 1842; 1843 to 1852; 1853 to 1861; and 1867 to 1871.;A family strategies approach is employed in this research because of its significant explanatory potential, and its capacity to incorporate many of the approaches used by other historians. From this perspective, the primary concern of most parents in the last century was for the security and survival of their families. Thus, before the passage of compulsory school laws, their response to public schooling is viewed as calculative; that is, most parents measured schooling in terms of gains and losses for the family unit. Moreover, these decisions were not made in isolation. They were strongly influenced by the cultural and class background of the parent; and they were reached in the midst of social, economic, political, and environmental changes.;Based on the literature concerning nineteenth-century school attendance, a number of hypotheses were generated and tested in each of the periods outlined above. In general, it was found that Protestant and upper and middle class males dominated schooling arrangements until Roman Catholic separate schools were established in 1858; that Protestants and Roman Catholics formed schools to transmit key cultural tenets to their children; that Protestant and upper and middle class children generally demonstrated higher attendance rates than Roman Catholic or Black and working class children; that lower class parents limited their children 2 ̆ 7 s education to the 3 R 2 ̆ 7 s; and that family schooling strategies were directly linked to the future occupational <b>destinations</b> of children.;Archival <b>files,</b> annual reports, board and city council minutes, newspapers, personal papers, censuses, city directories, church records, and school syllabi were used to measure qualitatively and quantitatively the school attendance behaviour of London children in the past century.;In general, the hypotheses advanced by historians of education in Canada were substantiated in this study. Thus, when compared and contrasted to similar studies for other communities, these findings could furnish some additional truth about the relationship between school and society in nineteenth-century Canada...|$|R
