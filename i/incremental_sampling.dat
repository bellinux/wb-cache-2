26|68|Public
40|$|<b>Incremental</b> <b>sampling</b> {{methodology}} (ISM) is {{a structured}} composite sampling and processing protocol having specific elements {{designed to reduce}} data variability and increase sample representativeness for a specified volume of soil under investigation. Variability in measured contaminant concentrations between discrete soil samples is due primarily to the particulat...|$|E
40|$|Sampling {{can be a}} {{significant}} source of error in the measurement process. The characterization and cleanup of hazardous waste sites require data that meet site-specific levels of acceptable quality if scientifically supportable decisions are to be made. In support of this effort, the US Environmental Protection Agency (EPA) is investigating methods that relate sample characteristics to analytical performance. Predicted uncertainty levels allow appropriate study design decisions to be made, facilitating more timely and less expensive evaluations. Gy sampling theory can predict {{a significant}} fraction of sampling error when certain conditions are met. We report on several controlled studies of subsampling procedures to evaluate the utility of Gy sampling theory applied to laboratory subsampling practices. Several sample types were studied and both analyte and non-analyte containing particles were shown to play important roles affecting the measured uncertainty. Gy sampling theory was useful in predicting minimum uncertainty levels provided the theoretical assumptions were met. Predicted fundamental errors ranged from 46 to 68 % of the total measurement variability. The study results also showed sectorial splitting outperformed <b>incremental</b> <b>sampling</b> for simple model systems and suggested that sectorial splitters divide each size fraction independently. Under the limited conditions tested in this study, <b>incremental</b> <b>sampling</b> with a spatul...|$|E
40|$|Abstract. The Hammersley and Halton point sets, two {{well known}} low {{discrepancy}} sequences, {{have been used}} for quasi-Monte Carlo integration in previous research. A deterministic formula generates a uniformly distributed and stochastic-looking sampling pattern, at low computational cost. The Halton point set is also useful for <b>incremental</b> <b>sampling.</b> In this paper, we discuss detailed implementation issues and our experience of choosing suitable bases of the point sets, not just on the 2 D plane, but also on a spherical surface. The sampling scheme is also applied to ray tracing, with a significant improvement in error. ...|$|E
30|$|IRS: The <b>incremental</b> random <b>sampling</b> {{algorithm}} {{shown in}} algorithm 2.|$|R
30|$|Possible feature {{reduction}} {{techniques are}} {{techniques such as}} principle components, heuristic feature selection with wrapper method and feature selection with decision trees. Examples for case reduction techniques are <b>incremental</b> <b>samples,</b> average samples, increasing the sampling period and strategic sampling of key events. For value reduction prominent techniques are rounding, using k-means clustering and discretization using entropy minimization.|$|R
40|$|The forward-greedy {{algorithm}} {{based on}} the neighborhood rough set is a simple and effective method for the attribute reduction. However, it is non-incremental reduction method, which limits its application in the dynamic decision system. In this article, an improved incremental attribute reduction algorithm is proposed by introducing concept of the relative positive region, which can update the original reduction set quickly and handle both the incremental attributes and the <b>incremental</b> <b>samples.</b> Finally, the correctness and effectiveness of the proposed algorithm are demonstrated by examples and experiments on 5 standard data sets from UCI...|$|R
40|$|Many {{people take}} photos and videos with {{smartphones}} {{and more recently}} with 360 -degree cameras at popular places and events, and share them in social media. Such visual content is produced in large volumes in urban areas, {{and it is a}} source of information that online users could exploit to learn what has got the interest of the general public {{on the streets of the}} cities where they live or plan to visit. A key step to providing users with that information is to identify the most popular k spots in specified areas. In this paper, we propose a clustering and <b>incremental</b> <b>sampling</b> (C&IS) approach that trades off accuracy of top-k results for detection speed. It uses clustering to determine areas with high density of visual content, and <b>incremental</b> <b>sampling,</b> controlled by stopping criteria, to limit the amount of computational work. It leverages spatial metadata, which represent the scenes in the visual content, to rapidly detect the hotspots, and uses a recently proposed Gaussian probability model to describe the capture intention distribution in the query area. We evaluate the approach with metadata, derived from a non-synthetic, user-generated dataset, for regular mobile and 360 -degree visual content. Our results show that the C&IS approach offers 2. 8 x- 19 x reductions in processing time over an optimized baseline, while in most cases correctly identifying 4 out of 5 top locations...|$|E
40|$|Abstract — This paper studies a {{class of}} approach-evasion {{differential}} games, in which one player aims to steer the state of a dynamic system to the given target set in minimum time, while avoiding some set of disallowed states, and the other player desires to achieve the opposite. We propose {{a class of}} novel anytime computation algorithms, analyze their convergence properties and verify their performance via a number of numerical simulations. Our algorithms significantly outperform the multi-grid method for the approach-evasion differential games both theoretically and numerically. Our technical approach leverages <b>incremental</b> <b>sampling</b> in robotic motion planning and viability theory. I...|$|E
30|$|Algorithm 2 {{shows the}} {{procedure}} of <b>incremental</b> <b>sampling.</b> Firstly, we reuse the previous structure SW_i- 1 (Line 4). Then tuples, which {{are beyond the}} range of current window, are removed from SW_i (Line 6). Function remove_head is used to remove the first item in SW_i.indexarr and update SW_i.sh depending on how many tuples are removed. Then we use a temporary structure T_SW to store the indexes of newly sampled tuples {{in the range from}} SW_i.indexarr[SW_i.st] to W_i.t (Line 9 – 11). Next, we append the indexes in T_SW.indexarr to SW_i and update SW_i.st (Line 12). Finally, we aggregate the tuples in SW_i to estimate the final result (Line 13 – 14).|$|E
40|$|The {{heterogeneous}} three-dimensional {{spatial distribution}} of mycotoxins {{has proven to be}} one of the main limitations for the design of effective sampling protocols. Current sample collection protocols for mycotoxins have been designed to estimate the mean concentration and fail to characterise the {{spatial distribution of}} the mycotoxin concentration due to the aggregation of the <b>incremental</b> <b>samples.</b> Geostatistical techniques have been successfully applied to overcome similar problems in many research areas. However, little work has been developed on the use of geostatistics for the design of sampling protocols for mycotoxins. This paper focuses on the analysis of the two and three-dimensional spatial structure of fumonisins B 1 (FB 1) and B 2 (FB 2) in maize in a bulk store using a geostatistical approach and on how results help determine the number and location of <b>incremental</b> <b>samples</b> to be collected. The spatial correlation between FB 1 and FB 2, as well as between the number of kernels infected and the level of contamination was investigated. For this purpose, a bed of maize was sampled at different depths to generate a unique three-dimensional data set of FB 1 and FB 2. The analysis found no clear evidence of spatial structure in either the two- dimensional or three-dimensional analyses. The number of Fusarium infected kernels was not a good indicator for the prediction of fumonisin concentration and there was no spatial correlation between the concentrations of the two fumonisins...|$|R
40|$|Queries {{over large}} scale (petabyte) data bases often mean waiting {{overnight}} for a result to come back. Scale costs time. Such time {{also means that}} potential avenues of exploration are ignored because the costs {{are perceived to be}} too high to run or even propose them. With sampleAction we have explored whether interaction techniques to present query results running over only <b>incremental</b> <b>samples</b> can be presented as sufficiently trustworthy for analysts both to make closer to real time decisions about their queries and to be more exploratory in their questions of the data. Our work with three teams of analysts suggests that we can indeed accelerate and open up the query process with such incremental visualizations. Author Keywords Incremental visualizations, large data, exploratory dat...|$|R
40|$|The aim of {{this paper}} was to update the {{sampling}} plan for analysis of mycotoxins in grains, formerly published by the author. The proposed alterations were based on the acquired experience on its application and on FAO recommendations. This update restricts the scope of the former plan and establishes a sampling plan for analysis of aflatoxin in peanuts and corn, by means of modified formulas, the minimum number of sacks or points (when in bulk) from which <b>incremental</b> <b>samples</b> should be drawn to make a bulk sample. Fractional exponents (square roots) of the formulas proportionally decrease the number of sacks/points to be sampled as the lot size increases. Operating Characteristic (OC) curves developed for in-shell and shelled peanuts and corn as well as trend curves of the coefficient variation for different sample sizes (weights) are presented...|$|R
30|$|For many drug therapies, {{there is}} little {{knowledge}} about the ultimate distribution patterns of the compounds within tissue compartments following treatment. It is possible to label drugs with a tracer and follow their uptake using technologies such as positron-emission tomography (PET) and autoradiography. For both of these methods the physical manipulation of the compound by the labeling could change {{the properties of the}} compound. Over the last decade a method to identify unlabeled drugs in tissue has been under development using MALDI-MSI. With MALDI-MSI continual <b>incremental</b> <b>sampling</b> can be performed upon tissue taken directly from the body to identify cellular locations that contain the specific drug ion signatures of the compound in question [21].|$|E
40|$|Abstract — In this paper, the {{filtering}} {{problem for}} a large class of continuous-time, continuous-state stochastic dynam-ical systems is considered. Inspired by recent advances in asymptotically-optimal sampling-based motion planning algo-rithms, such as the PRM ∗ and the RRT∗, an incremen-tal sampling-based algorithm is proposed. Using <b>incremental</b> <b>sampling,</b> this approach constructs a sequence of Markov chain approximations, and solves the filtering problem, in an incremental manner, on these discrete approximations. It is shown that the trajectories of the Markov chain approximations converge in distribution to the trajectories of the original stochastic system; moreover, the optimal filter calculated on these Markov chains converges to the optimal continuous-time nonlinear filter. The convergence results are verified {{in a number of}} simulation examples. I...|$|E
40|$|We use isotopic {{analyses}} of authigenic siderite and calcite cements within Rosselia socialis burrows from shoreface deposits in the Upper Cretaceous Horseshoe Canyon Formation of Alberta, Canada, to re-veal the early cementation {{history of the}} burrow and geochemical conditions of the initial sedimentary environment. Within the Horse-shoe Canyon Formation, two forms of the Rosselia burrows are pres-ent: bulbous in situ burrows, and transported, spindlelike burrows, which display similar internal shaft diameters but smaller overall size compared to in situ forms. Transverse, <b>incremental</b> <b>sampling</b> of cal-cite and siderite cements in the Rosselia burrows reveals symmetrical isotopic deviation in 13 C and 18 O around the burrow core, rep-resenting accretionary records of evolving pore-water conditions. The number of isotopic deviations recorded in bulbous specimens is equal to those observed in spindle-shaped burrows, suggesting that in sit...|$|E
40|$|We present {{procedures}} for selecting the best or near-best of {{a finite number}} of simulated systems when best is defined by maximum or minimum expected performance. The procedures are appropriate when it is possible to repeatedly obtain small, <b>incremental</b> <b>samples</b> from each simulated system. The goal of such a sequential procedure is to eliminate, at an early stage of experimentation, those simulated systems that are clearly inferior, and thereby reduce the overall computational effort required to find the best. The procedures we present accommodate unequal variances across systems and the use of common random numbers. However, they are based on the assumption of normally distributed data, so we analyze the impact of batching (to achieve approximate normality or independence) on the performance of the procedures. Comparisons with existing procedures are also provided. Key Words: Output Analysis; Multiple Comparisons; Ranking and Selection; Variance Reduction 1 Introduction In a series of [...] ...|$|R
40|$|Neural network plays a {{major role}} in the field of pattern recognition. For pattern recognition, a major {{drawback}} with traditional neural networks is that neural networks may easily be trapped in spurious states. Synergetic neural network (SNN) has been proposed in the literature to overcome this problem, however, when applying synergetic neural network on face recognition, the results are not satisfactory for large image databases due to low memory capacity. Therefore, the chaotic dynamic property is introduced to the conventional synergetic neural network in order to resolve the problem. In this paper, an additional control parameter is introduced to the chaotic synergetic neural network (CSNN) in order to terminate the recognition process whenever an image is recognized. This helps to alleviate processing memory demand which often accompanies such networks. Various imagery defects are tested and the accuracy of both methods is evaluated based on <b>incremental</b> <b>sample</b> siz...|$|R
30|$|This work {{develops}} {{several different}} sampling approaches, {{based on the}} sampling techniques in online aggregation, for executing window functions. The proposed sampling algorithms speed up query processing time by {{reducing the number of}} tuples used in window aggregation functions. And this work is an extension of our work [1], in which we proposed naive random <b>sampling</b> (NRS) and <b>incremental</b> random <b>sampling</b> (IRS) algorithms.|$|R
40|$|In this paper, the {{filtering}} {{problem for}} a large class of continuous-time, continuous-state stochastic dynamical systems is considered. Inspired by recent advances in asymptotically-optimal sampling-based motion planning algorithms, such as the PRM* and the RRT*, an incremental sampling-based algorithm is proposed. Using <b>incremental</b> <b>sampling,</b> this approach constructs a sequence of Markov chain approximations, and solves the filtering problem, in an incremental manner, on these discrete approximations. It is shown that the trajectories of the Markov chain approximations converge in distribution to the trajectories of the original stochastic system; moreover, the optimal filter calculated on these Markov chains converges to the optimal continuous-time nonlinear filter. The convergence results are verified {{in a number of}} simulation examples. United States. Army Research Office. Multidisciplinary University Research Initiative (Grant W 911 NF- 11 - 1 - 0046...|$|E
40|$|This paper {{focuses on}} a continuous-time, continuous-space {{formulation}} of the stochastic optimal control problem with nonlinear dynamics and observation noise. We lay the mathematical foundations to construct, via <b>incremental</b> <b>sampling,</b> an approximating sequence of discrete-time finite-state partially observable Markov decision processes (POMDPs), such that the behavior of successive approximations converges to {{the behavior of the}} original continuous system in an appropriate sense. We also show that the optimal cost function and control policies for these POMDP approximations converge almost surely to their counterparts for the underlying continuous system in the limit. We demonstrate this approach on two popular continuous-time problems, viz., the Linear-Quadratic-Gaussian (LQG) control problem and the light-dark domain problem. United States. Army Research Office. Multidisciplinary University Research Initiative (Grant W 911 NF- 11 - 1 - 0046...|$|E
40|$|Abstract — The Rapidly-exploring Random Tree (RRT) algorithm, {{based on}} <b>incremental</b> <b>sampling,</b> {{efficiently}} computes motion plans. Although the RRT algorithm quickly produces candidate feasible solutions, {{it tends to}} converge to a solution that is far from optimal. Practical applications favor “anytime” algorithms that quickly identify an initial feasible plan, then, given more computation time available during plan execution, improve the plan toward an optimal solution. This paper describes an anytime algorithm based on the RRT ∗ which (like the RRT) finds an initial feasible solution quickly, but (unlike the RRT) almost surely converges to an optimal solution. We present two key extensions to the RRT ∗, committed trajectories and branch-and-bound tree adaptation, that together enable the algorithm to make more efficient use of computation time online, resulting in an anytime algorithm for real-time implementation. We evaluate the method using a series of Monte Carlo runs in a high-fidelity simulation environment, and compare {{the operation of the}} RRT and RRT ∗ methods. We also demonstrate experimental results for an outdoor wheeled robotic vehicle. I...|$|E
30|$|In {{this part}} we compare the error levels and the {{influence}} of the parameters of each algorithm. Sampling rate is a very important parameter in our methods. Since naive random <b>sampling</b> (NRS) and <b>incremental</b> random <b>sampling</b> (IRS) are similar in this respect, all the pictures in Fig.  8 show the results of IRS algorithm based on the row mode window.|$|R
40|$|We present proced 657 for {{selecting}} the best or near-best of {{a finite number}} of simulated systems when best isdz 389 by maximum or minimum expected performance. The proced 855 are appropriate when it is possible to repeated 3 obtain small, <b>incremental</b> <b>samples</b> from each simulated system. The goal of such a sequential proced 05 is to eliminate, at an early stage of experimentation, those simulated systems that are apparently inferior, and thereby red 46 the overall computational e#ortrequired tofind the best. The proced 668 we present accommodom unequal variances across systems and the use of common rand 5 numbers. However, they arebased on the assumption of normallydmallyz 244 dma so we analyze the impact of batching (to achieve approximate normality or ind 5 end 3945 on the performance of the proced 1723 Comparisons with some existing indingz 25126 zV 3 proced 533 are also provid 60 Key Word 9 Output Analysis; Multiple Comparisons; Rankingand Selection; Variance Redcez 42 1 IntroductiA In a series of p [...] ...|$|R
40|$|Motion {{planning}} for robots with many {{degrees of freedom}} requires the exploration of an exponentially large configuration space. Single-query motion planners restrict exploration to regions of configuration space determined to be relevant to a particular planning query. The heuristics employed by existing single-query planners to estimate the relevance of a region, however, remain unchanged throughout the planning process. An incorrect estimate by the heuristic for a configuration space region will only be corrected by explicit exploration. As a result, unnecessary exploration is performed. In this paper we propose an alternative approach. We observe that every <b>incremental</b> <b>sample</b> improves the planner’s understanding of configuration space. This improved understanding can be exploited to inform the singlequery heuristic of a motion planner. We formalize the improvement in understanding by employing the notion of entropy from information theory and derive a principled method of configuration space exploration in the single-query setting. Experiments show that the proposed single-query entropy-guided motion planner outperforms existing single-query techniques. ...|$|R
40|$|Abstract — We {{study the}} problem of {{predicting}} the Residual Link Lifetime (RLL) in MANETs, where the nodes are able to measure the relative distances between them (e. g., by using the UWB technology). We propose a Mobile-Projected Trajectory (MPT) algorithm, whose input is periodically sampled, noisy range measurements between the two nodes of a link. It estimates a projected trajectory, which is then used to compute the predicted RLL. An enhancement technique, which we call <b>Incremental</b> <b>Sampling,</b> is proposed where the estimated trajectory is further refined to improve {{the accuracy of the}} RLL prediction. We have evaluated the performance of the MPT algorithm with two different mobility models and for different parameters, and have shown that MPT yields robust performance; i. e., the main strength of the MPT algorithm lies in its capability to accurately predict the RLL with limited range input data. For example, after a measurementacquisition time equal to 25 % of the link lifetime, the algorithm yields 90 % prediction accuracy; 80 % accuracy is achieved after 20 % of the link lifetime. After only 15 % of link lifetime, the algorithm still achieves 60 % prediction accuracy...|$|E
40|$|Abstract. Hierarchical {{modeling}} and reasoning are fundamental in ma-chine intelligence, {{and for this}} the two-parameter Poisson-Dirichlet Pro-cess (PDP) plays an important role. The most popular MCMC sampling algorithm for the hierarchical PDP and hierarchical Dirichlet Process is to conduct an <b>incremental</b> <b>sampling</b> based on the Chinese restaurant metaphor, which originates from the Chinese restaurant process (CRP). In this paper, with the same metaphor, we propose a new table repre-sentation for the hierarchical PDPs by introducing an auxiliary latent variable, called table indicator, to record which customer takes respon-sibility for starting a new table. In this way, the new representation allows full exchangeability that is an essential condition for a correct Gibbs sampling algorithm. Based on this representation, we develop a block Gibbs sampling algorithm, which can jointly sample the data item and its table contribution. We test this out on the hierarchical Dirichlet process variant of latent Dirichlet allocation (HDP-LDA) developed by Teh, Jordan, Beal and Blei. Experiment {{results show that the}} proposed algorithm outperforms their “posterior sampling by direct assignment” algorithm in both out-of-sample perplexity and convergence speed. The representation can be used with many other hierarchical PDP models...|$|E
40|$|The Rapidly-exploring Random Tree (RRT) algorithm, {{based on}} <b>incremental</b> <b>sampling,</b> {{efficiently}} computes motion plans. Although the RRT algorithm quickly produces candidate feasible solutions, {{it tends to}} converge to a solution that is far from optimal. Practical applications favor “anytime” algorithms that quickly identify an initial feasible plan, then, given more computation time available during plan execution, improve the plan toward an optimal solution. This paper describes an anytime algorithm based on the RRT* which (like the RRT) finds an initial feasible solution quickly, but (unlike the RRT) almost surely converges to an optimal solution. We present two key extensions to the RRT*, committed trajectories and branch-and-bound tree adaptation, that together enable the algorithm to make more efficient use of computation time online, resulting in an anytime algorithm for real-time implementation. We evaluate the method using a series of Monte Carlo runs in a high-fidelity simulation environment, and compare {{the operation of the}} RRT and RRT* methods. We also demonstrate experimental results for an outdoor wheeled robotic vehicle. United States. Army. Logistics Innovation AgencyUnited States. Army Combined Arms Support CommandUnited States. Dept. of the Air Force (Air Force Contract FA 8721 - 05 -C- 0002...|$|E
40|$|A {{strategy}} is introduced for achieving high accuracy in {{synthetic aperture radar}} (SAR) automatic target recognition (ATR) tasks. Initially, a novel pose rectification process and an image normalization process are sequentially introduced to produce images with less variations prior to the feature processing stage. Then, feature sets that have a wealth of texture and edge information are extracted with the utilization of wavelet coefficients, where more effective and compact feature sets are acquired by reducing the redundancy and dimensionality of the extracted feature set. Finally, a group of discrimination trees are learned and combined into a final classifier {{in the framework of}} Real-AdaBoost. The proposed method is evaluated with the public release database for moving and stationary target acquisition and recognition (MSTAR). Several comparative studies are conducted {{to evaluate the effectiveness of}} the proposed algorithm. Experimental results show the distinctive superiority of the proposed method under both standard operating conditions (SOCs) and extended operating conditions (EOCs). Moreover, our additional tests suggest that good recognition accuracy can be achieved even with limited number of training images as long as these are captured with appropriately <b>incremental</b> <b>sample</b> step in target poses...|$|R
50|$|<b>Incremental</b> Implementation: The <b>Sample</b> UHID can be {{implemented}} on an incremental basis. With the {{development and use of}} appropriate procedures and establishment of the necessary bidirectional mapping, both the Sample UHID and existing patient identifiers can co-exist during the time of transition.|$|R
40|$|We present {{algorithms}} {{for generating}} deterministic <b>sample</b> sequences using <b>incremental</b> grid-based <b>sampling.</b> Our algorithms {{are designed to}} generate dense sample sequences over spaces common in robotics, such as the unit cube, SO(3), and SE(3). Our sampling techniques provide the advantageous properties of uniformity, lattice structure, and incremental quality. In addition, the inherent structure of grid-based sequences not only enables them {{to be used in}} the place of other sampling techniques in existing algorithms, but also permits the development of new algorithms aimed at exploiting this structure...|$|R
40|$|Previously {{reported}} low luminescence of basal glacial sediment {{has raised}} the possibility that processes operating at the ice–bedrock interface {{have the potential to}} reset (or ‘bleach’) natural luminescence signals (Swift et al., 2011). This finding indicates that certain types of glacial sediment (for example, sub-glacial diamicts) might be amenable to dating using luminescence-based techniques. Using a purpose-built ring-shear apparatus situated in a light-controlled environment, we have investigated the potential for mineral grains to be reset when subjected to conditions similar to those experienced by sediment that has undergone transport at the ice–bedrock interface. Reported here are the preliminary results of an initial experiment that used medium quartz sand with a naturally-acquired palaeodose of ∼ 4. 3 Gy that had been obtained from a relict dune system. <b>Incremental</b> <b>sampling</b> during the shearing experiment and measurements were made to track changes in the luminescence properties of the sand as strain/shearing increased. The results indicate that increased strain/shearing resulted in {{an increase in the number}} of zero-dose grains and evolution of the De distribution from unimodal to multimodal. In light of the very much longer shearing distances that sub-glacial sediment would endure in nature, these results would appear to suggest that geomechanical processes at the ice–bed interface of glaciers and ice sheets may be a viable mechanism for resetting sediment...|$|E
40|$|Established soil {{sampling}} methods for asbestos are inadequate to support risk assessment and risk-based decision making at Superfund sites due to difficulties in detecting asbestos at low concentrations and difficulty in extrapolating soil concentrations to air concentrations. Environmental Protection Agency (EPA) 's Office of Land and Emergency Management (OLEM) currently recommends the rigorous process of Activity Based Sampling (ABS) to characterize site exposures. The {{purpose of this}} study was to compare three soil analytical methods and two {{soil sampling}} methods to determine whether one method, or combination of methods, would yield more reliable soil asbestos data than other methods. Samples were collected using both traditional discrete ("grab") samples and <b>incremental</b> <b>sampling</b> methodology (ISM). Analyses were conducted using polarized light microscopy (PLM), transmission electron microscopy (TEM) methods or a combination of these two methods. Data show that the fluidized bed asbestos segregator (FBAS) followed by TEM analysis could detect asbestos at locations that were not detected using other analytical methods; however, this method exhibited high relative standard deviations, indicating the results may be more variable than other soil asbestos methods. The comparison of samples collected using ISM versus discrete techniques for asbestos resulted in no clear conclusions regarding preferred sampling method. However, analytical results for metals clearly showed that measured concentrations in ISM samples were less variable than discrete samples...|$|E
40|$|Response surface {{functions}} {{are often used}} as simple and inexpensive replacements for computationally expensive computer models that simulate {{the behavior of a}} complex system over some parameter space. Progressive response surfaces are ones that are built up progressively as global information is added from new sample points in the parameter space. As the response surfaces are globally upgraded based on new information, heuristic indications of the convergence of the response surface approximation to the exact (fitted) function can be inferred. Sampling points can be incrementally added in a structured fashion, or in an unstructured fashion. Whatever the approach, at least in early stages of sampling it is usually desirable to sample the entire parameter space uniformly. At later stages of sampling, depending {{on the nature of the}} quantity being resolved, it may be desirable to continue sampling uniformly over the entire parameter space (Progressive response surfaces), or to switch to a focusing/economizing strategy of preferentially sampling certain regions of the parameter space based on information gained in early stages of sampling (Adaptive response surfaces). Here we consider Progressive response surfaces where a balanced indication of global response over the parameter space is desired. We use a variant of Moving Least Squares to fit and interpolate structured and unstructured point sets over the parameter space. On a 2 -D test problem we compare response surface accuracy for three <b>incremental</b> <b>sampling</b> methods: Progressive Lattice Sampling; Simple-Random Monte Carlo; and Halton Quasi-Monte-Carlo sequences. We are ultimately after a system for constructing efficiently upgradable response surface approximations with reliable error estimates...|$|E
40|$|We {{present the}} Macquarie/AAO/Strasbourg Hα Planetary Nebula Catalogue (MASH) of over 900 true, likely and {{possible}} new Galactic planetary nebulae (PNe) discovered from the AAO/UKST Ha {{survey of the}} southern Galactic plane. The combination of depth, resolution, uniformity and areal coverage of the Ha survey has opened up a hitherto unexplored region of parameter space permitting the detection of this significant new PN sample. Away from the Galactic bulge the new PNe are typically more evolved, of larger angular extent, of lower surface brightness and more obscured (i. e. extinguished) than those in most previous surveys. We have also doubled the number of PNe in the Galactic bulge itself and although most are compact, we have also found more evolved examples. The MASH catalogue represents {{the culmination of a}} seven-year programme of identification and confirmatory spectroscopy. A key strength is that the entire sample has been derived from the same, uniform observational data. The 60 per cent increase in known Galactic PNe represents the largest ever <b>incremental</b> <b>sample</b> of such discoveries and will {{have a significant impact on}} many aspects of PN research. This is especially important for studies at the faint end of the PN luminosity function which was previously poorly represented. © 2006 RAS. link_to_subscribed_fulltex...|$|R
40|$|Abstract—This paper builds upon a {{previously}} defined fusion process that exploits multichannel receiver diversity to enhance received SNR. This particular diversity combiner aims to enhance SNR under the challenging constraints that channel gains are unknown, {{there is no}} direct knowledge of the transmitted signal, and no opportunity to precode the signal into a known waveform. Thus, fusion is blind in the sense that indirect techniques are invoked to intelligently weight each sample during fusion, and to measure the outcome. Having already established a critical threshold that deter-mines whether fusion does or does not enhance SNR, this paper takes the next step by pursuing rigorous analytical development of a statistical noise model for the effects of the combiner. We provide the probability distributions of this noise, termed Rayleigh-normalized Gaussian. With the probability distributions in hand, we apply them to sample sets of various sizes to understand how the combiner behaves with each <b>incremental</b> <b>sample.</b> This allows us to investigate the likelihood that the critical threshold for SNR gain is met, relative to additional samples, as well as the likelihood of meeting arbitrary target SNR gains. We also develop an expression for the average power of the Rayleigh-normalized Gaussian noise variable. I...|$|R
40|$|This article {{analyzes}} {{social consequences}} of smoking. Particular {{attention is paid}} to the preva-lence of smoking among urban youth. Smoking is rated as one of the forms of social deviance. The author traced economic, social and socio-cultural effects of tobacco use, discussed the connection of smoking with urban lifestyle. The author raises a question about the reasons for the wide dissemination of this type of deviation in modern society, especially among young people, and speaks with the assumption that {{this is due to the}} tolerance of smoking in the society in general. The work shows the current relationship of the cases of fires on construction sites with careless handling of fire while smoking. This may cause an extensive damage to the construction industry. Some statistics is offered on peaty and forest fires caused by carelessly discarded cigarette butts. The propaganda of smoking is taking place in media and advertising. Popular culture is also involved in the process of propaganda. The second problem solved in the article is finding new ways of agitation and propaganda in favor of a healthy lifestyle. The authors draw conclusions according to the poll of MGSU students based on <b>incremental</b> <b>sample.</b> Also the analysis of the media is made...|$|R
