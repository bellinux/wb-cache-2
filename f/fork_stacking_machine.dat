0|356|Public
40|$|Abstract. A <b>fork</b> <b>stack</b> is a stack {{that allows}} pushes and pops of several items at a time. An {{algorithm}} {{to determine which}} sequences of input streams can be sorted by a <b>fork</b> <b>stack</b> is given. The minimal unsortable sequences are found (there are a finite number only). The results are extended to <b>fork</b> <b>stacks</b> where there are bounds on how many items can be pushed and popped at one time. Some enumeration results {{for the number of}} sortable sequences are given. ...|$|R
50|$|The {{downside}} to {{the simplicity of}} compilers for <b>stack</b> <b>machines,</b> is that pure <b>stack</b> <b>machines</b> have fewer optimisations (see subsections in § performance disadvantages of <b>stack</b> <b>machines).</b> However optimisation of compiled stack code is quite possible. Back-end optimisation of compiler output has been demonstrated to significantly improve code, and potentially performance, whilst global optimisation within the compiler itself achieves further gains.|$|R
40|$|A <b>fork</b> <b>stack</b> is a {{generalised}} stack {{which allows}} pushes and pops of several items at a time. We consider {{the problem of}} determining which input streams can be sorted using a single forkstack, or dually, which permutations of a fixed input stream can be produced using a single forkstack. An algorithm is given to solve the sorting problem and the minimal unsortable sequences are found. The results are extended to <b>fork</b> <b>stacks</b> where there are bounds on how many items can be pushed and popped at one time. In this context we also establish how to enumerate the collection of sortable sequences. ...|$|R
50|$|<b>Stack</b> <b>machines</b> {{may have}} their {{expression}} stack and their call-return stack separated or as one integrated structure. If they are separated, the instructions of the <b>stack</b> <b>machine</b> can be pipelined with fewer interactions and less design complexity. Usually it can run faster.|$|R
50|$|The NonStop systems {{designed}} by Tandem Computers {{in the late}} 1970s and early 1980s were also 16-bit <b>stack</b> <b>machines,</b> influenced by the B5000 indirectly through the HP 3000 connection, as several of the early Tandem engineers were formerly with HP. Around 1990, these systems migrated to MIPS RISC architecture but continued to support execution of <b>stack</b> <b>machine</b> binaries by object code translation or direct emulation. Sometime after 2000, these systems migrated to Itanium architecture and continued to run the legacy <b>stack</b> <b>machine</b> binaries.|$|R
5000|$|... #Subtitle level 3: Performance {{disadvantages}} of <b>stack</b> <b>machines</b> ...|$|R
5000|$|<b>Stack</b> <b>machines</b> {{have much}} smaller {{instructions}} {{than the other}} styles of machines. Loads and stores to memory are separate and so stack code requires roughly twice as many instructions as the equivalent code for register machines. The total code size (in bytes) is still less for <b>stack</b> <b>machines</b> [...]|$|R
50|$|In {{computer}} science, {{computer engineering}} and programming language implementations, a <b>stack</b> <b>machine</b> {{is a type}} of computer. In some cases, the term refers to a software scheme that simulates a <b>stack</b> <b>machine.</b> The main difference from other computers is that most of its instructions operate on a pushdown stack of numbers rather than numbers in registers. A stack computer is programmed with a reverse Polish notation instruction set. Most computer systems implement a stack in some form to pass parameters and link to subroutines. This does not make these computers <b>stack</b> <b>machines.</b>|$|R
40|$|Abstract—A space bounded <b>Stack</b> <b>Machine</b> is {{a regular}} Turing Machine with a {{read-only}} input tape, several space bounded read-write work tapes, and an unbounded <b>stack.</b> <b>Stack</b> <b>Machines</b> with a logarithmic space bound have been connected to other classical models of computation, such as polynomial time Turing Machines (P) (Cook; 1971) and polynomial size, polylogarithmic depth, bounded fan-in circuits (NC) e. g., (Borodin et al.; 1989). In this paper, we give the first known lower bound for <b>Stack</b> <b>Machines.</b> This {{comes in the form}} of a trade-off lower bound between space and number of passes over the input tape. Specifically, we give an explicit permuted inner product function such that any <b>Stack</b> <b>Machine</b> computing this func-tion requires either sublinear polynomial space or sublinear polynomial number of passes. In the case of logarithmi...|$|R
5000|$|... #Subtitle level 3: Advantages of <b>stack</b> <b>machine</b> {{instruction}} sets ...|$|R
40|$|Compiler {{design for}} <b>stack</b> <b>machines,</b> in {{particular}} register allocation, is an under researched area. In this thesis I present {{a framework for}} analysing and developing register allocation techniques for <b>stack</b> <b>machines.</b> Using this framework, I analyse previous register allocation methods and develop two new algorithms for global register allocation, both of which outperform previous algorithms, and {{lay the groundwork for}} future enhancements. Finally I discuss how effective global register allocation for <b>stack</b> <b>machines</b> can influence the design of high performance stack architectures. Included with this thesis is a portable C compiler for <b>stack</b> <b>machines,</b> which incorporates these new register allocation methods. The C compiler and associated tools (assembler, linker and simulator) are included in the CD. Acknowledgements This work was done as part of the UFO project, to develop a soft-core stack processor utilising the latest research on stack based architectures. Its ai...|$|R
40|$|Register {{allocation}} is {{a critical}} part of any compiler, yet register allocation for <b>stack</b> <b>machines</b> has received relatively little attention in the past. We present a framework for the analysis of register allocation methods for <b>stack</b> <b>machines</b> which has allowed us to analyse current methods. We have used this framework to design the first truly procedure-wide register allocation methods for <b>stack</b> <b>machines.</b> We have designed two such methods, both of which outperform current techniques. This work was funded by the AMADEUS project, part of the DTI’s Next Wave Technologies and Markets Program, in collaboration with MPE Ltd...|$|R
5000|$|Some <b>stack</b> <b>machine</b> {{instruction}} {{sets are}} intended for interpretive execution of a virtual machine, rather than driving hardware directly. Interpreters for virtual <b>stack</b> <b>machines</b> are easier to build than interpreters for register or memory-to-memory machines; the logic for handling memory address modes is in just one place rather than repeated in many instructions. <b>Stack</b> <b>machines</b> also tend to have fewer variations of an opcode; one generalized opcode will handle both frequent cases and obscure corner cases of memory references or function call setup. (But code density is often improved by adding short and long forms for the same operation.) ...|$|R
40|$|The strong, intermediate, {{and weak}} Turing impossibility {{properties}} are introduced. Some facts concerning Turing impossibility for <b>stack</b> <b>machine</b> programming are trivially adapted from previous work. Several intriguing questions are {{raised about the}} Turing impossibility properties concerning different method interfaces for <b>stack</b> <b>machine</b> programming. Comment: arXiv admin note: substantial text overlap with arXiv: 0910. 556...|$|R
5000|$|The {{compiled}} {{code for}} a simple <b>stack</b> <b>machine</b> would take the form: ...|$|R
2500|$|Differentiable {{push and}} pop actions for {{alternative}} memory networks called neural <b>stack</b> <b>machines</b> ...|$|R
5000|$|As in {{the earlier}} {{migration}} from <b>stack</b> <b>machines</b> to MIPS microprocessors, all customer software was carried forward without source changes. [...] "Native mode" [...] source code compiled directly to MIPS machine code was simply recompiled for Itanium. Some older [...] "non native" [...] software was still in TNS <b>stack</b> <b>machine</b> form. These were automatically ported onto Itanium via object code translation techniques.|$|R
50|$|Patriot Scientific's Ignite <b>stack</b> <b>machine</b> {{designed}} by Charles H. Moore holds a leading functional density.|$|R
50|$|The {{relative}} {{merits of}} <b>stack</b> <b>machines</b> versus register-based approaches are {{a subject of}} ongoing debate.|$|R
5000|$|... #Caption: Employees {{working with}} the Automatic 16 Inch Powder <b>Stacking</b> <b>Machine</b> during World War II ...|$|R
40|$|AbstractFunctions computable on <b>stack</b> <b>machines</b> are studied algebraically. Computable {{functions}} N∗ → N∗ {{are considered}} as meanings of programs (on <b>stack</b> <b>machines)</b> which cause {{the content of}} the stack before execution of the program to be transformed into {{the content of the}} stack after its execution. Church's thesis is extended by showing that every computable function N∗ → N∗ can be characterized as a stack function...|$|R
50|$|Such a <b>stack</b> <b>machine</b> {{architecture}} is inherently simpler since all instructions {{operate on the}} top-most stack entries.|$|R
5000|$|META II outputs {{assemble}} {{code for}} a <b>stack</b> <b>machine,</b> evaluating {{this is like}} using an RPN calculator.|$|R
50|$|Out-of-order {{execution}} in <b>stack</b> <b>machines</b> seems {{to reduce or}} avoid many theoretical and practical difficulties. The cited research shows that such a <b>stack</b> <b>machine</b> can exploit instruction-level parallelism, and the resulting hardware must cache data for the instructions. Such machines effectively bypass most memory accesses to the stack. The result achieves throughput (instructions per clock) comparable to RISC register machines, with much higher code densities (because operand addresses are implicit).|$|R
50|$|The RTX2010 {{manufactured}} by Intersil is a radiation hardened <b>stack</b> <b>machine</b> microprocessor {{which has been}} used in numerous spacecraft.|$|R
5000|$|Responding to an {{interrupt}} involves {{saving the}} registers to a stack, and then branching to the interrupt handler code. In a <b>stack</b> <b>machine,</b> most parameters {{are already on}} a stack. Therefore, {{there is no need}} to push them there. Often <b>stack</b> <b>machines</b> respond more quickly to interrupts. [...] Some register machines deal with this by having multiple register files that can be instantly swapped but this increases costs and slows down the register file.|$|R
50|$|The common {{alternatives}} to <b>stack</b> <b>machines</b> are register machines, {{in which each}} instruction explicitly names specific registers for its operands and result.|$|R
50|$|WebAssembly is a {{portable}} <b>stack</b> <b>machine</b> {{which is designed}} to be faster to parse than JavaScript, as well as faster to execute.|$|R
5000|$|The {{next step}} up from this is a <b>stack</b> <b>machine</b> or {{interpreter}} with a single top-of-stack register. The above code then does: ...|$|R
40|$|Abstract: Nowadays, power {{consumption}} {{is a critical}} issue in embedded computing. <b>Stack</b> <b>machines</b> provide compact code and simple execution engines, which are important features to embedded systems. However, <b>stack</b> <b>machines</b> use memory very inefficiently, {{and this is a}} source of high power dissipation. This work presents a architecture for embedded system – FemtoJava Microcontroler – that uses all of the benefits of Java and stack-based machines, while hiding the inherent inefficient and power demanding features of stack-based architectures...|$|R
50|$|Some in the {{industry}} believe that <b>stack</b> <b>machines</b> execute more data cache cycles for temporary values and local variables than do register machines.|$|R
5000|$|The Forth {{virtual machine}} and other [...] "0-operand" [...] {{instruction}} sets lack any operand specifier fields, such as some <b>stack</b> <b>machines</b> including NOSC.|$|R
5000|$|The {{stack pointer}} {{is used to}} manage the {{run-time}} stack. Rarely, other data stacks are addressed by dedicated address registers, see <b>stack</b> <b>machine.</b>|$|R
40|$|Abstract. Binding {{the actual}} {{production}} of one sporting goods factory, {{in order to}} meet the needs of automatic production line of shooting flying saucer, the discoid products <b>stacking</b> <b>machine</b> was developed based on PLC, carried out mechanical system design of discoid products <b>stacking</b> <b>machine</b> and control system development based on PLC. After production testing proved reasonable structure of mechatronics systems, easy to operate, reliable, able to meet the requirements of automated production, with a strong practical and innovative, have some application value...|$|R
50|$|The ZPU is a {{microprocessor}} <b>stack</b> <b>machine</b> designed by Norwegian company Zylin AS to run supervisory code in electronic systems {{that include a}} field-programmable gate array (FPGA).|$|R
50|$|In 1998 Flottweg {{acquired}} Veronesi, an Italian {{manufacturer of}} disc stack centrifuges. In the same year, Flottweg {{began to develop}} and produce disc <b>stack</b> <b>machines</b> at Vilsbiburg.|$|R
