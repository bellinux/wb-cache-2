520|406|Public
25|$|Several {{concepts}} of a derivative {{may be defined}} on a Banach space. See the articles on the Fréchet derivative and the Gâteaux derivative for details. The Fréchet derivative allows for {{an extension of the}} concept of a <b>directional</b> <b>derivative</b> to Banach spaces. The Gâteaux derivative allows for an extension of a <b>directional</b> <b>derivative</b> to locally convex topological vector spaces. Fréchet differentiability is a stronger condition than Gâteaux differentiability. The quasi-derivative is another generalization of <b>directional</b> <b>derivative</b> that implies a stronger condition than Gâteaux differentiability, but a weaker condition than Fréchet differentiability.|$|E
25|$|Once again, {{the chain}} rule {{establishes}} {{that this is}} independent of the freedom in selecting γ from the equivalence class, since any curve with the same first order contact will yield the same <b>directional</b> <b>derivative.</b>|$|E
25|$|Another {{definition}} {{is that a}} vector field on a manifold M is a derivation of degree zero on the algebra of smooth functions on M. This {{definition is}} usually motivated {{in terms of the}} first definition: if X is a vector field according to the first definition, then the map sending a smooth function f to its derivative with respect to X is a vector field according to the second definition. Although it is less intuitively clear than the first definition, the second definition has the advantage that it often easier to work with. In particular, it is much simpler to define the <b>directional</b> <b>derivative</b> of a function using this definition: the <b>directional</b> <b>derivative</b> of f with respect to the vector field X is simply the value X(f) that results from inputting f into X.|$|E
40|$|Abstract. Various {{definitions}} of <b>directional</b> <b>derivatives</b> in topological vector spaces are compared. <b>Directional</b> <b>derivatives</b> {{in the sense}} of G~teaux, Fr 6 chet, and Hadamard are singled out from the general framework of cr-directional differentiability. It is pointed out that, in the case of finite-dimensional spaces and locally Lipschitz mappings, all these concepts of directional differentiability are equivalent. The chain rule for <b>directional</b> <b>derivatives</b> of a composite mapping is discussed. Key Words. <b>Directional</b> <b>derivatives,</b> positively homogeneous mapping, locally Lipschitz mapping, chain rule. 1...|$|R
40|$|Abstract. We {{consider}} fractional <b>directional</b> <b>derivatives</b> {{and establish}} some connection with stable densities. Solutions to advection equations involving fractional <b>directional</b> <b>derivatives</b> are presented and some properties investi-gated. In particular we obtain solutions written {{in terms of}} Wright functions by exploiting operational rules involving the shift operator. We also consider fractional advection diffusion equations involving fractional powers of the nega-tive Laplace operator and <b>directional</b> <b>derivatives</b> of fractional order and discuss the probabilistic interpretations of solutions. 1...|$|R
5000|$|Correspondingly, {{and with}} similar {{notation}} idealized functional models for spatial receptive fields {{can be expressed}} of the formThis model specifically generalizes the receptive field model in terms of Gaussian derivativesfrom <b>directional</b> <b>derivatives</b> of rotationally Gaussian kernels [...] to <b>directional</b> <b>derivatives</b> of affine Gaussian kernels [...]|$|R
25|$|Historically, at {{the turn}} of the 20th century, the {{covariant}} derivative was introduced by Gregorio Ricci-Curbastro and Tullio Levi-Civita in the theory of Riemannian and pseudo-Riemannian geometry. Ricci and Levi-Civita (following ideas of Elwin Bruno Christoffel) observed that the Christoffel symbols used to define the curvature could also provide a notion of differentiation which generalized the classical <b>directional</b> <b>derivative</b> of vector fields on a manifold. This new derivative – the Levi-Civita connection – was covariant in the sense that it satisfied Riemann's requirement that objects in geometry should be independent of their description in a particular coordinate system.|$|E
25|$|Riemannian {{geometry}} studies Riemannian manifolds, smooth manifolds with a Riemannian metric. This is {{a concept}} of distance expressed {{by means of a}} smooth positive definite symmetric bilinear form defined on the tangent space at each point. Riemannian geometry generalizes Euclidean geometry to spaces that are not necessarily flat, although they still resemble the Euclidean space at each point infinitesimally, i.e. in the first order of approximation. Various concepts based on length, such as the arc length of curves, area of plane regions, and volume of solids all possess natural analogues in Riemannian geometry. The notion of a <b>directional</b> <b>derivative</b> of a function from multivariable calculus is extended in Riemannian geometry to the notion of a covariant derivative of a tensor. Many concepts and techniques of analysis and differential equations have been generalized to the setting of Riemannian manifolds.|$|E
500|$|In {{some cases}} it may be easier to compute or {{estimate}} the <b>directional</b> <b>derivative</b> after changing {{the length of the}} vector. [...] Often this is done to turn the problem into the computation of a <b>directional</b> <b>derivative</b> {{in the direction of a}} unit vector. [...] To see how this works, suppose that [...] [...] Substitute [...] into the difference quotient. [...] The difference quotient becomes: ...|$|E
5000|$|... #Subtitle level 3: Tangent vectors as <b>directional</b> <b>derivatives</b> ...|$|R
2500|$|In general, {{the support}} {{function}} is not differentiable. However, <b>directional</b> <b>derivatives</b> ...|$|R
5000|$|... the {{coefficients}} for the <b>directional</b> <b>derivatives</b> [...] and ∆ must be zero, that is ...|$|R
500|$|The <b>{{direction}}al</b> <b>derivative</b> of f in {{the direction}} of v at the point x is the limit ...|$|E
500|$|When f is a {{function}} from an open subset of Rn to Rm, then the <b>directional</b> <b>derivative</b> of f in a chosen direction is the best linear approximation to f at that point and in that direction. [...] But when , no single <b>directional</b> <b>derivative</b> can give a complete picture {{of the behavior of}} f. The total derivative gives a complete picture by considering all directions at once. [...] That is, for any vector v starting at a, the linear approximation formula holds: ...|$|E
500|$|This is a {{consequence}} of the definition of the total derivative. [...] It follows that the <b>directional</b> <b>derivative</b> is linear in v, meaning that [...]|$|E
40|$|In {{this paper}} we present the exact {{value for the}} norm of <b>directional</b> <b>derivatives,</b> of all orders, for {{symmetric}} tensor powers of operators on finite dimensional vector spaces. Using this result we obtain an upper bound for the norm of all <b>directional</b> <b>derivatives</b> of immanants. This work is inspired in results by R. Bhatia, J. Dias da Silva, P. Grover and T. Jain...|$|R
5000|$|... is not {{differentiable}} at , {{but again}} {{all of the}} partial <b>derivatives</b> and <b>directional</b> <b>derivatives</b> exist.|$|R
5000|$|The metric-compatibility or torsion-freeness of the {{covariant}} derivative is recast {{into the}} commutators of the <b>directional</b> <b>derivatives,</b> ...|$|R
500|$|If all {{the partial}} {{derivatives}} of f exist and are continuous at x, then they determine the <b>directional</b> <b>derivative</b> of f {{in the direction}} v by the formula: ...|$|E
500|$|The same {{definition}} {{also works}} when f {{is a function}} with values in Rm. The above definition is applied to each component of the vectors. [...] In this case, the <b>directional</b> <b>derivative</b> is a vector in Rm.|$|E
500|$|Differentiation {{can also}} be defined for maps between {{infinite}} dimensional vector spaces such as Banach spaces and Fréchet spaces. [...] There is a generalization both of the <b>directional</b> <b>derivative,</b> called the Gâteaux derivative, and of the differential, called the Fréchet derivative.|$|E
40|$|This paper {{develops}} {{methodology for}} local sensitivity analysis based on <b>directional</b> <b>derivatives</b> associated with spatial processes. Formal gradient analysis for spatial processes was elaborated in previous papers, focusing on distribution theory for <b>directional</b> <b>derivatives</b> {{associated with a}} response variable assumed to follow a Gaussian process model. In the current work, these ideas are extended to additionally accommodate a continuous covariate whose <b>directional</b> <b>derivatives</b> are also of interest and to relate {{the behavior of the}} <b>directional</b> <b>derivatives</b> of the response surface to those of the covariate surface. It is of interest to assess whether, in some sense, the gradients of the response follow those of the explanatory variable. The joint Gaussian structure of all variables, including the <b>directional</b> <b>derivatives,</b> allows for explicit distribution theory and, hence, kriging across the spatial region using multivariate normal theory. Working within a Bayesian hierarchical modeling framework, posterior samples enable all gradient analysis to occur post model fitting. As a proof of concept, we show how our methodology can be applied to a standard geostatistical modeling setting using a simulation example. For a real data illustration, we work with point pattern data, deferring our gradient analysis to the intensity surface, adopting a log-Gaussian Cox process model. In particular, we relate elevation data to point patterns associated with several tree species in Duke Forest...|$|R
40|$|We {{describe}} and study geometric properties of discrete circular and spherical means of <b>directional</b> <b>derivatives</b> of functions, {{as well as}} discrete approximations of higher order differential operators. For an arbitrary dimension we present a general construction for obtaining discrete spherical means of <b>directional</b> <b>derivatives.</b> The construction is based on using the Minkowski's existence theorem and Veronese maps. Approximating the <b>directional</b> <b>derivatives</b> by appropriate finite differences allows one to obtain finite difference operators with good rotation invariance properties. In particular, we use discrete circular and spherical means to derive discrete approximations of various linear and nonlinear first- and second-order differential operators, including discrete Laplacians. A practical potential of our approach is demonstrated by considering applications to nonlinear filtering of digital images and surface curvature estimation...|$|R
5000|$|Next, {{choose a}} set of basis vectors [...] and {{consider}} the operators, noted , that perform <b>directional</b> <b>derivatives</b> in the directions of : ...|$|R
500|$|This is λ {{times the}} {{difference}} quotient for the <b>directional</b> <b>derivative</b> of f {{with respect to}} u. Furthermore, taking the limit as h tends to zero {{is the same as}} taking the limit as k tends to zero because h and k are multiples of each other. [...] Therefore, [...] [...] Because of this rescaling property, directional derivatives are frequently considered only for unit vectors.|$|E
500|$|If {{the total}} {{derivative}} exists at a, {{then all the}} partial derivatives and directional derivatives of f exist at a, and for all v, [...] is the <b>directional</b> <b>derivative</b> of f in the direction v. [...] If we write f using coordinate functions, so that , then the total derivative can be expressed using the partial derivatives as a matrix. [...] This matrix is called the Jacobian matrix of f at a: ...|$|E
2500|$|The <b>directional</b> <b>derivative</b> of a {{function}} {{with respect to}} a vector field {{is one of the most}} fundamental concepts in differential geometry, and the Lie derivative of {{a function}} is simply defined to be the <b>directional</b> <b>derivative</b> of the function. The precise definition of the <b>directional</b> <b>derivative</b> depends on what formalization one is using for vector fields. Two common formalizations exist: ...|$|E
40|$|Abstract. In this paper, {{the exact}} {{value for the}} norm of <b>directional</b> <b>derivatives,</b> of all orders, for {{symmetric}} tensor powers of operators on finite dimensional vector spaces is presented. Using this result, an upper bound for the norm of all <b>directional</b> <b>derivatives</b> of immanants is obtained. This work is inspired in results by R. Bhatia, J. Dias da Silva, P. Grover, and T. Jain. Key words. Norms, Derivatives, Compound matrix, Induced power of a matrix, Immanant...|$|R
40|$|Previous {{papers have}} {{elaborated}} formal gradient analysis for spatial processes, {{focusing on the}} distribution theory for <b>directional</b> <b>derivatives</b> associated with a response variable assumed to follow a Gaussian process model. In the current work, these ideas are extended to additionally accommodate one or more continuous covariate(s) whose <b>directional</b> <b>derivatives</b> are of interest and to relate {{the behavior of the}} <b>directional</b> <b>derivatives</b> of the response surface to those of the covariate surface(s). It is of interest to assess whether, in some sense, the gradients of the response follow those of the explanatory variable(s), thereby gaining insight into the local relationships between the variables. The joint Gaussian structure of the spatial random effects and associated <b>directional</b> <b>derivatives</b> allows for explicit distribution theory and, hence, kriging across the spatial region using multivariate normal theory. The gradient analysis is illustrated for bivariate and multivariate spatial models, non-Gaussian responses such as presence-absence and point patterns, and outlined for several additional spatial modeling frameworks that commonly arise in the literature. Working within a hierarchical modeling framework, posterior samples enable all gradient analyses to occur as post model fitting procedures. Dissertatio...|$|R
5000|$|... is not {{differentiable}} at , {{but all of}} {{the partial}} <b>derivatives</b> and <b>directional</b> <b>derivatives</b> exist at this point. For a continuous example, the function ...|$|R
2500|$|Therefore, any <b>directional</b> <b>derivative</b> can be {{identified}} with a corresponding vector, and any vector can {{be identified}} with a corresponding <b>directional</b> <b>derivative.</b> A vector can therefore be defined precisely as ...|$|E
2500|$|There {{are various}} ways {{to define the}} {{derivative}} of a function on a differentiable manifold, the most fundamental {{of which is the}} <b>directional</b> <b>derivative.</b> [...] The definition of the <b>directional</b> <b>derivative</b> is {{complicated by the fact that}} a manifold will lack a suitable affine structure with which to define vectors. [...] The <b>directional</b> <b>derivative</b> therefore looks at curves in the manifold instead of vectors.|$|E
2500|$|In this formalization, the <b>directional</b> <b>derivative</b> of a {{function}} {{can be defined}} using local coordinates as follows: the <b>directional</b> <b>derivative</b> of f {{with respect to a}} vector field X at a point p is the number ...|$|E
50|$|<b>Directional</b> <b>derivatives</b> can be also denoted by:where v is a {{parameterization}} of a curve {{to which}} v is tangent and which determines its magnitude.|$|R
40|$|Using {{standard}} {{nonlinear programming}} (NLP) theory, we establish formulas for {{first and second}} order <b>directional</b> <b>derivatives</b> for optimal value functions of parametric mathematical programs with complementarity constraints (MPCCs). The main point is that under a linear independence condition on the active constraint gradients, optimal value sensitivity of MPCCs {{is essentially the same}} as for NLPs, in spite of the combinatorial nature of the MPCC feasible set. Unlike NLP however, second order <b>directional</b> <b>derivatives</b> of the MPCC optimal value function show combinatorial structure. ...|$|R
40|$|Presently, the MAD Automatic Differentiation {{package for}} matlab {{comprises}} an overloaded implementation of forward mode AD via the fmad class. A key design {{feature of the}} fmad class is a separation of the storage and manipulation of <b>directional</b> <b>derivatives</b> into a separate derivvec class. Within the derivvec class, <b>directional</b> <b>derivatives</b> are stored as matrices (2 -D arrays) allowing {{for the use of}} either full or sparse matrix storage. All manipulation of <b>directional</b> <b>derivatives</b> is performed using high-level matrix operations - thus assuring efficiency. In this paper: we briefly review implementation of the fmad class; we then present our implementation of high-level interfaces allowing users to utilise MAD in conjunction with stiff ODE solvers and numerical optimization routines; we then demonstrate the ease and utility of this approach via several examples; we conclude with a road-map for future developments...|$|R
