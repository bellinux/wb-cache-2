131|274|Public
5000|$|Joshua, {{a machine}} {{translation}} <b>decoding</b> <b>system</b> written in Java ...|$|E
50|$|To {{receive this}} service, {{you will need}} a special {{receiver}} with a subcarrier <b>decoding</b> <b>system,</b> which the station gives information on where to get them. It is also available on live streaming online.|$|E
50|$|Synchronization of a <b>decoding</b> <b>system</b> with {{a channel}} is {{achieved}} {{through the use}} of the SCR in the program stream and by its analog, the PCR, in the transport stream. The SCR and PCR are time stamps encoding the timing of the bit stream itself, and are derived from the same time base used for the audio and video PTS values from the same program. Since each program may have its own time base, there are separate PCR fields for each program in a transport stream containing multiple programs. In some cases it may be possible for programs to share PCR fields.|$|E
40|$|Abstract. In this {{contribution}} {{we propose}} a 3 D-deblocking method for macroblock loss recovery in block-based video <b>decoding</b> <b>systems.</b> For a lost macroblock, the motion vector is estimated. Using the estimated motion vector a deblocking filter recovers the lost macroblock by the corresponding motion compensated image {{samples of the}} previous frame. Macroblock borders are filtered, if block artifacts are estimated by this deblocking filter. It is shown that in case of transmission errors this restoration technique can successfully be used in block-based video <b>decoding</b> <b>systems.</b> Index Terms Error concealment, video restoration, video coding and transmission...|$|R
3000|$|In {{order to}} reduce the {{complexity}} of <b>decoding</b> <b>systems,</b> Tensor <b>Decoding</b> Algorithm was used in [22, 23]. This algorithm consists of two main steps that are described below: [...]...|$|R
40|$|The {{means of}} {{telemetering}} digital {{data from a}} spacecraft to the Deep Space Network is addressed. Phase shift keying and uncoded quadriphase shift keying modulations are discussed along with demodulation and <b>decoding</b> <b>systems.</b> Telemetry system losses and noisy reference performance for suppressed carrier receivers are also examined...|$|R
5000|$|The optical soundtrack on a Dolby Stereo encoded 35 mm film carries {{not only}} [...] and [...] tracks for {{stereophonic}} sound, but also - through a matrix <b>decoding</b> <b>system</b> (Dolby Motion Picture matrix or Dolby MP) {{similar to that}} developed for [...] "quadraphonic" [...] or [...] "quad" [...] sound in the 1970s - a third center channel, and a fourth surround channel for speakers on the sides and rear of the theater for ambient sound and special effects. This yielded {{a total of four}} sound channels, as in the 4-track magnetic system, in the track space formerly allocated for one mono optical channel. Dolby also incorporated its A-Type noise reduction into the Dolby Stereo system.|$|E
50|$|As a comparison, {{consider}} the Quicksort algorithm. In the worst-case scenario, Quicksort makes O(n2) comparisons; however, such an occurrence is rare. Quicksort almost invariably makes O(n log n) comparisons instead, and even outperforms other algorithms which can guarantee O(n log n) behavior. Let us suppose an adversary wishes {{to force the}} Quicksort algorithm to make O(n2) comparisons. Then {{he would have to}} search all of the n! permutations of the input string and test the algorithm on each until he found the one for which the algorithm runs significantly slower. But since this would take O(n!) time, it is clearly infeasible for an adversary to do this. Similarly, it is unreasonable to assume an adversary for an encoding and <b>decoding</b> <b>system</b> would be able to test every single error pattern in order to find the most effective one.|$|E
5000|$|Since any {{computationally}} bounded adversary {{could in}} O(n) time flip a coin for each bit, it is intuitively clear that any encoding and <b>decoding</b> <b>system</b> which can work against this adversary must also {{work in the}} stochastic noise model. The converse is less simple; however, it can be shown that any system which works in the stochastic noise model can also efficiently encode and decode against a computationally bounded adversary, and only at an additional cost which is polynomial in n. The following method to accomplish this was designed by Dick Lipton, and is taken from:Let [...] be an encoder for the stochastic noise model and [...] be a simple decoder for the same, each of which runs in polynomial time. Furthermore, let both the sender and receiver share some random permutation function [...] and a random pattern [...]|$|E
40|$|Summary. Neurally {{controlled}} prosthetic devices {{capable of}} environmental manipulation have much potential towards restoring the physical functionality of disabled people. However, {{the number of}} user input variables provided by current neural <b>decoding</b> <b>systems</b> is {{much less than the}} number of control degrees-of-freedom (DOFs) of a prosthetic hand and/or arm. To address this sparse control problem, we propose the use of low-dimensional subspaces embedded within the pose space of a robotic limb. These subspaces are extracted using dimension reduction techniques to compress captured human hand motion into a (often two-dimensional) subspace that can be spanned by the output of neural <b>decoding</b> <b>systems.</b> To evaluate our approach, we explore a set of current state-of-the-art dimension reduction techniques and show results for effective control of a 13 DOF robot hand performing basic grasping tasks taking place in both static and dynamic environments. ...|$|R
50|$|Delay {{encoding}} is an encoding {{using only}} half the bandwidth for biphase encoding but features all the advantages of biphase encoding:To be rewritten: It is guaranteed to have transitions every other bit, meaning that <b>decoding</b> <b>systems</b> can adjust their clock/DC threshold continuously.One drawback is that it lacks easy human readability (e.g. on an oscilloscope).|$|R
40|$|Near-capacity {{performance}} may {{be achieved}} {{with the aid of}} iterative decoding, where extrinsic soft information is exchanged between the constituent decoders in order to improve the attainable system performance. Extrinsic information Transfer (EXIT) charts constitute a powerful semi-analytical tool used for analysing and designing iteratively <b>decoded</b> <b>systems.</b> In this tutorial, we commence by providing a rudimentary overview of the iterative decoding principle and the concept of soft information exchange. We then elaborate on the concept of EXIT charts using three iteratively <b>decoded</b> prototype <b>systems</b> as design examples. We conclude by illustrating further applications of EXIT charts, including near-capacity designs, the concept of irregular codes and the design of modulation schemes...|$|R
50|$|Hidden Markov Models {{were first}} applied to speech {{recognition}} by James K. Baker in 1975. Continuous speech recognition occurs {{by the following}} steps, modeled by a HMM. Feature analysis is first undertaken on temporal and/or spectral features of the speech signal. This produces an observation vector. The feature is then compared to all sequences of the speech recognition units. These units could be phonemes, syllables, or whole-word units. A lexicon <b>decoding</b> <b>system</b> is applied to constrain the paths investigated, so only words in the system's lexicon (word dictionary) are investigated. Similar to the lexicon decoding, the system path is further constrained {{by the rules of}} grammar and syntax. Finally, semantic analysis is applied and the system outputs the recognized utterance. A limitation of many HMM applications to speech recognition is that the current state only depends on the state at the previous time-step, which is unrealistic for speech as dependencies are often several time-steps in duration. The Baum-Welch algorithm also has extensive applications in solving HMMs used in the field of speech synthesis.|$|E
50|$|In late 2009, Li Deng invited Geoff Hinton to {{work with}} him and colleagues at Microsoft to apply deep {{learning}} to speech recognition. They co-organized the 2009 NIPS Workshop on Deep Learning for Speech Recognition. The workshop was motivated by the limitations of deep generative models of speech, and the possibility that the big-compute, big-data era warranted a serious try of deep neural nets (DNN). It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, Deng et al. at Microsoft soon after discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more advanced generative model-based speech recognition systems. This finding was verified by other research groups subsequently. Further, the nature of recognition errors produced by the two types of systems was found to be characteristically different, offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech <b>decoding</b> <b>system</b> deployed by all major speech recognition players.|$|E
5000|$|In late 2014, Meridian {{launched}} the Master Quality Authenticated (MQA) audio encoding {{system with the}} intention of enhancing the quality of digital music experienced via downloaded files or online streaming. MQA utilises a novel digital sampling technology that is capable of capturing extremely fine timing information accurately, down to the microsecond level, which Meridian claims is not possible with conventional techniques, even at high sample rates. Meridian maintains that current neuroscience indicates such timing accuracy is necessary for truly lifelike recording and reproduction. When an original master recording is MQA-encoded, this information is [...] "encapsulated" [...] into the bulk of the audio signal by [...] "folding" [...] it in two stages into a standard (typically 24-bit; 44.1 or 48 kHz sampling depending on the sample rate of the original signal) digital coding space but beneath the noise floor. The resulting data is transferred using any lossless audio format such as WAV, FLAC, Apple Lossless etc. As a result, the bandwidth required to stream an MQA file (or the size of a file for download) is significantly less than using conventional techniques, while retaining the full detail of the original master. On replay via an MQA decoder, the encapsulated detail is recovered, providing an exact representation of the original master recording, and this is confirmed ("authenticated") by the <b>decoding</b> <b>system,</b> generally illuminating an indicator light. If the information is replayed without an MQA decoder, the encapsulated data is ignored and audio quality somewhat better than Compact Disc (16-bit, 44.1 kHz sampling) is experienced. Initial listener reports appear to be positive. In 2015, Meridian Audio Ltd spun off MQA into its own company, MQA Limited, set up for the purpose in July 2014 and based at the Meridian address in Huntingdon, UK.|$|E
40|$|Abstract—The {{objective}} of dynamic voltage scaling (DVS) is {{to adapt the}} frequency and voltage for configurable platforms to ob-tain energy savings. DVS is especially attractive for video <b>decoding</b> <b>systems</b> due to their time-varying and highly complex workload and because the utility of decoding a frame is solely depending on the frame being decoded before its display deadline. Several DVS algorithms have been proposed for multimedia applications. How-ever, the prior work did {{not take into account}} the video compres-sion algorithm specifics, such as considering the temporal depen-dencies among frames and the required display buffer. Moreover, the effect of the passive (leakage) power when performing DVS for multimedia systems was not explicitly considered. In this paper, we determine the optimal scheduling of the active and passive states to minimize the total energy for video <b>decoding</b> <b>systems.</b> We pose our problem as a buffer-constrained optimization problem with a novel, compression-aware definition of processing jobs. We pro-pose low-complexity algorithms to solve the optimization problem and show through simulations that significant improvements can be achieved over state-of-the art DVS algorithms that aim to min-imize only the active power. Index Terms—Buffer control, dynamic voltage scaling (DVS), power optimization for multimedia systems, video compression. I...|$|R
40|$|Abstract—This {{correspondence}} {{presents an}} optimal soft-in soft-out (SISO) decoding algorithm for the binary image of Reed–Solomon (RS) codes {{that is based}} on Vardy and Be’ery’s optimal soft-in hard-out algorithm. A novel suboptimal list-based SISO decoder that exploits Vardy and Be’ery’s decomposition is also presented. For those codes with very high rate, which allows practical decoding with the proposed algorithms, the proposed suboptimal SISO significantly outperforms standard list-based decoding techniques in iteratively <b>decoded</b> <b>systems.</b> Index Terms—Graphical models, Reed–Solomon (RS) codes, soft-in soft-out (SISO) decoding...|$|R
40|$|Abstract Soft output Viterbi {{decoding}} {{has evolved}} {{as a key}} tech nology for advanced <b>decoding</b> <b>systems</b> during the recent years The article presents a modication of the soft output Viterbi algorithm and the resulting hardware architecture The new approach leads to storage savings while maintaining the low computational complexity of former approaches and is thus advantageous for hardware {{as well as for}} software implementations The complexity of soft output Viterbi decoding with the new approach is clearly less than twice that of hard decision Viterbi decodin...|$|R
40|$|An {{artificial}} {{neural network}} (ANN) <b>decoding</b> <b>system</b> decodes a convolutionally-encoded data stream at high speed and with high efficiency. The ANN <b>decoding</b> <b>system</b> implements the Viterbi algorithm and is significantly faster than comparable digital-only designs due to its fully parallel architecture. Several modifications to the fully analog system are described, including an analog/digital hybrid design that results in an extremely fast and efficient Viterbi <b>decoding</b> <b>system.</b> A complexity and analysis shows that the modified ANN <b>decoding</b> <b>system</b> is much simpler and easier to implement than its fully digital counterpart. The structure of the ANN <b>decoding</b> <b>system</b> of the invention provides a natural fit for VLSI implementation. Simulation {{results show that the}} performance of the ANN <b>decoding</b> <b>system</b> exactly matches that of an ideal Viterbi <b>decoding</b> <b>system.</b> Georgia Tech Research Corporatio...|$|E
40|$|AbstractWe {{evaluate}} {{the performance of}} the write-margin for the low-density parity-check (LDPC) coding and iterative <b>decoding</b> <b>system</b> in the bit-patterned media (BPM) R/W channel affected by the write-head field gradient, the media switching field distribution (SFD), the demagnetization field from adjacent islands and the island position deviation. It is clarified that the LDPC coding and iterative <b>decoding</b> <b>system</b> in R/W channel using BPM at 3 Tbit/inch 2 has a write-margin of about 20 %...|$|E
40|$|Abstract- Hidden Markov Models {{are used}} in {{different}} kinds of sequence recognition problems. Specially, Hidden Markov Models are suited for speech/speaker recognition systems. Due {{to the complexity of}} the algorithms involved, general-purpose computing solutions are typically significantly slower than real time. For many applications, however, real-time is essential and thus a system based in specific purpose hardware becomes necessary. For the probability computation in pattern recognition systems using Hidden Markov Models, a state <b>decoding</b> <b>system</b> is necessary. The state <b>decoding</b> <b>system</b> must be able to decide, based on the input sequence, which is the most probable state sequence that produces the input sequence and therefore the reference pattern which best matches with the input pattern. In this work, the implementation of a real-time Hidden Markov Model state <b>decoding</b> <b>system</b> is described. The prototype was implemented for left-right Markov Models. 1...|$|E
50|$|Referat IIb: Cryptanalysis and <b>decoding</b> of French <b>systems.</b>|$|R
50|$|Referat IIc: Cryptanalysis and <b>decoding</b> of Balkan <b>systems.</b>|$|R
30|$|In most cases, {{there is}} not enough {{residual}} capacity of battery to enable portable devices users to watch any video programs at any time as they wish, because of the exhausting battery. At the same time, in general video <b>decoding</b> <b>systems,</b> each module consumes a different amount of power and can affect a different rating of video quality. That is, the modules have different contributions in an environment with energy/battery constraint. Therefore, there is a tradeoff between maximum available lifetime of battery and minimum distortion caused by as possible as balanced decoding control.|$|R
40|$|The {{development}} of Reed-Solomon (RS) codes has allowed for improved data transmission over {{a variety of}} communication media. Although Reed-Solomon decoding provides a powerful defense against burst data errors, the significant circuit area and power consumption of customized RS decoder hardware can be limiting for embedded computing environments. To support enhanced performance decoding with minimal power consumption, a dynamicallyreconfigurable FPGA-based Reed-Solomon decoder has been developed. Our errors-and-erasures <b>decoding</b> <b>system</b> uses multiple erasure blocks to identify the location of likely corrupted data and multiple decoders to attempt error correction. The RS decoder design is implemented in reconfigurable hardware to leverage architectural parallelism and specialization. Run-time dynamic reconfiguration of the <b>decoding</b> <b>system</b> is used in response to variations in channel conditions to support the fastest possible data rate while, as a secondary metric, minimizing decoder power consumption. Algorithm parameters for the <b>decoding</b> <b>system</b> have been determined via simulation and the design has been implemented in Altera Stratix FPGAs. Through experimentation using an Altera 1 S 40 Stratix FPGA, we show that dynamic reconfiguration can result in an 14 % performance improvement versus a non-reconfigurable decoder implementation. Comparisons with a Pentium IV microprocessor illustrate five orders of magnitude performance improvement...|$|E
3000|$|... [...]. This {{suggests}} that, {{in order}} to maximize utility, all MBs in the video <b>decoding</b> <b>system</b> should try to improve the decoding effect while as possible as less consume the energy. So that the utility function is suitable for measuring power efficiency of video decoding systems.|$|E
40|$|Results of a {{study to}} {{evaluate}} the performance and implementation complexity of a concatenated and a hybrid coding system for moderate-speed deep-space applications. It is shown that with a total complexity of less than three {{times that of the}} basic Viterbi decoder, concatenated coding improves a constraint length 8 rate 1 / 3 Viterbi <b>decoding</b> <b>system</b> by 1. 1 and 2. 6 dB at bit error probabilities of 0. 0001 and one hundred millionth, respectively. With a somewhat greater total complexity, the hybrid coding system is shown to obtain a 0. 9 -dB computational performance improvement over the basic rate 1 / 3 sequential <b>decoding</b> <b>system.</b> Although substantial, these complexities are much less than those required to achieve the same performances with more complex Viterbi or sequential decoder systems...|$|E
40|$|A {{hierarchical}} system design flow {{was developed to}} facilitate concurrent development and Time-to-Market reductions. The system design flow provides for codesign of (embedded) driver software, digital hardware, and analogue hardware. The flow starts from a prosaic functional target specification, which is formally recorded in a system algorithm, in VHDL. Through functional decomposition and partitioning, individual parts of the system algorithm are projected onto software, digital hardware, and analogue hardware design flows, all based on VHDL. The hierarchical flow {{was applied to the}} design of channel and source <b>decoding</b> <b>systems</b> for Digital Video Broadcast applications. 1...|$|R
40|$|The {{genetic code}} {{appears to be}} {{optimized}} in its robustness to missense errors and frameshift errors. In addition, the genetic code is near-optimal {{in terms of its}} ability to carry information in addition to the sequences of encoded proteins. As evolution has no foresight, optimality of the modern genetic code suggests that it evolved from less optimal code variants. The length of codons in the genetic code is also optimal, as three is the minimal nucleotide combination that can encode the twenty standard amino acids. The apparent impossibility of transitions between codon sizes in a discontinuous manner during evolution has resulted in an unbending view that the genetic code was always triplet. Yet, recent experimental evidence on quadruplet decoding, as well as the discovery of organisms with ambiguous and dual decoding, suggest that the possibility of the evolution of triplet <b>decoding</b> from living <b>systems</b> with non-triplet <b>decoding</b> merits reconsideration and further exploration. To explore this possibility we designed a mathematical model of the evolution of primitive digital coding <b>systems</b> which can <b>decode</b> nucleotide sequences into protein sequences. These coding systems can evolve their nucleotide sequences via genetic events of Darwinian evolution, such as point-mutations. The replication rates of such coding systems depend on the accuracy of the generated protein sequences. Computer simulations based on our model show that <b>decoding</b> <b>systems</b> with codons of length greater than three spontaneously evolve into predominantly triplet <b>decoding</b> <b>systems.</b> Our findings suggest a plausible scenario for the evolution of the triplet genetic code in a continuou...|$|R
30|$|This paper {{proposed}} ESVD {{framework in}} power control video <b>decoding</b> <b>systems.</b> It aims at providing the scalable decoding output which is adaptive to energy resource. It proposed {{a method to}} make the video decoder adapt resource under battery constraint, which can be widely used in handheld devices. At the same time, it gives a method to maximum video decoding quality when playing on portable terminals, through building a decoding information database. The experiments demonstrate the efficiency of ESVD. In future research, {{we will try to}} study fine-grained energy scalable control in energy consumption through improving the scalability of each decoding module.|$|R
40|$|In {{comparison}} with incremental and traditional absolute encoder, the single-ring absolute encoder processes more advantage. And {{the research of}} <b>decoding</b> <b>system</b> and algorithm {{are essential to the}} single-ring absolute encoder. In order to improve single-ring absolute photoelectric encoder&# 39;s circuit integration, response speed and accuracy, a <b>decoding</b> <b>system</b> was designed in this paper. Linear CCD was used in this system as the disc image receiving and photoelectric conversion device which also provided the basement of subdivision algorithm. And FPGA was used to achieve circuit control and decoding algorithm. A new decoding algorithm was also proposed in this paper. On the one hand, the coding information was achieved by counting high electrical level of the binaryzation signal of CCD&# 39;s output. Coding information was obtained by recognition of the disc image and combination of 12 -bits coding number. On the other hand, precise value was obtained by calculating the shifting between the centroid of image and virtual center. The centroid of stripe was calculated based on CCD centroid location algorithm. Finally, the value of angle was precisely received from combination of these two values. A prototype of theodolite was developed based on this single-ring absolute encoder <b>decoding</b> <b>system</b> and can reach the accuracy of 2 ". © 2017, Editorial Board of Journal of Infrared and Laser Engineering. All right reserved. </p...|$|E
40|$|Abstract—The {{presented}} compression {{scheme is}} a novel solution {{that is based on}} deterministic vector clustering and encompasses three data reduction features in one on-chip <b>decoding</b> <b>system.</b> The approach preserves all benefits of continuous flow decompression and offers compression ratios of order 1000 x with encoding efficiency much higher than 1. 00. 1...|$|E
40|$|Abstract: We {{propose a}} 1 D barcode {{acquisition}} and <b>decoding</b> <b>system</b> {{based on the}} use of J 2 ME enabled mobile phones. The approach relies on image processing techniques to correct the distortions introduces by the acquisition device, segments the useful information, decodes it and subsequently sends it to a web server for further processing...|$|E
40|$|Herein {{we present}} the tRNA core hypothesis, which {{emphasizes}} {{the central role}} of tRNAs molecules in the origin and evolution of fundamental biological processes. tRNAs gave origin to the first genes (mRNA) and the peptidyl transferase center (rRNA), proto-tRNAs were at the core of a proto-translation system, and the anticodon and operational codes then arose in tRNAs molecules. Metabolic pathways emerged from evolutionary pressures of the <b>decoding</b> <b>systems.</b> The transitions from the RNA world to the ribonucleoprotein world to modern biological systems were driven by three kinds of tRNAs transitions, to wit, tRNAs leading to both mRNA and rRNA...|$|R
40|$|Several recent {{publications}} {{have shown}} that joint sourcechannel decoding (JSCD) could be a powerful technique for taking advantage of residual source redundancy. Furthermore, {{it appeared that the}} principle could also be successfully applied to variable length codes (VLCs). This paper gives an in-depth analysis of a new method recently proposed in [1], [2] to provide a low complexity soft decoding of VLCs with and without channel coding. Furthermore we also present new results obtained in a video coding context, using a VLC table from the MPEG 4 video standard. Compared to separated <b>decoding</b> <b>systems,</b> the gain in signal to noise ratio varies from 0. 3 dB up to 2 dB...|$|R
40|$|Abstract—Following {{the work}} of Méasson, Montanari, and Ur-banke, this paper {{considers}} the maximum a posteriori (MAP) de-coding thresholds of three iterative <b>decoding</b> <b>systems.</b> First, irreg-ular repeat-accumulate (IRA) and accumulate-repeat-accumulate (ARA) code ensembles are analyzed on the binary erasure channel (BEC). Next, the joint iterative decoding of LDPC codes is studied on the dicode erasure channel (DEC). The DEC is a two-state intersymbol-interference (ISI) channel with erasure noise, {{and it is the}} simplest example of an ISI channel with erasure noise. The MAP threshold bound for the joint decoder is based on a slight generalization of the EXIT area theorem. Index Terms—MAP threshold, iterative decoding, LDPC codes, EXIT function, erasure channel. I...|$|R
