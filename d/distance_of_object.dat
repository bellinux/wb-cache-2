8|10000|Public
40|$|It is {{important}} for recommendation system of E business to predicate consumer's habits by eyes gaze tracking. We explore algorithms to identify people's enjoyment by referencing Find s algorithm，it includes getting the weight of eyes parameters by getting the <b>distance</b> <b>of</b> <b>object</b> examples and negative examples，eyes parameters include the eye gazing time，the pupil size，the blink times and the looking back times. We use SEBET (Semantic Extraction Based Eye Tracking) algorithm to calculate the <b>distance</b> <b>of</b> <b>object</b> examples and negative examples，so we can decide whether consumers enjoy the goods or not from the distance，we extract the emotion semantic from eye tracking successfully. Experiments show the efficiency of our algorithm...|$|E
40|$|Session - Digital Holographic Optical Processing (DT 4 A) : paper DT 4 A. 4 In optical {{scanning}} holography, extracting <b>distance</b> <b>of</b> <b>object</b> is an indispensable step for numerical reconstruction. In this paper, we use entropy as a measurement to achieve autofocusing under different situations. © 2015 OSApostprin...|$|E
40|$|Control system enables {{robot hand}} to grasp objects of varied shapes. Key {{features}} of system: reflective proximity sensors furnishing data on position, orientation, and <b>distance</b> <b>of</b> <b>object</b> and software protocol controlling {{sequence of operations}} in approaching and grasping objects. Reflected-beam sensing concept applied to simple opposed-jaw industrial grippers {{as well as to}} dexterous robot hands...|$|E
40|$|This {{activity}} {{is designed to}} give students {{a better understanding of}} what parallax is and to gain some experience in using it. They will devise a method to calculate the <b>distance</b> <b>of</b> <b>objects</b> using observational data, describe how our eyes use parallax to determine the <b>distance</b> <b>of</b> <b>objects,</b> and make estimates <b>of</b> <b>distances</b> using parallax. Educational levels: High school...|$|R
6000|$|... 'The <b>distance</b> <b>of</b> <b>objects</b> across {{water is}} very deceiving,' he said. 'It is from eight to nine miles to those {{buildings}} you see.' ...|$|R
6000|$|... "The <b>distance</b> <b>of</b> <b>objects</b> across {{water is}} very deceiving," [...] he said. [...] "It is from eight to nine miles to those {{buildings}} you see." ...|$|R
40|$|Now {{have been}} made control {{position}} system of lens experiment by serial port communication and stepper motor. These system contains of software and interfecing peripheral such us stepper motor, lens, stepper motor driver, and RS 232. The software on microcontroller system use assembler programming and view of monitor interface use Delphi 7. 0 language. These system use stepper motor which able to rotate eight degree for each step use fullstep methode. System motor connecting to sprocket to make moving of light source and lamp position. The rotating of stepper motor controlled by computer with stepper motor controlling program based on Delphi 7. 0. Controlling of stepper motor use serial port interfacing from computer which current and voltage level have been convert by MAX 232 to match with microcontroller and stepper motor driver. On interfacing of application view will get the distance of shadow and <b>distance</b> <b>of</b> <b>object.</b> Trial result show us that control system of lens experiment position have the <b>distance</b> <b>of</b> <b>object</b> y = (0, 1623 x + 17, 426) cm, and it’s equal to 0, 004 cm. And for the distance of shadow y = (0, 1622 x + 17, 031) cm and it,s equal to 0, 005 cm...|$|E
40|$|In {{order to}} {{reconstruct}} the viewable surface of an object completely, multiple views of the same object have to be used and integrated into a common coordinate system. One of the major problems of the 3 d surface reconstruction using a turntable, is the varying resolution in the direction to the camera, due to the varying <b>distance</b> <b>of</b> <b>object</b> points to the rotational axis of the turntable. To guarantee a uniform object resolution, we calculate the next angle dynamically, depending on the entropy of the surface part actually acquired. To minimize the loss of information and to guarantee a uniform surface resolution, we derive a relation between the entropy and the next viewing angle, based on the profile sections acquired {{in the last two}} steps of the acquisition. 1 Introduction The reconstruction of the viewable object surface requires images from multiple views. The number of acquisition steps and the respective orientation of the camera relative to the object surface are unknown for arb [...] ...|$|E
40|$|Video {{summarization}} is {{a process}} to extract objects and their activities from a video and represent them in a condensed form. Existing methods for video summarization fail to detect moving (dynamic) objects in the low color contrast area of a video frame due to the pixel intensities of objects and non-objects are almost similar. However, edges of objects are prominent in the low contrast regions. Moreover, to represent objects, geometric primitives (such as lines, arcs) are distinguishable and high level shape descriptors than edges. In this paper, a novel method is proposed for video summarization using geometric primitives such as conic parts, line segments and angles. Using these features, objects are extracted from each video frame. A cost function is applied to measure the dissimilarity of locations of geometric primitives to detect the movement of objects between consecutive frames. The total <b>distance</b> <b>of</b> <b>object</b> movement is calculated and each video frame is assigned a probability score. Finally, a set of key frames is selected based on the probability scores as per user provided skimming ratio or system default skimming ratio. The proposed approach is evaluated using three benchmark datasets-BL- 7 F, Office, and Lobby. The experimental results show that our approach outperforms the state-of-the-art method in terms of accuracy...|$|E
50|$|In a {{computer}} graphics, draw distance (render distance or view distance) is the maximum <b>distance</b> <b>of</b> <b>objects</b> in a three-dimensional scene that are {{drawn by the}} rendering engine. Polygons that lie beyond the draw distance will not be drawn to the screen.|$|R
50|$|Convergence {{micropsia}} {{implies that}} the state of convergence of the eyes contributes to determining the <b>distance</b> <b>of</b> an <b>object</b> from the eyes, that it acts as a depth cue. At six meters, to view an object without double vision the optic axes of the eyes are essentially parallel (no convergence). At closer distances, to view an object without double vision the optic axes need to approach each other at an angle (increasing convergence). Normally, the convergence angle gives accurate information about the <b>distance</b> <b>of</b> <b>objects.</b> Under the conditions that yield convergence micropsia, the overconverged vergence angle specifies a shorter distance than the actual <b>distance</b> <b>of</b> the object; this affects apparent size.|$|R
30|$|Depth {{perception}} is the visual ability {{to judge the}} relative <b>distance</b> <b>of</b> <b>objects</b> and the spatial relationship <b>of</b> <b>objects</b> at different <b>distances.</b> As the three-dimensional world projects onto a two-dimensional retina, this projection on its own cannot provide depth information. The brain has to combine various monocular and binocular cues given by the eyes to recover the depth, distance, and three-dimensional shape <b>of</b> <b>objects.</b>|$|R
40|$|The {{physical}} {{principle of}} flat coil sensor {{was based on}} the changing inductivity of a flat coil due to disturbance of conductive material in its electromagnetic fields, so that eddy current on the conductive material was  occurred. The displacement between flat coil and conductive material was a function of the total inductance L of the sensor system, which will be measured as resonance frequency by using an inductive  capacitive oscillator. The measurement system consists of a flat coil, oscillator circuit LC, multimeter and micrometer. In measuring, as independent variables were <b>distance</b> <b>of</b> <b>object</b> and time, while dependent variable was output voltage of flat coil sensor by using differential technique. Data was collected through two ways i. e. direct and indirect measurement. Then data was analyzed by using graph methods and error analysis. Data analysis shown that: 1).   The output sensor without differential technique isn’t linear with distance of conductive material; 2). The output sensor with differential technique is inversely proportional with distance of conductive material  with negative sensitivity 1, 2783 Volt/mm; 3). The precision of sensor was high with average of precision is 0, 999, and 4). The stability of sensor was also high with small output voltage variation.   Key words: sensor, flat coil, characteristics, sensitivity, precision, stabilit...|$|E
40|$|Introduction: Many methods {{exist for}} {{evaluating}} ergonomic {{risk factors for}} LBP at workplace, including biomechanical, physiological and psycho-physical methods. Digital Human Modeling (DHM) as a tool based on computer for ergonomic evaluation that Because having advantages such as saving time and costs in assessment and actively evaluation of ergonomic solutions in the digital environment. Aim {{of this study was}} evaluation occupational causes of LBP with the use of digital human modeling.. Method and Materials: This study was a case quasi-experimental study in the engine assembly of the car manufacturing industry was conducted. First, The characteristics of job and risk factors for low back pain In all workstations were documented, then workstations with high risk of low back disorders were selected. Finally, a workstation for simulating and evaluating in the human digital modeling softwares, which includes 3 DSSPP and Catia were selected. A total 22 posture of the lifting and lowering moment of the three tasks of workstation selected for simulation. After evaluation in the digital environment, the risk areas identified and solutions were presented.. Results: The results showed that compressive and shear forces on the L 5 /S 1 disc increased with increase of anthropometric dimensions and Horizontal distance from the body and decrease of height Lowering the site, and the percentage of people capable to perform a task in joint, decreased with increase of anthropometric dimensions. Ligament strain in postures with sever bending trunk were more increasing. RULA scores increased with the Unsuitable conditions of back and arms. High risk areas, were mainly related to the low height of pallets in lifting and lowering and high Horizontal distance from the body.. Conclusion: According to The results of this study, Biomechanical Causes for LBP, including anthropometric characteristics (height and weight), Horizontal <b>distance</b> <b>of</b> <b>object</b> to the body, height of lifting and lowering location and trunk bending and torsion. This study showed that DHM is an effective tool in the evaluation of job tasks and workplace design, that can be identified risk area in each task and to achieve the ideal design. Using DHM can be implement Desired solution in a virtual environment and With the re-evaluation, Be sure of having effective solutions in the real environment...|$|E
40|$|AbstractInsects {{depend upon}} optic flow to supply {{much of their}} {{information}} about the three-dimensional structure of the world. Many insects use translational flow to measure the <b>distance</b> <b>of</b> <b>objects</b> from themselves. A recent study has provided {{new insights into the}} way Drosophila use optic flow to pick out a close target to approach...|$|R
30|$|Compute the Euler <b>distance</b> <b>of</b> every <b>object</b> to K class {{clustering}} {{centers and}} distance matrix D(i,j).|$|R
50|$|Improving {{measurements}} were continually checked and cross-checked {{by means of}} {{our understanding of the}} laws of celestial mechanics, which govern the motions <b>of</b> <b>objects</b> in space. The expected positions and <b>distances</b> <b>of</b> <b>objects</b> at an established time are calculated (in AU) from these laws, and assembled into a collection of data called an ephemeris. NASA Jet Propulsion Laboratory provides one of several ephemeris computation services.|$|R
40|$|Computing object {{distance}} using image processing {{is an important}} research area {{in the field of}} computer vision and robot navigation applications. In this paper we have proposed a new method to compute the <b>distance</b> <b>of</b> an <b>object</b> using a single image. According to our observation there exists a relationship between the physical <b>distance</b> <b>of</b> an <b>object</b> and its pixel height. We exploit this relationship to train a system that finds a mapping between an object’s pixel height and physical distance. This mapping is then used to find the physical <b>distance</b> <b>of</b> test <b>objects</b> from the pixel height in the image. Experimental results demonstrate the capability of our proposed technique by estimating physical distance with accuracy as high as 98. 76 %...|$|R
50|$|Canonical {{units are}} useful when the precise <b>distances</b> and masses <b>of</b> <b>objects</b> in space are not available. However, by setting the mass <b>of</b> a given <b>object</b> to be 1 mass unit {{and the mean}} <b>distance</b> <b>of</b> the {{reference}} <b>object</b> to another object in question, many calculations can be simplified.|$|R
40|$|Abstract—Finding {{the size}} and <b>distance</b> <b>of</b> <b>objects</b> viewed through a single optical path—a typical {{endoscopic}} condition—is a challenge. Stereo imaging, which would provide {{the size and}} distance information, typically requires multiple optical channels or other elaborate techniques, increasing the endoscope diameter and system complexity. This paper reports {{the development of a}} miniature flipping disk device that enables stereo measurement, and demonstrates the operation of the device installed in an endo-scope. The fabricated Pyrex disk (1. 2 mm in diameter and 485 μm in thickness), anodically bonded to a 50 -μm-thick silicon vertical comb-drive actuator, is flipped to ± 10. 7 ◦ at a resonant frequency of 414 Hz by 10 VAC of electrostatic actuation. The image shift made by such glass flipping provides a reference dimension, against which the real size and <b>distance</b> <b>of</b> <b>objects</b> seen through a standard single-channel endoscope can be estimated. [2011 - 0320] Index Terms—Electrostatic actuation, endoscope, endoscopic observation, optical microelectromechanical systems (MEMS) ...|$|R
40|$|This master's thesis {{will take}} us through {{theoretical}} procedure {{that allows us to}} determine the <b>distance</b> <b>of</b> an <b>object</b> by stereoscopic sensor. Part of this work presents the description of the steps to achieve image <b>of</b> <b>objects,</b> calibraton and rectification. At the next part our study provides an overview of algortihms for creating disparity maps and determining the <b>distance</b> <b>of</b> the <b>object</b> from sensor. In the following part of thesis deals with the implementation of these processes into aplication which aim is to measure the distance...|$|R
5000|$|To {{identify}} the unknown shape, use a nearest-neighbor classifier to compare its shape distance to shape <b>distances</b> <b>of</b> known <b>objects.</b>|$|R
5000|$|Depth {{perception}} is the visual ability {{to perceive the}} world in three dimensions (3D) and the <b>distance</b> <b>of</b> an <b>object.</b> Depth sensation is the corresponding term for animals, since although {{it is known that}} animals can sense the <b>distance</b> <b>of</b> an <b>object</b> (because <b>of</b> their ability to move accurately, or to respond consistently, according to that distance), it is not known whether they [...] "perceive" [...] it in the same subjective way that humans do.|$|R
40|$|Various {{hypotheses}} {{derived from}} Gibson's psychophysical theory of perception are experimentally tested. Results show that judgments {{of the relative}} physical sizes and relative physical <b>distances</b> <b>of</b> <b>objects,</b> made over variously textured surfaces, are strongly influenced by the texture density gradients of stimulation, derived from the surfaces. Results, however, also indicate that, in isolation, the monocular stimulation gradient of texture density cannot supply observers with sufficient "stimulus information" to enable their phenomenal judgments of the spatial ordering of the environment to accord exactly with the actual physical dimensions of the environment. Despite the presence of well defined texture density gradients of stimulation, Ss' judgments of both the relative physical sizes and the relative physical <b>distances</b> <b>of</b> <b>objects</b> are influenced by, theoretically incidental, extraneous stimuli. Ss' judgments also show variations under conditions of monocular or binocular vision. Ss' judgments of the relative physical <b>distances</b> <b>of</b> certain <b>objects</b> are shown to be significantly related to their judgments of the relative physical sizes <b>of</b> these <b>objects,</b> in accordance with predictions derived from the "Size-Distance Invariance Hypothesis. " Despite {{the presence of a}} well defined texture density gradient of stimulation, relative differences in the retinal image sizes <b>of</b> various <b>objects</b> are seen to influence judgments <b>of</b> the relative <b>distances</b> <b>of</b> the <b>objects,</b> both when the objects are dissimilar in shape and, even when Ss are not manipulatively familiar with the objects. Results are discussed in relation to Gibson's theory (considered as a "cue theory" of perception). Several directives for future research projects are suggested. In particular, results of a preliminary investigation indicate that following a period of "texture deprivation", during which translucent goggles are worn, Ss become more responsive to textural factors in the environment. It is suggested that specific forms of sensory deprivation may create specific types of "stimulus hunger. "...|$|R
60|$|While {{there was}} yet a little daylight, our hunter looked well about him; {{took note of}} the exact {{position}} of the fence, {{the entrance to the}} enclosure, and the grave; judged the various <b>distances</b> <b>of</b> <b>objects,</b> and arranged the sights of the rifle, which was already loaded with a brace of hardened balls. Then he looked up through the tree-tops and wished for darkness.|$|R
60|$|Edgar {{explained}} {{to him that the}} sight was raised or lowered according to the <b>distance</b> <b>of</b> the <b>object</b> to be aimed at.|$|R
40|$|ABSTRACT Among various {{range finding}} methods stereo vision {{is worthy of}} notice since it needs no active media. However because of {{correspondence}} problem, the stereo vision has not been fully established as a computer vision system. In this paper we used fuzzy logic to bypass the correspondence problem in stereo vision for calculating the <b>distances</b> <b>of</b> <b>objects.</b> Index Terms — Correspondence, disparity, Mamdani’s fuzzy model, 3 -d reconstruction...|$|R
2500|$|Notice {{that this}} [...] "centrifugal force" [...] has {{differences}} {{from the case}} of a rotating frame. In the rotating frame the centrifugal force is related to the <b>distance</b> <b>of</b> the <b>object</b> from the origin of frame B, while in {{the case of a}}n orbiting frame, the centrifugal force is independent <b>of</b> the <b>distance</b> <b>of</b> the <b>object</b> from the origin of frame B, but instead depends upon the <b>distance</b> <b>of</b> the origin of frame B from its center of rotation, resulting in the same centrifugal fictitious force for all objects observed in frame B.|$|R
30|$|Select next initial {{centroids}} from n-objects {{in such a}} way so {{that the}} Euclidean <b>distance</b> <b>of</b> that <b>object</b> is maximum from other selected initial centroids.|$|R
50|$|Convergence {{micropsia}} {{is a type}} of micropsia {{characterized by}} the reduction in apparent size <b>of</b> <b>objects</b> viewed when the eyes are more converged than they need to be for the <b>distance</b> <b>of</b> the <b>object</b> from the eyes.|$|R
50|$|Light-time {{correction}} {{occurs in}} principle during the observation <b>of</b> any moving <b>object,</b> because {{the speed of}} light is finite. The magnitude and direction of the displacement in position depends upon the <b>distance</b> <b>of</b> the <b>object</b> from the observer and the motion <b>of</b> the <b>object,</b> and is measured at the instant at which the object's light reaches the observer. It is independent of the motion of the observer. It should be contrasted with the aberration of light, which depends upon the instantaneous velocity of the observer at the time of observation, and is independent of the motion or <b>distance</b> <b>of</b> the <b>object.</b>|$|R
50|$|There {{are also}} other styles of tape {{measures}} that have incorporated lasers and ultrasonic technology {{to measure the}} <b>distance</b> <b>of</b> an <b>object</b> with fairly reliable accuracy.|$|R
30|$|We propose {{two types}} of {{collision}} alarms for our vision system, {{the first is a}} contact type and the second is a non-contact type. The collision alarms are designed to visualize the risk of collisions in the image. They provide visual signs to warn about collisions instead of displaying the exact <b>distance</b> <b>of</b> <b>objects.</b> Since it is important to pass objects during the avoidance <b>of</b> <b>objects</b> in front, our collision alarms are adjusted to warn about collisions on the left and right sides of the robot.|$|R
40|$|Browsing large {{information}} spaces such as maps on {{the limited}} screen of mobile devices often requires people to perform panning and zooming operations that move relevant display content offscreen. This {{makes it difficult}} to perform spatial tasks such as finding the location of Points Of Interest (POIs) in a city. Visualizing the location <b>of</b> off-screen <b>objects</b> can mitigate this problem: in this paper, we present a user study comparing the Halo [2] approach with two other techniques based on arrows. Halo surrounds off-screen objects with circles that reach the display window, so that users can derive the location and <b>distance</b> <b>of</b> <b>objects</b> by observing the visible portion of the corresponding circles. In the two arrow-based techniques, arrows point at objects and their size and body length, respectively, inform about the <b>distance</b> <b>of</b> <b>objects.</b> Our study involved four tasks requiring users to identify and compare off-screen objects locations, and also investigated the effectiveness of the three techniques with respect to the number <b>of</b> off-screen <b>objects.</b> Arrows allowed users to order off-screen objects faster and more accurately according to their distance, while Halo allowed users to better identify the correct location <b>of</b> off-screen <b>objects.</b> Implications <b>of</b> these results for mobile map-based applications are also discussed...|$|R
40|$|Depth {{perception}} {{comes naturally}} to humans. However in a Computer Vision scenario estimation <b>of</b> <b>distance</b> between <b>object</b> and camera {{is an area}} still under research. This thesis aims to use binocular stereo vision to reconstruct a 3 D scene from 2 D images of the scene taken {{by a pair of}} cameras and use it to estimate the <b>distance</b> <b>of</b> the <b>object</b> from the camera. Further, this estimated distance is used to calculate the Zoom of a PTZ camera. Although there are various ways to determine the <b>distance</b> <b>of</b> an <b>object</b> from the camera using Sensors, Lasers and other such external devices, the method used in this thesis is independent of the use of such external devices and uses only image processing techniques to determine the distance. Results obtained and the process are clearly outlined...|$|R
40|$|In digital holography, {{computing}} {{a focused}} image <b>of</b> an <b>object</b> requires a prior knowledge <b>of</b> the <b>distance</b> <b>of</b> the <b>object</b> from the camera. When this distance is not known, {{it is necessary}} to repeat the image reconstruction at a range <b>of</b> <b>distances</b> followed by evaluation of each image with a sharpness metric to determine the in-focus <b>distance</b> <b>of</b> the <b>object.</b> Here, we present a method to find the focus distance by processing the image transverse to the <b>object</b> plane instead <b>of</b> the processing in the image plane as it is usually done. Since the reconstructed hologram image is spatially symmetric around the focus point along the propagation axis, simply finding the symmetry points in the image cross-section specifies the focus location, and no other sharpness metrics are necessary to use. Also with this method, it is possible to find the focus <b>distances</b> <b>of</b> multiple <b>objects</b> simultaneously, including the phase only objects without any staining. We will present the simulations and the experimental results obtained by a digital holographic microscope...|$|R
