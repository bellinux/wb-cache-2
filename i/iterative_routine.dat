19|59|Public
40|$|The {{estimation}} of commodity spot price models often involves the {{estimation of}} risk premiums. We show in a simulation study that the market prices of risk cannot be accurately estimated using two popular estimation techniques; the Kalman filter and an <b>iterative</b> <b>routine.</b> Risk premium parameters may be dependent on the starting value for the <b>iterative</b> <b>routine,</b> and cannot be accurately estimated using the Kalman filter technique. We conclude with a short analysis of results from the spot price model literature by examining the implied volatility term structure from other published research papers...|$|E
30|$|Once the <b>iterative</b> <b>routine</b> has converged, a final bundle {{adjustment}} scheme {{can be implemented}} using non-linear projection maps [16, 17]. This would insure a more accurate alignment, and eventually correct for any morphological distortions [18]. For multiple tilt series however, this non-linear bundle adjustment should be carried out when the entire procedure is completed.|$|E
40|$|The {{authors have}} derived {{a model for}} {{predicting}} the roughness coefficient of an alluvial river. Their approach was verified with hundreds of laboratory and prototype observations. However, the computation needs an <b>iterative</b> <b>routine,</b> and the main parameters are not obviously detected. Given in this discussion are the following: (1) simplifications for (4) and the Shields’ function; and (2) explicit expression for the roughness coefficient n...|$|E
40|$|Abstract. Flash-algorithm track-reconstruction {{routines}} {{with speed}} factors 3000 - 4000 in execess those of traditional <b>iterative</b> <b>routines</b> are presented. The methods were successfully {{tested in the}} alignment of the Test Beam setup for the ATLAS Pixel Detector MCM-D modules yielding a 60 fold increase in alignment resolution over <b>iterative</b> <b>routines,</b> {{for the same amount}} of alocated CPU time. ...|$|R
40|$|This paper {{establishes}} {{the convergence of}} <b>iterative</b> <b>routines</b> formatrix balancing in which some row and some column totals must equal prespecified positive numbers. This balancing procedure is called a modified biproportional problem. Applications of the procedure are cited, various theoretical properties discussed, and {{its relationship with the}} full biproportional problem, treated in an earlier paper (Macgill, 1977 a), is noted. ...|$|R
40|$|This paper {{describes}} {{the design and}} implementation of a parallel iterative linear system solver for distributed memory multicomputers and workstation clusters. It is capable of applying heterogeneous data distribution and dynamic load balancing within an <b>iterative</b> solver <b>routine</b> at matrix level. Matrices as well as vectors are distributed heterogeneously according to the available performances of the processors...|$|R
30|$|The overall {{automation}} {{scheme is}} outlined in Fig.  11. It is an iterative procedure that repeatedly builds projection traces from cluster seeds and refines the micrograph alignment until the average peak {{number in the}} traces becomes maximal. The initial state in this <b>iterative</b> <b>routine</b> corresponds to a rough alignment where micrographs are aligned by means of simple cross-correlations. It has consistently shown {{to lead to the}} correct final alignment.|$|E
40|$|The basic {{uniqueness}} and existence {{properties of}} biproportional matrix solutions are reviewed and {{a direct and}} constructive proof of the convergence of an <b>iterative</b> <b>routine</b> commonly adopted to adjust a given nonnegative matrix to produce a second matrix (biproportional to the first), whose row and column sums are given strictly positive numbers, is offered. Existing convergence proofs, {{most of which are}} limited to particular cases of the present form, are considered. ...|$|E
40|$|Fluxes and intensities {{of light}} {{scattered}} by a model atmosphere are computed by a spherical harmonics approximation and by an iterative method {{of solving the}} radiative transfer equation and are compared. The large differences in the net fluxes and intensities reported by Dave and Armstrong (1974) for the two methods are reduced here by making a few changes in the <b>iterative</b> <b>routine.</b> Decreasing the polar angle increment from 2 to 1 deg in the iterative method of computing the source function does not improve the results as suggested by Dave and Armstrong...|$|E
30|$|We {{demonstrate}} {{the utility of}} this method using two different test cases: the giant amoeba-infecting DNA virus Mimivirus [11 – 13] and a clathrin-coated vesicle in a human embryonic kidney HEK 293 T cell in culture. For each example, we employed both filtered back-projection and non-linear <b>iterative</b> reconstruction <b>routines</b> to generate the 3 D EMT volumes. Mimivirus and the clathrin lattice assembly have been extensively studied by other methods, establishing details of their structure and thus providing useful reference data with which to compare {{the quality of the}} results obtained using our multi-tilt EMT approach.|$|R
40|$|A wide {{class of}} {{discrete}} time non-linear {{systems can be}} represented by the non-linear autoregressive moving average model with exogenous inputs or NARMAX model. This paper develops a practical algorithm for identifying NARMAX models based on radial basis functions from noise corrupted data. The algorithm consists of an <b>iterative</b> orthogonal-forward-regression <b>routine</b> coupled with model validity tests. The orthogonal-forward-regression routine selects parsimonious radial-basis-function models while the model validity tests measure the quality of fit. The modelling of a liquid level system and an automotive diesel engine are included to demonstrate {{the effectiveness of the}} identification procedure...|$|R
40|$|Minimizing the {{empirical}} risk {{is a popular}} training strategy, but for learning tasks where the data may be noisy or heavy-tailed, one may require many observations in order to generalize well. To achieve better performance under less stringent requirements, we introduce a procedure which constructs a robust approximation of the risk gradient for use in an <b>iterative</b> learning <b>routine.</b> We provide high-probability bounds on the excess risk of this algorithm, by showing {{that it does not}} deviate far from the ideal gradient-based update. Empirical tests show that in diverse settings, the proposed procedure can learn more efficiently, using less resources (iterations and observations) while generalizing better...|$|R
40|$|A {{model of}} the viscoelastic/damage {{response}} of a filament-wound spherical vessel used for long-term pressure containment is developed. The matrix material of the composite system {{is assumed to be}} linearly viscoelastic. Internal accumulated damage based upon a quadratic relationship between transverse modulus and maximum circumferential strain is postulated. The resulting nonlinear problem is solved by an <b>iterative</b> <b>routine.</b> The elastic-viscoelastic correspondence is employed to produce, in the Laplace domain, the associated elastic solution for the maximum circumferential strain which is inverted by the method of collocation to yield the time-dependent solution. Results obtained with the model are compared to experimental observations...|$|E
40|$|Coherent {{diffraction}} imaging is a high-resolution {{imaging technique}} whose potential can be greatly enhanced {{by applying the}} extrapolation method presented here. We demonstrate enhancement in resolution of a non-periodical object reconstructed from an experimental X-ray diffraction record which contains about 10 % missing information, including the pixels {{in the center of}} the diffraction pattern. A diffraction pattern is extrapolated beyond the detector area and as a result, the object is reconstructed at an enhanced resolution and better agreement with experimental amplitudes is achieved. The optimal parameters for the <b>iterative</b> <b>routine</b> and the limits of the extrapolation procedure are discussed. Comment: 12 pages, 4 figure...|$|E
40|$|Damage {{detection}} and diagnostic techniques using vibration responses {{that depend on}} analytical models provide more information about a structure's integrity than {{those that are not}} model based. The drawback of these approaches is that some form of workable model is required. Typically, models of practical structures and their corresponding computational e#ort are very large. One method of detecting damage in a structure is to measure excess energy dissipation, which can be seen in damping matrices. Calculating damping matrices is important because there is a correspondence between a change in the damping matrices and the health of a structure. The objective of this research is to investigate the numerical problems associated with computing damping matrices using inverse methods. Two damping identification methods are tested for e#ciency in large-scale applications. One is an <b>iterative</b> <b>routine,</b> and the other a least squares method. Numerical simulations have been performed on [...] ...|$|E
40|$|We {{present a}} {{collection}} of public-domain Fortran 77 routines for the solution of systems of linear equations {{using a variety of}} <b>iterative</b> methods. The <b>routines</b> implement methods which have been modied for their ecient use on parallel architectures with either shared- or distributed-memory. PIM was designed to be portable across dierent machines. Results are presented for a variety of parallel computers...|$|R
40|$|The paper aims {{to present}} {{the results of the}} {{research}} toward improving the performance expressed in accuracy and time complexity of SVM implementations. The implementation of the search process for soft margin hyperplane uses a slight modification of Sequential Minimal Optimization (SMO) algorithm introduced by Platt in 1998 - 1999, to solve the quadratic programming problem involved in the learning process. The SMO is a simple algorithm that quickly solves the SVM problem by decomposing the overall quadratic programming problem into smaller quadratic programming sub-problems without any extra matrix storage and without invoking an <b>iterative</b> numerical <b>routine</b> for each sub-problem. The proposed method was tested on simulated data generated randomly from normal 2 -dimensional distributions...|$|R
40|$|A linear {{stability}} {{analysis shows}} that the jet in crossflow is characterized by self-sustained global oscillations for a jet-to-crossflow velocity ratio of 3. A fully three-dimensional unstable steady-state solution and its associated global eigenmodes are computed by direct numerical simulations and <b>iterative</b> eigenvalue <b>routines.</b> The steady flow, obtained by means of selective frequency damping, consists mainly of a (steady) counter-rotating vortex pair (CVP) in the far field and horseshoe-shaped vortices close to the wall. High-frequency unstable global eigenmodes associated with shear-layer instabilities on the CVP and low-frequency modes associated with shedding vortices {{in the wake of}} the jet are identified. Furthermore, different spanwise symmetries of the global modes are discussed. This work constitutes the first simulation-based global stability analysis of a fully three-dimensional base flow. 1...|$|R
40|$|A {{new method}} {{is used to}} {{estimate}} the volumes of sediments of glacial valleys. This method {{is based on the}} concept of sloping local base level and requires only a digital terrain model and the limits of the alluvial valleys as input data. The bedrock surface of the glacial valley is estimated by a progressive excavation of the digital elevation model (DEM) of the filled valley area. This is performed using an <b>iterative</b> <b>routine</b> that replaces the altitude of a point of the DEM by the mean value of its neighbors minus a fixed value. The result is a curved surface, quadratic in 2 D. The bedrock surface of the Rhone Valley in Switzerland was estimated by this method using the free digital terrain model Shuttle Radar Topography Mission (SRTM) (~ 92 m resolution). The results obtained are in good agreement with the previous estimations based on seismic profiles and gravimetric modeling, with the exceptions of some particular locations. The results from the present method and those from the seismic interpretation are slightly different from the results of the gravimetric data. This discrepancy may result from the presence of large buried landslides {{in the bottom of the}} Rhone Valley...|$|E
40|$|In this paper, we for {{the first}} time {{simulate}} the process of hydrodynamic bead aggregation in a flat micro-fluicid chamber by a porous-media model in an <b>iterative</b> <b>routine.</b> This allows us to optimize the chamber design of our recently developed experimental method to form periodical monolayers from the flow of bead suspension. Periodical monolayers are advantageous for parallel assay formats since they enhance the mechanical rigidity of the aggregated pattern. This is important to avoid a spatial rearrangement along various steps of a read-out procedure which would impair the correlation between measurements. Furthermore, the monolayer formation guarantees the individual optical accessibility of all probe beads. By modelling the monolayers with porous media, we can drastically reduce the degrees of freedom in a two-phase, multi-particle problem. This way, we are able to compute stationary hydrodynamic flow patterns in the chamber. In order to simulate the complete filling process from these stationary solutions, we developed an iterative master routine which takes the transient aggregation pattern as the initial condition, then evaluates the placement of the newly introduced beads, and finally converts the points of aggregation into porous media...|$|E
30|$|To {{reduce the}} PAPR of OFDM signals, {{numerous}} techniques {{have been proposed}} in the literature[3 – 17]. A comprehensive tutorial review of PAPR reduction techniques in OFDM systems is summarized in[3, 4]. It is known that clipping[4] is the simplest method, but it degrades the bit error rate (BER) {{of the system and}} results in out-of-band noise and in-band distortion. Among all existing techniques of PAPR reduction, selective mapping (SLM)[7] and partial transmit sequence (PTS)[8 – 10] are very attractive due to their good PAPR reduction without the restriction on the number of subcarriers. However, in SLM technique, the requirement of multiple IFFT operations increases the implementation complexity. The PTS technique uses an <b>iterative</b> <b>routine</b> similar to the trial-and-error method in finding optimum phase factors that leads to lower PAPR. However, the PTS technique requires an exhaustive search over all combinations of allowed phase factors, whose complexity increases exponentially with the number of subblocks. Hence, it achieves considerable PAPR reduction without distortion, but the high computational complexity of multiple Fourier transforms is a problem in practical systems[3, 4, 8 – 10]. As a result, for all these search methods, either computational complexity is still high or PAPR reduction performance is not good enough.|$|E
40|$|A {{methodology}} for minimizing the error in on-line Kalman filter-based aircraft engine performance estimation applications is presented. This technique specifically addresses the underdetermined estimation problem, {{where there are}} more unknown parameters than available sensor measurements. A systematic approach is applied to produce a model tuning parameter vector of appropriate dimension to enable estimation by a Kalman filter, while minimizing the estimation error in the parameters of interest. Tuning parameter selection is performed using a multi-variable <b>iterative</b> search <b>routine</b> which seeks to minimize the theoretical mean-squared estimation error. Theoretical Kalman filter estimation error bias and variance values are derived at steady-state operating conditions, and the tuner selection routine is applied to minimize these values. The new methodology yields an improvement in on-line engine performance estimation accuracy...|$|R
40|$|A model-assisted semiparametric {{method of}} {{estimating}} finite population totals is investigated to improve the precision of survey estimators by incorporating multivariate auxiliary information. The proposed superpopulation model is a single-index model which {{has proven to be}} a simple and efficient semiparametric tool in multivariate regression. A class of estimators based on polynomial spline regression is proposed. These estimators are robust against deviation from single-index models. Under standard design conditions, the proposed estimators are asymptotically design-unbiased, consistent and asymptotically normal. An <b>iterative</b> optimization <b>routine</b> is provided that is sufficiently fast for users to analyze large and complex survey data within seconds. The proposed method has been applied to simulated datasets and MU 281 dataset, which have provided strong evidence that corroborates with the asymptotic theory. Comment: 30 page...|$|R
40|$|Methods of {{three-dimensional}} deconvolution with a point-spread {{function as}} frequently employed in optical microscopy to reconstruct true three-dimensional distribution of objects are extended to holographic reconstructions. Two such schemes {{have been developed}} and are discussed: an instant deconvolution using the Wiener filter {{as well as an}} <b>iterative</b> deconvolution <b>routine.</b> The instant 3 d-deconvolution can be applied to restore the positions of volume-spread objects such as small particles. The iterative deconvolution can be applied to restore the distribution of complex and extended objects. Simulated and experimental examples are presented and demonstrate artifact and noise free three-dimensional reconstructions from a single two-dimensional holographic record. Keywords: digital holography, volumetric deconvolution, three-dimensional volumetric deconvolution, particle tracking, holographic particle tracking, resolution, PSFComment: including MATLAB cod...|$|R
40|$|The LORAN {{program is}} stored in CMS disk files for use by Avionics Engineering Center {{terminal}} users. A CMS EXEC file named LORAN controls program operation. The user types LORAN and the program then prompts for data input and produces output on the terminal. The FORTRAN program refers to a disk file of LORAN master data giving station locations, coding delays, repetition rate and station pair identification letters. For Geographic-to-LORAN conversion, no iterative computations are required; the program is a straightforward coordinate conversion based upon the techniques described by the Navy. For LORAN-to-Geographic conversion, the original Navy program required a dead-reckoned position, near the actual unknown fix, to begin computations. No iteration was performed to obtain the LORAN fix, but internal program errors occurred at execution time if the dead-reckoned fix were displaced from the actual fix {{by more than a}} few minutes of latitude or longitude. In order to enhance usefulness of the program for the terminal user, an <b>iterative</b> <b>routine</b> was added which allows a single dead-reckoned position to be entered from the master data file for each LORAN chain. The results compare exactly with the LORAN-C navigation chart, and provide adequate benchmark data for general aviation flight planning and data analysis...|$|E
40|$|The Secant Method is an {{iterative}} method {{in which the}} peak displacement response of a structure or structural component is determined from linear dynamic analyses of a model whose stiffness is updated to reflect a computed degree of degradation {{that is consistent with}} the computed peak displacement. Numerous descriptions of the Secant Method and related procedures exist in the literature. Division 95 of the City of Los Angeles Building Code, for example, includes a formulation of the Secant Method that is specifically intended for evaluation and rehabilitation of infill frame buildings with limited ductility. In the Secant Method, the degree of degradation of a structure is derived from the secant as determined from nonlinear force-displacement relationships for the structure as a whole or from appropriate sub-assemblies and this information is substituted into the linear dynamic analysis environment. In this way, the dynamic analysis model is updated to approximate intermediate states of degradation until a rational and consistent convergence in each sub-assembly as well as the whole structure is achieved. The Secant Method can be unwieldy, however, when applied to multistory complex structures, due to the involvement of a number of modes in their dynamic response and due to the requirement that convergence be achieved for the system as a whole and for its various sub-assemblies. In this paper, we explain the basic Secant Method, apply it to complex multi-story buildings, and describe an <b>iterative</b> <b>routine</b> that facilitates capturing higher mod...|$|E
40|$|An {{energy release}} rate based {{simulation}} method for dynamic fracture mechanics is developed to model crack initi-ation and propagation in elastic–plastic solid. Potential crack surface is modeled {{by a series}} of paired nodes bond-ed together by nodal constraint forces before crack propagation. The nodal constraint force of the paired nodes at the current crack tip is linearly decreased to zero in a specifi ed time interval to mimic the crack propagation with a corresponding crack speed. During this process, the nodal force vectors and the nodal displacement vectors of the paired nodes are obtained. Based on the force-displacement curve, energy release rate can be calculated. It is found that energy release rate is monotonically decreasing with crack speed, a physical phenomenon predicted by Freund [1]. In this study, we hypothesize that the energy release rate is equal to a constant critical value during the entire crack propagation process. Hence, we may fi nd the variable crack speed as a function of time. It is noticed that crack initiation is a special case of crack propagation with the crack speed approaching zero. The direction of crack propagation is determined by an <b>iterative</b> <b>routine</b> that adjusts the mesh such that the crack path is perpen-dicular to the nodal force of the paired nodes. The constitutive theory of the material is formulated based on large strain plasticity with return mapping algorithm. Notice that small strain elasticity is a special case of large strain plasticity. Hence we can verify our numerical results with those in linear elastic fracture mechanics. It is found tha...|$|E
40|$|International audienceA linear {{stability}} {{analysis shows}} that the jet in crossflow is characterized by self-sustained global oscillations for a jet-to-crossflow velocity ratio of 3. A fully three-dimensional unstable steady-state solution and its associated global eigenmodes are computed by direct numerical simulations and <b>iterative</b> eigenvalue <b>routines.</b> The steady flow, obtained by means of selective frequency damping, consists mainly of a (steady) counter-rotating vortex pair (CVP) in the far field and horseshoe-shaped vortices close to the wall. High-frequency unstable global eigenmodes associated with shear-layer instabilities on the CVP and low-frequency modes associated with shedding vortices {{in the wake of}} the jet are identified. Furthermore, different spanwise symmetries of the global modes are discussed. This work constitutes the first simulation-based global stability analysis of a fully three-dimensional base flow. © 2009 Cambridge University Press...|$|R
40|$|We {{describe}} {{a method for}} time-critical decision making involving sequential tasks and stochastic processes. The method employs several <b>iterative</b> refinement <b>routines</b> for solving {{different aspects of the}} decision making problem. This paper concentrates on the meta-level control problem of deliberation scheduling, allocating computational resources to these routines. We provide different models corresponding to optimization problems that capture the different circumstances and computational strategies for decision making under time constraints. We consider precursor models in which all decision making is performed prior to execution and recurrent models in which decision making is performed in parallel with execution, accounting for the states observed during execution and anticipating future states. We {{describe a}}lgorithms for precursor and recurrent models and provide the results of our empirical investigations to date. 1 Introduction We are interested in solving sequential decision ma [...] ...|$|R
40|$|Abstract: A model-assisted semiparametric {{method of}} {{estimating}} finite population totals is investigated to improve the precision of survey estimators by incorporating multiple auxiliary variables. The proposed superpopulation model is a single-index model which {{has proven to be}} a simple and efficient semiparametric tool in multivariate regression. A class of estimators based on polynomial spline regression is proposed. These estimators are robust against deviation from single-index models. Under standard design conditions, the proposed estimators are asymptotically designunbiased, consistent and asymptotically normal. An <b>iterative</b> optimization <b>routine</b> is provided that is sufficiently fast for users to analyze large and complex survey data within seconds. The proposed method is applied to simulated datasets and MU 281 dataset, and compared with alternative methods, which have provided strong evidence that corroborates with the asymptotic theory. Key words and phrases: Horvitz-Thompson estimator; model-assisted estimation; semiparametric; spline smoothing; superpopulation. ...|$|R
40|$|An {{unbiased}} method {{capable of}} classifying hazelnut (Corylus avellana L.) {{in compliance with}} the statements set out by the Commission Regulation (EC) No. 1284 / 2002 is economically important to the fresh and processed industries. Thus, in this study, the feasibility of High Dynamic Range (HDR) hyperspectral imaging for hazelnut kernel sorting (cv. Tonda Gentile Romana) of four quality classes (‘Class Extra’, ‘Class I’, ‘Class II’ and ‘Waste’) has been investigated. Two different exposure times (5 and 8 ms) were selected for experiments, and the respective spectra were combined to obtain a high dynamic range over the full spectral range. The illumination setup was also optimized to improve the intensity and uniformity of the light along the field of view of the camera. PLS-DA was used to classify the pixels based on their spectra and the spectral pre-treatment was optimized through an <b>iterative</b> <b>routine.</b> The performance of each PLS-DA model was defined based on its sensitivity, selectivity, α-error, β-error and accuracy rates. All of the selected models provided a very-good (> 90 %) or good (> 80 %) sensitivity and selectivity for the predefined classes. Misclassified kernels were primarily assigned to the low-quality classes (i. e. ‘Class II’ and Waste). Moreover, the spatial domain was used to evaluate the feasibility of distinguishing hazelnut classes {{on the basis of their}} size and shape. It was found that hazelnut dimensions can be used to improve the accuracy of the classification of the kernels. Thanks to this combination of both spectral and spatial information spectral imaging could be used for quality sorting of hazelnuts. status: submitte...|$|E
40|$|The {{three-dimensional}} {{structure and}} the inhomogeneity of clouds poses a field of challenges. The characterization of their spatial structure, their microphysical properties, and their variability is difficult. On the other hand this kind of knowledge is crucial to any investigation {{on the impact of}} clouds on the radiation budget or on the reliability of cloud remote sensing data. In this article the characteristics of horizontal transport of radiation in inhomogeneous clouds are studied using three-dimensional (3 D) simulations of radiative transport and the independent pixel approximation (IPA). The opposing 3 D effects of radiative smoothing and sharpening due to horizontal photon transport are examined in terms of the Green’s function which describes the interrelation of the radiance fields calculated using IPA and 3 D radiative transport. Based on these considerations a novel method was developed allowing the determination of realistic 3 D stratocumulus structures from high spatial resolution radiance fields observed by a compact airborne spectrographic imager (CASI, 15 m resolution). An initial 3 D distribution of liquid water content and effective droplet size retrieved using the IPA assumption and an adiabatic microphysical model is adjusted with the objective to reproduce the observation by a 3 D forward radiative transfer simulation. Therefor an approximate Green’s function is utilized in an <b>iterative</b> <b>routine</b> to remove 3 D effects from the observation. A set of 27 stratocumulus cloud structures is provided for subsequent investigations. The capability of the method is characterized through a test case and the comparison of average cloud properties to in-situ data of various field campaigns...|$|E
40|$|Abstract. Energy {{power from}} {{renewable}} sources, especially wind turbine generators, are being considered {{as an important}} generation alternative in the electrical power systems around the world due to their non contaminant nature and low environmental effects. In particular, the power supplied by wind generators is widely random following the random nature of weather conditions; therefore a probabilistic approach during planning seems much more appropriate than the deterministic approach actually used in the electrical utilities. This paper presents a methodology that allows the application of probabilistic analysis, thru the use of programming tools, in the load flow planning studies. The methodology described provides the opportunity to obtain the steady state response in systems where wind power exists using a wind power generation model (WPGM) and an <b>iterative</b> <b>routine</b> to the load flow analysis considering each value of generation of the wind sources. A wind power generation model (WPGM) was developed using a random wind speed generator. The random wind speed generator describes the Weibull distribution of wind measurements, and then using Montecarlo simulation techniques, a program in Visual Basic executes the automatic evaluation of random samples of speed of wind considering the characteristic power curve of a wind turbine, obtaining a widely database of wind generation conditions. As {{a result of the}} investigation, a computational tool was developed in order to use it, in combination with the PSS/E load flow analysis software, for steady state power system analysis. The methodology was used as an assessment tool to identify the effects of wind turbines generation into the substation voltages and power flow profiles in the power network, for a wind farm project intended to be installed in Margarita Island. The analysis aimed to identify the transmission adjustments, reinforcements and expansions needed to guarantee the wind farm project integration successful into the power system...|$|E
40|$|This report {{describes}} {{the design and}} implementation of the parallel iterative linear system solver PAISS (Parallel Adaptive Iterative linear System Solver) for distributed memory multicomputers and workstation clusters. It is capable to apply a heterogeneous data distribution and dynamic load balancing within an <b>iterative</b> solver <b>routine</b> at matrix level. Sparse and dense matrices, as well as vectors are distributed heterogeneously according to the available performances of the processors, and redistribution is carried out at run time if the load of the processors changes. The concepts behind the chosen matrix data structures and the load measurement are discussed, and the dynamic load balancing algorithm is presented in details. Important aspects of the implementation are given, and the report completes with an overview of all implemented modules and data structures. Keywords: parallel iterative linear system solver, workstation cluster, distributed memory multicomputer, dynamic load b [...] ...|$|R
40|$|We {{describe}} a protocol {{for the use}} of a control feedback loop incorporating an <b>iterative</b> optimization <b>routine</b> for a range of time-independent adaptive optics applications. These applications are characterized by the quasi steady state of the aberrative effects (> 0 : 1 s) and contrast, for instance, to astronomical applications where the aberrations constantly vary at frequencies above 10 Hz. For optimal performance in such time-independent applications, the control systems typically require specialized tailoring. A typical example of two different types of time-independent adaptive optics applications-an adaptive optic microscope and an adaptive optic laser platform-are detailed and compared. It is shown that implementing a number of minor, but crucial, application-specific modifications to the control system results in an improved efficiency of an already extremely successful technique for aberration compensation. We present a description of the crucial parameters to consider in a search-based adaptive optics system...|$|R
40|$|This paper {{describes}} {{the design and}} implementation of a parallel iterative linear system solver for distributed memory multicomputers and workstation clusters. It is capable of applying heterogeneous data distribution and dynamic load balancing within an <b>iterative</b> solver <b>routine</b> at matrix level. Matrices as well as vectors are distributed heterogeneously according to the available performances of the processors, and redistributions are carried out at run time if the load of the processors changes. We present the concepts behind the chosen matrix data structures and load measurements, and discuss our dynamic load balancing algorithm. The results show the suitability of our approach. 1 Introduction and Related Work Solving large and sparse linear systems {{is the core of}} many applications in scientific computing and engineering. On parallel machines, iterative solvers are specially attractive. Only a small number of computational kernels [...] which can be parallelized e#ciently - [...] ...|$|R
