140|2618|Public
25|$|One {{of the key}} {{characteristics}} of using blow flies in developing a post mortem <b>interval</b> <b>estimate</b> is the succession of insects that colonize the body. Based on the insects present at the time, a reasonable time frame for death may be established. C. vicina has its own part in the succession of the corpse.|$|E
5000|$|... #Subtitle level 3: Toxin {{confounding}} of postmortem <b>interval</b> <b>estimate</b> ...|$|E
5000|$|The {{output of}} PE i {{includes}} a point estimate and an <b>interval</b> <b>estimate</b> ...|$|E
50|$|When <b>interval</b> <b>estimates</b> are reported, {{they should}} have a {{commonly}} held interpretation in the scientific community and more widely. In this regard, credible intervals are held to be most readily understood by the general public. <b>Interval</b> <b>estimates</b> derived from fuzzy logic have much more application-specific meanings.|$|R
40|$|Imprecise {{probability}} {{models are}} applied to logistic regression to produce <b>interval</b> <b>estimates</b> of regression parameters. The lengths of <b>interval</b> <b>estimates</b> are of main interest. Shorter <b>interval</b> <b>estimates</b> correspond to less imprecision in regression parameters estimates. This thesis applies imprecise probabilistic methods to the logit model. Imprecise logistic regression, briefly called ImpLogit model, is presented and established for the first time. ImpLogit model is applied based on an inferential paradigm that applies Bayes theorem to a family of prior distributions, yielding interval posterior probabilities. The so-called <b>interval</b> <b>estimates</b> of regression parameters are computed using Metropolis-Hastings algorithm. Two imprecise prior probability models {{are applied to}} 2 -parameter ImpLogit model : the imprecise Dirichlet model (IDM) and the imprecise logit-normal model (ILnM). The 2 -parameter ImpLogit model is fitted using real life dose-response data. This {{takes into account the}} cases of increasing, decreasing and mixed-belief ImpLogit models. The relation between the lengths of <b>interval</b> <b>estimates</b> of regression parameters and both of covariate values and imprecise prior hyperparameters, in 2 -parameter ImpLogit model, is studied by simulation. Different designs are applied in order to investigate a way to shorten the lengths of <b>interval</b> <b>estimates</b> of regression parameters. Having covariate fixed values to surround the prior believed median value of the logistic distribution results in reducing the imprecision in <b>interval</b> <b>estimates.</b> Fixing covariate values around the prior believed median value in a short range increases the lengths of <b>interval</b> <b>estimates.</b> The number of fixed covariate values (say number of distinct dose levels in a dose-response experiment) affects the produced imprecision. A larger number of fixed covariate values increases the lengths of <b>interval</b> <b>estimates.</b> Therefore, a good design has a small number of fixed covariate values, located and spread out not in a short range. ImpLogit model designs that are recommended by the simulation study, are compared to optimal designs in the frequentist approach using Fisher information matrix (FIM). Designs in FIM agree with designs that reduce imprecision in 2 -parameter ImpLogit model, in the necessity of having covariate values to be fixed around the prior believed median value of the logistic distribution, not in a short range...|$|R
40|$|Coefficient alpha is a {{commonly}} used index of measurement reliability {{in the social}} and behavioral sciences. Although point estimates of coefficient alpha are readily computed (and are provided by the PROC CORR procedure in SAS), a variety of methods for constructing <b>interval</b> <b>estimates</b> have been suggested in the literature. This paper presents a SAS macro that calculates eight <b>interval</b> <b>estimates</b> of coefficient alpha. The macro computes coefficient alpha from a matrix of item scores provided as input, and outputs the point <b>estimate</b> and <b>interval</b> <b>estimates</b> in a simple table. The paper provides a demonstration of the SAS/IML code, sample output, and examples of applications in simulation studies...|$|R
5000|$|A {{fiducial}} or objective Bayesian {{argument can}} be used to derive the <b>interval</b> <b>estimate</b> ...|$|E
5000|$|Calculate {{the point}} {{estimate}} [...] of PE i as [...] and the <b>interval</b> <b>estimate</b> is ...|$|E
50|$|An <b>interval</b> <b>estimate</b> is {{one type}} of outcome of a {{statistical}} analysis. Some other types of outcome are point estimates and decisions.|$|E
40|$|Percentile {{norms for}} TFLS 3 Point and <b>interval</b> <b>estimates</b> of {{percentile}} ranks are useful tools in assisting with {{the interpretation of}} neurocognitive test results. We provide percentile ranks for raw subscale scores on the Texas Functional Living Scale (TFLS; Cullum, Weiner, & Saine, 2009) using the TFLS standardization sample data (N = 800). Percentile ranks with <b>interval</b> <b>estimates</b> are also provided for the overall TFLS T score. Conversion tables are provided along with the option of obtaining the point and <b>interval</b> <b>estimates</b> using a computer program written to accompany this paper (TFLS_PRs. exe). The percentile ranks for the subscales offer an alternative to using the cumulative percentage tables in the test manual and provide a useful and quick way for neuropsychologists to assimilate information on the case’s profile of scores on the TFLS subscales. The provision of <b>interval</b> <b>estimates</b> for the percentile ranks {{is in keeping with}} the contemporary emphasis on the use of confidence intervals in psychological statistic...|$|R
40|$|Point and <b>interval</b> <b>estimates</b> of {{percentile}} ranks {{are useful}} tools in assisting with {{the interpretation of}} neurocognitive test results. We provide percentile ranks for raw subscale scores on the Texas Functional Living Scale (TFLS; Cullum, Weiner, & Saine, 2009) using the TFLS standardization sample data (N[*]=[*] 800). Percentile ranks with <b>interval</b> <b>estimates</b> are also provided for the overall TFLS T score. Conversion tables are provided along with the option of obtaining the point and <b>interval</b> <b>estimates</b> using a computer program written to accompany this paper (TFLS_PRs. exe). The percentile ranks for the subscales offer an alternative to using the cumulative percentage tables in the test manual and provide a useful and quick way for neuropsychologists to assimilate information on the case's profile of scores on the TFLS subscales. The provision of <b>interval</b> <b>estimates</b> for the percentile ranks {{is in keeping with}} the contemporary emphasis on the use of confidence intervals in psychological statistics...|$|R
40|$|For noninformative nonparametric {{estimation}} of finite population quantiles under simple random sampling, estimation {{based on the}} Polya posterior is similar to estimation based on the Bayesian approach developed by Ericson (1969, JRSSB, 31, 195 - 233) in that the Polya posterior distribution is the limit of Ericson’s posterior distributions as the weight placed on the prior distribution diminishes. Furthermore, Polya posterior quantile estimates can {{be shown to be}} admissible under certain conditions. We demonstrate the admissibility of the sample median as an estimate of the population median under such a set of conditions. As with Ericson’s Bayesian approach, Polya posterior based <b>interval</b> <b>estimates</b> for population quantiles are asymptotically equivalent to the <b>interval</b> <b>estimates</b> obtained from standard frequentist approaches. In addition, for small to moderate populations, Polya posterior based <b>interval</b> <b>estimates</b> for quantiles of a continuous characteristic of interest tend to agree with the standard frequentist <b>interval</b> <b>estimates.</b> Key words: finite population sampling, admissibility, quantile estimation, noninformative inference, nonparametric inferenc...|$|R
5000|$|A Bayesian <b>interval</b> <b>estimate</b> {{is called}} a {{credible}} interval. Using {{much of the same}} notation as above, the definition of a credible interval for the unknown true value of θ is, for a given γ, ...|$|E
50|$|The idea of basing an <b>interval</b> <b>estimate</b> on the {{relative}} likelihood goes back to Fisher in 1956 and {{has been used by}} many authors since then. A likelihood interval can be used without claiming any particular coverage probability; as such, it differs from confidence intervals.|$|E
5000|$|... an <b>interval</b> <b>estimate,</b> e.g. a {{confidence}} interval (or set estimate), i.e. an interval constructed using a dataset {{drawn from a}} population so that, under repeated sampling of such datasets, such intervals would contain the true parameter value with the probability at the stated confidence level; ...|$|E
30|$|Overconfidence is {{commonly}} measured by asking participants to answer knowledge-based {{questions in a}} multiple choice format and to indicate their level of confidence in each answer. Overconfidence is present if the expectation of accuracy exceeds actual accuracy (Pulford and Colman 1997). An alternative method, which we applied for this study, requires participants to make numerical <b>interval</b> <b>estimates</b> at a given level of accuracy (McKenzie et al. 2008; Soll and Klayman 2004; Teigen and Jørgensen 2005). A confidence level of 80  %, for instance, thus implies that 80  % of <b>interval</b> <b>estimates</b> should include the actual value. People commonly exhibit {{a high level of}} overconfidence when completing this exercise, such that less than 50  % of their <b>interval</b> <b>estimates</b> contain the actual value (Moore and Healy 2008).|$|R
40|$|Robust maximum {{likelihood}} (RML) and asymptotically generalized least squares (AGLS) {{methods have been}} recommended for fitting ordinal structural equation models. Studies show {{that some of these}} methods underestimate standard errors. However, these studies have not investigated the coverage and bias of <b>interval</b> <b>estimates.</b> An estimate with a reasonable standard error could still be severely biased. This can only be known by systematically investigating the <b>interval</b> <b>estimates.</b> The present study compares Bayesian, RML, and AGLS <b>interval</b> <b>estimates</b> of factor correlations in ordinal confirmatory factor analysis models (CFA) for small sample data. Six sample sizes, 3 factor correlations, and 2 factor score distributions (multivariate normal and multivariate mildly skewed) were studied. Two Bayesian prior specifications, informative and relatively less informative were studied. Undercoverage of confidence intervals and underestimation of standard errors was common in non-Bayesian methods. Underestimated standard errors may lead to inflated type-I error rates. Non-Bayesian intervals were more positive biased than negatively biased, that is, most intervals that did not contain the true value were greater than the true value. Some non-Bayesian methods had non-converging and inadmissible solutions for small samples and non-normal data. Bayesian empirical standard error estimates for informative and relatively less informative priors were closer to the average standard errors of the estimates. The coverage of Bayesian credibility intervals was closer to what was expected with overcoverage in a few cases. Although some Bayesian credibility intervals were wider, they reflected the nature of statistical uncertainty that comes with the data (e. g. small sample). Bayesian point estimates were also more accurate than non-Bayesian estimates. The results illustrate the importance of analyzing coverage and bias of <b>interval</b> <b>estimates,</b> and how ignoring <b>interval</b> <b>estimates</b> can be misleading. Therefore, editors and policymakers should continue to emphasize the inclusion of <b>interval</b> <b>estimates</b> in research...|$|R
30|$|We {{note that}} {{confidence}} <b>intervals</b> <b>estimated</b> {{based on a}} Gaussian assumption {{did not lead to}} different conclusions in our case study.|$|R
50|$|One {{of the key}} {{characteristics}} of using blow flies in developing a post mortem <b>interval</b> <b>estimate</b> is the succession of insects that colonize the body. Based on the insects present at the time, a reasonable time frame for death may be established. C. vicina has its own part in the succession of the corpse.|$|E
50|$|The idea to use imprecise {{probability}} {{has a long}} history. The first formal treatment {{dates back}} at least {{to the middle of the}} nineteenth century, by George Boole, who aimed to reconcile the theories of logic (which can express complete ignorance) and probability. In the 1920s, in A Treatise on Probability, Keynes formulated and applied an explicit <b>interval</b> <b>estimate</b> approach to probability.|$|E
50|$|Interval {{estimates}} can be {{contrasted with}} point estimates. A point estimate {{is a single}} value given as the estimate of a population parameter that is of interest, for example, the mean of some quantity. An <b>interval</b> <b>estimate</b> specifies instead a range within which the parameter is estimated to lie. Confidence intervals are commonly reported in tables or graphs along with point estimates of the same parameters, to show {{the reliability of the}} estimates.|$|E
40|$|Abstract Background This article {{describes}} classical and Bayesian interval estimation of genetic susceptibility based on random samples with pre-specified numbers of unrelated cases and controls. Results Frequencies of genotypes in cases and controls {{can be estimated}} directly from retrospective case-control data. On the other hand, genetic susceptibility defined as the expected proportion of cases among individuals with a particular genotype depends on the population proportion of cases (prevalence). Given this design, prevalence is an external parameter and hence the susceptibility cannot be estimated based on only the observed data. Interval estimation of susceptibility that can incorporate uncertainty in prevalence values is explored from both classical and Bayesian perspective. Similarity between classical and Bayesian <b>interval</b> <b>estimates</b> in terms of frequentist coverage probabilities for this problem allows an appealing interpretation of classical intervals as bounds for genetic susceptibility. In addition, {{it is observed that}} both the asymptotic classical and Bayesian <b>interval</b> <b>estimates</b> have comparable average length. These <b>interval</b> <b>estimates</b> serve as a very good approximation to the "exact" (finite sample) Bayesian <b>interval</b> <b>estimates.</b> Extension from genotypic to allelic susceptibility intervals shows dependency on phenotype-induced deviations from Hardy-Weinberg equilibrium. Conclusions The suggested classical and Bayesian <b>interval</b> <b>estimates</b> appear to perform reasonably well. Generally, the use of exact Bayesian interval estimation method is recommended for genetic susceptibility, however the asymptotic classical and approximate Bayesian methods are adequate for sample sizes of at least 50 cases and controls. </p...|$|R
40|$|A {{framework}} for comparing normal population {{means in the}} presence of heteroscedasticity and outliers is provided. A single number called the weighted effect size summarizes the differences in population means after weighting each according to the difficulty of estimating their respective means, whether the difficulty is due to unknown population variances, unequal sample sizes or the presence of outliers. For an ANOVA weighted for unequal variances, we find <b>interval</b> <b>estimates</b> for the weighted effect size. In addition, the weighted effect size is shown to be a monotone function of a suitably defined weighted coefficient of determination, which means that <b>interval</b> <b>estimates</b> of the former are readily transformed into <b>interval</b> <b>estimates</b> of the latter. Extensive simulations demonstrate the accuracy of the nominal 95 % coverage of these intervals {{for a wide range of}} parameters...|$|R
50|$|Psychological {{studies of}} the {{perception}} of statistics reveal that reporting <b>interval</b> <b>estimates</b> leaves a more accurate perception of the data than reporting p-values.|$|R
5000|$|The {{smaller the}} {{overlapping}} area {{the greater the}} gap between the two functions [...] and , in which case the sexual dimorphism is greater. Obviously, this index {{is a function of the}} five parameters that characterize a mixture of two normal components (. Its range is in the interval , and the interested reader can see, in the work of the authors who proposed the index, the way in which an <b>interval</b> <b>estimate</b> is constructed.|$|E
50|$|In statistics, a {{confidence}} interval (CI) {{is a type}} of <b>interval</b> <b>estimate</b> (of a population parameter) that is computed from the observed data. The confidence level is the frequency (i.e., the proportion) of possible confidence intervals that contain the true value of their corresponding parameter. In other words, if confidence intervals are constructed using a given confidence level in an infinite number of independent experiments, the proportion of those intervals that contain the true value of the parameter will match the confidence level.|$|E
50|$|The primary aim of {{estimation}} methods is {{to estimate the}} size of an effect and report an effect size along with its confidence intervals, the latter of which is related to the precision of the estimate. Estimation at its core involves analyzing data to obtain a point estimate (an effect size calculated from data used as estimate of the population effect size) and an <b>interval</b> <b>estimate</b> that summarizes a range of likely values of the underlying population effect. Proponents of estimation see reporting a p-value as an unhelpful distraction from the important business of reporting an effect size with its confidence intervals, and believe that estimation should replace significance testing for data analysis.|$|E
40|$|This paper further {{examines}} the bootstrap method proposed by Simar and Wilson (1998) for DEA efficiency estimators. Some simplifications are provided, and we provide Monte Carlo {{evidence on the}} coverage probabilities of confidence <b>intervals</b> <b>estimated</b> by the method. In addition, we provide similar evidence for confidence <b>intervals</b> <b>estimate</b> with the so-called naive bootstrap, which {{is known to be}} inconsistent in the DEA setting. Finally, we propose an interated version of the bootstrap which may be used to improve bootstrap <b>estimates</b> of confidence <b>intervals...</b>|$|R
40|$|In the paper, the {{problems}} of approximating an unknown function from data and deriving reliable <b>interval</b> <b>estimates</b> are first considered. An algorithm is proposed to solve these problems, based on a sparsification technique and a nonparametric Set Membership optimality analysis. Assuming that the noise affecting the data is bounded and that the unknown function satisfies a mild regularity assumption, it is shown that the algorithm provides an almost-optimal approximation (in a worst-case sense), and tight <b>interval</b> <b>estimates</b> are evaluated. An innovative approach to fault detection for nonlinear systems is then proposed, based on the derived <b>interval</b> <b>estimates,</b> overcoming some relevant problems proper of the standard techniques. The proposed algorithm is applied in a simulation study to solve the challenging problem of fault detection for {{a new class of}} wind energy generators, which use kites to capture the power from high-altitude wind...|$|R
40|$|The {{problem of}} {{approximating}} an unknown function from data and deriving reliable <b>interval</b> <b>estimates</b> {{is important in}} many fields of science and technology. In this paper, an algorithm is proposed to solve this problem, based on a sparsification technique and a non-parametric Set Membership analysis. Assuming that the noise affecting the data is bounded and the unknown function satisfies a mild regularity assumption, it is shown that the algorithm provides an approximation with suitable optimality properties, together with tight <b>interval</b> <b>estimates.</b> An innovative approach to fault detection, based on the derived <b>interval</b> <b>estimates,</b> is then proposed, overcoming some relevant problems proper of the "classical" techniques. The approach is applied in a simulation study to solve the challenging problem of fault detection for {{a new class of}} wind energy generators, which use kites to capture the power from high-altitude winds...|$|R
50|$|In {{making an}} estimate, {{the goal is}} often most useful to {{generate}} a range of possible outcomes that is precise enough to be useful, but not so precise that {{it is likely to}} be inaccurate. For example, in trying to guess the number of candies in the jar, if fifty were visible, and the total volume of the jar seemed to be about twenty times as large as the volume containing the visible candies, then one might simply project that there were a thousand candies in the jar. Such a projection, intended to pick the single value that is believed to be closest to the actual value, is called a point estimate. However, a point estimation is likely to be incorrect, because the sample size - in this case, the number of candies that are visible - is too small a number to be sure that it does not contain anomalies that differ from the population as a whole. A corresponding concept is an <b>interval</b> <b>estimate,</b> which captures a much larger range of possibilities, but is too broad to be useful. For example, if one were asked to estimate the percentage of people who like candy, it would clearly be correct that the number falls between zero and one hundred percent. Such an estimate would provide no guidance, however, to somebody who is trying to determine how many candies to buy for a party to be attended by a hundred people.|$|E
30|$|The <b>interval</b> <b>estimate</b> for {{homogeneous}} loads is (18).|$|E
40|$|This paper {{presents}} the <b>interval</b> <b>estimate</b> for specific points in polynomial regression: zero of a linear regression, abscissa {{of the extreme}} of a quadratic regression, abscissa of the inflection point of a cubic regression. Two different approaches are under study. An application of these two approaches based on quadratic regression in presented: <b>interval</b> <b>estimate</b> for the plant density giving optimal yield of maize is under consideration...|$|E
50|$|As {{discussed}} below, it is {{also possible}} for place error bounds on the accuracy of the value of a definite <b>interval</b> <b>estimated</b> using a trapezoidal rule.|$|R
50|$|There {{is another}} {{approach}} to statistical inference, namely fiducial inference, that also considers interval estimation. Non-statistical methods {{that can lead}} to <b>interval</b> <b>estimates</b> include fuzzy logic.|$|R
40|$|In many {{behavioural}} {{and social}} sciences reformers are urging wider use of <b>interval</b> <b>estimates.</b> We believe confidence intervals can improve research communication markedly, but several problems are raised by our empirical studies of how people understand and misunderstand intervals. We describe three of these problems: an incorrect belief about confidence interval overlap {{and its relation to}} statistical significance; failure to distinguish between confidence intervals and standard error bars; and finally, neglect of the importance of research design in applying and interpreting intervals. Our suggested solution is better guidelines, or ‘rules of eye’, and improved graphical presentations to assist with confidence interval presentation and interpretation. The rules of eye are also pedagogic tools, for teaching deeper understanding of <b>interval</b> <b>estimates.</b> By confronting existing misconceptions, these guidelines should facilitate conceptual change in thinking not only about <b>interval</b> <b>estimates</b> themselves, but also the often misunderstood concept of statistical significance. CONFIDENCE INTERVALS: ADVANTAGES AND MISCONCEPTION...|$|R
