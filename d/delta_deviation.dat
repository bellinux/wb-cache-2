1|28|Public
40|$|VIX指數為市場對「標的物未來三十(日曆) 天波動程度」的預期，VIX是一序列不同履約價格買權及賣權的平均行為。若不同履約價格對未來波動率有不同預期，則形成 Volatility Skew /Smile。本研究建議以「交易日」為時間軸，來建構波動率曲面 (Implied Volatility Surface, IVS) ，以利於了解 VIX 和 Volatility Skew 每日相對變化情形。 在IVS上，定性觀察「VIX等位線」(該線上的波動率與VIX值相等) 在不同VIX水平的行為模式，發現其與 VIX 本身水平無關，卻與標的指數振幅有關。隨後，計算 VIX等位線在Delta Moneyness 軸上投影座標相對於 Black-Scholes ATM 的偏離程度，並以 DLTvix 命名之。本研究認為，DLTvix可視為選擇權市場對標的指數未來三十(日曆) 天報酬率的預期。對賣(買) 權而言，DLTvix 為負(正) ；負(正) 的越多，表示預期負(正) 報酬越高。 計算 2006 / 12 / 01 至 2010 / 02 / 28 共 802 個交易日之 DLTvix 與 RT 30 (標的指數未來三十日曆天報酬率) ，並以OLS法進行廻歸分析。結果顯示， DLTvix 對 RT 30 有統計上顯著的預測能力。因此，本研究主張，台股選擇權市場隱藏有資訊內涵，可據以預測標的指數報酬率。VIX {{index is}} {{implied by the}} current prices of {{underlying}} index option and represents expected future market volatility over the next 30 calendar days. VIX {{can be thought of}} as a weighted average of implied volatilities for put/call options across a series of strikes. Volatility Skew/Smile phenomenon result from volatility expected discrepancies between various exercise prices. We adopt the trading time, but not the expiration time, as time axis in IVS construction, this is helpful to observe relative daily variations between VIX and volatility skew. On this volatility contour plane, we observe characteristics of "VIX location line" (i. e. zero value relative with daily VIX index) in various VIX levels, and find out its flow path is independent of VIX level, but relate to underlying index variation. We then define DLTvix as Black-Scholes <b>Delta</b> <b>deviation</b> between ATM and VIX location line projection component along Delta moneyness axis. We state that DLTvix can be considered as option participators'expectation on underlying index return over the next 30 calendar days. For the put (call) option, DLTvix is negative (positive); and the lower (higher) value of DLTvix, represents the less (more) expected return. We calculate DLTvix and RT 30 (index return over the next 30 calendar days) from 2006 / 12 / 01 to 2010 / 02 / 28, a total of 802 trading days. These results computed by OLS regression model show that, DLTvix have statistically significant predictive power on RT 30. Our findings contend that the information extracted from Taiwan option market, is able to forecast the movement of underlying index. 第一章 緒論	 1 一、研究動機	 1 二、分析構想	 3 三、研究貢獻	 4 第二章 文獻探討	 5 一、VIX的資訊內涵	 5 二、利用VIX擬訂交易策略	 7 三、IVS文獻探討	 9 第三章 資料處裡	 12 一、MONEYNESS定義	 12 二、DELTA MONEYNESS IVS	 13 三、LOG MONEYNESS IVS	 15 三、VIX- ADJUSTED IVS	 17 四、IVS建構過程所做的一些假設	 20 第四章 研究成果	 21 一、IVS在不同VIX水平的定性觀察	 21 二、IVS在波段高/低點的變化情形	 25 三、量化指標的選取與檢測	 27 四、VIXLOC落點的細部檢視	 29 第五章 結論與建議	 31 一、研究主要發現	 31 二、未來精進建議	 32 參考文獻	 33 附錄：台股波段高/低點前後IVS變化	 3...|$|E
40|$|To {{improve the}} {{understanding}} of the molecular interactions of water with tetraalkyl ammonium-based ionic liquids (ILs) such as tetramethylammonium hydroxide, tetraethylammonium hydroxide, tetrapropylammonium hydroxide, and tetrabutylammonium hydroxide, thermophysical properties such as density (rho), speed of sound (u), viscosity (eta) and refractive index (n(D)) were measured and a computational study using COSMO-RS was performed. The derived properties such as excess volumes (V-E), deviation in isentropic compressibilities (<b>Delta</b> kappa(s)) <b>deviation</b> in viscosities (<b>Delta</b> eta), and <b>deviation</b> in refractive indices (Delta n(D)) under the same experimental conditions for these systems were also estimated. The observed V-E and Delta kappa(s) values are negative over the entire composition of ILs at all investigated temperatures, whereas Delta eta and Delta n(D) values are positive under the same experimental conditions. These results reveal that the ammonium-based ILs significantly affect the intermolecular interactions between the solvent molecules. The computational study allows a qualitative analysis of the results in terms of the ion dipole interactions, ion-pair formation, and hydrogen bonding between ammonium-based ILs and water...|$|R
40|$|The densities and viscosities of 1 -ethyl- 3 -methylimidazolium bis(trifluoromethylsulfonyl)) imide ([EMIM][NTf 2]) + N-methyl- 2 -pyrrolidone (NMP) and [EMIM] [NTf 2] + ethanol {{mixtures}} {{were investigated}} over the mole fraction range from (0. 1 to 0. 9) and at temperatures from (293. 15 to 323. 15) K at intervals of 5 K. The densities can be well-represented by the quadratic equation, and the viscosities {{can be represented}} {{in the form of}} the Vogel equation. The excess molar volumes (V-E) and viscosity <b>deviations</b> (<b>Delta</b> eta) were calculated, and the results were fitted to the Redlich-Kister equation using a multiparametric nonlinear regression method. The estimated parameters of the Redlich-Kister equation and standard deviation were also presented. The results showed that the densities and viscosities were dependent strongly on NMP or ethanol content. Comparatively, the viscosity <b>deviation</b> <b>Delta</b> eta was more sensitive to temperature than the excess molar volume V-E...|$|R
40|$|We study quantum Hall ferromagnets in the {{presence}} of a random electrostatic impurity potential. Describing these systems with a classical nonlinear sigma model and using analytical estimates supported by results from numerical simulations, we examine the nature of the ground state as a function of disorder strength, <b>Delta,</b> and <b>deviation,</b> deltanu, of the average Landau level filling factor from unity. Screening of an impurity potential requires distortions of the spin configuration, and in the absence of Zeeman coupling there is a disorder-driven, zero-temperature phase transition from a ferromagnet at small Delta and /deltanu/ to a spin glass at larger Delta or /deltanu/. We examine ground-state response functions and excitations...|$|R
40|$|A {{number of}} interplanetary {{missions}} now being planned involve placing deterministic maneuvers along the flight path {{to alter the}} trajectory. Lee and Boain (1973) examined the statistics of trajectory correction maneuver (TCM) magnitude with no deterministic ('bias') component. The Delta v vector magnitude statistics were generated for several values of random <b>Delta</b> v standard <b>deviations</b> using expansions in terms of infinite hypergeometric series. The present investigation uses a different technique (Monte Carlo simulation) to generate Delta v magnitude statistics for a wider selection of random <b>Delta</b> v standard <b>deviations</b> and also extends the analysis {{to the case of}} nonzero deterministic Delta v's. These Delta v magnitude statistics are plotted parametrically. The plots are useful in assisting the analyst in quickly answering questions about the statistics of Delta v magnitude for single TCM's consisting of both a deterministic and a random component. The plots provide quick insight into the nature of the Delta v magnitude distribution for the TCM...|$|R
40|$|Density and {{viscosity}} for the methanol + {{methyl methacrylate}} (MMA) binary system {{over the whole}} concentration range in the temperature range from (383. 15 to 333. 15) K were measured. From the all experimental data, the excess molar volumes V-E and viscosity <b>deviations</b> <b>delta</b> eta were calculated. Excess molar volumes and viscosity deviations were correlated by the Redlich-Kister type equations. Optimally fitted parameters are presented, and the correlation results are in satisfactory agreement with the experimental data. Density and viscosity for the methanol + methyl methacrylate (MMA) binary system over the whole concentration range in the temperature range from (383. 15 to 333. 15) K were measured. From the all experimental data, the excess molar volumes V-E and viscosity <b>deviations</b> <b>delta</b> eta were calculated. Excess molar volumes and viscosity deviations were correlated by the Redlich-Kister type equations. Optimally fitted parameters are presented, and the correlation results are in satisfactory agreement with the experimental data...|$|R
40|$|Densities and viscosities of {{the binary}} {{mixtures}} of the tributyl phosphate + hexane and tributyl phosphate + dodecane {{systems have been}} experimentally determined at different temperatures and atmospheric pressure, over the entire composition range. Other mixing properties such as the excess molar volumes and the viscosity deviations have been also obtained {{for each of the}} systems. The excess molar volumes (V (E)) and viscosity <b>deviations</b> (<b>Delta</b> eta) have been fitted to the Redlich-Kister equation, and the coefficients and estimate of the standard error values are presented. A discussion on these quantities in terms of molecular interactions is reported. Densities and viscosities of the binary mixtures of the tributyl phosphate + hexane and tributyl phosphate + dodecane systems have been experimentally determined at different temperatures and atmospheric pressure, over the entire composition range. Other mixing properties such as the excess molar volumes and the viscosity deviations have been also obtained for each of the systems. The excess molar volumes (V (E)) and viscosity <b>deviations</b> (<b>Delta</b> eta) have been fitted to the Redlich-Kister equation, and the coefficients and estimate of the standard error values are presented. A discussion on these quantities in terms of molecular interactions is reported...|$|R
40|$|An {{efficient}} {{method to}} compute local density-based outliers in high dimensional data was proposed. In our work, {{we have shown}} {{that this type of}} outlier is present even in any subset of the dataset. This property is used to partition the data set into random subsets to compute the outliers locally. The outliers are then combined from different subsets. Therefore, the local density-based outliers can be computed efficiently. Another challenge in outlier detection in high dimensional data is that the outliers are often suppressed when the majority of dimensions do not exhibit outliers. The contribution of this work is to introduce a filtering method whereby outlier scores are computed in sub-dimensions. The low sub-dimensional scores are filtered out and the high scores are aggregated into the final score. This aggregation with filtering eliminates the effect of accumulating <b>delta</b> <b>deviations</b> in multiple dimensions. Therefore, the outliers are identified correctly. In some cases, the set of outliers that form micro patterns are more interesting than individual outliers. These micro patterns are considered anomalous with respect to the dominant patterns in the dataset. In the area of anomalous pattern detection, there are two challenges. The first challenge is that the anomalous patterns are often overlooked by the dominant patterns using the existing clustering techniques. A common approach is to cluster the dataset using the k-nearest neighbor algorithm. The contribution of this work is to introduce the adaptive nearest neighbor and the concept of dual-neighbor to detect micro patterns more accurately. The next challenge is to compute the anomalous patterns very fast. Our contribution is to compute the patterns based on the correlation between the attributes. The correlation implies that the data can be partitioned into groups based on each attribute to learn the candidate patterns within the groups. Thus, a feature-based method is developed that can compute these patterns efficiently. Ph. D. Committee Chair: Mark, Leo; Committee Chair: Omiecinski, Edward; Committee Member: Kim, Jinho; Committee Member: Liu, Ling; Committee Member: Navathe, Sha...|$|R
40|$|It {{is shown}} how a {{commercial}} time interval counter {{can be used}} to measure the relative stability of two signals that are offset in frequency and mixed down to a beat note of about 1 Hz. To avoid the dead-time problem, the counter is set up to read the time interval between each beat note upcrossing and the next pulse of a 10 Hz reference pulse train. The actual upcrossing times are recovered by a simple algorithm whose outputs can be used for computing residuals and Allan variance. A noise floor-test yielded a <b>delta</b> f/f Allan <b>deviation</b> of 1. 3 times 10 to the minus 9 th power/tau relative to the beat frequency...|$|R
40|$|Particle decay during {{inflation}} is studied by implementing a dynamical renormalization group resummation {{combined with a}} small Delta expansion. <b>Delta</b> measures the <b>deviation</b> from the scale invariant power spectrum and regulates the infrared. In slow roll inflation, Delta is a simple function of the slow roll parameters epsilon_V, eta_V. We find that quantum fluctuations can self-decay {{as a consequence of}} the inflationary expansion through processes which are forbidden in Minkowski space-time. We compute the self-decay of the inflaton quantum fluctuations during slow roll inflation. For wavelengths deep inside the Hubble radius the decay is enhanced by the emission of ultrasoft collinear quanta, i. e. bremsstrahlung radiation of superhorizon quanta which becomes the leading decay channel for physical wavelengths H 3. 6 10 ^{- 9 }...|$|R
40|$|We study {{analytically}} and numerically {{the corrections}} to scaling in turbulence which arise due to finite size effects as anisotropic forcing or boundary conditions at large scales. We {{find that the}} <b>deviations</b> <b>delta</b> zeta m from the classical Kolmogorov scaling zeta m=m/ 3 of the velocity moments [is proportional to] k- zeta m decrease like delta zeta m(Re) =cmRe- 3 / 10. If, on the contrary, anomalous scaling in the inertial subrange can experimentally be verified in the large Re limit, this will support the suggestion that small scale structures should be responsible, originating form viscous effects either in the bulk (vortex tubes or sheets) or from the boundary layers (plumes or swirls), as both are underestimated in our reduced wave vector set approximation of the Navier-Stokes dynamics...|$|R
40|$|Particle decay during {{inflation}} is studied by implementing a dynamical renormalization group resummation {{combined with a}} small Delta expansion. <b>Delta</b> measures the <b>deviation</b> from the scale invariant power spectrum and regulates the infrared. In slow roll inflation, Delta is a simple function of the slow roll parameters epsilon_V, eta_V. We find that quantum fluctuations can self-decay {{as a consequence of}} the inflationary expansion through processes which are forbidden in Minkowski space-time. We compute the self-decay of the inflaton quantum fluctuations during slow roll inflation. For wavelengths deep inside the Hubble radius the decay is enhanced by the emission of ultrasoft collinear quanta, i. e. bremsstrahlung radiation of superhorizon quanta which becomes the leading decay channel for physical wavelengths H 3. 6 10 ^- 9. Comment: 27 pages, LaTex, 5. eps figures, to appear in Phys. Rev. ...|$|R
40|$|Temperature {{dependent}} Mossbauer {{measurements are}} {{done on the}} samples of La 1 - xCaxMn 1 -y (FeyO 3) -Fe- 57 with x= 0 and 0. 25, and y= 0. 01. With decreasing temperature, the specimen with x= 0. 25 shows a paramagnetic to ferromagnetic transition around 175 K. In the specimen x= 0. 0, the temperature dependence of both the center shift (delta) and the recoilless fraction (f) can be fitted very well with the Debye theory with a theta(D) = 320 +/- 50 K. But for the specimens with x= 0. 25, f and <b>delta</b> show distinct <b>deviations</b> from the Debye behavior in the temperature range in which the resistivity shows a sharp decrease. Dips observed in both the f and delta around the transition temperature suggest that the Jahn-Teller distortion observed in these systems is dynamic in nature...|$|R
40|$|Trend shift {{detection}} is {{posed as}} a two-part problem: filtering {{of the gas}} turbine measurement deltas followed {{by the use of}} edge detection algorithms. Measurement <b>deltas</b> are <b>deviations</b> in engine gas path measurements from a "good" baseline engine and are a key, health signal used for gas turbine performance diagnostics. The measurements used in this study are exhaust gas temperature, low rotor speed, high rotor speed and fuel flow, which are called cockpit measurements and are typically found on most commercial jet engines. In this study, a cascaded recursive median (RM) filter of increasing order is used for the purpose of noise reduction and outlier removal, and a hybrid edge detector that uses both gradient and Laplacian of the cascaded RM filtered signal are used for the detection of step change in the measurements. Simulated results with test signals indicate that cascaded RM filters can give a noise reduction of more than 38 % while preserving the essential features of the signal. The cascaded RM filter also shows excellent robustness in dealing with outliers, which are quite often found in gas turbine data, and call cause spurious trend detections. Suitable thresholding of the gradient edge detector coupled {{with the use of the}} Laplacian edge detector for cross checking can reduce the system false alarms and missed detection rate. Further reduction in the trend shift detection false alarm and missed detection rate can be achieved by selecting gas path measurements with higher signal-to-noise ratios...|$|R
40|$|Background. Corticosteroid {{withdrawal}} (CW) after pediatric kidney transplantation potentially improves growth {{while avoiding}} metabolic and other adverse events. We have recently reported {{the results of}} a 196 subject randomized controlled trial comparing early CW (tacrolimus, mycophenolate mofetil (MMF), daclizumab, and corticosteroids until day 4) with tacrolimus, MMF, and corticosteroid continuation (CC). At 6 months, CW subjects showed better growth with no adverse impact on acute rejection or graft survival (Am J Transplant 2010; 10 : 828 – 836). This 2 -year investigator-driven follow-up study aimed to determine whether improved growth persisted in the longer term. Methods. Data regarding growth, graft outcomes and adverse events were collected at 1 year (113 patients) and 2 years (106 patients) after transplantation. The primary endpoint, longitudinal growth calculated as <b>delta</b> height standard <b>deviation</b> score, was analyzed using a mixed model repeated measures model. Results. Corticosteroid withdrawal subjects grew better at 1 year (difference in adjusted mean change, 0. 25; 95...|$|R
40|$|Durand, M., Kafer, J., Quilliet, C., Cox, S., Talebi, S. A. and Graner, F. (2011). Statistical me-a chanics of {{two-dimensional}} shuffled foams: {{prediction of}} {{the correlation between}} geometry and topology. Phys. Rev. Letts. 107 : 168304 We propose an analytical model for the statistical mechanics of shuffled two-dimensional foams with moderate bubble size polydispersity. It predicts without any adjustable parameters {{the correlations between the}} number of sides n of the bubbles (topology) and their areas A (geometry) observed in experiments and numerical simulations of shuffled foams. Detailed statistics show that in shuffled cellular patterns n correlates better with sqrt(A) (as claimed by Desch and Feltham) than with A (as claimed by Lewis and widely assumed in the literature). At the level of the whole foam, standard <b>deviations</b> <b>Delta</b> n and Delta A are in proportion. Possible applications include correlations of the detailed distributions of n and A, three-dimensional foams, and biological tissues. Peer reviewe...|$|R
40|$|Line-edge {{roughness}} induced fin-edge roughness (FER) is {{the primary}} source of V-T variation in FinFETs. Conventionally, stochastic simulations are performed to predict the device variability due to FER for a technology, which are computationally expensive. An analytical formulation to predict variability due to FER enables understanding of the effect of input parameters as well as provides quantitative results at fractional computational costs. In this paper, we develop and present an analytical model to estimate saturation V-T (V-T-sat) variability due to FER. The model is capable of capturing the V-T variability dependence on device parameters (L-G and W-fin) and variability parameters (correlation length Lambda and standard <b>deviation</b> <b>Delta)</b> accurately. The entire V-T-sat distribution obtained by the model is also presented and compared against the VT-sat distribution of stochastic simulations to show that the model captures the distribution effectively. We show that not only sigma V-T but even mu V-T is affected by variability parameters. Hence, such modeling is critical to defining nominal FinFET structure (L-G and W-fin), which is affected by variability (Lambda and Delta) especially for scaled FinFETs, where quantum-confinement effects are enhanced...|$|R
40|$|The {{physicochemical}} {{properties of}} the binary mixtures of magneticionic liquids (MILs) are essential for industrial process designs of their applications. In this work, three MILs were synthesized and characterized, including 1 -butyl- 3 -methylimidazolium tetrachioroferrate ([13 mim][FeCl 4]), 1 -hexyl- 3 -methylimidazolium tetrachloroferrate ([Hmim][FeCl 4]) and 1 -octy 1 - 3 -methylimidazolium tetrachloroferrate (10 miml[FeCl 4]). Densities and viscosities of binary mixtures of these MILs with ethyl acetate (EA) were determined over {{the whole range of}} compositions at temperatures (293. 15 to 323. 15) K at 5 K intervals. The data of densities as a function of temperature were fitted with linear equation and viscosities were correlated with Vogel-Fucher-Tammann (VET) equation. Results showed that the densities and viscosities of the binary mixtures decreased significantly with the IL concentration decreasing and with temperature increasing. Excess molar volumes (V-E) and viscosity <b>deviations</b> (<b>Delta</b> eta) were calculated and fitted well with the Redlich-Kister equation. The negative V-E and Delta eta over the entire composition range indicated that there were stronger interactions between MILs and EA than those among MILs and among EA. (C) 2017 Elsevier B. V. All rights reserved...|$|R
40|$|Background: It {{has been}} {{difficult}} to demonstrate circadian rhythm in the two parameters of heart rate turbulence, turbulence onset (TO) and turbulence slope (TS). Objective: To devise a new method for detecting circadian rhythm in noisy data, and apply it to selected Holter recordings from two post-myocardial infarction databases, Cardiac Arrhythmia Suppression Trial (CAST, n= 684) and Innovative Stratification of Arrhythmic Risk (ISAR, n= 327). Methods: For each patient, TS and TO were calculated for each hour with > 4 VPCs. An autocorrelation function Corr(Delta t) = was then calculated, and averaged over all patients. Positive Corr(Delta t) indicates that TS at a given hour and Delta t hours later are similar. TO was treated likewise. Simulations and mathematical analysis showed that circadian rhythm required Corr(Delta t) to have a U-shape consisting of positive values near Delta t= 0 and 23, and negative values for intermediate <b>Delta</b> t. Significant <b>deviation</b> of Corr(Delta t) from the correlator function of pure noise was evaluated as a chi-squared value. Results: Circadian patterns were not apparent in hourly averages of TS and TO plotted against clock time, which had large error bars. Their correlator functions, however, produced chi-squared values of ~ 10 in CAST (both p< 0. 0001) and ~ 3 in ISAR (both p< 0. 0001), indicating presence of circadian rhythmicity. Conclusion: Correlator functions may be a powerful tool for detecting presence of circadian rhythms in noisy data, even with recordings limited to 24 hours. Comment: 26 pages, 4 figures include...|$|R
40|$|We {{analyzed}} {{measurements of}} ion number density {{made by the}} retarding potential analyzer aboard the Atmosphere Explorer-E (AE-E) satellite, which was in an approximately circular orbit at an altitude near 300 km in 1977 and later at an altitude near 400 km. Large-scale (greater than 60 km) density measurements in the high-altitude regions show large depletions of bubble-like structures which are confined to narrow local time longitude, and magnetic latitude ranges, while those in the low-altitude regions show relatively small depletions which are broadly distributed,in space. For this reason we considered the altitude regions below 300 km and above 350 km and investigated the global distribution of irregularities using the rms <b>deviation</b> <b>delta</b> N/N over a path length of 18 km {{as an indicator of}} overall irregularity intensity. Seasonal variations of irregularity occurrence probability are significant in the Pacific regions, while the occurrence probability is always high in die Atlantic-African regions and is always low in die Indian regions. We find that the high occurrence probability in the Pacific regions is associated with isolated bubble structures, while that near 0 deg longitude is produced by large depictions with bubble structures which are superimposed on a large-scale wave-like background. Considerations of longitude variations due to seeding mechanisms and due to F region winds and drifts are necessary to adequately explain the observations at low and high altitudes. Seeding effects are most obvious near 0 deg longitude, while the most easily observed effect of the F region is the suppression of irregularity growth by interhemispheric neutral winds...|$|R
40|$|A new {{approach}} is hereby presented {{to study the}} influence of surface properties on ultrafine alumina (Al 2 O 3) particles towards a fluidized performance by changing the ultrafine particles (Al 2 O 3) surface hydrophobic degree. A number of variables that affects fluidization performance of ultrafine particles (Al 2 O 3), including hydrophobicity has been studied. The minimum full fluidization velocities (U-mff), average bed pressure drop (<b>Delta</b> P-a), standard <b>deviation</b> in pressure drop (sigma(p)) and so on, {{has been found to}} depend on the hydrophobic degree, which {{is a function of the}} ultrafine particles (Al 2 O 3) surface properties. After treatment of the ultrafine particles (Al 2 O 3) with a surfactant, the liquid-solid contact angle on the ultrafine particles (Al 2 O 3) raises, thus changing the interaction between the ultrafine particles (Al 2 O 3) and also between the ultrafine particles (Al 2 O 3) with the micro-fluidized bed wall surface. Furthermore, some changes in important parameters such as particle size (simple agglomerate size), bulk density, flow function plots, maximum wall friction angle, BET parameters and so on were also evaluated. The fluidization of ultrafine particles is attractive, if the interaction forces between the ultrafine particles (Al 2 O 3) and also between the ultrafine particles (Al 2 O 3) with the micro-fluidized bed wall surface can be artificially controlled. Moreover, it is possible to achieve a better fluidization performance on the ultrafine particles (Al 2 O 3). However, more investigation is required to improve the fluidization performance of the ultrafine particles. (C) 2016 Elsevier B. V. All rights reserved. </p...|$|R
40|$|This thesis {{explores the}} effects of fluid flow on shear {{localization}} and frictional strength of fault gouge {{through the use of}} a coupled 2 -phase (pore fluid-grain) Finite Difference-Discrete Element Numerical model. The model simulates slip at earthquake velocities (~ 1 m/s) in a fluid saturated gouge-filled fault. We find three types of shear behavior: (I) distributed shear, (II) random internal localization, and (III) boundary localization. Each shear type is dependent on the applied shear velocity, V, effective confining stress, N, and internal permeability, k. Through quantitative analysis of the positions and magnitude of localized shear bands, we show under which conditions the presence of and transitions between these shear types will occur. During shear, fluid pressure <b>deviations,</b> <b>delta</b> P, are generated by dilation and compaction cycles. The fluid effects on the system are more pronounced in simulations with higher V and lower N and k. Relative to the dry experiments, fluid saturated systems have an increased localization toward the boundaries of the gouge layer (type III), and no occurrence of distributed (type I) shear. Systems with lower N and k show liquefaction events. Liquefaction events originate from increases in fluid pressure, delta P, around force chains between grains. Once delta P, the high pressures weaken the frictional forces between grains and destroy force chains. Shear then occurs at essentially zero friction until a new grain configuration recreates force chains. A reduction in mean friction is seen for systems with large liquefaction events (without inclusion of thermal pressurization), which could introduce a new mechanism in low friction faults. We also find that systems undergoing different types of shear will all trend toward type (III) shear following a liquefaction event...|$|R
40|$|We {{study the}} final-state {{interaction}} (FSI) effects in charmless B_u,d,s to PP decays. We consider a FSI approach with both short- and long-distance contributions, where {{the former are}} from in-elastic channels and are contained in factorization amplitudes, while the latter are from the residual rescattering among PP states. Flavor SU(3) symmetry is used to constrain the residual rescattering S-matrix. We fit to all available data on the CP-averaged decay rates and CP asymmetries, and make predictions on unmeasured ones. Our main results are as follows: (i) Results are in agreement with data {{in the presence of}} FSI. (ii) For B decays, the pi^+pi^- and pi^ 0 pi^ 0 rates are suppressed and enhanced respectively by FSI. (iii) The FSI has a large impact on direct CP asymmetries of many modes. (iv) The <b>deviation</b> (<b>Delta</b> A) between A(Bbar^ 0 to K^-pi^+) and A(B^-to K^-π^ 0) can be understood in the FSI approach. (v) Sizable and complex color-suppressed tree amplitudes, which are crucial for the large π^ 0 π^ 0 rate and Delta A, are generated through exchange rescattering. The correlation of the ratio B(pi^ 0 pi^ 0) /B(pi^+pi^-) and Delta A is studied. (vi) The B^- to pi^-pi^ 0 direct CP violation is very small and is not affected by FSI. (vii) Several B_s decay rates are enhanced. In particular, the eta'eta' branching ratio is enhanced to the level of 1. 0 X 10 ^- 4, which can be checked experimentally. (viii) Time-dependent CP asymmetries S in B_d,s decays are studied. CP asymmetries in these modes will be useful to test the SM. Comment: 33 pages, 6 figures, version to appear in Phys. Rev. ...|$|R
40|$|Recent {{discoveries of}} {{supposedly}} pure alpha-tetragonal boron require to revisit its structure. The system is also interesting {{with respect to}} a new type of geometrical frustration in elemental crystals, which was found in beta-rhombohedral boron. Based on density functional theory calculations, the present study has resolved the structural and thermodynamic characteristics of pure alpha-tetragonal boron. Different from beta-rhombohedral boron, the conditions for stable covalent bonding (a band gap and completely filled valence bands) are almost fulfilled at a composition B_ 52 with two 4 c interstitial sites occupied. This indicates that the ground state of pure alpha-tetragonal boron is stoichiometric. However, the covalent condition is not perfectly fulfilled because non-bonding in-gap states exist that cannot be eliminated. The half occupation of the 4 c sites yields a macroscopic amount of residual entropy, which is as large as that of beta-rhombohedral boron. Therefore, alpha-tetragonal boron can be classified as an elemental crystal with geometrical frustration. Deviations from stoichiometry can occur only at finite temperatures. Thermodynamic considerations show that <b>deviations</b> <b>delta</b> from the stoichiometric composition B_(52 +delta) are small and positive. For reported high-pressure syntheses conditions delta is predicted to be about 0. 1 to 0. 2. An important difference between pure and C- or N-containing alpha-tetragonal boron is found in the occupation of interstitial sites: the pure form prefers to occupy the 4 c sites, whereas in C- or N-containing forms a mixture of 2 a, 8 h, and 8 i sites are occupied. The present article provides relations of site occupation, delta values, and lattice parameters, which enable us to identify pure alpha-tetragonal and distinguish the pure form from other ones. Comment: 31 pages, 8 figures, 5 tables, accepted by Phys. Rev. ...|$|R
40|$|In the {{standard}} forward shock model for gamma-ray burst (GRB) afterglow, the observed afterglow emission is synchrotron radiation from a quasi-spherical, adiabatic, self-similar, relativistic blast wave, that propagates into the external medium. This model predicts a smooth light curve where the flux scales {{as a power}} law in time, and may at most smoothly transition to a different power law. However, some GRB afterglow light curves show significant variability, which often includes episodes of rebrightening. Such temporal variability had been attributed in several cases to a large enhancement in the external density, or a density 'bump', that is encountered by the self-similar adiabatic blast wave. Here we {{examine the effect of}} a sharp increase in the external density on the afterglow light curve in this scenario by considering, for the first time, a full treatment of both the hydrodynamic evolution and the radiation. To this end we develop a semi-analytic model for the light curve and carry out numerical simulations using a one-dimensional hydrodynamic code together with a synchrotron radiation code. Two spherically symmetric cases are explored in detail - a density jump in a uniform external medium (which is used to constrain the effect of a density clump) and a wind termination shock. We find that even a very sharp (modelled as a step function) and large (by a factor of a > 1) increase in the external density does not produce sharp features in the light curve, and cannot account for significant temporal variability in GRB afterglows in the forward shock model. For a wind termination shock, the light curve smoothly transitions between the asymptotic power laws over about one decade in time, and there is no rebrightening in the optical or X-rays that could serve as a clear observational signature. For a sharp jump in a uniform density profile, we find that the maximal <b>deviation</b> <b>Delta</b> alpha(max) of the temporal decay index alpha from its asymptotic value (at early and late times) is bounded (e. g, Delta alpha(max) < 0. 4 for a = 10); Delta alpha(max) slowly increases with a, converging to Delta alpha(max) approximate to 1 at very large a values. Therefore, no optical rebrightening is expected in this case as well. In the X-rays, while the asymptotic flux is unaffected by the density jump, the fluctuations in alpha are found to be comparable to those in the optical. Finally, we discuss the implications of our results for the origin of the observed fluctuations in several GRB afterglows...|$|R
40|$|Thesis (Master's) [...] University of Washington, 2017 - 06 A {{proof-of-concept}} software-in-the-loop {{study is}} performed {{to assess the}} accuracy of predicted net and charge-gaining energy consumption for potential effective use in optimizing powertrain management of hybrid vehicles. With promising results of improving fuel efficiency of a thermostatic control strategy for a series, plug-ing, hybrid-electric vehicle by 8. 24 %, the route and speed prediction machine learning algorithms are redesigned and implemented for real- world testing in a stand-alone C++ code-base to ingest map data, learn and predict driver habits, and store driver data for fast startup and shutdown of the controller or computer used to execute the compiled algorithm. Speed prediction is performed using a multi-layer, multi-input, multi- output neural network using feed-forward prediction and gradient descent through back- propagation training. Route prediction utilizes a Hidden Markov Model with a recurrent forward algorithm for prediction and multi-dimensional hash maps to store state and state distribution constraining associations between atomic road segments and end destinations. Predicted energy is calculated using the predicted time-series speed and elevation profile over the predicted route and the road-load equation. Testing of the code-base is performed over a known road network spanning 24 x 35 blocks on the south hill of Spokane, Washington. A large set of training routes are traversed once to add randomness to the route prediction algorithm, and {{a subset of the}} training routes, testing routes, are traversed to assess the accuracy of the net and charge-gaining predicted energy consumption. Each test route is traveled a random number of times with varying speed conditions from traffic and pedestrians to add randomness to speed prediction. Prediction data is stored and analyzed in a post process Matlab script. The aggregated results and analysis of all traversals of all test routes reflect the performance of the Driver Prediction algorithm. The error of average energy gained through charge-gaining events is 31. 3 % and the error of average net energy consumed is 27. 3 %. The average delta and average standard <b>deviation</b> of the <b>delta</b> of predicted energy gained through charge-gaining events is 0. 639 and 0. 601 Wh respectively for individual time-series calculations. Similarly, the average delta and average standard <b>deviation</b> of the <b>delta</b> of the predicted net energy consumed is 0. 567 and 0. 580 Wh respectively for individual time-series calculations. The average <b>delta</b> and standard <b>deviation</b> of the <b>delta</b> of the predicted speed is 1. 60 and 1. 15 respectively also for the individual time-series measurements. The percentage of accuracy of route prediction is 91 %. Overall, test routes are traversed 151 times for a total test distance of 276. 4 km...|$|R
40|$|Sediment {{dispersal}} at {{the river}} mouths {{has been an important}} topic in the fields of geomorphology and hydraulics {{for a long time and}} estimating sediment budget of the deltas is a principal method for quantifying the sediment dispersal system at many river mouths. Many reports about sediment dispersal at the Yellow River mouth have been given previously using this method. However, since the dry bulk density of deposits in the delta and the boundaries set for calculating the volumes of deposits did not receive a proper treatment, big discrepancy existed between estimates of sediment budgets of the delta provided by the previous studies. Considering the effects of grain size composition, burial depth, and exposure conditions on the dry bulk density of deposits and based on abundant data about the dry bulk density of deposits in the delta, this study defined the dry bulk densities for the deposits in the delta plain and buried delta front deposits, buried prodelta, exposed subaqueous prodelta, and newly deposited top 1 m layers on the delta front. Combining the constructed models of dry bulk density for different depositional settings with the results of analyzing the sedimentary framework of the delta, sediment budgets at Diaokouhe lobe of the Yellow River delta were calculated. The foot of the delta front slope was set as the outer margin of area for defining the sediment budgets. This margin is of geomorphologic significance and is easy to be located on the surface of delta recorded by the bathymetric data. Results show that sediment deposited in the delta plain and front of the Diaokouhe lobe over the period from 1965 to 1974 was about 7. 10 × 10 9 tons, accounting for 73. 5 % of the incoming sediment. Errors resulting from ignoring clay layers in the deposits on delta plain and front, consolidation of soft layers underlying <b>delta</b> deposits, and <b>deviations</b> in records of the incoming sediment were proved to be about 2. 6 % for the percentage of sediment deposited in the delta, suggesting the higher reliability of the sediment budgets given by this study. From the mass and volume of sediments deposited in the Diaokouhe lobe over the period of 1965 - 1974, a mean dry bulk density of 1. 36 g/cm 3 was acquired. On account of the dominance of silt in the deposits of the delta, it seems to be an appropriate approximation of the mean dry bulk density for the deposits in the other lobes of the delta. 以往在黃河三角洲沉積量的估算中,對沉積物干容重和計算邊界等問題不夠重視,導致計算結果存在明顯出入。本項研究通過廣泛收集資料和大量采樣分析得到了多種沉積環境下沉積物干容重的計算模型,結合三角洲沉積結構分析和利用地形測量數據,計算了黃河口釣口河流路時期亞三角洲不同時期的沉積量。其中 196 5 年至 1974 年間釣口河亞三角洲前緣坡腳以內的總淤積量為 71. 0 億t。其平均干容重為 1. 36 g/cm 3 。這一干容重用于估算其它亞三角洲沉積量不會造成明顯誤差。認為忽略三角洲下松軟沉積層的壓實沉降、三角洲平原相和前緣相中粘性土與非粘性土干容重的差別以及來沙量的測量誤差對計算結果影響較小...|$|R
40|$|Enhancements {{have been}} made to the REBUS- 3 /DIF 3 D code suite to {{facilitate}} its use for the design and analysis of prismatic Very High Temperature Reactors (VHTRs). A new cross section structure, using table-lookup, has been incorporated to account for cross section changes with burnup and fuel and moderator temperatures. For representing these cross section dependencies, three new modules have been developed using FORTRAN 90 / 95 object-oriented data structures and implemented within the REBUS- 3 code system. These modules provide a cross section storage procedure, construct microscopic cross section data for all isotopes, and contain a single block of banded scattering data for efficient data management. Fission products other than I, Xe, Pm, and Sm, can be merged into a single lumped fission product to save storage space, memory, and computing time without sacrificing the REBUS- 3 solution accuracy. A simple thermal-hydraulic (thermal-fluid) feedback model has been developed for prismatic VHTR cores and implemented in REBUS- 3 for temperature feedback calculations. Axial conduction was neglected in the formulation because of its small magnitude compared to radial (planar) conduction. With the simple model, the average fuel and graphite temperatures are accurately estimated compared to reference STAR-CD results. The feedback module is currently operational for the non-equilibrium fuel cycle analysis option of REBUS- 3. Future work should include the extension of this capability to the equilibrium cycle option of the code and additional verification of the feedback module. For the simulation of control rods in VHTR cores, macroscopic cross section <b>deviations</b> (<b>deltas)</b> have been defined to account for the effect of control rod insertion. The REBUS- 3 code has been modified to use the appropriately revised cross sections when control rods are inserted in a calculation node. In order to represent asymmetric core blocks (e. g., fuel blocks or reflector blocks containing asymmetric absorber rods), surface-dependent discontinuity factors based on nodal equivalence theory have been introduced into the nodal diffusion theory option of the DIF 3 D code (DIF 3 D-nodal) to improve modeling accuracy. Additionally, the discontinuity factors based on the Simplified Equivalence Theory (SET) have been incorporated as an alternative and may be employed for both the DIF 3 D-nodal and DIF 3 D-VARIANT (nodal transport) solution options. Two- and three-dimensional core calculations have been performed using the routines developed and modified in this work, along with cross sections generated from single fuel block and one-dimensional or two-dimensional fuel-reflector model. Generally, REBUS- 3 /DIF 3 D results for the core multiplication factor and power distribution are found to be in good agreement with reference results (generated with MCNP continuous energy calculations) particularly when discontinuity factors are applied. The DIF 3 D-VARIANT option was found to provide a more accurate solution in its diffusion approximation than the DIF 3 D-nodal option. Control rod worths can be estimated with acceptably small errors compared to MCNP results. However, estimation of the core power tilt needs to be improved by introducing the surface-dependent discontinuity factor capability in DIF 3 D-VARIANT...|$|R
40|$|The routing problem (finding {{an optimal}} route from {{one point in}} a {{computer}} network to another) is surrounded by impossibility results. These results are usually expressed as lower and upper bounds {{on the set of}} nodes (or the set of links) of a network and represent the complexity of a solution to the routing problem (a routing function). The routing problem dealt with here, in particular, is a dynamic one (it accounts for network dynamics) and concerns wireless networks of sensors. Sensors form wireless links of limited capacity and time-variable quality to route messages amongst themselves. It is desired that sensors self-organize ad hoc in order to successfully carry out a routing task, e. g. provide daily soil erosion reports for a monitored watershed, or provide immediate indications of an imminent volcanic eruption, in spite of network dynamics. Link dynamics are the first barrier to finding an optimal route between a node x and a node y in a sensor network. The uncertainty of the outcome (the best next hop) of a routing function lies partially with the quality fluctuations of wireless links. Take, for example, a static network. It is known that, given the set of nodes and their link weights (or costs), a node can compute optimal routes by running, say, Dijkstra's algorithm. Link dynamics however suggest that costs are not static. Hence, sensors need a metric (a measurable quantity of uncertainty) to monitor for fluctuations, either improvements or degradations of quality or load; when a fluctuation is sufficiently large (say, by Delta), sensors ought to update their costs and seek another route. Therein lies the other fundamental barrier to find an optimal route - complexity. A crude argument would suggest that sensors (and their links) have an upper bound on the number of messages they can transmit, receive and store due to resource constraints. Such messages can be application traffic, in which case it is desirable, or control traffic, in which case it should be kept minimal. The first type of traffic is demand, and a user should provision for it accordingly. The second type of traffic is overhead, and it is necessary if a routing system (or scheme) is to ensure its fidelity to the application requirements (policy). It is possible for a routing scheme to approximate optimal routes (by Delta) by reducing its message and/or memory complexity. The common denominator of the routing problem and the desire to minimize overhead while approximating optimal routes is <b>Delta,</b> the <b>deviation</b> (or stretch) of a computed route from an optimal one, as computed by a node that has instantaneous knowledge of the set of all nodes and their interaction costs (an oracle). This dissertation deals with both problems in unison. To do so, it needs to translate the policy space (the user objectives) into a metric space (routing objectives). It does so by means of a cost function that normalizes metrics into a number of hops. Then it proceeds to devise, design, and implement a scheme that computes minimum-hop-count routes with manageable complexity. The theory presented is founded on (well-ordered) sets with respect to an elementary proposition, that a route from a source x to a destination y can be computed either by y sending an advertisement to the set of all nodes, or by x sending a query to the set of all nodes; henceforth the proactive method (of y) and the reactive method (of x), respectively. The debate between proactive and reactive routing protocols appears in many instances of the routing problem (e. g. routing in mobile networks, routing in delay-tolerant networks, compact routing), and it is focussed on whether nodes should know a priori all routes and then select the best one (with the proactive method), or each node could simply search for a (hopefully best) route on demand (with the reactive method). The proactive method is stateful, as it requires the entire metric space - the set of nodes and their interaction costs - in memory (in a routing table). The routes computed by the proactive method are optimal and the lower and upper bounds of proactive schemes match those of an oracle. Any attempt to reduce the proactive overhead, e. g. by introducing hierarchies, will result in sub-optimal routes (of known stretch). The reactive method is stateless, as it requires no information whatsoever to compute a route. Reactive schemes - at least as they are presently understood - compute sub-optimal routes (and thus far, of unknown stretch). This dissertation attempts to answer the following question: "what is the least amount of state required to compute an optimal route from a source to a destination?" A hybrid routing scheme is used to investigate this question, one that uses the proactive method to compute routes to near destinations and the reactive method for distant destinations. It is shown that there are cases where hybrid schemes can converge to optimal routes, despite possessing incomplete routing state, and that the necessary and sufficient condition to compute optimal routes with local state alone is related neither to the size nor the density of a network; it is rather the circumference (the size of the largest cycle) of a network that matters. Counterexamples, where local state is insufficient, are discussed to derive the worst-case stretch. The theory is augmented with simulation results and a small experimental testbed to motivate the discussion on how policy space (user requirements) can translate into metric spaces and how different metrics affect performance. On the debate between proactive and reactive protocols, it is shown that the two classes are equivalent. The dissertation concludes with a discussion on the applicability of its results and poses some open problems. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R

