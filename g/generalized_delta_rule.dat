24|235|Public
50|$|The output {{targets in}} the {{response}} units (i.e., the examinee attributes) are compared to the pattern associated with each stimulus input or exemplar (i.e., the expected response patterns). The solution produced initially with the stimulus and association connection weights {{is likely to be}} discrepant resulting in a relatively large error. However, this discrepant result can be used to modify the connection weights thereby leading to a more accurate solution and a smaller error term. One popular approach for approximating the weights so the error term is minimized is with a learning algorithm called the <b>generalized</b> <b>delta</b> <b>rule</b> that is incorporated in a training procedure called back propagation of error.|$|E
40|$|The <b>generalized</b> <b>delta</b> <b>rule</b> (which is {{also known}} as error backpropagation) is a {{significant}} advance over previous procedures for network learning. In this paper, we compare network learning using the <b>generalized</b> <b>delta</b> <b>rule</b> to human learning on two concept identification tasks: • Relative ease of concept identification • Generalizing from incomplete dat...|$|E
40|$|This paper {{describes}} a neuro-fuzzy paradigm which unifies several neural and fuzzy paradigms. Similarities and {{differences among the}} various neural and fuzzy paradigms are analyzed and results {{have been used to}} derive the proposed unification paradigm. A conversion method is also presented to map perceptrons, radial basis networks and fuzzy systems onto each other. It will also be shown that, back-propagation, self-organizing and fuzzy training rules are particular cases of the <b>generalized</b> <b>delta</b> <b>rule</b> applied to the proposed unification paradigm...|$|E
40|$|In this paper, a {{geometrical}} {{scheme is}} presented to show how to overcome an encountered problem arising {{from the use of}} <b>generalized</b> <b>delta</b> learning <b>rule</b> within competitive learning model. It is introduced a theoretical methodology for describing the quantization of data via rotating prototype vectors on hyper-spheres. The proposed learning algorithm is tested and verified on different multidimensional datasets including a binary class dataset and two multiclass datasets from the UCI repository, and a multiclass dataset constructed by us. The proposed method is compared with some baseline learning vector quantization variants in literature for all domains. Large number of experiments verify the performance of our proposed algorithm with acceptable accuracy and macro f 1 scores...|$|R
40|$|About {{a decade}} ago, {{artificial}} neural networks (ANN) {{have been introduced}} to chemometrics for solving problems in analytical chemistry. ANN {{are based on the}} functioning of the brain and can be used for modeling complex relationships within chemical data. An ANN-model can be obtained by earning or training with examples. The model can be realized without any a priory theoretical assumptions about the associations in the data, as is the case for parametric physical or chemical models. The universal applicability, the simple concept and the impressive modeling capability have contributed to the enormous number of successful applications in analytical chemistry, published in the last ten years. In the literature, various paradigms of neural networks, based on different biological and psychological concepts, have been elaborated. The most familiar and the most frequently used concept is given by the multi-layer feed-forward neural network also referred as the error back-propagation network. The MLF-network, which is the central objective in this thesis, {{can be used as a}} non-parametric method for modeling nonlinear relations in chemical data. MLF-networks are commonly trained by means of the <b>generalized</b> <b>delta</b> learning <b>rule</b> which can be considered as an iterative least mean squares method. The <b>generalized</b> <b>delta</b> learning <b>rule</b> has proven to be robust and moreover generally applicable for different network-configurations. However, slow learning behavior is frequently observed, yielding unacceptable long training times. Especially for chemical problems where extensive numbers of variables (f. e. spectra) or objects (f. e. images) are used, the slow learning behavior becomes a serious drawback. The last five years, a lot of fundamental research has been conducted on faster and more efficient algorithms for training MLF networks. In this thesis, some improved training methods have been described and applied on a number of chemical problems...|$|R
40|$|Artificial neural {{networks}} [...] - especially those using the error backpropagation algorithm [...] - {{are capable of}} learning to control an unknown plant by autonomously extracting the necessary information from the plant. Following the approach of Psaltis et al. and Saerens et al. a control architecture based on error backpropagation has been developed and trained to control a third order linear and time invariant plant with dead [...] time. Simulation {{results show that the}} network is able to invert the plant's behaviour and characteristics, thus learning to control the plant accurately. The time to reach the desired outputs of the plant decreases while learning. It is accelerated by local adaptation of the learning rate. Keywords: Error [...] Backpropagation, <b>generalized</b> <b>delta</b> [...] <b>rule,</b> interactive and autonomous learning, qualitative knowledge, global and local learning rate adaptation, self [...] tuning. 1 Introduction The primary objective of a controller is to provide appropriate inputs to a plant in or [...] ...|$|R
40|$|Neural Network Environment on Transputer System (NNETS) {{computer}} program provides users {{high degree of}} flexibility in creating and manipulating wide variety of neural-network topologies at processing speeds not found in conventional computing environments. Supports back-propagation and back-propagation-related algorithms. Back-propagation algorithm used is implementation of Rumelhart's <b>generalized</b> <b>delta</b> <b>rule.</b> NNETS developed on INMOS Transputer(R). Predefines back-propagation network, Jordan network, and reinforcement network to assist users in learning and defining own networks. Also enables users to configure other neural-network paradigms from NNETS basic architecture. Small portion of software written in OCCAM(R) language...|$|E
40|$|Networks. This thesis {{explores the}} use of Time-Delay Neural Networks (TDNNs) in a {{practical}} speaker adaptive phoneme recognition system. Investigations are made into suitable technologies and architectures for the system, and evaluations are made on these design decisions. Novel algorithms for recursive re-labeling of phoneme samples and for network pruning are proposed and evaluated, together with a simple speaker-adaptation system {{taking advantage of the}} inherent on-line adaptive nature of <b>Generalized</b> <b>Delta</b> <b>Rule</b> (GDR) based neural networks. The proposed system is evaluated against a similar Hidden Markov Model (HMM) system in terms of recognition accuracies in clean and adverse conditions. Finally, some recommendations for future work are made. Keywords: Speaker adaptation, phoneme recognition, time-delay neural networks, Kohone...|$|E
40|$|In {{this paper}} the Sigma-if {{artificial}} neural network model is considered, which is a generalization of an MLP network with sigmoidal neurons. It {{was found to be}} a potentially universal tool for automatic creation of distributed classification and selective attention systems. To overcome the high nonlinearity of the aggregation function of Sigma-if neurons, the training process of the Sigma-if network combines an error backpropagation algorithm with the self-consistency paradigm widely used in physics. But for the same reason, the classical backpropagation delta rule for the MLP network cannot be used. The general equation for the backpropagation <b>generalized</b> <b>delta</b> <b>rule</b> for the Sigma-if neural network is derived and a selection of experimental results that confirm its usefulness are presented...|$|E
40|$|This paper {{evaluates the}} {{performance}} of restricted feed forward neural network trained by hybrid evolutionary algorithm with <b>generalized</b> <b>delta</b> learning <b>rule</b> for distributed error to obtain the pattern classification for the given training set of Handwritten Hindi ‘MATRAS’. Generally, the feed forward neural network considers the performance index as back-propagated instantaneous unknown error for output of hidden layers. Within this proposed endeavor, we are considering the performance index of distributed instantaneous unknown errors i. e. different errors for different layers. In this case, the convergence is obtained only when the minimum of every error on different layer is determined. The simulation for the performance evaluation is conducted for hand-written ‘MATRAS’ of Hindi language scripted by five different people. These samples are stored as scanned images. The MATLAB is {{used to determine the}} densities of these scanned images after partitioning each image into 16 portions. These 16 densities for each character are used as an input pattern of training set. We consider five trials for each learning method and results are presented with their mean value...|$|R
40|$|The <b>delta</b> <b>rule</b> of {{associative}} learning {{has recently been}} used in several models of human category learning, and applied to categories with different relative frequencies, or base rates. Previous research has emphasized predictions of the <b>delta</b> <b>rule</b> after extensive learning. Our first experiment measures the relative acquisition rates of categories with different base rates, and the <b>delta</b> <b>rule</b> significantly and systematically deviates from the human data. We suggest that two additional mechanisms are involved, namely, short-term memory and strategic guessing. Two additional experiments highlight {{the effects of these}} mechanisms. The mechanisms are formalized and combined with the <b>delta</b> <b>rule,</b> and provide good fits to the data from all three experiments. Several recent models of category learning in humans incorporated the <b>delta</b> <b>rule</b> of {{associative learning}} (Rumelhart, Hinton, & Williams, 1986). The <b>delta</b> <b>rule</b> posits that the growth in the strength of association between a cue and an outcome is error driven: The associative strength changes in magnitude proportionally to the discrepancy, or error, betwee...|$|R
5000|$|For a {{single-layer}} network, this expression {{becomes the}} <b>Delta</b> <b>Rule.</b>|$|R
40|$|Learning {{algorithms}} {{are described}} for layered feedforward type neural networks, {{in which a}} unit generates a real-valued output through a logistic function. The problem of adjusting the weights of internal hidden units {{can be regarded as}} a problem of estimating (or identifying) constant parametes with a non-linear observation equation. The present algorithm based on the extended Kalman filter has just the time-varying learning rate, while the well-known back-propagation (or <b>generalized</b> <b>delta</b> <b>rule)</b> algorithm based on gradient descent has a constant learning rate. From some simulation examples it is shown that when a sufficiently trained network is desired, the learning speed of the proposed algorithm is faster than that of the traditional back-propagation algorithm...|$|E
40|$|This paper focuses {{classification}} of crystal classes in a periodic table using the known neural network (NN) learning algorithm, viz, <b>generalized</b> <b>delta</b> <b>rule</b> (GDR) by feeding {{the set of}} input features in max-min-max sub arrays. We have taken eighteen independent physical parameters for each element, trained the network from atomic number (AN) 1 to 84 and we validated the crystal class from AN 86 to 95 from the trained network and achieved 100 per cent accuracy, which was later extended from AN 96 to 120. Further, we have also evaluated the dependencies of the neural network in different confidence intervals and hidden layers, We would like to call this learning algorithm as max-min-max GDR...|$|E
40|$|AbstractIn {{this paper}} the {{comparison}} of the surface roughness prediction models based on response surface methodology (RSM) and artificial neural networks (ANN) is described. The models were developed based on five-level design of experiments conducted on Aluminum alloy 6061 work material with spindle speed, interference, feed, and number of tool pass as the roller burnishing process parameters. The ANN predictive models of surface roughness was developed using a multilayer feed forward neural network and trained {{with the help of}} an error back propagation learning algorithm based on the <b>generalized</b> <b>delta</b> <b>rule.</b> Mathematical models of second order RSM and developed ANN models were compared for surface roughness. The comparison evidently indicates that the prediction capabilities of ANN models are far better as compared to the RSM models. The minutiae of experimentation, development of model, testing, and performance comparison are presented in the paper...|$|E
5000|$|While the <b>delta</b> <b>rule</b> {{is similar}} to the perceptron's update rule, the {{derivation}} is different. The perceptron uses the Heaviside step function as the activation function , and that means that [...] does not exist at zero, and is equal to zero elsewhere, which makes the direct application of the <b>delta</b> <b>rule</b> impossible.|$|R
5000|$|In machine learning, the <b>delta</b> <b>rule</b> is a {{gradient}} descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. [...] It {{is a special}} case of the more general backpropagation algorithm. For a neuron [...] with activation function , the <b>delta</b> <b>rule</b> for 's th weight [...] is given by ...|$|R
5000|$|The <b>delta</b> <b>rule</b> is {{commonly}} stated in simplified form for a neuron with a linear activation function as ...|$|R
40|$|Abstract. During the {{machining}} process {{of a complex}} rotating proÞle, numerous nonlinear characteristics of the process come into effect. It is thus difficult to control such a system efficiently. In this {{paper we propose a}} method to control such systems. We develop a direct adaptive control scheme for such processes, with a dynamic neural network as a controller. According {{to the idea of a}} direct adaptive control, the neural network controller can be directly adjusted based on the system-output error. The training algorithm is based on a modiÞed <b>generalized</b> <b>delta</b> <b>rule</b> and the only a priori knowledge required is that of response behavior of the controlled system. The proposed scheme is used to control {{the machining}} process of turning complex rotary proÞles, and the experiments of turning a typical engine piston demonstrate good real-time control performance of this system...|$|E
40|$|ABSTRACT: In {{this paper}} a {{procedure}} for Monte Carlo simulation of univariate stationary stochastic processes {{with the aid}} of neural networks is presented. As an alternative to traditional model-based simulation procedures, this one circumvents the difficulty of specifying a priori statistical properties of the process. This is particularly advantageous when only limited data are available. Neural networks operate model-free and learn directly from the data observed. They capture the pattern of short time series during the training procedure. The trained network can then generate a set of random process realizations which reflect the properties of the training data. In the present study a time lagged feed-forward network with logistic sigmoid activation functions is applied. The training of the network is realized by back-propagation with a <b>generalized</b> <b>delta</b> <b>rule.</b> An example demonstrates the usefulness of the proposed procedure. ...|$|E
40|$|In {{this article}} it is {{presented}} a proposal of improving the data analysis process of Operational Risk (OpRisk) assessment in the financial institutions, for the Loss Distribution Approach (LDA) method, using the Artificial Intelligence (AI). In {{the first part of}} the paper a substitute tool of the traditional model-based Autoregressive Moving Average (ARMA) is described, for analyzing and representing stochastic processes. An Artificial Neural Network (ANN) is particularly suitable for this challenge, especially when dealing with limited data sets. In this case, an ANN is able to operate model-free by extracting the pattern of the training data set and by learning from the data observed during the <b>generalized</b> <b>delta</b> <b>rule</b> back-propagation training. The proposed ANN is a time lagged Feed-Forward Network (FFN) with log-sigmoid activation function. Operational Risk, Advanced Measurement Approach, Loss Distribution Approach, Artificial Neural Networks, Genetic Algorithms...|$|E
40|$|We studied four {{children}} with diagnosis of absence seizures (generalized primary epilepsy), {{and with a}} <b>generalized</b> <b>delta</b> activity on the EEG during clinical attacks provoked by hyperventilation. The lack of ictal generalized spike-and-wave discharges with a frequency of 3 Hz in our patients, makes this an atypical pattern. All children had complete control of their seizures and disappearance of the EEG changes with valproate. We concluded that <b>generalized</b> <b>delta</b> activity observed on EEG during the hyperventilation in children should not always {{be considered as a}} normal finding for age, since it could be an ictal event of an absence seizure...|$|R
30|$|On day 23 (the {{patient was}} comatose) EEG {{revealed}} <b>generalized</b> <b>delta</b> slowing and single generalized epileptic discharges being consisted with metabolic encephalopathy. Seizures ceased quickly {{and the patient}} had a normal level of consciousness again (day 25).|$|R
5000|$|... "Energy Distribution and Redistribution and Chemical Reactivity. The <b>Generalized</b> <b>Delta</b> Overlap-Density Method for Ground State and Electron Transfer Reactions; A new Quantitative Counterpart of Electron Pushing”, Zimmerman, H. E.; Alabugin, I. V. J. Am. Chem. Soc. 2001, 121, 2265-2270 ...|$|R
40|$|Traditional connectionist {{networks}} have homogeneous nodes wherein each node executes the same function. Networks where each node executes a different function {{can be used}} to achieve efficient supervised learning. A modified back-propagation algorithm for such networks, which performs gradient descent in "function space," is presented and its advantages are discussed. The benefits of the suggested paradigm include faster learning and ease of interpretation of the trained network. 1 Introduction Connectionist networks (Rosenblatt, 1962; Grossberg, 1981; Rumelhart, McClelland & the PDP Research Group, 1986) are usually thought of to be graph-like interconnections of processing elements. Each processing element is capable of a simple computation such as summation or summation coupled with thresholding. In such networks, the delta rule has been used for learning linear concepts and the <b>generalized</b> <b>delta</b> <b>rule</b> (Rumelhart, McClelland & the PDP Research Group, 1986) has been employed for lea [...] ...|$|E
40|$|<b>Generalized</b> <b>delta</b> <b>rule,</b> {{popularly known}} as {{back-propagation}} (BP) [9, 5] {{is probably one of}} the most widely used procedures for training multi-layer feed-forward networks of sigmoid units. Despite reports of success on a number of interesting problems, BP can be excruciatingly slow in converging on a set of weights that meet the desired error criterion. Several modifications for improving the learning speed have been proposed in the literature [2, 4, 8, 1, 6]. BP is known to suffer from the phenomenon of flat spots [2]. The slowness of BP is a direct consequence of these flat-spots together with the formulation of the BP Learning rule. This paper proposes a new approach to minimizing the error that is suggested by the mathematical properties of the conventional error function and that effectively handles flat-spots occurring in the output layer. The robustness of the proposed technique is demonstrated on a number of data-sets widely studied in the machine learning community. 1 Introduc [...] ...|$|E
40|$|There are two {{measures}} for the optimality of a trained feed-forward network for the given training patterns. One is the global error function {{which is the}} sum of squared differences between target outputs and actual outputs over all output units of all training patterns. The most popular training method, backpropagation based on the <b>Generalized</b> <b>Delta</b> <b>Rule,</b> is to minimize the value of this function. In this method, the smaller the global error is, the better the network is supposed to be. The other measure is the correctness ratio which shows, when the network's outputs are converted into binary outputs, for what percentage of training patterns the network generates the correct binary outputs. Actually, this is the measure that often really matters. This paper argues that those {{two measures}} are not parallel and presents a technique with which the back-propagation method results in a high correctness ratio. The results show that the trained networks with this technique often exhibit high [...] ...|$|E
40|$|The Dirac delta {{function}} {{is widely used}} {{in many areas of}} physics and mathematics. Here we consider the generalization of a Dirac {{delta function}} to allow the use of complex arguments. We show that the properties of a <b>generalized</b> <b>delta</b> function are very different from those of a Dirac delta function and that they behave more like a pole in the complex plane. We use the <b>generalized</b> <b>delta</b> function to derive the Glauber-Sudarshan P-function for a Schrödinger cat state in a surprisingly simple form. Aside from their potential applications in classical electromagnetism and quantum optics, these results provide insight into the ability of the diagonal P-function to describe density operators with off-diagonal elements. Comment: 11 pages, 5 figure...|$|R
5000|$|Artificial Neural {{networks}} library implements {{some common}} network architectures (multi-layer feed forward and distance networks) and learning algorithms (back propagation, <b>delta</b> <b>rule,</b> simple perceptron, evolutionary learning).|$|R
40|$|Current {{learning}} {{theories are}} {{based on the idea that}} learning is driven by the difference between expectations and experience (the <b>delta</b> <b>rule).</b> In extinction, one learns that certain expectations no longer apply. Here, we test the potential validity of the <b>delta</b> <b>rule</b> by manipulating memory retrieval (and thus expectations) during extinction learning. Adrenergic signaling is critical for the time-limited retrieval (but not acquisition or consolidation) of contextual fear. Using genetic and pharmacologic approaches to manipulate adrenergic signaling, we find that long-term extinction requires memory retrieval but not conditioned responding. Identical manipulations of the adrenergic system that do not affect memory retrieval do not alter extinction. The results provide substantial support for the <b>delta</b> <b>rule</b> of learning theory. In addition, the timing over which extinction is sensitive to adrenergic manipulation suggests a model whereby memory retrieval occurs during, and several hours after, extinction learning to consolidate long-term extinction memory...|$|R
40|$|Abstract- Artificial neural {{networks}} (ANNs) {{are used for}} content based image retrieval (CBIR). Training of a neural network requires that the user specifies the network structure and sets the learning parameters. In this study, the optimum design of ANN's for retrieval of images is investigated. We use test image datasets {{in a series of}} experiments that evaluate the effects on network performance (measured in terms of the Mean Square Error and number of iteration (time required for training)) of different choices of network size and structure, network parameters, training samples size. We use a test image database of 1000 images including 10 classes. Each class has 100 images. The backpropagation algorithm, also called the <b>generalized</b> <b>delta</b> <b>rule,</b> was used for neural network training. An activation function was hyperbolic tangent. Experiments show that with used category of images optimal number of neurons in hidden layer is half of number of images in used training set. We intend to design small CBIR system for educational purposes and potentially for mobility environment...|$|E
40|$|Reclamation {{projects}} give {{an impression}} that a severe environmental damage is certain. However, the extent of damage cannot be measured by a rule of thumb. In this paper, {{an analysis of the}} environmental conditions focusing on the population of certain bird species was performed. It is only natural to surmise that bird population is decreasing due to man-made structures, much more destroying a natural habitat, though, {{at this stage of the}} study the degree as to how much the population has changed remains unknown. A model of the biological brain, known as artificial neural networks (ANN), which is the main essence of this research, might open doors to a more rigorous investigation of the environmental changes that are occurring due to tidal flat reclamations. This network is able to train itself from input parameter values and thus can predict values of desired output variables. A specific type of ANN algorithm used for calculation is the backpropagation algorithm, which is also known as the <b>generalized</b> <b>delta</b> <b>rule.</b> Input parameters like air temperature, daylight hours, and tidal flat organisms that birds feed were chosen. Sensitivity analysis was performed to identify the birds 2 ̆ 7 behavioral patterns and their reaction to the state variables...|$|E
40|$|Abstract: 2 ̆ 2 Currently {{the most}} popular {{learning}} algorithm for connectionist networks is the <b>generalized</b> <b>delta</b> <b>rule</b> (GDR) developed by Rumelhart, Hinton 2 ̆ 6 Williams (1986). The GDR learns by performing gradient descent on the error surface in weight space whose height at any point is equal to {{a measure of the}} network 2 ̆ 7 s error. The GDR is plagued by two major problems. First, the progress towards a solution using the GDR is often quite slow. Second, networks employing the GDR frequently become trapped in local minima on the error surface and hence do not reach good solutions. To solve the problems of the GDR, a new connectionist architecture and learning algorithm is developed in this thesis. The new architectural components are called meta-connections, which are connections from a unit to the connection between two other units. Meta-connections are able to temporarily alter the weight of the connection to which they are connected. In doing this, meta-connections are able to tailor the weights of individual connections for particular input/output patterns. The new learning algorithm, called the meta-generalized delta rule (MGDR), is an extension of the GDR to provide for learning the proper weights for meta-connections. Empirical tests show that the tailoring of weights using meta-connections allows the MGDR to develop solutions more quickly and reliably than the GDR {{in a wide range of}} problems. 2 ̆...|$|E
40|$|This {{article was}} {{published}} in the journal, Monatshefte fur Mathematik [© Springer-Verlag]. The final publication is available at Springer via [URL] study the topological duals of the Colombeau algebras Gc(Ω), G(Ω) and GS(Rn), discussing some continuous embeddings and the properties of <b>generalized</b> <b>delta</b> functionals...|$|R
5000|$|... #Subtitle level 3: Definitions of <b>generalized</b> Kronecker <b>delta</b> ...|$|R
40|$|We {{study the}} topological duals of the Colombeau algebrasGc(Ω),G(Ω) and G S (R n), {{discussing}} some continuous embeddings and {{the properties of}} <b>generalized</b> <b>delta</b> functionals. Key words: algebras of generalized functions, duality theory, generalization of the Dirac measure AMS 2000 subject classification: 46 F 30, 13 J 99...|$|R
