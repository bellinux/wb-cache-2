51|172|Public
5000|$|IO {{channels}} {{can take}} advantage of <b>data</b> <b>striping</b> and software mirroring ...|$|E
5000|$|<b>Data</b> <b>striping</b> {{can also}} be {{achieved}} with Linux's Logical Volume Management (LVM). The LVM system allows for the adjustment of coarseness of the striping pattern. LVM tools will allow implementation of <b>data</b> <b>striping</b> in conjunction with mirroring; however, LVM1 will not allow adding additional disks to a striped Logical Volume (LV). This can be achieved with LVM2 using LVM2 format metadata.|$|E
50|$|In {{computer}} data storage, <b>data</b> <b>striping</b> is {{the technique}} of segmenting logically sequential data, such as a file, so that consecutive segments are stored on different physical storage devices.|$|E
40|$|Water {{information}} {{cannot be}} accurately extracted using TM images because true information {{is lost in}} some images because of blocking clouds and missing <b>data</b> <b>stripes,</b> thereby water information cannot be accurately extracted. Water is continuously distributed in natural conditions; thus, this paper proposed a new method of water body extraction based on probability statistics to improve the accuracy of water information extraction of TM images with missing information. Different disturbing information of clouds and missing <b>data</b> <b>stripes</b> are simulated. Water information is extracted using global histogram matching, local histogram matching, and the probability-based statistical method in the simulated images. Experiments show that smaller Areal Error and higher Boundary Recall can be obtained using this method compared with the conventional methods...|$|R
40|$|Abstract: ~n {{analysis}} of mirrored discs and of RAID 5 shows that mirrors have considerably better throughput, measured as requests/second, on random requests of arbitrary size (up to 1 MB). Mirrors have comparable or better response time for requests of reasonable size (less than looKB). But mirrors have a 100 % storage penalty: storing the <b>data</b> twice. Parity <b>striping</b> is a <b>data</b> layout that <b>stripes</b> the parity across the discs, {{but does not}} <b>stripe</b> the <b>data.</b> Parity <b>striping</b> has throughput {{almost as good as}} mirrors, and has costlGB comparable to RAIDS designs [...] combing the advantages of both for high-traffic disc resident <b>data.</b> Parity <b>striping</b> has additional fault containment and software benefits as well. Parity striping sacrifices the high data transfer rates of RAID designs for high throughput. It is argued that response time and throughput are preferable performanc...|$|R
50|$|The MM {{feature is}} encoded {{in the middle}} layer of an ISO/IEC 7810 card as a bar code formed by two {{materials}} with different electrical properties. A capacitive sensor head near the magstripe reader observes the changing capacitance as the card is moved past the sensor and decodes the represented number. This sensor works {{in a similar fashion}} to the magnetic read head found in a magstripe card reader, except that it senses not a change in magnetic flux, but a change in the dielectric constant of the card's material. It reads a second <b>data</b> <b>stripe</b> that, unlike the magstripe, cannot easily be rewritten with off-the-shelf equipment.|$|R
50|$|Multilink {{striping}} {{is a type}} of <b>data</b> <b>striping</b> used in telecommunications {{to achieve}} higher throughput or increase the resilience of a network connection by data aggregation over multiple network links simultaneously.|$|E
50|$|Storage appliances: provide {{massive amounts}} of storage and {{additional}} higher level functionality (ex: Disk mirroring and <b>Data</b> <b>striping)</b> for multiple attached systems using the transparent local storage area networks computer paradigm.|$|E
50|$|Data {{transmitted}} on multiple-lane links is interleaved, {{meaning that}} each successive byte is sent down successive lanes. The PCIe specification {{refers to this}} interleaving as <b>data</b> <b>striping.</b> While requiring significant hardware complexity to synchronize (or deskew) the incoming striped <b>data,</b> <b>striping</b> can significantly reduce the latency of the nth byte on a link. While the lanes are not tightly synchronized, {{there is a limit}} to the lane to lane skew of 20/8/6 ns for 2.5/5/8 GT/s so the hardware buffers can re-align the striped data. Due to padding requirements, striping may not necessarily reduce the latency of small data packets on a link.|$|E
5000|$|RAID 0 (also {{known as}} a stripe set or striped volume) splits ("stripes") data evenly across two or more disks, without parity information, redundancy, or fault tolerance. Since RAID 0 {{provides}} no fault tolerance or redundancy, the failure of one drive will cause the entire array to fail; {{as a result of}} having <b>data</b> <b>striped</b> across all disks, the failure will result in total data loss. This configuration is typically implemented having speed as the intended goal. [...] RAID 0 is normally used to increase performance, although it can also be used as a way to create a large logical volume out of two or more physical disks.|$|R
40|$|An {{increasingly}} popular method of improving I/O performance is by distributing data across multiple disks in parallel. This organization of <b>data</b> is called <b>striping,</b> {{and a group}} of disks organized in this manner is called a disk array. This work investigates the performance of <b>striping</b> <b>data</b> in an array of Serial Storage Architecture (SSA) disks connected in a loop topology...|$|R
50|$|There is {{also the}} ever-growing list of <b>data</b> for the <b>Stripe</b> 82 region of the SDSS.|$|R
5000|$|Nested RAID {{levels are}} usually {{numbered}} using {{a series of}} numbers, where {{the most commonly used}} levels use two numbers. The first number in the numeric designation denotes the lowest RAID level in the [...] "stack", while the rightmost one denotes the highest layered RAID level; for example, RAID 50 layers the <b>data</b> <b>striping</b> of RAID 0 on top of the distributed parity of RAID 5. Nested RAID levels include RAID 01, RAID 10, RAID 100, RAID 50 and RAID 60, which all combine <b>data</b> <b>striping</b> with other RAID techniques; {{as a result of the}} layering scheme, RAID 01 and RAID 10 represent significantly different nested RAID levels.|$|E
50|$|XOR calculations, for example, are {{necessary}} for calculating parity data, and for maintaining data integrity when writing to a disk array that uses a parity drive or <b>data</b> <b>striping.</b> An RPU may perform these calculations more efficiently than the computer's central processing unit (CPU).|$|E
5000|$|Awareness of <b>data</b> <b>striping</b> in RAID 5 and RAID 6 layouts adding {{awareness}} of the stripe layout to the write-back policy, so decisions on caching will be giving preference to already [...] "dirty" [...] stripes, and actual background flushes will be writing out complete stripes first ...|$|E
40|$|In this paper, {{we propose}} an {{architectural}} approach, Supplementary Partial Parity (SPP), to addressing the availability issue of parity encoded RAID systems. SPP exploits free storage space and idle time to generate and update {{a set of}} partial parity units that cover a subset of disks (or <b>data</b> <b>stripe</b> units) during failure-free and idle/lightly-loaded periods, thus supplementing the existing full parity units for improved availability. By applying the exclusive OR operations appropriately among partial parity, full parity and data units, SPP can reconstruct the data on the failed disks with {{a fraction of the}} original overhead that is proportional to the partial parity coverage, thus significantly reducing the overhead of data regeneration...|$|R
50|$|In general {{there are}} two classes of {{contactless}} bank cards: magnetic <b>stripe</b> <b>data</b> (MSD) and contactless EMV.|$|R
40|$|RAID {{architectures}} {{have been}} used {{for more than two decades}} to recover data upon disk failures. Disk failure is just one of the many causes of damaged data. Data can be damaged by virus attacks, user errors, defective software/firmware, hardware faults, and site failures. The risk of these types of data damage is far greater than disk failure with today’s mature disk technology and networked information services. It has therefore become increasingly important for today’s disk array to be able to recover data to any point in time when such a failure occurs. This paper presents a new disk array architecture that provides Timely Recovery to Any Point-in-time, referred to as TRAP-Array. TRAP-Array stores not only the <b>data</b> <b>stripe</b> upon a write to the array, but also the time-stamped Exclusive-ORs of successiv...|$|R
50|$|<b>Data</b> <b>striping</b> {{is used in}} some databases, such as Sybase, and {{in certain}} RAID devices under {{software}} or hardware control, such as IBM's 9394 RAMAC Array subsystem. File systems of clusters also use striping. Oracle Automatic Storage Management allows ASM files to be either coarse or fine striped.|$|E
5000|$|A parity {{drive is}} a hard drive used in a RAID array to provide fault tolerance. For example, RAID 3 uses a parity drive to create {{a system that is}} both fault {{tolerant}} and, because of <b>data</b> <b>striping,</b> fast. [...] Basically, a single data bit is added {{to the end of a}} data block to ensure the number of bits in the message is either odd or even.|$|E
50|$|Some RAID 1 {{implementations}} treat differently arrays {{with more}} than two disks, creating a non-standard RAID level known as RAID 1E. In this layout, <b>data</b> <b>striping</b> is combined with mirroring, by mirroring each written stripe {{to one of the}} remaining disks in the array. Usable capacity of a RAID 1E array equals to 50% of the total capacity of all drives forming up the array; if drives of different sizes are used, only the portions equaling to the size of smallest member are utilized on each drive.|$|E
40|$|Striped {{dolphins}} {{are distributed}} world-wide in tropical and warm-temperate pelagic waters. On recent shipboard surveys extending about 300 nmi offshore of California, they were sighted within about 100 - 300 nmi {{from the coast}} (Figure 1). No sightings have been reported for Oregon and Washington waters, but striped dolphins have stranded in both states (Oregon Department of Fish and Wildlife, unpublished data; Washington Department of Fish and Wildlife, unpublished <b>data).</b> <b>Striped</b> dolphins are also commonly found in the central North Pacific, but sampling between this region and California has been insufficient {{to determine whether the}} distribution is continuous. Based on sighting records off California and Mexico, striped dolphins appear to have a continuous distribution in offshore waters of these two regions (Perrin et al. 1985; Mangels and Gerrodette 1994). N...|$|R
5000|$|RAID 3 {{consists}} of byte-level striping with dedicated parity. All disk spindle rotation is synchronized and <b>data</b> is <b>striped</b> such that each sequential byte is {{on a different}} drive. Parity is calculated across corresponding bytes and stored on a dedicated parity drive. Although implementations exist, RAID 3 is not commonly used in practice.|$|R
40|$|Part 5 : I/O, File Systems, and Data ManagementInternational audienceThis paper {{presents}} a novel mechanism to dynamically re-size and re-distribute stripes on the storage servers in distributed file systems. To put this mechanism to work, {{the information about}} logical I/O access on the client side is piggybacked to physical I/O access on the storage server side, for building {{the relationship between the}} logical I/O access and physical I/O access. Moreover, this newly presented mechanism supports varying size of stripes on the storage servers to obtain finer concurrency granularity on accessing to <b>data</b> <b>stripes.</b> As a result, the mapping relationship can be utilized to direct stripe re-sizing and re-distributing on the storage servers dynamically for better system performance. Experimental results show that this stripe management mechanism can reduce I/O response time and boost I/O data throughput significantly for applications with complicated access patterns...|$|R
50|$|Micron and Intel {{initially}} made faster SSDs {{by implementing}} <b>data</b> <b>striping</b> (similar to RAID 0) and interleaving in their architecture. This enabled {{the creation of}} ultra-fast SSDs with 250 MB/s effective read/write speeds with the SATA 3 Gbit/s interface in 2009. Two years later, SandForce continued to leverage this parallel flash connectivity, releasing consumer-grade SATA 6 Gbit/s SSD controllers which supported 500 MB/s read/write speeds. SandForce controllers compress the data prior to sending it to the flash memory. This process may result in less writing and higher logical throughput, depending on the compressibility of the data.|$|E
5000|$|In {{addition}} to the regular My Book drives, Western Digital has also released special high-capacity [...] "Edition II" [...] versions of the Premium, Pro, and World Edition My Books. In {{addition to}} the features present in the respective My Book edition, these drives feature two 500 GB RAID configured hard disks which can be selected by the end user as RAID 0 (<b>Data</b> <b>striping),</b> or RAID 1 (Data mirroring), depending on personal preference. If selected as RAID 0, the end user has 1 TB of available storage. Either way, {{if one of the}} internal drives of the Edition II My Books fails, it can be easily removed and replaced by the user without voiding the warranty. Western Digital uses this feature to their advantage, claiming that their drives needn't be returned for costly service {{in the case of a}} drive failure.|$|E
40|$|We compare {{performance}} of a multimedia storage server based on a random data allocation layout and block replication with traditional <b>data</b> <b>striping</b> techniques. <b>Data</b> <b>striping</b> techniques in multimedia servers are often designed for restricted workloads, e. g. sequential access patterns with CBR (constant bit rate) requirements. On the other hand, a system based on random data allocation can support virtually any type of multimedia application, including VBR (variable bit rate) video or audio, and interactive applications with unpredictable access patterns, such as 3 D interactive virtual worlds, interactive scientific visualizations, etc. Surprisingly, our results show that system performance with random data allocation is competitive and sometimes even outperforms traditional <b>data</b> <b>striping</b> techniques, for the workloads for which <b>data</b> <b>striping</b> is designed to work best; i. e. streams with sequential access patterns and CBR requirements. Due to its superiority in supporting general workloads [...] ...|$|E
40|$|Professionals {{in various}} #elds such as medical imaging# biology# and civil {{engineering}} require rapid access to {{huge amounts of}} image data. Mul# timedia interfaces further increase the demand for high#performance image and media servers. We consider a parallel image server archi# tecture that relies on arrays of intelligent disk nodes# each disk node consisting of one processor and one or more disks. The multiprocessor multidisk design is applied in an original waytoprovide both high# performance real#time access and unlimited# inexpensive mass storage capacities {{through the use of}} removable CD#ROM medium. Reading uncompressed or lossless compressed image <b>data</b> <b>striped</b> across multi# ple CD#ROM disks yields sustained performance of up to 2. 5 Mbytes#s. Unlike RAID storage devices# the proposed CD#ROM architecture o#ers information distribution control and local processing capabilities. Two application #elds that bene#t from such features are presented. Keywords# CD#ROM# disk [...] ...|$|R
40|$|Abstract. The DWS (<b>Data</b> Warehouse <b>Striping)</b> {{technique}} is a round-robin data partitioning approach especially designed for distributed data warehousing environments. In DWS the fact tables are distributed by an arbitrary number of low-cost {{computers and the}} queries are executed in parallel by all the computers, guarantying a nearly optimal speed up and scale up. However, {{the use of a}} large number of inexpensive nodes increases the risk of having node failures that impair the computation of queries. This paper proposes an approach that provides <b>Data</b> Warehouse <b>Striping</b> with the capability of answering to queries even in the presence of node failures. This approach is based on the selective replication of data over the cluster nodes, which guarantees full availability when one or more nodes fail. The proposal was evaluated using the newly TPC-DS benchmark and the results show that the approach is quite effective...|$|R
40|$|Informed {{prefetching}} and caching {{based on}} application disclosure of future I/O accesses (hints) can dramatically reduce the execution time of I/O-intensive applications. A recent study showed that, {{in the context}} of a single hinting application, prefetching and caching algorithms should adapt to the dynamic load on the disks to obtain the best performance. In this paper, we show how to incorporate adaptivity to disk load into the TIP 2 system, which uses cost-benefit analysis to allocate global resources among multiple processes. We compare the resulting system, which we call TIPTOE (TIP with Temporal Overload Estimators) to Cao et al's LRU-SP allocation scheme, also modified to include adaptive prefetching. Using disk-accurate trace-driven simulation we show that, averaged over eleven experiments involving pairs of hinting applications, and with <b>data</b> <b>striped</b> over one to ten disks, TIPTOE delivers 7 % lower execution time than LRU-SP. Where the computation and I/O demands of each experi [...] ...|$|R
40|$|The {{distribution}} of data across multiple, parallel disks, known as <b>data</b> <b>striping,</b> {{is a popular}} method of data storage as it allows for faster access. In this paper, we develop an analytical model that estimates the mean response time, throughput, and mean queue length of data requests when <b>data</b> <b>striping</b> storage techniques are used. Our model is shown via simulations to provide a good approximation of the actual access time {{under a variety of}} system and workload parameters designed to emulate real-world conditions. Since analytical results can be readily computed (as opposed to empirical and simulation models), our model should prove useful to file and storage managers in their decisions with regard to the optimal placement of data on storage devices...|$|E
40|$|This paper {{studies the}} impacts of work-file disk {{allocation}} and <b>data</b> <b>striping</b> {{on the performance of}} concurrent mergesorts in a multiprocessor database system. We exam-ine through detailed simulations an approach where workfile disks are logically partitioned into equal-sized groups and an arriving sort job selects one group to do the mergesort. The results show that (1) without data strip-ing, the best performance is achieved by using the entire workfile disks as a single partition if there are abundant workfile disks (or sys-tem workload is light); (2) however, if there are limited workfile disks (or system workload is heavy), the workfile disks should be parti-tioned into multiple groups and the optimal partition size is workload dependent; (3) <b>data</b> <b>striping</b> is beneficial only if the striping unit size is properly chosen. ...|$|E
40|$|We use the {{technique}} of storing the data of a single object across several storage servers, called <b>data</b> <b>striping,</b> to achieve high transfer data rates in a local area network. Using parallel paths to data allows a client to transfer data to and from storage {{at a higher rate}} than that supported by a single storage server. We have implemented a network data service, called Swift, that uses <b>data</b> <b>striping.</b> Swift exhibits the expected scaling property in the number of storage servers connected to a network and in the number of interconnection networks present in the system. We have also simulated a version of Swift to explore the limits of possible future configurations. We observe that the system can evolve to support very high speed interconnection networks as well as large numbers of storage servers. Since Swift is a distributed system made up of independently replaceable components, any component that limits the performance can either be replaced by a faster component when it [...] ...|$|E
40|$|Storage {{system failure}} {{is a serious}} concern as we {{approach}} Petascale computing. Even at today’s sub-Petascale levels, I/O failure {{is the leading cause}} of downtimes and job failures. We contribute a novel, on-the-fly recovery framework for job input data into supercomputer parallel file systems. The framework exploits key traits of the HPC I/O workload to reconstruct lost input data during job execution from remote, immutable copies. Each reconstructed <b>data</b> <b>stripe</b> is made immediately accessible in the client request order due to the delayed metadata update and finegranular locking while unrelated access to the same file remains unaffected. We have implemented the recovery component within the Lustre parallel file system, thus building a novel application-transparent online recovery solution. Our solution is integrated into Lustre’s two-level locking scheme using a two-phase blocking protocol. Combining parametric and simulation studies, our experiments demonstrate a significant improvement in HPC center serviceability and user job turnaround time. 1...|$|R
40|$|Redundant disk arrays are an {{increasingly}} popular {{way to improve}} I/O system performance. Past research has studied how to <b>stripe</b> <b>data</b> in non-redundant (RAID Level 0) disk arrays, but none has yet been done on how to <b>stripe</b> <b>data</b> in redundant disk arrays such as RAID Level 5, or on how the choice of striping unit varies {{with the number of}} disks. Using synthetic workloads, we derive simple design rules for <b>striping</b> <b>data</b> in RAID Level 5 disk arrays given varying amounts of workload information. We then validate the synthetically derived design rules using real workload traces to show that the design rules apply well to real systems. We find no difference in the optimal striping units for RAID Level 0 and 5 for read-intensive workloads. For write-intensive workloads, in contrast, the overhead of maintaining parity causes full-stripe writes (writes that span the entire error-correction group) to be more efficient than read-modify writes or reconstruct writes. This additional factor causes [...] ...|$|R
40|$|Redundant disk arrays are an {{increasingly}} popular {{way to improve}} I/O system performance. Past research has studied how to <b>stripe</b> <b>data</b> in non-redundant (RAID Level 0) disk arrays, but none has yet been done on how to <b>stripe</b> <b>data</b> in redundant disk arrays such as RAID Level 5. In this paper we discuss the tradeoffs involved in <b>striping</b> <b>data</b> in RAID Level 5 disk arrays, particularly for write-intensive workloads. We find that the overhead of maintaining parity causes fullstripe writes (writes that span the entire error-correction group) to be more efficient than readmodify writes or reconstruct writes. This additional factor causes the optimal striping unit for RAID Level 5 to be smaller for write-intensive workloads than for read-intensive workloads. We synthesize simple design rules for how to choose an optimal striping unit for RAID Level 5 when workload concurrency is known and also when no workload information is known. Last, we investigate how the optimal striping unit varies with [...] ...|$|R
