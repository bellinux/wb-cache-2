4|10000|Public
40|$|Future space {{transportation}} programme reasonable alternatives are considered with {{regard for the}} total risk forecast. The analysis run tool utilizing the methodology of arriving at a decision under uncertainty conditions and a given goals hierarchy, is bound to provide for <b>input</b> <b>data</b> <b>validation,</b> procedures verification and interactive analysis and selection mode. The statement of the problem actuality is demonstrated by the example of forming future Space Transportation Systems (STS) development programme options...|$|E
40|$|Abstract—Atlas-based {{approaches}} {{have demonstrated the}} ability to automatically identify detailed brain structures from 3 -D magnetic resonance (MR) brain images. Unfortunately, the accuracy {{of this type of}} method often degrades when processing data acquired on a different scanner platform or pulse sequence than the data used for the atlas training. In this paper, we improve the performance of an atlas-based whole brain segmentation method by introducing an intensity renormalization procedure that automatically adjusts the prior atlas intensity model to new <b>input</b> <b>data.</b> <b>Validation</b> using manually labeled test datasets has shown that the new procedure improves the segmentation accuracy (as measured by the Dice coefficient) by 10 % or more for several structures including hippocampus, amygdala, caudate, and pallidum. The results verify that this new procedure reduces the sensitivity of the whole brain segmentation method to changes in scanner platforms and improves its accuracy and robustness, which can thus facilitate multicenter or multisite neuroanatomical imaging studies. Index Terms—Brain atlas, brain imaging, computational neuroanatomy, magnetic resonance imaging (MRI) segmentation. I...|$|E
40|$|Balancing {{the supply}} of on-farm grown forages with the {{production}} targets of the dairy herd is a crucial aspect of the management of a dairy farm. Models which provides a rapid insight {{of the impact of}} the ration, feed quality and feeding management on feed intake and performance of dairy cows are indispensable to optimize feeding strategies, allocation of feeds and purchased concentrates, in order to find the best compromise between milk performance, nutrient use efficiency, manure excretion, gaseous emissions and profitability. This thesis describes the development of the Wageningen UR Dairy Cow Model (Wageningen DCM), a model for the prediction of feed intake and performance of dairy cows. The Wageningen DCM is constructed from two modules: a feed intake model and an energy partitioning model which describes the partitioning of the ingested net energy to milk energy output and body reserves. For the development of the feed intake model a calibration dataset was compiled with 38515 weekly records of ration feed composition, diet composition, individual feed intakes, milk yield and composition, parity, days in lactation and days pregnant from 1507 cows. The feed intake model predicts dry matter intake (DMI) from feed and animal characteristics. Data of standard feed analysis were used to estimate the satiety value (SV) of numerous feeds. The SV is the measure of the extent to which a feed limits intake. The cows’ ability to process the intake-limiting satiety value-units is expressed as the feed intake capacity (FIC). The FIC is estimated from parity, days in milk and days of pregnancy which are indicators of the size and physiological state of the cow. An  evaluation of the feed intake model was performed using an independent dataset containing 8974 weekly means of DMI from 348 cows. On the basis of mean square prediction error (MSPE) and relative prediction error (RPE) as criteria, it was concluded that feed intake model was robust and can be applied to various diets and feeding management situations in lactating HF cows. A second model was developed to predict the partitioning of ingested net energy (NEL) to milk energy and body reserves. This energy partitioning model describes the baselines of daily NEL intake and milk energy output (MEO) during successive lactation cycles of a ‘reference’ cow. The MEO and change in body energy of a cow is estimated from deviation of NEL intake from the baseline. A NEL intake above the baselines results in a higher predicted MEO and reduced mobilization of body energy reserves. Whereas, a NEL intake below the baseline results in a lower predicted MEO and increased mobilization. The proportion of ingested NEL partitioned to MEO depends parity number, days in lactation and pregnant, reflecting the changes in priority in energy partitioning during successive lactation cycles of a dairy cow The feed intake model and energy partitioning model are integrated in the Wageningen DCM. Model simulations showed that the Wageningen DCM is able to simulate the effects of diet composition, nutritional strategies and effects of cow characteristics (parity, days in milk and pregnancy) on dry matter and nutrient intake, and the partitioning of ingested NEL into MEO and body energy. The Wageningen DCM requires easily available <b>input</b> <b>data.</b> <b>Validation</b> of the Wageningen DCM with external data indicated a good accuracy of the prediction of intake and milk energy output with relatively low prediction errors ≤ 0. 1. The Wageningen DCM enables users to analyse and compare different feeding strategies, identify limitations of feeding strategies, formulate diets, calculate feed budgets and to develop economic and environmental sustainable feeding strategies...|$|E
50|$|Many {{computer}} systems implement data entry forms, but data collection systems {{tend to be}} more complex, with possibly many related forms containing detailed user <b>input</b> fields, <b>data</b> <b>validations,</b> and navigation links among the forms.|$|R
40|$|We {{present the}} HU-MOD- 2 {{model for the}} {{assessment}} of management impact on organic matter levels in arable soils. The model aims at optimal applicability as a management support tool in framing practice and therefore requires only easily available <b>input</b> <b>data.</b> In <b>validation,</b> the tool proved to be capable of giving a rough estimate on soil organic matter changes in arable soils. Taking into account the low demand for <b>input</b> <b>data,</b> the modeling error seems tolerable for a practice applicable decision support tool...|$|R
40|$|This paper aims {{to provide}} an {{introduction}} to {{the current state of the}} art of EUROMOD, the European Union tax-benefit microsimulation model. It explains the original motivations for building a multi-country EU-wide model and summarises its current organisation. It provides an overview of EUROMOD components, covering its policy scope, the <b>input</b> <b>data,</b> the <b>validation</b> process and some technical aspects such as the tax-benefit programming language and the user interface. The paper also reviews some recent applications of EUROMOD and, finally, considers future developments...|$|R
40|$|The {{objective}} {{of this research is}} to examine the possibilities of modelling air pollution concentrations in cities outside Europe at a high spatial resolution (five to ten meters). This is ideally done with datasets that are available globally, which would ultimately allow comparison of different cities. This is done by using a PM 2. 5 land use regression equation which was developed in the ESCAPE (European Study of Cohorts for Air Pollution Effects) project for the city of London. Also personal exposure to air pollution is examined in this study. The main question of this research is: ‘To what extent is it possible to model air pollution concentrations in cities outside Europe using the London ESCAPE LUR model, how valid is the model and to what extent can the personal exposure of the population to air pollution be modelled?’ The main question is divided in three sub-questions which deal with the input data of the models, the validation and sensitivity analysis of the models and the personal exposure of the population. This paragraph deals with the methodology of this research. Land use regression models were used to model the relationship between the response variable air pollution and two or more explanatory variables. These explanatory variables in the ESCAPE project were for instance land use, traffic density and topography. The regression equation used in this study consists of two variables: ‘INTMAJORINVDIST’ (‘i’) and ‘ROADLENGTH_ 500 ’ (‘l’). The former is the product of the number of cars on the nearest major road and the inverse of distance to this nearest major road. The latter is the total amount of roads within a buffer of 500 meters. The road network data used for this regression equation is OpenStreetMap (OSM, 2015). The number of cars on the roads are estimated by assuming that all registered cars within a city are driving while they are counted. This assumption was needed because traffic intensity data was not available. Two parts of the cities Bangkok and Mexico City were modelled in this research project. Validation and sensitivity analysis were carried out to examine model errors. To examine the exposure of the population, the population numbers of the neighbourhoods were distributed across the modelled areas with a global population density layer grid used as a weighting layer. Then then the personal exposure could be measured. This paragraph discusses the results of the first question, which is formulated as follows: ‘which input data can be used to compare different cities outside Europe using the model?’ The results have shown {{that it is possible to}} model PM 2. 5 values with the London regression equation. However, a traffic intensity dataset was not available. This is why assumptions needed to be made about the number of cars on a road. The PM 2. 5 values of the Bangkok area varied from 7. 2 to 37066 microgram per cubic meter. For Benito Juárez (Mexico City) this ranged from 13. 8 to 183727. The high values for both cities were all located on and near the major roads. This is why another output was generated without these major road locations and a buffer of 10 meter around these major roads. With this output the values range from 7. 2 to 38. 7 for Bangkok and from 13. 8 to 46. 3 for Benito Juárez. The results of the second sub-question are discussed in this paragraph. The question was formulated as follows: ‘to what extent do model errors occur when the model is applied in a study area outside Europe and what causes these errors?’ The <b>input</b> <b>data</b> <b>validation</b> was done by comparing two model outputs with different input data. One model output modelled the air pollution with data used in the ESCAPE project and one model output modelled air pollution with data used in this research project. A direct comparison between the input data was not possible, because the data used in the ESCAPE project was not available as open data. By comparing the two model outputs the effects of using different input data can be analyzed. The <b>input</b> <b>data</b> <b>validation</b> showed a weak positive correlation between the model outputs for Rotterdam of this study and the model output of the ESCAPE project. There was an underestimation of the PM 2. 5 values, but the model errors were not large. This was concluded based on the small difference between the Root Mean Squared Error and the Mean Absolute Error. The validation of the models for Bangkok and Mexico City showed that the average 4 model outputs were higher than the remote sensing data. This validation also showed that the London regression equation predicted the average PM 2. 5 values better for Bangkok than for Mexico City. The differences in model output between Bangkok and Mexico City can probably be explained by the distribution of the major roads. The major roads of Benito Juárez are more evenly distributed than the roads in Bangkok. The consequence is that for each grid cell in Benito Juárez a major road is, on average, closer than for each grid cell in Bangkok. This could lead to higher PM 2. 5 values predicted by the model. The sensitivity analysis showed that the ‘l’ variable had more influence on the model output than the ‘i’ variable. This paragraph discusses the results of the third sub-question, which was formulated as follows: ‘to what extent is it possible to model the personal exposure of the population to air pollution?’ The exposure results showed that the population in Mexico City was exposed to higher PM 2. 5 values than the population of Bangkok. The distribution of the population raised the average personal PM 2. 5 for both populations, because the population was concentrated at locations with higher PM 2. 5 values, like roads. A comparison of multiple locations at 50 meters from a major road and locations at 200 meters from a major road showed that the differences between these locations vary from 1. 6 to 4. 4 microgram per cubic metre. This paragraph deals with the conclusions of this research. With the available global datasets it is possible to model air pollution concentrations to a certain extent. The road network dataset can be used without much data pre-processing. The traffic intensity dataset is the most problematic one and should be obtained locally or be created using assumptions. The model errors are partly explained by the input data. The validation of the input data showed an underestimation of the PM 2. 5 values in Rotterdam. The absolute errors showed that there were not many large errors in the predictions. This implies that there is a constant underestimation of PM 2. 5 values in Rotterdam. The model errors are also caused by the model itself. The differences between Bangkok and Mexico City showed that local calibration would be a suitable solution to take city-specific characteristics into account. This will lead to smaller prediction errors. The sensitivity analysis showed that especially the ‘l’ variable has a substantial influence on the model output, which means that it is important to use an accurate and unambiguously mapped road network dataset. The results of the third sub-question have shown that the personal exposure of the population in Bangkok and Mexico City can be estimated with the regression equation. For any location in the research area the PM 2. 5 can be estimated. However, the air pollution map and the distribution of the population caused some uncertainties...|$|E
40|$|Abstract. In {{this paper}} we {{elaborate}} on the usage of multi-agent-based simula-tion (MABS) for quantitative impact assessment of transport policy and infras-tructure measures. We provide a general discussion {{on how to use}} MABS for freight transport analysis, focusing on issues related to <b>input</b> <b>data</b> management, <b>validation</b> and verification, calibration, output data analysis, and generalization of results. The discussion is built around an agent-based transport chain simulation tool called TAPAS (Transportation And Production Agent-based Simulator) and a simulation study concerning a transport chain around the Southern Baltic Sea...|$|R
40|$|The {{estimation}} of Net Primary Production {{is of great}} impoartance when trying {{to better understand the}} global carbon cycle, especially in the context of international environmental treaty monitoring [...] In this presentation several of the NPP estimation methodologies that are currently in use will be described, as will the plans for selection of the method, or combination of methods, that could be adopted for an operational product. Other issues concerning the development of an operational system, such as spatial and temporal interpolation of <b>input</b> <b>data,</b> process <b>validation,</b> and implementation, will also be addresse...|$|R
40|$|This paper {{presents}} {{a summary of}} the methods used in the first ∼ 40 years of AGR neutron dosimetry and nuclear heating calculations, and the influence of the earlier Magnox reactor dosimetry programme. While the current state-of-the-art Monte Carlo methods are extremely powerful they still require very careful consideration {{of the quality of the}} <b>input</b> <b>data,</b> nuclear <b>data</b> <b>validation</b> and variance reduction techniques; in particular, this paper examines the difficulties in assuring the adequate convergence of calculations when Monte Carlo acceleration is applied in the presence of significant streaming paths through attenuating or scattering media...|$|R
40|$|A novel {{characterization}} {{tool for}} identification of full cohesive laws is introduced. Cohesive zones, with associated traction separation laws, are {{most commonly used}} either to describe or to simulate delamination phenomena. Our tool {{is based on the}} Global Digital Image Correlation, in which the kinematic space is defined. After summarizing the principle of the method, we focus on its validation and then on its behavior with noisy <b>input</b> <b>data.</b> This <b>validation</b> is numerically made (i) by building a set of images using an advanced simulation tool, and (ii) by identifying, from theses images, interface behavior characteristics. First successful results already show the effectivity of the proposed method and its robustness...|$|R
50|$|Plessey also {{pioneered the}} {{gathering}} and consolidation of accounting information {{from around the}} world using in-house software. Each of their 140 management reporting entities used HP125s with DIVAT (<b>data</b> <b>input,</b> <b>validation</b> and transmission) software. Nearly 450 validation rules ensured accuracy within and between various reports.The data were then transmitted to Ilford where a HP3000 used Fortran software for consolidation and reporting - also on HP125s.|$|R
40|$|The paper {{deals with}} {{description}} of the validation model, °ow conditions and mainly it presents some numerical results. Reference and <b>input</b> <b>data</b> for the <b>validation</b> study are based on work of Eidsvik [3]. The mathematical model {{is based on the}} system of RANS-equations closed by two-equation k ¡ " turbulence model together with wall functions. The thermal strati¯cation is modeled using transport equation for the potential temperature. The ¯nite volume method and the explicit Runge{Kutta time integration method are utilized for the numerics...|$|R
30|$|Threat analysis: This {{phase is}} carried out in three steps: Threat identification, Identification of {{application}} vulnerability and risk assessment. After disintegration phase threat has been identified and modeled through SPNs. Threat has been categorized using STRIDE in which identified threats marked on the SPNs as pointcuts. In next step security vulnerabilities for individual applications in e-learning were identified. Some of vulnerabilities are (authentication, authorization, <b>input</b> and <b>data</b> <b>validation,</b> configuration management, session management, auditing and logging etc.) for applications like virtual learning environment, student administration, mobile learning, virtual learning, certification etc. (Hayaati and Fan 2010). In last phase, the effect of threat can be identified on e-learning system using risk assessment. Threat matrix has been generated along with threats corresponding to vulnerabilities and prioritizes them {{on the basis of}} their likelihood of occurrence.|$|R
40|$|We {{present a}} suite of programs, named CING for Common Interface for NMR Structure Generation that {{provides}} for a residue-based, integrated validation of the structural NMR ensemble {{in conjunction with the}} experimental restraints and other <b>input</b> <b>data.</b> External <b>validation</b> programs and new internal validation routines compare the NMR-derived models with empirical data, measured chemical shifts, distance- and dihedral restraints and the results are visualized in a dynamic Web 2. 0 report. A red-orange-green score is used for residues and restraints to direct the user to those critiques that warrant further investigation. Overall green scores below ~ 20  % accompanied by red scores over ~ 50  % are strongly indicative of poorly modelled structures. The publically accessible, secure iCing webserver ([URL]) allows individual users to upload the NMR data and run a CING validation analysis. 12237...|$|R
30|$|In addition, {{the applied}} methods can be transferred/adapted easily for other regions {{and are not}} {{restricted}} to river basins. There is a risk to significantly over- or underestimate LULCC-related impacts due to small scale processes which cannot be adequately accounted for yet. Therefore, the methodical approach could be significantly improved by higher spatial resolution and easier availability of <b>input</b> <b>data.</b> Further, for <b>validation</b> of results and more appropriate inclusion of land management practices {{there is a need}} for more comprehensive monitoring (Strauch et al. 2013) of water related parameters (e.g., water quality, sediment generation), management (e.g., fertilizer application), and land use.|$|R
40|$|Suggested fluid {{mechanics}} research {{to be conducted}} in the National Transonic Facility include: wind tunnel calibration; flat plate skin friction, flow visualization and measurement techniques; leading edge separation; high angle of attack separation; shock-boundary layer interaction; submarine shapes; low speed studies of cylinder normal to flow; and wall interference effects. These theoretical aerodynamic investigations will provide empirical <b>inputs</b> or <b>validation</b> <b>data</b> for computational aerodynamics, and increase the usefulness of existing wind tunnels...|$|R
40|$|The paper {{reports a}} field {{investigation}} on a {{reach of the}} lower Zambezi River about 230 – 240 km downstream of the Cahora Bassa dam in the Republic of Mozambique. In {{the framework of a}} wider research program, bathymetric measures of the riverbed were performed on a 10 km stretch of the river using an echo sounder, a GPS receiver, and an integrated navigation-acquisition system. Field observations and measures revealed a general agreement with macro-features of river morphology reported in early literature, dealing with the morphological response of the river to the construction of large dams, {{in the second half of}} the 20 th century. Results hereby reported are some of the few examples of direct field measures in the lower Zambezi reported in literature, and could be used by researchers and practitioners either as a knowledge base for further surveys or as <b>input</b> <b>data</b> for <b>validation</b> and calibration of mathematical models and remote sensing observations...|$|R
40|$|In recent years, {{computer}} simulation {{has become a}} mainstream decision support tool in an industry. In order to maximise the benefits of using simulation within businesses, simulation models should be designed, developed and deployed in a shorter time span. A number of factors, such as excessive model details, inefficient data collection, lengthy model documentation and poorly planned experiments, increase the overall lead-time of simulation projects. Among these factors, <b>input</b> <b>data</b> modeling and model documentation are seen as major obstacles. <b>Input</b> <b>data</b> identification, collection, <b>validation</b> and analysis typically take {{more than one-third of}} project time. This paper presents an IDEF (Integrated computer-aided manufacturing DEFinition) based approach to accelerate identification and collection of <b>input</b> <b>data.</b> A functional module library and a reference data model, both developed using the IDEF family of constructs, are the core elements of the methodology. In addition, this paper also intends to give a methodological approach that helps and motivates the project team to document simulation projects...|$|R
40|$|AbstractTallinn (Estonia) {{water network}} has been {{recently}} updated (calibrated pipe roughness values, demand/leakage calibration) with commercially available tools. In current study two different approaches (scenarios) {{are used for}} model recalibration, using: (a) commercially available tools (genetic algorithm) and (b) custom research tools (Levenberg-Marquardt). Calibration studies are carried out with real network data (medium sized pressure zone: 2000 pipe model). The whole calibration process is also described from engineering perspective, including the knowledge {{that is needed to}} conduct the calibration itself (<b>data</b> <b>input,</b> <b>validation,</b> calibration parameter ranges, etc.). It was found that the calibration results are comparable to some extent...|$|R
40|$|Field {{experiments}} {{were carried out}} at various sites in Europe to compare ambient air and precipitation concentrations of atmospheric mercury with model data. In addition, ozone and aerosol black carbon concentrations in air were measured simultaneously as <b>input</b> <b>data</b> for <b>validation</b> of the EMEP-based European long-range transport model. Data sets for mercury in air and precipitation, ozone and aerosol black carbon show that the selcted sites range from background conditions for Northwestern Europe to regionally influenced air masses from heavily industrialized Central Europe. Measured 12 h average mercury concentrations in air range from 2. 1 ng m"-" 3 for Mace Head, Ireland to 8. 2 ng m"-" 3 for the Halle/Leipzig/Bitterfeld area in former German Democratic Republic. Precipitation concentrations of total mercury from 17 to 460 ng l"-" 1 were measured from 1991 to 1993 for the same sites. Experimental data and model results for total geaseous mercury and mercury in precipitation are in good agreement. (orig.) Special print from: Atmospheric Environment (1995) v. 29 (22) p. 3333 - 3344 Available from TIB Hannover: RA 3251 (95 /E/ 67) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekSIGLEDEGerman...|$|R
40|$|International audienceIn {{order to}} {{simulate}} the spiking activity of in-vitro neuron networks recorded by MEA, the engine SIMONE (Statistical sIMulation Of Neuronal networks Engine) has been updated and used. SIMONE has been originally designed to produce realistic extracellular signals used as <b>input</b> <b>data</b> for the <b>validation</b> of spike detection and sorting algorithms. The {{aim of this study}} is to compare simulated with measured spikes trains. We present different simulations based on the architecture of a real network composed by 14 neurons grown around one electrode. The comparison between real and simulated data is quantitatively performed using different spike detection tools based on thresholding or wavelet analysis. A careful adjustment of SIMONE parameters allows to reproduce the spontaneous bursting pattern of this real network while further investigations emphasize the role of synaptic noise in network activity...|$|R
40|$|In {{order to}} give a {{concrete}} response to the need of predictive contact drying simulation tools in pharmaceutical industry, simulation programs for contact drying of pharmaceutical powders were developed in this thesis work. These are two programs for simulate the two main contact drying operation conditions used in the pharmaceutical industry: vacuum and atmospheric contact drying of agitated beds. The programs give a predictive estimation of drying rate curve and bulk bed temperature during contact drying. Only initial conditions, operating conditions, geometrical data, type of substances, solid phase properties and two parameter {{for the evaluation of}} an empirical mixing coefficient, are required as <b>input</b> <b>data.</b> A first <b>validation</b> of the developed programs was made on experimental data from literature, regarding two common pharmaceutical excipient powders, wetted with water and dried in a disc contact dryer. The simulation results show a good agreement with the experimental dat...|$|R
40|$|In {{order to}} explore {{scenarios}} {{on the future}} of power systems, a variety of numerical models have been developed. As the share of variable renewable energy sources, particularly wind and solar, is projected to significantly increase, accounting for their temporal and spatial variability becomes ever more important in developing sound long-term scenarios. Computational restrictions prevent many long-term power system models being developed with an hourly resolution; instead they use time slices that aggregate periods with similar load and renewable electricity generation levels. There is to date no reproducible and validated method to derive and select time slices for power system models with multiple fluctuating time series. In this paper, we present a novel and effective method that is easily applied to <b>input</b> <b>data</b> for all kinds of power system models. We utilize this procedure in the long-term power system model LIMES-EU and show that a small number of representative days developed in this way are sufficient to reflect the characteristic fluctuations of the <b>input</b> <b>data.</b> Alongside a <b>validation</b> of the method, we discuss the conditions under which seasonal differentiation, and the use of representative weeks instead of days, is necessary...|$|R
40|$|Thorough {{knowledge}} {{of the characteristics of}} the human body and its behaviour under extreme loading conditions is essential in order to prevent the serious consequences of road and other accidents. In order to study the human body response five type of models for the human body can be distinguished: human volunteers, human cadavers, living and dead animals, mechanical models (crash dummies) and mathematical models. These models will be briefly introduced. The main part of this paper will concentrate on mathematical models of the human body. The high standard current crash simulation technology has reached, can be attributed mainly to three developments: 1. developments in the field of multi-body techniques 2. developments in the field of finite element techniques and 3. developments in methodology and technology dealing with the determination of <b>input</b> <b>data</b> and <b>validation</b> of models for crash dummies. A review of recent developments in this field will be presented, with special emphasis on a suitable general research methodology to be used for crash dummy model development and validation. Various types of models will be presented with their application fields, capabilities and limitations. Area's of future development will be identified. Most of the current human models in use for vehicle design and safety system optimisation represent crash dummies, as prescribed in vehicle safety regulations, rather than simulating the real human body. The challenge for the future is to develop mathematical models that offer a more realistic description of the human body than current crash dummies do...|$|R
40|$|Data {{analysis}} in real-world environments {{is not well}} defined. Data are usually more faulty than expected and the knowledge available is fuzzy and incomplete. Additionally, data analysis {{has to deal with}} different observation frequencies, different regularities, and different data types. We propose a fact-finding process that takes the characteristics of real-world problems into account. All kinds of information available should be used to interpret the context. Therefore continuously and discontinuously numerical as well as qualitative data are used. Our approach consists of three main components based on temporal ontologies: <b>data</b> <b>validation,</b> <b>data</b> interpretation, and task adequate visualization. The <b>data</b> <b>validation</b> process classifies the <b>input</b> <b>data</b> according to their reliability. The data interpretation process leads to unified qualitative descriptions of point and interval data. Finally, these gathered and derived information is visualized task-oriented with more detailed descriptions [...] ...|$|R
40|$|Developing an {{application}} with some tables must concern the validation of input (specially in Table Child). In order {{to maximize the}} accuracy and <b>data</b> <b>input</b> <b>validation.</b> Its called lookup (took data from other dataset). There {{are two ways to}} look up data from Table Parent: 1) Using Objects (DBLookupComboBox and DBookupListBox), or 2) Arranging the properties of data types fields (shown by using DBGrid). In this article is using Borland Delphi software (Inprise product). The method is offered using 5 (five) practise steps: 1) Relational Database Scheme, 2) Form Design, 3) Object DatabasesRelationships Scheme, 4) Properties and Field Type Arrangement, and 5) Procedures. The result of this paper are: 1) The relationship that using lookup objects are valid, and 2) Delphi Lookup Objects can be used for 1 - 1, 1 -N, and M-N relationship. Comment: 16 page...|$|R
40|$|International audienceFull-field strain {{measurements}} are applied {{in studies of}} textile deformability during composite processing: (1) in testing of shear and tensile deformations of textiles (picture frame, bias and biaxial extension test) as an "optical extensometer", allowing accurate assessment of the sample deformation, which may differ significantly from the deformation applied by the testing device; (2) to study mechanisms of the textile deformation {{on the scale of}} the textile unit cell and of the individual yarns (meso- and micro-scale full-field strain measurements); (3) to measure the 3 D-deformed shape and the distribution of local deformations (e. g., shear angles) of a textile reinforcement after draping, providing <b>input</b> <b>data</b> for the <b>validation</b> of material drape models and for the prediction of the consolidated part performance via structural finite element analysis. This paper discusses these three applications of the full-field strain measurements, providing examples of studies of deformability of woven (glass, glass/PP) and non-crimp (carbon) textile reinforcements. The authors conclude that optical full-field strain techniques are the preferable (sometimes the only) way of assuring correct deformation measurements during tensile or shear tests of textile...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedThis thesis addresses {{the problem of}} small user groups being forced to use <b>input</b> <b>data</b> collected and processel by sources outside their span of control. Specifically, {{the use of an}} active data dictionary to locally validate such <b>input</b> <b>data</b> is examined. The thesis proceeds from a general review of <b>data</b> <b>validation</b> techniques and criteria, through an examination of data dictionaries, to an illustration cf how an active data dictionary can be configured to act as a "data filter" for <b>input</b> <b>data.</b> Key initial planning and design steps are set forth, including requirements analysis, data definition, and initial logical design. A checklist of questions to answer during each of these activities is included. The concepts discussed in the paper are then applied to a specific case (DCSPLANS Branch, a. S. Army Military Personnel Center, Alexandria, VA) resulting in a "data filter" structure diagram that is tailored to the DCSPLANS' environment and their unique validation needs. [URL] United States Arm...|$|R
40|$|This paper {{reports on}} the {{application}} of Artificial Neural Network techniques to coagulation control in drinking water treatment plants. The coagulation process involves many complex physical and chemical phenomena which are difficult to model using traditional methods. The amount of coagulant ensuring optimal treatment efficiency has been shown experimentally to be non-linearly correlated to raw water characteristics such as turbidity, conductivity, pH, temperature, etc. The software sensor developed is a hybrid system including a self-organising map (SOM) for sensor <b>data</b> <b>validation</b> and missing <b>data</b> reconstruction, and a multi-layer perceptron (MLP) for modelling the coagulation process. A key feature {{of the system is}} its ability to take into account various sources of uncertainty, such as atypical <b>input</b> <b>data,</b> measurement errors and limited information content of the training set. Experimental results with real data are presented. Keywords: <b>Data</b> <b>validation,</b> Missing <b>data</b> reconstructio [...] ...|$|R
40|$|<b>Data</b> <b>validation</b> rules {{constitute}} the constraints that <b>data</b> <b>input</b> and processing must adhere to {{in addition to}} the structural constraints imposed by a data model. Web modeling tools do not make all types of <b>data</b> <b>validation</b> explicit in their models, hampering full code generation and model expressivity. Web application frameworks do not offer a consistent interface for <b>data</b> <b>validation.</b> In this paper, we present a solution for the integration of declarative <b>data</b> <b>validation</b> rules with user interface models in the domain of web applications, unifying syntax, mechanisms for error handling, and semantics of validation checks, and covering value well-formedness, <b>data</b> invariants, <b>input</b> assertions, and action assertions. We have implemented the approach in WebDSL, a domain-specific language for the definition of web applications. Software TechnologyElectrical Engineering, Mathematics and Computer Scienc...|$|R
40|$|The European Commission and the EEA {{agreed to}} {{reinforce}} {{efforts to improve}} knowledge on implementation of air quality legislation through a joint pilot project. The Air Implementation Pilot run from March 2012 to June 2013 and aimed at better understanding the challenges cities faced in implementing air quality policy. Twelve European cities were selected and {{invited to join the}} project. One of the focus of the Pilot project was to assess the use of models for air quality assessment and management, share experiences, and identify needs for further guidance. The results of the analysis of modelling practices are presented in this work. More than 20 different models have been used for air quality assessment and management in these cities. The main purposes for which cities applied models are air quality assessment, quantification of source contribution and long term planning. The cities have found models helpful and the outputs have been used in urban air quality assessment and management, including the evaluation of strategies to reduce PM ambient levels. However, the cities found difficulties in the application of models as for instance the quality and availability of <b>input</b> <b>data</b> or the <b>validation</b> and uncertainty estimation of the model results...|$|R
40|$|This {{paper is}} a pre-print of: Danny M. Groenewegen, Eelco Visser. Integration of <b>Data</b> <b>Validation</b> and User Interface Concerns in a DSL for Web Applications. In Mark G. J. van den Brand, Jeff Gray, editors, Software Language Engineering, Second International Conference, SLE 2009, Denver, USA, October, 2009. Lecture Notes in Computer Science, Springer, 2009. <b>Data</b> <b>validation</b> rules {{constitute}} the constraints that <b>data</b> <b>input</b> and processing must adhere to {{in addition to}} the structural constraints imposed by a data model. Web modeling tools do not address <b>data</b> <b>validation</b> concerns explicitly, hampering full code generation and model expressivity. Web application frameworks do not offer a consistent interface for <b>data</b> <b>validation.</b> In this paper, we present a solution for the integration of declarative <b>data</b> <b>validation</b> rules with user interface models in the domain of web applications, unifying syntax, mechanisms for error handling, and semantics of validation checks, and covering value well-formedness, <b>data</b> invariants, <b>input</b> assertions, and action assertions. We have implemented the approach in WebDSL, a domain-specific language for the definition of web applications. Software Computer TechnologyElectrical Engineering, Mathematics and Computer Scienc...|$|R
40|$|Abstract. Monte Carlo {{simulation}} is {{a common}} method for studying the volatility of market traded instruments. It is less employed in retail lending, because of the inherent nonlinearities in consumer behavior. In this paper, we leverage the approach of Dual-time Dynamics to separate loan performance dynamics into three components: a maturation function of months-on-books, an exogenous function of calendar date, and a quality function of vintage origination date. Of these three, the exogenous function captures the impacts from the macroeconomic environment. As such, we might naturally want to generate scenarios for the possible futures of these environmental impacts. To generate such scenarios, we must go beyond the random walk methods most commonly applied {{in the analysis of}} market-traded instruments. Retail portfolios exhibit autocorrelation structure and variance growth with time that requires more complex modeling than a random walk. This paper describes work using ARMA and ARIMA models for scenario generation, rules for selecting the correct model given the <b>input</b> <b>data,</b> and <b>validation</b> methods on the scenario generation. We find when the goal is capturing the future volatility via Monte Carlo scenario generation, that model selection does not follow the same rules as for forecasting. Instead, test more appropriate to reproducing volatility are proposed, which assure that distributions of scenarios have the proper statistical characteristics. These results are supported by studies of the variance growth properties of macroeconomic variables and theoretical calculations of the variance growth properties of various models. We also provide studies on historical data showing minimum training lengths and differences by macroeconomic epochs. Key words. Economic Capital, Retail Lending, Monte Carlo Simulation, Dual-time Dynamic...|$|R
40|$|This {{contribution}} {{focuses on}} results obtained by processing, {{for the first}} time, ROSA Radio Occultation observations performed on-board the Indian mission OCEANSAT- 2. It summarizes an in-depth quality check of ROSA data, performed during a Visiting Scientist activity funded by the ROM-SAF (Radio Occultation Meteorology - Satellite Application Facility) and in close cooperation with EUMETSAT and the Italian Space Agency. The focus here is on the potential use for operational weather forecasting. A set of ROSA raw data was processed using the EUMETSAT YAROS processor, a prototype created to develop new algorithms and to test them before introducing them into the operational chain. YAROS outputs are phases-amplitudes (at level 1 a) and bending angles over impact parameter (at level 1 b). Robust bias and standard deviation of bending angles against forward propagated ECMWF atmospheric data are {{used to evaluate the}} quality of the ROSA observations. Moreover, the ROM-SAF ROPP (Radio Occultation Processing Package) processor was also used for ROSA data processing. This second part focuses in addition on bending angles, refractivity and higher level products obtained from the YAROS level 1 a <b>input</b> <b>data.</b> The <b>validation</b> was again based on ECMWF and also on co-located occultation profiles. The main issue affecting the ROSA data quality is tracking of the second GPS frequency L 2. L 2 data is often only acquired at altitudes above 20 km, which makes the extrapolation of this data problematic. This currently severely limits the usefulness of the ROSA data (at least on OCEANSAT- 2) and requires the development of more robust ionospheric correction algorithms and an investigation and possible mitigation of the problems onboard the satellit...|$|R
3000|$|... meta-level test data: {{this part}} {{is similar to}} the part of {{generating}} meta-level training <b>data</b> in the <b>validation</b> process. The only difference is that the <b>input</b> <b>data</b> are the test data. This part is shown through lines 2 - 14.|$|R
