4203|8|Public
2500|$|The IASP {{criteria}} for CRPS I diagnosis {{has shown a}} sensitivity ranging from 98–100% and a specificity ranging from 36–55%. Per the IASP guidelines, <b>interobserver</b> reliability for CRPS I diagnosis is poor. [...] Two other criteria used for CRPS I diagnosis are Bruehl's criteria and Veldman's criteria, which have moderate to good <b>interobserver</b> reliability. In the absence of clear evidence supporting one set of criteria over the other, clinicians may use IASP, Bruehl’s, or Veldman’s clinical {{criteria for}} diagnosis. While the IASP criteria are nonspecific and possibly not as reproducible as Bruehl’s or Veldman’s criteria, they are cited more widely in literature, including treatment trials.|$|E
50|$|For the grading of {{osteoarthritis}} in the knee, the International Knee Documentation Committee (IKDC) {{system is}} regarded {{to have the}} most favorable combination of <b>interobserver</b> precision and correlation to knee arthroscopy findings. The Ahlbäck system has been found to have comparable <b>interobserver</b> precision and arthroscopy correlation, but most of its span is focused at various degrees of bone defect or loss, and it is therefore less useful in early osteoarthritis. Systems that have been found to have lower <b>interobserver</b> precision and/or arthroscopy correlation are those developed by Kellgren-Lawrence, Fairbank, Brandt, and Jäger-Wirth.|$|E
5000|$|It occurs {{frequently}} in the elderly irrespective of them being hypertensive, and has moderate to modest intraobserver and <b>interobserver</b> agreement. It {{is also known as}} [...] "Osler's maneuver".|$|E
5000|$|There {{are many}} grading systems for {{degeneration}} of intervertebral discs and facet joints in the cervical and lumbar vertebrae, {{of which the}} following radiographic systems can be recommended in terms of <b>interobserver</b> reliability: ...|$|E
50|$|The IASP {{criteria}} for CRPS I diagnosis {{has shown a}} sensitivity ranging from 98-100% and a specificity ranging from 36-55%. Per the IASP guidelines, <b>interobserver</b> reliability for CRPS I diagnosis is poor. Two other criteria used for CRPS I diagnosis are Bruehl's criteria and Veldman's criteria, which have moderate to good <b>interobserver</b> reliability. In the absence of clear evidence supporting one set of criteria over the other, clinicians may use IASP, Bruehl’s, or Veldman’s clinical {{criteria for}} diagnosis. While the IASP criteria are nonspecific and possibly not as reproducible as Bruehl’s or Veldman’s criteria, they are cited more widely in literature, including treatment trials.|$|E
50|$|When {{the gold}} {{standard}} is not a perfect one, its sensitivity and specificity must be calibrated against more accurate tests or against {{the definition of the}} condition. This calibration is especially important when a perfect test is available only by autopsy.It is important to emphasize that a test has to meet some <b>interobserver</b> agreement, to avoid some bias induced by the study itself.|$|E
50|$|Another {{criticism}} {{is the fact}} that many of the measurements that are essential to this program are subject to both <b>interobserver</b> error and intraobserver error. Measurements between researchers can vary substantially in size and that this degree of variation in measurements can have a striking effect on the results of CranID. Due to this potential error, the results of CranID should be taken into account when assessing how accurate any findings formed by CranID are.|$|E
50|$|The {{scale was}} {{originally}} introduced in 1957 by Dr. John Rankin of Stobhill Hospital, Glasgow, Scotland, and then modified to its currently accepted form by Prof. C. Warlow's group at Western General Hospital in Edinburgh {{for use in}} the UK-TIA study in the late 1980s. The first publication of the current modified Rankin Scale was in 1988 by van Swieten, et al., who also published the first <b>interobserver</b> agreement analysis of the modified Rankin Scale.|$|E
5000|$|The most {{commonly}} used scale to rate the severity of spasmodic torticollis is the Toronto Western Spasmodic Torticollis Rating Scale (TWSTRS). It {{has been shown that}} this rating system has widespread acceptance for use in clinical trials, and has been shown to have “good <b>interobserver</b> reliability.” [...] There are three scales in the TWSTRS: torticollis severity scale, disability scale, and pain scale. These scales are used to represent the severity, the pain, and the general lifestyle of spasmodic torticollis.|$|E
50|$|The {{utilities}} of STE {{are increasingly}} recognized. Strain results derived from STE have been validated using sonomicrometry and tagged MRI and results correlate significantly with Tissue Doppler-derived measurements. Tissue Doppler technology, the alternative method for strain rate imaging to speckle tracking technology, requires achieving sufficient parallel orientation between {{the direction of}} motion and the ultrasound beam. Its use has remained limited due to angle dependency, substantial intraobserver and <b>interobserver</b> variability and noise interference. Speckle tracking technology has {{to a certain degree}} overcome these limitations.|$|E
50|$|<b>Interobserver</b> {{reliability}} of the mRS can be improved by using a structured questionnaire during the interview process and by having raters undergo a multimedia training process. The multimedia mRS training system which was developed by Prof. K. Lees' group at the University of Glasgow is available online. The mRS is frequently criticized for its subjective nature which is viewed as skewing results, but is used throughout hospital systems to assess rehabilitation needs and outpatient course. These criticisms were addressed by researchers creating structured interviews which ask simple questions both the patient and/or the caregiver can respond to.|$|E
50|$|There {{are many}} {{factors that can}} {{play a role in the}} blood {{pressure}} reading by physician, such as hearing problem, auditory perception of the physician.Karimi Hosseini et al evaluated the <b>interobserver</b> differences among specialists without any auditory impairment, and reported 68% of observers recorded systolic blood pressure in a range of 9.4 mmHg, diastolic blood pressure in a range of 20.5 mmHg and mean blood pressure in a range of 16.1mmHg. Neufeld et al reported standard deviations for both systolic and diastolic readings were roughly 3.5 to 5.5 mm Hg. In general standard deviation for the diastolic pressure would be greater because of the difficulty in judging when the sounds disappear.|$|E
50|$|Inter-observer {{reliability}} is {{the extent}} to which two or more observers agree with each other. Researchers can help foster higher <b>interobserver</b> reliability if they clearly define the constructs they are interested in measuring. If there is low inter-observer reliability, it is likely that the construct being observed is too ambiguous, and the observers are all imparting their own interpretations. For instance, in Donna Eder's study on peer relations and popularity for middle school girls, it was important that observers internalized a uniform definition of “friendship” and “popularity”. While it’s possible for multiple people to agree about something and all be incorrect, the more people that agree the less likely it is that they will be in error.|$|E
5000|$|Unique hue is a {{term used}} in certain {{theories}} of color vision, which implies that human perception distinguishes between [...] "unique" [...] (psychologically primary) and composite (mixed) hues. A unique hue {{is defined as a}} color which an observer perceives as a pure, without any admixture of the other colors. There {{is a great deal of}} variability when defining unique hues experimentally. Often the results show a great deal of <b>interobserver</b> and intraobserver variability leading to much debate on the number of unique hues. Another source of variability is environmental factors in color naming. Despite the inconsistencies, often four color perceptions are associated as unique; [...] "red", [...] "green", [...] "blue", and [...] "yellow".|$|E
5000|$|Another methodological {{problem is}} that <b>interobserver</b> {{reliability}} (the likelihood that the {{two members of the}} measured dyad respond similarly) is near zero for tested husband and wife couples. That is, the chances of a given couple reporting similar answers about events they both experienced is no greater than chance. On the most severe CTS items, husband-wife agreement is actually below chance: [...] "On the item [...] "beat up," [...] concordance was nil: although there were respondents of both sexes who claimed to have administered beatings and respondents of both sexes who claimed to have been on the receiving end, {{there was not a single}} couple in which one party claimed to have administered and the other to have received such a beating." ...|$|E
5000|$|A later {{study of}} 50 {{subtrochanteric}} fractures, which were assessed using the Seinsheimer classification, criticized the classification system for having poor inter-rater reliability. They noted that [...] "Earlier studies of pertrochanteric sic and femoral neck fractures {{show that the}} use of classification systems is often difficult, with low agreement (Frandsen et al. 1988, Andersen et al. 1990, Gehrchen et al. 1993). The results of these studies accord with the low level of <b>interobserver</b> agreement in our study. A total agreement of 26% or at the best 60% is not acceptable." [...] A further study published in 2014 assessed four subtrochanteric fracture classification systems, including the Seinsheimer system, and concluded that [...] "the four subtrochanteric classification systems which we assessed were not found to be sufficiently reproducible to be of any significant value in clinical practice." ...|$|E
5000|$|In statistics, in the {{analysis}} of two-way randomized block designs where the response variable can take only two possible outcomes (coded as 0 and 1), Cochran's Q test is a non-parametric statistical test to verify whether k treatments have identical effects. It is named after William Gemmell Cochran. Cochran's Q test {{should not be confused with}} Cochran's C test, which is a variance outlier test. Put in simple technical terms, Cochran's Q test requires that there only be a binary response (e.g. success/failure or 1/0) and that there be more than 2 groups of the same size. The test assesses whether the proportion of successes is the same between groups. Often it is used to assess if different observers of the same phenomenon have consistent results (<b>interobserver</b> variability).|$|E
50|$|Regardless of methodology, the {{analysis}} of testis biopsy histology lacks clinical value in cases of infertility {{because there is no}} clear correlation between histologic patterns or Johnsen score and the underlying etiology of infertility. That is to say, the clinical utility of understanding the histology pattern is low, because biopsy patterns do not correlate well with specific and correctable diseases. In addition, the <b>interobserver</b> variability in testis biopsy readings for infertility is significant. This was aptly demonstrated in a study by Cooperberg et al. in which the histology readings from two independent pathological reviews were prospectively compared with 113 testis biopsies undertaken for infertility. Importantly, in 28% of cases preparation artifact or insufficient biopsy size rendered the sample suboptimal for interpretation. In addition, in 46% of cases, the two reviews disagreed, and this discordance resulted in significant changes in clinical care in 27% of cases. The most common error in pathological review was the under-appreciation of mixed histological patterns that are common and characteristic of infertile men with no sperm counts. Thus, although commonly used, the classical testis biopsy has little or no correlation with specific diseases, is associated with significant variability in interpretation, and can miss mixed patterns of spermatogenesis that may qualify infertile men for assisted reproduction.|$|E
5000|$|The histopathologic grading of oligodendrogliomas is controversial. Currently {{the most}} {{commonly}} used grading schema is based on year 2007 World Health Organization (WHO) guidelines. An updated classification is in progress. [...] Oligodendrogliomas are generally dichotomized into grade II (low grade) and grade III (high grade) tumors. The designation of grade III oligodendroglioma (high grade) generally subsumes the previous diagnoses of anaplastic or malignant oligodendroglioma. [...] Unfortunately, the WHO guidelines include subjective criteria in differentiating grade II and grade III tumors including the appreciation of [...] "significant" [...] hypercellularity and pleomorphism in the higher grade lesion. In addition, the presence of low mitotic activity, vascular proliferation and necrosis, including pseudopallisading necrosis are insufficient by themselves to elevate the grade of these tumors. This leads to inevitable <b>interobserver</b> variability in diagnosis by pathologists. The ultimate responsibility for making treatment decisions and interpretation of these diagnoses lies with the oncologist in consultation with the patient and their family. It has been proposed that WHO guidelines should contain a category for grade IV oligodendrogliomas which essentially appear to be glial neoplasms with overwhelming features of glioblastoma multiforme (GBM) arising from known lower grade oligodendrogliomas or GBM with a significant proportion of oligodendroglial differentiation. The diagnostic utility of this latter category is uncertain as these tumors may behave either like glioblastoma or grade III oligodendrogliomas. As such, this is an exceptionally unusual diagnosis. [...] The updated WHO guidelines published in 2007 recommends classifying such tumors for the time being as 'glioblastoma with oligodendroglioma component'. It remains to be established whether or not these tumors carry a better prognosis than standard glioblastomas.|$|E
40|$|OBJECTIVES: Severe acute {{pancreatitis}} {{is associated with}} peripancreatic morphologic changes as seen on imaging. Uniform communication regarding these morphologic findings is crucial for accurate diagnosis and treatment. For the original 1992 Atlanta classification, <b>interobserver</b> agreement is poor. We hypothesized that for the revised Atlanta classification, <b>interobserver</b> agreement will be better. METHODS: An international, <b>interobserver</b> agreement study was performed among expert and nonexpert radiologists (n = 14), surgeons (n = 15), and gastroenterologists (n = 8). Representative computed tomographies of all stages of {{acute pancreatitis}} were selected from 55 patients and were assessed according to the revised Atlanta classification. The <b>interobserver</b> agreement was calculated among all reviewers and subgroups, that is, expert and nonexpert reviewers; <b>interobserver</b> agreement was defined as poor (</= 0. 20), fair (0. 21 - 0. 40), moderate (0. 41 - 0. 60), good (0. 61 - 0. 80), or very good (0. 81 - 1. 00). RESULTS: <b>Interobserver</b> agreement among all reviewers was good (0. 75 [standard deviation, 0. 21]) for describing the type of acute pancreatitis and good (0. 62 [standard deviation, 0. 19]) {{for the type of}} peripancreatic collection. Expert radiologists showed the best and nonexpert clinicians the lowest <b>interobserver</b> agreement. CONCLUSIONS: <b>Interobserver</b> agreement was good for the revised Atlanta classification, supporting the importance for widespread adaption of this revised classification for clinical and research communications...|$|E
40|$|To {{evaluate}} the <b>interobserver</b> reproducibility of two-dimensional {{shear wave elastography}} (2 D-SWE) in measuring liver stiffness (LS) and to investigate factors related to liver 2 D-SWE. A prospective study was conducted between August 2011 and August 2012 in rheumatoid arthritis patients who had been treated with methotrexate. <b>Interobserver</b> reproducibility of 2 D-SWE was evaluated, {{and the relationship between}} <b>interobserver</b> difference in LS and related factors was analyzed using linear regression analyses. We considered age, sex, alanine transaminase, cholesterol, body mass index (BMI), and waist circumference as clinical factors, and the mean value of standard deviation (SDM), its difference between two examiners, mean diameter of the regions of interest (ROIM), and its difference in the elasticity map as investigation factors. The cut-off value for significant factors to predict <b>interobserver</b> discrepancies in LS-based fibrosis stage was also inspected. In total, 176 patients were enrolled. The intraclass correlation coefficient between the two examiners was 0. 784. In the univariate analysis, SDM and ROIM were independently associated with <b>interobserver</b> differences in LS as well as BMI, waist circumference, and the difference of ROI, but SDM and ROIM were the only ones significantly related in multivariate analysis (p< 0. 001 and p = 0. 021, respectively). The best cut-off value for SDM in predicting <b>interobserver</b> discrepancy in LS-based fibrosis stage was 1. 4. <b>Interobserver</b> reproducibility of 2 D-SWE for measuring LS was good and SDM was the most significantly associated factor with <b>interobserver</b> differences in LS and <b>interobserver</b> discordance in LS-based fibrosis stage...|$|E
40|$|AIM: To {{carry out}} an {{objective}} assessment of two systems of scoring immunohistochemical staining, evaluating <b>interobserver</b> and intraobserver error. METHODS: 92 cervical tumours underwent immunohistochemical staining for p 53 and epidermal growth factor receptor. Staining was assessed using two methods: a standard 4 point scale and a descriptive method, performed by three observers. <b>Interobserver</b> and intraobserver error were assessed for both scoring methods. RESULTS: In terms of <b>interobserver</b> error between three observers, no difference was found between a simple 4 point scale method of evaluation {{and the use of}} a highly circumscribed method. In all evaluations, <b>interobserver</b> error was scored as moderate (kappa w 0. 48 - 0. 49). However, evaluation of immunohistochemical staining by a panel of observers led to a marked improvement in the <b>interobserver</b> error scores (kappa w 0. 63). CONCLUSIONS: There should be standardisation of immunohistochemical staining and scoring methods. More attention should be paid to measurement of <b>interobserver</b> and intraobserver error in studies. Use of a panel of tissue control slides and consensus scoring by several observers can lead to improvement in reproducibility...|$|E
40|$|SummaryIntroductionThe Lagrange and Rigault {{classification}} {{was designed}} to describe extension-type supracondylar fractures of the humerus. It can also help in treatment decision-making. HypothesisThe reliability of this classification {{has not yet been}} proven. The goal {{of this study was to}} assess this system's intra- and <b>interobserver</b> reliability. MethodsOne hundred supracondylar fracture radiographs were randomly retrieved and reviewed by five different observers on two occasions in a different order. The kappa index was used to calculate the intra- and <b>interobserver</b> reliability. ResultsIntraobserver reliability was 0. 76 and <b>interobserver</b> reliability was 0. 69. DiscussionThe study shows good intra- and <b>interobserver</b> reliability. The Lagrange and Rigault classification has similar reliability to other supracondylar fracture classifications...|$|E
30|$|<b>Interobserver</b> {{agreement}} was obtained for oral responses in tacting and reading probes. The researcher {{and a second}} observer separately watched the videos and transcribed the speech of each participant independently. <b>Interobserver</b> {{agreement was}} computed using the following formula (Kazdin, 1982): total number of agreements divided by agreements plus disagreements, multiplied by 100 (number of agreements / [number of agreements + number of disagreements] × 100). The <b>interobserver</b> agreement for each sentence was ≥[*] 93.89 %.|$|E
40|$|OBJECTIVES: To {{evaluate}} {{the reliability of}} semiquantitative Vertebral Fracture Assessment (VFA) on chest Computed Tomography (CT). METHODS: Four observers performed VFA twice upon sagittal reconstructions of 50 routine clinical chest CTs. Intra- and <b>interobserver</b> agreement (absolute agreement or 95 % Limits of Agreement) and reliability (Cohen's kappa or intraclass correlation coefficient(ICC)) were calculated for the visual VFA measures (fracture present, worst fracture grade, cumulative fracture grade on patient level) and for percentage height loss of each fractured vertebra compared to the adjacent vertebrae. RESULTS: Observers classified 24 - 38 % patients as having at least one vertebral fracture, giving rise to kappa's of 0. 73 - 0. 84 (intraobserver) and 0. 56 - 0. 81 (<b>interobserver).</b> For worst fracture grade we found good intraobserver (76 - 88 %) and <b>interobserver</b> (74 - 88 %) agreement, and excellent reliability with square-weighted kappa's of 0. 84 - 0. 90 (intraobserver) and 0. 84 - 0. 94 (<b>interobserver).</b> For cumulative fracture grade the 95 % Limits of Agreement were maximally ± 1, 99 (intraobserver) and ± 2, 69 (<b>interobserver)</b> and the reliability (ICC) varied from 0. 84 - 0. 94 (intraobserver) and 0. 74 - 0. 94 (<b>interobserver).</b> For percentage height-loss on a vertebral level the 95 % Limits of Agreement were maximally ± 11, 75 % (intraobserver) and ± 12, 53 % (<b>interobserver).</b> The ICC was 0. 59 - 0. 90 (intraobserver) and 0. 53 - 0 - 82 (<b>interobserver).</b> Further investigation is needed to {{evaluate the}} prognostic value of this approach. CONCLUSION: In conclusion, these results demonstrate acceptable reproducibility of VFA on CT...|$|E
40|$|Objective To analyze intra and <b>interobserver</b> {{agreement}} of two radiographic methods {{for evaluation of}} posterolateral lumbar arthrodesis. Methods Twenty patients undergoing instrumented posterolateral fusion were evaluated by anteroposterior and dynamic lateral radiographs in maximal flexion and extension. The images were evaluated initially by 6 orthopedic surgeons, and after 8 weeks, reassessed by 4 of them, totaling 400 radiographic measurements. Intra and <b>interobserver</b> reliability were analyzed using the Kappa coefficient and Landis and Koch criteria. Results Intra and <b>interobserver</b> agreement regarding anteroposterior radiographs were, respectively, 76 and 63 %. On lateral views, these values were 78 and 84 %, respectively. However, the Kappa analysis showed poor intra and <b>interobserver</b> agreement in most cases, regardless of the radiographic method used. Conclusion There was poor intra and <b>interobserver</b> agreement {{in the evaluation of}} lumbosacral fusion by plain film in anteroposterior and dynamic lateral views, with no statistical superiority between the methods...|$|E
30|$|Several studies {{concerning}} the intraobserver and <b>interobserver</b> agreements {{of the methods}} were published, {{and most of them}} were focused on the Graf method. The reported intraobserver and <b>interobserver</b> reliability {{concerning the}} hip typing in the Graf method ranged from moderate to substantial and from fair to substantial, respectively [21, 22, 23, 24, 25, 26]. Besides, the reported intraobserver and <b>interobserver</b> measurement variability of the α angle ranged from 4 ° to 11 ° and from 3 ° to 13 °, respectively [21, 23, 24, 26, 27]. The reported intraobserver and <b>interobserver</b> measurement variability of the β angle was between 6 ° and 14 ° and between 6 ° and 19 °, respectively [23, 24, 26, 27].|$|E
40|$|Objectives. The {{measurement}} of mucociliary transport velocity by rhinoscintigraphy with Tc- 99 m-macroaggregated albumin (99 mTc-MAA) is reliable measure of mucociliary clearance. The {{aim of this}} study is to assess the intratest, <b>interobserver,</b> and intraobserver reproducibility of nasal mucociliary transport rate (NMTR) measurement. Materials and Methods. Twenty-two subjects were evaluated to determine intratest reproducibility and a group of 35 subjects was examined to determine inter- and intraobserver reproducibility. Rhinoscintigraphy with 99 mTc-MAA was used to measure NMTR in all study subjects. Paired NMTR measurements were compared using a range of statistical methodologies. Intraclass correlation coefficients (ICC) and repeatability coefficients and Bland-Altman plots were applied to assess the degree of intratest, <b>interobserver,</b> and intraobserver variation. Results. Statistical analysis of test and retest experiments demonstrated the statistical equivalence of intratest NMTR measurements, <b>interobserver</b> NMTR measurements, and intraobserver NMTR measurements. The intratest ICC, <b>interobserver</b> ICC, and intraobserver ICC were 0. 96, 0. 83, and 0. 91, respectively, indicating that intratest and intraobserver reproducibility are excellent and <b>interobserver</b> reproducibility is good. Conclusions. Rhinoscintigraphy using 99 mTc-MAA results in highly reproducible {{measurement of}} NMTR. The use of radionuclide imaging in measuring NMTR results in excellent intratest and intraobserver reproducibility and good <b>interobserver</b> reliability...|$|E
40|$|International audienceBACKGROUND:Assessment {{of disease}} {{activity}} in UC {{is important for}} designing an optimal therapeutic strategy. No single histology score is considered optimum. The {{aim of this study}} was to compare intraobserver reproducibility and the <b>interobserver</b> agreement of available histological UC activity indexes. METHODS:One hundred and two biopsy specimens (collected between 2003 and 2014) were scored blindly by three pathologists by determining Geboes, Riley, Gramlich and Gupta indexes and global visual evaluation (GVE). Intraobserver reproducibility and <b>interobserver</b> agreements for index and items of index were studied by intraclass correlation coefficient for quantitative parameter and by κ values and Krippendorff index for qualitative parameters. Relationship between indexes was studied by computation of Pearson's and Spearman's correlation coefficients. RESULTS:Geboes, Riley, Gramlich and Gupta indexes and GVE showed good intraobserver reproducibility and a good <b>interobserver</b> agreement. Histological items that showed the best <b>interobserver</b> agreement were 'erosion/ulceration or surface epithelial integrity' and 'acute inflammatory cells infiltrate/neutrophils in lamina propria'. The five scores were strongly correlated. CONCLUSIONS:Correlation between indexes is strong. Intraobserver reproducibility and <b>interobserver</b> agreement for all indexes is very good. Histological items that showed the best <b>interobserver</b> agreement are 'erosion/ulceration' and 'acute inflammatory cells infiltrate/neutrophils in lamina propria'...|$|E
40|$|The {{purpose was}} to {{evaluate}} <b>interobserver</b> agreement on Doppler ultrasonographic (US) diagnosis of liver vascular malformations (VMs) in hereditary hemorrhagic teleangiectasia (HHT) and on their severity grading. During the <b>interobserver</b> agreement study, three observers with at least seven years of specific experience using Doppler US for the diagnosis of liver VMs, judged about the presence/absence of liver VMs and their severity {{on a set of}} images and videoclips. <b>Interobserver</b> agreement was estimated with kappa statistics. One-hundred ten cases were reviewed during <b>interobserver</b> study (80 cases with liver VMs, 30 without). Very good kappa values of the <b>interobserver</b> agreement were found for all pairs concerning the distinction between presence and absence of hepatic VMs. All observers demonstrated excellent sensitivity and specificity in identifying hepatic VMs, with their respective areas under the curve ranging from 0. 97 to 0. 99. <b>Interobserver</b> agreement among the three investigators in staging the hepatic VMs in HHT patients was moderate (Kendall's coefficient of concordance = 0. 26). Study results indicate that Doppler US diagnosis of liver VMs in HHT has a high degree of agreement among ultrasonographers; a moderate agreement was found regarding severity staging...|$|E
40|$|Introduction The Cl. NPhys Trial 3 {{showed that}} {{attributes}} of nerve conduction (NC) were without significant intraobserver differences, {{although there were}} significant <b>interobserver</b> differences. Methods : Trial 4 tested whether use of written instructions and pretrial agreement on techniques and use of standard reference values, diagnostic percentile values, or broader categorization of abnormality could reduce significant <b>interobserver</b> disagreement and improve agreement among clinical neurophysiologists. Results : The Trial 4 modifications markedly decreased, but did not eliminate, significant <b>interobserver</b> differences of measured attributes of NC. Use of standard reference values and defined percentile values of abnormality decreased <b>interobserver</b> disagreement and improved agreement of judgment of abnormality among evaluators. Therefore, the same clinical neurophysiologist should perform repeat NCs of therapeutic trial patients. Conclusions : Differences in <b>interobserver</b> judgment of abnormality decrease with use of common standard reference values and a defined percentile level of abnormality, providing a rationale for their use in therapeutic trials and medical practice. Muscle Nerve 50 : 900 – 908, 201...|$|E
40|$|Introduction: This study {{evaluated}} the <b>interobserver</b> reliability of plain radiograpy versus computed tomography (CT) for the Universal and AO classification systems for distal radius fractures. Patients and methods: Five observers classified 21 sets of distal radius fractures using plain radiographs and CT independently. Kappa statistics {{were used to}} establish a relative level of agreement between observers for both readings. Results: <b>Interobserver</b> agreement was rated as moderate for the Universal classification and poor for the AO classification. Reducing the AO system to 9 categories and to its three main types reliability was raised to a ""moderate"" level. No difference was found for <b>interobserver</b> reliability between the Universal classification using plain radiographs and the Universal classification using computed tomography. <b>Interobserver</b> reliability of the AO classification system using plain radiographs {{was significantly higher than}} the <b>interobserver</b> reliability of the AO classification system using only computed tomography. Conclusion: From these data, we conclude that classification of distal radius fractures using CT scanning without plain radiographs is not beneficial...|$|E
30|$|Reader {{agreement}} {{was assessed by}} kappa statistics for each imaging modality. The <b>interobserver</b> agreement for F PET/CT was 0.68 for spinal lesions and 0.88 for SIJ lesions. <b>Interobserver</b> agreement for MRI ranged from 0.64 for spinal lesions to 0.93 for the SIJ lesions.|$|E
40|$|The aim of {{this study}} was to {{determine}} the variability of various measurement protocols for measurement of abdominal aortic aneurysm (AAA) and the clinical relevance of variability. Three radiologists performed computed tomographic angiography measurements of both the aorta and the largest portion of the aneurysm on selected axial slices. Then measurements of the largest portion of the aneurysm were performed on unselected axial slices, sagittal and coronal reformatted. Finally, aortic volume was calculated. Measurements and volume calculation were performed before and after endovascular repair and assessed: Part 1 : <b>interobserver</b> variability for maximum anteroposterior (MAP) and maximum transverse (MTR) diameters on selected slices; part 2 : <b>interobserver</b> variability for unselected slices considering MAP and MTR; part 3 : <b>interobserver</b> variability considering maximum diameter in any direction (MAD); part 4 : <b>interobserver</b> variability for sagittal (SAG) and coronal (COR) free curved multiplanar reformation (MPR); and part 5 : volume calculations. We then determined which technique of measurement was the most clinically relevant for detecting changes in aneurysm size or aortic volume. Parts 1 and 2 : <b>interobserver</b> variability was 4. 1 mm for both MAP and MTR; part 3 : <b>interobserver</b> variability was 7 mm for MAD; part 4 : <b>interobserver</b> variability was 5. 5 mm (COR) and 4. 9 mm (SAG); part 5 : <b>interobserver</b> variability for volume was 5. 5 ml. A combination of MAP and MTR was the most useful for detecting aortic modification. Volume calculation was needed in only a few cases. We recommend avoiding MAD and MPR measurements and suggest instead measuring both maximum anteroposterior and maximum transverse diameters. If aneurysm size remains stable after endovascular repair, aneurysm volume should be measured. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|E
40|$|Introduction: Thrombus density {{may be a}} {{predictor}} for acute ischemic stroke treatment success. However, only limited data on observer variability for thrombus density measurements exist. This study assesses the variability and bias of four common thrombus density measurement methods by expert and non-expert observers. Methods: For 132 consecutive patients with acute ischemic stroke, three experts and two trained observers determined thrombus density by placing three standardized regions of interest (ROIs) in the thrombus and corresponding contralateral arterial segment. Subsequently, absolute and relative thrombus densities were determined using either one or three ROIs. Intraclass correlation coefficient (ICC) was determined, and Bland–Altman analysis was performed to evaluate <b>interobserver</b> and intermethod agreement. Accuracy of the trained observer was evaluated with a reference expert observer using the same statistical analysis. Results: The highest <b>interobserver</b> agreement was obtained for absolute thrombus measurements using three ROIs (ICCs ranging from 0. 54 to 0. 91). In general, <b>interobserver</b> agreement was lower for relative measurements, and for using one instead of three ROIs. <b>Interobserver</b> agreement of trained non-experts and experts was similar. Accuracy of the trained observer measurements was comparable to the expert <b>interobserver</b> agreement and was better for absolute measurements and with three ROIs. The agreement between the one ROI and three ROI methods was good. Conclusion: Absolute thrombus density measurement has superior <b>interobserver</b> agreement compared to relative density measurement. <b>Interobserver</b> variation is smaller when multiple ROIs are used. Trained non-expert observers can accurately and reproducibly assess absolute thrombus densities using three ROIs...|$|E
40|$|Introduction: The Lagrange and Rigault {{classification}} {{was designed}} to describe extension-type supracondylar fractures of the humerus. It can also help in treatment decision-making. Hypothesis: The reliability of this classification {{has not yet been}} proven. The goal {{of this study was to}} assess this system's intra- and <b>interobserver</b> reliability. Methods: One hundred supracondylar fracture radiographs were randomly retrieved and reviewed by five different observers on two occasions in a different order. The kappa index was used to calculate the intra- and <b>interobserver</b> reliability. Results: Intraobserver reliability was 0. 76 and <b>interobserver</b> reliability was 0. 69. Discussion: The study shows good intra- and <b>interobserver</b> reliability. The Lagrange and Rigault classification has similar reliability to other supracondylar fracture classifications. (c) 2010 Elsevier Masson SAS. All rights reserved...|$|E
