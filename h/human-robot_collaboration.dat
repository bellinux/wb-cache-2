214|13|Public
5|$|Air-Cobot (Aircraft Inspection {{enhanced}} by smaRt & Collaborative rOBOT) is a French {{research and development}} project of a wheeled collaborative mobile robot able to inspect aircraft during maintenance operations. This multi-partner project involves research laboratories and industry. Research around this prototype was developed in three domains: autonomous navigation, nondestructive testing and <b>human-robot</b> <b>collaboration.</b>|$|E
25|$|In future, {{co-operation}} between robots {{and humans}} will be diversified, with robots increasing their autonomy and <b>human-robot</b> <b>collaboration</b> reaching completely new forms. Current approaches and technical standards aiming to protect employees from {{the risk of}} working with collaborative robots {{will have to be}} revised.|$|E
2500|$|Safety {{standards}} {{are being developed}} by the [...] (RIA) {{in conjunction with the}} American National Standards Institute (ANSI). On October 5, 2017, OSHA, NIOSH and RIA signed an [...] to work together to enhance technical expertise, identify and help address potential workplace hazards associated with traditional industrial robots and the emerging technology of <b>human-robot</b> <b>collaboration</b> installations and systems, and help identify needed research to reduce workplace hazards. On October 16 NIOSH launched the [...] to [...] "provide scientific leadership to guide the development and use of occupational robots that enhance worker safety, health, and wellbeing." [...] So far, the research needs identified by NIOSH and its partners include: tracking and preventing injuries and fatalities, intervention and dissemination strategies to promote safe machine control and maintenance procedures, and on translating effective evidence-based interventions into workplace practice.|$|E
40|$|AbstractThis {{study is}} {{concerned}} with performance parameters while pushing and pulling a trolley to evaluate the characteristics of <b>human-robot</b> <b>collaborations.</b> To rescind the linkage between estimated weight and exerted force the trolley is laden with three different weights and three different object sizes. The participants had to maneuver the trolley on a given path, similar to a real production scenario in automotive assembly lines. Twenty-two subjects participated (13 men, 9 women) and were recorded by a VICON motion tracking system. The applied forces were measured independently on each handle in three coordinates via a Kistler hand force measuring system. The subjective impressions were acquired through surveys. The acquired results {{in this study are}} divided in maximum thrust force and applied force in the first 150 ms, 300 ms, and 1000 ms. The first refers to different handle angles. The last relate to the mentioned size-weight illusion...|$|R
40|$|We present OPTIMo: an Online Probabilistic Trust Infer-ence Model for {{quantifying}} {{the degree}} of trust that a human supervisor has in an autonomous robot “worker”. Repre-sented as a Dynamic Bayesian Network (DBN), OPTIMo infers beliefs over the human’s moment-to-moment latent trust states, based {{on the history of}} observed interaction experiences. A separate model instance is trained on each user’s experiences, leading to an interpretable and person-alized characterization of that operator’s behaviors and at-titudes. Using datasets collected from an interaction study with a large group of roboticists, we empirically assess OP-TIMo’s performance under a broad range of configurations. These evaluation results highlight OPTIMo’s advances in both prediction accuracy and responsiveness over several existing trust models. This accurate and near real-time human-robot trust measure makes possible the development of autonomous robots that can adapt their behaviors dynam-ically, to actively seek greater trust and greater efficiency within future <b>human-robot</b> <b>collaborations.</b> Categories and Subject Descriptors H. 1. 2 [Models and Principles]: User/Machine Systems— software psycholog...|$|R
40|$|Modeling of {{physical}} <b>human-robot</b> <b>collaborations</b> is generally a challenging problem {{due to the}} unpredictive nature of human behavior. To address this issue, we present a data-efficient reinforcement learning framework which enables a robot {{to learn how to}} collaborate with a human partner. The robot learns the task from its own sensorimotor experiences in an unsupervised manner. The uncertainty of the human actions is modeled using Gaussian processes (GP) to implement action-value functions. Optimal action selection given the uncertain GP model is ensured by Bayesian optimization. We apply the framework to a scenario in which a human and a PR 2 robot jointly control the ball position on a plank based on vision and force/torque data. Our experimental results show the suitability of the proposed method in terms of fast and data-efficient model learning, optimal action selection under uncertainties and equal role sharing between the partners. Comment: The paper is accepted for publication at the 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2016...|$|R
50|$|In future, {{co-operation}} between robots {{and humans}} will be diversified, with robots increasing their autonomy and <b>human-robot</b> <b>collaboration</b> reaching completely new forms. Current approaches and technical standards aiming to protect employees from {{the risk of}} working with collaborative robots {{will have to be}} revised.|$|E
50|$|Air-Cobot (Aircraft Inspection {{enhanced}} by smaRt & Collaborative rOBOT) is a French {{research and development}} project of a wheeled collaborative mobile robot able to inspect aircraft during maintenance operations. This multi-partner project involves research laboratories and industry. Research around this prototype was developed in three domains: autonomous navigation, nondestructive testing and <b>human-robot</b> <b>collaboration.</b>|$|E
50|$|Comau {{innovation}} {{focuses on}} {{easy to use}} solutions, value-added manufacturing and <b>human-robot</b> <b>collaboration.</b> Driven by the concepts of co-creation and open-innovation, the company has collaborated for over 40 years with universities, organizations and institutions to generate disruptive innovations to products and solutions. Comau employs more than 9,000 people in 17 countries - Italy, Russia, China, India, Thailand, Turkey, Argentina, Mexico, Brazil, Romania, Poland, Czech Republic, Germany, France, Spain, the United States and the United Kingdom - and has more than 600 registered patents.|$|E
40|$|This paper {{identifies}} key {{legal issues}} which are emerging for Mobile Servant Robots (MSRs), a sub-type of Personal Care Robots (PCR) defined in ISO 13482. New cases {{are likely to}} be introduced in the market soon even though appropriate and specific binding legal regulations regarding MSRs are missing and several questions need to be carefully considered. The main issues of concern are the need for a concrete and holistic definition of MSR, clarification on the confusion among new emerging ISO/IEC robot categories (especially between boundaries and gaps in machinery with medical device regulations), unclear liability scenarios (avoiding harm, prospective liability, butterfly effect), defining and regulating <b>human-robot</b> <b>collaborations</b> and relationships, ethical issues (mass surveillance, post-monitoring personal data), autonomy (from the robot but also from the user perspective), isolation scenarios, etc. Despite the recent technical advances, there {{is still a long way}} ahead and further research is needed to overcome a variety of associated legal and ethical issues which are emergin...|$|R
40|$|Abstract — This {{short summary}} paper briefly {{describes}} {{a method for}} using the embodied social communication capabilities of a robot to achieve and enhance coordination in <b>human-robot</b> task <b>collaboration</b> scenarios. The approach focuses on planning coordinating social behaviors using the formalism of roles to allow a robot to produce and interpret communicative feedback expressing a desired allocation of duties and to issue positive or negative reinforcement to a person as the task progresses {{and in response to}} the inferred future activity of the collaborating partner...|$|R
40|$|Abstract—In {{this paper}} we {{consider}} {{a method of}} increasing team fluency in co-located <b>human-robot</b> task <b>collaboration.</b> In such scenarios, the robot can use explicit communication actions to convey internal state and intention, thus providing the collaborating partner with more information than s/he could glean from observation of a robot that does not communicate. We discuss the fundamental requirements of effective coordinating communication as derived from humanhuman collaboration tendencies. A preliminary approach, based on probabilistic modeling of intentional behavior for production of multimodal communication collaborative tasks, is presented {{with the aim of}} reducing idle time and increasing team situational awareness. I...|$|R
50|$|Comau (COnsorzio MAcchine Utensili) is an Italian {{multinational}} {{company based in}} Turin, Italy and {{is part of the}} FCA Group.  Comau is an integrated company specialized in industrial automation with an international network of 35 operative centers, 15 manufacturing plants and 5 innovation centers worldwide.  The company offers complete and comprehensive solutions, services, products and technologies to meet specific manufacturing needs for industries ranging from automotive, railway and heavy industrial to renewable energy and general industry.  Its automated manufacturing systems integrate products, processes and services, with competencies ranging from metal cutting to complete, robotized manufacturing systems. All of which is supported by an advanced 3D Development environment, and a strong focus on innovation based upon three main pillars: easy to use solutions, value-added manufacturing and <b>human-robot</b> <b>collaboration.</b>|$|E
5000|$|At the Media Lab, Breazeal {{continues}} to work on social interaction and socially situated learning between people and robots. Leonardo is another globally recognized robot (co-developed with Stan Winston Studio) that was developed as a successor to Kismet (recognized in 2006 by Wired Magazine {{as one of the}} [...] "50 Best Robots Ever"). Leonardo was also used to investigate social cognition and Theory of Mind abilities on robots with application to <b>human-robot</b> <b>collaboration,</b> in addition to developing social learning abilities for robots such as imitation, tutelage, and social referencing. Nexi is the most recent robot in this tradition (awarded a TIME Magazine 50 Best Inventions of 2008). Nexi is a MDS robot (Mobile, Dexterous, Social) that combines rich social communication abilities with mobile dexterity to investigate more complex forms of human-robot teaming.|$|E
5000|$|A common {{approach}} to program social cues into robots is to first study human-human behaviors and then transfer the learning. For example, co-ordination mechanisms in <b>human-robot</b> <b>collaboration</b> [...] {{are based on}} the seminal work in neuroscience which looked at how to enable joint action in human-human configuration by studying perception and action in social context rather than in isolation. These studies have revealed that maintaining a shared representation of the task is crucial for accomplishing tasks in groups. For example, the authors have examined the task of driving together by separating responsibilities of acceleration and braking i.e, one person is responsible for accelerating and the other for braking, the study revealed that pairs reached the same level of performance as individuals only when they received feedback about the timing of each other’s actions. Similarly, researchers have studied the aspect of human-human handovers with household scenarios like passing dining plates in order to enable an adaptive control of the same in human-robot handovers. Most recently, researchers have studied a system that automatically distributes assembly tasks among co-located workers to improve co-ordination.|$|E
40|$|Abstract — In this {{extended}} abstract {{we discuss}} how social communication during <b>human-robot</b> task <b>collaboration</b> {{can be used}} to improve user task performance and guide decision-making. Our approach integrates recognition of a person’s intentional role-driven behavior with planning of the robot’s verbal feedback using a Markov decision process representation of the task environment. Using this type of model allows for the robot to track user performance on a task overtime, provide contextual verbal feedback, and adjust the presentation of feedback overtime according to observed changes in the user’s activity. The approach will be evaluated in an assistive meal preparation task with older adults, in which an autonomous robot provides verbal guidance in addition to directly assisting the user by performing parts of the task. I...|$|R
40|$|We detail an {{approach}} to planning effective verbal feedback during pairwise <b>human-robot</b> task <b>collaboration.</b> The ap-proach is motivated by social science literature as well as existing work in robotics and is applicable {{to a variety of}} task scenarios. It consists of a dynamic, synthetic task im-plemented in an augmented reality environment. The result is combined robot task control and speech production, al-lowing the robot to actively participate and communicate with its teammate. A user study was conducted to experi-mentally validate the efficacy of the approach on a task in which a single user collaborates with an autonomous robot. The results demonstrate that the approach is capable of im-proving both objective measures of team performance and the user’s subjective evaluation of both the task and the robot as a teammate...|$|R
40|$|Human-like {{movement}} is fundamental for natural <b>human-robot</b> interaction and <b>collaboration.</b> We have {{developed in a}} model for generating arm and hand movements an anthropomorphic robot. This model {{was inspired by the}} Posture-Based Motion-Planning Model of human reaching and grasping movements. In this paper we present some changes to the model we have proposed in [4] and test and compare different nonlinear constrained optimization techniques for solving the large-scale nonlinear constrained optimization problem that rises from the discretization of our time-continuous model. Furthermore, we test different time discretization steps. FCT-FCOMP- 01 - 0124 -FEDER- 022674...|$|R
50|$|Leonardo is a 2.5 foot social robot, {{the first}} {{created by the}} Personal Robots Group of the Massachusetts Institute of Technology. Its {{development}} is credited to Cynthia Breazeal. The body is by Stan Winston Studios, leaders in animatronics. Its body was completed in 2002. It was the most complex robot the studio had ever attempted as of 2001. Other contributors to the project include NevenVision, Inc., Toyota, NASA’s Lyndon B. Johnson Space Center, and the Navy Research Lab. It was created to facilitate the study of human-robot interaction and collaboration. A DARPA Mobile Autonomous Robot Software (MARS) grant, Office of Naval Research Young Investigators Program grant, Digital Life, and Things that Think consortia have partially funded the project. The MIT Media Lab Robotic Life Group, who also studied Robonaut 1, set out {{to create a more}} sophisticated social-robot in Leonardo. They gave Leonardo a different visual tracking system and programs based on infant psychology that they hope will make for better <b>human-robot</b> <b>collaboration.</b> One of the goals of the project was {{to make it possible for}} untrained humans to interact with and teach the robot much more quickly with fewer repetitions. Leonardo was awarded a spot in Wired Magazine’s 50 Best Robots Ever list in 2006.|$|E
40|$|While <b>human-robot</b> <b>collaboration</b> {{has been}} studied intensively in the literature, little {{attention}} has been given to understanding the role of collaborative endeavours on enhancing the companionship between humans and robots. In this position paper, we explore the possibilities of building the human-robot companionship through collaborative activities. The design guideline of a companion robot Nancy developed at SRL is introduced, and preliminary studies on <b>human-robot</b> <b>collaboration</b> conducted at SRL and I 2 R are elaborated. Critical issues and technical challenges in <b>human-robot</b> <b>collaboration</b> systems are discussed. Through these discussions, we aim to draw the attention of the social robotics community to the importance of <b>human-robot</b> <b>collaboration</b> in companionship building, and stimulate more research effort in this emerging area...|$|E
40|$|Future {{factories}} in digitized industries will require highly versatile automation systems. Seamless <b>human-robot</b> <b>collaboration,</b> utilizing {{the strengths of}} both, combined with advanced machine perception and automated planning are key factors for success {{in a world of}} “mass customized” products and increasingly faster product changes. Two examples, in welding and assembly, show advanced planning, sensing and <b>human-robot</b> <b>collaboration</b> technologies and discuss their benefits...|$|E
40|$|In this study, {{we sought}} to clarify the effects of users' {{cultural}} background and cultural context on <b>human-robot</b> team <b>collaboration</b> by investigating attitudes toward {{and the extent to}} which people changed their decisions based on the recommendations of a robot collaborator. We report the results of a 2 × 2 experiment with nationality (Chinese vs. US) and communication style (implicit vs. explicit) as dimensions. The results confirm expectations that when robots behave in more culturally normative ways, subjects are more likely to heed their recommendations. Specifically, subjects with a Chinese vs. a US cultural background changed their decisions more when collaborating with robots that communicated implicitly vs. explicitly. We also found evidence that Chinese subjects were more negative in their attitude to robots and, as a result, relied less on the robot's advice. These findings suggest that cultural values affect responses to robots in collaborative situations and reinforce the importance of culturally sensitive design in HRI...|$|R
40|$|Tight <b>human-robot</b> {{interaction}} and <b>collaboration</b> will characterize future robot tasks. Robot working environments {{will be increasingly}} unstructured, as safety barriers will be removed to allow a continuous cooperation of robotic and human workers. Such a working scenario calls for novel safety systems capable of combining productivity with workers’ safety. In this paper, a method for {{the definition of a}} task-consistent collision avoidance safety strategy is presented. A classification of task constraints based on relevance for task completion is introduced. Control of task constraints enforcement is performed through a state machine. A template for such state machine is proposed. Experimental validation of the proposed safety system on a dual-arm industrial robot prototype is presented...|$|R
40|$|A {{widespread}} and flexible use of robots in rapidly changing working environments could be greatly enhanced by <b>human-robot</b> interaction and <b>collaboration.</b> Humans and robots have complementary skills. The robotic worker can relieve the human from repetitive work, {{and the human}} can make robot deployment easier by managing nonstandard or particularly skilful operations. Such a scenario, however, requires new safety systems to preserve human workers from potential danger {{and at the same}} time to make human-robot interaction productive and advantageous. In this paper, a system for safe and task consistent human-robot interaction integrated with an industrial controller is proposed. The robot executes evasive motions to avoid impacts with obstacles consistently with the task. A classification of constraints constituting the task is proposed and a safety strategy based on such classification is defined. This paper finally presents integration of the safety system with an industrial controller and experimental validation on an assembly operation...|$|R
40|$|Abstract—Combining the {{intelligent}} and situation dependent decision making capabilities {{of a human}} with the accuracy and power of a robot, performance of many tasks can be improved. The <b>human-robot</b> <b>collaboration</b> scenarios are increasing. Human-robot interaction is not only restricted to the humanoid robots interacting with the humans or to the mobile service robots providing different services but also industrial robots opens {{a wide range of}} <b>human-robot</b> <b>collaboration</b> set-ups. Intention recognition {{plays a key role in}} intuitive <b>human-robot</b> <b>collaboration.</b> In this paper we present a novel approach for recognizing the human intention using weighted probabilistic state machines. We categorize the recognition task into two categories namely explicit and implicit intention communication. We present a general intention recognition approach that can be applied to any human-robot cooperation situation. The algorithm is tested with an industrial robotic arm...|$|E
30|$|Li-Hui Wang, is a {{professor}} at KTH Royal Institute of Technology, Sweden. He focuses on process planning, cloud manufacturing, and <b>human-robot</b> <b>collaboration.</b>|$|E
40|$|International audienceIn <b>human-robot</b> <b>collaboration</b> the robot's {{behavior}} impacts the worker's safety, {{comfort and}} acceptance of the robotic system. In this paper we {{address the problem of}} how to improve the worker's posture during <b>human-robot</b> <b>collaboration.</b> Using postural assessment techniques, and a personalized human kinematic model, we optimize the model body posture to fulfill a task while avoiding uncomfortable or unsafe postures. We then derive a robotic behavior that leads the worker towards that improved posture. We validate our approach in an experiment involving a joint task with 39 human subjects and a Baxter torso-humanoid robot...|$|E
40|$|Developing {{behaviours}} for {{interaction with}} objects {{close to the}} body is a primary goal for any organism {{to survive in the}} world. Being able to develop such behaviours will be an essential feature in autonomous humanoid robots in order to improve their integration into human environments. Adaptable spatial abilities will make robots safer and improve their social skills, <b>human-robot</b> and robot-robot <b>collaboration</b> abilities. This work investigated how a humanoid robot can explore and create action-based representations of its peripersonal space, the region immediately surrounding the body where reaching is possible without location displacement. It presents three empirical studies based on peripersonal space findings from psychology, neuroscience and robotics. The experiments used a visual perception system based on active-vision and biologically inspired neural networks. The first study investigated the contribution of binocular vision in a reaching task. Results indicated the signal from vergence is a useful embodied depth estimation cue in the peripersonal space in humanoid robots. The second study explored the influence of morphology and postural experience on confidence levels in reaching assessment. Results showed that a decrease of confidence when assessing targets located farther from the body, possibly in accordance to errors in depth estimation from vergence for longer distances. Additionally, it was found that a proprioceptive arm-length signal extends the robot’s peripersonal space. The last experiment modelled development of the reaching skill by implementing motor synergies that progressively unlock degrees of freedom in the arm. The model was advantageous when compared to one that included no developmental stages. The contribution to knowledge of this work is extending the research on biologically-inspired methods for building robots, presenting new ways to further investigate the robotic properties involved in the dynamical adaptation to body and sensing characteristics, vision-based action, morphology and confidence levels in reaching assessment. CONACyT, Mexico (National Council of Science and Technology...|$|R
40|$|Abstract: In this article, after {{surveying}} the problems incurred by the conventional technology-centered automation {{in a variety}} of fields, we put an emphasis on the fact that a concept of “sociality ” is really needed to form the ideal human-automation and/or human-robot relations emerging out of active interactions. Then, some key technical issues are to be presented for realizing such socially-centered automation and establishing the human-automation and/or <b>human-robot</b> <b>collaboration.</b> Brief introductions of the ongoing works by the author’s group are also presented. Keywords: Human-centered automation, <b>human-robot</b> <b>collaboration,</b> sociality, ecological interface design. 1...|$|E
40|$|NASA?s {{vision for}} space {{exploration}} stresses {{the cultivation of}} human-robotic systems. Similar systems are also envisaged {{for a variety of}} hazardous earthbound applications such as urban search and rescue. Recent research has pointed out that to reduce human workload, costs, fatigue driven error and risk, intelligent robotic systems will need to be a significant part of mission design. However, little {{attention has been paid to}} joint human-robot teams. Making <b>human-robot</b> <b>collaboration</b> natural and efficient is crucial. In particular, grounding, situational awareness, a common frame of reference and spatial referencing are vital in effective communication and collaboration. Augmented Reality (AR), the overlaying of computer graphics onto the real worldview, can provide the necessary means for a human-robotic system to fulfill these requirements for effective collaboration. This article reviews the field of human-robot interaction and augmented reality, investigates the potential avenues for creating natural <b>human-robot</b> <b>collaboration</b> through spatial dialogue utilizing AR and proposes a holistic architectural design for <b>human-robot</b> <b>collaboration...</b>|$|E
40|$|The {{production}} {{industry is}} moving towards {{the next generation}} of assembly, which is conducted based on safe and reliable robots working in the same workplace alongside with humans. Focusing on assembly tasks, this paper presents a review of <b>human-robot</b> <b>collaboration</b> research and its classification works. Aside from defining key terms and relations, the paper also proposes means of describing <b>human-robot</b> <b>collaboration</b> that can be relied on during detailed elaboration of solutions. A human-robot collaborative assembly system is developed with a novel and comprehensive structure, and a case study is presented to validate the proposed framework. © 2017...|$|E
40|$|With {{the advent}} of {{collaborative}} robots, {{there is a great}} potential to improve work performance by <b>human-robot</b> <b>collaboration</b> in engineering tasks. Construction is no exception. Many construction tasks are based on the movement of objects (e. g., material), which are viable candidates for <b>human-robot</b> <b>collaboration.</b> However, due to the physically imposing nature of robot operations and the unstructured environments typical in construction, it is crucial to provide a safe and reliable environment for human workers when performing collaborative work with robots. In this paper, we use Immersive Virtual Environments (IVEs) to evaluate a human response to robots (e. g. perceived safety, trust, and team identification) while performing collaborative construction tasks with robots. By adopting IVEs, various types of robots, interactions, and tasks can be easily tested and evaluated to determine the best HRC practice, without the need to build and evaluate a physical prototype. Several experimental scenarios simulating collaborative masonry tasks were implemented using the Unity 3 D Game Engine and an Oculus Rift 3 D Head-Mounted Display (HMD). The results demonstrate {{that it is important to}} take into account work environment of <b>human-robot</b> <b>collaboration</b> in order to understand how humans perceive robots when working with them...|$|E
40|$|Although {{robotics}} is {{well established}} as a research field, there has been relatively little work on <b>human-robot</b> <b>collaboration.</b> This type of collaboration {{is going to become}} an increasingly important issue as robots work ever more closely with humans. Clearly, there is a growing need for research on <b>human-robot</b> <b>collaboration</b> and communication between humans and robotic systems. Research into human-human communication {{can be used as a}} starting point in developing a robust <b>human-robot</b> <b>collaboration</b> system. Previous research into collaborative efforts with humans has shown that grounding, situational awareness, a common frame of reference and spatial referencing are vital in effective communication. Therefore, these items comprise a list of required attributes of an effective human-robot collaborative system. Augmented Reality (AR) is a technology for overlaying three-dimensional virtual graphics onto the user's view of the real world. It also allows for real time interaction with these virtual graphics, enabling a user to reach into the augmented world and manipulate it directly. The internal state of a robot and its intended actions can be displayed through the virtual imagery in the AR environment. Therefore, AR can bridge the divide between human and robotic systems and enable effective <b>human-robot</b> <b>collaboration.</b> This thesis describes the work involved in developing the Augmented Reality <b>Human-Robot</b> <b>Collaboration</b> (AR-HRC) System. It first garners design criteria for the system from a review of communication and collaboration in human-human interaction, the current state of Human-Robot Interaction (HRI) and related work in AR. A review of research in multimodal interfaces is then provided highlighting the benefits of using such an interface design. Therefore, an AR multimodal interface was developed to determine if this type of design improved performance over a single modality design. Indeed, the multimodal interface was found to improve performance, thereby providing the impetus to use a multimodal design approach for the AR-HRC system. The architectural design of the system is then presented. A user study conducted to determine what kind of interaction people would use when collaborating with a mobile robot is discussed and then the integration of a mobile robot is described. Finally, an evaluation of the AR-HRC system is presented...|$|E
40|$|Students {{and faculty}} at the Naval Postgraduate School (NPS) are working with NASA {{scientists}} 60 feet beneath the ocean's surface at Florida International University's Aquarius Habitat on a novel effort that seeks to fundamentally transform the possibilities of <b>human-robot</b> <b>collaboration.</b> [URL] (Duration: 0 : 08...|$|E
40|$|AbstractIn today's {{manufacturing}} environment, safe <b>human-robot</b> <b>collaboration</b> is {{of paramount}} importance, to improve efficiency and flexibility. Targeting the safety issue, {{this paper presents}} an approach for <b>human-robot</b> <b>collaboration</b> in a shared workplace in close proximity, where real data driven 3 D model of a robot and multiple depth images of the workplace are used for monitoring and decision-making to perform a task. The strategy for robot control depends on the current task and {{the information about the}} operator's presence and position. A case study of assembly is carried out in a robotic assembly cell with human collaboration. The results show that this approach can be applied in real-world applications such as human-robot collaborative assembly with human operators safeguarded at all time...|$|E
40|$|In {{order to}} improve <b>human-robot</b> <b>collaboration,</b> it is {{necessary}} to consider how robots may be able to act {{in a way that is}} understandable to the people with whom they are working. This paper presents a preliminary experimental <b>human-robot</b> <b>collaboration</b> study with 10 human subjects. The paper analyzes the effect of a robot’s emotionally expressive non-verbal behavior on human-robot teamwork. The study was modeled and performed in the immersive simulator SIGVerse. The findings of the study reveal that embodied emotional expressiveness improves the integration of human-robot activity. The results of the study show that embodied expressiveness increases the duration of the activities that have a positive value for the collaborative task. The embodied expressiveness also has a significant influence on the distance between human-robot collaborators...|$|E
