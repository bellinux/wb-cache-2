19|2070|Public
5000|$|Offender <b>Data</b> <b>Screen</b> from North Carolina Department of Correction ...|$|E
5000|$|Offender <b>Data</b> <b>Screen.</b> North Carolina Department of Correction. Retrieved on 2007-11-13.|$|E
5000|$|... #Caption: A vacuum {{shearography}} {{hood and}} data display unit {{is applied to}} a composite structure to check for defects. An indication of a possible defect appears as the ripple pattern on the bottom right of the <b>data</b> <b>screen.</b>|$|E
40|$|A median (L 1 -norm) {{filtering}} program using polynomials was developed. This {{program was}} used in automatic recycling <b>data</b> <b>screening.</b> Additionally, a special adaptive program to work with asymmetric distributions was developed. Examples of adaptive median filtering of satellite laser range observations and TV satellite time measurements are given. The program proved to be versatile and time saving in <b>data</b> <b>screening</b> of time series measurements...|$|R
40|$|Abstract—To early {{detect and}} defend the threats in the Internet caused by botnet, darknet {{monitoring}} {{is very important to}} understand various botnet activities. However, common illegal accesses by ordinary malwares makes such detection difficult. In this paper, in order to remove such accesses by ordinary malwares from the results of network monitoring, we propose a <b>data</b> <b>screening</b> method based on finding frequent sequential patterns which appear in given traffic data. Besides, we apply our method to traffic data observed in darknet and report the results. Index Terms—incident detection, frequent pattern mining, sequential pattern, <b>data</b> <b>screening,</b> darknet monitoring. I...|$|R
40|$|AbstractThe {{efficiency}} of an optimization method for acoustic emission/microseismic (AE/MS) source location {{is determined by}} the compatibility of its error definition with the errors contained in the input data. This compatibility can be examined in terms of the distribution of station residuals. For an ideal distribution, the input error is held at the station where it takes place as the station residual and the error is not permitted to spread to other stations. A comparison study of two optimization methods, namely the least squares method and the absolute value method, shows that the distribution with this character constrains the input errors and minimizes their impact, which explains the much more robust performance by the absolute value method in dealing with large and isolated input errors. When the errors in the input data are systematic and/or extreme in that the basic data structure is altered by these errors, none of the optimization methods are able to function. The only means to resolve this problem is the early detection and correction of these errors through a <b>data</b> <b>screening</b> process. An efficient <b>data</b> <b>screening</b> process is of primary importance for AE/MS source location. In addition to its critical role in dealing with those systematic and extreme errors, <b>data</b> <b>screening</b> creates a favorable environment for applying optimization methods...|$|R
50|$|Screen Procedures: Used {{for data}} entry and updating, can be {{accessed}} from a menu and is navigatable. A Procedure of this type consists of the Menu selection and any key screens and <b>data</b> <b>screen.</b> These are generated and compiled together. You can look upon the screen procedure as the MAIN procedure which is called MAIN by default in CorVision.|$|E
50|$|Bloomberg {{was well}} known for using a datascreen format that {{occupied}} most of the television screen and the camera shots. Until 1998, Bloomberg {{did not have a}} moving ticker. Instead, it had boxes that were dedicated to world news, as well as weather conditions in selected cities, in addition to market data which was confined {{to the bottom of the}} screen. This changed gradually to focus more on business news. The <b>data</b> <b>screen</b> was reformatted several times to include a moving stock ticker and accommodate new graphics.|$|E
40|$|A {{computerized}} {{medical information}} system {{was designed for}} the Division of Rheumatology, University of Connecticut School of Medicine. Several issues pertaining to systems analysis, data collection instruments, database schema and database software {{will be discussed in}} relation to the design process. The proposed system contains a hierarchical database structure and allows for networking, <b>data</b> <b>screen</b> forms entry, and multi-level security control...|$|E
5000|$|Configuration changes survive {{upgrades}} to {{new software}} versions. Some customizations (e.g., code that uses pre-defined [...] "hooks" [...] that are called before/after displaying <b>data</b> <b>screens)</b> survive upgrades, though they require retesting. Other customizations (e.g., those involving changes to fundamental data structures) are overwritten during upgrades {{and must be}} re-implemented.|$|R
40|$|A common {{feature in}} {{large-scale}} scientific studies is that signals are sparse {{and it is}} desirable to significantly narrow down the focus to a much smaller subset in a sequential manner. In this paper, we consider two related <b>data</b> <b>screening</b> problems: One {{is to find the}} smallest subset such that it virtually contains all signals and another is to find the largest subset such that it essentially contains only signals. These screening problems are closely connected to but distinct from the more conventional signal detection or multiple testing problems. We develop data-driven screening procedures which control the error rates with near optimality properties and study how to design the experiments efficiently to achieve the goals in <b>data</b> <b>screening.</b> A class of new phase diagrams is developed to characterize the fundamental limitations in simultaneous inference. An application to multistage high-throughput studies is given to illustrate the merits of the proposed screening methods...|$|R
40|$|The {{business}} data {{could be considered}} as the raw material for decision-making process, {{for the development of}} corporate strategies and overall running of the business. Therefore, adequate attention should be paid to quality of the data. The main goal of the diploma thesis is elaboration of a specific framework for data quality assurance, which combines three theoretical concepts: time series analysis, <b>data</b> <b>screening</b> and <b>data</b> profiling [...] business-specific data profiles are monitored by <b>data</b> <b>screening</b> during the <b>data</b> warehouse ETL (extract, transform and load) process and results are afterwards compared with the values predicted by time series analysis. Achievement of this goal is based on the analysis of "data quality" in literature, exact problem definition and selection of appropriate means for its solution. Moreover, the thesis is analysing alternative solutions available on the market and comparing their functionality with the functionality of own framework, as well...|$|R
40|$|Access to data-dictionary {{relations}} and attributes made more convenient. Data Dictionary Editor (DDE) application program provides more convenient read/write access to data-dictionary table ("descriptions table") via <b>data</b> <b>screen</b> using SMARTQUERY function keys. Provides three main advantages: (1) User works with table names and field names {{rather than with}} table numbers and field numbers, (2) Provides online access to definitions of data-dictionary keys, and (3) Provides displayed summary list that shows, for each datum, which data-dictionary entries currently exist for any specific relation or attribute. Computer program developed to give developers of data bases more convenient access to the OMNIBASE VAX/IDM data-dictionary {{relations and}} attributes...|$|E
40|$|One of {{the biggest}} {{challenges}} in exploratory visualization is to develop techniques that are effective for analyzing large-scale data and information sets. Common definitions of large scale include ■ data sets with tens or hundreds of millions of records, ■ data records with thousands of dimensions, ■ grids with hundreds of millions of cells, ■ graphs with millions of links and nodes, and ■ time series with hundreds of thousands of time steps. Bottlenecks in analyzing such data include the speed at which a processor can receive and display <b>data,</b> <b>screen</b> space limitations, and human capabilities to interpret complex and cluttered displays. Several strategies have evolved for addressing these bottlenecks. Sampling 1 and filtering can extract subsets of the data; clustering an...|$|E
40|$|Corpora and corpus-based {{resources}} have received much attention {{with regard to}} translator training, terminology, and specialized resource development. With a specialized monolingual corpus and a specialized online dictionary, the DiCoEnviro, we sought to {{provide insight into the}} usability and effectiveness of both types of resources in improving translation students’ comprehension and usage of specialized terminology in the field of the environment. We assessed a specialized corpus and the DiCoEnviro through three lenses adapted from the usability framework proposed by Nielsen (2001) : effectiveness, efficiency, and satisfaction. We used <b>data</b> (<b>screen</b> recordings, questionnaires, translation exercises) collected from six translation students enrolled in undergraduate and graduate programs at the University of Ottawa School of Translation and Interpretation (UO-STI). Through quantitative and qualitative data analysis, we provide insight into the usability of both types of resources and into the prospective application of these findings in translator training programs and the development of specialized resources...|$|E
40|$|Quality {{controlled}} energy-use data is {{the foundation}} of energy performance evaluation for a building. The ?Energy Balance Load? (EBL), a parameter derived from the first law of thermodynamics based on a whole-building energy analysis, has been theoretically proved to be an effective tool for verifying whole-building energy-use data (Shao and Claridge, 2006). Quality control methodology using EBL has been proposed and applied to more than one hundred buildings on a large university campus by Baltazar et al. (2007). They picked the outside air dry-bulb temperature (TOA) as the explanatory variable of EBL, and used a plot of EBL versus TOA, called energy balance plot, to find faulty behavior in the data by visually observing the pattern. It has been demonstrated that this methodology can detect significant data problems caused by variety of reasons such as scale factor error and mislabeled meter successfully. This paper presents a possible enhancement on the existent EBL analysis technique by using the outside air enthalpy (hOA) as the explanatory variable of EBL instead of TOA. This enthalpy based analysis accounts for the effect of latent load on EBL, and therefore, may enhance the <b>data</b> <b>screening</b> capability for buildings operated at locations with hot and humid climate. Numerical threshold of <b>data</b> <b>screening</b> proposed by Masuda et al. (2008) has been applied to this enthalpy based methodology to determine the difference in the results of <b>data</b> <b>screening</b> between enthalpy based analysis and temperature based analysis...|$|R
30|$|In conclusion, we {{increased}} CBL {{production by}} S. cerevisiae through expressing selected genes of T. cacao potentially involved in CB biosynthesis, {{which might be}} used in yeast CBL production in future. Additionally, our approach of integrating plant genome <b>data</b> <b>screening</b> and metabolic engineering may also find application in production of other value-added plant metabolites using S. cerevisiae as a cell factory.|$|R
40|$|Given the {{important}} role of lake ecosystems in social and economic development, and the current severe environmental degradation in China, a systematic diagnosis of the ecological security of lakes is essential for sustainable development. A Driving-force, Pressure, Status, Impact, and Risk (DPSIR) model, combined with <b>data</b> <b>screening</b> for lake ecological security assessment was developed to overcome the disadvantages of data selection in existing assessment methods. Correlation and principal component analysis were used to select independent and representative data. The DPSIR model was then applied to evaluate the ecological security of Dianchi Lake in China during 1988 - 2007 using an ecological security index. The results revealed a V-shaped trend. The application of the DPSIR model with <b>data</b> <b>screening</b> provided useful information regarding the status of the lake’s ecosystem, while ensuring information efficiency and eliminating multicollinearity. The modeling approach described here is practical and operationally efficient, and provides an attractive alternative approach to assess the ecological security of lakes...|$|R
40|$|In {{order to}} {{illustrate}} {{the nature of the}} diurnal temperature variations in the atmospheric surface layer in all seasons a set of hourly observations at the Zagreb-Maksimir Observatory (Croatia), measured at three different levels (5 cm, 50 cm and 2 m above ground) during the year 2005, was used. An approximate method for calculating air temperature at 5 cm, using the air temperature at 2 m, is presented. For this purpose, hourly <b>data</b> (<b>screen</b> height temperature, cloudiness, air pressure at barometer level and wind speed at 2 m) collected at the Zagreb-Maksimir Observatory during the summer season of 2005 have been used. Th is method is based on the Monin-Obukhov similarity theory. Estimated values have been compared with observations. The results obtained are the most accurate for cloudy weather, and the least accurate in the case of clear sky. A systematic error of this approach was discovered using a clustering procedure and is briefly discussed...|$|E
40|$|Pyraflufen-ethyl is an {{herbicide}} used {{to control}} broadleaf weeds for several crops (cotton, potatoes, corn, soybeans, and wheat). It also has several non-dietary uses, including use at airports, roadsides, railroads, nurseries, golf courses, and on ornamental turf. In 2009, 462 pounds of pyraflufen-ethyl were used on cotton fields in California; 728 pounds were used on all crops combined. Occupational exposure may occur {{as a result of}} application and use of the herbicide. Exposure to the general population may occur as a result of residential or recreational exposure to areas treated with the herbicide or consumption of residues in foods. Pyraflufen-ethyl passed the animal <b>data</b> <b>screen,</b> underwent preliminary toxicological evaluation, and is being brought to the Carcinogen Identification Committee for consultation. This is a compilation of the relevant studies identified during the preliminary toxicological evaluation. Epidemiological data No cancer epidemiology studies were identified. Animal carcinogenicity data • Long-term studies in rats o 104 -week diet studies in male and female CR:CD rats: U. S. EPA (2002) No treatment-related tumor findings in males or females • Long-term studies in mic...|$|E
40|$|A {{number of}} triazoles are broad {{spectrum}} antifungal agents used as pesticides and pharmaceuticals. They inhibit the biosynthesis of ergosterol, {{which is an}} essential component of fungal membranes. Triazole antifungal agents are extensively used. The general population can be exposed {{as a result of the}} use of triazole pharmaceuticals and through consumption of food or water containing triazole pesticide residues. Occupational exposure may occur to workers involved in the manufacture or use of triazole antifungal agents. Triazole moiety Triazole antifungal agents (as a chemical group) passed the animal <b>data</b> <b>screen,</b> underwent a preliminary toxicological evaluation, and are being brought to the Carcinogen Identification Committee (CIC) for consultation. This is a compilation of the relevant studies identified during the preliminary toxicological evaluation. The CIC is being asked to advise OEHHA on whether triazole antifungal agents as group, or any individual triazoles, should be brought to the committee for a full evaluation of the carcinogenicity evidence at a future meeting. Epidemiological data No cancer epidemiology studies on triazole antifungal agents were identified. Animal carcinogenicity data Tables 1 and 2 present animal carcinogenicity data for several triazole antifungal agents. Triazole antifungal agents are hepatotoxic, with many producing liver tumors in mice. Some produce thyroid and other tumors in rats...|$|E
40|$|This work {{considers}} various {{techniques to}} extract {{information from the}} vast amount of data collected at a modern wastewater treatment plant. If the information extracted is to be considered reliable is highly dependent on the <b>data</b> <b>screening.</b> <b>Data</b> <b>screening</b> includes validation and quality improvement of data. Adequate methods for validation, noise reduction {{and other forms of}} quality improvements of wastewater treatment data are discussed. In order to detect deviations and disturbances, the measurement variables can be investigated individually or many variables simultaneously. Single variable detection involves investigation of the basic signal characteristics such as amplitude, mean and spread. Usable methods are discussed and examples are given. In order to detect synergetic effects, techniques capable of investigating several variables simultaneously, are needed. Multivariate statistics based methods, such as principal component analysis (PCA), principal component regression (PCR) and projection to latent structures (PLS), are considered and their applicability discussed. Some possibilities to adapt the methods to the dynamic situation in a wastewater treatment plant are also outlined...|$|R
40|$|The aim of {{this paper}} is to conduct a {{preliminary}} analysis and <b>Data</b> <b>screening</b> with relation to the effect of Attitude, subjective norm and perceived behavioural control, on the entrepreneurial Intentions of Nigerian Postgraduates. 240 Master and PhD candidates were surveyed from Universiti Utara Malaysia (UUM) and the study utilized the convenience sampling method, which result to 156 respondents. The study was equally conducted to suit the multivariate analysis assumptions. Using the Statistical Package for Social Science (SPSS) software version 20, the univariate and multivariate outliers are checked and treated, the check for missing Data was performed, so also the kurtosis and skewness, factor analysis and the reliability test of the cronbach coefficient alpha. The data was finally ready for the multivariate analysis as it fulfilled the necessary assumptions for that. The findings are therefore important to the study and that of other researchers whom will benefit from the literature to conduct <b>data</b> <b>screening</b> and preliminary analysis...|$|R
40|$|The {{evaluation}} of propulsion system test and flight performance data involves reviewing an extremely {{large volume of}} sensor data generated by each test. An automated system that screens large volumes of data and identifies propulsion system parameters which appear unusual or anomalous will increase the productivity of data analysis. Data analysts may then focus on a smaller subset of anomalous data for further {{evaluation of}} propulsion system tests. Such an automated <b>data</b> <b>screening</b> system would give NASA {{the benefit of a}} reduction in the manpower and time required to complete a propulsion system data evaluation. A phase 1 effort to develop a prototype <b>data</b> <b>screening</b> system is reported. Neural networks will detect anomalies based on nominal propulsion system data only. It appears that a reasonable goal for an operational system would be to screen out 95 pct. of the nominal data, leaving less than 5 pct. needing further analysis by human experts...|$|R
40|$|This paper {{describes}} how use the HTMLEditorKit to perform web data mining on EDGAR (Electronic Data-Gathering, Analysis, and Retrieval system). EDGAR is the SEC’s (U. S. Securities and Exchange Commission) means of automating the collection, validation, indexing, acceptance, and forwarding of submissions. Some entities are {{regulated by the}} SEC (e. g. publicly traded firms) and are required, by law, to file with the SEC. Our focus is on making use of EDGAR to get information about company offers to purchase stock, known as tender offers. These offers are filed with companies, using their Central Index Key (CIK). The CIK is used on the SEC’s computer system to identify corporations and individual people who have filed a disclosure with the SEC. We show how to map a stock ticker symbol into a CIK and how to extract tender offer data. Our example show how we extract the number of shares tendered, the price range for the auction, the honoring of “odd lots ” and the initial termination date for the auction. The methodology for converting the web data source into internal data structures is based on using HTML as input into a parser-call-back facility that builds up a data structure using a context sensitive table <b>data.</b> <b>Screen</b> scraping is a popular means of data entry, but the unstructured nature of HTML pages makes this a challenge...|$|E
40|$|Paragraph 25 {{has been}} amended {{to provide that}} NAP crop acreage is only {{applicable}} to 2012 and prior years and update the definition of State conservation acres and other conservation acres. Subparagraph 26 A has been amended to update the classification of land. Subparagraph 105 C has been amended to update the FedEx information. Subparagraphs 112 D and E have been amended to update the contact phone number. Paragraph 130 has been amended to update the Farm <b>Data</b> <b>Screen</b> and Acre-Yr references, and add that NAP crop acreage is applicable to 2012 and prior years. Subparagraph 131 C has been amended to update operator CW exceptions. Paragraph 152 has been amended to add that NAP crop acres will only be displayed for 2012 and prior years and update instructions for farmland and cropland entry. Subparagraph 154 C has been amended to update tract producer CW exceptions. Subparagraph 155 C has been amended to update contact information. Paragraph 169 has been amended to add references for CRP Redn and CRP Pending that CRP pending and reduction acres will only be displayed for 2012 and prior years. Paragraph 171 has been amended to remove reference to farms with active CRP- 15 ’s. Subparagraph 173 D has been amended to remove references to DCP pending acres. Paragraph 191 has been amended to add references for CRP Pending, CRP Redn, and CRP Yield will only be displayed for 2012 and prior years...|$|E
40|$|Abstract Background To {{evaluate}} institutional {{nursing care}} {{performance in the}} context of national comparative statistics (benchmarks), approximately one in every three major healthcare institutions (over 1, 800 hospitals) across the United States, have joined the National Database for Nursing Quality Indicators® (NDNQI®). With over 18, 000 hospital units contributing data for nearly 200 quantitative measures at present, a reliable and efficient input data screening for all quantitative measures for data quality control is critical to the integrity, validity, and on-time delivery of NDNQI reports. Methods With Monte Carlo simulation and quantitative NDNQI indicator examples, we compared two ad-hoc methods using robust scale estimators, Inter Quartile Range (IQR) and Median Absolute Deviation from the Median (MAD), to the classic, theoretically-based Minimum Covariance Determinant (FAST-MCD) approach, for initial univariate outlier detection. Results While the theoretically based FAST-MCD used in one dimension can be sensitive and is better suited for identifying groups of outliers because of its high breakdown point, the ad-hoc IQR and MAD approaches are fast, easy to implement, and could be more robust and efficient, depending on the distributional property of the underlying measure of interest. Conclusion With highly skewed distributions for most NDNQI indicators within a short <b>data</b> <b>screen</b> window, the FAST-MCD approach, when used in one dimensional raw data setting, could overestimate the false alarm rates for potential outliers than the IQR and MAD with the same pre-set of critical value, thus, overburden data quality control at both the data entry and administrative ends in our setting. </p...|$|E
40|$|Energy {{management}} {{processes such as}} accounting energy cost, finding overuse in energy, {{and determining}} savings from energy conservation programs largely depends on measured energy use data. Identifying and correcting faulty data properly would avoid over/underestimation in energy use and increase accuracy in further analysis. It allows engineers and administrators to make more confident and low-risk decisions. This paper proposes a methodology to construct statistical control limits for <b>data</b> <b>screening</b> using ?Energy Balance? methodology (Shao and Claridge, 2005). Energy Balance (EBL) parameter represents quasi-steady state thermal energy storage in a building and indicates a predominant linear behavior when it is plotted versus the outside air temperature. A regression model of EBL parameter developed {{as a function of}} the outside air temperature from a long-term data can be used as a <b>data</b> <b>screening</b> tool for newly measured energy use in the building. However, EBL model is known to have functional discontinuities called ?change points? and non-uniform residuals. To construct control limits that fit the EBL data uniformly over a wide range of outside air temperature, a new technique was introduced which estimate mean square error (MSE) for a change point model as a function of outside air temperature by using Bin-MSE data. This methodology has successfully constructed data quality control limits of EBL parameter for example buildings. The numerical criteria developed by this methodology would help to have uniform results in quality control for energy use data, and it would be used as a rule for an automated <b>data</b> <b>screening</b> process...|$|R
30|$|In {{order to}} improve the quality of data, we {{selected}} articles and reviews as the research object, which may omit some important research results. Besides, top 100 and top 400 were identified as the analysis indicators, which may not reflect the effect of time factor. In the follow-up study, data sources and <b>data</b> <b>screening</b> criteria will be optimized so as {{to improve the quality}} of research continuously.|$|R
40|$|This paper aims {{to share}} the {{experiences}} of using two empirically-related yet conceptually distinct instruments i. e., Goldberg’s IP-IP and Costa and McCrae’s NEO-PI-R to measure the respondent’s personality factors and facets. This research paper presents findings on six methodological issues i. e., sampling, validity, reliability, <b>data</b> collection, <b>screening</b> confirmatory factor analyses. For validity concerns both instruments reached to obsolescence as these were developed ages ago and not updated with pace of human development. For replication in non-English countries Goldberg’s IP-IP has edge over Costa & McCrae’s NEO-PI-R for three counts; (a) it has less words and relatively simple syntax made it really easy for participant with linguistic barriers; (b) {{shows no sign of}} US cultural specificity in terms of places, slang or implicit meanings made it closer to participants in a non-English speaking country like Pakistan and (c) it takes less than half time to complete the survey. For reliability issues NEO-PI-R represented better results. For both instruments consistency might be compromised while replicating these instruments in non English speaking country. IP-IP does not provide any guideline for <b>data</b> <b>screening</b> while NEO-PI-R provides a very detailed <b>data</b> <b>screening</b> process. During factor analyses it was revealed both instruments replicated five factor model at both levels i. e., first order factor analyses resulting in facets and second order factor analyses provided the factors but during the NEO-PI-R facto...|$|R
40|$|Rock glaciers are creep {{phenomena}} of mountain permafrost and {{are composed of}} ice and rocks. Active rock glaciers move downslope by force of gravity. Maximum creep/flow velocities of individual rock glaciers may vary from a few centimeters up to several meters per year, depending on the underlying terrain, mechanical parameters of the material involved, etc. Inter-annual variation of rock glacier flow has been observed and attributed to various reasons, e. g., climate change. It is believed that the observed warming of the atmosphere accelerates rock glacier flow. This paper proposes a method for detecting active rock glaciers and, where applicable, quantifying their movement relatively or absolutely using multi-temporal image data (i. e., high-resolution orthoimages/orthophotos) of virtual globes, such as Google Maps and Microsoft Bing Maps. The present work was originally triggered by the task of detecting all active rock glaciers of a larger mountain region, i. e., {{the western part of}} the Schober Mountains located in the Austrian Hohe Tauern range. In support of this task the proposed method was additionally applied to two well-studied rock glaciers, i. e., Hinteres Langtalkar (eastern part of the Schober Mountains) and Äußeres Hochebenkar (Ötztal Alps, Austria). In this paper we present the results obtained from the two rock glaciers. It can be summarized that change detection and consequently the high-precision measurement of flow velocities of active rock glaciers using image <b>data</b> (<b>screen</b> shots) of virtual globes (geobrowsers) is possible. It must be admitted, however, that the proposed method has some obvious drawbacks: (a) limited availability of high-resolution image data in high mountain areas, (b) limited availability of multi-temporal image data, (c) lack of information about exact acquisition dates or source of image data, (d) lack of information about the accuracy of the image data (orthophotos), and (e) potential legal obstacles to using the image data...|$|E
40|$|Abstract Background Implementing {{initiatives}} {{to achieve the}} targets of MDG 5 requires sufficient financial resources that are mobilized and utilized in an equitable, efficient and sustainable manner. Informed decision making to this end requires the availability of reliable health financing information. This is accomplished by means of Reproductive Health (RH) sub-account, which captures and organizes expenditure on RH services in two-dimensional tables from financing sources to end users. The specific objectives of this study are: (i) to quantify total expenditure on reproductive health services; and (ii) to examine the flow of RH funds from sources to end users. Methods The RH sub-account {{was part of the}} general National Health Accounts exercise covering the Financial Years 2007 / 08 and 2008 / 09. Primary data were collected from employers, medical aid schemes, donors and government ministries using questionnaire. Secondary data were obtained from various documents of the Namibian Government and the health financing database of the World Health Organization. Data were analyzed using a <b>data</b> <b>screen</b> designed in Microsoft Excel. Results RH expenditure per woman of reproductive age was US$ 148 and US$ 126 in the 2007 / 08 and 2008 / 09 financial years respectively. This is by far higher than what is observed in most African countries. RH expenditure constituted more than 10 - 12 % of the total expenditure on health. Out-of-pocket payment for RH was minimal (less than 4 % of the RH spending in both years). Government is the key source of RH spending. Moreover, the public sector is the main financing agent with programmatic control of RH funds and also the main provider of services. Most of the RH expenditure is spent on services of curative care (both in- and out-patient). The proportion allocated for preventive and public health services was not more than 5 % in the two financial years. Conclusion Namibia's expenditure on reproductive health is remarkable by the standards of Africa and other middle-income countries. However, an increasing maternal mortality ratio does not bode well with the level of reproductive health expenditure. It is therefore important to critically examine the state of efficiency in the allocation and use of reproductive health expenditures in order to improve health outcomes. </p...|$|E
40|$|Background: The {{preventable}} {{risk factors}} for cardiovascular diseases (CVD) and diabetes cluster together and can be tracked from childhood into adulthood. There is evidence that sleep, cardiorespiratory fitness and socioeconomic status (SES) {{are related to the}} component risk factors of cardiometabolic syndrome, but this is mainly in adults. Current interventions to prevent cardiometabolic syndrome, also, are not robust enough and multisectoral approaches are still lacking in preventing these preventable risk factors. The aim of this thesis is to evaluate prevalence, identify some of the causes of these risk factors, their social determinants and how they are associated with another in English schoolchildren. Setting: Field·based research including schoolchildren from the East of England, United Kingdom. Methods: Cross~section of English state schoolchildren between lO~ 16 years participating in the East of England Healthy Hearts study. Height, weight and blood pressure (BP) measured by stadiometer, weighing scales and automated BP monitors using standard procedures respectively. Cardiorespiratory fitness assessed by the 20 m shuttle~run test. Demographic <b>data,</b> <b>screen</b> time, sleep & wake time and physical activity (P A) levels were measured using questionnaires. Schoolchildren level of deprivation was measured using English Index of Multiple Deprivation. Metabolic risk profiles were categorised based on weight status and SBP status. Findings: Overall, 36 % of schoolchildren were exposed to over 2 hours of daily screen time. Those reportiIlg 4 h (OR 2. 26, 95 %CI 1. 91 - 2. 67). Prevalence of elevated mean arterial pressure (MPJl) was 14. 8 % overall but rose to 35. 7 % in those who were both obese and unfit. When compared with schoolchildren engaging in PA daily with adults in their household, schoolchildren who have reported no PA daily with adults in their household were likely (OR 1. 54, 95 %CI 1. 07 - 2. 20) to have an elevated MR profile. Rural children were more active than those from urban areas, but this was not evident when a trilateral division was used. About 40 % of schoolchildren go to bed late at night. Late bedtime was associated with deprivation in schoolchildren. Compared to those with 4 hours of daily screen time (OR 1. 97, 95 %CI 1. 34 - 2. 89). Interpretations: Sedentary behaviour, parental influences, cardiorespiratory fitness, place and location of habitation are associated with the causes of CVD and component factors of cardiometabolic syndrome in schoolchildren. There was a high prevalence of> 2 h daily screen time in English schoolchildren. PA is lower in children reporting 2 - 4 h versus 4 h) independent of deprivation, Increasing cardiorespiratory fitness level may {{have a positive impact on}} the weight-related elevations of MAP seen in obese and overweight schoolchildren. Joint PA with an adult within household could increase schoolchildren's cardiorespiratory fitness level, PA level and may reduce the risk of metabolic diseases. Rural environments support PA in children but not PA of adole scents, Town and fringe areas with mixed elements of rural and urban land use appear to fac ilitate and sustain PA in both children and adolescents, High screen time 2, j and deprivation may explain lateness in bedtime in English schoolchildren. Family centred interventions may be important to reduce screen time and improve metabolic profile. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
40|$|The {{physical}} {{interpretation of}} the spectral-temporal structure of LANDSAT data can be conveniently {{described in terms of}} a graphic descriptive model called the Tassled Cap. This model {{has been a source of}} development not only in crop-related feature extraction, but also for <b>data</b> <b>screening</b> and for haze effects correction. Following its qualitative description and an indication of its applications, the model is used to analyze several feature extraction algorithms...|$|R
5000|$|IPC's {{products}} called trading turrets, are specialized, multi-line, multi-speaker communications devices used by traders. Turrets {{can have}} access to hundreds of lines and allow traders to monitor multiple connections simultaneously to maintain communication with counterparties, liquidity providers, intermediaries and exchanges [...] IPC's desktop system for traders provides multiple market <b>data</b> <b>screens.</b> and gives traders the option to use instant messaging for colleague communications while checking on incoming calls.|$|R
40|$|Abstract—Vicarious {{calibration}} <b>data</b> <b>screening</b> method {{based on}} the measured atmospheric optical depth and the variance of the measured surface reflectance at the test sites is proposed. Reliability of the various calibration data has to be improved. In order to improve the reliability of the vicarious calibration data, some screenings have to be made. Through experimental study, it is found that vicarious calibration <b>data</b> <b>screening</b> would be better to apply with the measured atmospheric optical depth and variance of the measured surface reflectance due to the facts that thick atmospheric optical depth means that the atmosphere contains serious pollution sometime and that large deviation of the surface reflectance from the average means that the solar irradiance has an influence due to cirrus type of clouds. As the results of the screening, the uncertainty of vicarious calibration data from the approximated radiometric calibration coefficient is remarkably improved. Also, it is found that cross calibration uncertainty is poorer than that of vicarious calibration. Keywords—Vicarious calibration; Surface reflectance; Atmospheric Optical Depth; Sky-radiometer; Terra/ASTER...|$|R
