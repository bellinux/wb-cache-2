35|432|Public
5|$|Even {{after the}} OS and SSD are {{configured}} {{to support the}} TRIM command, other conditions might prevent any benefit from TRIM. , databases and RAID systems are not yet TRIM-aware and consequently will {{not know how to}} pass that information on to the SSD. In those cases the SSD will continue to save and <b>garbage</b> <b>collect</b> those blocks until the OS uses those LBAs for new writes.|$|E
5|$|When an SSD {{is writing}} {{large amounts of}} data sequentially, the write {{amplification}} is equal to one meaning there is no write amplification. The reason is as the data is written, the entire block is filled sequentially with data related to the same file. If the OS determines that file is to be replaced or deleted, the entire block can be marked as invalid, {{and there is no}} need to read parts of it to <b>garbage</b> <b>collect</b> and rewrite into another block. It will need only to be erased, which is much easier and faster than the read-erase-modify-write process needed for randomly written data going through garbage collection.|$|E
5|$|If the {{controller}} were to background <b>garbage</b> <b>collect</b> {{all of the}} spare blocks before it was absolutely necessary, new data written from the host could be written without having to move any data in advance, letting the performance operate at its peak speed. The trade-off {{is that some of}} those blocks of data are actually not needed by the host and will eventually be deleted, but the OS did not tell {{the controller}} this information. The result is that the soon-to-be-deleted data is rewritten to another location in the flash memory, increasing the write amplification. In some of the SSDs from OCZ the background garbage collection clears up {{only a small number of}} blocks then stops, thereby limiting the amount of excessive writes. Another solution is to have an efficient garbage collection system which can perform the necessary moves in parallel with the host writes. This solution is more effective in high write environments where the SSD is rarely idle. The SandForce SSD controllers and the systems from Violin Memory have this capability.|$|E
50|$|The foulmouthed {{chief of}} {{mechanics}} of Sena's new <b>garbage</b> <b>collecting</b> company.|$|R
40|$|We {{study the}} cost of storage {{management}} for garbagecollected programs compiled with the Standard ML of New Jersey compiler. We show that {{the cost of}} storage management {{is not the same as}} the time spent <b>garbage</b> <b>collecting.</b> For many of the programs, the time spent <b>garbage</b> <b>collecting</b> is less than the time spent doing other storage management tasks. ...|$|R
5000|$|The young {{generation}} stores short-lived {{objects that}} are created and immediately <b>garbage</b> <b>collected.</b>|$|R
5|$|The TRIM command {{also needs}} {{the support of}} the SSD. If the {{firmware}} in the SSD does not have support for the TRIM command, the LBAs received with the TRIM command will not be marked as invalid and the drive will continue to <b>garbage</b> <b>collect</b> the data assuming it is still valid. Only when the OS saves new data into those LBAs will the SSD know to mark the original LBA as invalid. SSD Manufacturers that did not originally build TRIM support into their drives can either offer a firmware upgrade to the user, or provide a separate utility that extracts the information on the invalid data from the OS and separately TRIMs the SSD. The benefit would be realized only after each run of that utility by the user. The user could set up that utility to run periodically in the background as an automatically scheduled task.|$|E
5|$|The {{separation}} of static and dynamic data to reduce write amplification {{is not a}} simple process for the SSD controller. The process requires the SSD controller to separate the LBAs with data which is constantly changing and requiring rewriting (dynamic data) from the LBAs with data which rarely changes and does not require any rewrites (static data). If the data is mixed in the same blocks, as with almost all systems today, any rewrites will require the SSD controller to <b>garbage</b> <b>collect</b> both the dynamic data (which caused the rewrite initially) and static data (which did not require any rewrite). Any garbage collection of data that would not have otherwise required moving will increase write amplification. Therefore, separating the data will enable static data to stay at rest and if it never gets rewritten it will have the lowest possible write amplification for that data. The drawback to this process is that somehow the SSD controller must still find a way to wear level the static data because those blocks that never change will not get a chance to be written to their maximum P/E cycles.|$|E
50|$|Object {{deletion}} {{is rarely}} {{used as the}} scripting engine will <b>garbage</b> <b>collect</b> objects that are no longer being referenced.|$|E
40|$|An {{effective}} {{way to reduce the}} number of aborts in software transactional memory (STM) is to keep multiple versions of transactional objects. In this paper, we study inherent properties of STMs that use multiple versions to guarantee successful commits of all read-only transactions. We first show that these STMs cannot be disjoint-access parallel. We then consider the problem of <b>garbage</b> <b>collecting</b> old object versions, and show that no STM can be optimal in the number of previous versions kept. Moreover, we show that <b>garbage</b> <b>collecting</b> useless versions is impossible in STMs that implement invisible reads. Finally, we present an STM algorithm using visible reads that efficiently <b>garbage</b> <b>collects</b> useless object versions. ...|$|R
5000|$|A {{class can}} be {{designated}} to be <b>garbage</b> <b>collected</b> via the [...] extension keyword.|$|R
5000|$|The memory {{allocated}} for the servlet and its objects {{can then be}} <b>garbage</b> <b>collected.</b>|$|R
50|$|For peak {{usage of}} the flash-memory drive, it is {{necessary}} to compact data so that flash-memory blocks are full of useful data. This is accomplished by garbage collection. LogFS's garbage collection strategy relies on file data being placed in a certain way into flash-memory blocks: a flash-memory block will hold only file data from the same level in the inode tree. LogFS can <b>garbage</b> <b>collect</b> the top level of the trees using just 1 empty flash-memory block. It can <b>garbage</b> <b>collect</b> the top 2 levels of the trees using 2 empty flash-memory blocks. And can <b>garbage</b> <b>collect</b> all N levels of the trees using N empty flash memory blocks. The algorithm is exponential time in the worst case, but the worst case is rare and the algorithm requires reserving only a handful of flash-memory blocks.|$|E
50|$|If the IDS doesn't <b>garbage</b> <b>collect</b> TCBs {{correctly}} and efficiently, an attacker can exhaust the IDS's memory by starting {{a large number}} of TCP connections very quickly. Similar attacks can be made by fragmenting {{a large number of}} packets into a larger number of smaller packets, or send a large number of out-of-order TCP segments.|$|E
50|$|Even {{after the}} OS and SSD are {{configured}} {{to support the}} TRIM command, other conditions might prevent any benefit from TRIM. , databases and RAID systems are not yet TRIM-aware and consequently will {{not know how to}} pass that information on to the SSD. In those cases the SSD will continue to save and <b>garbage</b> <b>collect</b> those blocks until the OS uses those LBAs for new writes.|$|E
5000|$|United States Patent 7865536 - Ghemawat, Gobioff, & Leung (2011). <b>Garbage</b> <b>collecting</b> {{systems and}} methods.|$|R
5|$|Managed memory {{cannot be}} {{explicitly}} freed; instead, it is automatically <b>garbage</b> <b>collected.</b> <b>Garbage</b> collection addresses {{the problem of}} memory leaks by freeing the programmer of responsibility for releasing memory {{that is no longer}} needed.|$|R
50|$|Managed C++ is <b>garbage</b> <b>collected.</b> In {{standard}} C++, {{memory management}} and allocation {{is the responsibility}} of the programmer.|$|R
50|$|In effect, blocks may be flushed at {{any time}} and the soft-update code will always provide the disk a {{consistent}} version of it (as long as it knows which blocks have physically been flushed). Recovery then simply becomes a matter of running a background walk of the file system when it is next mounted to <b>garbage</b> <b>collect</b> any allocated space that has been orphaned. This also permits the filesystem to selectively flush certain files without having to flush all metadata blocks or all of the records.|$|E
5000|$|In Python, {{prior to}} Python 3.4, the {{standard}} CPython implementation would treat resurrected objects identically to other objects (which {{had never been}} finalized), making indestructible objects possible. Further, it would not <b>garbage</b> <b>collect</b> cycles that contained an object with a finalizer, to avoid possible problems with object resurrection. Starting in Python 3.4, behavior is largely the same as Java: objects are only finalized once (being marked as [...] "already finalized"), garbage collection of cycles is in two phases, with the second phase checking for resurrected objects.|$|E
5000|$|The JVM has a garbage-collected heap {{for storing}} objects and arrays. Code, constants, and other class data {{are stored in}} the [...] "method area". The method area is logically part of the heap, but {{implementations}} may treat the method area separately from the heap, and for example might not <b>garbage</b> <b>collect</b> it. Each JVM thread also has its own call stack (called a [...] "Java Virtual Machine stack" [...] for clarity), which stores frames. A new frame is created each time a method is called, and the frame is destroyed when that method exits.|$|E
50|$|To {{conserve}} memory, {{the interpreter}} included a <b>garbage</b> <b>collecting</b> memory manager, used for both string data and byte-code.|$|R
30|$|This rule {{is kind of}} <b>garbage</b> <b>collecting</b> rule. Network {{resource}} will {{be released}} by those useless data chunks in the end.|$|R
50|$|With <b>garbage</b> <b>collected</b> {{languages}} it {{may become}} a source of memory leaks as it introduces global strong references to the objects.|$|R
50|$|When an SSD {{is writing}} {{large amounts of}} data sequentially, the write {{amplification}} is equal to one meaning there is no write amplification. The reason is as the data is written, the entire block is filled sequentially with data related to the same file. If the OS determines that file is to be replaced or deleted, the entire block can be marked as invalid, {{and there is no}} need to read parts of it to <b>garbage</b> <b>collect</b> and rewrite into another block. It will need only to be erased, which is much easier and faster than the read-erase-modify-write process needed for randomly written data going through garbage collection.|$|E
50|$|The TRIM command {{also needs}} {{the support of}} the SSD. If the {{firmware}} in the SSD does not have support for the TRIM command, the LBAs received with the TRIM command will not be marked as invalid and the drive will continue to <b>garbage</b> <b>collect</b> the data assuming it is still valid. Only when the OS saves new data into those LBAs will the SSD know to mark the original LBA as invalid. SSD Manufacturers that did not originally build TRIM support into their drives can either offer a firmware upgrade to the user, or provide a separate utility that extracts the information on the invalid data from the OS and separately TRIMs the SSD. The benefit would be realized only after each run of that utility by the user. The user could set up that utility to run periodically in the background as an automatically scheduled task.|$|E
5000|$|SWT widgets, unlike {{almost any}} other Java toolkit, {{requires}} manual object deallocation, {{in contrast to the}} standard Java practice of automatic garbage collection. SWT objects must be explicitly deallocated using the [...] method, which is analogous to the C language's [...] [...] If this is not done, memory leaks or other unintended behavior may result. On this matter, some have commented that [...] "explicitly de-allocating the resources could be a step back in development time (and costs) at least for the average Java developer" [...] and that [...] "this is a mixed blessing. It means more control (and more complexity) for the SWT developer instead of more automation (and slowness) when using Swing." [...] The need for manual object deallocation when using SWT is largely due to SWT's use of native objects. These objects are not tracked by the Java JVM, so it cannot track whether or not such objects are in use, and thus cannot <b>garbage</b> <b>collect</b> them at a suitable time.|$|E
50|$|Although this is {{possible}} with statically compiled <b>garbage</b> <b>collected</b> languages, a bytecode system can more easily rearrange executed code for better cache utilization.|$|R
50|$|Another {{language}} {{that does not}} make this terminology distinction is D. Although D classes are <b>garbage</b> <b>collected,</b> their cleanup functions are called destructors.|$|R
50|$|The lapsed {{listener}} {{problem is}} a common source of memory leaks for object-oriented programming languages, among the most common ones for <b>garbage</b> <b>collected</b> languages.|$|R
50|$|If the {{controller}} were to background <b>garbage</b> <b>collect</b> {{all of the}} spare blocks before it was absolutely necessary, new data written from the host could be written without having to move any data in advance, letting the performance operate at its peak speed. The trade-off {{is that some of}} those blocks of data are actually not needed by the host and will eventually be deleted, but the OS did not tell {{the controller}} this information. The result is that the soon-to-be-deleted data is rewritten to another location in the flash memory, increasing the write amplification. In some of the SSDs from OCZ the background garbage collection clears up {{only a small number of}} blocks then stops, thereby limiting the amount of excessive writes. Another solution is to have an efficient garbage collection system which can perform the necessary moves in parallel with the host writes. This solution is more effective in high write environments where the SSD is rarely idle. The SandForce SSD controllers and the systems from Violin Memory have this capability.|$|E
50|$|The {{separation}} of static and dynamic data to reduce write amplification {{is not a}} simple process for the SSD controller. The process requires the SSD controller to separate the LBAs with data which is constantly changing and requiring rewriting (dynamic data) from the LBAs with data which rarely changes and does not require any rewrites (static data). If the data is mixed in the same blocks, as with almost all systems today, any rewrites will require the SSD controller to <b>garbage</b> <b>collect</b> both the dynamic data (which caused the rewrite initially) and static data (which did not require any rewrite). Any garbage collection of data that would not have otherwise required moving will increase write amplification. Therefore, separating the data will enable static data to stay at rest and if it never gets rewritten it will have the lowest possible write amplification for that data. The drawback to this process is that somehow the SSD controller must still find a way to wear level the static data because those blocks that never change will not get a chance to be written to their maximum P/E cycles.|$|E
40|$|Abstract. This paper {{describes}} {{the architecture of}} a toolkit, called Mihda, providing facilities to minimise labelled transition systems for name passing calculi. The structure of the toolkit {{is derived from the}} co-algebraic formulation of the partition-refinement minimisation algorithm for HD-automata. HD-automata have been specifically designed to allocate and <b>garbage</b> <b>collect</b> names and they provide faithful finite state representations of the behaviours of π-calculus processes. The direct correspondence between the coalgebraic specification and the implementation structure facilitates the proof of correctness of the implementation. We evaluate the usefulness of Mihda in practise by performing finite state verification of π-calculus specifications. ...|$|E
40|$|AbstractWe {{show that}} {{on-the-fly}} garbage collection algorithms {{can be obtained}} by transforming distributed termination detection protocols. Virtually all known on-the-fly <b>garbage</b> <b>collecting</b> algorithms are obtained by applying the transformation. The approach leads to a novel and insightful derivation of, e. g., the concurrent garbage collection algorithms of Dijkstra et al. and of Hudak and Keller. The approach also leads to several new, highly parallel algorithms for concurrent garbage collection. We also analyze a <b>garbage</b> <b>collecting</b> system due to Hughes from our current perspective...|$|R
50|$|In {{any case}} the {{statements}} in the finally block are always executed. This {{can be used to}} free resources, although memory is automatically <b>garbage</b> <b>collected.</b>|$|R
40|$|Submitted for publication. This {{paper is}} also {{published}} as Fox Memorandum CMU-CS-FOX- 94 - 08. We study {{the cost of}} storage management for garbage-collected programs compiled with the Standard ML of New Jersey compiler. We show {{that the cost of}} storage management {{is not the same as}} the time spent <b>garbage</b> <b>collecting.</b> For many of the programs, the time spent <b>garbage</b> <b>collecting</b> is less than the time spent doing other storage-management tasks. This document has been approved for public release and salej its distribution is unlimite...|$|R
