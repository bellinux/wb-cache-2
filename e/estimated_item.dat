34|403|Public
30|$|High {{proficiency}} group As {{discussed in}} the previous sections, the general tendency for all test takers is that the DWD affects the <b>estimated</b> <b>item</b> difficulty the most. However, this tendency is not retained in the high proficiency group, where SIM has the most significant influence on the item difficulty (p value =. 005), followed by DWD (p value =. 06), and RPD (p value =. 95). This finding suggests that the high proficiency {{students are more likely}} to get confused when the correct answer and distractors have a similar meaning to each other. Moreover, in this group, RPD does not significantly affect the <b>estimated</b> <b>item</b> difficulty. This suggests that the students probably did not read the reading passage to answer the questions in the experiment; they were probably aware of the target word’s meaning regardless of its context.|$|E
3000|$|... is {{conditioned}} on race, age, census region, and urban/rural status, thereby {{allowing for}} the possibility that there are differences in distributions of proficiency for blacks and whites of different age groups, census regions, and urban/rural status. We also <b>estimated</b> <b>item</b> parameters in (8), since we do not have them from the test publishers. For each discrimination parameter [...]...|$|E
30|$|We {{analysed}} {{the collected}} data against to answer three research questions: (1) whether the item difficulty {{can be controlled}} using the investigated factors, (2) which factor contributes the most to item difficulty and, (3) how these factors affect the item difficulty across test takers with different proficiency. We first estimated the item difficulty for each item using both CTT and IRT. The <b>estimated</b> <b>item</b> difficulties from both theories correlated quite highly (r =. 80); hence, for further analysis, we used CTT. The analysis revealed the following results.|$|E
50|$|In psychometrics, EM {{is almost}} {{indispensable}} for <b>estimating</b> <b>item</b> parameters and latent abilities of item response theory models.|$|R
50|$|Huang, C.-Y., Kalohn, J.C., Lin, C.-J., and Spray, J. (2000). <b>Estimating</b> <b>Item</b> Parameters from Classical Indices for Item Pool Development with a Computerized Classification Test. (Research Report 2000-4). Iowa City, IA: ACT, Inc.|$|R
30|$|There {{are several}} ways of <b>estimating</b> <b>item</b> {{difficulty}} from the test taker’s responses. In test theory, such as CTT and item response theory (IRT), the difficulty {{is defined as the}} likelihood of correct responses, not as the perceived difficulty or necessary amount of effort (DeMars 2010).|$|R
40|$|A {{simulation}} {{study of}} the effectiveness of four item characteristic curve estimation programs was conducted. Using the three-parameter logistic model, three groups of 2, 000 simulated subjects were administered 80 -item tests. These simulated test responses were then calibrated using the four programs. The <b>estimated</b> <b>item</b> parameters were compared to the known item parameters in four analyses for each program in all three data sets. It was concluded that the selection of an item calibration procedure should be dependent on the distribution of ability in the calibration sample, the later uses of the item parameters, and the computer resources available...|$|E
30|$|In {{order to}} {{simulate}} our data, we used existing released item parameters {{that could be}} found on typical ILSAs. Specifically, for non-DIF conditions, we <b>estimated</b> <b>item</b> parameters from the 2007 TIMSS grade 4 assessment. On 2007 TIMSS, a total of 179 items were released for grade 4 across all content areas, of which 174 were scored dichotomously (i.e., either multiple-choice or constructed response items with only two categories; Olson et al. [2008]). From this pool of 174 released items (i.e., their parameters), we randomly drew without replacement a total of 105 items. The 105 items represented {{the total number of}} items used in our study, where each of the seven blocks contained 15 items.|$|E
30|$|Because the SB {{index was}} defined only for items with an RT threshold, RTF was not {{available}} for all items in the MCBS. We first correlated item RTF with item position in each block. There were five content areas assessed in the MCBS—algebra, data analysis statistics and probability (abbreviated as data), geometry, measurement, and number properties and operations (abbreviated as number), and it was of interest to examine whether the students' test-taking behavior {{was associated with the}} items' content area. Thus the box plot of item RTF by content area was made. The relationship between RTF and item characteristics was further explored by plotting the RTF values against the <b>estimated</b> <b>item</b> discrimination and difficulty parameters obtained from the IRT modelse; Pearson correlation was also computed.|$|E
40|$|Relationships {{between a}} {{mathematical}} measurement model and its real-world applications are discussed. A distinction is made between large data matrices {{commonly found in}} educational measurement and smaller matrices found in attitude and personality measurement. Nonparametric methods are evaluated for <b>estimating</b> <b>item</b> response functions and (unconditional and conditional) inter-item covariances...|$|R
40|$|The thesis {{explains}} the {{fundamental difference between}} unipolar and bipolar measurement scales for psychological characteristics. We explore the use of correspondence analysis (CA), a technique {{that is similar to}} principal component analysis and is available in SAS and SPSS, to select items that together form a bipolar scale. CA <b>estimates</b> both <b>item</b> locations and subject locations and is a useful tool in the psychometric evaluation of bipolar measurement scales. Additionally we present a new methodology (OCM) to <b>estimate</b> <b>item</b> response functions of single-peaked items with a model-free approach. Promotor: W. J. Heiser, Co-Promotor: M. de Rooij With Summary in DutchWith Summary in DutchStichting onderzoeksfonds Ontwikkelingsprofiel, Amsterdam Stichting Psychoanalytische Fondsen, Amsterda...|$|R
40|$|The {{present study}} {{presents}} {{the formulation of}} graded response models in the multilevel framework (as nonlinear mixed models) and demonstrates their use in <b>estimating</b> <b>item</b> parameters and investigating the group-level effects for specific covariates using Bayesian estimation. The graded responsemultilevel model (GRMM) combines the formulation of graded response models with the discrimination parameter fixed at one for all items by Tuerlinckx andWang and of two parameter models by Rijmen and Briggs to offer graded response models with item-specific discrimination parameters. Apart from the contribu-tion {{to the body of}} knowledge by formulating GRMMs, the significance of the present study includes providing a meeting point between psychometrics and statistics, over-coming the Neyman–Scott problem by using Bayesian estimation, estimation of abilities of persons with extreme scores, and demonstration of general purpose software for <b>estimating</b> <b>item</b> response theory parameters. Data from the emotional functioning scale on 11, 158 healthy and chronically ill children and adolescents were used from the PedsQL 4. 0 Generic Core Scales database to illustrate the model. <b>Estimates</b> for the <b>item</b> parameters fromWINBUGS using Bayesian priors andMultilog were compared for the GRMM and the ordinary graded response models, respectively...|$|R
40|$|The use of item-ability {{regressions}} (the {{comparison of}} the regression of the observed proportion of people answering an item correctly on estimated &thetas; with the <b>estimated</b> <b>item</b> response function) to investigate {{the psychometric properties of}} particular item types in a given population was explored using data from four administrations of 10 item types (a total of 806 items) from the Graduate Record Examinations General Test. Although the method does not allow an absolute determination of fit for a latent trait model (in this case, for the three-parameter logistic model), it does show that certain item types consistently fit the model worse than other item types, and it led to and supported a specific hypothesis as to why the model probably did not fit these item types...|$|E
40|$|The {{accuracy}} of marginal maximum likelihood {{estimates of the}} item parameters of the two-parameter logistic model was investigated. Estimates were obtained for four sample sizes and four test lengths; joint maximum likelihood estimates were also computed for the two longer test lengths. Each condition was replicated 10 times, which allowed evaluation of the {{accuracy of}} <b>estimated</b> <b>item</b> characteristic curves, item parameter estimates, and estimated standard errors of item parameter estimates for individual items. Items that are typical of a widely used job satisfaction scale and moderately easy tests had satisfactory marginal estimates for all sample sizes and test lengths. Larger samples were required for items with extreme difficulty or discrimination parameters. Marginal estimation was substantially better than joint maximum likelihood estimation. Index terms: Fletcher-Powell algorithm, item parameter estimation, item response theory, joint maximum likelihood estimation, marginal maximum likelihood estimation, two-parameter logistic model...|$|E
40|$|The {{presence}} of biased items may seriously affect {{methods used to}} link metrics in item response theory. An iterative procedure designed to minimize this methodological problem was examined in a monte carlo investigation using the two-parameter item response model. The iterative procedure links the scales of independently calibrated parameter estimates using only those items identified as unbiased. Two methods for transforming parameter estimates to a common metric were incorporated into the iterative procedure. The first method links scales by equating the first two moments of the distributions of <b>estimated</b> <b>item</b> difficulties. The second method determines the linking transformation by minimizing differences across IRT characteristic curve estimates. Results indicate that iterative linking provides a substantial improvement in item bias detection over the noniterative approach. Index terms: Item bias, Item response theory, Iterative method, Linking, Metric linking, Two-parameter item response model...|$|E
50|$|The {{reason for}} using the Fibonacci {{sequence}} is to reflect the inherent uncertainty in <b>estimating</b> larger <b>items.</b>|$|R
40|$|A {{study was}} {{conducted}} to investigate whether augmenting the calibration of items using computerized adaptive test (CAT) data matrices produced estimates that were unbiased and improved the stability of existing <b>item</b> parameter <b>estimates.</b> <b>Item</b> parameter <b>estimates</b> from four pools of items constructed for operational use were used in the study to arrive at a final number of 1, 392 unique items. Fifty sets of true parameter estimates were generated from the base item prior information, and each true set served as the parameter estimated for a CAT simulation that incorporated content constraints and exposure control. One thousand test takers were simulated at each of 41 points on the ability scale. The examination of the influence of CAT data on item response theory three-parameter logistic model (3 PL) item characteristic curves and on <b>item</b> parameter <b>estimates</b> for the 3 PL model in a Bayesian context showed that the error in the item characteristic curve as a whole and in the <b>estimates</b> of <b>item</b> difficulty appeared to be reduced when CAT data information was incorporated into the estimation of the parameter. Results suggest that error in <b>estimating</b> <b>item</b> characteristic curves may be reduced by incorporating CAT information into the calibration process. (Contains 4 tables, 27 figures, and 4 references.) (SLD) Reproductions supplied by EDRS are the best that can be made from the original document...|$|R
30|$|With lsasim {{designed}} to generate item responses, {{it has no}} functionality to <b>estimate</b> <b>item</b> parameters or achievement estimates. For this, users should turn to existing packages, for example, TAM, as is demonstrated later in this article. The data output from lsasim is formatted {{to be used with}} TAM or mirt without any further data manipulation. Furthermore, the ibd package (Mandal 2018) can be used in tandem with lsasim to generate balanced incomplete designs.|$|R
40|$|The {{computer}} program PC-BILOG uses the estimated posterior θ distribution {{to establish the}} location and metric of the θ scale. This approach to solving the identification problem has not been examined extensively. Consequently, this study investigated the equating of PC-BILOG results to an underlying metric when a two-parameter IRT model was used. The simulation {{results showed that the}} means of the <b>estimated</b> <b>item</b> and θ parameters generally were insensitive to characteristics of the prior distribution on the item discriminations. The finding of greatest interest was that the PC-BILOG procedures preserved the variability of true θ distributions having small variances while standardizing the variability of those having large variances. However, in both cases the results could be equated to the true metric using existing techniques. Index terms: ability metric, Bayesian estimation, BILOG, equating, item response theory, prior distributions...|$|E
40|$|This {{study has}} {{investigated}} whether mental attentional capacity increases as a lineair function of age during normal childhood development, as was predicted from Pascual-Leone's neo-Piagetian theory. To test this prediction, Pascual-Leone's Figural Intersections Test (FIT) and Raven's Standard Progressive Matrices (RSPM) {{were administered to}} 215 children aged 5 to 12 years. The responses on the total set of items showed a good fit to the Verhelst and Glas generalization of the on-parameter logistic model. The <b>estimated</b> <b>item</b> parameters were used to construct to seperate mental attentional capacity scales. The average mental attentional capacity estimates of children aged 5 to 6, 7 to 8, and 9 to 10 were approximately localized in the predicted areas of the scale based on RSPM items. The average mental attentional capacity estimates based on the FIT items were localized above the predicted scale areas. Consequently, the results only partly provide empirical support for the assumptions of Pascual-Leone's theory...|$|E
40|$|The {{comparative}} format used in ranking and paired comparisons tasks {{can significantly}} reduce the impact of uniform response biases typically associated with rating scales. Thurstone's (1927, 1931) model provides a powerful framework for modeling comparative data such as paired comparisons and rankings. Although Thurstonian models are generally presented as scaling models, that is, stimuli-centered models, {{they can also be}} used as person-centered models. In this article, we discuss how Thurstone's model for comparative data can be formulated as item response theory models so that respondents' scores on underlying dimensions can be <b>estimated.</b> <b>Item</b> parameters and latent trait scores can be readily estimated using a widely used statistical modeling program. Simulation studies show that item characteristic curves can be accurately estimated with as few as 200 observations and that latent trait scores can be recovered to a high precision. Empirical examples are given to illustrate how the model may be applied in practice and to recommend guidelines for designing ranking and paired comparisons tasks in the future...|$|E
40|$|Two new prophecy {{formulas}} for <b>estimating</b> <b>item</b> response theory (IRT) –based {{reliability of}} a shortened or lengthened test are proposed. Some {{of the relationships}} between the two formulas, one of which is identical to the well-known Spearman-Brown prophecy for-mula, are examined and illustrated. The major assumptions underlying these formulas are outlined and discussed. Both prophecy formulas appear to provide comparable esti-mates of reliability in the IRT context {{as well as in the}} classical test theory context...|$|R
40|$|A 67 -item Assessment Practices Inventory (API) was {{administered}} to 311 inservice teachers. The application of principal components analysis to the data yielded a 6 -factor solution that explained 64 % of the variance. The Rasch rating scale model {{was applied to the}} API to <b>estimate</b> <b>item</b> calibrations. The factor analyzed assessment categories were then ranked in order by difficulty based on mean logits. The distribution of mean logits ranged from-. 35 to 0. 78. Communicating assessment results was the easiest assessmen...|$|R
40|$|Several {{studies have}} {{compared}} different judgmental methods of setting passing scores by <b>estimating</b> <b>item</b> difficulties for the minimally competent examinee. Usually, a direct method of <b>estimating</b> <b>item</b> difficulties {{has been compared}} with an indirect method suggested by Nedelsky (1954). Nedelsky’s method has usually resulted in a substantially lower cutoff score than that arrived at with a direct method. Two {{studies were carried out}} for the purpose of comparing a direct method of setting passing scores with an indirect method that allowed judges to estimate the probability of the minimally competent examinee eliminating each incorrect alternative. In Study 1 a sample of 52 first-level supervisors used both methods to estimate passing scores on a content-oriented selection test for building maintenance specialists. In Study 2 a sample of 62 first-level supervisors used both methods to estimate passing scores on an entry level auto mechanics test. Results of both studies showed that the variance component for method was relatively small and that for raters was relatively large. Reliability estimates of judgments and correlations between judged difficulties and empirical difficulties showed the Angoff (1971) approach to be slightly superior. Results showed no particular advantage to using an indirect approach for estimating minimal competence. Recently, the problem of setting passing score...|$|R
40|$|First-order growth mixture model (1 -GMM) has {{received}} increased attention {{over the past}} decade. It models class-specific latent growth trajectory and individual classification using composite scores computed over items of the same scale across multiple time points. By default, using composite scores assumes identical item-to-construct relationship over time (longitudinal measurement invariance; L-MI), which {{is not necessarily the}} case in research practice. Violation of L-MI assumption has been studied using latent growth curve modeling where subjects are assumed to be sampled from one latent class. Deviation from L-MI assumption impacted the growth characteristics, thus producing invalid conclusions on the pattern of change. This study extends the prior research on the impact of L-MI violation to the situation where multiple latent classes exist. A Monte Carlo study was performed to examine how systematically varied measurement non-invariance impacted class-specific growth factor parameter recovery and classification accuracy. Five factors were systematically manipulated in studying the impact of L-MI assumption violation: directional change in non-invariant item intercepts, patterns of item loadings and item intercepts, percent of items containing a set of non-invariant item parameters, presence of time-adjacent within-item correlated measurement error, and latent class distances. Additionally, three GMMs were compared to assess their robustness against longitudinal measurement non-invariance, including 1 -GMM, second order GMM with constrained measurement invariance, and second order GMM with freely <b>estimated</b> <b>item</b> factor loadings and item intercepts. Accuracy, precision, Type I error, and power were examined on the slope factor parameter estimates. Additionally, mixture proportion and individual classification were assessed. Results show that the second order GMM with freely <b>estimated</b> <b>item</b> loadings and item intercepts was robust under various violation of L-MI and able to produce accurate estimates of slope factor parameters. Performance of the second order GMM with constrained measurement invariance on slope factor parameters recovery depended on the specific generating measurement non-invariance configuration. 1 -GMM, on the other hand, was not able to recover the slope factor parameters with deviation from the L-MI assumption. With extremely unbalanced mixture proportions, class membership assignment was found not satisfactory regardless of simulated measurement non-invariance condition and analysis model...|$|E
40|$|Differential item {{functioning}} (DIF) {{has been}} informally conceptualized as multidimensionality. Recently, more formal descriptions of DIF as multidimensionality have become {{available in the}} item response theory literature. This approach assumes that DIF is not {{a difference in the}} item parameters of two groups; rather, it is a shift in the distribution of ability along a secondary trait that influences the probability of a correct item response. That is, one group is relatively more able on an ability such as test-wiseness. The parameters of the secondary distribution are confounded with item parameters by unidimensional DIF detection models, and this manifests as differences between <b>estimated</b> <b>item</b> parameters. However, DIF is confounded with impact in multidimensional tests, which may be a serious limitation of unidimensional detection methods in some situations. In the multidimensional approach, DIF {{is considered to be a}} function of the educational histories of the examinees. Thus, a better tool for understanding DIF may be provided through structural modeling with external variables that describe background and schooling experience. Index terms: differential item functioning, factor analysis, IRT, item bias, LISREL, multidimensionality...|$|E
40|$|We {{begin by}} {{describing}} {{the factor analysis}} we used to initially explore the data and assess the plausibility of using items on particular dimension. We go on to give the exact ques-tion wording from the CCES for each item and the <b>estimated</b> <b>item</b> difficulty/discrimination parameters. Factor Analysis and Item Selection To select the items used in our analyses {{of the economic and}} social dimensions, we began by factor analyzing all of the “ideological ” items included in the common content module of the CCES, 1 which results in 15 items for analysis. An eigenvalue decomposition suggests a 3 -factor solution, which is represented in table 1. The loadings in table 1 suggest that the first dimension is primarily an economic di-mension of ideology, the second dimension is primarily a social dimension, and the third dimension is a “globalization ” dimension dominated by attitudes towards trade and immi-gration. We take the items from the first dimension and use those items to measure economic ideology, with a few exceptions. First, we exclude the items measuring attitudes towards Ira...|$|E
30|$|The goal of lsasim is {{to provide}} a set of {{functions}} that enable users to design and modify test designs that are commonly utilized in large-scale educational surveys. Such goals are similar to the goals of catR (Magis and Raiche 2012) for generating item response patterns from computer adaptive tests and mstR (Magis et al. 2017) for generating item response patterns from multi-stage tests. The difference, however, is that multi-matrix sampling designs utilized in large-scale assessments are not (yet) adaptive, and can thus, depend on other packages to <b>estimate</b> <b>item</b> parameters and achievement estimates.|$|R
40|$|The {{method of}} {{successive}} intervals, a procedure for obtaining equal intervals from category data, {{is applied to}} social preference data for a health status index. Several innovations are employed, including an approximate analysis of variance test for determining whether the intervals are of equal width, a regression model for estimating {{the width of the}} end intervals in finite scales, and a transformation to equalize interval widths and <b>estimate</b> <b>item</b> locations on the new scale. A computer program has been developed to process large data sets with a larger number of categories than previous programs...|$|R
40|$|An {{evaluation}} of the variation of <b>item</b> <b>estimates</b> was conducted for the multidimensional extension of the logistic item response theory (MIRT) model. The empirically determined standard errors (SEs) of marginal maximum likelihood estimation (MMLE) /Bayesian <b>item</b> <b>estimates</b> from 40 <b>items</b> from the ACT Assessment (Form 24 b, 1985) were obtained when {{the same set of}} items was repeatedly eliminated from test data. These empirically determined SEs were then compared with their corresponding analytical (or formula-based) ones. Both approaches, in general, resulted in similar SE estimates for the same set of items. This empirical comparison implies that the analytical approach has the potential of being used for approximately estimating the magnitudes of SEs of the MMLE/Bayesian <b>item</b> <b>estimates.</b> Tabulation of the analytical SEs for several combinations of item parameters (e. g., low item difficulty, high item discrimination, and low item discrimination was provided as a reference. In addition, the graphical three-dimensional representation of the SEs of <b>item</b> <b>estimates</b> as the bivariate function of item difficulty together with item discrimination was displayed. How to apply the analytical SEs of MIRT <b>item</b> <b>estimates</b> in a MIRT item linking study is illustrated. (Contains 4 tables, 2 figures, and 19 references.) (Author/SLD) Reproductions supplied by EDRS are the best that can be made from the original document. The Consistency between the Empirical and the Analytical Standard Error...|$|R
40|$|Mappings of spatially-varying Item Response Theory (IRT) {{parameters}} are proposed, allowing for visual investigation of potential Differential Item Functioning (DIF) based upon geographical location without need for pre-specified groupings and before any confirmatory DIF testing. This proposed {{model is a}} localized approach to IRT modeling and DIF detection that provides a flexible framework, with current emphasis being on 1 PL/Rasch and 2 PL models. Applications to both simulated and empirical survey data, utilizing a box-car kernel weighting scheme with several fixed bandwidths on irregular spatial lattices, are presented both to demonstrate the methodology and to illustrate the benefit of localized IRT modeling. There is not only practical value with this method but also visual appeal when initial attempts to consider measurement invariance are being made across national, state, or other political and geographical boundaries, especially when comparisons are made to traditional DIF techniques. This approach, making use of surface mappings of <b>estimated</b> <b>item</b> parameters, serves to detect DIF across space without a priori groupings, thereby identifying regional disparities and latent spatial trends in item functionality that may be unobservable on a more aggregate, global level...|$|E
40|$|Simulated {{data were}} used to {{investigate}} the per-formance of modified versions of the Mantel-Haenszel method of differential item functioning (DIF) analysis in computerized adaptive tests (CATs). Each simulated examinee received 25 items from a 75 -item pool. A three-parameter logistic item response theory (IRT) model was assumed, and examinees were matched on expected true scores based on their CAT responses and <b>estimated</b> <b>item</b> parameters. The CAT-based DIF statistics {{were found to be}} highly correlated with DIF statistics based on nonadaptive administration of all 75 pool items and with the true magnitudes of DIF in the simulation. Average DIF statistics and average standard errors also were examined for items with various characteristics. Finally, a study was conducted of the accuracy with which the modified Mantel-Haenszel procedure could identify CAT items with substantial DIF using a classi-fication system now implemented by some testing programs. These additional analyses provided further evidence that the CAT-based DIF procedures performed well. More generally, the results supported the use of IRT-based matching variables in DIF analysis. Index terms: adaptive testing, computerized adaptive test-ing, differential item functioning, item bias, item re-sponse theory. Many large-scale testing programs are now piloting or implementing computerized adaptive tests (CATS) ...|$|E
40|$|When {{administering}} large-scale assessments, item-position {{effects are}} of particular importance be-cause the applied test designs very often contain several test booklets with the same items presented at different test positions. Establishing such position effects would be most critical; it {{would mean that the}} <b>estimated</b> <b>item</b> parameters do not depend exclusively on the items ’ difficulties due to content but also on their presentation positions. As a consequence, item calibration would be biased. By means of the linear logistic test model (LLTM), item-position effects can be tested. In this paper, the results of a simulation study demonstrating how LLTM is indeed able to detect certain position effects in the framework of a large-scale assessment are presented first. Second, empirical item-position effects of a specific large-scale competence assessment in mathematics (4 th grade students) are analyzed using the LLTM. The results indicate that a small fatigue effect seems to take place. The most important conse-quence of the given paper is that it is advisable to try pertinent simulation studies before an analysis of empirical data takes place; the reason is, that for the given example, the suggested Likelihood-Ratio test neither holds the nominal type-I-risk, nor qualifies as “robust”, and furthermore occasionally show...|$|E
30|$|The booklet {{design was}} such that each cluster {{appeared}} {{in each of the}} four possible positions within a booklet exactly once, and each cluster occurred once in conjunction with each of the other clusters. Each test item, therefore, appeared in four of the test booklets. This linked design made it possible, when <b>estimating</b> <b>item</b> difficulties and student proficiencies, to apply standard measurement techniques to the resulting student response data (OECD, 2008). Student performance results were reported in terms of one overall scale in science, five science subscales, one overall scale for mathematics, and one overall scale for reading.|$|R
40|$|There {{are many}} {{software}} packages that <b>estimate</b> <b>item</b> response theory parameters and examinee abilities. This study evaluates {{the accuracy of}} the item parameter and ability estimates generated by the open-source R package ltm. In this simulation study, <b>item</b> and ability <b>estimates</b> were compared to the true parameters under six conditions that differed in the numbers of items and examinees. After looking at the resulting bias, mean absolute deviation, and root mean square error, we concluded that item parameter and ability estimates from ltmwere estimated reasonably accurately with results similar to previous studies of established commercial software...|$|R
40|$|The {{generalized}} graded unfolding model (GGUM) is a {{very general}} parametric, unidimen-sional item response theory model for unfolding either binary or polytomous responses to test items. Roberts, Donoghue, and Laughlin have described a marginal maximum likelihood (MML) approach to <b>estimate</b> <b>item</b> parameters in the GGUM along with an expected a posteriori (EAP) method to estimate person parameters. This article examines the data demands required to produce accurate parameter estimates using these techniques under ideal condi-tions. It also examines the robustness of parameter estimates under nonideal conditions, {{in which there are}} inconsistencies between the prior distribution of person parameters that must be specified whe...|$|R
