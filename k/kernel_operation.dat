35|112|Public
50|$|Currently {{the system}} exists in alpha version for ia32 processors. Port to ARM {{architecture}} is underway (currently being tested, {{not yet ready}} for use) and port to MIPS and amd64 has been started. <b>Kernel</b> <b>operation</b> has been demonstrated at the biggest Russian IT-conferences RIT 2011, ADD 2010, CC 2010, and 2009.|$|E
5000|$|In many situations, {{discrete}} convolutions can {{be converted}} to circular convolutions so that fast transforms with a convolution property can be used to implement the computation. For example, convolution of digit sequences is the <b>kernel</b> <b>operation</b> in multiplication of multi-digit numbers, which can therefore be efficiently implemented with transform techniques ( [...] ; [...] ).|$|E
50|$|OpenVPN {{offers several}} {{internal}} security features. It has up to 256-bit encryption through OpenSSL library, although some service providers may offer lower rates, effectively making the connection faster. It runs in userspace instead of requiring IP stack (therefore <b>kernel)</b> <b>operation.</b> OpenVPN {{has the ability}} to drop root privileges, use mlockall to prevent swapping sensitive data to disk, enter a chroot jail after initialization and apply a SELinux context after initialization.|$|E
40|$|The {{effect of}} <b>kernel</b> <b>operations</b> on cache optimisations in a soft-core {{reconfigurable}} system {{is important for}} dynamic cache switching design. Considering <b>kernel</b> <b>operations</b> changes the subset of cache configurations that would be chosen for dynamic cache switching and also the decisions on when to cache switch. The results show that <b>kernel</b> <b>operations</b> can skew the effectiveness of application driven cache optimisations up to 20 % over the original execution time. This skew is shown by mapping {{the performance of the}} applications both with and without <b>kernel</b> <b>operations.</b> The majority of the <b>kernel</b> <b>operations</b> is due to trap events generated by system calls like memory allocation or file reading. A cache configuration analysis methodology for fast searching of the design space is also explained and was used to find relevant changes due to kernel interference. 1...|$|R
25|$|Other {{monolithic}} and hybrid kernels, like Linux and Windows NT, {{are also}} susceptible to malfunctioning drivers impeding the <b>kernel's</b> <b>operation.</b>|$|R
5000|$|Initially, these subroutines used {{hard-coded}} loops {{for their}} low-level operations. For example, if a subroutine need {{to perform a}} matrix multiplication, then the subroutine would have three nested loops. Linear algebra programs have many common low-level operations (the so-called [...] "kernel" [...] operations, not related to operating systems). Between 1973 and 1977, several of these <b>kernel</b> <b>operations</b> were identified. These <b>kernel</b> <b>operations</b> became defined subroutines that math libraries could call. The kernel calls had advantages over hard-coded loops: the library routine would be more readable, there were fewer chances for bugs, and the kernel implementation could be optimized for speed. A specification for these <b>kernel</b> <b>operations</b> using scalars and vectors, the level-1 Basic Linear Algebra Subroutines (BLAS), was published in 1979. BLAS was used to implement the linear algebra subroutine library LINPACK.|$|R
40|$|We {{report on}} a {{high-performance}} in-kernel web server for Linux known as the Threaded linUX http layer, or TUX, for short. TUX uses aggressive network layer data caching to accelerate static content delivery, and invokes CGI scripts directly from the kernel to accelerate dynamic content generation. We describe the TUX web server architecture, modifications included in the patch, and how they affect <b>kernel</b> <b>operation</b> and web server performance...|$|E
40|$|Separation kernels mediate {{interaction}} between partitions in a secure system. System security policies {{can be developed}} about systems that use separation kernels that rely only on the appropriate separation <b>kernel</b> <b>operation.</b> We introduce a formal security policy for a separation kernel in ACL 2, and argue for its usefulness by comparing it with other formalisms and by using it in a proof involving the correctness of a firewall application...|$|E
40|$|Abstract: Minix 3 isareal micro <b>kernel</b> <b>operation</b> {{system with}} alot of {{remarkable}} security features. Twoofthe main points are size and isolation. The Minix 3 kernel {{is less than}} one thousand {{times the size of}} Linux. All drivers and the IP stack live in user land. We show aport of the netfilter framework, which leads to asystem with better stability and security than the widely used Linux solutions [We 07]. Additionally we present some newideas regarding virtualized systems. ...|$|E
5000|$|Pre-R2xx/NV2x: no {{explicit}} {{support for}} stream processing. <b>Kernel</b> <b>operations</b> were {{hidden in the}} API and provided too little flexibility for general use.|$|R
5000|$|... ktrace is {{a utility}} {{included}} with certain versions of BSD Unix and Mac OS X that traces kernel interaction {{with a program}} and dumps it to disk {{for the purposes of}} debugging and analysis. Traced <b>kernel</b> <b>operations</b> include system calls, namei translations, signal processing, and I/O.|$|R
40|$|Abstract. The paper {{presents}} a hard real-time kernel for distributed computer control systems (DCCS). The kernel uses advanced task and time management techniques featuring Boolean vectors and bitwise vector processing, {{and this has}} resulted in very low overhead and constant execution time of <b>kernel</b> <b>operations,</b> independent {{of the number of}} tasks involved. That idea has been further used to implement the kernel communication protocol consisting of two sublayers- an event notification layer and a message exchange layer supporting implicit (content-oriented) message addressing. Constant execution time of <b>kernel</b> <b>operations</b> has made it possible to develop an enhanced computational test which is used to estimate task response times, taking into account kernel execution effects. This test has been incorporated into a system development and analysis tool supporting distributed hard real-time applications...|$|R
40|$|Abstract: Matrix {{multiplication}} is the <b>kernel</b> <b>operation</b> {{used in many}} {{image and}} signal processing applications. This paper demonstrates an effective design for the Matrix Multiplication using Systolic Architecture. This architecture increases the computing speed by using the concept of parallel processing and pipelining into a single concept. The selected platform is a FPGA (Field Programmable Gate Array) device since, in systolic computing, FPGAs {{can be used as}} dedicated computers in order to perform certain computations at very high frequencies. The description language used as an entry tool to model the hardware architecture is VERILOG HDL...|$|E
40|$|For {{a general}} acceptance,-kernels must be fast and not burden applications. For ful lling these conditions, cache {{architectures}} require-kernels to be small. The L 4 -kernel shows that smallness can be achieved. 1. -kernels must be small This is not obvious. Most rst-generation-kernels were large � typically they need 300 Kbyte of code and 140 system calls. Some of their architects argued that ` ' {{in this context}} means `lower level ' and not `small size'. Demanding smallness radically di ers from this approach. It could (and in fact it does) change-kernel technology dramatically. Why should-kernels be as small as possible? (We avoid the term &quot; because of its mathematical implications.) The reasons are performance, exibility and perhaps correctness. 1. 1. non-small-kernel is not fast. &quot; The most relevant performance costs of a-kernel result from its cache consumption. If a frequently invoked <b>kernel</b> <b>operation</b> accesses {{a substantial part of}} the primary cache (oods the cache&quot;), the user is punished twice. First, the <b>kernel</b> <b>operation</b> itself is degraded by cache misses, since it must displace user code and data. Second, the user program has to pay for additional cache misses, since it must re-establish its cache working set. From Chen's [1993] measurements for a Mach benchmark on a DS 5000 / 200, we can calculate that on average 20...|$|E
40|$|Abstract. The paper {{presents}} advanced {{task management}} techniques featuring Boolean vectors and bitwise vector operations on kernel data {{structures in the}} context of the HARTEX TM hard real-time kernel. These techniques have been consistently applied to all aspects of task management and interaction. Hence, the execution time of system functions no longer depends on the number of tasks involved, resulting in predictable, jitter-free <b>kernel</b> <b>operation.</b> This approach has been further extended to time management resulting in a new type of kernel component, which can be used to implement timed multitasking- a novel technique providing for jitter-free execution of hard real-time tasks. ...|$|E
40|$|Because {{of their}} {{flexibility}} and cost effectiveness, software implementations of number-theoretic cryptographic algorithms (e. g., RSA and Diffie-Hellman) are often desired. In {{order to obtain}} the required level of performance (speed) on a selected platform, the developers turn to algorithm-level optimizations and assembly language programming. In this paper, we examine these implementation issues and propose a design methodology for obtaining high-speed implementations. We show that between the full assembler implementation and the standard C implementation, there is a design option in which {{only a small number}} of code segments (<b>kernel</b> <b>operations)</b> are written in assembler in order to obtain a significant portion of the speed increase gained by the full assembler implementation. We propose a small set of <b>kernel</b> <b>operations</b> which are as simple as a·b+c, where the numbers a, b, c are 1 -word integers. The results of our experiments on several processors are also summarized...|$|R
40|$|We {{demonstrate}} how a new data structure for sparse distributed polynomials in the Maple 17 kernel significantly accelerates several key Maple library routines. The POLY data structure {{and its associated}} <b>kernel</b> <b>operations</b> are programmed for compactness, scalability, and low overhead. This allows polynomials to have {{tens of millions of}} terms, increases parallel speedup, and improves the performance of Maple library routines. ...|$|R
40|$|This paper {{deals with}} the use of X-ray {{tomography}} and subsequent <b>kernel</b> <b>operations</b> on the reconstructed images to generate 3 D density maps and phase labelling of polygranular graphite subjected to thermal oxidation which, in some respects, simulates radiolytic oxidation. This shows the suitability of the procedures to quantify density variations in oxidised samples. © 2005 Acta Materialia Inc. Published by Elsevier Ltd. All rights reserved...|$|R
40|$|Matrix {{multiplication}} is the <b>kernel</b> <b>operation</b> {{used in many}} transform, {{image and}} discrete signal processing application. We develop new algorithms and new techniques for matrix multiplication on configurable devices. In this paper, we have proposed three designs for matrix-matrix multiplication. These design reduced hardware complexity, throughput rate and different input/output data format to match different application needs. These techniques have been designed implementation on Virtex- 4 FPGA. We have synthesized the proposed designs and the existing design using Synopsys tools. Interestingly, the proposed parallel-fixed-input and multiple-output (PPI-MO) structure consumes 40 % less energy than other two proposed structures and 70 % less energy than the existing structure...|$|E
40|$|Abstract—Matrix-vector {{multiplication}} is a computationally {{intensive and}} <b>kernel</b> <b>operation</b> {{used in many}} image processing applications. This paper presents a preliminary Field Programmable Gate Array (FPGA) design and implementation of dense matrix-vector multiplication for use in an image processing application. The design is optimized for speed which is the main requirement for such applications. The design has been implemented on Virtex- 4 FPGA using Xilinx ISE 9. 2 i and the performance is evaluated by computing the execution time on FPGA. FPGA implementation results demonstrate that it can provide a maximum throughput of 16970 frames per second utilizing only 14 % Virtex- 4 slices and 57 % DSP 48 blocks which is quite adequate for most real-time image processing applications...|$|E
40|$|The {{multiplication}} of a vector by {{a matrix}} is the <b>kernel</b> <b>operation</b> in many algorithms used in scientific computation. This paper outlines four parallel matrix – vector multiplication implementations on {{a cluster of}} workstations. These parallel implementations {{are based on the}} dynamic master – worker paradigm. Furthermore, the parallel implementations are analyzed experimentally using the Message Passing Interface (MPI) library on two kinds of high performance cluster of workstations: homogeneous and heterogeneous. Also, we propose a general analytical prediction model {{that can be used to}} predict the performance of the matrix – vector implementation for two kinds of cluster (homogeneous and heterogeneous). The developed performance model has been checked and it has been shown that this model is able to predict the parallel performance accurately...|$|E
40|$|Many {{software}} security solutions—including malware analyzers, information flow tracking systems, auditing utilities, and host-based intrusion detectors—rely on knowledge of standard system call interfaces to reason about process execution behavior. In this work, we first obfuscate the Windows and Linux system call interfaces to degrade {{the effectiveness of}} these tools. Our attack, called Illusion, invokes privileged <b>kernel</b> <b>operations</b> in the <b>kernel</b> at the request of user-level processes without requiring those processes to call the actual system calls corresponding to the operations. The Illusion interface hides system operations from user-, kernel-, and hypervisor-level monitors mediating the conventional system-call interface. Illusion alters neither static kernel code nor read-only dispatch tables, remaining elusive from tools protecting kernel memory. We then consider the problem of Illusion attacks and augment system call data with kernel-level execution information to expose the hidden <b>kernel</b> <b>operations.</b> We present a Xen-based monitoring system, Sherlock, that adds kernel execution watchpoints to the stream of system call events. Sherlock automatically adapts its sensitivity based on security requirements to remain performant on desktop systems. ...|$|R
50|$|Drivers {{written for}} Windows 9x/Windows ME are {{loaded into the}} same address space as the kernel. This means that drivers can by {{accident}} or design overwrite critical sections of the operating system. Doing {{this can lead to}} system crashes, freezes and disk corruption. Faulty operating system drivers were a source of instability for the operating system.Other monolithic and hybrid kernels, like Linux and Windows NT, are also susceptible to malfunctioning drivers impeding the <b>kernel's</b> <b>operation.</b>|$|R
40|$|We {{demonstrate}} how a new data structure for sparse distributed polynomials in the Maple kernel significantly accelerates a large subset of Maple library routines. The POLY data structure {{and its associated}} <b>kernel</b> <b>operations</b> (degree, coeff, subs, has, diff, eval, [...] .) are programmed for high scalability, allowing polynomials to have {{hundreds of millions of}} terms, and very low overhead, increasing parallel speedup in existing routines and improving the performance of high level Maple library routines. ...|$|R
40|$|We {{report on}} a {{high-performance}} in-kernel web server for Linux known as the Threaded linUX http layer, or TUX, for short. TUX uses aggressive network layer data caching to accelerate static content delivery, and invokes CGI scripts directly from the kernel to accelerate dynamic content generation. We describe the TUX web server architecture, modifications included in the patch, and how they affect <b>kernel</b> <b>operation</b> and web server performance. November 16, 2000 Center for Information Technology Integration University of Michigan 519 West William Street Ann Arbor, MI 48103 - 4943 This document was written {{as part of the}} Linux Scalability Project. The work described in this paper was supported via grants from the Sun-Netscape Alliance, Intel, Dell, and IBM. For more information, see our home page. If you have comments or suggestions, email Copyright 2000 by the Regents of the University of Michigan, and by AOL-Netscape Inc. All rights reserved. T [...] ...|$|E
40|$|Abstract. Motivated by {{the need}} to formalize {{generation}} of fast run-ning code for linear algebra applications, we show how an index-free, calculational approach to matrix algebra can be developed by regard-ing matrices as morphisms of a category with biproducts. This shifts the traditional view of matrices as indexed structures to a type-level perspective analogous to that of the pointfree algebra of programming. The derivation of fusion, cancellation and abide laws from the biprod-uct equations makes it easy to calculate algorithms implementing matrix multiplication, the <b>kernel</b> <b>operation</b> of matrix algebra, ranging from its divide-and-conquer version to the conventional, iterative one. From errant attempts to learn how particular products and coproducts emerge from biproducts, we not only rediscovered block-wise matrix com-binators but also found a way of addressing other operations calculation-ally such as e. g. Gaussian elimination. A strategy for addressing vector-ization along the same lines is also given. ...|$|E
40|$|We {{report on}} our {{experience}} with integrating and using graphics processing units (GPUs) as fast parallel floatingpoint co-processors to accelerate two fundamental computational scientific kernels on the GPU: sparse direct factorization and nonlinear interior-point optimization. Since a full re-implementation of these complex kernels is typically not feasible, we identify the matrix-matrix multiplication {{as a first}} natural entry-point for a minimally invasive integration of GPUs. We investigate the performance on the NVIDIA GeForce 8800 multicore chip initially architectured for intensive gaming applications. We exploit the architectural features of the GeForce 8800 GPU to design an efficient GPU-parallel sparse matrix solver. A prototype approach to leverage the bandwidth and computing power of GPUs for these matrix <b>kernel</b> <b>operation</b> is demonstrated resulting in an overall performance of over 110 GFlops/s on the desktop for large matrices. We use our GPU algorithm for PDE-constrained optimization problems and demonstrate that the commodity GPU is a useful co-processor for scientific applications...|$|E
40|$|Abstract. Many {{software}} security solutions—including malware analyzers, information flow tracking systems, auditing utilities, and hostbased intrusion detectors—rely on knowledge of standard system call interfaces to reason about process execution behavior. In this work, we show how a rootkit can obfuscate a commodity kernel’s system call interfaces to degrade {{the effectiveness of}} these tools. Our attack, called Illusion, allows user-level malware to invoke privileged <b>kernel</b> <b>operations</b> without requiring the malware to call the actual system calls corresponding to the operations. The Illusion interface hides system operations from user-, kernel-, and hypervisor-level monitors mediating the conventional system-call interface. Illusion alters neither static kernel code nor readonly dispatch tables, remaining elusive from tools protecting kernel memory. We then consider the problem of Illusion attacks and augment system call data with kernel-level execution information to expose the hidden <b>kernel</b> <b>operations.</b> We present a Xen-based monitoring system, Sherlock, that adds kernel execution watchpoints to the stream of system calls. Sherlock automatically adapts its sensitivity based on security requirements to remain performant on desktop systems: in normal execution, it adds 1 % to 10 % overhead to a variety of workloads. ...|$|R
50|$|Other machine {{features}} {{became available}} and {{could also be}} exploited. Consequently, BLAS was augmented from 1984 to 1986 with level-2 <b>kernel</b> <b>operations</b> that concerned vector-matrix operations. Memory hierarchy was also recognized as something to exploit. Many computers have cache memory that is much faster than main memory; keeping matrix manipulations localized allows better usage of the cache. In 1987 and 1988, the level 3 BLAS were identified to do matrix-matrix operations. The level 3 BLAS encouraged block-partitioned algorithms. The LAPACK library uses level 3 BLAS.|$|R
30|$|The {{algorithm}} in [7] {{is based}} on the kernel least squares (KLS) algorithm. Its main advantage is that although it is computed in a distributed way, it converges to the solution of the centralized version. As discussed, the distributed KLS (DKLS) is highly demanding in terms of communication and computation capabilities. It requires matrix and <b>kernel</b> <b>operations</b> at the same time than multi-node communication management. Our solution is possible thanks to the versatile features of TinyOS, the operating system that will be used to program and compile the application into the motes.|$|R
40|$|Motivated by {{the need}} to formalize {{generation}} of fast running code for linear algebra applications, we show how an index-free, calculational approach to matrix algebra can be developed by regarding matrices as morphisms of a category with biproducts. This shifts the traditional view of matrices as indexed structures to a type-level perspective analogous to that of the pointfree algebra of programming. The derivation of fusion, cancellation and abide laws from the biproduct equations makes it easy to calculate algorithms implementing matrix multiplication, the <b>kernel</b> <b>operation</b> of matrix algebra, ranging from its divide-and-conquer version to the conventional, iterative one. From errant attempts to learn how particular products and coproducts emerge from biproducts, we not only rediscovered block-wise matrix com- binators but also found a way of addressing other operations calculation- ally such as e. g. Gaussian elimination. A strategy for addressing vector- ization along the same lines is also given. FCT, Mondrian Project funded by contract PTDC/EIA-CCO/ 108302 / 2008...|$|E
40|$|One well adopted {{power grid}} {{simulation}} methodology is to factorize matrix once and perform only backward forward substitution with a deliberately chosen step size along the simulation. Since the required simulation time is usually {{long for the}} power grid design, the costly factorization is amortized. However, such fixed step size cannot exploit larger step size for the low frequency response in the power grid to speedup the simulation. In this work, we utilize the matrix exponential method with the rational Krylov subspace approximation to enable adaptive step size in the power grid simulation. The <b>kernel</b> <b>operation</b> in our method only demands one factorization and backward forward substitutions. Moreover, the rational Krylov subspace approximation can relax the stiffness constraint of the previous works. The cheap computation of adaptivity in our method could exploit the long low frequency response in a power grid and significantly accelerate the simulation. The experimental results show that our method achieves up to 18 X speedup over the trapezoidal method with fixed step size...|$|E
40|$|Coarse grained overlay {{architectures}} improve FPGA design productivity {{by providing}} fast compilation and software-like programmability. Throughput oriented spatially configurable overlays typically suffer from area overheads {{due to the}} requirement of one functional unit for each compute <b>kernel</b> <b>operation.</b> Hence, these overlays have often been of limited size, supporting only relatively small compute kernels while consuming considerable FPGA resources. This paper examines the possibility of sharing the functional units among kernel operations for reducing area overheads. We propose a linear interconnected array of time-multiplexed FUs as an overlay architecture with reduced instruction storage and interconnect resource requirements, which uses a fully-pipelined, architecture-aware FU design supporting a fast context switching time. The results presented show a reduction of up to 85 % in FPGA resource requirements compared to existing throughput oriented overlay architectures, with an operating frequency which approaches the theoretical limit for the FPGA device. Comment: Presented at 2 nd International Workshop on Overlay Architectures for FPGAs (OLAF 2016) arXiv: 1605. 0814...|$|E
40|$|Abstract: This paper {{presents}} schedulability analysis {{which has}} been developed in order to precisely estimate task response times, taking into account kernel execution effects. The analysis takes into consideration specific features of the HARTEX hard real-time kernel, and in particular: asynchronous event-driven operation; integrated scheduling of hard and soft real-time tasks (as well as tasks and resources) and advanced task management, whereby <b>kernel</b> <b>operations</b> are executed in constant time, independent {{of the number of}} tasks involved. The developed test has been incorporated into a system specification and analysis tool supporting HARTEX-based real-time applications...|$|R
40|$|AbstractSimple games {{correspond}} to monotonic Boolean functions, but their study {{is associated with}} a distinct set of intuitions and questions. We show how “trading properties” from simple games can be adapted to the nonmonotonic context, and how a number of these trading properties (some new and some old) are tied to each other via two <b>kernel</b> <b>operations</b> of the Alexandrov topology. These results provide an answer to a question of Peter Hammer about the relationship between the property referred to here as weak monotonicity and some better-known properties of Boolean functions...|$|R
40|$|This paper {{describes}} a technique for utilizing predication to support software pipelining on EPIC architectures {{in the presence}} of dynamic memory aliasing. The essential idea is that the compiler generates an optimistic softwarepipelined schedule that assumes there is no memory aliasing. The operations in the pipeline kernel are predicated, however, so that if memory aliasing is detected by a run-time check, the predicate registers are set to disable the iterations that are so tightly overlapped as to violate the memory dependences. We refer to these disabled <b>kernel</b> <b>operations</b> as software bubbles. ...|$|R
