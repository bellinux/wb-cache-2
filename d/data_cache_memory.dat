14|4314|Public
30|$|The work {{presented}} in [34] briefly {{points to the}} C 64 x but is lacking details. Furthermore, as of now (submission time of the paper) {{the model for the}} C 64 x is not included in the library of the latest SoftExplorer tool [35]. Unlike the model introduced in [36] that employs the parallelism factor as the affecting parameter for the processing units (PUs) block, the fact that the NOP does not require any PU for its execution convinced us that another parameter yields a better description of the PUs. Moreover, as we will explain later, the level- 1 <b>data</b> <b>cache</b> <b>memory</b> submodel is different as well.|$|E
40|$|Abstract: Static energy {{dissipation}} in cache [1][2][9][11][13]. Most {{of the proposed}} techniques are based memories will constitute an increasingly larger portion on turning off portions of the cache {{at the cost of}} losing data of total microprocessor {{energy dissipation}} due to and increasing miss rates. Even techniques that try to control nanoscale technology characteristics and the large size this negative effect incur performance overhead to a certain of on-chip caches. We propose to reduce the static extent [9]. In this paper, we propose a technique that can turn energy dissipation of an on-chip data cache by taking off cache portions with zero information loss and no advantage of the frequent values (FV) that widely exist in performance degradation with proper design of cache cell a <b>data</b> <b>cache</b> <b>memory.</b> The original FV-based low-powe...|$|E
40|$|This {{report is}} {{dedicated}} to the processor characterization method and software cost esti-mation technique used in the Polis Codesign tool environment. The processor characteri-zation method has been exercised by applying it to the ARM processor family. In particu-lar, two processors, ARM 7 TDMI and ARM 920 T, have been examined. An improved method is proposed, which is supported and partially automated by two utility tools. The improved method is based on an iterative two-pass technique. The first pass involves proc-essor characteristic extraction from generic software templates. The second pass improves the parameters from the first pass using a validation method. The results obtained during the exercise are presented and discussed. In particular the effect of instruction and <b>data</b> <b>cache</b> <b>memory</b> is addressed. The estimation technique used today is well suited for proces-sors without cache, but processors with cache calls for new techniques...|$|E
40|$|We {{develop an}} {{analytic}} model, {{and a set}} of microbenchmark programs for the measurement of the structural parameters of <b>data</b> <b>cache</b> <b>memories</b> and <b>data</b> TLBs. Running under Linux, our microbenchmarks accurately measure <b>data</b> <b>cache</b> capacity, <b>data</b> <b>cache</b> line size, <b>data</b> <b>cache</b> associativity, effective <b>cache</b> latency, effective <b>data</b> path parallelism, data TLB size, data TLB associativity, and TLB latency. We present experimental results from running our microbenchmarks on Pentium II and Pentium III workstations...|$|R
40|$|Abstract. We have {{developed}} a set of microbenchmakrs for accurately determining the structural characteristics of <b>data</b> <b>cache</b> <b>memories</b> and TLBs. These characteristics include cache size, cache line size, <b>cache</b> as-sociativity, <b>memory</b> page size, number of data TLB entries, and data TLB associativity. Unlike previous microbenchmarks that used time-based measurements, our microbenchmarks use hardware event counts to more accurately and quickly determine these characteristics while re-quiring fewer limiting assumptions. ...|$|R
40|$|To {{improve the}} {{efficiency}} of a processor to work with <b>data,</b> <b>cache</b> <b>memories</b> are used to compensate the latency delay to access data from the main memory. But because of the installation of different caches in different processors in a shared memory architecture, makes {{it very difficult to}} maintain consistency between the <b>cache</b> <b>memories</b> of different processors. For that reason, having a cache coherency protocol is really essential in those kinds of system. There are different coherency protocols for caches to maintain consistency between different caches in a shared memory system. Few of the famous cache coherency protocols ar...|$|R
40|$|Superscalar {{implementations}} of RISC architectures {{are emerging}} as the dominant high-performance microprocessor technology for the mid- 1990 's. This paper proposes and evaluates a split <b>data</b> <b>cache</b> <b>memory</b> design, a new memory system enhancement for superscalar processor architectures. This design allows floating-point and integer memory accesses to be executed in parallel. The configuration is well matched to the dual-path execution hardware of many current superscalar processors. It doubles peak bandwidth without the expense or complexity of multi-ported memory and increases the processor's ability to exploit fine-grained parallelism. The reported simulation results show that by using this enhancement, a speedup of more than 1. 5 over the traditional unified cache model can be achieved on some standard benchmarks. The speedup is not uniform among all programs. Several hypotheses are presented and experimentally validated to explain these results. 1. Introduction Although superscalar imp [...] ...|$|E
40|$|RISC {{processors}} {{have approached}} an execution {{rate of one}} instruction per cycle by using pipelining to speed up execution. However, to achieve an execution {{rate of more than}} one instruction per cycle requires multiple instruction issue (MII) processors which employ multiple pipelines. This paper evaluates the important architectural features of iHARP, the University of Hertfordshire's VLIW processor. Using a resource limited scheduler (RLS), the work shows that the inclusion of various architectural features, for example, conditional instruction execution or {{the increase in the number}} of <b>data</b> <b>cache</b> <b>memory</b> ports can improve the performance of a MII processor. A review of the work undertaken by a number of groups in the areas of potential instruction level parallelism and static scheduling show that a great amount of fine-grained parallelism is theoretically available. However, for a processor with four pipelines, our work achieves an instruction execution rate approaching two instructions per cycle...|$|E
40|$|In recent work, we {{described}} a data prefetch mechanism for pointer-intensive and numeric computations, and presented some aggregate measurements on {{a suite of}} benchmarks to quantify its performance potential [MH 95]. The basis for this device is a simple classification of memory access patterns in programs that we introduced earlier [HM 94]. In this paper we {{take a close look}} at two codes from our suite, an English parser called Link-Gram, and the circuit simulation program spice 2 g 6, and present a detailed analysis of them in the context of our model. Focusing on just two programs allows us to display a wider range of data, and discuss relevant code fragments extracted from their source distributions. Results from this study provide a deeper understanding of our memory access classification scheme, and suggest additional optimizations for future data prefetch mechanisms. Keywords: CPU architecture, <b>data</b> <b>cache,</b> <b>memory</b> access pattern classification, instruction profiling, memory latency t [...] ...|$|E
40|$|<b>Cache</b> <b>memories</b> {{are used}} in modern, medium and {{high-speed}} CPUs to hold temporarily those portions {{of the contents of}} main memory which are {believed to be) currently in use. Since instructions and <b>data</b> in <b>cache</b> <b>memories</b> can usually be referenced in 10 to 25 percent of the time required to access main <b>memory,</b> <b>cache</b> <b>memories</b> permit th...|$|R
30|$|LEON 3 [7] is a 32 -bit synthesizable soft-processor that is {{compatible}} with SPARC V 8 architecture: it has a seven-stage pipeline and Harvard architecture. It uses separate instruction and <b>data</b> <b>cache</b> <b>memories</b> and supports multiprocessor configurations: in particular, an SMP-aware configuration is well supported thanks to available memory management unit and snooping unit for cache coherence. It represents a soft-processor for aerospace applications. LEON 3 is described {{by means of an}} open-source VHDL model and provides full configurability by means of the Gaisler Research IP Library (GRLIB).|$|R
40|$|AbstractCompact {{numerical}} schemes provide high-order {{solution of}} PDEs with low dissipation and dispersion. Computer implementation of these schemes requires numerous passes of <b>data</b> through <b>cache</b> <b>memory</b> that considerably reduces performance of these schemes. To reduce this difficulty, a novel {{algorithm is proposed}} here. This algorithm {{is based on a}} wavefront approach and sweeps through <b>cache</b> <b>memory</b> only twice...|$|R
40|$|This work {{presents}} {{design of}} a configurable and observable model of L 1 <b>data</b> <b>cache</b> <b>memory</b> and a novel method for integrating the model into an FPGA prototype. Embedded system software designers use in-circuit emulation on FPGA platforms to validate the functionality and performance of embedded software. Data cache, particularly L 1, has a major impact of system performance, yet remains unobservable during software debugging and analysis. Our solution is to model the data cache as an on-chip hardware peripheral that can {{be integrated into the}} processor system and can display the state of the data cache at any given time. The model is synthesized on Xilinx Virtex 5 FPGA and validated using several benchmarks. The experimental results show that the model can accurately track cache hits and misses and can estimate the run time of an embedded software application with an average error of only 5. 4 %, and a worst case error of only 13. 7 %...|$|E
40|$|The power {{consumed}} by the memory hierarchy of a microprocessor can contribute {{to as much as}} 50 % of the total microprocessor system power, and is thus a good candidate for power and energy optimizations. We discuss four methods for tuning a microprocessors' cache subsystem to the needs of any executing application for low-energy embedded systems. We introduce onchip hardware implementing an efficient cache tuning heuristic that can automatically, transparently, and dynamically tune a configurable level-one cache's total size, associativity and line size to an executing application. We extend the single-level cache tuning heuristic for a two-level cache using a methodology applicable to both a simulation-based exploration environment and a hardware-based system prototyping environment. We show that a victim buffer can be very effective as a configurable parameter in a memory hierarchy. We reduce static energy dissipation of on-chip data cache by compressing the frequent values that widely exist in a <b>data</b> <b>cache</b> <b>memory...</b>|$|E
40|$|This custom RISC {{microprocessor}} delivers approximately 230 Drystone/MIPS at 200 MHz while dissipating 0. 5 W from a 1. 5 V internal supply. The internal supply can {{be biased}} from 1. 5 V to 2. 0 V, while the external supply ia always biased at 3. 3 V. A split supply minimizes power consumption {{for a given}} application, while maintaining compatibility with a 3. 3 V external environ-ment. The die contains 2. 5 M transistors and is 8. 24 x 9. 12 mm 2. It is fabricated in a 0. 35 µm, 2. 0 V, n-well, single poly, 3 -metal-layer CMOS process (Table 1). It is packaged in a 208 pin thin quad flat pack. A previous, 160 MHz, 0. 5 W design contained only a microprocessor core and cache [1, 2]. Incorporated onto this die are: a 16 kB instruction cache, 8 kB data cache, a second, separate, 0. 5 kB <b>data</b> <b>cache,</b> <b>memory</b> management unit, write buffer, read buffer, memory controller, integer unit, integer multiplier, 6 -channel DMA engine...|$|E
5000|$|Another {{class of}} side effects are changes to the {{concrete}} state of the computational system, such as loading <b>data</b> into <b>cache</b> <b>memories.</b> Languages which are often described as [...] "side effect-free" [...] will generally still have concrete side effects which can be exploited, for example, in side-channel attacks.|$|R
40|$|Abstract. Compact {{numerical}} schemes provide high-order {{solution of}} PDEs with low dissipation and dispersion. Computer implementation of these schemes requires numerous passes of <b>data</b> through <b>cache</b> <b>memory</b> that considerably reduces performance of these schemes. To reduce this di culty, anovel {{algorithm is proposed}} here. This algorithm {{is based on a}} wavefront approach and sweeps through cache only twice...|$|R
40|$|Abstract. This paper {{presents}} a new algorithm for computing the singular value decomposition (SVD) on multilevel memory hierarchy architectures. This algorithm {{is based on}} one-sided JRS iteration, which enables the computation of all Jacobi rotations of a sweep in parallel. One key point of our proposed block JRS algorithm is reusing the loaded <b>data</b> into <b>cache</b> <b>memory</b> by performing computations on matrix blocks (b rows) instead of on strips of vectors as in JRS iteration algorithms. Another key point is that on a reasonably large number of processors the number of sweeps is {{less than that of}} one-sided JRS iteration algorithm and closer to the cyclic Jacobi method even though not all rotations in a block are independent. The relaxation technique helps to calculate and apply all independent rotations per block at the same time. On blocks of size b×n, the block JRS performs O(b 2 n) floating-point operations on O(bn) elements, which reuses the loaded <b>data</b> in <b>cache</b> <b>memory</b> by a factor of b. Besides, on P parallel processors, (2 P- 1) steps based on block computations are needed per sweep. ...|$|R
40|$|Abstract: Static energy {{dissipation}} in cache memories will constitute an increasingly larger portion of total microprocessor {{energy dissipation}} due to nanoscale technology characteristics {{and the large}} size of on-chip caches. We propose to reduce the static energy dissipation of an on-chip data cache {{by taking advantage of}} the frequent values (FV) that widely exist in a <b>data</b> <b>cache</b> <b>memory.</b> The original FV-based low-power cache design aimed at only reducing dynamic power, at the cost of a 5 % slowdown. We propose a better design that reduces both static and dynamic cache power, and that uses a circuit design that eliminates performance overhead. A designer can utilize our architecture by simulating an application and then synthesizing the FVs into an application-specific FV cache design when values will not change, or by simulating and then writing to an FV-cache with configuration registers when values could change. Furthermore, we describe hardware that can dynamically determine FVs and write to the configuration registers completely transparently. Experiments on 11 Spec 2000 benchmarks show that, in addition to the dynamic power savings, 33 % static energy savings for data caches can be achieved. 1...|$|E
40|$|Clustered microarchitectures are {{becoming}} a common organization due to their potential to reduce the penalties caused by wire delays and power consumption. Fully-distributed architectures are particularly effective {{to deal with these}} constraints, and besides they are very scalable. However, the distribution of the <b>data</b> <b>cache</b> <b>memory</b> poses a significant challenge and may be critical for performance. In this work, a distributed data cache VLIW architecture based on an interleaved cache organization along with cyclic scheduling techniques are proposed. Moreover, the use of Attraction Buffers for such an architecture is introduced. Attraction Buffers are a novel hardware mechanism to increase the percentage of local accesses. The idea is to allow the movement of some data towards the clusters that need it. Performance results for 9 Mediabench benchmarks show that our scheduling techniques are able to hide the increased memory latency when accessing data mapped in a remote cluster. In addition, the local hit ratio is increased by 15 % and stall time is reduced by 30 % when using the same scheduling techniques with an interleaved cache clustered processor with Attraction Buffers. Finally, the proposed architecture is compared with a state-of-the-art distributed architecture such as the multiVLIW. Results show that the performance of an interleaved cache clustered VLIW processor with Attraction Buffers {{is similar to that of}} the multiVLIW architecture, whereas the former has a lower hardware complexity...|$|E
40|$|Abstract. The cache {{hierarchy}} {{design in}} existing SMT and superscalar processors is optimized for latency, {{but not for}} bandwidth. The size of the L 1 data cache did not scale over the past decade. Instead, larger unified L 2 and L 3 caches were introduced. This cache hierarchy has a high overhead due {{to the principle of}} containment. It also has a complex design to maintain cache coherence across all levels. Furthermore, this cache hierarchy is not suitable for future large-scale SMT processors, which will demand high bandwidth instruction and data caches with a large number of ports. This paper suggests the elimination of the cache hierarchy and replacing it with one-level caches for instruction and data. Multiple instruction caches can be used in parallel to scale the instruction fetch bandwidth and the overall cache capacity. A one-level data cache can be split into a number of block-interleaved cache banks to serve multiple memory requests in parallel. An interconnect is used to connect the data cache ports to the different cache banks, thus increasing the data cache access time. This paper shows that large-scale SMTs can tolerate long data cache hit times. It also shows that small line buffers can enhance the performance and reduce the required number of ports to the banked <b>data</b> <b>cache</b> <b>memory.</b> ...|$|E
40|$|Leakage {{power in}} <b>data</b> <b>cache</b> <b>memories</b> {{represents}} a sizable fraction of total power consumption, and many techniques {{have been proposed}} to reduce it. As a matter of fact, during a fixed period of time, only a small subset of cache lines is used. Drowsy technique, for instance, put unused lines to drowsy state {{in order to save}} power. Our idea is to adaptively select mostly used cache lines in order to maintain mostly used data always available. We found that this can be achieved automatically by using a tiny cache acting as a filter â€œL 0 â€ cache. Our main contributions are: i) evaluation of filter cache to reduce leakage; ii) improvement of an existing power-saving techniques. Our experiments, with complete MiBench suite for ARM based processor, show (in average) 10 % improvement in leakage saving and 17 % in leakage energy-delay versus drowsy-cache...|$|R
40|$|Given {{their high}} {{computational}} power, General Purpose Graphics Processing Units (GPGPUs) are increasingly adopted: GPGPUs {{have begun to}} be preferred to CPUs for several computationally intensive applications, not necessarily related to computer graphics. However, their sensitivity to radiation still requires to be fully evaluated. In this context, GPGPU <b>data</b> <b>caches</b> and shared <b>memory</b> have a key role since they allow to increase performance by sharing data between the parallel resources of a GPGPU and minimizing the memory accesses overhead. In this paper we present three new algorithms designed to support radiation experiments aimed at evaluating the radiation sensitivity of GPGPU <b>data</b> <b>caches</b> and shared <b>memory.</b> We also report the cross-section and Failure In Time results from neutron testing experiments performed on a commercial-off-the-shelf GPGPU using the proposed algorithms, with particular emphasis on the shared memory and on the L 1 and L 2 <b>data</b> <b>cache...</b>|$|R
40|$|Maintaining cache {{coherence}} {{can be very}} costly for on-chip multiprocessors from an energy perspective. Observing this, we propose a compiler-directed strategy that replicates array <b>data</b> in <b>cache</b> <b>memories</b> of its potential consumer processors {{at the time the}} data is brought from off-chip memory. The goal is to eliminate the energy costs associated with bus snooping without negatively impacting overall performance. Our strategy can perform a much better job as compared to static replication strategies, where each array element is replicated based on the same fixed policy...|$|R
40|$|Multi-core {{platforms}} {{are well}} established, {{and they are}} slowly moving into the area of embedded and real-time systems. Nowadays {{to take advantage of}} multi-core systems in terms of throughput, soft real-time applications are run together with general purpose applications under an operating system such as Linux. But due to shared hardware resources in multi-core architectures, it is likely that these applications will interfere and compete with each other. This can cause slower response times for soft real-time tasks. In order to investigate this problem, a number of memory intensive and computation intensive soft real-time tasks were co-scheduled with Linux SMP running a general purpose task on it. For performance measurement a test environment is created that uses hardware registers to count core, level- 2 cache and memory bus events at run-time instead of having a simulator tool. In particular events related to L 1 and L 2 instruction and <b>data</b> <b>cache,</b> <b>memory</b> bus utilization and difference in response times of soft real-time tasks are measured. After completing this research, we can say that it is only possible for soft real-time applications to co-exist with Linux SMP cores running general purpose applications without major performance degradation, if the task on Linux is not much memory intensive. If it is memory intensive then there is a trade-off between number of cores running general purpose applications and the amount of tolerance an embedded system can have in response times...|$|E
40|$|Abstract—Recently, {{the level}} of realism in PC {{graphics}} applications has been approaching that of high-end graphics workstations, necessitating a more sophisticated texture <b>data</b> <b>cache</b> <b>memory</b> to overcome the finite bandwidth of the AGP or PCI bus. This paper proposes a multilevel parallel texture cache memory to reduce the required data bandwidth on the AGP or PCI bus and to accelerate the operations of parallel graphics pipelines in PC graphics cards. The proposed cache memory is fabricated by 0. 16 - m DRAM-based SOC technology. It is composed of four components: an 8 -MB DRAM L 2 cache, 8 -way parallel SRAM L 1 caches, pipelined texture data filters, and a serial-to-parallel loader. For high-speed parallel L 1 cache data replacement, the internal bus bandwidth has been maximized up to 75 GB/s with a newly proposed hidden double data transfer scheme. In addition, the cache memory has a reconfigurable architecture in its line size for optimal caching performance in various graphics applications from three-dimensional (3 -D) games to high-quality 3 -D movies. This architecture also leads to optimal power consumption with an adaptive sub-wordline activation scheme. The pipelined texture data filters and the dedicated structure of the L 1 caches implemented by the DRAM peripheral transistors show the potential of DRAM-based SOC design with better performance-to-cost ratio. Index Terms— 3 -D graphics, DRAM-based SOC, DRAM L 2 cache, multilevel parallel cache, texture cache...|$|E
40|$|The {{purpose of}} this study is to explore the {{relationship}} between hit ratio of <b>cache</b> <b>memory</b> and design parameters. <b>Cache</b> <b>memories</b> are widely used in the design of computer system architectures to match relatively slow memories against fast CPUs. Caches hold the active segments of a program which are currently in use. Since instructions and <b>data</b> in <b>cache</b> <b>memories</b> can be referenced much faster than the time required to access main <b>memory,</b> <b>cache</b> <b>memories</b> permit the execution rate of the machine to be substantially increased. In order to function effectively, <b>cache</b> <b>memories</b> must be carefully designed and implemented. In this study, a trace-driven simulation study of direct mapped, associative mapped and set-associative mapped <b>cache</b> <b>memories</b> is made. In the simulation, cache fetch algorithm, placement policy, cache size and various parameters related to cache design and the resulting effect on system performance is investigated. The <b>cache</b> <b>memories</b> are simulated using the C language and the simulation results are analyzed for the design and implementation of <b>cache</b> <b>memories.</b> Department of Physics and AstronomyThesis (M. S. ...|$|R
30|$|However, picoJava {{implements}} a 64 -word stack buffer as discrete registers. Spill {{and fill}} of that stack buffer is performed in background by the hardware. Therefore, the stack buffer closely {{interacts with the}} <b>data</b> <b>cache.</b> The interference between the folding unit, the instruction buffer, the instruction cache, the stack buffer, the <b>data</b> <b>cache,</b> and the <b>memory</b> interface causes complications in modeling picoJava for WCET analysis.|$|R
40|$|Some {{algorithms}} {{running with}} compromised <b>data</b> select <b>cache</b> <b>memory</b> {{as a type}} of secure memory where data is confined and not transferred to main memory. However, cold-boot attacks that target <b>cache</b> <b>memories</b> exploit the <b>data</b> remanence. Thus, a sudden power shutdown may not delete data entirely, giving the opportunity to steal data. The biggest challenge for any technique aiming to secure the <b>cache</b> <b>memory</b> is performance penalty. Techniques based on data scrambling have demonstrated that security can be improved with a limited reduction in performance. However, they still cannot resist side-channel attacks like power or electromagnetic analysis. This paper presents a review of known attacks on memories and countermeasures proposed so far and an improved scrambling technique named random masking interleaved scrambling technique (RM-ISTe). This method is designed to protect the <b>cache</b> <b>memory</b> against cold-boot attacks, even if these are boosted by side-channel techniques like power or electromagnetic analysis. Postprint (author's final draft...|$|R
40|$|The paper {{presents}} a new unifying formalism introduced to effectively support the automatic generation of assembly test programs {{to be used}} as SBST (Software Based Self-Testing) for both <b>data</b> and instruction <b>cache</b> <b>memories.</b> In particular, the new formalism allows the description of the target memory, of the selected March Test algorithm, and the way this has to be customize to adapt it to the selected cach...|$|R
40|$|As {{processor}} performance {{continues to}} improve, more emphasis {{must be placed}} on the performance of the memory system. In this paper, a detailed characterization of <b>data</b> <b>cache</b> behavior for individual load instructions is given. We show that by selectively applying cache line allocation according the characteristics of individual load instructions, overall performance can be improved for both the <b>data</b> <b>cache</b> and the <b>memory</b> system. This approach can improve some aspects of memory performance by as much as 60 percent on existing executables. 1...|$|R
40|$|High speed <b>cache</b> <b>memory</b> is {{commonly}} used to address the disparity between {{the speed of a}} computer's central processing unit and the speed of a computer's main memory. It is advantageous to maximize {{the amount of time that}} <b>data</b> spends in <b>cache.</b> Tiling is a software technique which is often used to do just this. Tiling is not able, however, to handle dynamically changing data structures, such as those encountered in adaptively chosen, unstructured grids. We develop a variant of the Gauss-Seidel method for second order elliptic partial differential equations with variable coefficients. This variant keeps <b>data</b> in <b>cache</b> <b>memory</b> for much longer than non-cache implementations. As a result, our method is significantly faster than noncache implementations. Examples from the structured grid case demonstrate the benefits of such a variant and provide motivation for the more difficult unstructured grid case. For this case, a publicly available load balancing package is used to decompose the grid [...] ...|$|R
50|$|Processor {{affinity}} {{takes advantage}} of the fact that remnants of a process that was run on a given processor may remain in that processor's state (for example, <b>data</b> in the <b>cache</b> <b>memory)</b> after another process was run on that processor. Scheduling that process to execute on the same processor improves its performance by reducing performance-degrading events such as cache misses. A practical example of processor affinity is executing multiple instances of a non-threaded application, such as some graphics-rendering software.|$|R
5000|$|... microAptiv is a compact, {{real-time}} {{embedded processor}} core with a five-stage pipeline and the microMIPS code compression instruction set. microAptiv {{can be either}} configured as a microprocessor (microAptiv UP) with instruction and <b>data</b> <b>caches</b> and a <b>Memory</b> Management Unit or as a microcontroller (microAptiv UC) with a Memory Protection Unit. The CPU integrates DSP and SIMD functionality to address signal processing requirements for entry-level embedded segments including industrial control, smart meters, automotive and wired/wireless communications.|$|R
40|$|We {{describe}} <b>cache</b> <b>memory</b> design {{suitable for}} use in FPGA-based cache controller and processor. Cache systems are on-chip memory element used to store <b>data.</b> <b>Cache</b> serves as a buffer between a CPU and its main <b>memory.</b> <b>Cache</b> <b>memory</b> is used to synchronize the data transfer rate between CPU and main <b>memory.</b> As <b>cache</b> <b>memory</b> closer to the micro processor, it is faster than the RAM and main memory. The advantage of storing <b>data</b> on <b>cache,</b> as compared to RAM, {{is that it has}} faster retrieval times, but it has disadvantage of on-chip energy consumption. In term of detecting miss rate in <b>cache</b> <b>memory</b> and less power consumption, The efficient <b>cache</b> <b>memory</b> will proposed by this research work, by implementation of <b>cache</b> <b>memory</b> on FPGA. We believe that our implementation achieves low complexity and low energy consumption in terms of FPGA resource usage. present in <b>cache</b> <b>memory</b> then the term is called „cache hit‟. The advantage of storing <b>data</b> on <b>cache,</b> as compared to RAM, is that it has faster retrieval times, but it has disadvantage of on-chip energy consumption. This paper deals with the design of efficient <b>cache</b> <b>memory</b> for detecting miss rate in <b>cache</b> <b>memory</b> and less power consumption. This <b>cache</b> <b>memory</b> may used in future work to design FPGA based cache controller...|$|R
40|$|International audienceIn this paper, a new {{methodology}} for computing the Dense Matrix Vector Multiplication, for both embedded (processors without SIMD unit) and general purpose processors (single and multi-core processors, with SIMD unit), is presented. This methodology achieves higher execution speed than ATLAS state-of-the-art library (speedup from 1. 2 up to 1. 45). This {{is achieved by}} fully exploiting {{the combination of the}} software (e. g., data reuse) and hardware parameters (e. g., <b>data</b> <b>cache</b> associativity) which are considered simultaneously as one problem and not separately, giving a smaller search space and high-quality solutions. The proposed methodology produces a different schedule for different values of the (i) number of the levels of data cache; (ii) <b>data</b> <b>cache</b> sizes; (iii) <b>data</b> <b>cache</b> associativities; (iv) <b>data</b> <b>cache</b> and main <b>memory</b> latencies; (v) data array layout of the matrix and (vi) number of cores...|$|R
