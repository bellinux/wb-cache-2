0|49|Public
50|$|There {{are three}} types of control records used {{to keep track of}} insertions and deletions applied in {{different}} deltas. They are insertion control <b>record,</b> <b>deletion</b> control <b>record</b> and end control record. Whenever a user changes some part of the text, a control record is inserted surrounding the change. These control records are stored in the body along with the original source code or text records.|$|R
40|$|The Frequent-Pattern-tree (FP tree) is an {{efficient}} data structure for association-rule mining without generation of candidate itemsets. It {{was used to}} represent a database into a tree structure which stored only frequent items. It, however, needed to process all transactions in a batch way. In the past, Hong etal. thus proposed {{an efficient}} incremental mining algorithm for handling newly inserted transactions. In addition to <b>record</b> insertion, <b>record</b> <b>deletion</b> from databases is also commonly seen in real-applications. In this paper, we thus attempt to modify the FP-tree construction algorithm for efficiently handling <b>deletion</b> of <b>records.</b> A fast updated FP-tree (FUFP-tree) structure is used, which makes the tree update process become easier. An FUFP-tree maintenance algorithm for the <b>deletion</b> of <b>records</b> is also proposed for reducing the execution time in reconstructing the tree when records are deleted. Experimental results also show that the proposed FUFP-tree maintenance algorithm for <b>deletion</b> of <b>records</b> runs faster than the batch FP-tree construction algorithm for handling deleted records and generates nearly the same tree structure as the FP-tree algorithm. The proposed approach can thus achieve a good trade-off between execution time and tree complexity. ...|$|R
40|$|My paper covers {{two aspects}} of MARC: 1) the MARC Distribution Service and 2) the MARC users themselves. The MARC Distribution Service is the {{arrangement}} by which the MARC data are sent off every week {{to each of the}} users. Each weekly issue of MARC is complete on one 300 -foot reel of magnetic computer tape. Thus each user receives one reel of tape containing the MARC data for that week and a printed listing showing the LC card order numbers in the shipment, their status (new, correction or deletion), and the number of new, correction and <b>deletion</b> <b>records,</b> plus a total record count. published or submitted for publicatio...|$|R
5000|$|Update and <b>deletion</b> of {{temporal}} <b>records</b> with automatic splitting and coalescing of time periods ...|$|R
5000|$|A record type, or table, can {{be defined}} as “{{circular}}.” With circular tables, when the table becomes full, RDM will still allow new record instances to be created. The new record instances will overwrite existing ones, starting with the oldest. RDM does not allow explicit <b>deletion</b> of <b>record</b> instances in a circular table.|$|R
30|$|Besides {{evaluating}} queries efficiently, many deployment scenarios {{also require}} the modification of outsourced data. While some proposed CPI approaches {{assume that the}} data is outsourced once without being modified at a future point in time, other approaches allow changes {{to be made to}} the outsourced data without harming its confidentiality. Modifications of the data may constitute insertions, updates or <b>deletions</b> of <b>records.</b>|$|R
40|$|Abstract:- The {{frequent}} pattern tree (FP-tree) is {{an efficient}} data structure for association-rule mining without generation of candidate itemsets. It, however, needed to process all transactions in a batch way. In addition to <b>record</b> insertion, <b>record</b> <b>deletion</b> is also commonly seen in real-application. In this paper, we propose {{the structure of}} prelarge trees for efficiently handling <b>deletion</b> of <b>records</b> based {{on the concept of}} pre-large itemsets. Due to the properties of pre-large concepts, the proposed approach does not need to rescan the original database until a number of records have been deleted. The proposed approach can thus achieve a good execution time for tree construction especially when a small number of records are deleted each time. Experimental results also show that the proposed approach has a good performance for incrementally handling deleted records...|$|R
50|$|A {{database}} (.DBF) file {{is composed}} of a header, data <b>records,</b> <b>deletion</b> flags, and an end-of-file marker. The header contains information about the file structure, and the records contain the actual data. One byte of each record is reserved for the deletion flag.|$|R
30|$|Furthermore, all {{services}} {{could benefit}} from a DNS cache in TO 1, as DNS is an ideal candidate for caching, and thus for performance gain, as it is designed as a hierarchical distributed naming system with DNS records having a long lifetime, typically {{in the order of}} a couple of hours. A negative cache, which maintains unresolvable records, could also be kept. As the slow propagation in the whole DNS system does not support fast addition and <b>deletion</b> of <b>records,</b> the cache should neither.|$|R
5000|$|In 2007, {{after many}} years of <b>deletion,</b> Sire <b>Records</b> finally elected to re-release both Bringing Home the Ashes and Space Flower, this time giving them a full UK release. Both albums were re-packaged as a 2-CD set called Magnitude, whose cover and artwork this time {{mirrored}} 1988's Wild Swans - Music and Talk From Liverpool album, complete with familiar Swan design. The album itself was released as part of Sire's April 2007 relaunch of the Korova label, alongside other re-releases from acts like Ian McCulloch and Electrafixion, all 2-CD sets with extra tracks.|$|R
50|$|FileMaker 11, {{released}} on March 9, 2010, introduced charting, which was further streamlined in FileMaker 12, released April 4, 2012. That version also added themes, more database templates (so-called starter solutions) and simplified creation of iOS databases. FileMaker Go 11 (July 20, 2010) and FileMaker Go 12 for iPhone and iPad (April 4, 2012) allow only the creation, modification, and <b>deletion</b> of <b>records</b> on these handheld devices. Design and schema changes {{must be made}} within the full FileMaker Pro application. FileMaker Go 12 offers multitasking, improved media integration, export of data to multiple formats and enhanced container fields.|$|R
50|$|Keylight 4.1 {{introduced}} the Keylight Ambassador. It {{was the first}} GRC platform to allow for both SAML and LDAP integration, the first to perform bulk tasks on data records, including data edits, workflow and <b>record</b> <b>deletion,</b> {{and the first to}} create ad-hoc reports on historic content. Keylight 4.1 also added support for syslog data collection.|$|R
2500|$|The {{justification}} {{used for}} this <b>deletion,</b> as <b>recorded</b> in the NCPC's [...] "Red Book", was that traffic with destinations beyond the District should {{not be permitted to}} travel through the District, but instead bypass it via the Beltway. Traffic with destinations within the District, as noted above, were expected to use the street grid; the NCPC further justified this by stating that the Interstate Highway System as defined did not guarantee direct inner-city access. Furthermore, the NCPC also baldly stated that the construction of the North Central Freeway would simply provide another way into the District for commuters, which was seen as undesirable and unnecessary.|$|R
40|$|Act, which {{among other}} things {{provided}} for the <b>deletion</b> of <b>records</b> of sokshing (woodlots) and tsamdro (pastures) from private and community land registers called lagthrams. These lagthrams are the records of categories and sizes of landholdings owned by Bhutanese families or communities. Records of sokshing and tsamdro owned by peasants are also reflected in these lagthrams. Why did the National Assembly decide to delete records of tsamdro and sokshing from the lagthrams? Its rationale was that sokshing and tsamdro lands belonged to the state although they were reflected in private lagthrams. In case of sokshing, it argued that lagthram-holders were only granted the „right ‟ to collect leaf litter for use as organic manure in agricultural fields. That is why it reasoned that peasants {{were not required to}} pay taxes for sokshing, whereas they pay taxes for other categories of their landholdings. They were also not allowed to cut down trees. The argument justifying the <b>deletion</b> of sokshing <b>records</b> from private lagthrams and I {{would like to thank the}} Members of National Council and its Natural Resource and Environment Committee for kindly commenting on the arguments of this paper and for the suggestions they made while the issue of sokshing and tsamdro were discussed in two sessions of the National Council...|$|R
40|$|We {{document}} widespread {{changes to}} the historical I/B/E/S analyst stock recommendations database. Across seven I/B/E/S downloads, obtained between 2000 and 2007, we find that between 6, 580 (1. 6 %) and 97, 582 (21. 7 %) of matched observations are different from one download to the next. The changes include alterations of recommendations, additions and <b>deletions</b> of <b>records,</b> and removal of analyst names. These changes are nonrandom, clustering by analyst reputation, broker size and status, and recommendation boldness, and affect trading signal classifications and back-tests of three stylized facts: profitability of trading signals, profitability of consensus recommendation changes, and persistence in individual analyst stock-picking ability. Copyright (c) 2009 the American Finance Association. ...|$|R
40|$|Data mining {{algorithms}} {{have been}} the focus of much research recently. In practice, the input data to a data mining process resides in a large data warehouse whose data is kept up-to-date through periodic or occasional addition and deletion of blocks of data. Most data mining algorithms have either assumed that the input data is static, or have been designed for arbitrary insertions and <b>deletions</b> of data <b>records...</b>|$|R
40|$|Abstract:- In the past, we {{proposed}} an incremental mining algorithm for maintenance of generalized association rules as new transactions were inserted. <b>Deletion</b> of <b>records</b> in databases is, however, commonly seen in real-world applications. In this paper, we thus attempt to extend our previous approach {{to solve this}} issue. The proposed algorithm maintains generalized association rules based {{on the concept of}} pre-large itemsets for deleted data. The concept of pre-large itemsets is used to reduce the need for rescanning original databases and to save maintenance costs. The proposed algorithm doesn't need to rescan the original database until a number of records have been deleted. It can thus save much maintenance time. Key-Words:- data mining, generalized association rule, taxonomy, large itemset, pre-large itemset. ...|$|R
40|$|We {{document}} widespread ex post {{changes to}} the historical contents of the I/B/E/S analyst stock recommendations database. Across a sequence of seven downloads of the entire I/B/E/S recommendations database, obtained between 2000 and 2007, we find that between 6, 594 (1. 6 %) and 97, 579 (21. 7 %) of matched observations are different from one download to the next. The changes, which include alterations of recommendation levels, additions and <b>deletions</b> of <b>records,</b> and removal of analyst names, are non-random in nature: They cluster by analyst reputation, brokerage firm size and status, and recommendation boldness. The changes have a large and {{significant impact on the}} classification of trading signals and back-tests of three stylized facts: The profitability of trading signals, the profitability of changes in consensus recommendations, and persistence in individual analyst stock-picking ability. ...|$|R
40|$|Mechanisms {{of missing}} data and methods are {{described}} in this thesis. Three mechanisms are considered - MCAR, MAR, MNAR. Two simple methods using <b>deletion</b> of incomplete <b>records</b> are shown and their properties and shortcomings are demonstrated. Secondly, the principle of simple imputations is explained. EM algorithm which uses the classical statistics and the algorithm of data augmentation which uses Bayesian framework are derived and compared. The last method included in the thesis is the multiple imputation. The described methods are compared on real data set, first on continuous variables and then on a contingency table. ...|$|R
40|$|Although {{the general}} {{concepts}} {{provided by the}} standard concurrency control the# ory #e. g. #BHG 87 ## remain a solid guide for systems behavior and design# a look at implementation#driven research #e. g. ARIES#IM #ML 92 ## shows {{that there is a}} non# trivial gap between the usual abstract model and practical DBMS situations. For exam# ple# most theoretical work on concurrency control has ignored access to redundant data stores# which are the basis for indices# and even simple operations such as insertion and <b>deletion</b> of <b>records</b> introduce unexpected subtleties in their correct concurrent handling. In this paper we propose extensions to the concepts of standard concurrency control to cover locking problems found in practice. Our results should help generalize previous proposals# understand the tradeo#s involved# and facilitate the development of robust implementations. 1 Introduction A basic service of database mangement systems #DBMSs# is to provide data access to multiple users [...] ...|$|R
50|$|Laravel 4, codenamed Illuminate, was {{released}} in May 2013. It was made as a complete rewrite of the Laravel framework, migrating its layout into a set of separate packages distributed through Composer, which serves as an application-level package manager. Such a layout improved the extendibility of Laravel 4, which was paired with its official regular release schedule spanning six months between minor point releases. Other new features in the Laravel 4 release include database seeding for the initial population of databases, support for message queues, built-in support for sending different types of email, and support for delayed <b>deletion</b> of database <b>records</b> called soft <b>deletion.</b>|$|R
30|$|Data mining {{technology}} is a very common computer technology, which has been widely used in many fields because of its superior performance. The method of talent management data cleaning in wireless sensor networks is studied based on data {{mining technology}}. The research status of data mining technology is first introduced at home and abroad, and the specific application forms of wireless sensor networks are analyzed. Then, the structure characteristics of wireless sensor networks are introduced, and a data cleansing technology is proposed based on clustering model. A cluster-based replication <b>record</b> <b>deletion</b> algorithm is proposed, and finally, the accuracy of data cleansing methods is verified. The {{results show that the}} research method of this paper is correct and effective.|$|R
5000|$|Despite his typical {{focus on}} {{military}} issues, Ryan advises President Durling {{to deal with}} the economic crisis first. Ryan also realizes that Japan's <b>deletion</b> of trade <b>records</b> could be an advantage in responding to the economic threat. He engineers a [...] "do-over", where all of the transactions that were deleted {{on the day of the}} mass deletion are ignored and all trade information is restored to its condition at noon of that day. Accompanied by a presidential address to the nation and behind-the-scenes bullying of investment banks, the plan is a success: the U.S. stock market is restored with only minor disruption. Concurrently, a group of U.S. investment banks start a massive economic unloading of Japanese investment products, effectively eliminating any gains made by the Zaibatsu.|$|R
40|$|Proteins of the PSD- 95 –like membrane-associated {{guanylate}} kinase (PSD-MAGUK) {{family are}} vital for trafficking AMPA receptors (AMPARs) to synapses, a process necessary for both basal synaptic transmission {{and forms of}} synaptic plasticity. Synapse-associated protein 97 (SAP 97) exhibits protein interactions, such as direct interaction with the GluA 1 AMPAR subunit, and subcellular localization (synaptic, perisynaptic, and dendritic) unique within this protein family. Due {{in part to the}} lethality of the germline knockout of SAP 97, this protein’s role in synaptic transmission and plasticity is poorly understood. We found that overexpression of SAP 97 during early development traffics AMPARs and NMDA receptors (NMDARs) to synapses, and that SAP 97 rescues the deficits in AMPAR currents normally seen in PSD- 93 /- 95 double-knockout neurons. Mature neurons that have experienced the overexpression of SAP 97 throughout development exhibit enhanced AMPAR and NMDAR currents, as well as faster NMDAR current decay kinetics. In loss-of-function experiments using conditional SAP 97 gene <b>deletion,</b> we <b>recorded</b> no deficits in glutamatergic transmission or long-term potentiation. These results support the hypothesis that SAP 97 is part of the machinery that traffics glutamate receptors and compensates for other PSD-MAGUKs in knockout mouse models. However, due to functional redundancy, other PSD-MAGUKs can presumably compensate when SAP 97 is conditionally deleted during development...|$|R
40|$|Clustering is {{the process}} of {{locating}} patterns in large data sets. As databases continue to grow in size, efficient and effective clustering algorithms play a paramount role in data mining applications. Traditional clustering approaches usually analyze static datasets in which objects are kept unchanged after being processed, but many practical datasets are dynamically modified which means some previously learned patterns have to be updated accordingly. Re-clustering the whole dataset from scratch is not a good choice due to the frequent data modifications and the limited out-of-service time, so the development of incremental clustering approaches is highly desirable. In this paper, we propose an incremental algorithm, IPYRAMID: Incremental Parallel hYbrid clusteRing using genetic progrAmming and Multiobjective fItness with Density employs a combination of data parallelism, genetic programming (GP), special operators, and multi-objective densitybased incremental fitness function. Although many incremental clustering algorithms have been proposed which can handle insertion of new record properly using incremental approach but cannot handle <b>deletion</b> of <b>record</b> properly. This issue is resolved in the proposed algorithm and density based incremental fitness function that helps to handle outliers. Use of parallelism increases the speed of execution as well as identifies clusters of arbitrary shapes. The incremental merge engine can dynamically determine the number of clusters. Preliminary experimental results show that it can increase the efficiency of clustering process...|$|R
40|$|BACKGROUND: Recent {{data from}} {{genome-wide}} chromosome conformation capture analysis {{indicate that the}} human genome is divided into conserved megabase-sized self-interacting regions called topological domains. These topological domains form the regulatory backbone of the genome and are separated by regulatory boundary elements or barriers. Copy-number variations can potentially alter the topological domain architecture by deleting or duplicating the barriers and thereby allowing enhancers from neighboring domains to ectopically activate genes causing misexpression and disease, a mutational mechanism that has recently been termed enhancer adoption. RESULTS: We use the Human Phenotype Ontology database to relate the phenotypes of 922 <b>deletion</b> cases <b>recorded</b> in the DECIPHER database to monogenic diseases associated with genes in or adjacent to the deletions. We identify combinations of tissue-specific enhancers and genes adjacent to the deletion and associated with phenotypes in the corresponding tissue, whereby the phenotype matched that observed in the deletion. We compare this computationally with a gene-dosage pathomechanism that attempts to explain the deletion phenotype based on haploinsufficiency of genes located within the deletions. Up to 11. 8 % of the deletions could be best explained by enhancer adoption {{or a combination of}} enhancer adoption and gene-dosage effects. CONCLUSIONS: Our results suggest that enhancer adoption caused by deletions of regulatory boundaries may contribute to a substantial minority of copy-number variation phenotypes and should thus be taken into account in their medical interpretation...|$|R
40|$|Regulations and {{societal}} expectations have recently expressed {{the need to}} mediate access to valuable databases, even by insiders. At {{one end of the}} spectrum is the approach of restricting access to information and on the other that of information accountability. The focus of the proposed work is effecting information accountability of data stored in databases. One way to ensure appropriate use and thus end-to-end accountability of such information is tamper detection in databases via a continuous assurance technology based on cryptographic hashing. In our current research we are working to show how to develop the necessary approaches and ideas to support accountability in high-performance databases. This will include the design of a reference architecture for information accountability and several of its variants, the development of forensic analysis algorithms and their cost model, and a systematic formulation of forensic analysis for determining when the tampering occurred and what data were tampered with. Finally, for privacy, we would like to create mechanisms for allowing as well as (temporarily) preventing the physical <b>deletion</b> of <b>records</b> in a monitored database. In order to evaluate our ideas we will design and implement an integrated tamper detection and forensic analysis system. This work will show that information accountability is a viable alternative to information restriction for ensuring the correct storage, use, and maintenance of databases. 1...|$|R
40|$|Poster {{won first}} place in the {{graduate}} division of Physical Sciences, Mathematics, Computer Engineering and Computer Science at GPSC Student Showcase 2011. Regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. At one end of the spectrum is the approach of restricting access to information and on the other that of information accountability. The focus of the proposed work is effecting information accountability of data stored in databases. One way to ensure appropriate use and thus end-to-end accountability of such information is tamper detection in databases via a continuous assurance technology based on cryptographic hashing. In our current research we are working to show how to develop the necessary approaches and ideas to support accountability in high performance databases. This will include the design of a reference architecture for information accountability and several of its variants, the development of forensic analysis algorithms and their cost model, and a systematic formulation of forensic analysis for determining when the tampering occurred and what data were tampered with. Finally, for privacy, we would like to create mechanisms for allowing as well as (temporarily) preventing the physical <b>deletion</b> of <b>records</b> in a monitored database. In order to evaluate our ideas we will design and implement an integrated tamper detection and forensic analysis system. This work will show that information accountability is a viable alternative to information restriction for ensuring the correct storage, use, and maintenance of databases...|$|R
40|$|Data mining {{algorithms}} {{have been}} the focus of much research recently. In practice, the input data to a data mining process resides in a large data warehouse whose data is kept up-to-date through periodic or occasional addition and deletion of blocks of data. Most data mining algorithms have either assumed that the input data is static, or have been designed for arbitrary insertions and <b>deletions</b> of data <b>records.</b> In this paper, we consider a dynamic environment that evolves through systematic addition or deletion of blocks of data. We introduce a new dimension called the data span dimension, which allows userdefined selections of a temporal subset of the database. Taking this new degree of freedom into account, we describe efficient model maintenance algorithms for frequent itemsets and clusters. We then describe a generic algorithm that takes any traditional incremental model maintenance algorithm and transforms it into an algorithm that allows restrictions on the data span di [...] ...|$|R
40|$|This is {{the first}} {{instalment}} of a two-part revision of the classical translation sections of the second edition of The Cambridge Bibliography of English Literature, Vols 2 – 3. The recent discontinuation of the revised edition of CBEL deprives the scholarly world of an up-to-date version of the most complete bibliography of its kind; this contribution makes good that loss for this topic. Over its eventual two parts 1550 – 1800 it runs to some 1, 500 items of translation for what might be held to constitute {{the golden age of}} the English classical translating tradition. Checking of existing entries in the listings has led to a large number of internal corrections, including <b>deletions,</b> but the <b>records</b> have been expanded by a net 20 %, with several minor classical authors added. As compared to the previous CBEL editions of the 1940 s, this reflects the availability of digital-era resources such as the English Short Title Catalogue...|$|R
40|$|Abstract—Y {{chromosome}} microdeletions are {{the most}} common genetic cause of male infertility and screening for these microdeletions in azoospermic or severely oligospermic men is now standard practice. Analysis of the Y chromosome in men with azoospermia or severe oligozoospermia has resulted in the identification of three regions in the euchromatic part of the long arm of the human Y chromosome (Yq 11) that are frequently deleted in men with otherwise unexplained spermatogenic failure. PCR analysis of microdeletions in the AZFa, AZFb and AZFc regions of the human Y chromosome is an important screening tool. The aim {{of this study was to}} analyse the type of microdeletions in men with fertility disorders in Slovakia. We evaluated 227 patients with azoospermia and with normal karyotype. All patient samples were analyzed cytogenetically. For PCR amplification of sequence-tagged sites (STS) of the AZFa, AZFb and AZFc regions of the Y chromosome was used Devyser AZF set. Fluorescently labeled primers for all markers in one multiplex PCR reaction were used and for automated visualization and identification of the STS markers we used genetic analyzer ABi 3500 xl (Life Technologies). We reported 13 cases of deletions in the AZF region 5, 73 %. Particular types of <b>deletions</b> were <b>recorded</b> in each region AZFa,b,c. The presence of microdeletions in the AZFc region was the most frequent. The study confirmed that percentage of microdeletions in the AZF region is low in Slovak azoospermic patients, but important from a prognostic view. Keywords—AZF, male infertility, microdeletions, Y chromosome. I...|$|R
40|$|In multi-version databases, updates and <b>deletions</b> of <b>records</b> by {{transactions}} require appending a {{new record}} to tables rather than performing in-place updates. This mechanism incurs non-negligible performance overhead {{in the presence of}} multiple indexes on a ta-ble, where changes need to be propagated to all indexes. Addition-ally, an uncommitted record update will block other active trans-actions from using the index to fetch the most recently committed values for the updated record. In general, in order to support snap-shot isolation and/or multi-version concurrency, either each active transaction is forced to search a database temporary area (e. g., roll-back segments) to fetch old values of desired records, or each trans-action is forced to scan the entire table to find the older versions of the record in a multi-version database (in the absence of specialized temporal indexes). In this work, we describe a novel kV-Indirection structure to en-able efficient (parallelizable) optimistic and pessimistic multi-version concurrency control by utilizing the old versions of records (at most two versions of each record) to provide direct access to the recent changes of records without the need of temporal indexes. As a result, our technique results in higher degree of concurrency by re-ducing the clashes between readers and writers of data and avoiding extended lock delays. We have a working prototype of our concur-rency model and kV-Indirection structure in a commercial database and conducted an extensive evaluation to demonstrate the benefits of our multi-version concurrency control, and we obtained orders of magnitude speed up over the single-version concurrency control. 1...|$|R
40|$|Estimated {{breeding}} value was calculated based on individual phenotype (SP), {{an index of}} individual phenotype and full- and half-sib family averages (SI), or Best Linear Unbiased Rediction (BLUP). Traits considered were litter size (LS), backfat (BF), and ADG. Estimated {{breeding value}}s were calculated using all data and after deletion of the poorest 5, 10, 15, or 20 % of the records for BF and ADG, or 4. 8, 8, 13, or 21 % of the records for LS. When ail data were used, expected genetic gain from BLUP was greater than for SP by 22, 7, and 31 % and greater than for SI by 10. 4, and 21 % for LS, BF, and ADG, respectively. Expected genetic gain was 4, 0, and 3 % lower for IS, BF, and ADG, respectively, for selection on breeding values estimated by SI after the poorest 20 % of the records were deleted compared with selection on estimates by SI using all the data. Genetic gain using BLUP on data with the poorest 20 % of the records deleted was reduced by 5, 2, and 8 % for LS, BF, and ADG, respectively, compared with genetic gain using BLUP on all the data. The advantage in genetic gain of BLUP, with 20 % of the poorest records deleted, over SP was 15, 5, and 21 % for LS, BF, and ADG, respectively. Although BLUP is affected {{to a greater degree}} by <b>deletion</b> of <b>records</b> than is SP or SI, selection of swine using BLUP on field data would improve response to selection over the use of SP or SI...|$|R
40|$|One-dimensional range queries, {{as one of}} {{the most}} basic type of queries in databases, have been studied {{extensively}} in the literature. For large databases, the goal is to build an external index that is optimized for disk block accesses (or I/Os). The problem is well understood in the static case. Theoretically, there exists an index of linear size that can answer a range query in O(1 + K/B) I/Os, where K is the output size and B is the disk block size, but it is highly impractical. In practice, the standard solution is the B-tree, which answers a query in O(log(B) N/M + K/B) I/Os on a data set of size N, where M is the main memory size. For typical values of N, M, and B, log(B) N/M can be considered a constant. However, the problem is still wide open in the dynamic setting, when insertions and <b>deletions</b> of <b>records</b> are to be supported. With smart buffering, it is possible to speed up updates significantly to o(1) I/Os amortized. Indeed, several dynamic B-trees have been proposed, but they all cause certain levels of degradation in the query performance, with the most interesting tradeoff point at O(1 /B log N/M) I/Os for updates and O(log N/M + K/B) I/Os for queries. In this article, we prove that the query-update tradeoffs of all the known dynamic B-trees are optimal, when log(B) N/M is a constant. This implies that one should not hope for substantially better solutions for all practical values of the parameters. Our lower bounds hold in a dynamic version of the indexability model, which is of independent interests. Dynamic indexability is a clean yet powerful model for studying dynamic indexing problems, and can potentially lead to more interesting lower bound results...|$|R
40|$|AbstractÐData mining {{algorithms}} {{have been}} the focus of much research recently. In practice, the input data to a data mining process resides in a large data warehouse whose data is kept up-to-date through periodic or occasional addition and deletion of blocks of data. Most data mining algorithms have either assumed that the input data is static, or have been designed for arbitrary insertions and <b>deletions</b> of data <b>records.</b> In this paper, we consider a dynamic environment that evolves through systematic addition or deletion of blocks of data. We introduce a new dimension, called the data span dimension, which allows user-defined selections of a temporal subset of the database. Taking this new degree of freedom into account, we describe efficient model maintenance algorithms for frequent itemsets and clusters. We then describe a generic algorithm that takes any traditional incremental model maintenance algorithm and transforms it into an algorithm that allows restrictions on the data span dimension. We also develop an algorithm for automatically discovering a specific class of interesting block selection sequences. In a detailed experimental study, we examine the validity and performance of our ideas on synthetic and real datasets. Index TermsÐData Mining, dynamic databases, evolving data, trends. ...|$|R
40|$|From the 2011 Census round, the {{population}} “usual residents” {{is defined as}} those who have lived or intend to live {{for a period of}} more than 12 months in their place of usual residence. Most of the register-based statistics describe the total number of registered persons in e. g. a population register at a particular moment or period. To estimate the non-registered population of usual residents, capture – recapture methods are available. By making a three list estimation, making restrictions to one day for the period-based registers and a short period for the event-based register, applying a stringent linkage method and <b>deletion</b> of erroneous <b>records,</b> most of the assumptions of the capture – recapture method are met. However, to estimate the number of usual residents, we need to divide the estimated total number of persons into those who stay longer than a year in The Netherlands and those who did not. In this paper we present a method to estimate the number of usual residents even though the residence duration is observed in only one of the lists. We apply the method to Dutch empirical data...|$|R
