13|10000|Public
30|$|Performance is {{typically}} {{referred to as}} one of the most important non-functional features. It directly relates to the <b>execution</b> <b>of</b> <b>requests</b> by the DMSs [38, 39]. Typical performance metrics are throughput and latency.|$|E
40|$|Database {{performance}} {{is an important}} aspect of database usability. Distributed real time database systems (DRTDBS) must be designed on all levels of database architecture to support timely <b>execution</b> <b>of</b> <b>requests.</b> The primary performance objective in DRTDBS is to minimize the number of missed deadlines. Due to the demanding nature of this objective, traditional approaches are inadequate. However, the research in DRTDBS has been mostly devoted to extending traditional transaction processing techniques to solve the issues important for the design of DRTDBS. In this environment, new policies/protocols must be designed to efficiently handle the transactions execution. Our works involves design of new priority assignment policies and commit protocols and comparison of their performances with existing policies/protocols. ...|$|E
40|$|Abstract—Web hotspot is {{a serious}} problem often {{experienced}} in case popular websites. It provides dramatic load spike in a website, which occurs when a huge number of users accessing the same website. A prominent solution to this problem is server load balancing. Dynamic load balancing involves allocation of requests to the server or processor dynamically when they arrive. For effective load balancing, a near-optimal schedule of incoming requests or processes must be determined “on-the-fly”, so that <b>execution</b> <b>of</b> <b>requests</b> can be completed in shortest possible time. So we have proposed a Genetic Algorithm based load balancing scheme which relies on a process scheduling policy. Genetic Algorithm provides to search for the optimal solution out a search of candidate solutions. It follows the survival-of-the-fittest principle, to achieve the optimal solution, through a number of generations. The proposed algorithm is evaluated for various population size and number of generations, to maximize the processor utilization of nodes / processors in the system...|$|E
5000|$|Exception Service: Generates {{information}} about any error existence during the <b>execution</b> <b>of</b> a <b>request.</b> Canigó offers {{a series of}} exceptions by default (BaseException, CoreException, ...). It also has an exceptions interception mechanism to avoid the excessive use of [...] "try-catch".|$|R
30|$|Mean Service Time (S) – {{characterizes the}} <b>execution</b> times <b>of</b> the <b>requests</b> that are uniformly {{distributed}} [39].|$|R
40|$|All {{practical}} applications contain {{some degree of}} nondeterminism. When such applications are replicated to achieve Byzantine fault tolerance (BFT), their nondeterministic operations must be sanitized to ensure replica consistency. To {{the best of our}} knowledge, only a specific type of nondeterminism, namely, the type whose values can be independently chosen by the primary and verified by other replicas prior to the <b>execution</b> <b>of</b> a <b>request,</b> has been successfully addressed under the Byzantine fault model. There are other types of nondeterminism in many {{practical applications}}. Some of them require the collaboration of all correct replicas, while others require a post-determination of a consistent set of nondeterministic values after the <b>execution</b> <b>of</b> a <b>request</b> at a particular replica. This paper points out the inadequacy of current approaches in handling such types of replica nondeterminism, and presents a systematic solution for the problem {{in the context of a}} unified BFT framework...|$|R
30|$|Queueing Networks. A {{queueing}} network can {{be described}} as a collection of queues interacting through request arrivals and departures. Each queue represents either a physical resource (e.g., CPU, network bandwidth, etc) or a software buffer (e.g., admission control, or connection pools). Cloud applications are often tiered and queueing networks can capture the interactions between tiers. An example of cloud management solutions exploiting queueing network models is [55], where the cloud service center is modeled as an open queueing network of multiclass single-server queues. PS scheduling is assumed at the resources to model CPU sharing. Each layer of queues represents the collection of applications supporting the <b>execution</b> <b>of</b> <b>requests</b> at each tier of the cloud service center. This model is used to provide performance guarantees when defining resource allocation policies in a cloud platform. Also, [56] uses a queueing network to represent a multi-tier application deployed in a cloud platform, and to derive an SLA-aware resource allocation policy. Each node in the network has exponential processing times and a generalized PS policy to approximate the operating system scheduling.|$|E
40|$|Quotidian system {{administration}} is often {{characterized by the}} fulfillment of common user requests, especially on sites that serve a variety of needs. User creation, group management, and mail alias maintenance are just three examples of the many repetitive tasks that can crowd the sysadmin’s day. Matters worsen when users neglect to provide necessary information for the job. They can grow bleakest, however, at volunteer-run or otherwise loosely-coordinated sites, where sysadmins often collectively hope {{for someone else to}} attend to the task. The Los Task Request System addresses all three problems. It mitigates user vagueness with web forms generated from XML parameter specification files. It skirts sysadmin sloth by requiring one simple review and approval step to set changes into motion. It then saves time by automatically executing commands tailored from user input. Amidst this convenience, cryptographic signatures on Los directives ensure that only administrators can alter the system. Overall, Los aims to make life easier for users and sysadmins by standardizing and streamlining the submission, review, and <b>execution</b> <b>of</b> <b>requests</b> for common system tasks...|$|E
30|$|We propose new {{transaction}} models (or schemes) {{in order}} to provide NoSQL databases with transactional aspects as well as to analyze the effects of transactions on data consistency and efficiency. Implementing transactions in NoSQL databases is not trivial. The 3 Vs characteristics of NoSQL databases make it difficult to apply transaction models. Similarly, the distinct underlying models and design principles significantly complicate the process of transaction processing in NoSQL databases. Building transaction models and implementing them involve various factors. Transactions must adhere to certain transaction properties or correctness criteria such as Atomicity, Consistency, Isolation, Durability (ACID), semantic atomicity, eventual consistency and so on [6]. They should cater for concurrent <b>execution</b> <b>of</b> <b>requests</b> accessing and modifying shared data. Locking, optimistic protocols, and Snapshot Isolation are some of the possible techniques used to ensure concurrency in databases. It is also required to provide ways for recovering transactions from failures {{in order to}} maintain the correctness of applications and data, for example, using forward or backward recovery mechanisms. Our work focuses on implementing transaction properties and the concurrency aspects. Recovery is crucial to transaction processing but it is beyond the scope of this paper.|$|E
30|$|Each thread {{can trigger}} {{a change in}} the number of {{resources}} and thus facilitate the auto-scaling operations. Once requests have been scheduled, they cannot be cancelled by the scheduler. The decision to accept or reject the request is sent back to the user. Thus, the broker guarantees <b>execution</b> <b>of</b> a <b>request</b> that has been accepted by the system, by the specified deadline.|$|R
3000|$|... cpu(op): This {{statistic}} {{gives the}} expected time <b>of</b> <b>execution</b> {{for a specific}} operator. In general, {{we assume that the}} <b>execution</b> <b>of</b> a <b>request</b> in a hybrid web application will be dominated by the CPU processing of the application- tier and the network latency. So in many cases, this statistic is negligible. However, we include it to detect the odd case of expensive query operations which can benefit from executing on the public cloud.|$|R
40|$|Web servers {{often become}} {{overloaded}} with irregular surges in web traffic. Several techniques have been explored {{to cope with}} these overloads such as distributing load throughout different servers. This thesis presents Pipelined Apache HTTP Server, {{a modified version of}} the Apache Software Foundation’s HTTP Server that utilizes a pipelined <b>execution</b> <b>of</b> Apache’s <b>request</b> cycle. We discuss Apache’s original architecture, the modifications necessary for implementation <b>of</b> pipelined <b>execution,</b> and analyze its run time. Ultimately, we hoped to increase throughput of Apache but fall short because <b>of</b> unbalanced <b>request</b> phases and pipelining overhead...|$|R
40|$|In this paper, we {{describe}} ZooKeeper, a service for coordinating processes of distributed applications. Since ZooKeeper {{is part of}} critical infrastructure, ZooKeeper aims to provide a simple and high performance kernel for building more complex coordination primitives at the client. It incorporates elements from group messaging, shared registers, and distributed lock services in a replicated, centralized service. The interface exposed by Zoo-Keeper has the wait-free aspects of shared registers with an event-driven mechanism similar to cache invalidations of distributed file systems to provide a simple, yet powerful coordination service. The ZooKeeper interface enables a high-performance service implementation. In addition to the wait-free property, ZooKeeper provides a per client guarantee of FIFO <b>execution</b> <b>of</b> <b>requests</b> and linearizability for all requests that change the ZooKeeper state. These design decisions enable {{the implementation of a}} high performance processing pipeline with read requests being satisfied by local servers. We show for the target workloads, 2 : 1 to 100 : 1 read to write ratio, that ZooKeeper can handle tens to hundreds of thousands of transactions per second. This performance allows ZooKeeper to be used extensively by client applications. ...|$|E
40|$|In {{distributed}} systems, it {{is difficult}} to predict whether the node is genuine or not and this may lead to the addition of selfish/ malicious nodes. The selfish/malicious nodes are the nodes that continue the execution of their codes for unknown/prolonged duration. For allotting the nodes with resources, the mutual exclusion algorithms, either token based, non-token based quorum-based or agreement based, do not keep check over the sites requesting execution. The selfish nodes try to sabotage the processing in the network by acquiring the resources. Hence, presence of a gap is observed {{from the point of view}} of security in the distributed systems. Therefore, to make the network resilient enough to prevent itself from these attacks, here we are proposing an algorithm which is an extension to the current algorithms, for allocation of resources to the nodes and execution of their requests with certain modifications in the request queue and its execution by using priority queue and round–robin scheduling for execution of all the sites in the distributed system. This will make the distributed systems safe from the <b>execution</b> <b>of</b> <b>requests</b> by malicious nodes and will work for both the monitored and the automated networks in the distributed environment...|$|E
40|$|Modern disk-storage {{systems have}} to {{accomplish}} the requirements {{of a variety of}} application classes. Applications that process continuous-media data such as video and audio streams require the storage system to guarantee sustained bandwidths. Interactive applications demand the storage system to ensure bounded response times, posing timing constraints on the execution of individual disk requests. Traditional timesharing applications may require both high throughput or overall short response times. With the described applications being more and more used together in todays computing systems, the disk-storage subsystems have to efficiently combine the different requirements of this application mix. In this thesis, I develop the design of a storage system that comprehensively addresses the various challenges posed by including the support for quality-of-service guarantees in disk-storage systems. The presented storage system provides three main properties. First, the admission control includes the support for statistical guarantees to increase the share of the disk bandwidth that can be utilized by the admission control. Second, the disk-request scheduling clearly separates the enforcement of real-time guarantees from the task to establish the optimal execution order of the requests, and it provides a flexible mechanism to combine the <b>execution</b> <b>of</b> <b>requests</b> with different quality-of-service requirements. Finally, the file system addresses both the needs of the former two elements of the storage system and of the various file types used by the applications by providing a flexible block-allocation policy and customized client interfaces. I show the implementation of the presented designs with the DROPS Disk-Storage System and I provide a detailed evaluation based on this implementation...|$|E
30|$|This {{approach}} works {{even when}} the start time and the request arrival time coincide by either locating a resource that can accommodate the new request immediately. If no suitable resources are found, the system either rejects the request, or if the request is deemed profitable by the broker, a new resource is acquired. If a new resource is acquired, there is an overhead to start the resource. In such a case, if the deadline cannot be met after delaying the start time <b>of</b> the <b>execution</b> <b>of</b> the <b>request,</b> the request is rejected.|$|R
30|$|The {{research}} {{investigated by}} Sambasivan et al. [3], proposes {{an approach to}} find, categorize and compare similar <b>execution</b> flows <b>of</b> different <b>requests</b> to diagnose performance changes. Their way of extracting similarities between different requests comprises some similarity to our method. However, our solution {{can be used in}} different purposes, from comparing the different execution flows to understanding the overall <b>execution</b> <b>of</b> VMs and extracting the relations between the different <b>executions</b> <b>of</b> different processes of the VMs and the host machine.|$|R
40|$|MDBAS is a {{prototype}} of a multidatabase management system based on mobile agents. The system integrates a set of autonomous databases distributed over a network, enables users to create a global database scheme, and manages transparent distributed <b>execution</b> <b>of</b> user <b>requests</b> and procedures including distributed transactions. This paper highlights the issues related to mobile database procedures, especially the MDBAS execution strategy. In order to adequately assess MDBAS's qualities and bottlenecks, we have carried out complex performance evaluation with real databases distributed in real Internet. The evaluation included a comparison to a commercial database with distributed database capabilities. The most interesting results are presented and commented. 1...|$|R
40|$|AbstractThe PanDA (Production and Distributed Analysis) {{workload}} {{management system}} (WMS) {{was developed to}} meet the scale and complexity of LHC distributed computing for the ATLAS experiment. PanDA currently distributes jobs among more than 100, 000 cores at well over 120 Grid sites, supercomputing centers, commercial and academic clouds. ATLAS physicists submit more than 1. 5 M data processing, simulation and analysis PanDA jobs per day, and the system keeps all meta-information about job submissions and execution events in Oracle RDBMS. The above information is used for monitoring and accounting purposes. One of the most challenging monitoring issues is tracking errors that has occurred during the execution of the jobs. Current meta-data storage technology doesn’t support inner tools for data aggregation, needed to build error summary tables, charts and graphs. Delegating these tasks to the monitor slows down the <b>execution</b> <b>of</b> <b>requests.</b> We will describe a project aimed at optimizing interaction between PanDA front-end and back-end, by meta-data storage segmentation into two parts – operational and archived. Active meta-data are remained in Oracle database (operational part), due to the high requirements for data integrity. Historical (read-only) meta-data used for the system analysis and accounting are exported to NoSQL storage (archived part). New data model based on usage of Cassandra as the NoSQL backend has been designed as a set of query-specific data structures. This allowed to remove most of data preparation workload from PanDA Monitor and improve its scalability and performance. Segmentation and synchronization between operational and archived parts of jobs meta-data is provided by a Hybrid Meta-data Storage Framework (HMSF). PanDA monitor was partly adopted to interact with HMSF. The operational data queries are forwarded to the primary SQL-based repository and the analytic data requests are processed by NoSQL database. The results of performance and scalability tests of HMSF-adopted part of PanDA Monitor shows that presented method of optimization, in conjunction with a properly configured NoSQL database and reasonable data model, provides performance improvements and scalability...|$|E
40|$|This project {{examines}} {{the process of}} planning, management and allocation of resources required {{for the implementation of}} the Ugandan Universal Primary Education program. It commences with an introduction to the UPE program along with a detailed highlight of major education statistics in Uganda. It also highlights the importance of the UPE and aligns it with the global and in-country millennium development goal aimed at ensuring that the world’s children are able to complete a full course of primary schooling by 2015. The literature review encompasses a utilization of the basic economic province of the 5 W’s and H, and uses it to link with the research question in order to generate the theories models and concepts necessary to analysing the issues and problems which is dependent on data gathered from primary first hand sources involved with the allocation of resources relevant to the Implementation of the UPE program. Details are presented on the internal stakeholders of the programs implementation which primarily include the Information and Documentation Centre within the Ministry of Education and Sports in Uganda. From utilizing concepts of stakeholder interaction and IT business process modelling, it is found that the IDC serves as the main unit as all inbound and outbound information concerned with the Primary education implementation lays in the units’ control. The Minister of state for primary education serves as the direct overseer of the program, but delegates’ majority of the implementation duties to the IDC. This concentration of control proves to be detrimental as request for resources made by the dependent stakeholders end-up inheriting a unilateral flow of information and execution from request to execution. The poor state of the paper based ICT enabled infrastructure utilized by both internal and external stakeholders also proves incapable of handling the planning for the allocation of required resources thus leading to the need for improved ICT enabled infrastructure, and processes driven by IT to develop on the implementation of the program especially the <b>execution</b> <b>of</b> <b>requests</b> made by external stakeholders for much needed consumable, welfare and infrastructural resources. Conclusions are made to justify the relevance of ICT enabled infrastructure as well as IT driven processes including the potential for value addition. It is found that the benefits of using IT driven processes can help in discovering other problems that exist from in depth analysis often yielding in the need to change existing processes to align with goals set-up by the managers of the program...|$|E
40|$|Replication, {{a common}} {{approach}} to protecting applications against failures, refers to maintaining several {{copies of a}} service on independent machines (replicas). Unlike a stand-alone service, a replicated service remains available to its clients despite the failure {{of some of its}} copies. Consistency among replicas is an immediate concern raised by replication. In effect, an important factor for providing the illusion of an uninterrupted service to clients is to preserve consistency among the multiple copies. State-machine replication is a popular replication technique that ensures consistency by ordering client requests and making all the replicas execute them deterministically and sequentially. The overhead of ordering the requests, and the sequentiality of request execution, the two essential requirements in realizing state-machine replication, are also the two major obstacles that prevent the performance of state-machine replication from scaling. In this thesis we concentrate on the performance of state-machine replication and enhance it by overcoming the two aforementioned bottlenecks, the overhead of ordering and the overhead of sequentially executing commands. To realize a truly scalable system, one must iteratively examine and analyze all the layers and components of a system and avoid or eliminate potential performance obstructions and congestion points. In this dissertation, we iterate between optimizing the ordering of requests and the strategies of replicas at request execution, in order to stretch the performance boundaries of state-machine replication. To eliminate the negative implications of the ordering layer on performance, we devise and implement several novel and highly efficient ordering protocols. Our proposals are based on practical observations we make after closely assessing and identifying the shortcomings of existing approaches. Communication {{is one of the most}} important components of any distributed system and thus selecting efficient communication patterns is a must in designing scalable systems. We base our protocols on the most suitable communication patterns and extend their design with additional features that altogether realize our protocol's high efficiency. The outcome of this phase is the design and implementation of the Ring Paxos family of protocols. According to our evaluations these protocols are highly scalable and efficient. We then assess the performance ramifications of sequential <b>execution</b> <b>of</b> <b>requests</b> on the replicas of state-machine replication. We use some known techniques such as state-partitioning and speculative execution, and thoroughly examine their advantages when combined with our ordering protocols. We then exploit the features of multicore hardware and propose our final solution as a parallelized form of state-machine replication, built on top of Ring Paxos protocols, that is capable of accomplishing significantly high performance. Given the popularity of state-machine replication in designing fault-tolerant systems, we hope this thesis provides useful and practical guidelines for the enhancement of the existing and the design of future fault-tolerant systems that share similar performance goals...|$|E
40|$|A {{problematic}} {{aspect of}} software management systems {{in view of}} integrity preservation is the handling, approval, tracking and eventual <b>execution</b> <b>of</b> change <b>requests.</b> In {{the context of the}} relation between clients and repositories, trust can help identifying all packages required by the intended installation. Negative trust, in turn, can be used to approach the complementary problem induced by removing packages. In this paper we offer a logic for negative trust which allows to identify admissible and no-longer admissible software packages in the current installation profile in view of uninstall processes. We provide a simple working example and the system is formally verified using the Coq theorem prover...|$|R
30|$|When {{the broker}} {{received}} a new request via RH, it used MMS {{to determine whether}} the request could be scheduled on one of the resources available to the broker. Scheduling <b>of</b> the <b>request</b> was performed as explained in “Matchmaking and scheduling” section. The decision to accept or reject a request was sent back to the user using the REST framework. If MMS allowed the request to be scheduled, the broker sent the request to the chosen resource via REST, where it was queued. The matchmaker would select the first resource in its queue and start executing a request after its earliest start time. The <b>execution</b> <b>of</b> a <b>request</b> was emulated by running a for-loop to keep the CPU busy for the predetermined service time duration <b>of</b> the <b>request.</b> Once the request finished execution on the resource, the resource informed the broker <b>of</b> the <b>request</b> completion.|$|R
50|$|Hours after Gorsuch {{and four}} other Supreme Court conservatives justices voted on April 20 to deny a stay <b>of</b> <b>execution</b> <b>request</b> from eight inmates on Arkansas death-row, Ledell Lee was put {{to death with a}} lethal injection, the first in Arkansas since 2005. Two inmates—Jack Jones and Marcel Williams—received lethal {{injections}} on April 24.|$|R
40|$|Web {{services}} middleware {{are typically}} designed optimised for throughput. Requests are accepted unconditionally and no differentiation {{is made in}} processing. Many use the thread-pool pattern to execute requests in parallel using processor sharing. Clusters hosting web services dispatch requests only to balance out the load among the executors. Such optimisations for throughput work out negatively on the predictability of execution. Processor sharing results in the increase of execution time {{with the number of}} concurrent requests, making it impossible to predict or control the execution of a request. Existing works fail to address the need for predictability in web service execution. Some achieve a level of differentiated processing, but fail to consider predictability as their main quality attribute. Some give a probabilistic guarantee on service levels. However, from a predictability perspective they are inconsistent. A few achieve predictable execution times, though only in closed systems where request properties are known at system design time. Web services operate on the Internet, where request properties are relatively unknown. This thesis investigates the problem of achieving predictable times in web service executions. We introduce the notion of a processing deadline for service execution, which the web services engine must adhere to in completing the request in a repeatable and a consistent manner. Reaching such execution deadlines by the services engine is made possible by three main features. Firstly a deadline based scheduling algorithm introduced, ensures the processing deadlines are followed. A laxity based analytical model and an admission control algorithm it is based on, selects requests for execution, resulting in a wider range of laxities to enable more requests with overlapping executions to be scheduled together. Finally, a real-time scheduler component introduced in to the server uses a priority model to schedule the <b>execution</b> <b>of</b> <b>requests</b> by controlling the execution of individual worker threads in custom-made thread pools. Predictability of execution in cluster based deployments is further facilitated by four dispatching algorithms that consider the request deadlines and laxity property in the dispatching process. A performance model derived for a similar system approximates the waiting time where requests with smaller deadlines (having higher priority) experience smaller waiting times than requests with longer deadlines. These techniques are implemented in web services middleware in standalone and cluster-based configurations. They are evaluated against their unmodified versions and techniques such as round-robin and class based dispatching, to measure their predictability gain. Empirical evidence indicate the enhancements enable the middleware to achieve more than 90 % of the deadlines, while accepting at least 20 % of the requests in high traffic conditions. The enhancements additionally prevent the middleware from reaching overloaded conditions in heavy traffic, while maintaining comparable throughput rates to the unmodified versions of the middleware. Analytical and simulation results for the performance model confirms that deadline based preemptive scheduling results in a better balance of waiting times where high priority requests experience lower waiting times while lower priority requests are not over-starved compared to other techniques such as static priority ordering, First-Come-First-Served, Round-Robin and non-preemptive deadline based scheduling...|$|E
40|$|To {{achieve a}} good {{balancing}} concept in an cluster servers, task allocation plays a important role. The {{aim of the}} paper is to minimize the execution time, produces better batch completion time (avoids delay in <b>execution</b> <b>of</b> a <b>request)</b> and improve load balancing at workstations. This also enable the system to adopt sudden arrival of large batch and to provide the equal treatment to all the processors. Here the tasks are batched and scheduled on the web clusters based on the content types to improve overall system performance. The aim {{of this paper is}} to optimize the <b>execution</b> time <b>of</b> a server, increasing reliability improving QOS and optimizing the system performance. These are achieved by carrying simulations on the proposed method...|$|R
40|$|Web Services has instigated it’s transcend and now {{education}} has been made simple through Web Services. With the advent of Web Services, {{education has}} become far more personal, flexible and available across global borders. Workflow is a sequence of business tasks to be realized for the <b>execution</b> <b>of</b> user’ <b>request.</b> Identifying required e-learning web services and dynamic composition and realization of those services is a challenging process. In this study we have suggested e-learning services workflow composing architecture and relevant algorithms for matching and composing e-learning flow for the learners with different learning styles. We suggested non logic based hybrid matching and composing algorithms which uses OWL-S profile and process ontologies for dynamic workflow composition of e-learning web services...|$|R
40|$|Part 3 : Applications of TrustInternational audienceA {{problematic}} {{aspect of}} software management systems {{in view of}} integrity preservation is the handling, approval, tracking and eventual <b>execution</b> <b>of</b> change <b>requests.</b> In {{the context of the}} relation between clients and repositories, trust can help identifying all packages required by the intended installation. Negative trust, in turn, can be used to approach the complementary problem induced by removing packages. In this paper we offer a logic for negative trust which allows to identify admissible and no-longer admissible software packages in the current installation profile in view of uninstall processes. We provide a simple working example and the system is formally verified using the Coq theorem prover...|$|R
40|$|Coordination of {{parallel}} activities on a shared memory machine {{is a crucial}} issue for modern software, even more {{with the advent of}} multi-core processors. Unfortunately, traditional concurrency abstractions force programmers to tangle the application logic with the synchronization concern, thereby compromising understandability and reuse, and fall short when fine-grained and expressive strategies are needed. This paper presents a new concurrency abstraction called POM, Parallel Object Monitor, supporting expressive means for coordination {{of parallel}} activities over one or more objects, while allowing a clean separation of the coordination concern from application code. Expressive and reusable strategies for concurrency control can be designed, thanks to a full access to the queue <b>of</b> pending <b>requests,</b> parallel <b>execution</b> <b>of</b> dispatched <b>requests</b> together with after actions, and complete control over reentrancy. A small domainspecific aspect language is provided to adequately configure pre-packaged, off-the-shelf synchronizations...|$|R
30|$|The core {{functionality}} of the PaaSHopper middleware is offering fine-grained {{control over the}} <b>execution</b> <b>of</b> a tenant’s <b>request</b> using policies. These policies describe the constraints and rules of the different stakeholders in a declarative way, extracting them from the application code in a modular and reusable way. Based on the different tenant- and provider-specific policies, the middleware decides where in the multi-cloud a task will be executed or data will be stored. Thus, the <b>execution</b> flow <b>of</b> the multi-cloud application has to continuously adapt based on the context.|$|R
40|$|In this paper, we {{analyze and}} {{experimentally}} compare state-machine-based and deferred-update (or transactional) replication, both relying on atomic broadcast. We define {{a model that}} describes {{the upper and lower}} bounds on the <b>execution</b> <b>of</b> concurrent <b>requests</b> by a service replicated using either scheme. The model is parametrized by the degree of parallelism in either scheme, the number of processor cores, and the type <b>of</b> <b>requests.</b> We use our model to make a comparison with a non-replicated service, considering separately abcast- and request-execution-dominant workloads. To evaluate transactional replication experimentally, we developed Paxos STM—a novel fault-tolerant distributed software transactional memory with programming constructs for transaction creation, abort, and retry. We used JPaxos for state-machine-based replication. Both systems share the same implementation of atomic broadcast built on the Paxos algorithm. We present the results of performance evaluation of both replication schemes, and a non-replicated (thus prone to failures) service, considering various workloads. The key result of our theoretical and experimenta...|$|R
40|$|The {{most common}} problem of an {{overloaded}} electronic-commerce server {{is an increase}} in the response time perceived by customers, who may restart their requests hoping to get a faster response, or simply abort them, giving up on the store. Both behaviors generate #orphan"requests: although they werereceived by the server, they should not beansweredbecause their requestors have already abandoned them. Orphan requests waste system resources, since the server becomes aware of their cancellation only when it tries to send a response and #nds out that the connection was closed. In this paper we propose a new kernel service, the Connection Sentry, which keeps track <b>of</b> <b>requests</b> being performed and notify processes about an eventual cancellation. Once noti#ed, aprocess can interrupt the <b>execution</b> <b>of</b> the <b>request,</b> saving system resources and bandwidth. We evaluated the gains by using our proposal in a virtual bookstore, where we observed that the Connection Sentry reduced service latency by up to [...] ...|$|R
40|$|Cloud {{computing}} is {{an advance}} computing paradigm in the Information Technology domain. It is usually {{used to describe}} the large scale distributed infrastructure, platform and software services provided by cloud service provider. Resource scheduling is one of the important tasks in cloud computing because it is involved with multiple resources like both hardware as well as software. In this paper we have described various resource scheduling algorithms used for cloud computing environment and also proposed design of new resource scheduling algorithm for cloud computing environment. So it can achieve more improvement in the resource scheduling process. This algorithm improves availability in cloud computing environment for resource scheduling as comparative with other standard algorithms. The efficiency <b>of</b> the user <b>request</b> will first be optimized and then processor executes the request. Finally it specifies fast response and <b>execution</b> <b>of</b> user <b>request...</b>|$|R
40|$|A cloud {{environment}} {{is one of}} the most shareable environments where multiple clients are connected to the common environment to access the services and the products. A cloud environment can be public or the private cloud. In such case, some approach is required to perform the effective scheduling and the resource allocation. The situation becomes critical when the cloud server is overloaded, in such case to provide the effective service to the client, the process migration is done. The migration is the concept to switch the allocated process to some other virtual machine or the cloud to release the load and to perform the effective <b>execution</b> <b>of</b> cloud <b>requests.</b> The obtained results shows the effective allocation of the process to the associated cloud in overload and the under load conditions...|$|R
40|$|This paper {{presents}} a novel Network Request Scheduler (NRS) for a large-scale, Lustre TM storage system. It proposes a quantum-based, Object Based Round Robin (OBRR) NRS algorithm that reorders the <b>execution</b> <b>of</b> I/O <b>requests</b> per data object, presenting a workload to backend storage {{that can be}} optimized more easily. According to the drawback of static deadlines in large-scale workloads, it proposes a novel, two-level deadline setting strategy that not only avoids starvation, but also guarantees that urgent I/O requests are serviced in a specified time period. Via a series of simulation experiments using a Lustre simulator, it demonstrates that I/O performance increases as much as 40 % when using the OBRR NRS algorithm, and the two-level deadline setting strategy can avoid starvation and ensure that urgent I/O requests are serviced in the required time...|$|R
50|$|Windows Vista also {{implements}} I/O scheduling as prioritized I/O. Disk I/O requests in Windows Vista {{are assigned}} priorities; {{a higher priority}} request is given preferential treatment, over a request that has a lower priority, during the <b>execution</b> <b>of</b> the <b>request.</b> Windows Vista defines five priority classes - Very Low, Low, Normal, High and Critical. By default I/O requests are assigned Normal priority. Windows Vista also allows reservation of bandwidth on a per-application basis during disk access; this aims to guarantee the required throughput rate to the application when it accesses the disk. Both these features are used by Windows Media Player with respect to media playback. In addition, the following applications and components use prioritized I/O: Disk Defragmenter, SuperFetch, Windows Defender, Windows Search, and applications that run at startup.|$|R
