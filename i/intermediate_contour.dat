8|18|Public
40|$|A {{new method}} for {{recovering}} shape from cross-sectional contours with complex branching structures is presented. First, each branching problem by providing an <b>intermediate</b> <b>contour</b> using distance function and image processing technology is solved. Then, all contours {{are divided into}} several groups of simple contours. For each group, a NURBS curve is fitted to contour points in each section within a given accuracy on a common knot vector. Finally, the NURBS surface skinning of these contours is performed for providing a smooth geometric model. The method is suitable to reproduce the object by NC machining or rapid prototyping. Some results demonstrate its usefulness and feasibility...|$|E
40|$|We {{present a}} {{technique}} {{for creating a}} digital elevation model (DEM) from grid-based contour data. The method computes new, intermediate contours in between existing isolines. These are found by finding the shortest line segment that connects points on two neighboring contours with differing elevations. The midpoint of the line segment becomes a point on the <b>intermediate</b> <b>contour.</b> The contours are completed by connecting individual points. The new contours are then used as data for successive iterations, until an initial surface is formed. Peaks are computed by Hermite splines that follow the slope trend. Gaussian smoothing {{is applied to the}} entire surface or only to newly computed elevations, yielding an approximated or interpolated surface, respectively. The DEMs are tested with quantitative methods, and are shown to compare favorably to well established algorithms...|$|E
40|$|Straight {{skeleton}} (Angular Bisector Network, ABN) of a planar polygon, {{which can}} be grasped as a modi cation of a planar Voronoi diagram without parabolic arcs, has been successfully used by Oliva et al. [5] {{as a part of}} a system for three dimensional reconstruction of objects from a given set of 2 D contours in parallel cross sections. The algorithm itself is used for the construction of <b>intermediate</b> <b>contour</b> layers during the reconstruction process, in order not to create self intersected surface triangles or a surface with holes. But { Oliva's algorithm is not publicly available and we have not found any other useful code on the net. We have followed our older ideas [4] and implemented our version of straight skeleton. Our algorithm runs in O(nm + n log n) time, where n denotes the total number of polygon vertices and m the number of re ex ones. ...|$|E
40|$|Given a {{sequence}} of object contours at certain time interval, a tool is developed to generate <b>intermediate</b> <b>contours.</b> For application domain like 3 D organ visualization, <b>intermediate</b> <b>contours</b> are generated in between two spatially apart contours of 2 D slice images of a 3 D organ. For meteorological applications, the proposed algorithm can generate intermediate shapes of, for example, a developing storm, when satellite images are taken at considerably apart time points. The interpolated contours between an initial and a target contour act as a missing link and establish a path along which contour deformation has taken place. The proposed technique of interpolation is an offline approach {{and is based on}} level set analysis. The initial contour, embedded in a level set function, is deformed using a speed function derived by integrating contour expansion-contraction force as well as the shape and the intensity based features of an image sequence. The extent of overlap between the initial and final contours generates the expansion-contraction force. The curvature of the initial contour that gradually deforms to final contour is taken as the shape feature. The optic flow vectors between initial and final images provide intensity-based feature. The efficacy of the proposed approach is demonstrated with real life examples from image sequences of cloud and medical images...|$|R
40|$|We present two new {{pre-processing}} {{techniques that}} improve thin plate Digital Elevation Model (DEM) approximations from grid-based contour data. One method computes gradients from an initial interpolated or approximated surface. The aspects {{are used to}} create gradient lines that are interpolated using Catmull-Rom splines. The computed elevations {{are added to the}} initial contour data set. Thin plate methods are applied to all of the data. The splines allow information to flow across contours, improving the final surface. The second method successively computes new, <b>intermediate</b> <b>contours</b> in between existing isolines, which provide additional data for subsequent thin plate processing. Both methods alleviate artifacts visible in previous thin plate methods. The surfaces are tested with published methods to show qualitative and quantitative improvements over previous methods. 2...|$|R
50|$|Of course, to {{determine}} differences in elevation between two points, the contour interval, or distance in altitude between two adjacent contour lines, must be known, {{and this is}} normally stated in the map key. Usually contour intervals are consistent throughout a map, but there are exceptions. Sometimes <b>intermediate</b> <b>contours</b> are present in flatter areas; these can be dashed or dotted lines at half the noted contour interval. When contours are used with hypsometric tints on a small-scale map that includes mountains and flatter low-lying areas, {{it is common to}} have smaller intervals at lower elevations so that detail is shown in all areas. Conversely, for an island which consists of a plateau surrounded by steep cliffs, it is possible to use smaller intervals as the height increases.|$|R
40|$|This {{document}} {{presents a}} technique {{based on the}} JBIG algorithm for binary shape coding in both lossless and lossy modes. Because it is applied directly to the bitmap representing the shape information, it bypasses the overhead in computation of an <b>intermediate</b> <b>contour</b> representation and its associated conversions. This leads to a simpler algorithm which is more suitable for a larger class of shape data. In addition a mechanism is proposed which allows a rate control for lossy coding mode. 1. INTRODUCTION Second generation video coding algorithms are {{also referred to as}} object oriented, that is, the scene to be coded is segmented into several regions, each of them coded separately. These regions generally identify objects. Each object is represented by four channels: three color channels and an alpha channel which defines the shape of the object. This alpha channel can be either binary or multilevel. The multilevel case allows for semi-transparent object. However the scope of this pape [...] ...|$|E
40|$|A Raster-DEM {{derived from}} {{interferometric}} {{synthetic aperture radar}} (InSAR) AeS- 1 data, operated by AeroSensing Radarsysteme GmbH, serves as input for a contour line extraction software system. Main processing tasks are extracting and generalization of vector contour lines and inscription for given equidistance. The contour lines are organized in point and polygon lists. Various algorithms operate on the polygon/point list data structure, e. g. breaking of contours touching the map border, removal of small contours by thresholding {{on the number of}} tie points. The core of the program is the generalization module performing line smoothing and generalization. Three different approaches have been implemented: take each Nth point, Pavlidis and the Douglas-Peuker algorithm. Another important capability is automatic generation of contour line inscription. Graphic attributes for lines and inscriptions, especially font type and size, type and thickness, inscription insertion point and angle are derived from the map scale requirements and local line properties. The resulting output in DXF-format consists of three layers: main and <b>intermediate</b> <b>contour</b> lines and the text. Usable contour lines derived from InSAR-DEMs for topographic maps at large and small scales and in an appealing graphic style are automatically derivable. For generalization, the Douglas-Peuker algorithm produces the best results. Despite of the large amount of data, computational effort with respect to time and cost becomes moderate due to efficient vector processing and the inexpensive PC-based Linux platform used. The DXF output format is a common vector format which {{can be used in a}} large number of DTP and GIS software...|$|E
40|$|Derivation of {{contour lines}} from a Digital Elevation Model (DEM) has {{to cope with}} a number of problems. (1) bad {{approximation}} of angular lines, (2) none or badly placed line inscriptions, (3) discontinuity when combining contour line sets of adjacent map sheets. We present a software system capable of dealing with the mentioned problems. A RasterDEM derived from interferometric synthetic aperture radar (InSAR) AeS- 1 data, operated by AeroSensing Radarsysteme GmbH, serves as input for the software system. Prior to vector processing of the contour lines, DEM smoothing and local peak detection are perfomed on the raster data. Contour lines are organized in point and polygon lists. Various algorithms operate on the polygon/point list data structure, e. g. breaking of contours touching the map border, removal of small contours by thresholding on the number of tie points. The core of the program is the generalization module performing line smoothing and generalization. Three different approaches have been implemented: take each Nth point, Pavlidis and the Douglas-Peuker algorithm. The latter performed best with respect to contour line quality / time consumption tradeoff. To ensure continuity between map sheets, an overlap area forming a skirt on all four borders is constructed and contour lines are cutted in appropriate fashion. Another important capability is automatic generation of line inscription. Graphic attributes for lines and inscriptions, especially font size, line type and thickness, inscription insertion point and angle are derived from the map scale requirements and local line properties. The resulting DXF-file consists of three layers: the main contour lines, the <b>intermediate</b> <b>contour</b> lines and the text, i. e. the line inscription. Further use of resulting DXF-data [...] ...|$|E
40|$|Isolines {{have proved}} to be a highly {{effective}} way of conveying the shape of a surface (most commonly in the form of height contours to convey geographical landscape). Selecting the right contour interval is a compromise between showing sufficient detail in flat regions, whilst avoiding excessive crowding of lines in steep and morphologically complex areas. The traditional way of avoiding coalescence and confusion across steep regions has been to manually remove short sections of <b>intermediate</b> <b>contours,</b> while retaining index contours. Incorporating humans in automated environments is not viable. This research reports on the design, implementation and evaluation of an automated solution to this problem involving the automatic identification of coalescing lines, and removal of line segments to ensure clarity in the interpretation of contour information. Evaluation was made by subjective comparison with Ordnance Survey products. The results were found to be very close to the quality associated with manual techniques...|$|R
40|$|We {{propose a}} novel {{approach}} to generate <b>intermediate</b> <b>contours</b> given a sequence of object contours. The proposal unifies shape features through contour curvature analysis and motion between the contours through optic flow analysis. The major contribution of this work is in integrating this shape and image intensity based contour interpolation scheme in a level set framework. The interpolated contours between an initial and a target contour act as a missing link and establish a path along which contour deformation has taken place. We have shown that for different application domains like, 3 D organ visualization (generation of contours between two spatially apart contours of 2 D slice images of a 3 D organ), meteorological applications of tracing {{the path of a}} developing cyclone (when satellite images are taken at distant time points and the shape of cyclone in between two consecutive satellite images are of interest) the proposal has outperformed the competing approaches...|$|R
40|$|The shape {{reconstruction}} of a 3 D object from its 2 D cross-sections {{is important for}} reproducing it by NC machining or rapid prototyping. In this paper, we present method of surface approximation to cross-sections with multiple branching problems. In this method, we first decompose each multiple branching problem into a set of single branching problems by providing a set of <b>intermediate</b> <b>contours</b> using distance maps. For each single branching region, a procedure then performs the skinning of contour curves represented by cubic B-spline curves on a common knot vector, {{each of which is}} fitted to its contour points within a given accuracy. In order to acquire a more compact representation for the surface, the method includes an algorithm for reducing the number of knots in the common knot vector. The approximation surface to the cross-sections is represented by a set of by cubic B-spline surfaces. This method provides a smooth surface model, yet realizes efficient data reduction. close 122...|$|R
40|$|Advancement of {{the imaging}} {{technology}} nowadays demands sophisticated method {{to represent the}} captured image {{in the form of}} curves and surfaces. However, the challenge in curve and surface fitting techniques is the smoothness and accurateness of the result and the complexity of the techniques. Therefore, the primary goal of this research is to develop new curve and surface fitting techniques using beta-spline which has G 2 continuity, high accuracy and less number of computations. This is due to the properties of cubic beta-spline itself with G 2 conditions, and located close to the control polygon. Additionally, beta-spline can be controlled in three ways using control points, shape parameters and weights. In beta-spline curve fitting, the control points are calculated using least squares method. The data points are first segmented in comer detection process, then the curve control points are calculated based on the comer points. The developed curve fitting technique is applied on 2 D font of ‘<_s’ (ya) and‘e’ (epsilon). The results are then compared with the another technique using Bspline in terms of total processing time, approximation error, and the number of computation in the algorithm. For beta-spline surface fitting, a new technique to solve branching contours has been carried out. An <b>intermediate</b> <b>contour</b> called composite contour is generated and inserted between the two adjacent image slices. Beta-spline surface is then fitted to the extracted data points from each slice. The developed surface fitting algorithm is applied on 3 D Computerized Tomography (CT) image of human face and Stanford bunny. The reconstructed 3 D images are compared visually with the images using the other techniques. The results show that the reconstructed images using beta-spline surface give similar result obtained using the other techniques...|$|E
40|$|This paper {{introduces}} two efficient algorithms that {{compute the}} Contour Tree of a 3 D scalar field F and its augmented version with the Betti numbers of each isosurface. The Contour Tree {{is a fundamental}} data structure in scientific visualization {{that is used to}} preprocess the domain mesh to allow optimal computation of isosurfaces with minimal overhead storage. The Contour Tree {{can also be used to}} build user interfaces reporting the complete topological characterization of a scalar field, as shown in Figure 1. Data exploration time is reduced since the user understands the evolution of level set components with changing isovalue. The Augmented Contour Tree provides even more accurate information segmenting the range space of the scalar field in portion of invariant topology. The exploration time for a single isosurface is also improved since its genus is known in advance. Our first new algorithm augments any given Contour Tree with the Betti numbers of all possible corresponding isocontours in linear time with the size of the tree. Moreover we show how to extend the scheme introduced in [3] with the Betti number computation without increasing its complexity. Thus, we improve on the time complexity from our previous approach [10] from O(m log m) to O(n log n + m), where m is the number of cells and n is the number of vertices in the domain of F. Our second contribution is a new divide-and-conquer algorithm that computes the Augmented Contour Tree with improved efficiency. The approach computes the output Contour Tree by merging two <b>intermediate</b> <b>Contour</b> Trees and is independent of the interpolant. In this way we confine any knowledge regarding a specific interpolant to an independent function that computes the tree for a single cell. We have implemented this function for the trilinear interpolant and plan to replace it with higher order interpolants when needed. The time complexity is O(n + t log n), where t is the number of critical points of F. For the first time we can compute the Contour Tree in linear time in many practical cases where t = O(n 1 −ɛ). We report the running times for a parallel implementation, showing good scalability with the number of processors...|$|E
40|$|A {{time warp}} contour {{calculator}} {{for use in}} an audio signal decoder for providing a decoded audio signal representation {{on the basis of}} an encoded audio signal representation is configured to receive an encoded warp ratio information, to derive a sequence of warp ratio values from the encoded warp ratio information, and to obtain warp contour node values starting from a time warp contour start value. Ratios between the time warp contour node values and the time warp contour starting value associated with a time warp contour start node are determined by the warp ratio values. The time warp contour calculator is configured to compute a time warp contour node value of a given time warp contour node, which is spaced from the time warp contour starting node by an <b>intermediate</b> time warp <b>contour</b> node {{on the basis of a}} product-formation comprising a ratio between the time warp contour node value of the <b>intermediate</b> time warp <b>contour</b> node and the time warp contour starting value and a ratio between the time warp contour node value of the given time warp contour node and the time-warp contour node value of the <b>intermediate</b> time warp <b>contour</b> node as factors...|$|R
40|$|Abstract—Geographically {{contours}} are virtual lines drawn {{across the}} terrain to join points {{that are at}} same elevation from certain reference point. Contours are essential morphological features that are used along with the associated elevation as basis for generating Terrain Model or Digital Elevation Model for the area of interest. These lines are represented at different scales depending upon the scale of data representation. The smoothness of the elevation model derived from these contours always depends {{on the number of}} contour lines covering the terrain. If there is lesser number of contour lines covering the terrain then the elevation model would rather appear sharp where as on the contrary if the number of line covering the terrain is more than the terrain would rather appear smooth. Further it implies that {{in order to create a}} smooth terrain from a set of contours, either contour map with smaller scale should be taken into consideration or efficient interpolation algorithms should be designed. Terrains for which small scale maps are not available the later proves effective. This research initiative aims at designing an efficient algorithm for determining reduced set of interpolation points for generating <b>intermediate</b> <b>contours</b> based on the concept of angular directional movement acquiring knowledge from weaving approach deployed by Spider...|$|R
40|$|Medical imaging from CT and MRI scans {{has become}} {{essential}} to clinicians for diagnosis, treatment planning and even prevention {{of a wide}} array of conditions. The presentation of image data volumes as 2 D slice series provides some challenges with visualising internal structures. 3 D reconstructions of organs and other tissue samples from data with low scan resolution leads to a ‘stepped’ appearance. This paper demonstrates how to improve 3 D visualisation of features and automated preparation for 3 D printing from such low resolution data, using novel techniques for morphing from one slice to the next. The boundary of the starting contour is grown until it matches the boundary of the ending contour by adapting a variant of the Fast Marching Method. Our spoke based approach generates scalar speed field for FMM by estimating distances to boundaries with line segments connecting the two boundaries. These can be regularly spaced radial spokes or spokes at radial extrema. We introduce clamped FMM by running the algorithm outwards from the smaller boundary and inwards from the larger boundary and combining the two runs to achieve FMM growth stability near the two region boundaries. Our method inserts a series of uniformly distributed <b>intermediate</b> <b>contours</b> between each pair of consecutive slices from the scan volume thus creating smoother feature boundaries. Whilst hard to quantify, our overall results give clinicians an evidently improved tangible and tactile representation of the tissues, that they can examine more easily and even handle...|$|R
40|$|The paper {{shows that}} the contour of a sheet blank at all stages of superplastic forming can be {{described}} using universal formulas known as a «superformula» Gielis and «superellipse» Lamé. The work provides information on the values range of the coefficients entering into these formulas. The paper shows the results of approximation {{by means of the}} proposed formulas of shell contours manufactured by superplastic forming by different methods. The application of the «superformula» to approximate the spherical shell contours in the first stage of molding has been tested. The graphs that show the ratio of the ordinates of the contours of the spherical shells and the hemisphere are given. It is shown that the contours of the shells from the AlMg 5 and Pb- 38 %Sn alloys are rejected {{in the direction of the}} formation of parabolas. It was found that the deviations increase with decreasing the coefficient of high-speed hardening of the alloy of the shell. The contour of AMg 6 alloy shells and blanks of variable thickness, with maximum in the central zone, is diverted from the hemisphere towards the ellipse. The first stage of forming in a non-uniform temperature field forms a contour of shells, similar to an ellipse. Forming shells with the ratio of the height (H) and the radius (R) as H = 0, 6 R in the angular zones of the matrix forms <b>intermediate</b> <b>contours</b> of different shapes. This shape depends on the presence of lubricant between the shell and the bottom of the matri...|$|R
40|$|Surface {{reconstruction}} of anatomical structures {{is an integral}} part of medical modeling. Contour information is extracted from serial cross-sections of tissue data and is stored as "slice" files. Although there are several reasonably efficient triangulation algorithms that reconstruct surfaces from slice data, the models generated from them have a jagged or faceted appearance due to the large inter-slice distance created by the sectioning process. Moreover, inconsistencies in user input aggravate the problem. So, we created a method that reduces inter-slice distance, as well as ignores the inconsistencies in the user input. Our method called the piecewise weighted implicit functions, is based on the approach of weighting smaller implicit functions. It takes only a few slices at a time to construct the implicit function. This method is based on a technique called variational interpolation. Other approaches based on variational interpolation have the disadvantage of becoming unstable when the model is quite large with more than a few thousand constraint points. Furthermore, tracing the <b>intermediate</b> <b>contours</b> becomes expensive for large models. Even though some fast fitting methods handle such instability problems, there is no apparent improvement in contour tracing time, because, the value of each data point on the contour boundary is evaluated using a single large implicit function that essentially uses all constraint points. Our method handles both these problems using a sliding window approach. As our method uses only a local domain to construct each implicit function, it achieves a considerable run-time saving over the other methods. The resulting software produces interpolated models from large data sets in a few minutes on an ordinary desktop computer...|$|R
40|$|Breast {{cancer is}} one of the most {{frequent}} forms of cancer among women all over the world and the early detection of the cancer provides a better chance of proper treatment. A method for automatically reconstructing a 3 -dimensional object (nodule) from serial cross sectional Ultra Sonographic data is presented in this paper. The segmentation of region of interest (ROI) is the most crucial step in 3 dimensional nodule construction for which Fuzzy Stopping Force Level Sets equation is employed [11]. The method combines the Dynamic Elastic Contour Interpolation algorithm [22] to generate a series of <b>intermediate</b> missing <b>contours</b> between each pair of consecutive cross sections. These slices are connected in a definite order and rendered using Phong shading for smooth and complete surface of 3 D nodule can thus be reconstructed...|$|R
40|$|An {{original}} {{method is}} proposed {{to extract the}} most significant volumetric structures in an illuminance im age. The method proceeds in three levels of organization managed by generic grouping principles: (i) from the illuminance im age to amore compact representation of its contents by generic structural in formation extraction leading to a basic contour primitive map; (ii) grouping of the basic primitives in order to form <b>intermediate</b> primitives, the <b>contour</b> junctions; (iii) grouping of these junctions {{in order to build}} the high-level contour primitives, the generic volumetric structures. Experimental results for various images of clutte red scenes show an ability to properly extract the structures of volumetric objects or parts with planar and curved surfaces...|$|R
40|$|Open-pit mining is an {{operation}} in which {{blocks from the}} ground are dug to extract the ore contained in them, and in this process a deeper and deeper pit is formed until the mining operation ends. Mining is often a highly complex industrial operation, with respect to both technological and planning aspects. The latter may involve decisions about which ore to mine and in which order. Furthermore, mining operations are typically capital intensive and long-term, and subject to uncertainties regarding ore grades, future mining costs, and the market prices of the precious metals contained in the ore. Today, most of the high-grade or low-cost ore deposits have already been depleted, and to obtain sufficient profitability in mining operations it is therefore today often a necessity to achieve operational efficiency with respect to both technological and planning issues. In this thesis, we study the open-pit design problem, the open-pit mining scheduling problem, and the open-pit design problem with geological and price uncertainty. These problems give rise to (mixed) discrete optimization models that in real-life settings are large scale and computationally challenging. The open-pit design problem is to find an optimal ultimate contour of the pit, given estimates of ore grades, that are typically obtained from samples in drill holes, estimates of costs for mining and processing ore, and physical constraints on mining precedence and maximal pit slope. As is well known, this problem can be solved as a maximum flow problem in a special network. In a first paper, we show that two well known parametric procedures for finding a sequence of <b>intermediate</b> <b>contours</b> leading to an ultimate one, {{can be interpreted as}} Lagrangian dual approaches to certain side-constrained design models. In a second paper, we give an alternative derivation of the maximum flow problem of the design problem. We also study the combined open-pit design and mining scheduling problem, which is the problem of simultaneously finding an ultimate pit contour and the sequence in which the parts of the orebody shall be removed, subject to mining capacity restrictions. The goal is to maximize the discounted net profit during the life-time of the mine. We show in a third paper that the combined problem can also be formulated as a maximum flow problem, if the mining capacity restrictions are relaxed; in this case the network however needs to be time-expanded. In a fourth paper, we provide some suggestions for Lagrangian dual heuristic and time aggregation approaches for the open-pit scheduling problem. Finally, we study the open-pit design problem under uncertainty, which is taken into account by using the concept of conditional value-atrisk. This concept enables us to incorporate a variety of possible uncertainties, especially regarding grades, costs and prices, in the planning process. In real-life situations, the resulting models would however become very computationally challenging...|$|R
40|$|International audienceThis {{anatomic}} study {{presents an}} analysis of the distribution of calbindin immunohistochemistry in the human striatopallidal complex. Entire brains were sectioned perpendicularly to the mid-commissural line into 70 -microm-thick sections. Every tenth section was immunostained for calbindin. Calbindin labeling exhibited a gradient on the basis of which three different regions were defined: poorly labeled, strongly labeled, and <b>intermediate.</b> Corresponding <b>contours</b> were traced in individual sections and reformatted as three-dimensional structures. The poorly labeled region corresponded to the dorsal part of the striatum and to {{the central part of the}} pallidum. The strongly labeled region included the ventral part of the striatum, the subcommissural part of the external pallidum but also the adjacent portion of its suscommissural part, and the anterior pole of the internal pallidum. The intermediate region was located between the poorly and strongly labeled regions. As axonal tracing and immunohistochemical studies in monkeys show a similar pattern, poorly, intermediate, and strongly labeled regions were considered as the sensorimotor, associative, and limbic territories of the human striatopallidal complex, respectively. However, the boundaries between these territories were not sharp but formed gradients of labeling, which suggests overlapping between adjacent territories. Similarly, the ventral boundary of the striatopallidal complex was blurred, suggesting a structural intermingling with the substantia innominata. This three-dimensional partitioning of the human striatopallidal complex could help to define functional targets for high-frequency stimulation with greater accuracy and help to identify new stimulation sites...|$|R
40|$|The {{effect of}} ethanol on the {{structure}} of DNA confined to mica in the presence of Mg 2 + was examined by varying the ethanol concentration and imaging the DNA by atomic force microscopy. Contour length measurements of the DNA show a transition from all-B-form at 0 % ethanol to all-A-form at> 25 % ethanol. At <b>intermediate</b> ethanol concentrations, <b>contour</b> lengths suggest that individual molecules of air-dried DNA are trapped with mixed compositions of A-form and B-form. The relative composition depends on the ethanol concentration. Fitting the length distributions at intermediate ethanol concentrations to a simple binomial model results in an upper bound estimate for the A-form and B-form domains of ∼ 54 bp in the individual molecules. In addition to length changes, the apparent persistence length of DNA decreases with increasing ethanol concentration. At high concen-trations of ethanol (> 20 %), DNA formed several higher order structures, including flower shaped condensates and toroids...|$|R
40|$|AbstractThis review {{focuses on}} low and <b>intermediate</b> stages of <b>contour</b> shape processing. It is {{split into two}} main sections, ‘Contour Detection’ and ‘Shape Discrimination and Representation’. The first section {{examines}} contrast detection of elements within a contour (“collinear facilitation”) and the detection of contours in noise (“contour integration”). The second section deals with the discrimination and representation of simple and complex shapes. Perceptual effects on contour detection {{have been linked to}} low-level, long-range lateral interactions between neighbouring neurons in V 1. Experimental results suggest a complex network of interactions that are context dependent, with collinearity being the dominant factor. While lateral connections are an obvious candidate for linking contour elements into spatially extended contours, the long-range interactions are insufficient to account for human performance in a variety of tasks. Data suggest the existence of global mechanisms that integrate information beyond that of neighbouring cells and are influenced by the overall features of a stimulus. Evidence from psychophysics and physiology is converging towards the identification of an intermediate level of shape processing, where sensitivity to such global attributes emerge...|$|R

