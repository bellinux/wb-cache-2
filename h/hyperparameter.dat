731|1370|Public
25|$|In 2011 it {{was shown}} by Polson and Scott that the SVM admits a Bayesian {{interpretation}} through the technique of data augmentation. In this approach the SVM {{is viewed as a}} graphical model (where the parameters are connected via probability distributions). This extended view allows for the application of Bayesian techniques to SVMs, such as flexible feature modeling, automatic <b>hyperparameter</b> tuning, and predictive uncertainty quantification. Recently, a scalable version of the Bayesian SVM was developed by Wenzel et al. enabling the application of Bayesian SVMs to big data.|$|E
2500|$|This {{suggests}} that we create a conditional prior of the mean on the unknown variance, with a <b>hyperparameter</b> specifying {{the mean of the}} pseudo-observations associated with the prior, and another parameter specifying the number of pseudo-observations. [...] This number serves as a scaling parameter on the variance, making it possible to control the overall variance of the mean relative to the actual variance parameter. [...] The prior for the variance also has two hyperparameters, one specifying the sum of squared deviations of the pseudo-observations associated with the prior, and another specifying once again the number of pseudo-observations. [...] Note that each of the priors has a <b>hyperparameter</b> specifying the number of pseudo-observations, and in each case this controls the relative variance of that prior. [...] These are given as two separate hyperparameters so that the variance (aka the confidence) of the two priors can be controlled separately.|$|E
2500|$|The {{respective}} {{numbers of}} pseudo-observations add {{the number of}} actual observations to them. [...] The new mean <b>hyperparameter</b> is once again a weighted average, this time weighted by the relative numbers of observations. [...] Finally, the update for [...] {{is similar to the}} case with known mean, but in this case the sum of squared deviations is taken with respect to the observed data mean rather than the true mean, and as a result a new [...] "interaction term" [...] needs to be added {{to take care of the}} additional error source stemming from the deviation between prior and data mean.|$|E
40|$|Learning <b>Hyperparameters</b> for Neural Network Models Using Hamiltonian Dynamics Kiam Choo Master of Science Graduate Department of Computer Science University of Toronto 2000 We {{consider}} a {{feedforward neural network}} model with <b>hyperparameters</b> controlling groups of weights. Given some training data, the posterior distribution of the weights and the <b>hyperparameters</b> {{can be obtained by}} alternately updating the weights with hybrid Monte Carlo and sampling from the <b>hyperparameters</b> using Gibbs sampling. However, this method becomes slow for networks with large hidden layers. We address this problem by incorporating the <b>hyperparameters</b> into the hybrid Monte Carlo update. However, the region of state space under the posterior with large <b>hyperparameters</b> is huge and has low probability density, while the region with small <b>hyperparameters</b> is very small and very high density. As hybrid Monte Carlo inherently does not move well between such regions, we reparameterize the weights to make the two [...] ...|$|R
30|$|The <b>hyperparameters,</b> α’, β’, and v in the {{evaluation}} function in Eq.  4 control {{the behavior of the}} estimated model parameter (s). The <b>hyperparameters</b> must be appropriately determined prior to minimization of {{the evaluation}} function in order to estimate accurate model parameters. Because of nonlinearity with{{in the evaluation}} function (Eq.  4), the maximization of marginal likelihood technique for all three <b>hyperparameters</b> is computationally high-cost and non-stable. Therefore, the <b>hyperparameters</b> were appropriately selected as described below.|$|R
3000|$|... {{are called}} <b>hyperparameters.</b> They {{describe}} the community-wide mean and SD of species responses. In the terminology of random-effect models, the <b>hyperparameters</b> are the fixed effects and species parameters are random effects (Gelman and Hill 2007).|$|R
50|$|One {{may take}} a single value for a given <b>hyperparameter,</b> or one can iterate and take a {{probability}} distribution on the <b>hyperparameter</b> itself, called a hyperprior.|$|E
5000|$|Instead {{of using}} a single value for a given <b>hyperparameter,</b> one can instead {{consider}} a probability distribution of the <b>hyperparameter</b> itself; {{this is called a}} [...] "hyperprior." [...] In principle, one may iterate this, calling parameters of a hyperprior [...] "hyperhyperparameters," [...] and so forth.|$|E
5000|$|Hyperopt is a {{distributed}} <b>hyperparameter</b> optimization {{library in}} Python.|$|E
40|$|I examine two {{approximate}} {{methods for}} computational implementation of Bayesian hierarchical models, that is, models which include unknown <b>hyperparameters</b> such as regularization constants and noise levels. In the `evidence framework' the model parameters are integrated over, {{and the resulting}} evidence is maximized over the <b>hyperparameters.</b> The optimized <b>hyperparameters</b> are used to define a Gaussian approximation to the posterior distribution. In the alternative `MAP' method, the true posterior probability is found by integrating over the <b>hyperparameters.</b> The true posterior is then maximized over the model parameters, and a Gaussian approximation is made. The similarities of the two approaches, and their relative merits, are discussed, and comparisons are made with the ideal hierarchical Bayesian solution. In moderately ill [...] posed problems, integration over <b>hyperparameters</b> yields a probability distribution with a skew peak which causes significant biases to arise in the MAP method. In co [...] ...|$|R
30|$|These {{methods are}} also {{applied to the}} {{optimization}} of <b>hyperparameters</b> of the Batch-Normalized Maxout Network in Network proposed by Chang et al. [25]. Note that this network is deeper and has many more <b>hyperparameters</b> to optimize than LeNet.|$|R
30|$|The {{evolution}} of deep neural networks (DNNs) has dramatically improved {{the accuracy of}} character recognition [1], object recognition [2, 3], and other tasks. However, the their increasing complexity increases the number of <b>hyperparameters,</b> which makes tuning of <b>hyperparameters</b> an intractable task.|$|R
5000|$|Harmonica is a Python 3 {{package for}} {{spectral}} <b>hyperparameter</b> optimization.|$|E
5000|$|Auto-WEKA is a Bayesian <b>hyperparameter</b> {{optimization}} {{layer on}} top of WEKA.|$|E
50|$|<b>Hyperparameter</b> {{optimization}} {{contrasts with}} actual learning problems, {{which are also}} often cast as optimization problems, but optimize a loss function on the training set alone. In effect, learning algorithms learn parameters that model/reconstruct their inputs well, while <b>hyperparameter</b> optimization is to ensure the model does not e.g., overfit its data by tuning, as by regularization.|$|E
40|$|Systems {{based on}} {{artificial}} neural networks (ANNs) have achieved state-of-the-art results in many {{natural language processing}} tasks. Although ANNs do not require manually engineered features, ANNs have many <b>hyperparameters</b> to be optimized. The choice of <b>hyperparameters</b> significantly impacts models' performances. However, the ANN <b>hyperparameters</b> are typically chosen by manual, grid, or random search, which either requires expert experiences or is computationally expensive. Recent approaches based on Bayesian optimization using Gaussian processes (GPs) is a more systematic way to automatically pinpoint optimal or near-optimal machine learning <b>hyperparameters.</b> Using a previously published ANN model yielding state-of-the-art results for dialog act classification, we demonstrate that optimizing <b>hyperparameters</b> using GP further improves the results, and reduces the computational time {{by a factor of}} 4 compared to a random search. Therefore it is a useful technique for tuning ANN models to yield the best performances for natural language processing tasks. Comment: Accepted as a conference paper at IEEE SLT 2016. The two authors contributed equally to this wor...|$|R
30|$|It {{is evident}} that simple {{classical}} manual search, grid search, and random search remain common; thus, we consider {{that most people are}} unwilling to adjust the <b>hyperparameters</b> of a difficult optimization method or implement the method and do not have sufficient computing resources to optimize DNN <b>hyperparameters.</b>|$|R
40|$|I examine two {{approximate}} {{methods for}} computational implementation of Bayesian hierarchical models, that is, models which include unknown <b>hyperparameters</b> such as regularization constants and noise levels. In the 'evidence framework' the model parameters are integrated over, {{and the resulting}} evidence is maximized over the <b>hyperparameters.</b> The optimize...|$|R
50|$|The {{traditional}} way of performing <b>hyperparameter</b> optimization has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the <b>hyperparameter</b> space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training setor evaluation on a held-out validation set.|$|E
50|$|One often uses a prior {{which comes}} from a {{parametric}} family of probability distributions - this is done partly for explicitness (so one can write down a distribution, and choose the form by varying the <b>hyperparameter,</b> {{rather than trying to}} produce an arbitrary function), and partly so that one can vary the <b>hyperparameter,</b> particularly in the method of conjugate priors, or for sensitivity analysis.|$|E
5000|$|... the <b>hyper{{parameter}}</b> of the parameter distribution, i.e., [...] This {{may in fact}} be a vector of hyperparameters.|$|E
50|$|Levy et al. (2015) {{show that}} much of the {{superior}} performance of word2vec or similar embeddings in downstream tasks is not a result of the models per se, but of the choice of specific <b>hyperparameters.</b> Transferring these <b>hyperparameters</b> to more 'traditional' approaches yields similar performances in downstream tasks.|$|R
3000|$|Note that if α→ 0 and β→ 0, {{then the}} {{posterior}} mean λ̂_B(θ [...]) {{is the same}} as the MLE λ̂(θ [...]) and the posterior mode λ̂_MAP(θ [...]) {{is the same as}} the unbiased estimator λ̂_p(θ [...]) when the nuisance parameter θ is known. However since the marginal distributions for λ and θ depend on <b>hyperparameters</b> α and β, if there is no sufficient information on the prior, then values of the <b>hyperparameters</b> cannot be determined. Now, a method for addressing these <b>hyperparameters</b> for robust estimation results is developed.|$|R
30|$|The full Bayesian {{framework}} for BGS-NMF model {{based on the}} posterior distribution of parameters and <b>hyperparameters</b> p(Θ,Φ|X) is not analytically tractable. A stochastic optimization scheme is adopted. We develop a MCMC sampling algorithm for approximate inference through iteratively generating samples of parameters Θ and <b>hyperparameters</b> Φ according to the posterior distribution. This algorithm converges by those samples. The key idea of MCMC sampling is to simulate a stationary ergodic Markov chain whose samples asymptotically follow the posterior distribution p(Θ,Φ|X). The estimates of parameters Θ and <b>hyperparameters</b> Φ are then computed via Monte Carlo integrations on the simulated Markov chains. For simplicity, the segment index l is neglected in derivation of MCMC algorithm for BGS-NMF. At each new iteration t+ 1, the BGS-NMF parameters Θ(t+ 1) and <b>hyperparameters</b> Φ(t+ 1) are sequentially sampled in an order of {Ar,Sr,Ah,Sh,Σ,αr,βr,αh,βh,λr,λh,γr,δr,γh,δh} according to their corresponding conditional posterior distributions. In this subsection, we describe the calculation of conditional posterior distributions under BGS-NMF parameters {Ar,Sr,Ah,Sh,Σ}. The conditional posterior distributions for <b>hyperparameters</b> {αr,βr,αh,βh,λr,λh,γr,δr,γh,δh} are derived in the Appendix.|$|R
50|$|<b>Hyperparameter</b> {{optimization}} or tuning is {{the problem}} of choosing a set of optimal hyperparameters for a learning algorithm.|$|E
5000|$|The {{estimate}} of the <b>hyperparameter</b> M is obtained using the moment estimates for the variance of the two-stage model: ...|$|E
5000|$|... mlr is a R {{package that}} {{contains}} {{a large number of}} different <b>hyperparameter</b> optimization techniques for machine learning problems.|$|E
2500|$|Therefore, the {{posterior}} is (dropping the <b>hyperparameters</b> as conditioning factors): ...|$|R
30|$|In TM, θ, ϕ, and z are {{the latent}} {{variables}} {{that have to}} be estimated. Together with the Dirichlet distributed <b>hyperparameters</b> α and β, the model is called Latent Dirichlet Allocation [17, 19]. The <b>hyperparameters</b> α and β should be interpreted as smoothing factors for respectively topic-to-document (θ) and word-to-topic (ϕ) assignments.|$|R
50|$|For {{specific}} learning algorithms, it {{is possible}} to compute the gradient with respect to <b>hyperparameters</b> and then optimize the <b>hyperparameters</b> using gradient descent. The first usage of these techniques was focused on neural networks. Since then, these methods have been extended to other models such as support vector machines or logistic regression.|$|R
5000|$|SUMO-Toolbox is a MATLAB toolbox for {{surrogate}} modeling {{supporting a}} wide collection of <b>hyperparameter</b> optimization algorithm for many model types.|$|E
50|$|In Bayesian statistics, a hyperprior is a prior {{distribution}} on a <b>hyperparameter,</b> that is, on a parameter of a prior distribution.|$|E
5000|$|... where [...] is a <b>hyperparameter</b> that {{controls}} {{how much the}} algorithm will prefer simpler functions to functions that fit the data better.|$|E
3000|$|To {{estimate}} the smoothing parameters σ_ϕ^ 2, σ_θ^ 2 [...] and τ_ε^ 2 [...] simultaneously {{with all the}} unknown smooth functions, highly dispersed but proper hyperpriors are assigned to them. An inverse gamma distribution with <b>hyperparameters</b> a and b was chosen, e.g. [...] p(τ_ε^ 2)∼ IG(a_j,b_j) [...]. Standard choices for the <b>hyperparameters</b> are a[*]=[*] 1 and b[*]=[*] 0.005 or a[*]=[*]b[*]=[*] 0.001. The latter choice is close to Jeffrey’s non-informative prior. Sensitivity analysis was performed by using different values for the <b>hyperparameters,</b> but the results {{turned out to be}} less sensitive to the different choices. The results presented are those for a[*]=[*]b[*]=[*] 0.001.|$|R
40|$|Abstract. In the {{framework}} of statistical learning, fitting a model to a given problem is usually done in two steps. First, model selection is performed, to set {{the values of the}} <b>hyperparameters.</b> Second, training results in the selection, for this set of values, of a function performing satisfactorily on the problem. Choosing the values of the <b>hyperparameters</b> remains a difficult task, which has only been addressed so far in the case of bi-class SVMs. We derive here a solution dedicated to M-SVMs. It is based on a new bound on the risk of large margin classifiers. Keywords: Multi-class SVMs, <b>hyperparameters,</b> soft margin parameter. ...|$|R
40|$|We {{consider}} {{the task of}} tuning <b>hyperparameters</b> in SVM models based on min-imizing a smooth performance validation function, e. g., smoothed k-fold cross-validation error, using non-linear optimization techniques. The key computation in {{this approach is that}} of the gradient of the validation function with respect to <b>hyperparameters.</b> We show that for large-scale problems involving a wide choice of kernel-based models and validation functions, this computation can be very ef-ciently done; often within just a fraction of the training time. Empirical results show that a near-optimal set of <b>hyperparameters</b> can be identied by our approach with very few training rounds and gradient computations. ...|$|R
