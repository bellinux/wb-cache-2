9|0|Public
5000|$|Data.Syntax (Uit <b>Dataplex),</b> Festival Voor Nieuwe Muziek > Hapy New Ears 2007 (Gonzo Circus, 2007) ...|$|E
5000|$|Nvelo <b>DataPlex</b> SSD caching {{software}} {{was announced in}} 2011, and was acquired by Samsung in 2012.|$|E
50|$|<b>Dataplex</b> is a 2005 album by Japanese sound artist Ryoji Ikeda. It was {{released}} on December 2005 by independent record label Raster-Noton on CD.|$|E
5000|$|... "Caution! This CD {{contains}} specific waveform {{data that}} performs a data-read test for optical drives. The last track will cause some CD players to experience playback errors, with no damage to equipment." [...] —Warning sticker attached inside <b>Dataplex.</b>|$|E
5000|$|<b>Dataplex</b> was Ryoji Ikeda's first {{full-length}} release {{since his}} 2002's orchestral Op. It {{is the first}} part in the Datamatics series, which [...] "interrogates and interprets the mass of raw computer data surrounding us all." [...] Over all, it is the musician's seventh solo album. The last track [...] "data.adaplex" [...] contains playback errors.|$|E
5000|$|Reflecting on Datamatics as a series, Dusted Magazines John Seelbach called <b>Dataplex</b> [...] "a {{surprising}} and enthralling start." [...] Allmusics Rob Theakston {{also said it}} was [...] "a promising start to what {{could be one of}} the most ambitious projects of the post-glitch movement." [...] Brainwasheds Jon Whitney revered the album with [...] "rather rhythmic, challenging, and completely enjoyable: something most computer musician types have failed at." ...|$|E
50|$|The {{previous}} version, MareNostrum 3, {{consisted of}} 3,056 IBM <b>DataPlex</b> DX360M4 compute nodes, {{for a total}} of 48,896 physical Intel Sandy Bridge cores running at 2.6 Ghz, and 84 Xeon Phi 5110P in 42 nodes. MareNostrum 3 had 36 racks dedicated to calculations. In total, each rack had 1,344 cores and 2,688 GB of memory. Each IBM iDataPlex Compute rack was composed of 84 IBM iDataPlex dx360 M4 compute nodes and 4 Mellanox 36-port Managed FDR10 IB Switches. dx360 M4 compute nodes were grouped into a 2U Chassis, having two columns of 42 2U Chassis.|$|E
40|$|This thesis {{addresses}} a particular {{aspect of the}} retrieval of information {{from a wide variety}} of global information sources. This aspect is based on the model of a user working on a topic of interest over an extended period of time. During the time period, information is accessed, assembled, and correlated to satisfy the user 2 ̆ 7 s view of the topic. The objective of the research described in this thesis is to ensure that the information accessed by the user over the extended period of time is both complete and consistent from the user 2 ̆ 7 s viewpoint. A fundamental problem is achieving such consistency is that the information sources, typically databases, or data files, are independently controlled with their own individual viewpoints. Changes in content, structure, and access can therefore be made without the direct knowledge of the user. In investigating this problem a number of current implementations of Heterogeneous Distributed Database Systems (HDDS) have been evaluated, including WAIS, ANSAware, and <b>DATAPLEX.</b> The mechanisms available in such systems do not address the requirements of the problem outlined in this thesis. A new set of mechanisms have been researched and implemented on a testbed as the central part of this thesis in order to match the requirements The core of this testbed is a workstation-based interface for the user termed the Computer Software Interface (CSI). The CSI has been implemented to demonstrate that the set of mechanisms proposed are viable. One major aspect of the CSI has been the design and development of a local working environment for the user, and the associated theoretical proof needed to demonstrate that successful and complete access of this environment may be performed. The thesis demonstrates, both theoretically and practically, how the user may be presented with consistent data from independent data sources over an extended time period...|$|E
40|$|Understanding the {{microstructure}} {{of the financial}} market requires the processing of {{a vast amount of}} data related to individual trades, and sometimes even multiple levels of quotes. Analyzing such a large volume of data requires tremendous computing power that is not easily available to financial academics and regulators. Fortunately, public funded High Performance Computing (HPC) power is widely available at the National Laboratories in the US. In this paper we demonstrate that the HPC resource and the techniques for data-intensive sciences can be used to greatly accelerate the computation of an early warning indicator called Volume-synchronized Probability of Informed trading (VPIN). The test data used in this study contains five and a half year?s worth of trading data for about 100 most liquid futures contracts, includes about 3 billion trades, and takes 140 GB as text files. By using (1) a more efficient file format for storing the trading records, (2) more effective data structures and algorithms, and (3) parallelizing the computations, we are able to explore 16, 000 different ways of computing VPIN in less than 20 hours on a 32 -core IBM <b>DataPlex</b> machine. Our test demonstrates that a modest computer is sufficient to monitor a vast number of trading activities in real-time ? an ability that could be valuable to regulators. Our test results also confirm that VPIN is a strong predictor of liquidity-induced volatility. With appropriate parameter choices, the false positive rates are about 7 percent averaged over all the futures contracts in the test data set. More specifically, when VPIN values rise above a threshold (CDF > 0. 99), the volatility in the subsequent time windows is higher than the average in 93 percent of the cases...|$|E

