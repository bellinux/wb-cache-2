1157|363|Public
5|$|As {{with the}} {{previous}} Transformers installments, Industrial Light & Magic (ILM) was the main visual effects company for Dark of the Moon. ILM {{had been working on}} the pre-visualization for six months before principal photography started, resulting in 20 minutes worth of footage. <b>Digital</b> <b>Domain</b> also rendered 350 shots, including the characters Laserbeak, Brains, Wheelie, and the Decepticon protoforms concealed on the moon, the space bridge, and a skydiving sequence.|$|E
5|$|Filming, {{originally}} {{scheduled to}} begin in Los Angeles in July2008, began in Kamloops, Savona, Cache Creek and Ashcroft, British Columbia. With a Screen Actors Guild strike looming, the film's producers made a contingency plan to salvage it. Uncharted Territory, <b>Digital</b> <b>Domain,</b> Double Negative, Scanline, and Sony Pictures Imageworks were hired to create 2012 computer-animated visual effects.|$|E
5|$|Six {{visual effects}} {{companies}} worked in The Rise of Cobra, {{the most prominent}} being <b>Digital</b> <b>Domain,</b> which handled the Paris action sequences and the opening convoy sequence. For the Eiffel Tower destruction, special software was written for depicting how the crumbling metal works. To create the digital Eiffel Tower, the technicians {{had access to the}} original building plans, and built a digital model so complex that it could not fit in a single computer file. For the nanomites, designers used two proprietary software applications for their depiction—one made by <b>Digital</b> <b>Domain,</b> and another by Prime Focus VFX, which also created tools to generate 3D cloud and sky environments for the aerial scenes. Many scenarios were almost fully developed by computer-generated imagery, such as the landing platform of the Pit, the Cobra ice caverns, and the final underwater battle. As for the sound effects themselves, only one is considered popular and isn't instantly recognizable. When the pulse cannon fires upon the main submarine during the polar assault, the sound of a program de-resolution from the 1982 cult movie classic TRON can be heard.|$|E
40|$|Part of {{a special}} issue on the public domain. The writer {{discusses}} how the digitalization of information {{and the development of}} global digital networks have contributed positively to the public domain. She provides a map of the public domain as a prelude to a discussion of how various legal and policy developments could affect the <b>digital</b> public <b>domain.</b> She argues that the Uniform Computer Information Transactions Act, the Collections of Information Anti-Piracy Act, and the Digital Millennium Copyright Act of 1998 affect a wide area of the <b>digital</b> public <b>domain</b> and may produce negative synergistic effects that undermine the <b>digital</b> public <b>domain.</b> She assesses methods for averting these threats to the <b>digital</b> public <b>domain...</b>|$|R
50|$|The Design Lab has {{improved}} steadfastly {{since its inception}} and now boasts of well-established research facilities in the analog and <b>digital</b> <b>domains,</b> with the infrastructure comprising workstations with advanced configuration and support available for almost all the latest CAD tools from Cadence Design Systems, Synopsys, Mentor Graphics, Coware, etc.|$|R
40|$|This paper {{highlights}} {{some important}} issues for those attempting to enable such entrepreneurial {{knowledge and skills}} to be developed using CBL materials. This study has implications for the design and use of CBL material for entrepreneurship education at all levels. The implications have importance for strategic decision makers, educators, technologists and designers within the educational and <b>digital</b> <b>domains</b> in the short, medium and long term...|$|R
5|$|Chris Townsend {{served as}} visual effects {{supervisor}} for the film, which featured over 2,000 visual effects shots and was worked on by 17 studios, including Weta Digital, <b>Digital</b> <b>Domain,</b> Scanline VFX, Trixter, Framestore, Luma Pictures, Fuel VFX, Cantina Creative, Cinesite, The Embassy Visual Effects, Lola, Capital T, Prologue and Rise FX. Townsend said that from January 2013 {{through the end}} of filming in April, the collective crew had one day of downtime, otherwise working seven days a week and 14 to 18 hours a day.|$|E
5|$|Today, it {{is often}} {{preferred}} to carry out filtering in the <b>digital</b> <b>domain</b> where complex algorithms are much easier to implement, but analogue filters do still find applications, especially for low-order simple filtering tasks and are often still the norm at higher frequencies where digital technology is still impractical, or at least, less cost effective. Wherever possible, and especially at low frequencies, analogue filters are now implemented in a filter topology which is active {{in order to avoid}} the wound components (i.e. inductors, transformers, etc.) required by passive topology.|$|E
5|$|After {{the release}} of Iron Man 2 in May 2010, Favreau, who served as director, decided not to return, and in February 2011 Black was hired to write and direct the film. Black and Pearce opted to make the script more character-centric and focused on thriller elements. Throughout April and May 2012, the film's {{supporting}} cast was filled out, with Kingsley, Pearce, and Hall brought in to portray key roles. Filming began on May 23, and lasted through December 17, 2012, primarily at EUE/Screen Gems Studios in Wilmington, North Carolina. Additional shooting took place at various locations around North Carolina, as well as Florida, China, and Los Angeles. The visual effects were handled by 17 companies, including Scanline VFX, <b>Digital</b> <b>Domain,</b> and Weta Digital. The film was converted to 3D in post-production.|$|E
30|$|Overcoming {{the classic}} {{dichotomy}} between analog and <b>digital</b> <b>domains,</b> such {{new generation of}} mixed-signal embedded systems is fueling {{the development of more}} efficient and performance solutions in several technology areas, to name just a few, distributed control-sensing-actuation units to increase safety, comfort, and engine efficiency in vehicles, software defined and cognitive radios for multimode multimedia communication, wireless sensor networks for ambient intelligence in home, industrial scenarios, or for environment control.|$|R
40|$|Traditional {{computer}} interfaces often {{create a}} separation between physical and digital spaces. This prevents people from working as easily in virtual worlds {{as they do}} in the real world, or from moving seamlessly between reality and virtual reality. In this paper we describe how this chasm can be crossed though the use of an interface metaphor, Tangible Augmented Reality, that combines elements from both the physical and <b>digital</b> <b>domains...</b>|$|R
30|$|The CONCERT data plane {{includes}} Radio Interfacing Equipments (RIEs), software-defined {{switches and}} computational resources. The RIEs {{deal with the}} signaling process between radio and <b>digital</b> <b>domains</b> besides taking care of radio resource slicing functions. These system’s components provide the last-hop communication to final users. Conjointly with the base stations, {{it is possible to}} provide local servers {{at the edge of the}} network (Fog) to minimize the latency of applications and provide ubiquity to final users.|$|R
5|$|Filming resumed on August 3, 2006 at the Bonneville Salt Flats in Utah for 70 {{days off}} the California coast, {{as all the}} {{shooting}} required in the Caribbean had been conducted in 2005. Davy Jones's Locker was shot at Utah, and it was shot in a monochromatic way to represent its different feeling from the usual colorful environment of a pirate. The climactic battle was shot in a former air hangar at Palmdale, California, where the cast had to wear wetsuits underneath their costumes on angle-tipped ships. The water-drenched set was kept in freezing temperatures, to make sure bacteria did not come inside and infect the crew. A second unit shot at Niagara Falls. Industrial Light & Magic did 750 effects shots, while <b>Digital</b> <b>Domain</b> also took on 300. They spent just five months finishing the special effects. The film posed numerous challenges in creating water-based effects.|$|E
5|$|The first-person shooter {{video game}} Halo 3 {{was the focus}} of an {{extensive}} marketing campaign which began with the game's developer, Bungie, announcing the game via a trailer at the Electronic Entertainment Expo in May 2006. Microsoft, the game's publisher, planned a five-pronged marketing strategy to maximize sales and to appeal to casual and hard-core gamers. Bungie produced trailers and video documentaries to promote the game, partnering with firms such as <b>Digital</b> <b>Domain</b> and Weta Workshop. Licensed products including action figures, toys, and Halo 3-branded soda were released in anticipation of the game; the franchise utilized more than forty licensees to promote the game, and the advertising campaign ultimately cost more than $40 million.|$|E
5|$|Visual {{effects for}} Deadpool were {{produced}} by <b>Digital</b> <b>Domain</b> (DD), Atomic Fiction, Blur Studio, Weta Digital, Rodeo FX, and Luma Pictures. Reynolds credited Miller and his visual effects experience with producing {{a film that}} looked liked others with much bigger budgets. Motion capture supervisor Greg LaSalle agreed with this, noting that Miller held off on working on the CGI for Colossus until after the film was edited to avoid spending money on shots {{that would not be}} used. Miller worked with visual effects supervisor Rothbart to design and complete the film's 1500 effects shots. These were up from a planned 700 shots, with 800 of them completed in the last four weeks of production.|$|E
40|$|We {{narrow the}} gap between {{simulations}} of nuclear magnetic resonance dynamics on <b>digital</b> <b>domains</b> (such as CT-images) and measurements in D-dimensional porous media. We point out with two basic domains, the ball and the cube in D dimensions, that due to a digital uncertainty in representing the real pore surfaces of dimension D- 1, there is a systematic error in simulated dynamics. We then reduce this error by introducing local Robin boundary conditions...|$|R
40|$|URL] Report is {{the outcome}} of the work of the EU funded COMMUNIA Network on the <b>Digital</b> Public <b>Domain.</b> The Report was {{undertaken}} to (i) review the activities of COMMUNIA; (ii) investigate the state of the <b>digital</b> public <b>domain</b> in Europe; and (iii) recommend policy strategies for enhancing a healthy public <b>domain</b> and making <b>digital</b> content in Europe more accessible and usable. The COMMUNIA Report website can be found here: [URL] 194 status: publishe...|$|R
40|$|International audienceThis paper {{proposes a}} readout design for CMOS image sensors. It has been {{squeezed}} into a 7. 5 um pitch under a 0. 28 um 1 P 3 M technology. The ADC performs one 14 -bit conversion in only 1. 5 us and targets a theoretical DNL feature about + 1. 3 /- 1 at 14 -bit accuracy. Correlated Double Sampling (CDS) is performed {{both in the}} analog and <b>digital</b> <b>domains</b> to preserve the image quality...|$|R
25|$|Cameron was the {{co-founder and}} CEO of <b>Digital</b> <b>Domain,</b> a visual-effects {{production}} and technology company.|$|E
25|$|This {{method of}} calculating the average power gives the active power {{regardless}} of harmonic {{content of the}} waveform. In practical applications, this would {{be done in the}} <b>digital</b> <b>domain,</b> where the calculation becomes trivial when compared to the use of rms and phase to determine active power.|$|E
25|$|The {{visual effects}} were done by two Los Angeles-based VFX houses. Rhythm and Hues Studios {{designed}} the Yetis and dragons, while <b>Digital</b> <b>Domain</b> handled the battle scenes with Jet Li's terracotta warriors. The {{pool of water}} resembling diamonds took Rhythm and Hues eleven months to complete. The A.I. software Massive, used for the Lord of the Rings films, was used to create the undead battle scenes.|$|E
40|$|A {{self-contained}} {{approach to}} DSP techniques and applications in radar imagingThe processing of radar images, in general, {{consists of three}} major fields: Digital Signal Processing (DSP); antenna and radar operation; and algorithms used to process the radar images. This book brings together material from these different areas to allow readers to gain {{a thorough understanding of}} how radar images are processed. The book is divided into three main parts and covers:* DSP principles and signal characteristics in both analog and <b>digital</b> <b>domains,</b> advanced signal sampling, an...|$|R
40|$|Analog-to-digital {{conversion}} and its reverse, digital-to-analog conversion, are ubiquitous in all modern electronics, from instrumentation and telecommunication equipment to computers and entertainment. We shall explore {{the consequences of}} converting signals between the analog and <b>digital</b> <b>domains</b> and give {{an overview of the}} internal architecture and operation of a number of converter types. The importance of analog input and clock signal integrity will be explained and methods to prevent or mitigate the effects of interference will be shown. Examples will be drawn from several manufacturers' datasheets...|$|R
40|$|Ambipolar conduction, {{characterized}} by a superposition of electron and hole currents, has been observed in many next-generation devices including carbon nanotube, graphene, silicon nanowire, and organic transistors. This paper describes exciting new design opportunities in both analog and <b>digital</b> <b>domains,</b> {{all of which are}} inspired by the ability to control ambipolarity during circuit operation. We illustrate this with (i) a single-transistor polarity controllable amplifier, which can greatly simplify communication circuits and (ii) polarity controllable ambipolar logic gates, which are highly expressive yet compact compared to conventional CMOS. ...|$|R
25|$|In March 2007, {{production}} {{moved to}} Los Angeles {{for two more}} months of filming. Principal photography was targeted to last a total of 150 days. Additional time was needed at visual effects house <b>Digital</b> <b>Domain</b> to make the visual effects for the metamorphosis of Brad Pitt's character to the infant stage. The director used a camera system called Contour, developed by Steve Perlman, to capture facial deformation data from live-action performances.|$|E
25|$|All three film {{scores for}} The Lord of the Rings film trilogy (The Fellowship of the Ring, The Two Towers and The Return of the King), {{composed}} by Howard Shore and {{performed by the}} London Philharmonic Orchestra, were mixed at Abbey Road Studios. The recordings themselves were done by CTS-Lansdowne Studios at their permanent studios in the old Watford Town Hall with engineer John Kurlander. All technical support by CTS engineers. All recordings up to 96 tracks were in the <b>digital</b> <b>domain.</b>|$|E
25|$|A DWDM {{terminal}} demultiplexer. At {{the remote}} site, the terminal de-multiplexer consisting of an optical de-multiplexer {{and one or}} more wavelength-converting transponders separates the multi-wavelength optical signal back into individual data signals and outputs them on separate fibers for client-layer systems (such as SONET/SDH). Originally, this de-multiplexing was performed entirely passively, except for some telemetry, as most SONET systems can receive 1,550nm signals. However, {{in order to allow}} for transmission to remote client-layer systems (and to allow for <b>digital</b> <b>domain</b> signal integrity determination) such de-multiplexed signals are usually sent to O/E/O output transponders prior to being relayed to their client-layer systems. Often, the functionality of output transponder has been integrated into that of input transponder, so that most commercial systems have transponders that support bi-directional interfaces on both their 1,550nm (i.e., internal) side, and external (i.e., client-facing) side. Transponders in some systems supporting 40GHz nominal operation may also perform forward error correction (FEC) via digital wrapper technology, as described in the ITU-T G.709 standard.|$|E
40|$|This paper {{provides}} {{a summary of}} the initial development of a Web portal for the <b>digital</b> government <b>domain.</b> Information retrieval techniques commonly used to find information on the Internet are discussed along with the problems associated with these techniques that {{led to the development of}} the Digital Government Web portal (DGPort). We also discuss the advantages that DGPort could have for researchers in the <b>digital</b> government <b>domain</b> as well as the value-added features that this portal provides. Future evaluation plans for the portal are also described. 1...|$|R
40|$|This {{thesis is}} {{concerned}} with understanding and describing the nature of 'community' within <b>digital</b> <b>domains.</b> A literature review indicates multiple media use within communities. The increasing range of personal and organisational technologies available suggests digital communities {{are more than just}} online communities. As such they require a new method of assessment. The design of digital communities should be based on an understanding of 'community' in <b>digital</b> <b>domains.</b> Previous assessments, often focusing exclusively on the Internet, failed to recognise the ways in which technologies are integrated within communities. A new assessment method should allow the examination of integrated technology effects on communities through an analysis of important community features. To assess digital communities a framework consisting of five headings was developed. The framework allows the effects of technologies to be examined across a range of communities. Taking a convergent methodologies approach five studies were undertaken covering a range of technologies and media integration issues. The results suggest that digital communities are groups of people using technology to support their social interaction needs. Media use within digital communities is heavily integrated and the social needs of community members drive technology use. Designers should provide communities with flexible technology that permits integration and member adaptation. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|Abstract. The {{design of}} digital-analog mixed SoC {{involves}} RF/analog and <b>digital</b> <b>domains,</b> {{how to effectively}} improve the design reliability and to reduce the development cycles has become a research hotspot. This paper establishes the appropriate behavioral models of RF / analog / digital IP modules, and carries out the behavioral simulation based on the built mixed-domain simulation platform and the behavioral libraries of RF/analog/digital IP module, which enhances the reliability and stability of mixed SoC design, and reduces the design cycle. Those explorations {{may be helpful to}} the designers of digital-analog mixed SoC...|$|R
25|$|The Lord of the Rings film series used many {{groundbreaking}} {{practical and}} digital visual effects. The first film has around 540 effects shots, the second 799, {{and the third}} 1488 (2730 in total). The total moves up to 3420 with the extended cuts. Two hundred and sixty visual effects artists worked on the trilogy, and the number would double by The Two Towers. The crew, led by Jim Rygiel and Randy Cook, would work long and hard hours overnight to produce special effects within a short space of time, especially with Jackson's active imagination. For example, they produced several major shots of Helm's Deep within {{the last six weeks}} of post-production of The Two Towers, and the same number of shots for The Two Towers within the last six weeks on The Return of the King. Despite Weta Workshop being the major stylistic force behind the films, a single scene where Arwen confronts the Black Riders in The Fellowship of the Ring was done by <b>Digital</b> <b>Domain.</b>|$|E
25|$|The film's {{special effects}} were {{completed}} by seven different visual effects houses: <b>Digital</b> <b>Domain,</b> Giant Studios, The Third Floor, MPC, Soho VFX, Rodeo FX and Hatch Productions. Creating the giants took four main steps. The {{first step was}} Pre-Capture, in which performance capture was used to capture the actor's facial and body movements and render them in a real-time virtual environment. The second step took place during principal photography, where Simulcam technology was used to help the human characters virtually interact with the giants that were rendered earlier in Pre-Capture. The third step was Post-Capture, a second performance capture shoot to adjust giants' movements to seamlessly fit the live-action performances. The final step involved {{putting the finishing touches}} on the giant's animation, skin, hair and clothing, and composition in the shots. Creating the beanstalk involved two main requirements: set extension for shots of the actors interacting with the beanstalk, which were shot against a bluescreen, and complete CG renderings for shots of the beanstalk growing and extending from Earth into the world of the giants.|$|E
25|$|A {{dedicated}} {{voltage comparator}} chip such as LM339 {{is designed to}} interface with a digital logic interface (to a TTL or a CMOS). The output is a binary state often used to interface real world signals to digital circuitry (see analog to digital converter). If there is a fixed voltage source from, for example, a DC adjustable device in the signal path, a comparator is just {{the equivalent of a}} cascade of amplifiers. When the voltages are nearly equal, the output voltage will not fall into one of the logic levels, thus analog signals will enter the <b>digital</b> <b>domain</b> with unpredictable results. To make this range as small as possible, the amplifier cascade is high gain. The circuit consists of mainly Bipolar transistors. For very high frequencies, the input impedance of the stages is low. This reduces the saturation of the slow, large P-N junction bipolar transistors that would otherwise lead to long recovery times. Fast small Schottky diodes, like those found in binary logic designs, improve the performance significantly though the performance still lags that of circuits with amplifiers using analog signals. Slew rate has no meaning for these devices. For applications in flash ADCs the distributed signal across eight ports matches the voltage and current gain after each amplifier, and resistors then behave as level-shifters.|$|E
40|$|By case studies, we {{have shown}} that {{system-level}} specifications can be mapped systematically to an optimized block diagram (case study “saw-tooth generator”). Furthermore, we could systematically and in a quantitative way evaluate the effect of oversampling to a system (case study “converter”). In another case study, we could evaluate {{the influence of a}} different partitioning (analog/SC/digital) to the design. The proposed methods {{can be seen as a}} prototype of a first tool for “analog/digital codesign ” because they permit the system-level optimization of mixed-signal applications across the border between analog and <b>digital</b> <b>domains...</b>|$|R
40|$|Artificial Intelligence Lab, Department of MIS, University of ArizonaThis paper {{provides}} {{a summary of}} the initial development of a Web portal for the <b>digital</b> government <b>domain.</b> Information retrieval techniques commonly used to find information on the Internet are discussed along with the problems associated with these techniques that {{led to the development of}} the Digital Government Web portal (DGPort). We also discuss the advantages that DGPort could have for researchers in the <b>digital</b> government <b>domain</b> as well as the value-added features that this portal provides. Future evaluation plans for the portal are also described...|$|R
40|$|In {{this paper}} we discuss a {{possible}} discrete approximation of non-metrical Minkowski distances. The existing approaches for Minkowski metrics considering distance functions based on local neighborhoods are not suitable for this task in their present form. We can overcome this difficulty with considering the minimum of such distance functions. In this way we can take full advantage of the former theoretical and applied results. We consider several possibilities to measure the error of approximation and propose corresponding distance functions for optimal approximation. A fast chamfering algorithm is also provided to generate the approximate distance values for applications in <b>digital</b> <b>domains.</b> ...|$|R
