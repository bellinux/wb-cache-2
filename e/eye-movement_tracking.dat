29|11|Public
25|$|Eye {{movement}} in reading involves the visual processing of written text. This {{was described by}} the French ophthalmologist Louis Émile Javal in the late 19th century. He reported that eyes do not move continuously along a line of text, but make short, rapid movements (saccades) intermingled with short stops (fixations). Javal's observations were characterised by a reliance on naked-eye observation of eye {{movement in}} the absence of technology. From the late 19th to the mid-20th century, investigators used early tracking technologies to assist their observation, in a research climate that emphasised the measurement of human behaviour and skill for educational ends. Most basic knowledge about eye movement was obtained during this period. Since the mid-20th century, there have been three major changes: the development of non-invasive <b>eye-movement</b> <b>tracking</b> equipment; the introduction of computer technology to enhance the power of this equipment to pick up, record and process the huge volume of data that eye movement generates; and the emergence of cognitive psychology as a theoretical and methodological framework within which reading processes are examined. Sereno & Rayner (2003) believed that the best current approach to discover immediate signs of word recognition is through the recordings of eye movements and event-related potential.|$|E
50|$|Hewitt D. Crane (1927-2008) was an American {{engineer}} {{best known}} for his pioneering work at SRI International on ERMA (Electronic Recording Machine, Accounting), for Bank of America, magnetic digital logic, neuristor logic, the development of an <b>eye-movement</b> <b>tracking</b> device, and a pen-input device for computers.|$|E
50|$|The {{technical}} {{principle of}} the paradigm involves a computer interfaced with both an <b>eye-movement</b> <b>tracking</b> system (Eye-tracker) and a display of the visual stimulus. A fast computer, eye-tracker and display allow reliable results (Veneri 2010, Pomplun 2001). In gaze-contingent paradigms the stimulus display is continuously updated {{as a function of}} the observers' current gaze position; for instance, Shimojo & Simion 2003 applied a central hole to see thescene only through the fovea, giving to subjects the sensation of seeing through a telescope.|$|E
25|$|Some {{of these}} {{artifacts}} {{can be useful}} in various applications. The EOG signals, for instance, can be used to detect and <b>track</b> <b>eye-movements,</b> which are very important in polysomnography, and is also in conventional EEG for assessing possible changes in alertness, drowsiness or sleep.|$|R
40|$|Literature {{suggests}} a significant {{relation between the}} use of blogs (intended as multimedia and interactive products) and the improvement of cognitive strategies: the management of a blog can be used to explore multimedia information processing. Moreover, emotional information is able to capture attention and to enhance memory in cognitive tasks. Our study was aimed at exploring the link among blogging, multimedia processing and emotions by means of eye-tracking technology. Five types of blogs were created, each including a neutral and an emotional post. Twenty-four students were then asked to explore the blogs while their <b>eye-movements</b> where <b>tracked.</b> Then they were asked to answer questionnaires about cognitive style, emotions and memory questions. Results showed that text was fixated longer than images and emotional posts were fixated longer than neutral ones. Females explore blogs more accurately and score better in accidental learning, which is fostered by medium fixation lengths...|$|R
40|$|Generation Y (age 18 - 31) is a {{very large}} and {{economically}} powerful generation, containing eighty-two million people and spending $ 200 billion annually. It is not surprising that companies are interested in gaining the patronage of this group, particularly via the web. Surprisingly, very little research into making web pages appealing to this important demographic has been done. This paper addresses this need through two separate studies. The first, an online survey, provides evidence that our proposed score for predicting the visual appeal of web pages reflects the self-report measure of what pages Generation Y likes. To refine these findings, an eye tracking study is conducted using the pages that were most and least liked in Study I. Participants’ <b>eye-movement</b> is <b>tracked</b> while browsing these pages, providing evidence of what attracts their attention. The results of these two studies suggest that Generation Y may prefer pages that include a main large image, images of celebrities, little text, and a search feature. This research has both important theoretical and practical implications...|$|R
5000|$|Viskontas's {{research}} has explored the neurological basis of memory, reasoning and self-identity, {{and most recently}} she has studied creativity in people with neurodegeneration. This is an appealing area to study, according to Viskontas, because it allows her to [...] "marvel at what's magnificent about the brain rather than just bemoan what's been lost when things go awry." [...] Techniques used in her research include single-unit recording in patients with epilepsy, high-resolution functional magnetic resonance imaging, <b>eye-movement</b> <b>tracking,</b> voxel-based morphometry, and various behavioral tasks in healthy adults, patients with epilepsy, and patients with neurodegenerative diseases such as frontotemporal dementia, semantic dementia and Alzheimer's disease. She has published over 30 research articles and book chapters.|$|E
50|$|When users shared similar {{interests}} or expertise, they could join or form a group. A group was often created {{in order to}} discuss a particular topic or to collaborate on a specific project. Unlike a Stage, groups might be either public or private. They could consist of as few as two members who wish to privately share content, to many members who wish to make public their collaborative efforts. For example, if a scientist in Germany has published a paper on landmark-identification using <b>eye-movement</b> <b>tracking,</b> she could upload this paper to her Stage. Once uploaded, the document could be linked to a video posted by a researcher in the U.S. in the same virtual group, and both of these could be subsequently linked to a company or university lab developing new eye-tracking technology presented on its Stage.|$|E
50|$|Eye {{movement}} in reading involves the visual processing of written text. This {{was described by}} the French ophthalmologist Louis Émile Javal in the late 19th century. He reported that eyes do not move continuously along a line of text, but make short, rapid movements (saccades) intermingled with short stops (fixations). Javal's observations were characterised by a reliance on naked-eye observation of eye {{movement in}} the absence of technology. From the late 19th to the mid-20th century, investigators used early tracking technologies to assist their observation, in a research climate that emphasised the measurement of human behaviour and skill for educational ends. Most basic knowledge about eye movement was obtained during this period. Since the mid-20th century, there have been three major changes: the development of non-invasive <b>eye-movement</b> <b>tracking</b> equipment; the introduction of computer technology to enhance the power of this equipment to pick up, record and process the huge volume of data that eye movement generates; and the emergence of cognitive psychology as a theoretical and methodological framework within which reading processes are examined. Sereno & Rayner (2003) believed that the best current approach to discover immediate signs of word recognition is through the recordings of eye movements and event-related potential.|$|E
40|$|The {{existence}} of an attentional window [...] a limited region in visual space at which attention is directed [...] has been invoked to explain why sudden visual onsets {{may or may not}} capture overt or covert attention. Here, we test the hypothesis that observers voluntarily control the size of this attentional window to regulate whether or not environmental signals can capture attention. We have used a novel approach to test this: participants <b>eye-movements</b> were <b>tracked</b> while they performed a search task that required dynamic gaze-shifts. During the search task, abrupt onsets were presented that cued the target positions at different levels of congruency. The participant knew these levels. We determined oculomotor capture efficiency for onsets that appeared at different viewing eccentricities. From these, we could derive the participant's attentional window size as a function of onset congruency. We find that the window was small during the presentation of low-congruency onsets, but increased monotonically in size with an increase in the expected congruency of the onsets. This indicates that the attentional window is under voluntary control and is set according to the expected relevance of environmental signals for the observer's momentary behavioral goals. Moreover, our approach provides a new and exciting method to directly measure the size of the attentional window...|$|R
40|$|The two {{components}} of voluntary <b>tracking</b> <b>eye-movements</b> in primates, pursuit and saccades, are generally viewed as relatively independent oculomotor subsystems that move the eyes {{in different ways}} using independent visual information. Although saccades have long been known to be guided by visual processes related to perception and cognition, only recently have psychophysical and physiological studies provided compelling evidence that pursuit is also guided by such higher-order visual processes, {{rather than by the}} raw retinal stimulus. Pursuit and saccades also {{do not appear to be}} entirely independent anatomical systems, but involve overlapping neural mechanisms that might be important for coordinating these two types of eye movement during the tracking of a selected visual object. Given that the recovery of objects from real-world images is inherently ambiguous, guiding both pursuit and saccades with perception could represent an explicit strategy for ensuring that these two motor actions are driven by a single visual interpretation...|$|R
40|$|Spontaneous human {{speech is}} {{peppered}} with errors and disfluencies. Previous {{research has demonstrated}} that such errors and disfluencies are not always detrimental to comprehension. When confronted with a filled pause, listener's are able to make predictions about upcoming speech. But, does the length of the edit interval affect how far listener's back-track through given information {{in order to make a}} prediction about the repair? The present experiment examines the on-line effect of fillers on reference comprehension in order to investigate whether edit-length has a direct effect on the listener when deciding how far to back-track through the information already heard in order to make a prediction about the upcoming speech. Participants <b>eye-movements</b> were <b>tracked</b> whilst they were presented with visual and auditory stimuli. Auditory stimuli involved disfluencies with either a long edit interval (830 ms silence) or a short edit interval (415 ms silence) preceded by an editing expression uh. Relative to the two word reparanda, listener's predicted that a no-change repair would be made after hearing a filler followed by a short edit-interval and that a repair involving a change in the word immediately preceding the filler would be made after hearing a filler followed by a long edit-interval. The study demonstrates that edit-length does have an effect on the type of predictions made by listener's. However, regardless of edit-length, listener's do not tend to back-track when making predictions. Rather, they expect speaker's to interrupt themselves immediately after an error has occurred...|$|R
40|$|Abstract. <b>Eye-movement</b> <b>tracking</b> proved its {{potentials}} in {{many areas}} of human-computer interaction. Resting on a hypothesis that eye-direction and mind are linked, some of the HCI researchers have employed eye-movement trackers to investigate the visual attention focus of the participants completing their tasks. Others have used the <b>eye-movement</b> <b>tracking</b> in real-time applications, either as a direct interaction device or as an input to gaze-aware interfaces. Inspired by the previous HCI applications, we propose to utilize eye-movement trackers in adaptive systems research and development in two ways. First, the evaluations of adaptive systems could get an access to the information otherwise unavailable, as for instance to how the visual attention and cognitive processing are influenced by an adaptivity implemented into the evaluated system. Second, we propose to employ the <b>eye-movement</b> <b>tracking</b> technologies for a real-time registration of users ’ loci of visual attention, therefore increasing the awareness of the adaptive systems about their current users. We discuss possible potentials, difficulties and pitfalls of <b>eye-movement</b> <b>tracking</b> when applied to adaptive systems. We argue that a methodological framework of applying eye-tracking into adaptive systems shall be developed. ...|$|E
40|$|Three {{experiments}} are reported that used <b>eye-movement</b> <b>tracking</b> {{to investigate the}} inspection-time effect predicted by Evans' (1996) heuristic-analytic account of the Wason selection task. Evans' account proposes that card selections {{are based on the}} operation of relevance-determining heuristics, whilst analytic processing only rationalizes selections. As such, longer inspection times should be associated with selected cards (which are subjected to rationalization) than with rejected cards. Evidence for this effect has been provided by Evans (1996) using computer-presented selection tasks and instructions for participants to indicate (with a mouse pointer) cards under consideration. Roberts (1998 b) has argued that mouse pointing gives rise to artefactual support for Evans&apos predictions because of biases associated with the task format and the use of mouse pointing. We eradicated all sources of artefact by combining careful task constructions with <b>eye-movement</b> <b>tracking</b> to measure directly on-line attentional processing. All three experiments produced good evidence for the robustness of the inspection-time effect, supporting the predictions of the heuristic-analytic account...|$|E
40|$|This is Restricted Access. The {{article was}} {{published}} in the journal, The Quarterly Journal of Experimental Psychology [© The Experimental Psychology Society] and is available at: [URL] experiments are reported that used <b>eye-movement</b> <b>tracking</b> to investigate the inspectiontime effect predicted by Evans’ (1996) heuristic–analytic account of the Wason selection task. Evans’ account proposes that card selections are based on the operation of relevance-determining heuristics, whilst analytic processing only rationalizes selections. As such, longer inspection times should be associated with selected cards (which are subjected to rationalization) than with rejected cards. Evidence for this effect has been provided by Evans (1996) using computerpresented selection tasks and instructions for participants to indicate (with a mouse pointer) cards under consideration. Roberts (1998 b) has argued that mouse pointing gives rise to artefactual support for Evans’ predictions because of biases associated with the task format and the use of mouse pointing. We eradicated all sources of artefact by combining careful task constructions with <b>eye-movement</b> <b>tracking</b> to measure directly on-line attentional processing. All three experiments produced good evidence for the robustness of the inspection-time effect, supporting the predictions of the heuristic–analytic account...|$|E
40|$|A second grader {{is seated}} {{in front of}} a {{computer}} monitor in Wayne State University 2 ̆ 7 s children 2 ̆ 7 s reading lab, reading aloud from The Wolf 2 ̆ 7 s Chicken Stew by Keiko Kasza [...] a story of a wolf with exceptional culinary skills. As he reads, a cursor appears on the screen, marking the path that his eyes make as he reads: following the text, then zigzagging to the illustration of the wolf carrying a stack of pancakes, back to the text, then to a second illustration, showing that the time is night. By <b>tracking</b> <b>eye-movement</b> patterns of elementary-age children while they read aloud, Karen Feathers, Ph. D., and Poonam Arya, Ph. D., both associate professors of teacher education in the College of Education, are discovering how elementary-age children strategically process text. Of particular interest is readers 2 ̆ 7 use of visual cues within texts to construct meaning while reading...|$|R
50|$|Eyelid {{fluttering}} {{artifacts of}} a characteristic type were previously called Kappa rhythm (or Kappa waves). It is usually {{seen in the}} prefrontal leads, that is, just over the eyes. Sometimes they are seen with mental activity. They are usually in the Theta (4-7 Hz) or Alpha (7-14 Hz) range. They were named because they were believed to originate from the brain. Later study revealed they were generated by rapid fluttering of the eyelids, sometimes so minute {{that it was difficult}} to see. They are in fact noise in the EEG reading, and should not technically be called a rhythm or wave. Therefore, current usage in electroencephalography refers to the phenomenon as an eyelid fluttering artifact, rather than a Kappa rhythm (or wave). Some of these artifacts can be useful in various applications. The EOG signals, for instance, can be used to detect and <b>track</b> <b>eye-movements,</b> which are very important in polysomnography, and is also in conventional EEG for assessing possible changes in alertness, drowsiness or sleep.|$|R
40|$|Adolescents {{and adults}} show {{preferences}} for {{male and female}} body shapes consistent with evolutionary theories of reproductive fitness and mate selection. However, when these preferences for females with narrow waists (i. e., 0. 7 waist-to-hip ratio) and men with broad shoulders (i. e., mesomorphic body shape) emerge during the lifespan is largely unknown. To address this knowledge gap, <b>eye-movements</b> were <b>tracked</b> in 144 infants (3 to 18 months of age) during computer presentation of three-dimensional human figures varying in body features thought relevant for reproductive success (e. g., secondary sex characteristics, waist-to-hip ratio). When presented with pairs of figures differing in apparent sex, male and female infants looked significantly longer at the female figure compared to the male figure, a new finding that extends previous research showing preferences for female faces in infancy. When presented with same-sex figures differing in characteristics associated with mate value, male and female infants looked longer at a low mate value male (i. e., an endomorphic body type) compared to a high mate value male (i. e., a mesomorphic body type), a finding that replicates the results of previous research. In addition, the novel use of high and low mate value female figures showed a sex difference in visual attention, such that female infants looked longer at the high mate value female figure compared to the low mate female figure whereas male infants showed the opposite pattern of results. In sum, these findings suggest that infants generally do not possess preferences for adult-defined attractive male body shapes. However, infant girls’ greater attention to a female figure with an adult-preferred waist-to-hip ratio raises the possibility that evolved preferences for 0. 7 waist-to-hip ratio influence girls’ later preference for toys representing females with an hourglass shape, perhaps supporting elaboration of adult social behaviors that enhance reproductive success (e. g., cooperative breeding) ...|$|R
30|$|Sometimes, {{evidence}} from neuroscience {{is used to}} answer psychological questions by measuring various outcomes. For example, the intensity of an emotional response can often be assessed with physiological measures, such as skin resistance and heart rate, in situations where it is inconvenient or questionable simply to ask people to rate the intensity of their emotions. Such uses of neuroscience are not explanations at all. They are tools of measurement, with the same status as response-time measurement, <b>eye-movement</b> <b>tracking,</b> or introspective reports.|$|E
40|$|Understanding {{bottom-up}} and top-down visual attention mechanisms {{related to}} visual quality perception can be greatly beneficial {{for the design}} of effective objective quality metrics. Subjective studies based on <b>eye-movement</b> <b>tracking</b> have been recently published that try to get more insight in these interactions. However, it is still not easy to find coherence across their results, also due to the different methodologies adopted to analyze eye-tracking data. In this paper we propose a robust methodology to measure differences between eye-tracking data collected under different experimental conditions. The proposed method takes into account inter-observer variability and content effects, producing results that give an accurate insight in attention variations...|$|E
40|$|Video {{communication}} systems for deaf people {{are limited in}} terms of quality and performance. Analysis of visual attention mechanisms for sign language may enable optimization of video coding systems for deaf users. <b>Eye-movement</b> <b>tracking</b> experiments were conducted with profoundly deaf volunteers while watching sign language video clips. Deaf people are found to fixate mostly on the facial region of the signer to pick up small detailed movements associated with facial expression and mouth shapes. Lower resolution, peripheral vision is used to process information from larger, rapid movements of the signer in the video clips. A coding scheme that gives priority to {{the face of the}} signer may be applied to improve perception of video quality for sign language communication. Visual perception is the process of acquiring knowledge about environmental objects and events by extracting information from the light they emit or reflect (Palmer, 2002). How we ‘‘see’ ’ remains an active research challenge for vision scientists and specialists. Understanding the detection, recognition, and interpretation of visual information could have a tremendous impact on how we present and use visual information and on the design of information systems. The challenge is to understand how visual information can be The authors would like to acknowledge the help and support of Jim Hunter who acted as BSL interpreter and organized volunteers for the experiments. Special thanks to Edith Ewen and the deaf people at the Aberdeen Deaf Social and Sports Club for their continued interest and support and for taking part in the <b>eye-movement</b> <b>tracking</b> experiments. Correspondence should be sent to Laura J...|$|E
40|$|Social {{participation}} {{requires the}} processing and utilization of visual information and early {{interactions with the}} environment can shape neurological development, setting children on a typical or atypical developmental trajectory. In the case of autism spectrum disorders (ASD), early manifestations of atypical visual, social attention (i. e., joint attention) {{are one of the}} earliest markers of atypical development {{and one of the most}} influential processes contributing to development in other domains (e. g., language). The current study aimed to assess multiple aspects of low-level visual attention, through a modified Posner-paradigm, that may contribute to social behavior and social cognition. Behavioral reaction time (RT) and <b>eye-movements</b> were <b>tracked</b> through an experimental task, for individuals (ages 8 to 18 years) with typical development (TD), ASD, and attention-deficit/hyperactivity disorder (ADHD), to measure how participants perceived and responded to directionally-meaningful visual information; social functioning was measured using standardized and experimental assessments of social behavior and cognition. Behavioral results indicated that the ASD group demonstrated more difficulty overriding and reallocating their attention when it was directed to an incorrect location; this finding was exaggerated for non-social (arrow) cues, but decreased for social (face) cues, when compared to their TD peers. Evidence also suggested reduced attentional engagement in the visual cues, as supported by both RT and eye-tracking evidence, when comparing the ASD group to both comparison groups. The contributions of social salience, response salience, and visual-field laterality were also assessed. The ADHD group, despite characteristic variability in RT, performed most similarly to their TD peers. The results from this study indicate that reduced engagement in visual information may limit individuals with ASD’s ability to identify relevant visual stimuli and that, once engaged, individuals with ASD may struggle to use this information to efficiently modify their behavioral response. Encouragingly, once attention is engaged, individuals with ASD appear able to interpret the directional cues as meaningful. These findings in the context of a controlled, experimental paradigm are likely exacerbated in the complex, dynamic nature of real-life social situations and implications for early intervention were discussed...|$|R
40|$|The {{effects of}} aging on eye {{movements}} are well {{studied in the}} laboratory. Increased saccade latencies or decreased smooth-pursuit gain are well established findings. The question remains whether these findings {{are influenced by the}} rather untypical environment of a laboratory; that is, whether or not they transfer to the real world. We measured 34 healthy participants between the age of 25 and 85 during two everyday tasks in the real world: (I) walking down a hallway with free gaze, (II) visual tracking of an earth-fixed object while walking straight-ahead. Eye movements were recorded with a mobile light-weight eye tracker, the EyeSeeCam (ESC). We find that age significantly influences saccade parameters. With increasing age, saccade frequency, amplitude, peak velocity, and mean velocity are reduced and the velocity/amplitude distribution as well as the velocity profile become less skewed. In contrast to laboratory results on smooth pursuit, we did not find a significant effect of age on <b>tracking</b> <b>eye-movements</b> in the real world. Taken together, age-related eye-movement changes as measured in the laboratory only partly resemble those in the real world. It is well-conceivable that in the real world additional sensory cues, such as head-movement or vestibular signals, may partially compensate for age-related effects, which, according to this view, would be specific to early motion processing. In any case, our results highlight the importance of validity for natural situations when studying the impact of aging on real-life performance...|$|R
40|$|Video {{communication}} systems for deaf people {{are limited in}} terms of quality and performance. Analysis of visual attention mechanisms for sign language may enable optimization of video coding systems for deaf users. <b>Eye-movement</b> <b>tracking</b> experiments were conducted with profoundly deaf volunteers while watching sign language video clips. Deaf people are found to fixate mostly on the facial region of the signer to pick up small detailed movements associated with facial expression and mouth shapes. Lower resolution, peripheral vision is used to process information from larger, rapid movements of the signer in the video clips. A coding scheme that gives priority to {{the face of the}} signer may be applied to improve perception of video quality for sign language communication. Visual perception is the process of acquiring knowledg...|$|E
40|$|The main {{contribution}} {{of this work}} is the design of an application framework based on both conversational agents and user profiling technologies {{for the development of}} e-commerce services. User profiles are exploited by conversational agents to help customers in retrieving potentially interesting products from a catalogue. Three techniques were used for collecting data for a usability test: <b>eye-movement</b> <b>tracking,</b> questionnaire, and recording the user-system dialogue. The main outcomes of the experimental sessions are: (1) the dialogue capabilities of the agent facilitate the interaction between the user and the e-commerce site; and, (2) user profiles improve the retrieval capabilities of the agent. Finally, some limitations of the user profiling techniques adopted in the framework are discussed and a more sophisticated content-based profiling technique is proposed...|$|E
40|$|AbstractQuantitative {{research}} into a pilot’s attention allocation mechanism {{is required in}} the optimization design of an aircraft human–machine interface and system evaluation. After making a comprehensive consideration of several factors, including the importance of information, information detective efficiency and human errors, a pilot attention allocation model {{was built on the}} basis of hybrid entropy. In order to make a verification of the pilot attention allocation model, a simulation model of a head-up display (HUD) used to present flight indicators was developed. After setting the membership degrees of the importance for different indicators according to their priorities, the experiments on the key-press response and <b>eye-movement</b> <b>tracking</b> were designed and carried out under the cruise and hold modes. As the experiment results are in good agreement with the theoretical model, the effectiveness of the pilot attention allocation model based on fuzzy theory is confirmed...|$|E
40|$|Bookmarks are a {{valuable}} webpage re-visitation technique, {{but it is}} often difficult to find desired items in extensive bookmark collections. This experiment used response-time measures and <b>eye-movement</b> <b>tracking</b> to investigate how different information structures within bookmarks influence their salience and recognizability. Participants were presented with a series of news websites. The task following presentation of each site was to find the bookmark indexing the previously-seen page as quickly as possible. The Informational Structure of bookmarks was manipulated (top-down vs. bottom-up verbal organizations), together with the Number of Informational Cues present (one, two or three). Only this latter factor affected gross search times: Two cues were optimal, one cue was highly sub-optimal. However, more detailed eye-movement analyses of fixation behaviour on target items revealed interactive effects of both experimental factors, suggesting that the efficacy of bookmark recognition is crucially dependent on having an optimal combination of information quantity and information organization...|$|E
40|$|<b>Eye-movement</b> <b>tracking</b> is {{a method}} that is {{increasingly}} being employed to study usability issues in HCI contexts. The objectives of the present chapter are threefold. First, we introduce the reader to the basics of eye-movement technology, and also present key aspects of practical guidance to those who {{might be interested in}} using eye tracking in HCI research, whether in usability-evaluation studies, or for capturing people’s eye movements as an input mechanism to drive system interaction. Second, we examine various ways in which eye movements can be systematically measured to examine interface usability. We illustrate the advantages of a range of different eyemovement metrics with reference to state-of-the-art usability research. Third, we discuss the various opportunities for eye-movement studies in future HCI research, and detail some of the challenges that need to be overcome to enable effective application of the technique in studying the complexities of advanced interactive-system use...|$|E
40|$|Despite the {{popularity}} of the Wason selection task in the psychology of reasoning, doubt remains as to whether card choices actually reflect a process of reasoning. One view is that while participants reason about the cards and their hidden sidesas indicated by protocol analysisthis reasoning merely confabulates explanations for cards that were preconsciously cued. This hypothesis has apparently been supported by studies that show that participants predominantly inspect cards which they end up selecting. In this paper, we reanalyse the data of one such study, which used <b>eye-movement</b> <b>tracking</b> to record card inspection times (Ball, Lucas, Miles, Gale, 2003). We show that while cards favoured by matching bias are inspected for roughly equal lengths of times, their selection rates are strongly affected by their logical status. These findings strongly support a two-stage account in which attention is necessary but not sufficient for card selections. Hence, reasoning does indeed affect participants' choices on this task...|$|E
40|$|Evans’ (e. g., 1996) Heuristic-Analytic {{theory of}} {{reasoning}} in Wason’s selection task proposes {{the existence of}} implicit processes that direct attention to ‘relevant’ aspects {{of the problem and}} thereby determine card selections. This account also proposes that people pursue explicit rationalisations of relevance-determined choices. Our recent studies (e. g., Ball et al., 2003) have measured aspects of on-line attentional processing using <b>eye-movement</b> <b>tracking</b> and have supported the idea that card selections are driven by relevance and then subjected to rationalisation processes. For example, eyemovement data have revealed a reliable inspection-time imbalance between selected and non-selected cards. Our results, to date, however, have related to selection tasks with indicative contents. Here we report an eye-tracking study that involved deontic selection tasks. Various eye-movement measures revealed predicted differences in inspection times between selected and non-selected cards, with the magnitude of the effect being similar to that observed in studies of the indicative task. We discuss our results in relation to current theories of processing in the selection task...|$|E
40|$|WOS: 000323733800002 International audienceThis study {{investigated}} whether an odor can affect infants' attention to visually presented objects {{and whether it}} can selectively direct visual gaze at visual targets {{as a function of}} their meaning. Four-month-old infants (n =  48) were exposed to their mother's body odors while their visual exploration was recorded with an <b>eye-movement</b> <b>tracking</b> system. Two groups of infants, who were assigned to either an odor condition or a control condition, looked at a scene composed of still pictures of faces and cars. As expected, infants looked longer at the faces than at the cars but this spontaneous preference for faces was significantly enhanced in presence of the odor. As expected also, when looking at the face, the infants looked longer at the eyes than at any other facial regions, but, again, they looked at the eyes significantly longer {{in the presence of the}} odor. Thus, 4 -month-old infants are sensitive to the contextual effects of odors while looking at faces. This suggests that early social attention to faces is mediated by visual as well as non-visual cues...|$|E
40|$|Evans ’ (e. g., 1996) Heuristic-Analytic {{theory of}} {{reasoning}} in Wason’s selection task proposes {{the existence of}} implicit processes that direct attention to ‘relevant ’ aspects {{of the problem and}} thereby determine card selections. This account also proposes that people pursue explicit rationalisations of relevance-determined choices. Our recent studies (e. g., Ball et al., 2003) have measured aspects of on-line attentional processing using <b>eye-movement</b> <b>tracking</b> and have supported the idea that card selections are driven by relevance and then subjected to rationalisation processes. For example, eye-movement data have revealed a reliable inspection-time imbalance between selected and non-selected cards. Our results, to date, however, have related to selection tasks with indicative contents. Here we report an eye-tracking study that involved deontic selection tasks. Various eye-movement measures revealed predicted differences in inspection times between selected and non-selected cards, with the magnitude of the effect being similar to that observed in studies of the indicative task. We discuss our results in relation to current theories of processing in the selection task...|$|E
40|$|<b>Eye-movement</b> <b>tracking</b> is a {{potential}} source of real-time adaptation in a learning environment. In {{order to have a}} more comprehensive and accurate picture of a user's interactions with a learning environment, we need to know which interface features he/she visually inspected, what strategies they used and what cognitive efforts they made to complete tasks. Such knowledge allows intelligent systems to be proactive, rather than reactive, to users' actions. Tutorial dialogues is one of the strategies used by Intelligent Tutoring Systems (ITSs) and has been empirically shown to significantly improve learning. EER-Tutor is a constraintbased ITS used to teach conceptual database design. This paper presents the preliminary results of a project that investigates how students interact with the tutorial dialogues in EERTutor using both eye-gaze data and student-system interaction logs. Our findings indicate that advanced students are selective of the interface areas they visually focus on whereas novices waste time by paying attention to interface areas that are inappropriate for the task at hand. Novices are also unaware that they require help with the tutorial dialogues. Myse Elmadani, Antonija Mitrovic & Amali Weerasingh...|$|E
40|$|This study {{investigated}} whether an odor can affect infants ’ attention to visually presented objects {{and whether it}} can selectively direct visual gaze at visual targets {{as a function of}} their meaning. Four-month-old infants (n = 48) were exposed to their mother’s body odors while their visual exploration was recorded with an <b>eye-movement</b> <b>tracking</b> system. Two groups of infants, who were assigned to either an odor condition or a control condition, looked at a scene composed of still pictures of faces and cars. As expected, infants looked longer at the faces than at the cars but this spontaneous preference for faces was significantly enhanced in presence of the odor. As expected also, when looking at the face, the infants looked longer at the eyes than at any other facial regions, but, again, they looked at the eyes significantly longer {{in the presence of the}} odor. Thus, 4 -month-old infants are sensitive to the contextual effects of odors while looking at faces. This suggests that early social attention to faces is mediated by visual as well as non-visual cues...|$|E
40|$|While {{there is}} an {{extensive}} literature on the tendency to mimic emotional expressions in adults, {{it is unclear how}} this skill emerges and develops over time. Specifically, it is unclear whether infants mimic discrete emotion-related facial actions, whether their facial displays are moderated by contextual cues and whether infants’ emotional mimicry is constrained by developmental changes in the ability to discriminate emotions. We therefore investigate these questions using Baby-FACS to code infants’ facial displays and <b>eye-movement</b> <b>tracking</b> to examine infants’ looking times at facial expressions. Three-, 7 -, and 12 -month-old participants were exposed to dynamic facial expressions (joy, anger, fear, disgust, sadness) of a virtual model which either looked at the infant or had an averted gaze. Infants did not match emotion-specific facial actions shown by the model, but they produced valence-congruent facial responses to the distinct expressions. Furthermore, only the 7 - and 12 -month-olds displayed negative responses to the model’s negative expressions and they looked more at areas of the face recruiting facial actions involved in specific expressions. Our results suggest that valence-congruent expressions emerge in infancy during a period where the decoding of facial expressions becomes increasingly sensitive to the social signal value of emotions...|$|E
30|$|<b>Eye-movement</b> <b>tracking</b> and student-system {{interaction}} logs provide {{different types}} of information {{which can be used}} as a potential source of real-time adaptation in learning environments. By analysing student interactions with an intelligent tutoring system (ITS), we can identify sub-optimal behaviour such as not paying attention to important interface components. On the basis of such findings, ITSs can be enhanced to be proactive, rather than reactive, to users’ actions. Tutorial dialogues are one of the teaching strategies used in ITSs which has been shown empirically to significantly improve learning. Enhanced entity-relationship (EER)-Tutor is a constraint-based ITS that teaches conceptual database design. This paper presents the preliminary results of a project that investigates how students interact with the tutorial dialogues in EER-Tutor using both eye-gaze data and student-system interaction logs. Our findings indicate that advanced students are selective of the interface areas they visually focus on, whereas novices waste time by paying attention to interface areas that are inappropriate for the task at hand. Novices are also unaware that they require help with the tutorial dialogues. Furthermore, we have demonstrated that the student’s prior knowledge, the problem complexity and the percentage of the dialogue’s prompts that are answered correctly are factors that can be used to predict future errors. The findings from our study can be used to further enhance EER-Tutor in order to support learning better, including real-time classification of students into novices and advanced students in order to adapt system feedback and interventions.|$|E
